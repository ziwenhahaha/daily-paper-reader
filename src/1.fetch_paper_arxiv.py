import arxiv
import json
import os
import sys
import time
from datetime import datetime, timedelta, timezone

# é¡¹ç›®æ ¹ç›®å½•ï¼ˆå½“å‰è„šæœ¬ä½äº src/ ä¸‹ï¼‰
SCRIPT_DIR = os.path.dirname(__file__)
ROOT_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, ".."))
CONFIG_FILE = os.path.join(ROOT_DIR, "config.yaml")
CRAWL_STATE_FILE = os.path.join(ROOT_DIR, "archive", "crawl_state.json")
SEEN_IDS_FILE = os.path.join(ROOT_DIR, "archive", "arxiv_seen.json")

# ArXiv çš„ä¸»è¦ä¸€çº§åˆ†ç±»åˆ—è¡¨
# æ³¨æ„ï¼šç‰©ç†å­¦æ¯”è¾ƒç‰¹æ®Šï¼ŒArXiv å†å²ä¸Šæœ‰å¾ˆå¤šç‹¬ç«‹çš„ç‰©ç†å­˜æ¡£ï¼Œä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬åˆ—å‡ºä¸»è¦çš„
CATEGORIES_TO_FETCH = [
    "cs", "math", "stat", "q-bio", "q-fin", "eess", "econ",
    "physics", "cond-mat", "hep-ph", "hep-th", "gr-qc", "astro-ph",
]


def load_config() -> dict:
    if not os.path.exists(CONFIG_FILE):
        return {}
    try:
        import yaml  # type: ignore
    except Exception:
        log("[WARN] æœªå®‰è£… PyYAMLï¼Œæ— æ³•è§£æ config.yamlã€‚")
        return {}
    try:
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}
            return data if isinstance(data, dict) else {}
    except Exception as e:
        log(f"[WARN] è¯»å– config.yaml å¤±è´¥ï¼š{e}")
        return {}


def resolve_days_window(default_days: int) -> int:
    config = load_config()
    paper_setting = (config or {}).get("arxiv_paper_setting") or {}
    crawler_setting = (config or {}).get("crawler") or {}

    value = paper_setting.get("days_window")
    if value is None:
        value = crawler_setting.get("days_window")
    try:
        days = int(value)
        return max(days, 1)
    except Exception:
        return max(default_days, 1)


def load_last_crawl_at() -> datetime | None:
    if not os.path.exists(CRAWL_STATE_FILE):
        return None
    try:
        with open(CRAWL_STATE_FILE, "r", encoding="utf-8") as f:
            payload = json.load(f) or {}
    except Exception:
        return None
    raw = str(payload.get("last_crawl_at") or "").strip()
    if not raw:
        return None
    try:
        dt = datetime.fromisoformat(raw.replace("Z", "+00:00"))
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None


def save_last_crawl_at(at_time: datetime) -> None:
    os.makedirs(os.path.dirname(CRAWL_STATE_FILE), exist_ok=True)
    payload = {"last_crawl_at": at_time.astimezone(timezone.utc).isoformat()}
    with open(CRAWL_STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)


def load_seen_state() -> tuple[set[str], datetime | None]:
    if not os.path.exists(SEEN_IDS_FILE):
        return set(), None
    try:
        with open(SEEN_IDS_FILE, "r", encoding="utf-8") as f:
            payload = json.load(f) or {}
    except Exception:
        return set(), None

    raw_ids = payload.get("ids") or []
    if not isinstance(raw_ids, list):
        raw_ids = []
    seen_ids = {str(i).strip() for i in raw_ids if str(i).strip()}

    raw_latest = str(payload.get("latest_published_at") or "").strip()
    latest_dt = None
    if raw_latest:
        try:
            latest_dt = datetime.fromisoformat(raw_latest.replace("Z", "+00:00"))
            if latest_dt.tzinfo is None:
                latest_dt = latest_dt.replace(tzinfo=timezone.utc)
            latest_dt = latest_dt.astimezone(timezone.utc)
        except Exception:
            latest_dt = None

    return seen_ids, latest_dt


def save_seen_state(seen_ids: set[str], latest_published_at: datetime | None) -> None:
    os.makedirs(os.path.dirname(SEEN_IDS_FILE), exist_ok=True)
    payload = {
        "updated_at": datetime.now(timezone.utc).isoformat(),
        "latest_published_at": latest_published_at.astimezone(timezone.utc).isoformat()
        if latest_published_at
        else "",
        "ids": sorted(seen_ids),
    }
    with open(SEEN_IDS_FILE, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

def log(message: str) -> None:
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
    try:
        print(f"[{ts}] {message}", flush=True)
    except BrokenPipeError:
        # å…è®¸ç”¨æˆ·ç”¨ `| head` ç­‰æ–¹å¼æˆªæ–­è¾“å‡ºè€Œä¸è®©è„šæœ¬å´©æºƒ
        try:
            sys.stdout.close()
        except Exception:
            pass


def group_start(title: str) -> None:
    try:
        print(f"::group::{title}", flush=True)
    except BrokenPipeError:
        try:
            sys.stdout.close()
        except Exception:
            pass


def group_end() -> None:
    try:
        print("::endgroup::", flush=True)
    except BrokenPipeError:
        try:
            sys.stdout.close()
        except Exception:
            pass


def iter_time_windows(
    start_date: datetime,
    end_date: datetime,
    chunk_days: int,
) -> list[tuple[datetime, datetime]]:
    """
    å°† [start_date, end_date] æŒ‰ chunk_days å¤©åˆ‡åˆ†æˆå¤šä¸ªâ€œåˆ†é’Ÿçº§é—­åŒºé—´â€çª—å£ã€‚
    ç›®çš„æ˜¯é¿å…å•æ¬¡ arXiv API æŸ¥è¯¢ç»“æœè¿‡å¤§å¯¼è‡´æ·±åˆ†é¡µ/500ã€‚
    """
    chunk_days = max(int(chunk_days or 1), 1)
    if start_date.tzinfo is None:
        start_date = start_date.replace(tzinfo=timezone.utc)
    if end_date.tzinfo is None:
        end_date = end_date.replace(tzinfo=timezone.utc)
    start_date = start_date.astimezone(timezone.utc)
    end_date = end_date.astimezone(timezone.utc)
    if start_date >= end_date:
        return [(start_date, end_date)]

    windows: list[tuple[datetime, datetime]] = []
    cursor = start_date
    delta = timedelta(days=chunk_days)
    minute = timedelta(minutes=1)

    while cursor < end_date:
        # ä»¥â€œåˆ†é’Ÿâ€ä¸ºæœ€å°ç²’åº¦ï¼Œé¿å…ç›¸é‚»çª—å£è¾¹ç•Œé‡å¤ï¼ˆsubmittedDate æŸ¥è¯¢æ˜¯é—­åŒºé—´ï¼‰
        raw_end = min(end_date, cursor + delta)
        if raw_end < end_date:
            window_end = raw_end - minute
        else:
            window_end = raw_end

        if window_end < cursor:
            window_end = cursor

        windows.append((cursor, window_end))

        next_cursor = window_end + minute
        if next_cursor <= cursor:
            next_cursor = cursor + minute
        cursor = next_cursor

    return windows


def fetch_category_in_windows(
    client: arxiv.Client,
    category: str,
    windows: list[tuple[datetime, datetime]],
    seen_ids: set[str],
    unique_papers: dict,
    split_on_error_depth: int = 1,
) -> datetime | None:
    """
    æŒ‰æ—¶é—´çª—å£æŠ“å–å•ä¸ªå¤§ç±»ã€‚
    - å¤±è´¥ç²’åº¦é™ä¸ºâ€œå•çª—å£å¤±è´¥â€ï¼Œä¸ä¼šä¸¢æ‰æ•´ä¸ªåˆ†ç±»ï¼›
    - è‹¥çª—å£ä»ç„¶è¿‡å¤§å¯¼è‡´ 500ï¼Œå¯ç»§ç»­åœ¨ä¸Šå±‚æŒ‰æ›´å°çª—å£é‡è¯•ï¼ˆå¯é€‰ï¼‰ã€‚
    """
    max_published_new: datetime | None = None

    for idx, (win_start, win_end) in enumerate(windows, start=1):
        start_str = win_start.strftime("%Y%m%d%H%M")
        end_str = win_end.strftime("%Y%m%d%H%M")
        group_start(f"Fetch category: {category} (window {idx}/{len(windows)} {start_str}..{end_str})")
        log(f"ğŸš€ Fetching category: {category} | window {idx}/{len(windows)} ...")

        query = f"cat:{category}* AND submittedDate:[{start_str} TO {end_str}]"
        search = arxiv.Search(
            query=query,
            max_results=None,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending,
        )

        count = 0
        try:
            for r in client.results(search):
                pid = r.get_short_id()
                if pid in seen_ids:
                    continue
                if pid in unique_papers:
                    continue

                pdf_link = getattr(r, "pdf_url", None) or r.entry_id
                paper_dict = {
                    "id": pid,
                    "source": "arxiv",
                    "title": r.title.replace("\n", " "),
                    "abstract": r.summary.replace("\n", " "),
                    "authors": [a.name for a in r.authors],
                    "primary_category": r.primary_category,
                    "categories": r.categories,
                    "published": str(r.published),
                    "link": pdf_link,
                }
                unique_papers[pid] = paper_dict
                count += 1

                seen_ids.add(pid)
                published_dt = r.published
                if isinstance(published_dt, datetime):
                    if published_dt.tzinfo is None:
                        published_dt = published_dt.replace(tzinfo=timezone.utc)
                    published_dt = published_dt.astimezone(timezone.utc)
                    if max_published_new is None or published_dt > max_published_new:
                        max_published_new = published_dt

                if count % 200 == 0:
                    log(f"   Category {category} (win {idx}/{len(windows)}): {count} papers fetched...")

            log(f"   âœ… Finished {category} (win {idx}/{len(windows)}): Got {count} new papers.")
        except Exception as e:
            # å•ä¸ªçª—å£å¤±è´¥ä¸å½±å“å…¶ä»–çª—å£/åˆ†ç±»
            log(f"   âŒ Error fetching category {category} (win {idx}/{len(windows)}): {e}")
            # å›é€€ï¼šå¦‚æœçª—å£ä»ç„¶å¾ˆå¤§ï¼Œå°è¯•æŠŠè¯¥çª—å£å†äºŒåˆ†ï¼ˆä»…ä¸€å±‚ï¼Œé¿å…è¿‡åº¦é€’å½’ï¼‰
            if split_on_error_depth > 0 and (win_end - win_start) >= timedelta(days=2):
                mid = win_start + (win_end - win_start) / 2
                mid = mid.replace(second=0, microsecond=0)
                minute = timedelta(minutes=1)
                left = (win_start, max(minute + win_start, mid - minute))
                right = (mid, win_end)
                log(
                    f"   ğŸ” Retry by splitting window: "
                    f"{left[0].strftime('%Y%m%d%H%M')}..{left[1].strftime('%Y%m%d%H%M')} | "
                    f"{right[0].strftime('%Y%m%d%H%M')}..{right[1].strftime('%Y%m%d%H%M')}",
                )
                cat_max_left = fetch_category_in_windows(
                    client=client,
                    category=category,
                    windows=[left],
                    seen_ids=seen_ids,
                    unique_papers=unique_papers,
                    split_on_error_depth=split_on_error_depth - 1,
                )
                cat_max_right = fetch_category_in_windows(
                    client=client,
                    category=category,
                    windows=[right],
                    seen_ids=seen_ids,
                    unique_papers=unique_papers,
                    split_on_error_depth=split_on_error_depth - 1,
                )
                for candidate in (cat_max_left, cat_max_right):
                    if candidate and (max_published_new is None or candidate > max_published_new):
                        max_published_new = candidate
            time.sleep(5)
        finally:
            group_end()

    return max_published_new


def fetch_all_domains_metadata_robust(
    days: int | None = None,
    output_file: str | None = None,
    ignore_seen: bool = False,
    chunk_days: int = 7,
) -> None:
    # 1. è®¡ç®—æ—¶é—´çª—å£ï¼ˆä¼˜å…ˆä½¿ç”¨ä¸Šæ¬¡æŠ“å–æ—¶é—´ï¼‰
    end_date = datetime.now(timezone.utc)
    if days is None:
        days = resolve_days_window(1)

    # ignore_seen è¯­ä¹‰ï¼šå®Œå…¨æŒ‰ days_window å›æº¯ï¼Œä¸ä½¿ç”¨ last_crawl_at / latest_published_at ä½œä¸ºèµ·ç‚¹
    if ignore_seen:
        log(
            "ğŸ§¹ [Global Ingest] ignore_seen=trueï¼šå°†å¿½ç•¥ arxiv_seenï¼ˆä¸è·³è¿‡å·²è§è®ºæ–‡ï¼Œä¸ä½¿ç”¨ latest_published_atï¼‰ï¼Œ"
            "å¹¶å¿½ç•¥ crawl_stateï¼ˆä¸ä½¿ç”¨ last_crawl_atï¼‰ï¼Œæ”¹ä¸ºä¸¥æ ¼æŒ‰ days_window å›æº¯ã€‚",
        )
        seen_ids, latest_published_at = set(), None
        start_date = end_date - timedelta(days=days)
        source_desc = f"days_window={days} (ignore_seen)"
    else:
        seen_ids, latest_published_at = load_seen_state()
        if latest_published_at:
            start_date = latest_published_at
            source_desc = "latest_published_at"
        else:
            last_crawl_at = load_last_crawl_at()
        if last_crawl_at:
            start_date = last_crawl_at
            source_desc = "last_crawl_at"
        else:
            start_date = end_date - timedelta(days=days)
            source_desc = f"days_window={days}"

    # å…œåº•ï¼šæ— è®ºæ¥æºå¦‚ä½•ï¼Œéƒ½ä¸æ—©äº (now - days_window)
    start_date = max(start_date, end_date - timedelta(days=days))

    if start_date >= end_date:
        start_date = end_date - timedelta(minutes=1)

    # æŒ‰å‘¨æ‹†åˆ†çª—å£ï¼Œé¿å…å•æ¬¡æŸ¥è¯¢è¿‡å¤§ï¼ˆå°¤å…¶ cs* è¿™ç§å¤§ç±»ï¼‰
    windows = iter_time_windows(start_date, end_date, chunk_days=chunk_days)
    start_str = start_date.strftime("%Y%m%d%H%M")
    end_str = end_date.strftime("%Y%m%d%H%M")
    
    group_start("Step 1 - fetch arXiv")
    log(f"ğŸŒ [Global Ingest] Window: {start_str} TO {end_str} ({source_desc})")
    if len(windows) > 1:
        log(f"ğŸ—“ï¸  [Global Ingest] å°†æŒ‰ {chunk_days} å¤©/ç‰‡æ‹†åˆ†çª—å£ï¼š{len(windows)} æ®µ")
    
    # ç»“æœé›†ä½¿ç”¨å­—å…¸å»é‡ (å› ä¸ºæœ‰äº›è®ºæ–‡è·¨é¢†åŸŸï¼Œæ¯”å¦‚åŒæ—¶åœ¨ cs å’Œ stat)
    unique_papers = {}
    max_published_new: datetime | None = None
    
    client = arxiv.Client(
        page_size=200,    # é™çº§ï¼šä» 1000 é™åˆ° 200ï¼Œé¿å…å•æ¬¡å“åº”è¿‡å¤§å¯¼è‡´ 500
        delay_seconds=3.0,
        num_retries=5
    )

    # 2. éå†åˆ†ç±»è¿›è¡ŒæŠ“å–
    for category in CATEGORIES_TO_FETCH:
        cat_max = fetch_category_in_windows(
            client=client,
            category=category,
            windows=windows,
            seen_ids=seen_ids,
            unique_papers=unique_papers,
        )
        if cat_max and (max_published_new is None or cat_max > max_published_new):
            max_published_new = cat_max

    # 3. ä¿å­˜æ±‡æ€»ç»“æœ
    total_count = len(unique_papers)
    log(f"âœ… All Done. Total unique papers fetched: {total_count}")
    
    if total_count > 0:
        # è‹¥æœªæ˜¾å¼æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œåˆ™æŒ‰æ—¥æœŸå‘½ååˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ archive/YYYYMMDD/raw ç›®å½•ï¼š
        # <ROOT_DIR>/archive/YYYYMMDD/raw/arxiv_papers_YYYYMMDD.json
        if not output_file:
            today_str = end_date.strftime("%Y%m%d")
            archive_dir = os.path.join(ROOT_DIR, "archive", today_str)
            raw_dir = os.path.join(archive_dir, "raw")
            output_file = os.path.join(
                raw_dir,
                f"arxiv_papers_{today_str}.json",
            )

        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(list(unique_papers.values()), f, ensure_ascii=False, indent=2)
        log(f"ğŸ’¾ File saved to: {output_file}")
    else:
        log("âš ï¸ No papers found. Check your date range or network.")
    if max_published_new:
        save_seen_state(seen_ids, max_published_new)
    else:
        save_seen_state(seen_ids, latest_published_at)
    save_last_crawl_at(end_date)
    group_end()

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="æŠ“å– arXiv å¤šé¢†åŸŸè®ºæ–‡å…ƒæ•°æ®ï¼ˆæŒ‰æäº¤æ—¶é—´çª—å£ï¼‰ã€‚")
    parser.add_argument(
        "--days",
        type=int,
        default=None,
        help="æŠ“å–çª—å£å¤©æ•°ï¼ˆä¼˜å…ˆçº§é«˜äº config.yamlï¼‰ã€‚ä¸å¡«åˆ™ä½¿ç”¨ config.yaml çš„ days_windowã€‚",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤å†™å…¥ archive/YYYYMMDD/raw/arxiv_papers_YYYYMMDD.jsonï¼‰ã€‚",
    )
    parser.add_argument(
        "--ignore-seen",
        action="store_true",
        help="æœ¬æ¬¡è¿è¡Œå¿½ç•¥ archive/arxiv_seen.json ä¸ archive/crawl_state.jsonï¼šä¸¥æ ¼æŒ‰ days_window å›æº¯çª—å£ï¼Œä¸è·³è¿‡å·²è§è®ºæ–‡ã€‚",
    )
    parser.add_argument(
        "--chunk-days",
        type=int,
        default=7,
        help="å°†æ—¶é—´çª—å£æ‹†åˆ†ä¸ºè‹¥å¹²æ®µï¼ˆé»˜è®¤ 7=æŒ‰å‘¨ï¼‰ï¼Œä»¥å‡å°‘å•æ¬¡æŸ¥è¯¢è§„æ¨¡å¹¶é™ä½ HTTP 500 æ¦‚ç‡ã€‚",
    )
    args = parser.parse_args()

    # å»ºè®®å…ˆç”¨ --days 1 æµ‹è¯•ä¸€ä¸‹ï¼Œæ²¡é—®é¢˜å†è·‘æ›´é•¿æ—¶é—´çª—å£
    fetch_all_domains_metadata_robust(
        days=args.days,
        output_file=args.output,
        ignore_seen=bool(args.ignore_seen),
        chunk_days=int(args.chunk_days or 7),
    )
