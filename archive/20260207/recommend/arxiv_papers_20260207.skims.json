{
  "mode": "skims",
  "generated_at": "2026-02-07T21:12:57.127746+00:00",
  "stats": {
    "mode": "skims",
    "forced_all_quick": true,
    "min_score": 8.0,
    "deep_divecandidates": 8,
    "deep_cap": null,
    "deep_selected": 0,
    "quick_candidates": 8,
    "quick_skim_target": null,
    "quick_selected": 8
  },
  "deep_dive": [],
  "quick_skim": [
    {
      "id": "2602.01510v1",
      "title": "Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization",
      "abstract": "Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.",
      "authors": [
        "Hengzhe Zhang",
        "Qi Chen",
        "Bing Xue",
        "Wolfgang Banzhaf",
        "Mengjie Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.NE"
      ],
      "published": "2026-02-02 00:46:16+00:00",
      "link": "https://arxiv.org/pdf/2602.01510v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Genetic programming-based feature construction for symbolic regression and generalization",
      "llm_evidence_cn": "基于遗传规划的符号回归特征构建与泛化研究",
      "llm_evidence": "基于遗传规划的符号回归特征构建与泛化研究",
      "llm_tldr_en": "Proposes an evolutionary framework to improve symbolic regression generalization by minimizing the Jensen gap.",
      "llm_tldr_cn": "提出一种通过最小化Jensen间隙来提高符号回归泛化能力的进化框架。",
      "llm_tldr": "提出一种通过最小化Jensen间隙来提高符号回归泛化能力的进化框架。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.02311v1",
      "title": "Introns and Templates Matter: Rethinking Linkage in GP-GOMEA",
      "abstract": "GP-GOMEA is among the state-of-the-art for symbolic regression, especially when it comes to finding small and potentially interpretable solutions. A key mechanism employed in any GOMEA variant is the exploitation of linkage, the dependencies between variables, to ensure efficient evolution. In GP-GOMEA, mutual information between node positions in GP trees has so far been used to learn linkage. For this, a fixed expression template is used. This however leads to introns for expressions smaller than the full template. As introns have no impact on fitness, their occurrences are not directly linked to selection. Consequently, introns can adversely affect the extent to which mutual information captures dependencies between tree nodes. To overcome this, we propose two new measures for linkage learning, one that explicitly considers introns in mutual information estimates, and one that revisits linkage learning in GP-GOMEA from a grey-box perspective, yielding a measure that needs not to be learned from the population but is derived directly from the template. Across five standard symbolic regression problems, GP-GOMEA achieves substantial improvements using both measures. We also find that the newly learned linkage structure closely reflects the template linkage structure, and that explicitly using the template structure yields the best performance overall.",
      "authors": [
        "Johannes Koch",
        "Tanja Alderliesten",
        "Peter A. N. Bosman"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-02 16:42:30+00:00",
      "link": "https://arxiv.org/pdf/2602.02311v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "State-of-the-art symbolic regression using GP-GOMEA",
      "llm_evidence_cn": "使用GP-GOMEA的最先进符号回归方法",
      "llm_evidence": "使用GP-GOMEA的最先进符号回归方法",
      "llm_tldr_en": "Improves GP-GOMEA for symbolic regression by rethinking linkage and handling introns in genetic programming.",
      "llm_tldr_cn": "通过重新思考遗传规划中的联动和内含子处理，改进了符号回归算法GP-GOMEA。",
      "llm_tldr": "通过重新思考遗传规划中的联动和内含子处理，改进了符号回归算法GP-GOMEA。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.03506v1",
      "title": "Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models",
      "abstract": "Following their success across many domains, transformers have also proven effective for symbolic regression (SR); however, the internal mechanisms underlying their generation of mathematical operators remain largely unexplored. Although mechanistic interpretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. In this article, we introduce PATCHES, an evolutionary circuit discovery algorithm that identifies compact and correct circuits for SR. Using PATCHES, we isolate 28 circuits, providing the first circuit-level characterisation of an SR transformer. We validate these findings through a robust causal evaluation framework based on key notions such as faithfulness, completeness, and minimality. Our analysis shows that mean patching with performance-based evaluation most reliably isolates functionally correct circuits. In contrast, we demonstrate that direct logit attribution and probing classifiers primarily capture correlational features rather than causal ones, limiting their utility for circuit discovery. Overall, these results establish SR as a high-potential application domain for mechanistic interpretability and propose a principled methodology for circuit discovery.",
      "authors": [
        "Arco van Breda",
        "Erman Acar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-03 13:27:10+00:00",
      "link": "https://arxiv.org/pdf/2602.03506v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Mechanistic interpretability of Transformer-based symbolic regression models",
      "llm_evidence_cn": "Transformer符号回归模型的机械可解释性研究",
      "llm_evidence": "Transformer符号回归模型的机械可解释性研究",
      "llm_tldr_en": "Introduces PATCHES to identify circuits in SR transformers, providing the first circuit-level characterization.",
      "llm_tldr_cn": "引入PATCHES算法识别符号回归Transformer中的电路，首次实现了电路级特征分析。",
      "llm_tldr": "引入PATCHES算法识别符号回归Transformer中的电路，首次实现了电路级特征分析。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.03816v1",
      "title": "SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving",
      "abstract": "We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions. SymPlex formulates symbolic PDE solving as tree-structured decision-making and optimizes candidate solutions using only the PDE and its boundary conditions. At its core is SymFormer, a structure-aware Transformer that models hierarchical symbolic dependencies via tree-relative self-attention and enforces syntactic validity through grammar-constrained autoregressive decoding, overcoming the limited expressivity of sequence-based generators. Unlike numerical and neural approaches that approximate solutions in discretized or implicit function spaces, SymPlex operates directly in symbolic expression space, enabling interpretable and human-readable solutions that naturally represent non-smooth behavior and explicit parametric dependence. Empirical results demonstrate exact recovery of non-smooth and parametric PDE solutions using deep learning-based symbolic methods.",
      "authors": [
        "Yesom Park",
        "Annie C. Lu",
        "Shao-Ching Huang",
        "Qiyang Hu",
        "Y. Sungtaek Ju",
        "Stanley Osher"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-03 18:18:30+00:00",
      "link": "https://arxiv.org/pdf/2602.03816v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Uses Transformer and RL for symbolic PDE solving without ground truth",
      "llm_evidence_cn": "使用Transformer和强化学习在无真值情况下求解符号偏微分方程",
      "llm_evidence": "使用Transformer和强化学习在无真值情况下求解符号偏微分方程",
      "llm_tldr_en": "SymPlex uses a structure-aware Transformer and RL to discover analytical symbolic solutions for PDEs.",
      "llm_tldr_cn": "SymPlex利用结构感知Transformer和强化学习发现偏微分方程的解析符号解。",
      "llm_tldr": "SymPlex利用结构感知Transformer和强化学习发现偏微分方程的解析符号解。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04492v1",
      "title": "Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish",
      "abstract": "Constructing mechanistic models of neural circuits is a fundamental goal of neuroscience, yet verifying such models is limited by the lack of ground truth. To rigorously test model discovery, we establish an in silico testbed using neuromechanical simulations of a larval zebrafish as a transparent ground truth. We find that LLM-based tree search autonomously discovers predictive models that significantly outperform established forecasting baselines. Conditioning on sensory drive is necessary but not sufficient for faithful system identification, as models exploit statistical shortcuts. Structural priors prove essential for enabling robust out-of-distribution generalization and recovery of interpretable mechanistic models. Our insights provide guidance for modeling real-world neural recordings and offer a broader template for AI-driven scientific discovery.",
      "authors": [
        "Jan-Matthis Lueckmann",
        "Viren Jain",
        "Michał Januszewski"
      ],
      "primary_category": "q-bio.NC",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-04 12:33:29+00:00",
      "link": "https://arxiv.org/pdf/2602.04492v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM-based tree search for mechanistic model discovery",
      "llm_evidence_cn": "基于大模型的树搜索用于机制模型发现",
      "llm_evidence": "基于大模型的树搜索用于机制模型发现",
      "llm_tldr_en": "Uses LLM-based tree search to discover interpretable mechanistic models of neural activity in zebrafish.",
      "llm_tldr_cn": "利用基于大语言模型的树搜索，在斑马鱼神经活动中发现可解释的机制模型。",
      "llm_tldr": "利用基于大语言模型的树搜索，在斑马鱼神经活动中发现可解释的机制模型。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.01493v1",
      "title": "OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference",
      "abstract": "Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execution success rate and numerical accuracy arises, particularly when generalization to unseen parameters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM parametric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including unseen parameters and configurations, and provides seamless integration with LLMs for natural language specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator inference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving.",
      "authors": [
        "Zhuoyuan Wang",
        "Hanjiang Hu",
        "Xiyu Deng",
        "Saviz Mowlavi",
        "Yorie Nakahira"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-02 00:04:50+00:00",
      "link": "https://arxiv.org/pdf/2602.01493v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "symbolic reasoning for PDE solving and scientific discovery",
      "llm_evidence_cn": "用于偏微分方程求解和科学发现的符号推理",
      "llm_evidence": "用于偏微分方程求解和科学发现的符号推理",
      "llm_tldr_en": "Combines LLMs with operator inference to solve parametric PDEs, bridging symbolic reasoning and physics.",
      "llm_tldr_cn": "结合大模型与算子推理求解参数化偏微分方程，连接符号推理与物理学。",
      "llm_tldr": "结合大模型与算子推理求解参数化偏微分方程，连接符号推理与物理学。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.02886v1",
      "title": "Mixture of Concept Bottleneck Experts",
      "abstract": "Concept Bottleneck Models (CBMs) promote interpretability by grounding predictions in human-understandable concepts. However, existing CBMs typically fix their task predictor to a single linear or Boolean expression, limiting both predictive accuracy and adaptability to diverse user needs. We propose Mixture of Concept Bottleneck Experts (M-CBEs), a framework that generalizes existing CBMs along two dimensions: the number of experts and the functional form of each expert, exposing an underexplored region of the design space. We investigate this region by instantiating two novel models: Linear M-CBE, which learns a finite set of linear expressions, and Symbolic M-CBE, which leverages symbolic regression to discover expert functions from data under user-specified operator vocabularies. Empirical evaluation demonstrates that varying the mixture size and functional form provides a robust framework for navigating the accuracy-interpretability trade-off, adapting to different user and task needs.",
      "authors": [
        "Francesco De Santis",
        "Gabriele Ciravegna",
        "Giovanni De Felice",
        "Arianna Casanova",
        "Francesco Giannini",
        "Michelangelo Diligenti",
        "Mateo Espinosa Zarlenga",
        "Pietro Barbiero",
        "Johannes Schneider",
        "Danilo Giordano"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-02 22:44:42+00:00",
      "link": "https://arxiv.org/pdf/2602.02886v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Symbolic regression used to discover expert functions in concept bottleneck models",
      "llm_evidence_cn": "在概念瓶颈模型中使用符号回归发现专家函数",
      "llm_evidence": "在概念瓶颈模型中使用符号回归发现专家函数",
      "llm_tldr_en": "Proposes M-CBEs using symbolic regression to discover interpretable expert functions from data.",
      "llm_tldr_cn": "提出M-CBEs框架，利用符号回归从数据中发现可解释的专家函数。",
      "llm_tldr": "提出M-CBEs框架，利用符号回归从数据中发现可解释的专家函数。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.04529v1",
      "title": "Landscape-aware Automated Algorithm Design: An Efficient Framework for Real-world Optimization",
      "abstract": "The advent of Large Language Models (LLMs) has opened new frontiers in automated algorithm design, giving rise to numerous powerful methods. However, these approaches retain critical limitations: they require extensive evaluation of the target problem to guide the search process, making them impractical for real-world optimization tasks, where each evaluation consumes substantial computational resources. This research proposes an innovative and efficient framework that decouples algorithm discovery from high-cost evaluation. Our core innovation lies in combining a Genetic Programming (GP) function generator with an LLM-driven evolutionary algorithm designer. The evolutionary direction of the GP-based function generator is guided by the similarity between the landscape characteristics of generated proxy functions and those of real-world problems, ensuring that algorithms discovered via proxy functions exhibit comparable performance on real-world problems. Our method enables deep exploration of the algorithmic space before final validation while avoiding costly real-world evaluations. We validated the framework's efficacy across multiple real-world problems, demonstrating its ability to discover high-performance algorithms while substantially reducing expensive evaluations. This approach shows a path to apply LLM-based automated algorithm design to computationally intensive real-world optimization challenges.",
      "authors": [
        "Haoran Yin",
        "Shuaiqun Pan",
        "Zhao Wei",
        "Jian Cheng Wong",
        "Yew-Soon Ong",
        "Anna V. Kononova",
        "Thomas Bäck",
        "Niki van Stein"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-04 13:18:45+00:00",
      "link": "https://arxiv.org/pdf/2602.04529v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Genetic Programming and LLM for algorithm discovery",
      "llm_evidence_cn": "遗传规划与大模型结合用于算法发现",
      "llm_evidence": "遗传规划与大模型结合用于算法发现",
      "llm_tldr_en": "Combines Genetic Programming with LLMs for automated algorithm design in real-world optimization.",
      "llm_tldr_cn": "结合遗传规划与大语言模型进行自动化算法设计，与方程发现方法论高度相关。",
      "llm_tldr": "结合遗传规划与大语言模型进行自动化算法设计，与方程发现方法论高度相关。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    }
  ]
}