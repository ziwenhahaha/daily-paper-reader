{
  "mode": "standard",
  "generated_at": "2026-02-06T04:30:27.086571+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 1,
    "deep_divecandidates": 0,
    "deep_cap": 6,
    "deep_selected": 0,
    "quick_candidates": 2,
    "quick_skim_target": 11,
    "quick_selected": 2
  },
  "deep_dive": [],
  "quick_skim": [
    {
      "id": "2602.05688v1",
      "title": "Mining Generalizable Activation Functions",
      "abstract": "The choice of activation function is an active area of research, with different proposals aimed at improving optimization, while maintaining expressivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behavior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaEvolve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python functions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their ability to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this framework, one can target not only performance improvements but also activation functions that encode particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale synthetic datasets can be sufficient for AlphaEvolve to discover meaningful activations.",
      "authors": [
        "Alex Vitvitskyi",
        "Michael Boratko",
        "Matej Grcic",
        "Razvan Pascanu",
        "Deep Shah",
        "Petar Veličković"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05 14:13:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05688v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "evolutionary search for mathematical functions",
      "llm_evidence_cn": "数学函数的进化搜索",
      "llm_evidence": "数学函数的进化搜索",
      "llm_tldr_en": "Uses evolutionary search and LLMs to discover novel, generalizable activation functions.",
      "llm_tldr_cn": "利用进化搜索和LLM发现新型、可泛化的激活函数。",
      "llm_tldr": "利用进化搜索和LLM发现新型、可泛化的激活函数。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.05977v1",
      "title": "Clifford Kolmogorov-Arnold Networks",
      "abstract": "We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.",
      "authors": [
        "Matthias Wolff",
        "Francesco Alesiani",
        "Christof Duhme",
        "Xiaoyi Jiang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-05 18:25:40+00:00",
      "link": "https://arxiv.org/pdf/2602.05977v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "function approximation for scientific discovery",
      "llm_evidence_cn": "用于科学发现的函数逼近",
      "llm_evidence": "用于科学发现的函数逼近",
      "llm_tldr_en": "Introduces Clifford Kolmogorov-Arnold Networks for function approximation in scientific and physics-inspired tasks.",
      "llm_tldr_cn": "引入克利福德柯尔莫哥洛夫-阿诺德网络，用于科学和物理启发任务中的函数逼近。",
      "llm_tldr": "引入克利福德柯尔莫哥洛夫-阿诺德网络，用于科学和物理启发任务中的函数逼近。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    }
  ]
}