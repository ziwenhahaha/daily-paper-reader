{
  "mode": "standard",
  "generated_at": "2026-02-04T04:24:48.044933+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 1,
    "deep_divecandidates": 3,
    "deep_cap": 6,
    "deep_selected": 3,
    "quick_candidates": 4,
    "quick_skim_target": 11,
    "quick_selected": 4
  },
  "deep_dive": [
    {
      "id": "2602.02886v1",
      "title": "Mixture of Concept Bottleneck Experts",
      "abstract": "Concept Bottleneck Models (CBMs) promote interpretability by grounding predictions in human-understandable concepts. However, existing CBMs typically fix their task predictor to a single linear or Boolean expression, limiting both predictive accuracy and adaptability to diverse user needs. We propose Mixture of Concept Bottleneck Experts (M-CBEs), a framework that generalizes existing CBMs along two dimensions: the number of experts and the functional form of each expert, exposing an underexplored region of the design space. We investigate this region by instantiating two novel models: Linear M-CBE, which learns a finite set of linear expressions, and Symbolic M-CBE, which leverages symbolic regression to discover expert functions from data under user-specified operator vocabularies. Empirical evaluation demonstrates that varying the mixture size and functional form provides a robust framework for navigating the accuracy-interpretability trade-off, adapting to different user and task needs.",
      "authors": [
        "Francesco De Santis",
        "Gabriele Ciravegna",
        "Giovanni De Felice",
        "Arianna Casanova",
        "Francesco Giannini",
        "Michelangelo Diligenti",
        "Mateo Espinosa Zarlenga",
        "Pietro Barbiero",
        "Johannes Schneider",
        "Danilo Giordano"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-02 22:44:42+00:00",
      "link": "https://arxiv.org/pdf/2602.02886v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "leverages symbolic regression to discover expert functions",
      "llm_evidence_cn": "利用符号回归发现专家函数",
      "llm_evidence": "利用符号回归发现专家函数",
      "llm_tldr_en": "Introduces M-CBEs which use symbolic regression to discover interpretable expert functions from data.",
      "llm_tldr_cn": "引入了M-CBEs框架，利用符号回归从数据中发现可解释的专家函数。",
      "llm_tldr": "引入了M-CBEs框架，利用符号回归从数据中发现可解释的专家函数。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.03506v1",
      "title": "Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models",
      "abstract": "Following their success across many domains, transformers have also proven effective for symbolic regression (SR); however, the internal mechanisms underlying their generation of mathematical operators remain largely unexplored. Although mechanistic interpretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. In this article, we introduce PATCHES, an evolutionary circuit discovery algorithm that identifies compact and correct circuits for SR. Using PATCHES, we isolate 28 circuits, providing the first circuit-level characterisation of an SR transformer. We validate these findings through a robust causal evaluation framework based on key notions such as faithfulness, completeness, and minimality. Our analysis shows that mean patching with performance-based evaluation most reliably isolates functionally correct circuits. In contrast, we demonstrate that direct logit attribution and probing classifiers primarily capture correlational features rather than causal ones, limiting their utility for circuit discovery. Overall, these results establish SR as a high-potential application domain for mechanistic interpretability and propose a principled methodology for circuit discovery.",
      "authors": [
        "Arco van Breda",
        "Erman Acar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-03 13:27:10+00:00",
      "link": "https://arxiv.org/pdf/2602.03506v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "directly investigates transformer-based symbolic regression and circuit discovery",
      "llm_evidence_cn": "直接研究基于 Transformer 的符号回归和电路发现",
      "llm_evidence": "直接研究基于 Transformer 的符号回归和电路发现",
      "llm_tldr_en": "Introduces PATCHES to interpret the internal mechanisms of transformer-based symbolic regression models.",
      "llm_tldr_cn": "引入 PATCHES 算法来解释基于 Transformer 的符号回归模型的内部机制。",
      "llm_tldr": "引入 PATCHES 算法来解释基于 Transformer 的符号回归模型的内部机制。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.03816v1",
      "title": "SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving",
      "abstract": "We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions. SymPlex formulates symbolic PDE solving as tree-structured decision-making and optimizes candidate solutions using only the PDE and its boundary conditions. At its core is SymFormer, a structure-aware Transformer that models hierarchical symbolic dependencies via tree-relative self-attention and enforces syntactic validity through grammar-constrained autoregressive decoding, overcoming the limited expressivity of sequence-based generators. Unlike numerical and neural approaches that approximate solutions in discretized or implicit function spaces, SymPlex operates directly in symbolic expression space, enabling interpretable and human-readable solutions that naturally represent non-smooth behavior and explicit parametric dependence. Empirical results demonstrate exact recovery of non-smooth and parametric PDE solutions using deep learning-based symbolic methods.",
      "authors": [
        "Yesom Park",
        "Annie C. Lu",
        "Shao-Ching Huang",
        "Qiyang Hu",
        "Y. Sungtaek Ju",
        "Stanley Osher"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-03 18:18:30+00:00",
      "link": "https://arxiv.org/pdf/2602.03816v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Directly addresses symbolic solving of PDEs using a structure-aware Transformer and reinforcement learning.",
      "llm_evidence_cn": "直接探讨了使用结构感知Transformer和强化学习进行偏微分方程的符号求解。",
      "llm_evidence": "直接探讨了使用结构感知Transformer和强化学习进行偏微分方程的符号求解。",
      "llm_tldr_en": "SymPlex uses RL and a grammar-constrained Transformer to find analytical symbolic solutions for PDEs.",
      "llm_tldr_cn": "SymPlex利用强化学习和语法约束Transformer寻找偏微分方程的解析符号解。",
      "llm_tldr": "SymPlex利用强化学习和语法约束Transformer寻找偏微分方程的解析符号解。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2602.02724v1",
      "title": "Automatic Design of Optimization Test Problems with Large Language Models",
      "abstract": "The development of black-box optimization algorithms depends on the availability of benchmark suites that are both diverse and representative of real-world problem landscapes. Widely used collections such as BBOB and CEC remain dominated by hand-crafted synthetic functions and provide limited coverage of the high-dimensional space of Exploratory Landscape Analysis (ELA) features, which in turn biases evaluation and hinders training of meta-black-box optimizers. We introduce Evolution of Test Functions (EoTF), a framework that automatically generates continuous optimization test functions whose landscapes match a specified target ELA feature vector. EoTF adapts LLM-driven evolutionary search, originally proposed for heuristic discovery, to evolve interpretable, self-contained numpy implementations of objective functions by minimizing the distance between sampled ELA features of generated candidates and a target profile. In experiments on 24 noiseless BBOB functions and a contamination-mitigating suite of 24 MA-BBOB hybrid functions, EoTF reliably produces non-trivial functions with closely matching ELA characteristics and preserves optimizer performance rankings under fixed evaluation budgets, supporting their validity as surrogate benchmarks. While a baseline neural-network-based generator achieves higher accuracy in 2D, EoTF substantially outperforms it in 3D and exhibits stable solution quality as dimensionality increases, highlighting favorable scalability. Overall, EoTF offers a practical route to scalable, portable, and interpretable benchmark generation targeted to desired landscape properties.",
      "authors": [
        "Wojciech Achtelik",
        "Hubert Guzowski",
        "Maciej Smołka",
        "Jacek Mańdziuk"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-02 19:42:14+00:00",
      "link": "https://arxiv.org/pdf/2602.02724v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "automatic design of optimization test functions using LLMs",
      "llm_evidence_cn": "利用大语言模型自动设计优化测试函数",
      "llm_evidence": "利用大语言模型自动设计优化测试函数",
      "llm_tldr_en": "Evolves interpretable test functions to match specific landscape features, relevant to benchmark generation.",
      "llm_tldr_cn": "演化出符合特定特征的可解释测试函数，与基准测试生成相关。",
      "llm_tldr": "演化出符合特定特征的可解释测试函数，与基准测试生成相关。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.02727v1",
      "title": "Search-Augmented Masked Diffusion Models for Constrained Generation",
      "abstract": "Discrete diffusion models generate sequences by iteratively denoising samples corrupted by categorical noise, offering an appealing alternative to autoregressive decoding for structured and symbolic generation. However, standard training targets a likelihood-based objective that primarily matches the data distribution and provides no native mechanism for enforcing hard constraints or optimizing non-differentiable properties at inference time. This work addresses this limitation and introduces Search-Augmented Masked Diffusion (SearchDiff), a training-free neurosymbolic inference framework that integrates informed search directly into the reverse denoising process. At each denoising step, the model predictions define a proposal set that is optimized under a user-specified property satisfaction, yielding a modified reverse transition that steers sampling toward probable and feasible solutions. Experiments in biological design and symbolic reasoning illustrate that SearchDiff substantially improves constraint satisfaction and property adherence, while consistently outperforming discrete diffusion and autoregressive baselines.",
      "authors": [
        "Huu Binh Ta",
        "Michael Cardei",
        "Alvaro Velasquez",
        "Ferdinando Fioretto"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-02 19:43:25+00:00",
      "link": "https://arxiv.org/pdf/2602.02727v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Neurosymbolic framework for constrained generation, relevant to the symbolic nature of the query.",
      "llm_evidence_cn": "用于约束生成的神经符号框架，与查询的符号化性质相关。",
      "llm_evidence": "用于约束生成的神经符号框架，与查询的符号化性质相关。",
      "llm_tldr_en": "SearchDiff integrates informed search into diffusion models for structured and symbolic generation.",
      "llm_tldr_cn": "SearchDiff将启发式搜索集成到扩散模型中，用于结构化和符号化生成。",
      "llm_tldr": "SearchDiff将启发式搜索集成到扩散模型中，用于结构化和符号化生成。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.02919v1",
      "title": "DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution",
      "abstract": "LLM-driven evolutionary systems have shown promise for automated science discovery, yet existing approaches such as AlphaEvolve rely on full-code histories that are context-inefficient and potentially provide weak evolutionary guidance. In this work, we first formalize the evolutionary agents as a general Expectation-Maximization framework, where the language model samples candidate programs (E-step) and the system updates the control context based on evaluation feedback (M-step). Under this view, constructing context via full-code snapshots constitutes a suboptimal M-step, as redundant implement details dilutes core algorithmic ideas, making it difficult to provide clear inspirations for evolution. To address this, we propose DeltaEvolve, a momentum-driven evolutionary framework that replaces full-code history with structured semantic delta capturing how and why modifications between successive nodes affect performance. As programs are often decomposable, semantic delta usually contains many effective components which are transferable and more informative to drive improvement. By organizing semantic delta through multi-level database and progressive disclosure mechanism, input tokens are further reduced. Empirical evaluations on tasks across diverse scientific domains show that our framework can discover better solution with less token consumption over full-code-based evolutionary agents.",
      "authors": [
        "Jiachen Jiang",
        "Tianyu Ding",
        "Zhihui Zhu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-02 23:47:54+00:00",
      "link": "https://arxiv.org/pdf/2602.02919v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "evolutionary systems for automated scientific discovery and program generation",
      "llm_evidence_cn": "用于自动科学发现和程序生成的进化系统",
      "llm_evidence": "用于自动科学发现和程序生成的进化系统",
      "llm_tldr_en": "Introduces DeltaEvolve, an EM-based evolutionary framework to accelerate scientific discovery via program synthesis.",
      "llm_tldr_cn": "提出 DeltaEvolve，一种基于 EM 框架的进化系统，通过程序合成加速科学发现。",
      "llm_tldr": "提出 DeltaEvolve，一种基于 EM 框架的进化系统，通过程序合成加速科学发现。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.03034v1",
      "title": "KANFIS A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning",
      "abstract": "Adaptive Neuro-Fuzzy Inference System (ANFIS) was designed to combine the learning capabilities of neural network with the reasoning transparency of fuzzy logic. However, conventional ANFIS architectures suffer from structural complexity, where the product-based inference mechanism causes an exponential explosion of rules in high-dimensional spaces. We herein propose the Kolmogorov-Arnold Neuro-Fuzzy Inference System (KANFIS), a compact neuro-symbolic architecture that unifies fuzzy reasoning with additive function decomposition. KANFIS employs an additive aggregation mechanism, under which both model parameters and rule complexity scale linearly with input dimensionality rather than exponentially. Furthermore, KANFIS is compatible with both Type-1 (T1) and Interval Type-2 (IT2) fuzzy logic systems, enabling explicit modeling of uncertainty and ambiguity in fuzzy representations. By using sparse masking mechanisms, KANFIS generates compact and structured rule sets, resulting in an intrinsically interpretable model with clear rule semantics and transparent inference processes. Empirical results demonstrate that KANFIS achieves competitive performance against representative neural and neuro-fuzzy baselines.",
      "authors": [
        "Binbin Yong",
        "Haoran Pei",
        "Jun Shen",
        "Haoran Li",
        "Qingguo Zhou",
        "Zhao Su"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-03 03:02:17+00:00",
      "link": "https://arxiv.org/pdf/2602.03034v1",
      "tags": [
        "keyword:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Neuro-symbolic architecture for interpretable learning and additive function decomposition",
      "llm_evidence_cn": "用于可解释学习和加性函数分解的神经符号架构",
      "llm_evidence": "用于可解释学习和加性函数分解的神经符号架构",
      "llm_tldr_en": "Proposes KANFIS, a neuro-symbolic framework that improves interpretability and scales linearly with dimensions.",
      "llm_tldr_cn": "提出KANFIS神经符号框架，通过加性函数分解提高模型可解释性并降低复杂度。",
      "llm_tldr": "提出KANFIS神经符号框架，通过加性函数分解提高模型可解释性并降低复杂度。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    }
  ]
}