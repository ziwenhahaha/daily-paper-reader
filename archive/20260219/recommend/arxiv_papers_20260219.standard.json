{
  "mode": "standard",
  "generated_at": "2026-02-19T18:12:09.971880+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 1,
    "deep_divecandidates": 7,
    "deep_cap": 6,
    "deep_selected": 6,
    "quick_candidates": 17,
    "quick_skim_target": 11,
    "quick_selected": 11
  },
  "deep_dive": [
    {
      "id": "2602.12259v1",
      "title": "Think like a Scientist: Physics-guided LLM Agent for Equation Discovery",
      "abstract": "Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.",
      "authors": [
        "Jianke Yang",
        "Ohm Venkatachalam",
        "Mohammad Kianezhad",
        "Sharvaree Vadgama",
        "Rose Yu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-12 18:49:27+00:00",
      "link": "https://arxiv.org/pdf/2602.12259v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "LLM agent for symbolic equation discovery and physical law extraction",
      "llm_evidence_cn": "用于符号方程发现和物理定律提取的LLM智能体",
      "llm_evidence": "用于符号方程发现和物理定律提取的LLM智能体",
      "llm_tldr_en": "Introduces KeplerAgent for scientific equation discovery using LLMs and physics-based reasoning.",
      "llm_tldr_cn": "引入KeplerAgent框架，利用大模型和物理推理进行科学方程发现。",
      "llm_tldr": "引入KeplerAgent框架，利用大模型和物理推理进行科学方程发现。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "matched_requirement_id": "req-3"
    },
    {
      "id": "2602.13021v2",
      "title": "Prior-Guided Symbolic Regression: Towards Scientific Consistency in Equation Discovery",
      "abstract": "Symbolic Regression (SR) aims to discover interpretable equations from observational data, with the potential to reveal underlying principles behind natural phenomena. However, existing approaches often fall into the Pseudo-Equation Trap: producing equations that fit observations well but remain inconsistent with fundamental scientific principles. A key reason is that these approaches are dominated by empirical risk minimization, lacking explicit constraints to ensure scientific consistency. To bridge this gap, we propose PG-SR, a prior-guided SR framework built upon a three-stage pipeline consisting of warm-up, evolution, and refinement. Throughout the pipeline, PG-SR introduces a prior constraint checker that explicitly encodes domain priors as executable constraint programs, and employs a Prior Annealing Constrained Evaluation (PACE) mechanism during the evolution stage to progressively steer discovery toward scientifically consistent regions. Theoretically, we prove that PG-SR reduces the Rademacher complexity of the hypothesis space, yielding tighter generalization bounds and establishing a guarantee against pseudo-equations. Experimentally, PG-SR outperforms state-of-the-art baselines across diverse domains, maintaining robustness to varying prior quality, noisy data, and data scarcity.",
      "authors": [
        "Jing Xiao",
        "Xinhai Chen",
        "Jiaming Peng",
        "Qinglin Wang",
        "Menghan Jia",
        "Zhiquan Lai",
        "Guangping Yu",
        "Dongsheng Li",
        "Tiejun Li",
        "Jie Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 15:26:21+00:00",
      "link": "https://arxiv.org/pdf/2602.13021v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Prior-guided symbolic regression for scientific consistency and equation discovery",
      "llm_evidence_cn": "先验引导的符号回归用于科学一致性与方程发现",
      "llm_evidence": "先验引导的符号回归用于科学一致性与方程发现",
      "llm_tldr_en": "Proposes PG-SR to ensure discovered equations align with fundamental scientific principles using prior constraints.",
      "llm_tldr_cn": "提出PG-SR框架，通过引入先验约束确保符号回归发现的方程符合基本科学原理。",
      "llm_tldr": "提出PG-SR框架，通过引入先验约束确保符号回归发现的方程符合基本科学原理。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "matched_requirement_id": "req-3"
    },
    {
      "id": "2602.12870v1",
      "title": "GAME: Genetic Algorithms with Marginalised Ensembles for model-independent reconstruction of cosmological quantities",
      "abstract": "Genetic Algorithms (GA) are a powerful tool for stochastic optimisation and non-parametric symbolic regression, already widely used in cosmology. They are capable of reconstructing analytical functions directly from data points without introducing new physical models. A limitation of this approach is that while the reconstructed function is very efficient at reproducing the behaviour of the data points, non-observable quantities involving derivatives are particularly sensitive to stochasticity, hyperparameters, and to the choice of the best-fit function obtained by the GA, which implies the risk of the algorithm getting stuck in a local minimum. In this work we propose an update to the GA methodology for the reconstruction of analytical functions that involves computing a weighted average of an ensemble of GA configurations (\\texttt{GAME}). We define the weights via a quantity that accounts for both the goodness-of-fit of the points and the smoothness of the resulting function. We also present a practical method to analytically estimate and correct the errors on the averaged function by combining a path-integral approach with an ensemble variance. We demonstrate the improvement offered by \\texttt{GAME} methodology on a generic test function. We then apply the new methodology to a non-parametric reconstruction of the Hubble rate $H(z)$ using Cosmic Chronometers data and, assuming a flat Friedmann-Lemaître-Robertson-Walker background and General Relativity, we infer the corresponding dark energy equation of state $w(z)$. Through consistency tests, we show that current data produces results compatible with $Λ$CDM, and that Stage IV cosmology surveys will allow GA reinforced with \\texttt{GAME} methodology to become an even more competitive tool for discriminating between different models.",
      "authors": [
        "Matteo Peronaci",
        "Matteo Martinelli",
        "Savvas Nesseris"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO"
      ],
      "published": "2026-02-13 12:20:46+00:00",
      "link": "https://arxiv.org/pdf/2602.12870v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Genetic Algorithms for non-parametric symbolic regression in cosmology",
      "llm_evidence_cn": "用于宇宙学非参数符号回归的遗传算法",
      "llm_evidence": "用于宇宙学非参数符号回归的遗传算法",
      "llm_tldr_en": "Proposes an updated Genetic Algorithm for model-independent reconstruction of cosmological quantities.",
      "llm_tldr_cn": "提出了一种改进的遗传算法，用于宇宙学量的模型无关重构。",
      "llm_tldr": "提出了一种改进的遗传算法，用于宇宙学量的模型无关重构。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Recent advances and state-of-the-art methods in symbolic regression",
      "matched_requirement_id": "req-2"
    },
    {
      "id": "2602.13513v2",
      "title": "Learning Gradient Flow: Using Equation Discovery to Accelerate Engineering Optimization",
      "abstract": "In this work, we investigate the use of data-driven equation discovery for dynamical systems to model and forecast continuous-time dynamics of unconstrained optimization problems. To avoid expensive evaluations of the objective function and its gradient, we leverage trajectory data on the optimization variables to learn the continuous-time dynamics associated with gradient descent, Newton's method, and ADAM optimization. The discovered gradient flows are then solved as a surrogate for the original optimization problem. To this end, we introduce the Learned Gradient Flow (LGF) optimizer, which is equipped to build surrogate models of variable polynomial order in full- or reduced-dimensional spaces at user-defined intervals in the optimization process. We demonstrate the efficacy of this approach on several standard problems from engineering mechanics and scientific machine learning, including two inverse problems, structural topology optimization, and two forward solves with different discretizations. Our results suggest that the learned gradient flows can significantly expedite convergence by capturing critical features of the optimization trajectory while avoiding expensive evaluations of the objective and its gradient.",
      "authors": [
        "Grant Norman",
        "Conor Rowan",
        "Kurt Maute",
        "Alireza Doostan"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.CE",
        "cs.LG",
        "math.DS",
        "math.NA"
      ],
      "published": "2026-02-13 22:44:33+00:00",
      "link": "https://arxiv.org/pdf/2602.13513v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Data-driven equation discovery for dynamical systems and optimization",
      "llm_evidence_cn": "动力系统和优化的数据驱动方程发现",
      "llm_evidence": "动力系统和优化的数据驱动方程发现",
      "llm_tldr_en": "Uses equation discovery to learn gradient flow dynamics as surrogates for accelerating engineering optimization.",
      "llm_tldr_cn": "利用方程发现技术学习梯度流动力学，作为加速工程优化的代理模型。",
      "llm_tldr": "利用方程发现技术学习梯度流动力学，作为加速工程优化的代理模型。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "matched_requirement_id": "req-3"
    },
    {
      "id": "2602.16166v1",
      "title": "Discovering Unknown Inverter Governing Equations via Physics-Informed Sparse Machine Learning",
      "abstract": "Discovering the unknown governing equations of grid-connected inverters from external measurements holds significant attraction for analyzing modern inverter-intensive power systems. However, existing methods struggle to balance the identification of unmodeled nonlinearities with the preservation of physical consistency. To address this, this paper proposes a Physics-Informed Sparse Machine Learning (PISML) framework. The architecture integrates a sparse symbolic backbone to capture dominant model skeletons with a neural residual branch that compensates for complex nonlinear control logic. Meanwhile, a Jacobian-regularized physics-informed training mechanism is introduced to enforce multi-scale consistency including large/small-scale behaviors. Furthermore, by performing symbolic regression on the neural residual branch, PISML achieves a tractable mapping from black-box data to explicit control equations. Experimental results on a high-fidelity Hardware-in-the-Loop platform demonstrate the framework's superior performance. It not only achieves high-resolution identification by reducing error by over 340 times compared to baselines but also realizes the compression of heavy neural networks into compact explicit forms. This restores analytical tractability for rigorous stability analysis and reduces computational complexity by orders of magnitude. It also provides a unified pathway to convert structurally inaccessible devices into explicit mathematical models, enabling stability analysis of power systems with unknown inverter governing equations.",
      "authors": [
        "Jialin Zheng",
        "Ruhaan Batta",
        "Zhong Liu",
        "Xiaonan Lu"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-02-18 03:46:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16166v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "discovering governing equations via sparse symbolic backbone",
      "llm_evidence_cn": "通过稀疏符号主干发现控制方程",
      "llm_evidence": "通过稀疏符号主干发现控制方程",
      "llm_tldr_en": "A physics-informed sparse machine learning framework to discover unknown governing equations in power systems.",
      "llm_tldr_cn": "一种物理感知的稀疏机器学习框架，用于发现电力系统中未知的控制方程。",
      "llm_tldr": "一种物理感知的稀疏机器学习框架，用于发现电力系统中未知的控制方程。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "matched_requirement_id": "req-3"
    },
    {
      "id": "2602.11630v1",
      "title": "Neuro-Symbolic Multitasking: A Unified Framework for Discovering Generalizable Solutions to PDE Families",
      "abstract": "Solving Partial Differential Equations (PDEs) is fundamental to numerous scientific and engineering disciplines. A common challenge arises from solving the PDE families, which are characterized by sharing an identical mathematical structure but varying in specific parameters. Traditional numerical methods, such as the finite element method, need to independently solve each instance within a PDE family, which incurs massive computational cost. On the other hand, while recent advancements in machine learning PDE solvers offer impressive computational speed and accuracy, their inherent ``black-box\" nature presents a considerable limitation. These methods primarily yield numerical approximations, thereby lacking the crucial interpretability provided by analytical expressions, which are essential for deeper scientific insight. To address these limitations, we propose a neuro-assisted multitasking symbolic PDE solver framework for PDE family solving, dubbed NMIPS. In particular, we employ multifactorial optimization to simultaneously discover the analytical solutions of PDEs. To enhance computational efficiency, we devise an affine transfer method by transferring learned mathematical structures among PDEs in a family, avoiding solving each PDE from scratch. Experimental results across multiple cases demonstrate promising improvements over existing baselines, achieving up to a $\\sim$35.7% increase in accuracy while providing interpretable analytical solutions.",
      "authors": [
        "Yipeng Huang",
        "Dejun Xu",
        "Zexin Lin",
        "Zhenzhong Wang",
        "Min Jiang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 06:25:44+00:00",
      "link": "https://arxiv.org/pdf/2602.11630v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Neuro-symbolic framework for discovering mathematical structures in PDE families",
      "llm_evidence_cn": "用于发现偏微分方程族数学结构的神经符号框架",
      "llm_evidence": "用于发现偏微分方程族数学结构的神经符号框架",
      "llm_tldr_en": "Proposes a neuro-symbolic framework to discover generalizable mathematical solutions for PDE families.",
      "llm_tldr_cn": "提出一种神经符号框架，用于发现偏微分方程族的通用数学解结构。",
      "llm_tldr": "提出一种神经符号框架，用于发现偏微分方程族的通用数学解结构。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "matched_requirement_id": "req-3"
    }
  ],
  "quick_skim": [
    {
      "id": "2602.16551v1",
      "title": "Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation",
      "abstract": "The preservation of cultural heritage is increasingly transitioning towards data-driven predictive maintenance and \"Digital Twin\" construction. However, the mechanical constitutive models required for high-fidelity simulations remain fragmented across decades of unstructured scientific literature, creating a \"Data Silo\" that hinders conservation engineering. To address this, we present an automated, two-stage agentic framework leveraging Large Language Models (LLMs) to extract mechanical constitutive equations, calibrated parameters, and metadata from PDF documents. The workflow employs a resource-efficient \"Gatekeeper\" agent for relevance filtering and a high-capability \"Analyst\" agent for fine-grained extraction, featuring a novel Context-Aware Symbolic Grounding mechanism to resolve mathematical ambiguities. Applied to a corpus of over 2,000 research papers, the system successfully isolated 113 core documents and constructed a structured database containing 185 constitutive model instances and over 450 calibrated parameters. The extraction precision reached 80.4\\%, establishing a highly efficient \"Human-in-the-loop\" workflow that reduces manual data curation time by approximately 90\\%. We demonstrate the system's utility through a web-based Knowledge Retrieval Platform, which enables rapid parameter discovery for computational modeling. This work transforms scattered literature into a queryable digital asset, laying the data foundation for the \"Digital Material Twin\" of built heritage.",
      "authors": [
        "Rui Hu",
        "Yue Wu",
        "Tianhao Su",
        "Yin Wang",
        "Shunbo Hu",
        "Jizhong Huang"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-18 15:53:15+00:00",
      "link": "https://arxiv.org/pdf/2602.16551v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Automated extraction of mechanical constitutive equations from scientific literature",
      "llm_evidence_cn": "从科学文献中自动提取机械本构方程",
      "llm_evidence": "从科学文献中自动提取机械本构方程",
      "llm_tldr_en": "Uses LLM agents to extract mechanical constitutive models and physical laws from unstructured scientific papers.",
      "llm_tldr_cn": "利用大模型智能体从非结构化科学文献中提取机械本构模型和物理定律。",
      "llm_tldr": "利用大模型智能体从非结构化科学文献中提取机械本构模型和物理定律。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "matched_requirement_id": "req-3",
      "quick_tier": "8plus"
    },
    {
      "id": "2602.09667v1",
      "title": "Differentiable Modeling for Low-Inertia Grids: Benchmarking PINNs, NODEs, and DP for Identification and Control of SMIB System",
      "abstract": "The transition toward low-inertia power systems demands modeling frameworks that provide not only accurate state predictions but also physically consistent sensitivities for control. While scientific machine learning offers powerful nonlinear modeling tools, the control-oriented implications of different differentiable paradigms remain insufficiently understood. This paper presents a comparative study of Physics-Informed Neural Networks (PINNs), Neural Ordinary Differential Equations (NODEs), and Differentiable Programming (DP) for modeling, identification, and control of power system dynamics. Using the Single Machine Infinite Bus (SMIB) system as a benchmark, we evaluate their performance in trajectory extrapolation, parameter estimation, and Linear Quadratic Regulator (LQR) synthesis.   Our results highlight a fundamental trade-off between data-driven flexibility and physical structure. NODE exhibits superior extrapolation by capturing the underlying vector field, whereas PINN shows limited generalization due to its reliance on a time-dependent solution map. In the inverse problem of parameter identification, while both DP and PINN successfully recover the unknown parameters, DP achieves significantly faster convergence by enforcing governing equations as hard constraints. Most importantly, for control synthesis, the DP framework yields closed-loop stability comparable to the theoretical optimum. Furthermore, we demonstrate that NODE serves as a viable data-driven surrogate when governing equations are unavailable.",
      "authors": [
        "Shinhoo Kang",
        "Sangwook Kim",
        "Sehyun Yun"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "published": "2026-02-10 11:22:59+00:00",
      "link": "https://arxiv.org/pdf/2602.09667v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Scientific discovery and physical law extraction using differentiable modeling and PINNs",
      "llm_evidence_cn": "利用微分建模和物理信息神经网络进行科学发现和物理定律提取",
      "llm_evidence": "利用微分建模和物理信息神经网络进行科学发现和物理定律提取",
      "llm_tldr_en": "Benchmarks differentiable modeling paradigms for identifying and controlling power system dynamics.",
      "llm_tldr_cn": "基准测试了用于电力系统动力学识别与控制的微分建模范式。",
      "llm_tldr": "基准测试了用于电力系统动力学识别与控制的微分建模范式。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "matched_requirement_id": "req-3",
      "quick_tier": "7"
    },
    {
      "id": "2602.08990v1",
      "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
      "abstract": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
      "authors": [
        "Shiyang Feng",
        "Runmin Ma",
        "Xiangchao Yan",
        "Yue Fan",
        "Yusong Hu",
        "Songtao Huang",
        "Shuaiyu Zhang",
        "Zongsheng Cao",
        "Tianshuo Peng",
        "Jiakang Yuan",
        "Zijie Guo",
        "Zhijie Zhong",
        "Shangheng Du",
        "Weida Wang",
        "Jinxin Shi",
        "Yuhao Zhou",
        "Xiaohan He",
        "Zhiyin Yu",
        "Fangchen Yu",
        "Qihao Zheng",
        "Jiamin Wu",
        "Mianxin Liu",
        "Chi Zhang",
        "Shaowei Hou",
        "Shuya Li",
        "Yankai Jiang",
        "Wenjie Lou",
        "Lilong Wang",
        "Zifu Wang",
        "Jiong Wang",
        "Wanghan Xu",
        "Yue Deng",
        "Dongrui Liu",
        "Yiheng Wang",
        "Wenlong Zhang",
        "Fenghua Ling",
        "Shufei Zhang",
        "Xiaosong Wang",
        "Shuangjia Zheng",
        "Xun Huang",
        "Siqi Sun",
        "Shuyue Hu",
        "Peng Ye",
        "Chunfeng Song",
        "Bin Wang",
        "Conghui He",
        "Yihao Liu",
        "Xin Li",
        "Qibin Hou",
        "Tao Chen",
        "Xiangyu Yue",
        "Bin Wang",
        "Liang He",
        "Dahua Lin",
        "Bowen Zhou",
        "Bo Zhang",
        "Lei Bai"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-09 18:36:06+00:00",
      "link": "https://arxiv.org/pdf/2602.08990v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "autonomous scientific discovery and computational modeling",
      "llm_evidence_cn": "自主科学发现与计算建模",
      "llm_evidence": "自主科学发现与计算建模",
      "llm_tldr_en": "A unified agentic framework for long-horizon autonomous scientific discovery and experimental coordination.",
      "llm_tldr_cn": "一种用于长周期自主科学发现和实验协调的统一智能体框架。",
      "llm_tldr": "一种用于长周期自主科学发现和实验协调的统一智能体框架。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "matched_requirement_id": "req-3",
      "quick_tier": "6"
    },
    {
      "id": "2602.11666v1",
      "title": "PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics",
      "abstract": "The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely semantic Retrieval Augmented Generation (RAG) often leads to \"context poisoning,\" where agents generate linguistically plausible but physically invalid configurations due to a fundamental Semantic-Physical Disconnect. To bridge this gap, this work introduces PhyNiKCE (Physical and Numerical Knowledgeable Context Engineering), a neurosymbolic agentic framework for trustworthy engineering. Unlike standard black-box agents, PhyNiKCE decouples neural planning from symbolic validation. It employs a Symbolic Knowledge Engine that treats simulation setup as a Constraint Satisfaction Problem, rigidly enforcing physical constraints via a Deterministic RAG Engine with specialized retrieval strategies for solvers, turbulence models, and boundary conditions. Validated through rigorous OpenFOAM experiments on practical, non-tutorial CFD tasks using Gemini-2.5-Pro/Flash, PhyNiKCE demonstrates a 96% relative improvement over state-of-the-art baselines. Furthermore, by replacing trial-and-error with knowledge-driven initialization, the framework reduced autonomous self-correction loops by 59% while simultaneously lowering LLM token consumption by 17%. These results demonstrate that decoupling neural generation from symbolic constraint enforcement significantly enhances robustness and efficiency. While validated on CFD, this architecture offers a scalable, auditable paradigm for Trustworthy Artificial Intelligence in broader industrial automation.",
      "authors": [
        "E Fan",
        "Lisong Shi",
        "Zhengtong Li",
        "Chih-yung Wen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-12 07:37:56+00:00",
      "link": "https://arxiv.org/pdf/2602.11666v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "neurosymbolic framework for computational fluid dynamics with empirical validation",
      "llm_evidence_cn": "具有实证验证的计算流体动力学神经符号框架",
      "llm_evidence": "具有实证验证的计算流体动力学神经符号框架",
      "llm_tldr_en": "A neurosymbolic agent for CFD that enforces physical laws, bridging AI and engineering.",
      "llm_tldr_cn": "一种用于CFD的神经符号智能体框架，强制执行物理定律，连接AI与工程。",
      "llm_tldr": "一种用于CFD的神经符号智能体框架，强制执行物理定律，连接AI与工程。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "帮我找符号回归与其他学科交叉并且有实证实验的最新论文",
      "matched_requirement_id": "req-1",
      "quick_tier": "7"
    },
    {
      "id": "2602.10282v1",
      "title": "Linear-LLM-SCM: Benchmarking LLMs for Coefficient Elicitation in Linear-Gaussian Causal Models",
      "abstract": "Large language models (LLMs) have shown potential in identifying qualitative causal relations, but their ability to perform quantitative causal reasoning -- estimating effect sizes that parametrize functional relationships -- remains underexplored in continuous domains. We introduce Linear-LLM-SCM, a plug-and-play benchmarking framework for evaluating LLMs on linear Gaussian structural causal model (SCM) parametrization when the DAG is given. The framework decomposes a DAG into local parent-child sets and prompts an LLM to produce a regression-style structural equation per node, which is aggregated and compared against available ground-truth parameters. Our experiments show several challenges in such benchmarking tasks, namely, strong stochasticity in the results in some of the models and susceptibility to DAG misspecification via spurious edges in the continuous domains. Across models, we observe substantial variability in coefficient estimates for some settings and sensitivity to structural and semantic perturbations, highlighting current limitations of LLMs as quantitative causal parameterizers. We also open-sourced the benchmarking framework so that researchers can utilize their DAGs and any off-the-shelf LLMs plug-and-play for evaluation in their domains effortlessly.",
      "authors": [
        "Kanta Yamaoka",
        "Sumantrak Mukherjee",
        "Thomas Gärtner",
        "David Antony Selby",
        "Stefan Konigorski",
        "Eyke Hüllermeier",
        "Viktor Bengs",
        "Sebastian Josef Vollmer"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 20:49:01+00:00",
      "link": "https://arxiv.org/pdf/2602.10282v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Extracting functional relationships and structural equations",
      "llm_evidence_cn": "提取函数关系和结构方程",
      "llm_evidence": "提取函数关系和结构方程",
      "llm_tldr_en": "Benchmarks LLMs on estimating effect sizes and parametrizing functional relationships in causal models.",
      "llm_tldr_cn": "评估大模型在因果模型中估计效应大小和参数化函数关系的能力。",
      "llm_tldr": "评估大模型在因果模型中估计效应大小和参数化函数关系的能力。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "matched_requirement_id": "req-3",
      "quick_tier": "6"
    },
    {
      "id": "2602.11849v1",
      "title": "Data-driven discovery of chemical reaction networks",
      "abstract": "We propose a unified framework that allows for the full mechanistic reconstruction of chemical reaction networks (CRNs) from concentration data. The framework utilizes an integral formulation of the differential equations governing the chemical reactions, followed by an automatic procedure to recover admissible mass-action mechanisms from the equations. We provide theoretical justification for the use of integral formulations using analytical and numerical error bounds. The integral formulation is demonstrated to offer superior robustness to noise and improved accuracy in both rate-law and graph recovery when compared to other commonly used formulations. Together, our developments advance the goal of fully automated, data-driven chemical mechanism discovery.",
      "authors": [
        "Abraham Reyes-Velazquez",
        "Stefan Güttel",
        "Igor Larrosa",
        "Jonas Latz"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-02-12 11:41:42+00:00",
      "link": "https://arxiv.org/pdf/2602.11849v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Data-driven discovery of chemical reaction mechanisms and rate laws",
      "llm_evidence_cn": "数据驱动的化学反应机制和速率定律发现",
      "llm_evidence": "数据驱动的化学反应机制和速率定律发现",
      "llm_tldr_en": "A framework for reconstructing chemical reaction networks and rate laws from concentration data.",
      "llm_tldr_cn": "一种从浓度数据中重建化学反应网络和速率定律的自动化框架。",
      "llm_tldr": "一种从浓度数据中重建化学反应网络和速率定律的自动化框架。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "matched_requirement_id": "req-3",
      "quick_tier": "7"
    },
    {
      "id": "2602.10480v2",
      "title": "Neuro-Symbolic Synergy for Interactive World Modeling",
      "abstract": "Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical consistency but lack semantic expressivity. To bridge this gap, we propose Neuro-Symbolic Synergy (NeSyS), a framework that integrates the probabilistic semantic priors of LLMs with executable symbolic rules to achieve both expressivity and robustness. NeSyS alternates training between the two models using trajectories inadequately explained by the other. Unlike rule-based prompting, the symbolic WM directly constrains the LLM by modifying its output probability distribution. The neural WM is fine-tuned only on trajectories not covered by symbolic rules, reducing training data by 50% without loss of accuracy. Extensive experiments on three distinct interactive environments, i.e., ScienceWorld, Webshop, and Plancraft, demonstrate NeSyS's consistent advantages over baselines in both WM prediction accuracy and data efficiency.",
      "authors": [
        "Hongyu Zhao",
        "Siyu Zhou",
        "Haolin Yang",
        "Zengyi Qin",
        "Tianyi Zhou"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 03:36:18+00:00",
      "link": "https://arxiv.org/pdf/2602.10480v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Neuro-symbolic integration for world modeling and rule consistency",
      "llm_evidence_cn": "用于世界建模和规则一致性的神经符号集成",
      "llm_evidence": "用于世界建模和规则一致性的神经符号集成",
      "llm_tldr_en": "Integrates LLMs with executable symbolic rules to create robust and expressive neuro-symbolic world models.",
      "llm_tldr_cn": "将大语言模型与可执行符号规则相结合，构建鲁棒且具有表现力的神经符号世界模型。",
      "llm_tldr": "将大语言模型与可执行符号规则相结合，构建鲁棒且具有表现力的神经符号世界模型。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Comparison of genetic programming and neural symbolic regression techniques",
      "matched_requirement_id": "req-4",
      "quick_tier": "6"
    },
    {
      "id": "2602.12851v1",
      "title": "Chimera: Neuro-Symbolic Attention Primitives for Trustworthy Dataplane Intelligence",
      "abstract": "Deploying expressive learning models directly on programmable dataplanes promises line-rate, low-latency traffic analysis but remains hindered by strict hardware constraints and the need for predictable, auditable behavior. Chimera introduces a principled framework that maps attention-oriented neural computations and symbolic constraints onto dataplane primitives, enabling trustworthy inference within the match-action pipeline. Chimera combines a kernelized, linearized attention approximation with a two-layer key-selection hierarchy and a cascade fusion mechanism that enforces hard symbolic guarantees while preserving neural expressivity. The design includes a hardware-aware mapping protocol and a two-timescale update scheme that together permit stable, line-rate operation under realistic dataplane budgets. The paper presents the Chimera architecture, a hardware mapping strategy, and empirical evidence showing that neuro-symbolic attention primitives can achieve high-fidelity inference within the resource envelope of commodity programmable switches.",
      "authors": [
        "Rong Fu",
        "Wenxin Zhang",
        "Xiaowen Ma",
        "Kun Liu",
        "Wangyu Wu",
        "Ziyu Kong",
        "Jia Yee Tan",
        "Tailong Luo",
        "Xianda Li",
        "Zeli Su",
        "Youjin Wang",
        "Yongtai Liu",
        "Simon Fong"
      ],
      "primary_category": "cs.NI",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "published": "2026-02-13 11:55:06+00:00",
      "link": "https://arxiv.org/pdf/2602.12851v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Neuro-symbolic framework for hardware-constrained inference",
      "llm_evidence_cn": "硬件受限推理的神经符号框架",
      "llm_evidence": "硬件受限推理的神经符号框架",
      "llm_tldr_en": "Combines neural attention with symbolic constraints for trustworthy and auditable network traffic analysis.",
      "llm_tldr_cn": "将神经注意力与符号约束相结合，用于可靠且可审计的网络流量分析。",
      "llm_tldr": "将神经注意力与符号约束相结合，用于可靠且可审计的网络流量分析。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Comparison of genetic programming and neural symbolic regression techniques",
      "matched_requirement_id": "req-4",
      "quick_tier": "7"
    },
    {
      "id": "2602.11863v1",
      "title": "In-Context Function Learning in Large Language Models",
      "abstract": "Large language models (LLMs) can learn from a few demonstrations provided at inference time. We study this in-context learning phenomenon through the lens of Gaussian Processes (GPs). We build controlled experiments where models observe sequences of multivariate scalar-valued function samples drawn from known GP priors. We evaluate prediction error in relation to the number of demonstrations and compare against two principled references: (i) an empirical GP-regression learner that gives a lower bound on achievable error, and (ii) the expected error of a 1-nearest-neighbor (1-NN) rule, which gives a data-driven upper bound. Across model sizes, we find that LLM learning curves are strongly influenced by the function-generating kernels and approach the GP lower bound as the number of demonstrations increases. We then study the inductive biases of these models using a likelihood-based analysis. We find that LLM predictions are most likely under less smooth GP kernels. Finally, we explore whether post-training can shift these inductive biases and improve sample-efficiency on functions sampled from GPs with smoother kernels. We find that both reinforcement learning and supervised fine-tuning can effectively shift inductive biases in the direction of the training data. Together, our framework quantifies the extent to which LLMs behave like GP learners and provides tools for steering their inductive biases for continuous function learning tasks.",
      "authors": [
        "Elif Akata",
        "Konstantinos Voudouris",
        "Vincent Fortuin",
        "Eric Schulz"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 12:09:48+00:00",
      "link": "https://arxiv.org/pdf/2602.11863v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "In-context function learning and regression using LLMs",
      "llm_evidence_cn": "使用大语言模型进行上下文函数学习和回归",
      "llm_evidence": "使用大语言模型进行上下文函数学习和回归",
      "llm_tldr_en": "Analyzes how LLMs learn functions in-context, comparing them to Gaussian Process regression.",
      "llm_tldr_cn": "分析了大语言模型如何进行上下文函数学习，并将其与高斯过程回归进行了比较。",
      "llm_tldr": "分析了大语言模型如何进行上下文函数学习，并将其与高斯过程回归进行了比较。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Recent advances and state-of-the-art methods in symbolic regression",
      "matched_requirement_id": "req-2",
      "quick_tier": "6"
    },
    {
      "id": "2602.13583v1",
      "title": "Differentiable Rule Induction from Raw Sequence Inputs",
      "abstract": "Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging neural networks to improve robustness and scalability. However, most differentiable ILP methods rely on symbolic datasets, facing challenges when learning directly from raw data. Specifically, they struggle with explicit label leakage: The inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels. In this work, we address this issue by integrating a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage. The learned rules effectively describe raw data through its features. We demonstrate that our method intuitively and precisely learns generalized rules from time series and image data.",
      "authors": [
        "Kun Gao",
        "Katsumi Inoue",
        "Yongzhi Cao",
        "Hanpin Wang",
        "Feng Yang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-14 03:54:08+00:00",
      "link": "https://arxiv.org/pdf/2602.13583v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Differentiable rule induction and neural symbolic integration",
      "llm_evidence_cn": "可微规则归纳与神经符号集成",
      "llm_evidence": "可微规则归纳与神经符号集成",
      "llm_tldr_en": "Proposes a differentiable rule induction method to map continuous inputs to symbolic variables without supervision.",
      "llm_tldr_cn": "提出一种可微规则归纳方法，在无监督情况下将连续输入映射到符号变量。",
      "llm_tldr": "提出一种可微规则归纳方法，在无监督情况下将连续输入映射到符号变量。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Comparison of genetic programming and neural symbolic regression techniques",
      "matched_requirement_id": "req-4",
      "quick_tier": "7"
    },
    {
      "id": "2602.13769v1",
      "title": "OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery",
      "abstract": "Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.",
      "authors": [
        "Qi Liu",
        "Wanjing Ma"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.NE"
      ],
      "published": "2026-02-14 13:32:03+00:00",
      "link": "https://arxiv.org/pdf/2602.13769v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Automated scientific discovery and algorithm discovery via evolutionary search",
      "llm_evidence_cn": "通过进化搜索实现自动化科学发现和算法发现",
      "llm_evidence": "通过进化搜索实现自动化科学发现和算法发现",
      "llm_tldr_en": "A multi-agent framework for automated scientific discovery using structured hypothesis management and evolution.",
      "llm_tldr_cn": "一种利用结构化假设管理和进化搜索实现自动化科学发现的多智能体框架。",
      "llm_tldr": "一种利用结构化假设管理和进化搜索实现自动化科学发现的多智能体框架。",
      "llm_tags": [
        "query:sr"
      ],
      "matched_query_tag": "query:sr",
      "matched_query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "matched_requirement_id": "req-3",
      "quick_tier": "7"
    }
  ]
}