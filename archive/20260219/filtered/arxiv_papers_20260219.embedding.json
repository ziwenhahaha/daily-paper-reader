{
  "top_k": 400,
  "generated_at": "2026-02-19T18:02:08.200064+00:00",
  "papers": [
    {
      "id": "2602.14960v1",
      "title": "DRAMA: Domain Retrieval using Adaptive Module Allocation",
      "abstract": "Neural models are increasingly used in Web-scale Information Retrieval (IR). However, relying on these models introduces substantial computational and energy requirements, leading to increasing attention toward their environmental cost and the sustainability of large-scale deployments. While neural IR models deliver high retrieval effectiveness, their scalability is constrained in multi-domain scenarios, where training and maintaining domain-specific models is inefficient and achieving robust cross-domain generalisation within a unified model remains difficult. This paper introduces DRAMA (Domain Retrieval using Adaptive Module Allocation), an energy- and parameter-efficient framework designed to reduce the environmental footprint of neural retrieval. DRAMA integrates domain-specific adapter modules with a dynamic gating mechanism that selects the most relevant domain knowledge for each query. New domains can be added efficiently through lightweight adapter training, avoiding full model retraining. We evaluate DRAMA on multiple Web retrieval benchmarks covering different domains. Our extensive evaluation shows that DRAMA achieves comparable effectiveness to domain-specific models while using only a fraction of their parameters and computational resources. These findings show that energy-aware model design can significantly improve scalability and sustainability in neural IR.",
      "authors": [
        "Pranav Kasela",
        "Marco Braga",
        "Ophir Frieder",
        "Nazli Goharian",
        "Gabriella Pasi",
        "Raffaele Perego"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-16 17:38:24+00:00",
      "link": "https://arxiv.org/pdf/2602.14960v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15091v1",
      "title": "Mixture-of-Experts under Finite-Rate Gating: Communication--Generalization Trade-offs",
      "abstract": "Mixture-of-Experts (MoE) architectures decompose prediction tasks into specialized expert sub-networks selected by a gating mechanism. This letter adopts a communication-theoretic view of MoE gating, modeling the gate as a stochastic channel operating under a finite information rate. Within an information-theoretic learning framework, we specialize a mutual-information generalization bound and develop a rate-distortion characterization $D(R_g)$ of finite-rate gating, where $R_g:=I(X; T)$, yielding (under a standard empirical rate-distortion optimality condition) $\\mathbb{E}[R(W)] \\le D(R_g)+δ_m+\\sqrt{(2/m)\\, I(S; W)}$. The analysis yields capacity-aware limits for communication-constrained MoE systems, and numerical simulations on synthetic multi-expert models empirically confirm the predicted trade-offs between gating rate, expressivity, and generalization.",
      "authors": [
        "Ali Khalesi",
        "Mohammad Reza Deylam Salehi"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.IT",
        "cs.LG"
      ],
      "published": "2026-02-16 17:26:12+00:00",
      "link": "https://arxiv.org/pdf/2602.15091v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14938v1",
      "title": "Variance-Reduced $(\\varepsilon,δ)-$Unlearning using Forget Set Gradients",
      "abstract": "In machine unlearning, $(\\varepsilon,δ)-$unlearning is a popular framework that provides formal guarantees on the effectiveness of the removal of a subset of training data, the forget set, from a trained model. For strongly convex objectives, existing first-order methods achieve $(\\varepsilon,δ)-$unlearning, but they only use the forget set to calibrate injected noise, never as a direct optimization signal. In contrast, efficient empirical heuristics often exploit the forget samples (e.g., via gradient ascent) but come with no formal unlearning guarantees. We bridge this gap by presenting the Variance-Reduced Unlearning (VRU) algorithm. To the best of our knowledge, VRU is the first first-order algorithm that directly includes forget set gradients in its update rule, while provably satisfying ($(\\varepsilon,δ)-$unlearning. We establish the convergence of VRU and show that incorporating the forget set yields strictly improved rates, i.e. a better dependence on the achieved error compared to existing first-order $(\\varepsilon,δ)-$unlearning methods. Moreover, we prove that, in a low-error regime, VRU asymptotically outperforms any first-order method that ignores the forget set.Experiments corroborate our theory, showing consistent gains over both state-of-the-art certified unlearning methods and over empirical baselines that explicitly leverage the forget set.",
      "authors": [
        "Martin Van Waerebeke",
        "Marco Lorenzi",
        "Kevin Scaman",
        "El Mahdi El Mhamdi",
        "Giovanni Neglia"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-02-16 17:20:14+00:00",
      "link": "https://arxiv.org/pdf/2602.14938v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14934v1",
      "title": "Activation-Space Uncertainty Quantification for Pretrained Networks",
      "abstract": "Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time.",
      "authors": [
        "Richard Bergna",
        "Stefan Depeweg",
        "Sergio Calvo-Ordoñez",
        "Jonathan Plenk",
        "Alvaro Cartea",
        "Jose Miguel Hernández-Lobato"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-16 17:17:08+00:00",
      "link": "https://arxiv.org/pdf/2602.14934v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14919v1",
      "title": "BHyGNN+: Unsupervised Representation Learning for Heterophilic Hypergraphs",
      "abstract": "Hypergraph Neural Networks (HyGNNs) have demonstrated remarkable success in modeling higher-order relationships among entities. However, their performance often degrades on heterophilic hypergraphs, where nodes connected by the same hyperedge tend to have dissimilar semantic representations or belong to different classes. While several HyGNNs, including our prior work BHyGNN, have been proposed to address heterophily, their reliance on labeled data significantly limits their applicability in real-world scenarios where annotations are scarce or costly. To overcome this limitation, we introduce BHyGNN+, a self-supervised learning framework that extends BHyGNN for representation learning on heterophilic hypergraphs without requiring ground-truth labels. The core idea of BHyGNN+ is hypergraph duality, a structural transformation where the roles of nodes and hyperedges are interchanged. By contrasting augmented views of a hypergraph against its dual using cosine similarity, our framework captures essential structural patterns in a fully unsupervised manner. Notably, this duality-based formulation eliminates the need for negative samples, a common requirement in existing hypergraph contrastive learning methods that is often difficult to satisfy in practice. Extensive experiments on eleven benchmark datasets demonstrate that BHyGNN+ consistently outperforms state-of-the-art supervised and self-supervised baselines on both heterophilic and homophilic hypergraphs. Our results validate the effectiveness of leveraging hypergraph duality for self-supervised learning and establish a new paradigm for representation learning on challenging, unlabeled hypergraphs.",
      "authors": [
        "Tianyi Ma",
        "Yiyue Qian",
        "Zehong Wang",
        "Zheyuan Zhang",
        "Chuxu Zhang",
        "Yanfang Ye"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16 16:55:37+00:00",
      "link": "https://arxiv.org/pdf/2602.14919v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14917v1",
      "title": "BFS-PO: Best-First Search for Large Reasoning Models",
      "abstract": "Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.",
      "authors": [
        "Fiorenzo Parascandolo",
        "Wenhui Tan",
        "Enver Sangineto",
        "Ruihua Song",
        "Rita Cucchiara"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-16 16:53:41+00:00",
      "link": "https://arxiv.org/pdf/2602.14917v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14913v1",
      "title": "Coverage Guarantees for Pseudo-Calibrated Conformal Prediction under Distribution Shift",
      "abstract": "Conformal prediction (CP) offers distribution-free marginal coverage guarantees under an exchangeability assumption, but these guarantees can fail if the data distribution shifts. We analyze the use of pseudo-calibration as a tool to counter this performance loss under a bounded label-conditional covariate shift model. Using tools from domain adaptation, we derive a lower bound on target coverage in terms of the source-domain loss of the classifier and a Wasserstein measure of the shift. Using this result, we provide a method to design pseudo-calibrated sets that inflate the conformal threshold by a slack parameter to keep target coverage above a prescribed level. Finally, we propose a source-tuned pseudo-calibration algorithm that interpolates between hard pseudo-labels and randomized labels as a function of classifier uncertainty. Numerical experiments show that our bounds qualitatively track pseudo-calibration behavior and that the source-tuned scheme mitigates coverage degradation under distribution shift while maintaining nontrivial prediction set sizes.",
      "authors": [
        "Farbod Siahkali",
        "Ashwin Verma",
        "Vijay Gupta"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "eess.IV"
      ],
      "published": "2026-02-16 16:48:39+00:00",
      "link": "https://arxiv.org/pdf/2602.14913v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14901v1",
      "title": "Picking the Right Specialist: Attentive Neural Process-based Selection of Task-Specialized Models as Tools for Agentic Healthcare Systems",
      "abstract": "Task-specialized models form the backbone of agentic healthcare systems, enabling the agents to answer clinical queries across tasks such as disease diagnosis, localization, and report generation. Yet, for a given task, a single \"best\" model rarely exists. In practice, each task is better served by multiple competing specialist models where different models excel on different data samples. As a result, for any given query, agents must reliably select the right specialist model from a heterogeneous pool of tool candidates. To this end, we introduce ToolSelect, which adaptively learns model selection for tools by minimizing a population risk over sampled specialist tool candidates using a consistent surrogate of the task-conditional selection loss. Concretely, we propose an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among the specialist models. Motivated by the absence of any established testbed, we, for the first time, introduce an agentic Chest X-ray environment equipped with a diverse suite of task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA) and develop ToolSelectBench, a benchmark of 1448 queries. Our results demonstrate that ToolSelect consistently outperforms 10 SOTA methods across four different task families.",
      "authors": [
        "Pramit Saha",
        "Joshua Strong",
        "Mohammad Alsharid",
        "Divyanshu Mishra",
        "J. Alison Noble"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MA"
      ],
      "published": "2026-02-16 16:36:32+00:00",
      "link": "https://arxiv.org/pdf/2602.14901v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14896v1",
      "title": "Algorithmic Simplification of Neural Networks with Mosaic-of-Motifs",
      "abstract": "Large-scale deep learning models are well-suited for compression. Methods like pruning, quantization, and knowledge distillation have been used to achieve massive reductions in the number of model parameters, with marginal performance drops across a variety of architectures and tasks. This raises the central question: \\emph{Why are deep neural networks suited for compression?} In this work, we take up the perspective of algorithmic complexity to explain this behavior. We hypothesize that the parameters of trained models have more structure and, hence, exhibit lower algorithmic complexity compared to the weights at (random) initialization. Furthermore, that model compression methods harness this reduced algorithmic complexity to compress models. Although an unconstrained parameterization of model weights, $\\mathbf{w} \\in \\mathbb{R}^n$, can represent arbitrary weight assignments, the solutions found during training exhibit repeatability and structure, making them algorithmically simpler than a generic program. To this end, we formalize the Kolmogorov complexity of $\\mathbf{w}$ by $\\mathcal{K}(\\mathbf{w})$. We introduce a constrained parameterization $\\widehat{\\mathbf{w}}$, that partitions parameters into blocks of size $s$, and restricts each block to be selected from a set of $k$ reusable motifs, specified by a reuse pattern (or mosaic). The resulting method, $\\textit{Mosaic-of-Motifs}$ (MoMos), yields algorithmically simpler model parameterization compared to unconstrained models. Empirical evidence from multiple experiments shows that the algorithmic complexity of neural networks, measured using approximations to Kolmogorov complexity, can be reduced during training. This results in models that perform comparably with unconstrained models while being algorithmically simpler.",
      "authors": [
        "Pedram Bakhtiarifard",
        "Tong Chen",
        "Jonathan Wenshøj",
        "Erik B Dam",
        "Raghavendra Selvan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16 16:30:38+00:00",
      "link": "https://arxiv.org/pdf/2602.14896v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14890v1",
      "title": "Lifted Relational Probabilistic Inference via Implicit Learning",
      "abstract": "Reconciling the tension between inductive learning and deductive reasoning in first-order relational domains is a longstanding challenge in AI. We study the problem of answering queries in a first-order relational probabilistic logic through a joint effort of learning and reasoning, without ever constructing an explicit model. Traditional lifted inference assumes access to a complete model and exploits symmetry to evaluate probabilistic queries; however, learning such models from partial, noisy observations is intractable in general. We reconcile these two challenges through implicit learning to reason and first-order relational probabilistic inference techniques. More specifically, we merge incomplete first-order axioms with independently sampled, partially observed examples into a bounded-degree fragment of the sum-of-squares (SOS) hierarchy in polynomial time. Our algorithm performs two lifts simultaneously: (i) grounding-lift, where renaming-equivalent ground moments share one variable, collapsing the domain of individuals; and (ii) world-lift, where all pseudo-models (partial world assignments) are enforced in parallel, producing a global bound that holds across all worlds consistent with the learned constraints. These innovations yield the first polynomial-time framework that implicitly learns a first-order probabilistic logic and performs lifted inference over both individuals and worlds.",
      "authors": [
        "Luise Ge",
        "Brendan Juba",
        "Kris Nilsson",
        "Alison Shao"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-16 16:24:13+00:00",
      "link": "https://arxiv.org/pdf/2602.14890v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14889v1",
      "title": "Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment",
      "abstract": "We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.",
      "authors": [
        "Mounvik K",
        "N Harshit"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.ET",
        "cs.HC",
        "cs.NE"
      ],
      "published": "2026-02-16 16:20:37+00:00",
      "link": "https://arxiv.org/pdf/2602.14889v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14885v1",
      "title": "Drift-Diffusion Matching: Embedding dynamics in latent manifolds of asymmetric neural networks",
      "abstract": "Recurrent neural networks (RNNs) provide a theoretical framework for understanding computation in biological neural circuits, yet classical results, such as Hopfield's model of associative memory, rely on symmetric connectivity that restricts network dynamics to gradient-like flows. In contrast, biological networks support rich time-dependent behaviour facilitated by their asymmetry. Here we introduce a general framework, which we term drift-diffusion matching, for training continuous-time RNNs to represent arbitrary stochastic dynamical systems within a low-dimensional latent subspace. Allowing asymmetric connectivity, we show that RNNs can faithfully embed the drift and diffusion of a given stochastic differential equation, including nonlinear and nonequilibrium dynamics such as chaotic attractors. As an application, we construct RNN realisations of stochastic systems that transiently explore various attractors through both input-driven switching and autonomous transitions driven by nonequilibrium currents, which we interpret as models of associative and sequential (episodic) memory. To elucidate how these dynamics are encoded in the network, we introduce decompositions of the RNN based on its asymmetric connectivity and its time-irreversibility. Our results extend attractor neural network theory beyond equilibrium, showing that asymmetric neural populations can implement a broad class of dynamical computations within low-dimensional manifolds, unifying ideas from associative memory, nonequilibrium statistical mechanics, and neural computation.",
      "authors": [
        "Ramón Nartallo-Kaluarachchi",
        "Renaud Lambiotte",
        "Alain Goriely"
      ],
      "primary_category": "cond-mat.dis-nn",
      "categories": [
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.LG",
        "q-bio.NC"
      ],
      "published": "2026-02-16 16:15:59+00:00",
      "link": "https://arxiv.org/pdf/2602.14885v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14881v1",
      "title": "Numerical exploration of the range of shape functionals using neural networks",
      "abstract": "We introduce a novel numerical framework for the exploration of Blaschke--Santaló diagrams, which are efficient tools characterizing the possible inequalities relating some given shape functionals. We introduce a parametrization of convex bodies in arbitrary dimensions using a specific invertible neural network architecture based on gauge functions, allowing an intrinsic conservation of the convexity of the sets during the shape optimization process. To achieve a uniform sampling inside the diagram, and thus a satisfying description of it, we introduce an interacting particle system that minimizes a Riesz energy functional via automatic differentiation in PyTorch. The effectiveness of the method is demonstrated on several diagrams involving both geometric and PDE-type functionals for convex bodies of $\\mathbb{R}^2$ and $\\mathbb{R}^3$, namely, the volume, the perimeter, the moment of inertia, the torsional rigidity, the Willmore energy, and the first two Neumann eigenvalues of the Laplacian.",
      "authors": [
        "Eloi Martinet",
        "Ilias Ftouhi"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.AI"
      ],
      "published": "2026-02-16 16:10:58+00:00",
      "link": "https://arxiv.org/pdf/2602.14881v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14879v1",
      "title": "CT-Bench: A Benchmark for Multimodal Lesion Understanding in Computed Tomography",
      "abstract": "Artificial intelligence (AI) can automatically delineate lesions on computed tomography (CT) and generate radiology report content, yet progress is limited by the scarcity of publicly available CT datasets with lesion-level annotations. To bridge this gap, we introduce CT-Bench, a first-of-its-kind benchmark dataset comprising two components: a Lesion Image and Metadata Set containing 20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, and size information, and a multitask visual question answering benchmark with 2,850 QA pairs covering lesion localization, description, size estimation, and attribute categorization. Hard negative examples are included to reflect real-world diagnostic challenges. We evaluate multiple state-of-the-art multimodal models, including vision-language and medical CLIP variants, by comparing their performance to radiologist assessments, demonstrating the value of CT-Bench as a comprehensive benchmark for lesion analysis. Moreover, fine-tuning models on the Lesion Image and Metadata Set yields significant performance gains across both components, underscoring the clinical utility of CT-Bench.",
      "authors": [
        "Qingqing Zhu",
        "Qiao Jin",
        "Tejas S. Mathai",
        "Yin Fang",
        "Zhizheng Wang",
        "Yifan Yang",
        "Maame Sarfo-Gyamfi",
        "Benjamin Hou",
        "Ran Gu",
        "Praveen T. S. Balamuralikrishna",
        "Kenneth C. Wang",
        "Ronald M. Summers",
        "Zhiyong Lu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-16 16:10:19+00:00",
      "link": "https://arxiv.org/pdf/2602.14879v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14869v1",
      "title": "Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution",
      "abstract": "As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster. We empirically validate Concept Influence and approximations across emergent misalignment benchmarks and real post-training datasets, and demonstrate they achieve comparable performance to classical influence functions while being substantially more scalable. More broadly, we show that incorporating interpretable structure within traditional TDA pipelines can enable more scalable, explainable, and better control of model behavior through data.",
      "authors": [
        "Matthew Kowal",
        "Goncalo Paulo",
        "Louis Jaburi",
        "Tom Tseng",
        "Lev E McKinney",
        "Stefan Heimersheim",
        "Aaron David Tucker",
        "Adam Gleave",
        "Kellin Pelrine"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-02-16 16:02:09+00:00",
      "link": "https://arxiv.org/pdf/2602.14869v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14868v1",
      "title": "Goldilocks RL: Tuning Task Difficulty to Escape Sparse Rewards for Reasoning",
      "abstract": "Reinforcement learning has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, relying on sparse rewards makes this process highly sample-inefficient, as models must navigate vast search spaces with minimal feedback. While classic curriculum learning aims to mitigate this by ordering data based on complexity, the right ordering for a specific model is often unclear. To address this, we propose Goldilocks, a novel teacher-driven data sampling strategy that aims to predict each question's difficulty for the student model. The teacher model selects questions of appropriate difficulty for the student model, i.e., questions that are neither too easy nor too hard (Goldilocks principle), while training the student with GRPO. By leveraging the student's performance on seen samples, the teacher continuously adapts to the student's evolving abilities. On OpenMathReasoning dataset, Goldilocks data sampling improves the performance of models trained with standard GRPO under the same compute budget.",
      "authors": [
        "Ilia Mahrooghi",
        "Aryo Lotfi",
        "Emmanuel Abbe"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16 16:01:27+00:00",
      "link": "https://arxiv.org/pdf/2602.14868v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14855v1",
      "title": "A Pragmatic Method for Comparing Clusterings with Overlaps and Outliers",
      "abstract": "Clustering algorithms are an essential part of the unsupervised data science ecosystem, and extrinsic evaluation of clustering algorithms requires a method for comparing the detected clustering to a ground truth clustering. In a general setting, the detected and ground truth clusterings may have outliers (objects belonging to no cluster), overlapping clusters (objects may belong to more than one cluster), or both, but methods for comparing these clusterings are currently undeveloped. In this note, we define a pragmatic similarity measure for comparing clusterings with overlaps and outliers, show that it has several desirable properties, and experimentally confirm that it is not subject to several common biases afflicting other clustering comparison measures.",
      "authors": [
        "Ryan DeWolfe",
        "Paweł Prałat",
        "François Théberge"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.SI",
        "math.CO"
      ],
      "published": "2026-02-16 15:51:09+00:00",
      "link": "https://arxiv.org/pdf/2602.14855v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14853v1",
      "title": "BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations",
      "abstract": "The traditional limitations of neural networks in reliably generalizing beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve PDEs in regimes far beyond anything which can be experimentally or analytically validated. In this paper, we show how it is possible to circumvent these limitations by constructing formally-verified neural network solvers for PDEs, with rigorous convergence, stability, and conservation properties, whose correctness can therefore be guaranteed even in extrapolatory regimes. By using the method of characteristics to predict the analytical properties of PDE solutions a priori (even in regions arbitrarily far from the training domain), we show how it is possible to construct rigorous extrapolatory bounds on the worst-case L^inf errors of shallow neural network approximations. Then, by decomposing PDE solutions into compositions of simpler functions, we show how it is possible to compose these shallow neural networks together to form deep architectures, based on ideas from compositional deep learning, in which the large L^inf errors in the approximations have been suppressed. The resulting framework, called BEACONS (Bounded-Error, Algebraically-COmposable Neural Solvers), comprises both an automatic code-generator for the neural solvers themselves, as well as a bespoke automated theorem-proving system for producing machine-checkable certificates of correctness. We apply the framework to a variety of linear and non-linear PDEs, including the linear advection and inviscid Burgers' equations, as well as the full compressible Euler equations, in both 1D and 2D, and illustrate how BEACONS architectures are able to extrapolate solutions far beyond the training data in a reliable and bounded way. Various advantages of the approach over the classical PINN approach are discussed.",
      "authors": [
        "Jonathan Gorard",
        "Ammar Hakim",
        "James Juno"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.NA",
        "physics.comp-ph"
      ],
      "published": "2026-02-16 15:49:19+00:00",
      "link": "https://arxiv.org/pdf/2602.14853v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14852v1",
      "title": "Lower Estimates for $L_1$-Distortion of Transportation Cost Spaces",
      "abstract": "Quantifying the degree of dissimilarity between two probability distributions on a finite metric space is a fundamental task in Computer Science and Computer Vision. A natural dissimilarity measure based on optimal transport is the Earth Mover's Distance (EMD). A key technique for analyzing this metric, pioneered by Charikar (2002) and Indyk and Thaper (2003), involves constructing low-distortion embeddings of EMD(X) into the Lebesgue space $L_1$.   It became a key problem to investigate whether the upper bound of $O(\\log n)$ can be improved for important classes of metric spaces known to admit low-distortion embeddings into $L_1$. In the context of Computer Vision, grid graphs, especially planar grids, are among the most fundamental. Indyk posed the related problem of estimating the $L_1$-distortion of the space of uniform distributions on $n$-point subsets of $R^2$. The Progress Report, last updated in August 2011, highlighted two key results: first, the work of Khot and Naor (2006) on Hamming cubes, which showed that the $L_1$-distortion for Hamming cubes meets the described above upper estimate, and second, the result of Naor and Schechtman (2007) for planar grids, which established that the $L_1$-distortion of for a planar $n$ by $n$ grid is $Ω(\\sqrt{\\log n})$.   Our first result is the improvement of the lower bound on the $L_1$-distortion for grids to $Ω(\\log n)$, matching the universal upper bound up to multiplicative constants. The key ingredient allowing us to obtain these sharp estimates is a new Sobolev-type inequality for scalar-valued functions on the grid graphs. Our method is also applicable to many recursive families of graphs, such as diamond and Laakso graphs. We obtain the sharp distortion estimates of $\\log n$ in these cases as well.",
      "authors": [
        "Chris Gartland",
        "Mikhail Ostrovskii"
      ],
      "primary_category": "math.FA",
      "categories": [
        "math.FA",
        "cs.CG",
        "math.CO",
        "math.MG"
      ],
      "published": "2026-02-16 15:49:06+00:00",
      "link": "https://arxiv.org/pdf/2602.14852v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14828v1",
      "title": "Exploring the limits of pre-trained embeddings in machine-guided protein design: a case study on predicting AAV vector viability",
      "abstract": "Effective representations of protein sequences are widely recognized as a cornerstone of machine learning-based protein design. Yet, protein bioengineering poses unique challenges for sequence representation, as experimental datasets typically feature few mutations, which are either sparsely distributed across the entire sequence or densely concentrated within localized regions. This limits the ability of sequence-level representations to extract functionally meaningful signals. In addition, comprehensive comparative studies remain scarce, despite their crucial role in clarifying which representations best encode relevant information and ultimately support superior predictive performance. In this study, we systematically evaluate multiple ProtBERT and ESM2 embedding variants as sequence representations, using the adeno-associated virus capsid as a case study and prototypical example of bioengineering, where functional optimization is targeted through highly localized sequence variation within an otherwise large protein. Our results reveal that, prior to fine-tuning, amino acid-level embeddings outperform sequence-level representations in supervised predictive tasks, whereas the latter tend to be more effective in unsupervised settings. However, optimal performance is only achieved when embeddings are fine-tuned with task-specific labels, with sequence-level representations providing the best performance. Moreover, our findings indicate that the extent of sequence variation required to produce notable shifts in sequence representations exceeds what is typically explored in bioengineering studies, showing the need for fine-tuning in datasets characterized by sparse or highly localized mutations.",
      "authors": [
        "Ana F. Rodrigues",
        "Lucas Ferraz",
        "Laura Balbi",
        "Pedro Giesteira Cotovio",
        "Catia Pesquita"
      ],
      "primary_category": "q-bio.QM",
      "categories": [
        "q-bio.QM",
        "cs.LG"
      ],
      "published": "2026-02-16 15:21:11+00:00",
      "link": "https://arxiv.org/pdf/2602.14828v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14819v1",
      "title": "Testimole-Conversational: A 30-Billion-Word Italian Discussion Board Corpus (1996-2024) for Language Modeling and Sociolinguistic Research",
      "abstract": "We present \"Testimole-conversational\" a massive collection of discussion boards messages in the Italian language. The large size of the corpus, more than 30B word-tokens (1996-2024), renders it an ideal dataset for native Italian Large Language Models'pre-training. Furthermore, discussion boards' messages are a relevant resource for linguistic as well as sociological analysis. The corpus captures a rich variety of computer-mediated communication, offering insights into informal written Italian, discourse dynamics, and online social interaction in wide time span. Beyond its relevance for NLP applications such as language modelling, domain adaptation, and conversational analysis, it also support investigations of language variation and social phenomena in digital communication. The resource will be made freely available to the research community.",
      "authors": [
        "Matteo Rinaldi",
        "Rossella Varvara",
        "Viviana Patti"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 15:12:46+00:00",
      "link": "https://arxiv.org/pdf/2602.14819v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14814v1",
      "title": "Learning State-Tracking from Code Using Linear RNNs",
      "abstract": "Over the last years, state-tracking tasks, particularly permutation composition, have become a testbed to understand the limits of sequence models architectures like Transformers and RNNs (linear and non-linear). However, these are often sequence-to-sequence tasks: learning to map actions (permutations) to states, which is incompatible with the next-token prediction setting commonly used to train language models. We address this gap by converting permutation composition into code via REPL traces that interleave state-reveals through prints and variable transformations. We show that linear RNNs capable of state-tracking excel also in this setting, while Transformers still fail. Motivated by this representation, we investigate why tracking states in code is generally difficult: actions are not always fully observable. We frame this as tracking the state of a probabilistic finite-state automaton with deterministic state reveals and show that linear RNNs can be worse than non-linear RNNs at tracking states in this setup.",
      "authors": [
        "Julien Siems",
        "Riccardo Grazzi",
        "Kirill Kalinin",
        "Hitesh Ballani",
        "Babak Rahmani"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-16 15:07:51+00:00",
      "link": "https://arxiv.org/pdf/2602.14814v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14812v1",
      "title": "Physical Commonsense Reasoning for Lower-Resourced Languages and Dialects: a Study on Basque",
      "abstract": "Physical commonsense reasoning represents a fundamental capability of human intelligence, enabling individuals to understand their environment, predict future events, and navigate physical spaces. Recent years have witnessed growing interest in reasoning tasks within Natural Language Processing (NLP). However, no prior research has examined the performance of Large Language Models (LLMs) on non-question-answering (non-QA) physical commonsense reasoning tasks in low-resource languages such as Basque. Taking the Italian GITA as a starting point, this paper addresses this gap by presenting BasPhyCo, the first non-QA physical commonsense reasoning dataset for Basque, available in both standard and dialectal variants. We evaluate model performance across three hierarchical levels of commonsense understanding: (1) distinguishing between plausible and implausible narratives (accuracy), (2) identifying the conflicting element that renders a narrative implausible (consistency), and (3) determining the specific physical state that creates the implausibility (verifiability). These tasks were assessed using multiple multilingual LLMs as well as models pretrained specifically for Italian and Basque. Results indicate that, in terms of verifiability, LLMs exhibit limited physical commonsense capabilities in low-resource languages such as Basque, especially when processing dialectal variants.",
      "authors": [
        "Jaione Bengoetxea",
        "Itziar Gonzalez-Dios",
        "Rodrigo Agerri"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 15:04:35+00:00",
      "link": "https://arxiv.org/pdf/2602.14812v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15089v1",
      "title": "Hybrid Feature Learning with Time Series Embeddings for Equipment Anomaly Prediction",
      "abstract": "In predictive maintenance of equipment, deep learning-based time series anomaly detection has garnered significant attention; however, pure deep learning approaches often fail to achieve sufficient accuracy on real-world data. This study proposes a hybrid approach that integrates 64-dimensional time series embeddings from Granite TinyTimeMixer with 28-dimensional statistical features based on domain knowledge for HVAC equipment anomaly prediction tasks. Specifically, we combine time series embeddings extracted from a Granite TinyTimeMixer encoder fine-tuned with LoRA (Low-Rank Adaptation) and 28 types of statistical features including trend, volatility, and drawdown indicators, which are then learned using a LightGBM gradient boosting classifier. In experiments using 64 equipment units and 51,564 samples, we achieved Precision of 91--95\\% and ROC-AUC of 0.995 for anomaly prediction at 30-day, 60-day, and 90-day horizons. Furthermore, we achieved production-ready performance with a false positive rate of 1.1\\% or less and a detection rate of 88--94\\%, demonstrating the effectiveness of the system for predictive maintenance applications. This work demonstrates that practical anomaly detection systems can be realized by leveraging the complementary strengths between deep learning's representation learning capabilities and statistical feature engineering.",
      "authors": [
        "Takato Yasuno"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-16 15:00:15+00:00",
      "link": "https://arxiv.org/pdf/2602.15089v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14795v1",
      "title": "Return of the Schema: Building Complete Datasets for Machine Learning and Reasoning on Knowledge Graphs",
      "abstract": "Datasets for the experimental evaluation of knowledge graph refinement algorithms typically contain only ground facts, retaining very limited schema level knowledge even when such information is available in the source knowledge graphs. This limits the evaluation of methods that rely on rich ontological constraints, reasoning or neurosymbolic techniques and ultimately prevents assessing their performance in large-scale, real-world knowledge graphs. In this paper, we present \\resource{} the first resource that provides a workflow for extracting datasets including both schema and ground facts, ready for machine learning and reasoning services, along with the resulting curated suite of datasets. The workflow also handles inconsistencies detected when keeping both schema and facts and also leverage reasoning for entailing implicit knowledge. The suite includes newly extracted datasets from KGs with expressive schemas while simultaneously enriching existing datasets with schema information. Each dataset is serialized in OWL making it ready for reasoning services. Moreover, we provide utilities for loading datasets in tensor representations typical of standard machine learning libraries.",
      "authors": [
        "Ivan Diliso",
        "Roberto Barile",
        "Claudia d'Amato",
        "Nicola Fanizzi"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-16 14:42:14+00:00",
      "link": "https://arxiv.org/pdf/2602.14795v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14793v1",
      "title": "Beyond Retractions: Forensic Scientometrics Techniques to Identify Research Misconduct, Citation Leakage, and Funding Anomalies",
      "abstract": "This paper presents a forensic scientometric case study of the Pharmakon Neuroscience Research Network, a fabricated research collective that operated primarily between 2019 and 2022 while embedding itself within legitimate scholarly publishing channels.",
      "authors": [
        "Leslie D. McIntosh",
        "Alexandra Sinclair",
        "Simon Linacre"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-16 14:41:03+00:00",
      "link": "https://arxiv.org/pdf/2602.14793v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14791v1",
      "title": "Extending Multi-Source Bayesian Optimization With Causality Principles",
      "abstract": "Multi-Source Bayesian Optimization (MSBO) serves as a variant of the traditional Bayesian Optimization (BO) framework applicable to situations involving optimization of an objective black-box function over multiple information sources such as simulations, surrogate models, or real-world experiments. However, traditional MSBO assumes the input variables of the objective function to be independent and identically distributed, limiting its effectiveness in scenarios where causal information is available and interventions can be performed, such as clinical trials or policy-making. In the single-source domain, Causal Bayesian Optimization (CBO) extends standard BO with the principles of causality, enabling better modeling of variable dependencies. This leads to more accurate optimization, improved decision-making, and more efficient use of low-cost information sources. In this article, we propose a principled integration of the MSBO and CBO methodologies in the multi-source domain, leveraging the strengths of both to enhance optimization efficiency and reduce computational complexity in higher-dimensional problems. We present the theoretical foundations of both Causal and Multi-Source Bayesian Optimization, and demonstrate how their synergy informs our Multi-Source Causal Bayesian Optimization (MSCBO) algorithm. We compare the performance of MSCBO against its foundational counterparts for both synthetic and real-world datasets with varying levels of noise, highlighting the robustness and applicability of MSCBO. Based on our findings, we conclude that integrating MSBO with the causality principles of CBO facilitates dimensionality reduction and lowers operational costs, ultimately improving convergence speed, performance, and scalability.",
      "authors": [
        "Luuk Jacobs",
        "Mohammad Ali Javidian"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-16 14:38:16+00:00",
      "link": "https://arxiv.org/pdf/2602.14791v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14789v1",
      "title": "On the Stability of Nonlinear Dynamics in GD and SGD: Beyond Quadratic Potentials",
      "abstract": "The dynamical stability of the iterates during training plays a key role in determining the minima obtained by optimization algorithms. For example, stable solutions of gradient descent (GD) correspond to flat minima, which have been associated with favorable features. While prior work often relies on linearization to determine stability, it remains unclear whether linearized dynamics faithfully capture the full nonlinear behavior. Recent work has shown that GD may stably oscillate near a linearly unstable minimum and still converge once the step size decays, indicating that linear analysis can be misleading. In this work, we explicitly study the effect of nonlinear terms. Specifically, we derive an exact criterion for stable oscillations of GD near minima in the multivariate setting. Our condition depends on high-order derivatives, generalizing existing results. Extending the analysis to stochastic gradient descent (SGD), we show that nonlinear dynamics can diverge in expectation even if a single batch is unstable. This implies that stability can be dictated by a single batch that oscillates unstably, rather than an average effect, as linear analysis suggests. Finally, we prove that if all batches are linearly stable, the nonlinear dynamics of SGD are stable in expectation.",
      "authors": [
        "Rotem Mulayoff",
        "Sebastian U. Stich"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-16 14:36:55+00:00",
      "link": "https://arxiv.org/pdf/2602.14789v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14784v1",
      "title": "Intent-Driven Dynamic Chunking: Segmenting Documents to Reflect Predicted Information Needs",
      "abstract": "Breaking long documents into smaller segments is a fundamental challenge in information retrieval. Whether for search engines, question-answering systems, or retrieval-augmented generation (RAG), effective segmentation determines how well systems can locate and return relevant information. However, traditional methods, such as fixed-length or coherence-based segmentation, ignore user intent, leading to chunks that split answers or contain irrelevant noise. We introduce Intent-Driven Dynamic Chunking (IDC), a novel approach that uses predicted user queries to guide document segmentation. IDC leverages a Large Language Model to generate likely user intents for a document and then employs a dynamic programming algorithm to find the globally optimal chunk boundaries. This represents a novel application of DP to intent-aware segmentation that avoids greedy pitfalls. We evaluated IDC on six diverse question-answering datasets, including news articles, Wikipedia, academic papers, and technical documentation. IDC outperformed traditional chunking strategies on five datasets, improving top-1 retrieval accuracy by 5% to 67%, and matched the best baseline on the sixth. Additionally, IDC produced 40-60% fewer chunks than baseline methods while achieving 93-100% answer coverage. These results demonstrate that aligning document structure with anticipated information needs significantly boosts retrieval performance, particularly for long and heterogeneous documents.",
      "authors": [
        "Christos Koutsiaris"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-16 14:32:18+00:00",
      "link": "https://arxiv.org/pdf/2602.14784v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14772v1",
      "title": "Learning Structural Hardness for Combinatorial Auctions: Instance-Dependent Algorithm Selection via Graph Neural Networks",
      "abstract": "The Winner Determination Problem (WDP) in combinatorial auctions is NP-hard, and no existing method reliably predicts which instances will defeat fast greedy heuristics. The ML-for-combinatorial-optimization community has focused on learning to \\emph{replace} solvers, yet recent evidence shows that graph neural networks (GNNs) rarely outperform well-tuned classical methods on standard benchmarks. We pursue a different objective: learning to predict \\emph{when} a given instance is hard for greedy allocation, enabling instance-dependent algorithm selection. We design a 20-dimensional structural feature vector and train a lightweight MLP hardness classifier that predicts the greedy optimality gap with mean absolute error 0.033, Pearson correlation 0.937, and binary classification accuracy 94.7\\% across three random seeds. For instances identified as hard -- those exhibiting ``whale-fish'' trap structure where greedy provably fails -- we deploy a heterogeneous GNN specialist that achieves ${\\approx}0\\%$ optimality gap on all six adversarial configurations tested (vs.\\ 3.75--59.24\\% for greedy). A hybrid allocator combining the hardness classifier with GNN and greedy solvers achieves 0.51\\% overall gap on mixed distributions. Our honest evaluation on CATS benchmarks confirms that GNNs do not outperform Gurobi (0.45--0.71 vs.\\ 0.20 gap), motivating the algorithm selection framing. Learning \\emph{when} to deploy expensive solvers is more tractable than learning to replace them.",
      "authors": [
        "Sungwoo Kang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16 14:26:25+00:00",
      "link": "https://arxiv.org/pdf/2602.14772v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14767v1",
      "title": "SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning",
      "abstract": "Continual learning remains constrained by the need for repeated retraining, high computational costs, and the persistent challenge of forgetting. These factors significantly limit the applicability of continuous learning in real-world settings, as iterative model updates require significant computational resources and inherently exacerbate forgetting. We present SAILS -- Segment Anything with Incrementally Learned Semantics, a training-free framework for Class-Incremental Semantic Segmentation (CISS) that sidesteps these challenges entirely. SAILS leverages foundational models to decouple CISS into two stages: Zero-shot region extraction using Segment Anything Model (SAM), followed by semantic association through prototypes in a fixed feature space. SAILS incorporates selective intra-class clustering, resulting in multiple prototypes per class to better model intra-class variability. Our results demonstrate that, despite requiring no incremental training, SAILS typically surpasses the performance of existing training-based approaches on standard CISS datasets, particularly in long and challenging task sequences where forgetting tends to be most severe. By avoiding parameter updates, SAILS completely eliminates forgetting and maintains consistent, task-invariant performance. Furthermore, SAILS exhibits positive backward transfer, where the introduction of new classes can enhance performance on previous classes.",
      "authors": [
        "Shishir Muralidhara",
        "Didier Stricker",
        "René Schuster"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-16 14:14:02+00:00",
      "link": "https://arxiv.org/pdf/2602.14767v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14763v1",
      "title": "Unlocking Reasoning Capability on Machine Translation in Large Language Models",
      "abstract": "Reasoning-oriented large language models (RLMs) achieve strong gains on tasks such as mathematics and coding by generating explicit intermediate reasoning. However, their impact on machine translation (MT) remains underexplored. We systematically evaluate several open- and closed-weights RLMs on the WMT24++ benchmark and find that enabling explicit reasoning consistently degrades translation quality across languages and models. Analysis reveals that MT reasoning traces are highly linear, lacking revision, self-correction and exploration of alternative translations, which limits their usefulness. Furthermore, injecting higher-quality reasoning traces from stronger models does not reliably improve weaker models' performance. To address this mismatch, we propose a structured reasoning framework tailored to translation, based on multi-step drafting, adequacy refinement, fluency improvement, and selective iterative revision. We curate a synthetic dataset of dynamic structured reasoning traces and post-train a large reasoning model on this data. Experiments show significant improvements over standard translation fine-tuning and injected generic reasoning baselines. Our findings demonstrate that reasoning must be task-structured to benefit MT.",
      "authors": [
        "Sara Rajaee",
        "Sebastian Vincent",
        "Alexandre Berard",
        "Marzieh Fadaee",
        "Kelly Marchisio",
        "Tom Kocmi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-16 14:05:59+00:00",
      "link": "https://arxiv.org/pdf/2602.14763v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14761v1",
      "title": "Universal Algorithm-Implicit Learning",
      "abstract": "Current meta-learning methods are constrained to narrow task distributions with fixed feature and label spaces, limiting applicability. Moreover, the current meta-learning literature uses key terms like \"universal\" and \"general-purpose\" inconsistently and lacks precise definitions, hindering comparability. We introduce a theoretical framework for meta-learning which formally defines practical universality and introduces a distinction between algorithm-explicit and algorithm-implicit learning, providing a principled vocabulary for reasoning about universal meta-learning methods. Guided by this framework, we present TAIL, a transformer-based algorithm-implicit meta-learner that functions across tasks with varying domains, modalities, and label configurations. TAIL features three innovations over prior transformer-based meta-learners: random projections for cross-modal feature encoding, random injection label embeddings that extrapolate to larger label spaces, and efficient inline query processing. TAIL achieves state-of-the-art performance on standard few-shot benchmarks while generalizing to unseen domains. Unlike other meta-learning methods, it also generalizes to unseen modalities, solving text classification tasks despite training exclusively on images, handles tasks with up to 20$\\times$ more classes than seen during training, and provides orders-of-magnitude computational savings over prior transformer-based approaches.",
      "authors": [
        "Stefano Woerner",
        "Seong Joon Oh",
        "Christian F. Baumgartner"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-16 14:05:07+00:00",
      "link": "https://arxiv.org/pdf/2602.14761v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14760v1",
      "title": "Residual Connections and the Causal Shift: Uncovering a Structural Misalignment in Transformers",
      "abstract": "Large Language Models (LLMs) are trained with next-token prediction, implemented in autoregressive Transformers via causal masking for parallelism. This creates a subtle misalignment: residual connections tie activations to the current token, while supervision targets the next token, potentially propagating mismatched information if the current token is not the most informative for prediction. In this work, we empirically localize this input-output alignment shift in pretrained LLMs, using decoding trajectories over tied embedding spaces and similarity-based metrics. Our experiments reveal that the hidden token representations switch from input alignment to output alignment deep within the network. Motivated by this observation, we propose a lightweight residual-path mitigation based on residual attenuation, implemented either as a fixed-layer intervention or as a learnable gating mechanism. Experiments on multiple benchmarks show that these strategies alleviate the representation misalignment and yield improvements, providing an efficient and general architectural enhancement for autoregressive Transformers.",
      "authors": [
        "Jonathan Lys",
        "Vincent Gripon",
        "Bastien Pasdeloup",
        "Lukas Mauch",
        "Fabien Cardinaux",
        "Ghouthi Boukli Hacene"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-16 14:04:42+00:00",
      "link": "https://arxiv.org/pdf/2602.14760v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14759v1",
      "title": "Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training",
      "abstract": "Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner representations (i.e. the input of these blocks) can be interpreted as iterative refinement of a propagated latent representation. Under this lens, many works suggest that the inner space is shared across layers, meaning that tokens can be decoded at early stages. Mechanistic interpretability even goes further by conjecturing that some layers act as refinement layers. Following this path, we propose inference-time inner looping, which prolongs refinement in pretrained off-the-shelf language models by repeatedly re-applying a selected block range. Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses of the resulting latent trajectories suggest more stable state evolution and continued semantic refinement. Overall, our results suggest that additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models.",
      "authors": [
        "Jonathan Lys",
        "Vincent Gripon",
        "Bastien Pasdeloup",
        "Lukas Mauch",
        "Fabien Cardinaux",
        "Ghouthi Boukli Hacene"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16 14:04:24+00:00",
      "link": "https://arxiv.org/pdf/2602.14759v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14757v1",
      "title": "Solving Inverse Parametrized Problems via Finite Elements and Extreme Learning Networks",
      "abstract": "We develop an interpolation-based reduced-order modeling framework for parameter-dependent partial differential equations arising in control, inverse problems, and uncertainty quantification. The solution is discretized in the physical domain using finite element methods, while the dependence on a finite-dimensional parameter is approximated separately. We establish existence, uniqueness, and regularity of the parametric solution and derive rigorous error estimates that explicitly quantify the interplay between spatial discretization and parameter approximation.   In low-dimensional parameter spaces, classical interpolation schemes yield algebraic convergence rates based on Sobolev regularity in the parameter variable. In higher-dimensional parameter spaces, we replace classical interpolation by extreme learning machine (ELM) surrogates and obtain error bounds under explicit approximation and stability assumptions. The proposed framework is applied to inverse problems in quantitative photoacoustic tomography, where we derive potential and parameter reconstruction error estimates and demonstrate substantial computational savings compared to standard approaches, without sacrificing accuracy.",
      "authors": [
        "Erik Burman",
        "Mats G. Larson",
        "Karl Larsson",
        "Jonatan Vallin"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA",
        "cs.LG"
      ],
      "published": "2026-02-16 14:01:50+00:00",
      "link": "https://arxiv.org/pdf/2602.14757v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14755v1",
      "title": "Measuring the relatedness between scientific publications using controlled vocabularies",
      "abstract": "Measuring the relatedness between scientific publications is essential in many areas of bibliometrics and science policy. Controlled vocabularies provide a promising basis for measuring relatedness and are widely used in combination with Salton's cosine similarity. The latter is problematic because it only considers exact matches between terms. This article introduces two alternative methods - soft cosine and maximum term similarities - that account for the semantic similarity between non-matching terms. The article compares the accuracy of all three methods using the assignment of publications to topics in the TREC 2006 Genomics Track and the assumption that accurate relatedness measures should assign high relatedness scores to publication pairs within the same topic and low scores to pairs from separate topics. Results show that soft cosine is the most accurate method, while the most widely used version of Salton's cosine is markedly less accurate than the other methods tested. These findings have implications for how controlled vocabularies should be used to measure relatedness.",
      "authors": [
        "Emil Dolmer Alnor"
      ],
      "primary_category": "cs.DL",
      "categories": [
        "cs.DL",
        "cs.IR",
        "cs.SI"
      ],
      "published": "2026-02-16 13:58:47+00:00",
      "link": "https://arxiv.org/pdf/2602.14755v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14748v1",
      "title": "Constant-Time Dynamic Enumeration of Word Infixes in a Regular Language",
      "abstract": "For a fixed regular language $L$, the enumeration of $L$-infixes is the following task: we are given an input word $w = a_1 \\cdots a_n$ and we must enumerate the infixes of $w$ that belong to $L$, i.e., the pairs $i \\leq j$ such that $a_i \\cdots a_j \\in L$. We are interested in dynamic enumeration of $L$-infixes, where we must additionally support letter substitution updates on $w$ (e.g., \"replace the $i$-th letter of $w$ by a letter $a$\"). Each update changes the set of infixes to enumerate, and resets the enumeration state.   We study for which regular languages $L$ we can perform dynamic enumeration of $L$-infixes in constant delay (i.e., the next infix is always produced in constant time) and constant additional memory throughout the enumeration, while supporting each update in constant time.   We show that, for languages $L$ with a neutral letter, if the language $L$ belongs to the class ZG and is extensible (i.e., if $u \\in L$ and $u$ is a factor of $v$ then $v \\in L$), then dynamic enumeration of $L$-infixes can be achieved with a simple algorithm that ensures constant-time updates and constant delay, but not constant additional memory. Our main contribution is then to show an algorithm that additionally uses only constant additional memory, and applies to a more general class of semi-extensible ZG languages for which we give several equivalent characterizations. We further discuss whether our results can be generalized to larger language classes and show some (conditional) lower bounds.",
      "authors": [
        "Antoine Amarilli",
        "Sven Dziadek",
        "Luc Segoufin"
      ],
      "primary_category": "cs.FL",
      "categories": [
        "cs.FL",
        "cs.DS"
      ],
      "published": "2026-02-16 13:47:18+00:00",
      "link": "https://arxiv.org/pdf/2602.14748v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14743v1",
      "title": "LLMStructBench: Benchmarking Large Language Model Structured Data Extraction",
      "abstract": "We present LLMStructBench, a novel benchmark for evaluating Large Language Models (LLMs) on extracting structured data and generating valid JavaScript Object Notation (JSON) outputs from natural-language text. Our open dataset comprises diverse, manually verified parsing scenarios of varying complexity and enables systematic testing across 22 models and five prompting strategies. We further introduce complementary performance metrics that capture both token-level accuracy and document-level validity, facilitating rigorous comparison of model, size, and prompting effects on parsing reliability.   In particular, we show that choosing the right prompting strategy is more important than standard attributes such as model size. This especially ensures structural validity for smaller or less reliable models but increase the number of semantic errors. Our benchmark suite is an step towards future research in the area of LLM applied to parsing or Extract, Transform and Load (ETL) applications.",
      "authors": [
        "Sönke Tenckhoff",
        "Mario Koddenbrock",
        "Erik Rodner"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-16 13:37:58+00:00",
      "link": "https://arxiv.org/pdf/2602.14743v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14737v1",
      "title": "Parameter-Minimal Neural DE Solvers via Horner Polynomials",
      "abstract": "We propose a parameter-minimal neural architecture for solving differential equations by restricting the hypothesis class to Horner-factorized polynomials, yielding an implicit, differentiable trial solution with only a small set of learnable coefficients. Initial conditions are enforced exactly by construction by fixing the low-order polynomial degrees of freedom, so training focuses solely on matching the differential-equation residual at collocation points. To reduce approximation error without abandoning the low-parameter regime, we introduce a piecewise (\"spline-like\") extension that trains multiple small Horner models on subintervals while enforcing continuity (and first-derivative continuity) at segment boundaries. On illustrative ODE benchmarks and a heat-equation example, Horner networks with tens (or fewer) parameters accurately match the solution and its derivatives and outperform small MLP and sinusoidal-representation baselines under the same training settings, demonstrating a practical accuracy-parameter trade-off for resource-efficient scientific modeling.",
      "authors": [
        "T. Matulić",
        "D. Seršić"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "published": "2026-02-16 13:29:38+00:00",
      "link": "https://arxiv.org/pdf/2602.14737v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14733v1",
      "title": "More than Decision Support: Exploring Patients' Longitudinal Usage of Large Language Models in Real-World Healthcare-Seeking Journeys",
      "abstract": "Large language models (LLMs) have been increasingly adopted to support patients' healthcare-seeking in recent years. While prior patient-centered studies have examined the capabilities and experience of LLM-based tools in specific health-related tasks such as information-seeking, diagnosis, or decision-supporting, the inherently longitudinal nature of healthcare in real-world practice has been underexplored. This paper presents a four-week diary study with 25 patients to examine LLMs' roles across healthcare-seeking trajectories. Our analysis reveals that patients integrate LLMs not just as simple decision-support tools, but as dynamic companions that scaffold their journey across behavioral, informational, emotional, and cognitive levels. Meanwhile, patients actively assign diverse socio-technical meanings to LLMs, altering the traditional dynamics of agency, trust, and power in patient-provider relationships. Drawing from these findings, we conceptualize future LLMs as a longitudinal boundary companion that continuously mediates between patients and clinicians throughout longitudinal healthcare-seeking trajectories.",
      "authors": [
        "Yancheng Cao",
        "Yishu Ji",
        "Chris Yue Fu",
        "Sahiti Dharmavaram",
        "Meghan Turchioe",
        "Natalie C Benda",
        "Lena Mamykina",
        "Yuling Sun",
        "Xuhai \"Orson\" Xu"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-16 13:24:35+00:00",
      "link": "https://arxiv.org/pdf/2602.14733v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14729v1",
      "title": "Scale redundancy and soft gauge fixing in positively homogeneous neural networks",
      "abstract": "Neural networks with positively homogeneous activations exhibit an exact continuous reparametrization symmetry: neuron-wise rescalings generate parameter-space orbits along which the input--output function is invariant. We interpret this symmetry as a gauge redundancy and introduce gauge-adapted coordinates that separate invariant and scale-imbalance directions. Inspired by gauge fixing in field theory, we introduce a soft orbit-selection (norm-balancing) functional acting only on redundant scale coordinates. We show analytically that it induces dissipative relaxation of imbalance modes to preserve the realized function. In controlled experiments, this orbit-selection penalty expands the stable learning-rate regime and suppresses scale drift without changing expressivity. These results establish a structural link between gauge-orbit geometry and optimization conditioning, providing a concrete connection between gauge-theoretic concepts and machine learning.",
      "authors": [
        "Rodrigo Carmo Terin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16 13:21:49+00:00",
      "link": "https://arxiv.org/pdf/2602.14729v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14728v1",
      "title": "D2-LoRA: A Synergistic Approach to Differential and Directional Low-Rank Adaptation",
      "abstract": "We systematically investigate the parameter-efficient fine-tuning design space under practical data and compute constraints, and propose D2-LoRA. D2-LoRA achieves 76.4 percent average accuracy across eight question answering and reading comprehension benchmarks using only 5k training samples per task and two epochs, while preserving algebraic mergeability at inference with near-exact numerical equivalence. The method combines signed low-rank residual updates with additive and subtractive components, together with a train-time column-wise projection that keeps each column close to its original norm. After training, the adapter is merged into a single weight matrix, adding zero inference latency. Compared with LoRA, D2-LoRA improves average accuracy by 2.2 percentage points; at matched parameter counts (LoRA rank 2r versus D2-LoRA rank r), the improvement is 1.6 points, indicating gains from architectural design rather than increased parameterization. Compared with DoRA, it matches or exceeds performance on most tasks. Beyond QA and reading comprehension, D2-LoRA improves generative tasks (plus 1.2 ROUGE-L and plus 1.1 percent win rate) and shows 36 percent lower training volatility. The merge preserves numerical fidelity (mean gap about 0.03 percentage points) and recovers about 1.91x evaluation throughput. Training overhead is 19 percent, comparable to DoRA, and decreases with longer input sequences. We provide a geometric analysis explaining how the projection stabilizes training, together with ablation studies isolating the contribution of each design component.",
      "authors": [
        "Nozomu Fujisawa",
        "Masaaki Kondo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16 13:19:42+00:00",
      "link": "https://arxiv.org/pdf/2602.14728v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14717v1",
      "title": "Optimal Program Synthesis via Abstract Interpretation",
      "abstract": "We consider the problem of synthesizing programs with numerical constants that optimize a quantitative objective, such as accuracy, over a set of input-output examples. We propose a general framework for optimal synthesis of such programs in a given domain specific language (DSL), with provable optimality guarantees. Our framework enumerates programs in a general search graph, where nodes represent subsets of concrete programs. To improve scalability, it uses A* search in conjunction with a search heuristic based on abstract interpretation; intuitively, this heuristic establishes upper bounds on the value of subtrees in the search graph, enabling the synthesizer to identify and prune subtrees that are provably suboptimal. In addition, we propose a natural strategy for constructing abstract transformers for monotonic semantics, which is a common property for components in DSLs for data classification. Finally, we implement our approach in the context of two such existing DSLs, demonstrating that our algorithm is more scalable than existing optimal synthesizers.",
      "authors": [
        "Stephen Mell",
        "Steve Zdancewic",
        "Osbert Bastani"
      ],
      "primary_category": "cs.PL",
      "categories": [
        "cs.PL"
      ],
      "published": "2026-02-16 13:02:53+00:00",
      "link": "https://arxiv.org/pdf/2602.14717v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14710v1",
      "title": "Orcheo: A Modular Full-Stack Platform for Conversational Search",
      "abstract": "Conversational search (CS) requires a complex software engineering pipeline that integrates query reformulation, ranking, and response generation. CS researchers currently face two barriers: the lack of a unified framework for efficiently sharing contributions with the community, and the difficulty of deploying end-to-end prototypes needed for user evaluation. We introduce Orcheo, an open-source platform designed to bridge this gap. Orcheo offers three key advantages: (i) A modular architecture promotes component reuse through single-file node modules, facilitating sharing and reproducibility in CS research; (ii) Production-ready infrastructure bridges the prototype-to-system gap via dual execution modes, secure credential management, and execution telemetry, with built-in AI coding support that lowers the learning curve; (iii) Starter-kit assets include 50+ off-the-shelf components for query understanding, ranking, and response generation, enabling the rapid bootstrapping of complete CS pipelines. We describe the framework architecture and validate Orcheo's utility through case studies that highlight modularity and ease of use. Orcheo is released as open source under the MIT License at https://github.com/ShaojieJiang/orcheo.",
      "authors": [
        "Shaojie Jiang",
        "Svitlana Vakulenko",
        "Maarten de Rijke"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-16 12:56:57+00:00",
      "link": "https://arxiv.org/pdf/2602.14710v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14708v1",
      "title": "A Unified Mathematical Framework for Distributed Data Fabrics: Categorical Hypergraph Models",
      "abstract": "Current distributed data fabrics lack a rigorous mathematical foundation, often relying on ad-hoc architectures that struggle with consistency, lineage, and scale. We propose a mathematical framework for data fabrics, unifying heterogeneous data management in distributed systems through a hypergraph-based structure \\( \\mathcal{F} = (D, M, G, T, P, A) \\). Datasets, metadata, transformations, policies, and analytics are modeled over a distributed system \\( Σ= (N, C) \\), with multi-way relationships encoded in a hypergraph \\( G = (V, E) \\). A categorical approach, with datasets as objects and transformations as morphisms, supports operations like data integration and federated learning. The hypergraph is embedded into a modular tensor category, capturing relational symmetries via braided monoidal structures, with geometric analogies to Hurwitz spaces enriching the algebraic modeling. We prove the NP-hardness of critical tasks, such as schema matching and dynamic partitioning, and propose spectral methods and symmetry-based alignments for scalable solutions. The framework ensures consistency, completeness, and causality under CAP and CAL theorems, leveraging sparse incidence matrices and braiding actions for fault-tolerant operations.",
      "authors": [
        "T. Shaska",
        "I. Kotsireas"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "math.CT"
      ],
      "published": "2026-02-16 12:52:51+00:00",
      "link": "https://arxiv.org/pdf/2602.14708v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14701v1",
      "title": "Unbiased Approximate Vector-Jacobian Products for Efficient Backpropagation",
      "abstract": "In this work we introduce methods to reduce the computational and memory costs of training deep neural networks. Our approach consists in replacing exact vector-jacobian products by randomized, unbiased approximations thereof during backpropagation. We provide a theoretical analysis of the trade-off between the number of epochs needed to achieve a target precision and the cost reduction for each epoch. We then identify specific unbiased estimates of vector-jacobian products for which we establish desirable optimality properties of minimal variance under sparsity constraints. Finally we provide in-depth experiments on multi-layer perceptrons, BagNets and Visual Transfomers architectures. These validate our theoretical results, and confirm the potential of our proposed unbiased randomized backpropagation approach for reducing the cost of deep learning.",
      "authors": [
        "Killian Bakong",
        "Laurent Massoulié",
        "Edouard Oyallon",
        "Kevin Scaman"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-16 12:40:59+00:00",
      "link": "https://arxiv.org/pdf/2602.14701v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14697v1",
      "title": "Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs",
      "abstract": "Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL",
      "authors": [
        "Lunjun Zhang",
        "Ryan Chen",
        "Bradly C. Stadie"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-16 12:34:27+00:00",
      "link": "https://arxiv.org/pdf/2602.14697v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14696v1",
      "title": "A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)",
      "abstract": "Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.",
      "authors": [
        "Nihal V. Nayak",
        "Paula Rodriguez-Diaz",
        "Neha Hulkund",
        "Sara Beery",
        "David Alvarez-Melis"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16 12:33:05+00:00",
      "link": "https://arxiv.org/pdf/2602.14696v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14687v1",
      "title": "SynthSAEBench: Evaluating Sparse Autoencoders on Scalable Realistic Synthetic Data",
      "abstract": "Improving Sparse Autoencoders (SAEs) requires benchmarks that can precisely validate architectural innovations. However, current SAE benchmarks on LLMs are often too noisy to differentiate architectural improvements, and current synthetic data experiments are too small-scale and unrealistic to provide meaningful comparisons. We introduce SynthSAEBench, a toolkit for generating large-scale synthetic data with realistic feature characteristics including correlation, hierarchy, and superposition, and a standardized benchmark model, SynthSAEBench-16k, enabling direct comparison of SAE architectures. Our benchmark reproduces several previously observed LLM SAE phenomena, including the disconnect between reconstruction and latent quality metrics, poor SAE probing results, and a precision-recall trade-off mediated by L0. We further use our benchmark to identify a new failure mode: Matching Pursuit SAEs exploit superposition noise to improve reconstruction without learning ground-truth features, suggesting that more expressive encoders can easily overfit. SynthSAEBench complements LLM benchmarks by providing ground-truth features and controlled ablations, enabling researchers to precisely diagnose SAE failure modes and validate architectural improvements before scaling to LLMs.",
      "authors": [
        "David Chanin",
        "Adrià Garriga-Alonso"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16 12:22:00+00:00",
      "link": "https://arxiv.org/pdf/2602.14687v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14682v1",
      "title": "Exposing Diversity Bias in Deep Generative Models: Statistical Origins and Correction of Diversity Error",
      "abstract": "Deep generative models have achieved great success in producing high-quality samples, making them a central tool across machine learning applications. Beyond sample quality, an important yet less systematically studied question is whether trained generative models faithfully capture the diversity of the underlying data distribution. In this work, we address this question by directly comparing the diversity of samples generated by state-of-the-art models with that of test samples drawn from the target data distribution, using recently proposed reference-free entropy-based diversity scores, Vendi and RKE. Across multiple benchmark datasets, we find that test data consistently attains substantially higher Vendi and RKE diversity scores than the generated samples, suggesting a systematic downward diversity bias in modern generative models. To understand the origin of this bias, we analyze the finite-sample behavior of entropy-based diversity scores and show that their expected values increase with sample size, implying that diversity estimated from finite training sets could inherently underestimate the diversity of the true distribution. As a result, optimizing the generators to minimize divergence to empirical data distributions would induce a loss of diversity. Finally, we discuss potential diversity-aware regularization and guidance strategies based on Vendi and RKE as principled directions for mitigating this bias, and provide empirical evidence suggesting their potential to improve the results.",
      "authors": [
        "Farzan Farnia",
        "Mohammad Jalali",
        "Azim Ospanov"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "math.OC"
      ],
      "published": "2026-02-16 12:15:34+00:00",
      "link": "https://arxiv.org/pdf/2602.14682v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14677v1",
      "title": "Kernel-based optimization of measurement operators for quantum reservoir computers",
      "abstract": "Finding optimal measurement operators is crucial for the performance of quantum reservoir computers (QRCs), since they employ a fixed quantum feature map. We formulate the training of both stateless (quantum extreme learning machines, QELMs) and stateful (memory dependent) QRCs in the framework of kernel ridge regression. This approach renders an optimal measurement operator that minimizes prediction error for a given reservoir and training dataset. For large qubit numbers, this method is more efficient than the conventional training of QRCs. We discuss efficiency and practical implementation strategies, including Pauli basis decomposition and operator diagonalization, to adapt the optimal observable to hardware constraints. Numerical experiments on image classification and time series prediction tasks demonstrate the effectiveness of this approach, which can also be applied to other quantum ML models.",
      "authors": [
        "Markus Gross",
        "Hans-Martin Rieser"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "published": "2026-02-16 12:04:42+00:00",
      "link": "https://arxiv.org/pdf/2602.14677v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14675v1",
      "title": "Crowdsourcing Piedmontese to Test LLMs on Non-Standard Orthography",
      "abstract": "We present a crowdsourced dataset for Piedmontese, an endangered Romance language of northwestern Italy. The dataset comprises 145 Italian-Piedmontese parallel sentences derived from Flores+, with translations produced by speakers writing in their natural orthographic style rather than adhering to standardized conventions, along with manual word alignment. We use this resource to benchmark several large language models on tokenization parity, topic classification, and machine translation. Our analysis reveals that Piedmontese incurs a tokenization penalty relative to higher-resource Romance languages, yet LLMs achieve classification performance approaching that of Italian, French, and English. Machine translation results are asymmetric: models translate adequately from Piedmontese into high-resource languages, but generation into Piedmontese remains challenging. The dataset and code are publicly released.",
      "authors": [
        "Gianluca Vico",
        "Jindřich Libovický"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 12:02:29+00:00",
      "link": "https://arxiv.org/pdf/2602.14675v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14674v2",
      "title": "From User Preferences to Base Score Extraction Functions in Gradual Argumentation (with Appendix)",
      "abstract": "Gradual argumentation is a field of symbolic AI which is attracting attention for its ability to support transparent and contestable AI systems. It is considered a useful tool in domains such as decision-making, recommendation, debate analysis, and others. The outcomes in such domains are usually dependent on the arguments' base scores, which must be selected carefully. Often, this selection process requires user expertise and may not always be straightforward. On the other hand, organising the arguments by preference could simplify the task. In this work, we introduce \\emph{Base Score Extraction Functions}, which provide a mapping from users' preferences over arguments to base scores. These functions can be applied to the arguments of a \\emph{Bipolar Argumentation Framework} (BAF), supplemented with preferences, to obtain a \\emph{Quantitative Bipolar Argumentation Framework} (QBAF), allowing the use of well-established computational tools in gradual argumentation. We outline the desirable properties of base score extraction functions, discuss some design choices, and provide an algorithm for base score extraction. Our method incorporates an approximation of non-linearities in human preferences to allow for better approximation of the real ones. Finally, we evaluate our approach both theoretically and experimentally in a robotics setting, and offer recommendations for selecting appropriate gradual semantics in practice.",
      "authors": [
        "Aniol Civit",
        "Antonio Rago",
        "Antonio Andriella",
        "Guillem Alenyà",
        "Francesca Toni"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-16 12:01:58+00:00",
      "link": "https://arxiv.org/pdf/2602.14674v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14670v1",
      "title": "FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery",
      "abstract": "Formulaic alpha factor mining is a critical yet challenging task in quantitative investment, characterized by a vast search space and the need for domain-informed, interpretable signals. However, finding novel signals becomes increasingly difficult as the library grows due to high redundancy. We propose FactorMiner, a lightweight and flexible self-evolving agent framework designed to navigate this complex landscape through continuous knowledge accumulation. FactorMiner combines a Modular Skill Architecture that encapsulates systematic financial evaluation into executable tools with a structured Experience Memory that distills historical mining trials into actionable insights (successful patterns and failure constraints). By instantiating the Ralph Loop paradigm -- retrieve, generate, evaluate, and distill -- FactorMiner iteratively uses memory priors to guide exploration, reducing redundant search while focusing on promising directions. Experiments on multiple datasets across different assets and Markets show that FactorMiner constructs a diverse library of high-quality factors with competitive performance, while maintaining low redundancy among factors as the library scales. Overall, FactorMiner provides a practical approach to scalable discovery of interpretable formulaic alpha factors under the \"Correlation Red Sea\" constraint.",
      "authors": [
        "Yanlong Wang",
        "Jian Xu",
        "Hongkang Zhang",
        "Shao-Lun Huang",
        "Danny Dongning Sun",
        "Xiao-Ping Zhang"
      ],
      "primary_category": "q-fin.TR",
      "categories": [
        "q-fin.TR",
        "cs.MA"
      ],
      "published": "2026-02-16 11:48:52+00:00",
      "link": "https://arxiv.org/pdf/2602.14670v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14663v1",
      "title": "Pseudo-differential-enhanced physics-informed neural networks",
      "abstract": "We present pseudo-differential enhanced physics-informed neural networks (PINNs), an extension of gradient enhancement but in Fourier space. Gradient enhancement of PINNs dictates that the PDE residual is taken to a higher differential order than prescribed by the PDE, added to the objective as an augmented term in order to improve training and overall learning fidelity. We propose the same procedure after application via Fourier transforms, since differentiating in Fourier space is multiplication with the Fourier wavenumber under suitable decay. Our methods are fast and efficient. Our methods oftentimes achieve superior PINN versus numerical error in fewer training iterations, potentially pair well with few samples in collocation, and can on occasion break plateaus in low collocation settings. Moreover, our methods are suitable for fractional derivatives. We establish that our methods improve spectral eigenvalue decay of the neural tangent kernel (NTK), and so our methods contribute towards the learning of high frequencies in early training, mitigating the effects of frequency bias up to the polynomial order and possibly greater with smooth activations. Our methods accommodate advanced techniques in PINNs, such as Fourier feature embeddings. A pitfall of discrete Fourier transforms via the Fast Fourier Transform (FFT) is mesh subjugation, and so we demonstrate compatibility of our methods for greater mesh flexibility and invariance on alternative Euclidean and non-Euclidean domains via Monte Carlo methods and otherwise.",
      "authors": [
        "Andrew Gracyk"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-02-16 11:40:58+00:00",
      "link": "https://arxiv.org/pdf/2602.14663v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14656v1",
      "title": "An Embarrassingly Simple Way to Optimize Orthogonal Matrices at Scale",
      "abstract": "Orthogonality constraints are ubiquitous in robust and probabilistic machine learning. Unfortunately, current optimizers are computationally expensive and do not scale to problems with hundreds or thousands of constraints. One notable exception is the Landing algorithm (Ablin et al., 2024) which, however comes at the expense of temporarily relaxing orthogonality. In this work, we revisit and improve on the ideas behind Landing, enabling the inclusion of modern adaptive optimizers while ensuring that orthogonal constraints are effectively met. Remarkably, these improvements come at little to no cost, and reduce the number of required hyperparemeters. Our algorithm POGO is fast and GPU-friendly, consisting of only 5 matrix products, and in practice maintains orthogonality at all times. On several challenging benchmarks, POGO greatly outperforms recent optimizers and shows it can optimize problems with thousands of orthogonal matrices in minutes while alternatives would take hours. As such, POGO sets a milestone to finally exploit orthogonality constraints in ML at scale. A PyTorch implementation of POGO is publicly available at https://github.com/adrianjav/pogo.",
      "authors": [
        "Adrián Javaloy",
        "Antonio Vergari"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.DG",
        "math.OC"
      ],
      "published": "2026-02-16 11:27:04+00:00",
      "link": "https://arxiv.org/pdf/2602.14656v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14655v1",
      "title": "Breaking Data Efficiency Dilemma: A Federated and Augmented Learning Framework For Alzheimer's Disease Detection via Speech",
      "abstract": "Early diagnosis of Alzheimer's Disease (AD) is crucial for delaying its progression. While AI-based speech detection is non-invasive and cost-effective, it faces a critical data efficiency dilemma due to medical data scarcity and privacy barriers. Therefore, we propose FAL-AD, a novel framework that synergistically integrates federated learning with data augmentation to systematically optimize data efficiency. Our approach delivers three key breakthroughs: First, absolute efficiency improvement through voice conversion-based augmentation, which generates diverse pathological speech samples via cross-category voice-content recombination. Second, collaborative efficiency breakthrough via an adaptive federated learning paradigm, maximizing cross-institutional benefits under privacy constraints. Finally, representational efficiency optimization by an attentive cross-modal fusion model, which achieves fine-grained word-level alignment and acoustic-textual interaction. Evaluated on ADReSSo, FAL-AD achieves a state-of-the-art multi-modal accuracy of 91.52%, outperforming all centralized baselines and demonstrating a practical solution to the data efficiency dilemma. Our source code is publicly available at https://github.com/smileix/fal-ad.",
      "authors": [
        "Xiao Wei",
        "Bin Wen",
        "Yuqin Lin",
        "Kai Li",
        "Mingyang gu",
        "Xiaobao Wang",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-16 11:26:06+00:00",
      "link": "https://arxiv.org/pdf/2602.14655v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14643v2",
      "title": "Arbor: A Framework for Reliable Navigation of Critical Conversation Flows",
      "abstract": "Large language models struggle to maintain strict adherence to structured workflows in high-stakes domains such as healthcare triage. Monolithic approaches that encode entire decision structures within a single prompt are prone to instruction-following degradation as prompt length increases, including lost-in-the-middle effects and context window overflow. To address this gap, we present Arbor, a framework that decomposes decision tree navigation into specialized, node-level tasks. Decision trees are standardized into an edge-list representation and stored for dynamic retrieval. At runtime, a directed acyclic graph (DAG)-based orchestration mechanism iteratively retrieves only the outgoing edges of the current node, evaluates valid transitions via a dedicated LLM call, and delegates response generation to a separate inference step. The framework is agnostic to the underlying decision logic and model provider. Evaluated against single-prompt baselines across 10 foundation models using annotated turns from real clinical triage conversations. Arbor improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves an average 14.4x reduction in per-turn cost. These results indicate that architectural decomposition reduces dependence on intrinsic model capability, enabling smaller models to match or exceed larger models operating under single-prompt baselines.",
      "authors": [
        "Luís Silva",
        "Diogo Gonçalves",
        "Catarina Farinha",
        "Clara Matos",
        "Luís Ungaro"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-16 11:09:02+00:00",
      "link": "https://arxiv.org/pdf/2602.14643v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14642v1",
      "title": "GenPANIS: A Latent-Variable Generative Framework for Forward and Inverse PDE Problems in Multiphase Media",
      "abstract": "Inverse problems and inverse design in multiphase media, i.e., recovering or engineering microstructures to achieve target macroscopic responses, require operating on discrete-valued material fields, rendering the problem non-differentiable and incompatible with gradient-based methods. Existing approaches either relax to continuous approximations, compromising physical fidelity, or employ separate heavyweight models for forward and inverse tasks. We propose GenPANIS, a unified generative framework that preserves exact discrete microstructures while enabling gradient-based inference through continuous latent embeddings. The model learns a joint distribution over microstructures and PDE solutions, supporting bidirectional inference (forward prediction and inverse recovery) within a single architecture. The generative formulation enables training with unlabeled data, physics residuals, and minimal labeled pairs. A physics-aware decoder incorporating a differentiable coarse-grained PDE solver preserves governing equation structure, enabling extrapolation to varying boundary conditions and microstructural statistics. A learnable normalizing flow prior captures complex posterior structure for inverse problems. Demonstrated on Darcy flow and Helmholtz equations, GenPANIS maintains accuracy on challenging extrapolative scenarios - including unseen boundary conditions, volume fractions, and microstructural morphologies, with sparse, noisy observations. It outperforms state-of-the-art methods while using 10 - 100 times fewer parameters and providing principled uncertainty quantification.",
      "authors": [
        "Matthaios Chatzopoulos",
        "Phaedon-Stelios Koutsourelakis"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-16 11:08:30+00:00",
      "link": "https://arxiv.org/pdf/2602.14642v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14641v1",
      "title": "Quantum Reservoir Computing with Neutral Atoms on a Small, Complex, Medical Dataset",
      "abstract": "Biomarker-based prediction of clinical outcomes is challenging due to nonlinear relationships, correlated features, and the limited size of many medical datasets. Classical machine-learning methods can struggle under these conditions, motivating the search for alternatives. In this work, we investigate quantum reservoir computing (QRC), using both noiseless emulation and hardware execution on the neutral-atom Rydberg processor \\textit{Aquila}. We evaluate performance with six classical machine-learning models and use SHAP to generate feature subsets. We find that models trained on emulated quantum features achieve mean test accuracies comparable to those trained on classical features, but have higher training accuracies and greater variability over data splits, consistent with overfitting. When comparing hardware execution of QRC to noiseless emulation, the models are more robust over different data splits and often exhibit statistically significant improvements in mean test accuracy. This combination of improved accuracy and increased stability is suggestive of a regularising effect induced by hardware execution. To investigate the origin of this behaviour, we examine the statistical differences between hardware and emulated quantum feature distributions. We find that hardware execution applies a structured, time-dependent transformation characterised by compression toward the mean and a progressive reduction in mutual information relative to emulation.",
      "authors": [
        "Luke Antoncich",
        "Yuben Moodley",
        "Ugo Varetto",
        "Jingbo Wang",
        "Jonathan Wurtz",
        "Jing Chen",
        "Pascal Jahan Elahi",
        "Casey R. Myers"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "published": "2026-02-16 11:03:31+00:00",
      "link": "https://arxiv.org/pdf/2602.14641v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14635v1",
      "title": "Alignment Adapter to Improve the Performance of Compressed Deep Learning Models",
      "abstract": "Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.",
      "authors": [
        "Rohit Raj Rai",
        "Abhishek Dhaka",
        "Amit Awekar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-02-16 10:53:02+00:00",
      "link": "https://arxiv.org/pdf/2602.14635v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14626v1",
      "title": "Concepts' Information Bottleneck Models",
      "abstract": "Concept Bottleneck Models (CBMs) aim to deliver interpretable predictions by routing decisions through a human-understandable concept layer, yet they often suffer reduced accuracy and concept leakage that undermines faithfulness. We introduce an explicit Information Bottleneck regularizer on the concept layer that penalizes $I(X;C)$ while preserving task-relevant information in $I(C;Y)$, encouraging minimal-sufficient concept representations. We derive two practical variants (a variational objective and an entropy-based surrogate) and integrate them into standard CBM training without architectural changes or additional supervision. Evaluated across six CBM families and three benchmarks, the IB-regularized models consistently outperform their vanilla counterparts. Information-plane analyses further corroborate the intended behavior. These results indicate that enforcing a minimal-sufficient concept bottleneck improves both predictive performance and the reliability of concept-level interventions. The proposed regularizer offers a theoretic-grounded, architecture-agnostic path to more faithful and intervenable CBMs, resolving prior evaluation inconsistencies by aligning training protocols and demonstrating robust gains across model families and datasets.",
      "authors": [
        "Karim Galliamov",
        "Syed M Ahsan Kazmi",
        "Adil Khan",
        "Adín Ramírez Rivera"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16 10:33:20+00:00",
      "link": "https://arxiv.org/pdf/2602.14626v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14622v2",
      "title": "Tabular Foundation Models Can Learn Association Rules",
      "abstract": "Association Rule Mining (ARM) is a fundamental task for knowledge discovery in tabular data and is widely used in high-stakes decision-making. Classical ARM methods rely on frequent itemset mining, leading to rule explosion and poor scalability, while recent neural approaches mitigate these issues but suffer from degraded performance in low-data regimes. Tabular foundation models (TFMs), pretrained on diverse tabular data with strong in-context generalization, provide a basis for addressing these limitations. We introduce a model-agnostic association rule learning framework that extracts association rules from any conditional probabilistic model over tabular data, enabling us to leverage TFMs. We then introduce TabProbe, an instantiation of our framework that utilizes TFMs as conditional probability estimators to learn association rules out-of-the-box without frequent itemset mining. We evaluate our approach on tabular datasets of varying sizes based on standard ARM rule quality metrics and downstream classification performance. The results show that TFMs consistently produce concise, high-quality association rules with strong predictive performance and remain robust in low-data settings without task-specific training. Source code is available at https://github.com/DiTEC-project/tabprobe.",
      "authors": [
        "Erkan Karabulut",
        "Daniel Daza",
        "Paul Groth",
        "Martijn C. Schut",
        "Victoria Degeler"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "published": "2026-02-16 10:25:55+00:00",
      "link": "https://arxiv.org/pdf/2602.14622v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14612v1",
      "title": "LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio",
      "abstract": "Long-duration audio is increasingly common in industrial and consumer settings, yet reviewing multi-hour recordings is impractical, motivating systems that answer natural-language queries with precise temporal grounding and minimal hallucination. Existing audio-language models show promise, but long-audio question answering remains difficult due to context-length limits. We introduce LongAudio-RAG (LA-RAG), a hybrid framework that grounds Large Language Model (LLM) outputs in retrieved, timestamped acoustic event detections rather than raw audio. Multi-hour streams are converted into structured event records stored in an SQL database, and at inference time the system resolves natural-language time references, classifies intent, retrieves only the relevant events, and generates answers using this constrained evidence. To evaluate performance, we construct a synthetic long-audio benchmark by concatenating recordings with preserved timestamps and generating template-based question-answer pairs for detection, counting, and summarization tasks. Finally, we demonstrate the practicality of our approach by deploying it in a hybrid edge-cloud environment, where the audio grounding model runs on-device on IoT-class hardware while the LLM is hosted on a GPU-backed server. This architecture enables low-latency event extraction at the edge and high-quality language reasoning in the cloud. Experiments show that structured, event-level retrieval significantly improves accuracy compared to vanilla Retrieval-Augmented Generation (RAG) or text-to-SQL approaches.",
      "authors": [
        "Naveen Vakada",
        "Kartik Hegde",
        "Arvind Krishna Sridhar",
        "Yinyi Guo",
        "Erik Visser"
      ],
      "primary_category": "eess.AS",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-16 10:15:22+00:00",
      "link": "https://arxiv.org/pdf/2602.14612v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14607v1",
      "title": "A Bayesian Approach to Low-Discrepancy Subset Selection",
      "abstract": "Low-discrepancy designs play a central role in quasi-Monte Carlo methods and are increasingly influential in other domains such as machine learning, robotics and computer graphics, to name a few. In recent years, one such low-discrepancy construction method called subset selection has received a lot of attention. Given a large population, one optimally selects a small low-discrepancy subset with respect to a discrepancy-based objective. Versions of this problem are known to be NP-hard. In this text, we establish, for the first time, that the subset selection problem with respect to kernel discrepancies is also NP-hard. Motivated by this intractability, we propose a Bayesian Optimization procedure for the subset selection problem utilizing the recent notion of deep embedding kernels. We demonstrate the performance of the BO algorithm to minimize discrepancy measures and note that the framework is broadly applicable any design criteria.",
      "authors": [
        "Nathan Kirk"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "cs.LG",
        "math.NA",
        "stat.CO"
      ],
      "published": "2026-02-16 10:11:07+00:00",
      "link": "https://arxiv.org/pdf/2602.14607v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14594v1",
      "title": "The Wikidata Query Logs Dataset",
      "abstract": "We present the Wikidata Query Logs (WDQL) dataset, a dataset consisting of 200k question-query pairs over the Wikidata knowledge graph. It is over 6x larger than the largest existing Wikidata datasets of similar format without relying on template-generated queries. Instead, we construct it using real-world SPARQL queries sent to the Wikidata Query Service and generate questions for them. Since these log-based queries are anonymized, and therefore often do not produce results, a significant amount of effort is needed to convert them back into meaningful SPARQL queries. To achieve this, we present an agent-based method that iteratively de-anonymizes, cleans, and verifies queries against Wikidata while also generating corresponding natural-language questions. We demonstrate the dataset's benefit for training question-answering methods. All WDQL assets, as well as the agent code, are publicly available under a permissive license.",
      "authors": [
        "Sebastian Walter",
        "Hannah Bast"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 09:49:44+00:00",
      "link": "https://arxiv.org/pdf/2602.14594v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14592v1",
      "title": "FO and MSO Model Checking on Temporal Graphs",
      "abstract": "Algorithmic meta-theorems provide an important tool for showing tractability of graph problems on graph classes defined by structural restrictions. While such results are well established for static graphs, corresponding frameworks for temporal graphs are comparatively limited.   In this work, we revisit past applications of logical meta-theorems to temporal graphs and develop an extended unifying logical framework. Our first contribution is the introduction of logical encodings for the parameters vertex-interval-membership (VIM) width and tree-interval-membership (TIM) width, parameters which capture the signature of vertex and component activity over time. Building on this, we extend existing monadic second-order (MSO) meta-theorems for bounded lifetime and temporal degree to the parameters VIM and TIM width, and establish novel first-order (FO) meta-theorems for all four parameters.   Finally, we signpost a modular lexicon of reusable FO and MSO formulas for a broad range of temporal graph problems, and give an example. This lexicon allows new problems to be expressed compositionally and directly yields fixed-parameter tractability results across the four parameters we consider.",
      "authors": [
        "Michelle Döring",
        "Jessica Enright",
        "Laura Larios-Jones",
        "George Skretas"
      ],
      "primary_category": "cs.DM",
      "categories": [
        "cs.DM",
        "cs.DS"
      ],
      "published": "2026-02-16 09:48:24+00:00",
      "link": "https://arxiv.org/pdf/2602.14592v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14580v1",
      "title": "Replicable Constrained Bandits",
      "abstract": "Algorithmic \\emph{replicability} has recently been introduced to address the need for reproducible experiments in machine learning. A \\emph{replicable online learning} algorithm is one that takes the same sequence of decisions across different executions in the same environment, with high probability. We initiate the study of algorithmic replicability in \\emph{constrained} MAB problems, where a learner interacts with an unknown stochastic environment for $T$ rounds, seeking not only to maximize reward but also to satisfy multiple constraints. Our main result is that replicability can be achieved in constrained MABs. Specifically, we design replicable algorithms whose regret and constraint violation match those of non-replicable ones in terms of $T$. As a key step toward these guarantees, we develop the first replicable UCB-like algorithm for \\emph{unconstrained} MABs, showing that algorithms that employ the optimism in-the-face-of-uncertainty principle can be replicable, a result that we believe is of independent interest.",
      "authors": [
        "Matteo Bollini",
        "Gianmarco Genalti",
        "Francesco Emanuele Stradi",
        "Matteo Castiglioni",
        "Alberto Marchesi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-16 09:22:23+00:00",
      "link": "https://arxiv.org/pdf/2602.14580v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14578v1",
      "title": "RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch",
      "abstract": "Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.",
      "authors": [
        "Isam Vrce",
        "Andreas Kassler",
        "Gökçe Aydos"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AR"
      ],
      "published": "2026-02-16 09:17:29+00:00",
      "link": "https://arxiv.org/pdf/2602.14578v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14573v1",
      "title": "Polar: An Algebraic Analyzer for (Probabilistic) Loops",
      "abstract": "We present the Polar framework for fully automating the analysis of classical and probabilistic loops using algebraic reasoning. The central theme in Polar comes with handling algebraic recurrences that precisely capture the loop semantics. To this end, our work implements a variety of techniques to compute exact closed-forms of recurrences over higher-order moments of variables, infer invariants, and derive loop sensitivities with respect to unknown parameters. Polar can analyze probabilistic loops containing if-statements, polynomial arithmetic, and common probability distributions. By translating loop analysis into linear recurrence solving, Polar uses the derived closed-forms of recurrences to compute the strongest polynomial invariant or to infer parameter sensitivity. Polar is both sound and complete within well-defined programming model restrictions. Lifting any of these restrictions results in significant hardness limits of computation. To overcome computational burdens for the sake of efficiency, Polar also provides incomplete but sound techniques to compute moments of combinations of variables.",
      "authors": [
        "Marcel Moosbrugger",
        "Julian Müllner",
        "Ezio Bartocci",
        "Laura Kovács"
      ],
      "primary_category": "cs.PL",
      "categories": [
        "cs.PL"
      ],
      "published": "2026-02-16 09:05:42+00:00",
      "link": "https://arxiv.org/pdf/2602.14573v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14564v1",
      "title": "Assessing Large Language Models for Medical QA: Zero-Shot and LLM-as-a-Judge Evaluation",
      "abstract": "Recently, Large Language Models (LLMs) have gained significant traction in medical domain, especially in developing a QA systems to Medical QA systems for enhancing access to healthcare in low-resourced settings. This paper compares five LLMs deployed between April 2024 and August 2025 for medical QA, using the iCliniq dataset, containing 38,000 medical questions and answers of diverse specialties. Our models include Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B Instruct, Llama-4-Maverick-17B-128E-Instruct, and GPT-5-mini. We are using a zero-shot evaluation methodology and using BLEU and ROUGE metrics to evaluate performance without specialized fine-tuning. Our results show that larger models like Llama 3.3 70B Instruct outperform smaller models, consistent with observed scaling benefits in clinical tasks. It is notable that, Llama-4-Maverick-17B exhibited more competitive results, thus highlighting evasion efficiency trade-offs relevant for practical deployment. These findings align with advancements in LLM capabilities toward professional-level medical reasoning and reflect the increasing feasibility of LLM-supported QA systems in the real clinical environments. This benchmark aims to serve as a standardized setting for future study to minimize model size, computational resources and to maximize clinical utility in medical NLP applications.",
      "authors": [
        "Shefayat E Shams Adib",
        "Ahmed Alfey Sani",
        "Ekramul Alam Esham",
        "Ajwad Abrar",
        "Tareque Mohmud Chowdhury"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 08:53:23+00:00",
      "link": "https://arxiv.org/pdf/2602.14564v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14536v1",
      "title": "Explainable Token-level Noise Filtering for LLM Fine-tuning Datasets",
      "abstract": "Large Language Models (LLMs) have seen remarkable advancements, achieving state-of-the-art results in diverse applications. Fine-tuning, an important step for adapting LLMs to specific downstream tasks, typically involves further training on corresponding datasets. However, a fundamental discrepancy exists between current fine-tuning datasets and the token-level optimization mechanism of LLMs: most datasets are designed at the sentence-level, which introduces token-level noise, causing negative influence to final performance. In this paper, we propose XTF, an explainable token-level noise filtering framework. XTF decomposes the complex and subtle contributions of token-level data to the fine-tuning process into three distinct and explicit attributes (reasoning importance, knowledge novelty, and task relevance), which can be assessed using scoring methods, and then masks the gradients of selected noisy tokens accordingly to optimize the performance of fine-tuned LLMs. We conduct extensive experiments on three representative downstream tasks (math, code and medicine) across 7 mainstream LLMs. The results demonstrate that XTF can significantly improve downstream performance by up to 13.7% compared to regular fine-tuning. Our work highlights the importance of token-level dataset optimization, and demonstrates the potential of strategies based on attribute decomposition for explaining complex training mechanisms.",
      "authors": [
        "Yuchen Yang",
        "Wenze Lin",
        "Enhao Huang",
        "Zhixuan Chu",
        "Hongbin Zhou",
        "Lan Tao",
        "Yiming Li",
        "Zhan Qin",
        "Kui Ren"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-16 07:49:33+00:00",
      "link": "https://arxiv.org/pdf/2602.14536v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15078v1",
      "title": "Computer Science as Infrastructure: the Spine of the Lean Computer Science Library (CSLib)",
      "abstract": "Following in the footsteps of the success of Mathlib - the centralised library of formalised mathematics in Lean - CSLib is a rapidly-growing centralised library of formalised computer science and software. In this paper, we present its founding technical principles, operation, abstractions, and semantic framework. We contribute reusable semantic interfaces (reduction and labelled transition systems), proof automation, CI/testing support for maintaining automation and compatibility with Mathlib, and the first substantial developments of languages and models.",
      "authors": [
        "Christopher Henson",
        "Fabrizio Montesi"
      ],
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO",
        "cs.PL"
      ],
      "published": "2026-02-16 07:41:16+00:00",
      "link": "https://arxiv.org/pdf/2602.15078v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14519v1",
      "title": "DeepMTL2R: A Library for Deep Multi-task Learning to Rank",
      "abstract": "This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attention mechanism of transformer architectures, enabling effective learning across diverse and potentially conflicting objectives. The framework includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models. By capturing complex dependencies and long-range interactions among items and labels, DeepMTL2R provides a scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies. We demonstrate its effectiveness on a publicly available dataset, report competitive performance, and visualize the resulting trade-offs among objectives. DeepMTL2R is available at \\href{https://github.com/amazon-science/DeepMTL2R}{https://github.com/amazon-science/DeepMTL2R}.",
      "authors": [
        "Chaosheng Dong",
        "Peiyao Xiao",
        "Yijia Wang",
        "Kaiyi Ji"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "published": "2026-02-16 07:11:38+00:00",
      "link": "https://arxiv.org/pdf/2602.14519v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14518v1",
      "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
      "abstract": "Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conflict. Through probing internal representations, we reveal that: (I) Linear Separability: different conflict types are explicitly encoded as linearly separable features rather than entangled; (II) Depth Localization: conflict signals concentrate in mid-to-late layers, indicating a distinct processing stage for conflict encoding; (III) Hierarchical Consistency: aggregating noisy token-level signals along trajectories robustly recovers input-level conflict types; and (IV) Directional Asymmetry: reinforcing the model's implicit source preference under conflict is far easier than enforcing the opposite source. Our findings provide a mechanism-level view of multimodal reasoning under knowledge conflict and enable principled diagnosis and control of long-CoT failures.",
      "authors": [
        "Jing Tang",
        "Kun Wang",
        "Haolang Lu",
        "Hongjin Chen",
        "KaiTao Chen",
        "Zhongxiang Sun",
        "Qiankun Li",
        "Lingjuan Lyu",
        "Guoshun Nan",
        "Zhigang Zeng"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-16 07:10:44+00:00",
      "link": "https://arxiv.org/pdf/2602.14518v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14517v1",
      "title": "Beyond Translation: Evaluating Mathematical Reasoning Capabilities of LLMs in Sinhala and Tamil",
      "abstract": "Large language models (LLMs) demonstrate strong mathematical reasoning in English, but whether these capabilities reflect genuine multilingual reasoning or reliance on translation-based processing in low-resource languages like Sinhala and Tamil remains unclear. We examine this fundamental question by evaluating whether LLMs genuinely reason mathematically in these languages or depend on implicit translation to English-like representations. Using a taxonomy of six math problem types, from basic arithmetic to complex unit conflict and optimization problems, we evaluate four prominent large language models. To avoid translation artifacts that confound language ability with translation quality, we construct a parallel dataset where each problem is natively authored by fluent speakers with mathematical training in all three languages. Our analysis demonstrates that while basic arithmetic reasoning transfers robustly across languages, complex reasoning tasks show significant degradation in Tamil and Sinhala. The pattern of failures varies by model and problem type, suggesting that apparent multilingual competence may not reflect uniform reasoning capabilities across languages. These findings challenge the common assumption that models exhibiting strong multilingual performance can reason equally effectively across languages, and highlight the need for fine-grained, type-aware evaluation in multilingual settings.",
      "authors": [
        "Sukumar Kishanthan",
        "Kumar Thushalika",
        "Buddhi Jayasekara",
        "Asela Hevapathige"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-16 07:08:37+00:00",
      "link": "https://arxiv.org/pdf/2602.14517v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14516v1",
      "title": "Efficient Multi-round LLM Inference over Disaggregated Serving",
      "abstract": "With the rapid evolution of Large Language Models (LLMs), multi-round workflows, such as autonomous agents and iterative retrieval, have become increasingly prevalent. However, this raises hurdles for serving LLMs under prefill-decode (PD) disaggregation, a widely adopted paradigm that separates the compute-bound prefill phase and memory-bound decode phase onto individual resources. Specifically, existing systems overlook the interleaved prefill-decode workload pattern in multi-round inference, leading to sub-optimal handling of the incremental prefill workloads and model deployment for the two phases.   In this work, we present AMPD, a brand new disaggregated serving framework for multi-round LLM inference. The core of AMPD is to coordinate the prefill workloads based on real-time workloads by adaptively determining where to carry out these workloads and how they are scheduled, in order to maximize service level objective (SLO) attainment. In addition, we tailor a planning algorithm for our scenario, facilitating the deduction of optimal resource allocation and parallel strategies for the two phases. Empirical results demonstrate that AMPD substantially improves SLO attainment compared to state-of-the-art baselines.",
      "authors": [
        "Wenhao He",
        "Youhe Jiang",
        "Penghao Zhao",
        "Quanqing Xu",
        "Eiko Yoneki",
        "Bin Cui",
        "Fangcheng Fu"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC"
      ],
      "published": "2026-02-16 07:07:30+00:00",
      "link": "https://arxiv.org/pdf/2602.14516v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14506v1",
      "title": "Covariance-Aware Transformers for Quadratic Programming and Decision Making",
      "abstract": "We explore the use of transformers for solving quadratic programs and how this capability benefits decision-making problems that involve covariance matrices. We first show that the linear attention mechanism can provably solve unconstrained QPs by tokenizing the matrix variables (e.g.~$A$ of the objective $\\frac{1}{2}x^\\top Ax+b^\\top x$) row-by-row and emulating gradient descent iterations. Furthermore, by incorporating MLPs, a transformer block can solve (i) $\\ell_1$-penalized QPs by emulating iterative soft-thresholding and (ii) $\\ell_1$-constrained QPs when equipped with an additional feedback loop. Our theory motivates us to introduce Time2Decide: a generic method that enhances a time series foundation model (TSFM) by explicitly feeding the covariance matrix between the variates. We empirically find that Time2Decide uniformly outperforms the base TSFM model for the classical portfolio optimization problem that admits an $\\ell_1$-constrained QP formulation. Remarkably, Time2Decide also outperforms the classical \"Predict-then-Optimize (PtO)\" procedure, where we first forecast the returns and then explicitly solve a constrained QP, in suitable settings. Our results demonstrate that transformers benefit from explicit use of second-order statistics, and this can enable them to effectively solve complex decision-making problems, like portfolio construction, in one forward pass.",
      "authors": [
        "Kutay Tire",
        "Yufan Zhang",
        "Ege Onur Taga",
        "Samet Oymak"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16 06:39:24+00:00",
      "link": "https://arxiv.org/pdf/2602.14506v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14503v1",
      "title": "Bounding Probabilities of Causation with Partial Causal Diagrams",
      "abstract": "Probabilities of causation are fundamental to individual-level explanation and decision making, yet they are inherently counterfactual and not point-identifiable from data in general. Existing bounds either disregard available covariates, require complete causal graphs, or rely on restrictive binary settings, limiting their practical use. In real-world applications, causal information is often partial but nontrivial. This paper proposes a general framework for bounding probabilities of causation using partial causal information. We show how the available structural or statistical information can be systematically incorporated as constraints in a optimization programming formulation, yielding tighter and formally valid bounds without full identifiability. This approach extends the applicability of probabilities of causation to realistic settings where causal knowledge is incomplete but informative.",
      "authors": [
        "Yuxuan Xie",
        "Ang Li"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-16 06:35:24+00:00",
      "link": "https://arxiv.org/pdf/2602.14503v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14502v1",
      "title": "Behavioral Feature Boosting via Substitute Relationships for E-commerce Search",
      "abstract": "On E-commerce platforms, new products often suffer from the cold-start problem: limited interaction data reduces their search visibility and hurts relevance ranking. To address this, we propose a simple yet effective behavior feature boosting method that leverages substitute relationships among products (BFS). BFS identifies substitutes-products that satisfy similar user needs-and aggregates their behavioral signals (e.g., clicks, add-to-carts, purchases, and ratings) to provide a warm start for new items. Incorporating these enriched signals into ranking models mitigates cold-start effects and improves relevance and competitiveness. Experiments on a large E-commerce platform, both offline and online, show that BFS significantly improves search relevance and product discovery for cold-start products. BFS is scalable and practical, improving user experience while increasing exposure for newly launched items in E-commerce search. The BFS-enhanced ranking model has been launched in production and has served customers since 2025.",
      "authors": [
        "Chaosheng Dong",
        "Michinari Momma",
        "Yijia Wang",
        "Yan Gao",
        "Yi Sun"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-16 06:35:05+00:00",
      "link": "https://arxiv.org/pdf/2602.14502v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14492v2",
      "title": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model",
      "abstract": "Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.",
      "authors": [
        "Jiahao Yuan",
        "Yike Xu",
        "Jinyong Wen",
        "Baokun Wang",
        "Ziyi Gao",
        "Xiaotong Lin",
        "Yun Liu",
        "Xing Fu",
        "Yu Cheng",
        "Yongchao Liu",
        "Weiqiang Wang",
        "Zhongle Xie"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-02-16 06:09:31+00:00",
      "link": "https://arxiv.org/pdf/2602.14492v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14488v1",
      "title": "BETA-Labeling for Multilingual Dataset Construction in Low-Resource IR",
      "abstract": "IR in low-resource languages remains limited by the scarcity of high-quality, task-specific annotated datasets. Manual annotation is expensive and difficult to scale, while using large language models (LLMs) as automated annotators introduces concerns about label reliability, bias, and evaluation validity. This work presents a Bangla IR dataset constructed using a BETA-labeling framework involving multiple LLM annotators from diverse model families. The framework incorporates contextual alignment, consistency checks, and majority agreement, followed by human evaluation to verify label quality. Beyond dataset creation, we examine whether IR datasets from other low-resource languages can be effectively reused through one-hop machine translation. Using LLM-based translation across multiple language pairs, we experimented on meaning preservation and task validity between source and translated datasets. Our experiment reveal substantial variation across languages, reflecting language-dependent biases and inconsistent semantic preservation that directly affect the reliability of cross-lingual dataset reuse. Overall, this study highlights both the potential and limitations of LLM-assisted dataset creation for low-resource IR. It provides empirical evidence of the risks associated with cross-lingual dataset reuse and offers practical guidance for constructing more reliable benchmarks and evaluation pipelines in low-resource language settings.",
      "authors": [
        "Md. Najib Hasan",
        "Mst. Jannatun Ferdous Rain",
        "Fyad Mohammed",
        "Nazmul Siddique"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-16 06:04:04+00:00",
      "link": "https://arxiv.org/pdf/2602.14488v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14482v1",
      "title": "TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning",
      "abstract": "We address fine-grained visual reasoning in multimodal large language models (MLLMs), where key evidence may reside in tiny objects, cluttered regions, or subtle markings that are lost under a single global image encoding. We introduce TikArt (Thinking Aperture), an aperture-guided agent that casts multi-step vision-language reasoning as a decision process over regions of interest. TikArt follows a Think-Aperture-Observe loop, alternating between language generation and two aperture actions: Zoom extracts rectangular crops, while Segment invokes SAM2 to obtain mask-based crops for irregular targets. After every action, the model must produce an explicit observation, turning local visual cues into persistent linguistic memory. Built on Qwen3-VL-8B, TikArt optimizes its reasoning policy with AGRPO, a GRPO-style reinforcement learning algorithm with a two-stage curriculum: it warms up segmentation actions and then jointly optimizes visual math, fine-grained VQA, and segmentation, using rewards that couple task success with purposeful aperture use. Experiments on V*, HR-Bench-4K/8K, MME-RealWorld-Lite, MMStar, RefCOCO, and ReasonSeg show consistent gains over the backbone and yield interpretable aperture trajectories for high-resolution reasoning.",
      "authors": [
        "Hao Ding",
        "Zhichuan Yang",
        "Weijie Ge",
        "Ziqin Gao",
        "Chaoyi Lu",
        "Lei Zhao"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-16 05:46:47+00:00",
      "link": "https://arxiv.org/pdf/2602.14482v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14476v1",
      "title": "Truthful Reverse Auctions for Adaptive Selection via Contextual Multi-Armed Bandits",
      "abstract": "We study the problem of selecting large language models (LLMs) for user queries in settings where multiple LLM providers submit the cost of solving a query. From the users' perspective, choosing an optimal model is a sequential, query-dependent decision problem: high-capacity models offer more reliable outputs but are costlier, while lightweight models are faster and cheaper. We formalize this interaction as a reverse auction design problem with contextual online learning, where the user adaptively discovers which model performs best while eliciting costs from competing LLM providers. Existing multi-armed bandit (MAB) mechanisms focus on forward auctions and social welfare, leaving open the challenges of reverse auctions, provider-optimal outcomes, and contextual adaptation. We address these gaps by designing a resampling-based procedure that generalizes truthful forward MAB mechanisms to reverse auctions and prove that any monotone allocation rule with this procedure is truthful. Using this, we propose a contextual MAB algorithm that learns query-dependent model quality with sublinear regret. Our framework unifies mechanism design and adaptive learning, enabling efficient, truthful, and query-aware LLM selection.",
      "authors": [
        "Pronoy Patra",
        "Sankarshan Damle",
        "Manisha Padala",
        "Sujit Gujar"
      ],
      "primary_category": "cs.GT",
      "categories": [
        "cs.GT"
      ],
      "published": "2026-02-16 05:27:48+00:00",
      "link": "https://arxiv.org/pdf/2602.14476v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14472v1",
      "title": "Frequentist Regret Analysis of Gaussian Process Thompson Sampling via Fractional Posteriors",
      "abstract": "We study Gaussian Process Thompson Sampling (GP-TS) for sequential decision-making over compact, continuous action spaces and provide a frequentist regret analysis based on fractional Gaussian process posteriors, without relying on domain discretization as in prior work. We show that the variance inflation commonly assumed in existing analyses of GP-TS can be interpreted as Thompson Sampling with respect to a fractional posterior with tempering parameter $α\\in (0,1)$. We derive a kernel-agnostic regret bound expressed in terms of the information gain parameter $γ_t$ and the posterior contraction rate $ε_t$, and identify conditions on the Gaussian process prior under which $ε_t$ can be controlled. As special cases of our general bound, we recover regret of order $\\tilde{\\mathcal{O}}(T^{\\frac{1}{2}})$ for the squared exponential kernel, $\\tilde{\\mathcal{O}}(T^{\\frac{2ν+3d}{2(2ν+d)}} )$ for the Matérn-$ν$ kernel, and a bound of order $\\tilde{\\mathcal{O}}(T^{\\frac{2ν+3d}{2(2ν+d)}})$ for the rational quadratic kernel. Overall, our analysis provides a unified and discretization-free regret framework for GP-TS that applies broadly across kernel classes.",
      "authors": [
        "Somjit Roy",
        "Prateek Jaiswal",
        "Anirban Bhattacharya",
        "Debdeep Pati",
        "Bani K. Mallick"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "published": "2026-02-16 05:18:13+00:00",
      "link": "https://arxiv.org/pdf/2602.14472v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14470v1",
      "title": "HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation",
      "abstract": "Graph-based retrieval-augmented generation (RAG) methods, typically built on knowledge graphs (KGs) with binary relational facts, have shown promise in multi-hop open-domain QA. However, their rigid retrieval schemes and dense similarity search often introduce irrelevant context, increase computational overhead, and limit relational expressiveness. In contrast, n-ary hypergraphs encode higher-order relational facts that capture richer inter-entity dependencies and enable shallower, more efficient reasoning paths. To address this limitation, we propose HyperRAG, a RAG framework tailored for n-ary hypergraphs with two complementary retrieval variants: (i) HyperRetriever learns structural-semantic reasoning over n-ary facts to construct query-conditioned relational chains. It enables accurate factual tracking, adaptive high-order traversal, and interpretable multi-hop reasoning under context constraints. (ii) HyperMemory leverages the LLM's parametric memory to guide beam search, dynamically scoring n-ary facts and entities for query-aware path expansion. Extensive evaluations on WikiTopics (11 closed-domain datasets) and three open-domain QA benchmarks (HotpotQA, MuSiQue, and 2WikiMultiHopQA) validate HyperRAG's effectiveness. HyperRetriever achieves the highest answer accuracy overall, with average gains of 2.95% in MRR and 1.23% in Hits@10 over the strongest baseline. Qualitative analysis further shows that HyperRetriever bridges reasoning gaps through adaptive and interpretable n-ary chain construction, benefiting both open and closed-domain QA.",
      "authors": [
        "Wen-Sheng Lien",
        "Yu-Kai Chan",
        "Hao-Lung Hsiao",
        "Bo-Kai Ruan",
        "Meng-Fen Chiang",
        "Chien-An Chen",
        "Yi-Ren Yeh",
        "Hong-Han Shuai"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 05:15:55+00:00",
      "link": "https://arxiv.org/pdf/2602.14470v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14469v1",
      "title": "Measuring and Mitigating Post-hoc Rationalization in Reverse Chain-of-Thought Generation",
      "abstract": "Reverse Chain-of-Thought Generation (RCG) synthesizes reasoning traces from query-answer pairs, but runs the risk of producing post-hoc rationalizations: when models can see the answer during generation, the answer serves as a cognitive anchor that shapes the entire explanation. We formalize this phenomenon through a three-level measurement hierarchy: lexical, entropic, and probabilistic anchoring, each captures surface artifacts, entropy dynamics, and latent answer dependence, respectively. We analyze semantic suppression, the intuitive mitigation strategy that instructs models to ignore the answer, to find out its counterproduction: while it reduces lexical overlap, it paradoxically increases entropic and probabilistic anchoring. Drawing on Ironic Process Theory from cognitive psychology, we attribute this failure to active monitoring of the forbidden answer, which inadvertently deepens dependence on it. To break this cycle, we propose Structural Skeleton-guided Reasoning (SSR), a two-phase approach that first generates an answer-invariant functional skeleton structure, then uses this skeleton to guide full trace generation. By redirecting the information flow to structural planning rather than answer monitoring, SSR consistently reduces anchoring across all three levels. We further introduce Distilled SSR (SSR-D), which fine-tunes models on teacher-generated SSR traces to ensure reliable structural adherence. Experiments across open-ended reasoning benchmarks demonstrate that SSR-D achieves up to 10% improvement over suppression baselines while preserving out-of-distribution (OOD) generalization.",
      "authors": [
        "Guangyue Peng",
        "Zongchao Chen",
        "Wen Luo",
        "Yuntao Wen",
        "Wei Li",
        "Ruixiang Feng",
        "Ran Le",
        "Chen Yang",
        "Zhenwei An",
        "Yang Song",
        "Tao Zhang",
        "Houfeng Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 05:13:06+00:00",
      "link": "https://arxiv.org/pdf/2602.14469v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14467v1",
      "title": "Conversational Decision Support for Information Search Under Uncertainty: Effects of Gist and Verbatim Feedback",
      "abstract": "Many real-world decisions rely on information search, where people sample evidence and decide when to stop under uncertainty. The uncertainty in the environment, particularly how diagnostic evidence is distributed, causes complexities in information search, further leading to suboptimal decision-making outcomes. Yet AI decision support often targets outcome optimization, and less is known about how to scaffold search without increasing cognitive load. We introduce SERA, an LLM-based assistant that provides either gist or verbatim feedback during search. Across two experiments (N1=54, N2=54), we examined decision-making outcomes and information search in SERA-Gist, SERA-Verbatim, and a no-feedback baseline across three environments varying in uncertainty. The uncertainty in environment is operationalized by the perceived gain of information across the course of sampling, which individuals may experience diminishing return of information gain (decremental; low-uncertainty), or a local drop of information gain (local optimum; medium-uncertainty), or no patterns in information gain (high-uncertainty), as they search more. Individuals show more accurate decision outcomes and are more confident with SERA support, especially under higher uncertainty. Gist feedback was associated with more efficient integration and showed a descriptive pattern of reduced oversampling, while verbatim feedback promoted more extensive exploration. These findings establish feedback representation as a design lever when search matters, motivating adaptive systems that match feedback granularity to uncertainty.",
      "authors": [
        "Kexin Quan",
        "Jessie Chin"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-16 05:09:34+00:00",
      "link": "https://arxiv.org/pdf/2602.14467v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14466v1",
      "title": "Robust Bias Evaluation with FilBBQ: A Filipino Bias Benchmark for Question-Answering Language Models",
      "abstract": "With natural language generation becoming a popular use case for language models, the Bias Benchmark for Question-Answering (BBQ) has grown to be an important benchmark format for evaluating stereotypical associations exhibited by generative models. We expand the linguistic scope of BBQ and construct FilBBQ through a four-phase development process consisting of template categorization, culturally aware translation, new template construction, and prompt generation. These processes resulted in a bias test composed of more than 10,000 prompts which assess whether models demonstrate sexist and homophobic prejudices relevant to the Philippine context. We then apply FilBBQ on models trained in Filipino but do so with a robust evaluation protocol that improves upon the reliability and accuracy of previous BBQ implementations. Specifically, we account for models' response instability by obtaining prompt responses across multiple seeds and averaging the bias scores calculated from these distinctly seeded runs. Our results confirm both the variability of bias scores across different seeds and the presence of sexist and homophobic biases relating to emotion, domesticity, stereotyped queer interests, and polygamy. FilBBQ is available via GitHub.",
      "authors": [
        "Lance Calvin Lim Gamboa",
        "Yue Feng",
        "Mark Lee"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 05:03:15+00:00",
      "link": "https://arxiv.org/pdf/2602.14466v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14462v1",
      "title": "Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment",
      "abstract": "Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of worker-level optimization dynamics before gradient aggregation. This paper identifies and studies this latent mismatch, termed \\emph{silent inconsistency}, where cross-worker divergence in losses and gradients can remain invisible under conventional aggregated monitoring signals. We propose a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Specifically, we introduce three complementary metrics: loss dispersion, gradient-norm dispersion, and gradient-direction consistency measured by inter-worker cosine similarity. The proposed metrics incur negligible overhead and require no modification to model architecture, synchronization mechanisms, or optimization algorithms. We validate the framework by fully fine-tuning the 1B-parameter \\texttt{openPangu-Embedded-1B-V1.1} model on the \\texttt{tatsu-lab/alpaca} dataset using an 8-NPU DP setup, under controlled perturbations of cross-rank stochasticity. Experimental results show that progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. These findings demonstrate that the proposed indicators provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment.",
      "authors": [
        "Hong Li",
        "Zhen Zhou",
        "Honggang Zhang",
        "Yuping Luo",
        "Xinyue Wang",
        "Han Gong",
        "Zhiyuan Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16 04:42:30+00:00",
      "link": "https://arxiv.org/pdf/2602.14462v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14456v1",
      "title": "Traceable Latent Variable Discovery Based on Multi-Agent Collaboration",
      "abstract": "Revealing the underlying causal mechanisms in the real world is crucial for scientific and technological progress. Despite notable advances in recent decades, the lack of high-quality data and the reliance of traditional causal discovery algorithms (TCDA) on the assumption of no latent confounders, as well as their tendency to overlook the precise semantics of latent variables, have long been major obstacles to the broader application of causal discovery. To address this issue, we propose a novel causal modeling framework, TLVD, which integrates the metadata-based reasoning capabilities of large language models (LLMs) with the data-driven modeling capabilities of TCDA for inferring latent variables and their semantics. Specifically, we first employ a data-driven approach to construct a causal graph that incorporates latent variables. Then, we employ multi-LLM collaboration for latent variable inference, modeling this process as a game with incomplete information and seeking its Bayesian Nash Equilibrium (BNE) to infer the possible specific latent variables. Finally, to validate the inferred latent variables across multiple real-world web-based data sources, we leverage LLMs for evidence exploration to ensure traceability. We comprehensively evaluate TLVD on three de-identified real patient datasets provided by a hospital and two benchmark datasets. Extensive experimental results confirm the effectiveness and reliability of TLVD, with average improvements of 32.67% in Acc, 62.21% in CAcc, and 26.72% in ECit across the five datasets.",
      "authors": [
        "Huaming Du",
        "Tao Hu",
        "Yijie Huang",
        "Yu Zhao",
        "Guisong Liu",
        "Tao Gu",
        "Gang Kou",
        "Carl Yang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16 04:29:32+00:00",
      "link": "https://arxiv.org/pdf/2602.14456v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14452v1",
      "title": "WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity",
      "abstract": "Large Language Models (LLMs) offer strong capabilities but incur high inference costs due to dense computation and memory access. Training-free activation sparsity is a promising approach for efficient LLM inference, yet existing methods often rely solely on activation information and uniform sparsity ratios. This overlooks the critical interplay with weights and inter-block sensitivity variation, leading to suboptimal performance. We identify two key phenomena in modern LLMs: 1) less significant activations may align with highly important weights, and 2) sparsity sensitivity varies non-monotonically across model blocks. We propose Weight-aware Mixed-Granularity Training-free Activation Sparsity (WiSparse), which leverages both activation and weight information for adaptive sparsity allocation. Specifically, we introduce a weight-aware mechanism integrating activation magnitudes with precomputed weight norms to accurately identify salient channels. This is combined with a mixed-granularity allocation scheme: a global budget is distributed across blocks via evolutionary search to protect sensitive regions, then refined within blocks to minimize reconstruction error. We improve sparse kernels and demonstrate effectiveness on three representative models. Notably, at 50% sparsity, WiSparse preserves 97% of Llama3.1's dense performance, surpassing the strongest baseline by 2.23 percentage points while achieving a 21.4% acceleration in end-to-end inference speed. Our research advances the limits of training-free approaches for efficient LLM inference, pushing the boundaries of achievable speedup without training.",
      "authors": [
        "Lei Chen",
        "Yuan Meng",
        "Xiaoyu Zhan",
        "Zhi Wang",
        "Wenwu Zhu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16 04:18:36+00:00",
      "link": "https://arxiv.org/pdf/2602.14452v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14451v1",
      "title": "Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning",
      "abstract": "Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.",
      "authors": [
        "Qianyue Wang",
        "Jinwu Hu",
        "Huanxiang Lin",
        "Bolin Chen",
        "Zhiquan Wen",
        "Yaofo Chen",
        "Yu Rong",
        "Mingkui Tan"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-16 04:17:46+00:00",
      "link": "https://arxiv.org/pdf/2602.14451v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14445v1",
      "title": "Selective Synchronization Attention",
      "abstract": "The Transformer architecture has become the foundation of modern deep learning, yet its core self-attention mechanism suffers from quadratic computational complexity and lacks grounding in biological neural computation. We propose Selective Synchronization Attention (SSA), a novel attention mechanism that replaces the standard dot-product self-attention with a closed-form operator derived from the steady-state solution of the Kuramoto model of coupled oscillators. In SSA, each token is represented as an oscillator characterized by a learnable natural frequency and phase; the synchronization strength between token pairs, determined by a frequency-dependent coupling and phase-locking condition, serves as the attention weight. This formulation provides three key advantages: (i) natural sparsity arising from the phase-locking threshold, whereby tokens with incompatible frequencies automatically receive zero attention weight without explicit masking; (ii) unified positional-semantic encoding through the natural frequency spectrum, eliminating the need for separate positional encodings; and (iii) a single-pass, closed-form computation that avoids iterative ODE integration, with all components (coupling, order parameter, synchronization) derived from the oscillatory framework. We instantiate SSA within the Oscillatory Synchronization Network (OSN), a drop-in replacement for the Transformer block. Analysis of the synchronization matrices reveals non-uniform, head-diverse coupling patterns even at initialization, demonstrating a stronger architectural inductive bias than the approximately uniform attention produced by randomly initialized Transformers.",
      "authors": [
        "Hasi Hays"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "published": "2026-02-16 03:58:12+00:00",
      "link": "https://arxiv.org/pdf/2602.14445v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15074v1",
      "title": "Structure-Aware Piano Accompaniment via Style Planning and Dataset-Aligned Pattern Retrieval",
      "abstract": "We introduce a structure-aware approach for symbolic piano accompaniment that decouples high-level planning from note-level realization. A lightweight transformer predicts an interpretable, per-measure style plan conditioned on section/phrase structure and functional harmony, and a retriever then selects and reharmonizes human-performed piano patterns from a corpus. We formulate retrieval as pattern matching under an explicit energy with terms for harmonic feasibility, structural-role compatibility, voice-leading continuity, style preferences, and repetition control. Given a structured lead sheet and optional keyword prompts, the system generates piano-accompaniment MIDI. In our experiments, transformer style-planner-guided retrieval produces diverse long-form accompaniments with strong style realization. We further analyze planner ablations and quantify inter-style isolation. Experimental results demonstrate the effectiveness of our inference-time approach for piano accompaniment generation.",
      "authors": [
        "Wanyu Zang",
        "Yang Yu",
        "Meng Yu"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "published": "2026-02-16 03:54:34+00:00",
      "link": "https://arxiv.org/pdf/2602.15074v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14441v1",
      "title": "D-SECURE: Dual-Source Evidence Combination for Unified Reasoning in Misinformation Detection",
      "abstract": "Multimodal misinformation increasingly mixes realistic im-age edits with fluent but misleading text, producing persuasive posts that are difficult to verify. Existing systems usually rely on a single evidence source. Content-based detectors identify local inconsistencies within an image and its caption but cannot determine global factual truth. Retrieval-based fact-checkers reason over external evidence but treat inputs as coarse claims and often miss subtle visual or textual manipulations. This separation creates failure cases where internally consistent fabrications bypass manipulation detectors and fact-checkers verify claims that contain pixel-level or token-level corruption. We present D-SECURE, a framework that combines internal manipulation detection with external evidence-based reasoning for news-style posts. D-SECURE integrates the HAMMER manipulation detector with the DEFAME retrieval pipeline. DEFAME performs broad verification, and HAMMER analyses residual or uncertain cases that may contain fine-grained edits. Experiments on DGM4 and ClaimReview samples highlight the complementary strengths of both systems and motivate their fusion. We provide a unified, explainable report that incorporates manipulation cues and external evidence.",
      "authors": [
        "Gagandeep Singh",
        "Samudi Amarasinghe",
        "Priyanka Singh"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-16 03:51:49+00:00",
      "link": "https://arxiv.org/pdf/2602.14441v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14440v1",
      "title": "CAIRO: Decoupling Order from Scale in Regression",
      "abstract": "Standard regression methods typically optimize a single pointwise objective, such as mean squared error, which conflates the learning of ordering with the learning of scale. This coupling renders models vulnerable to outliers and heavy-tailed noise. We propose CAIRO (Calibrate After Initial Rank Ordering), a framework that decouples regression into two distinct stages. In the first stage, we learn a scoring function by minimizing a scale-invariant ranking loss; in the second, we recover the target scale via isotonic regression. We theoretically characterize a class of \"Optimal-in-Rank-Order\" objectives -- including variants of RankNet and Gini covariance -- and prove that they recover the ordering of the true conditional mean under mild assumptions. We further show that subsequent monotone calibration guarantees recovery of the true regression function. Empirically, CAIRO combines the representation learning of neural networks with the robustness of rank-based statistics. It matches the performance of state-of-the-art tree ensembles on tabular benchmarks and significantly outperforms standard regression objectives in regimes with heavy-tailed or heteroskedastic noise.",
      "authors": [
        "Harri Vanhems",
        "Yue Zhao",
        "Peng Shi",
        "Archer Y. Yang"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-16 03:50:05+00:00",
      "link": "https://arxiv.org/pdf/2602.14440v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14433v1",
      "title": "Synthetic Reader Panels: Tournament-Based Ideation with LLM Personas for Autonomous Publishing",
      "abstract": "We present a system for autonomous book ideation that replaces human focus groups with synthetic reader panels -- diverse collections of LLM-instantiated reader personas that evaluate book concepts through structured tournament competitions. Each persona is defined by demographic attributes (age group, gender, income, education, reading level), behavioral patterns (books per year, genre preferences, discovery methods, price sensitivity), and consistency parameters. Panels are composed per imprint to reflect target demographics, with diversity constraints ensuring representation across age, reading level, and genre affinity. Book concepts compete in single-elimination, double-elimination, round-robin, or Swiss-system tournaments, judged against weighted criteria including market appeal, originality, and execution potential. To reject low-quality LLM evaluations, we implement five automated anti-slop checks (repetitive phrasing, generic framing, circular reasoning, score clustering, audience mismatch). We report results from deployment within a multi-imprint publishing operation managing 6 active imprints and 609 titles in distribution. Three case studies -- a 270-evaluator panel for a children's literacy novel, and two 5-person expert panels for a military memoir and a naval strategy monograph -- demonstrate that synthetic panels produce actionable demographic segmentation, identify structural content issues invisible to homogeneous reviewers, and enable tournament filtering that eliminates low-quality concepts while enriching high-quality survivors from 15% to 62% of the evaluated pool.",
      "authors": [
        "Fred Zimmerman"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "published": "2026-02-16 03:44:54+00:00",
      "link": "https://arxiv.org/pdf/2602.14433v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14432v1",
      "title": "S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations",
      "abstract": "Activation outliers in large-scale transformer models pose a fundamental challenge to model quantization, creating excessively large ranges that cause severe accuracy drops during quantization. We empirically observe that outlier severity intensifies with pre-training scale (e.g., progressing from CLIP to the more extensively trained SigLIP and SigLIP2). Through theoretical analysis as well as empirical correlation studies, we establish the direct link between these activation outliers and dominant singular values of the weights. Building on this insight, we propose Selective Spectral Decay ($S^2D$), a geometrically-principled conditioning method that surgically regularizes only the weight components corresponding to the largest singular values during fine-tuning. Through extensive experiments, we demonstrate that $S^2D$ significantly reduces activation outliers and produces well-conditioned representations that are inherently quantization-friendly. Models trained with $S^2D$ achieve up to 7% improved PTQ accuracy on ImageNet under W4A4 quantization and 4% gains when combined with QAT. These improvements also generalize across downstream tasks and vision-language models, enabling the scaling of increasingly large and rigorously trained models without sacrificing deployment efficiency.",
      "authors": [
        "Arnav Chavan",
        "Nahush Lele",
        "Udbhav Bamba",
        "Sankalp Dayal",
        "Aditi Raghunathan",
        "Deepak Gupta"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-02-16 03:41:06+00:00",
      "link": "https://arxiv.org/pdf/2602.14432v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14430v1",
      "title": "A unified framework for evaluating the robustness of machine-learning interpretability for prospect risking",
      "abstract": "In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of \"importance\" and \"relevance\" differ for different explanation strategies. Thus, grounding these ranked features using theoretically backed causal ideas of necessity and sufficiency can prove to be a more reliable and robust way to improve the trustworthiness of the concerned explanation strategies.We propose a unified framework to generate counterfactuals as well as quantify necessity and sufficiency and use these to perform a robustness evaluation of the explanations provided by LIME and SHAP on high dimensional structured prospect risking data. This robustness test gives us deeper insights into the models capabilities to handle erronous data and which XAI module works best in pair with which model for our dataset for hydorcarbon indication.",
      "authors": [
        "Prithwijit Chowdhury",
        "Ahmad Mustafa",
        "Mohit Prabhushankar",
        "Ghassan AlRegib"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16 03:32:10+00:00",
      "link": "https://arxiv.org/pdf/2602.14430v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14428v1",
      "title": "LLM-Guided Knowledge Distillation for Temporal Knowledge Graph Reasoning",
      "abstract": "Temporal knowledge graphs (TKGs) support reasoning over time-evolving facts, yet state-of-the-art models are often computationally heavy and costly to deploy. Existing compression and distillation techniques are largely designed for static graphs; directly applying them to temporal settings may overlook time-dependent interactions and lead to performance degradation. We propose an LLM-assisted distillation framework specifically designed for temporal knowledge graph reasoning. Beyond a conventional high-capacity temporal teacher, we incorporate a large language model as an auxiliary instructor to provide enriched supervision. The LLM supplies broad background knowledge and temporally informed signals, enabling a lightweight student to better model event dynamics without increasing inference-time complexity. Training is conducted by jointly optimizing supervised and distillation objectives, using a staged alignment strategy to progressively integrate guidance from both teachers. Extensive experiments on multiple public TKG benchmarks with diverse backbone architectures demonstrate that the proposed approach consistently improves link prediction performance over strong distillation baselines, while maintaining a compact and efficient student model. The results highlight the potential of large language models as effective teachers for transferring temporal reasoning capability to resource-efficient TKG systems.",
      "authors": [
        "Wang Xing",
        "Wei Song",
        "Siyu Lin",
        "Chen Wu",
        "Man Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 03:27:50+00:00",
      "link": "https://arxiv.org/pdf/2602.14428v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14425v1",
      "title": "Hierarchical Vision-Language Interaction for Facial Action Unit Detection",
      "abstract": "Facial Action Unit (AU) detection seeks to recognize subtle facial muscle activations as defined by the Facial Action Coding System (FACS). A primary challenge w.r.t AU detection is the effective learning of discriminative and generalizable AU representations under conditions of limited annotated data. To address this, we propose a Hierarchical Vision-language Interaction for AU Understanding (HiVA) method, which leverages textual AU descriptions as semantic priors to guide and enhance AU detection. Specifically, HiVA employs a large language model to generate diverse and contextually rich AU descriptions to strengthen language-based representation learning. To capture both fine-grained and holistic vision-language associations, HiVA introduces an AU-aware dynamic graph module that facilitates the learning of AU-specific visual representations. These features are further integrated within a hierarchical cross-modal attention architecture comprising two complementary mechanisms: Disentangled Dual Cross-Attention (DDCA), which establishes fine-grained, AU-specific interactions between visual and textual features, and Contextual Dual Cross-Attention (CDCA), which models global inter-AU dependencies. This collaborative, cross-modal learning paradigm enables HiVA to leverage multi-grained vision-based AU features in conjunction with refined language-based AU details, culminating in robust and semantically enriched AU detection capabilities. Extensive experiments show that HiVA consistently surpasses state-of-the-art approaches. Besides, qualitative analyses reveal that HiVA produces semantically meaningful activation patterns, highlighting its efficacy in learning robust and interpretable cross-modal correspondences for comprehensive facial behavior analysis.",
      "authors": [
        "Yong Li",
        "Yi Ren",
        "Yizhe Zhang",
        "Wenhua Zhang",
        "Tianyi Zhang",
        "Muyun Jiang",
        "Guo-Sen Xie",
        "Cuntai Guan"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-16 03:22:05+00:00",
      "link": "https://arxiv.org/pdf/2602.14425v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14423v1",
      "title": "The geometry of invariant learning: an information-theoretic analysis of data augmentation and generalization",
      "abstract": "Data augmentation is one of the most widely used techniques to improve generalization in modern machine learning, often justified by its ability to promote invariance to label-irrelevant transformations. However, its theoretical role remains only partially understood. In this work, we propose an information-theoretic framework that systematically accounts for the effect of augmentation on generalization and invariance learning. Our approach builds upon mutual information-based bounds, which relate the generalization gap to the amount of information a learning algorithm retains about its training data. We extend this framework by modeling the augmented distribution as a composition of the original data distribution with a distribution over transformations, which naturally induces an orbit-averaged loss function. Under mild sub-Gaussian assumptions on the loss function and the augmentation process, we derive a new generalization bound that decompose the expected generalization gap into three interpretable terms: (1) a distributional divergence between the original and augmented data, (2) a stability term measuring the algorithm dependence on training data, and (3) a sensitivity term capturing the effect of augmentation variability. To connect our bounds to the geometry of the augmentation group, we introduce the notion of group diameter, defined as the maximal perturbation that augmentations can induce in the input space. The group diameter provides a unified control parameter that bounds all three terms and highlights an intrinsic trade-off: small diameters preserve data fidelity but offer limited regularization, while large diameters enhance stability at the cost of increased bias and sensitivity. We validate our theoretical bounds with numerical experiments, demonstrating that it reliably tracks and predicts the behavior of the true generalization gap.",
      "authors": [
        "Abdelali Bouyahia",
        "Frédéric LeBlanc",
        "Mario Marchand"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-02-16 03:18:39+00:00",
      "link": "https://arxiv.org/pdf/2602.14423v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14406v1",
      "title": "TruthStance: An Annotated Dataset of Conversations on Truth Social",
      "abstract": "Argument mining and stance detection are central to understanding how opinions are formed and contested in online discourse. However, most publicly available resources focus on mainstream platforms such as Twitter and Reddit, leaving conversational structure on alt-tech platforms comparatively under-studied. We introduce TruthStance, a large-scale dataset of Truth Social conversation threads spanning 2023-2025, consisting of 24,378 posts and 523,360 comments with reply-tree structure preserved. We provide a human-annotated benchmark of 1,500 instances across argument mining and claim-based stance detection, including inter-annotator agreement, and use it to evaluate large language model (LLM) prompting strategies. Using the best-performing configuration, we release additional LLM-generated labels for 24,352 posts (argument presence) and 107,873 comments (stance to parent), enabling analysis of stance and argumentation patterns across depth, topics, and users. All code and data are released publicly.",
      "authors": [
        "Fathima Ameen",
        "Danielle Brown",
        "Manusha Malgareddy",
        "Amanul Haque"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-16 02:25:02+00:00",
      "link": "https://arxiv.org/pdf/2602.14406v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14404v1",
      "title": "Boule or Baguette? A Study on Task Topology, Length Generalization, and the Benefit of Reasoning Traces",
      "abstract": "Recent years have witnessed meteoric progress in reasoning models: neural networks that generate intermediate reasoning traces (RTs) before producing a final output. Despite the rapid advancement, our understanding of how RTs support reasoning, and the limits of this paradigm, remain incomplete. To promote greater clarity, we introduce PITA: a novel large-scale dataset of over 23 million statements in propositional logic and their corresponding proofs. As a benchmark for robust reasoning, we focus on length generalization: if a model is trained to determine truth or falsity on statements with proofs up to fixed length, how well does it generalize to statements requiring longer proofs? We propose notions of (1) task depth and (2) task breadth, which measure respectively (1) the number of steps required to solve an example from a task and (2) the number of unique examples across a task. We vary these quantities across subsets of PITA, and find that RT models generalize well on broad and shallow subsets, while deteriorating on narrow and deep subsets relative to non-RT baselines. To determine whether our results are idiosyncratic to PITA or indicative of general phenomena, we compare our results to a simple synthetic task based on syllogisms. Our resulting theory suggests fundamental scalings that limit how well RT models perform on deep tasks, and highlights their generalization strengths on broad tasks. Our findings overall identify fundamental benefits and limitations inherent in using reasoning traces.",
      "authors": [
        "William L. Tong",
        "Ege Cakar",
        "Cengiz Pehlevan"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "published": "2026-02-16 02:20:37+00:00",
      "link": "https://arxiv.org/pdf/2602.14404v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14401v1",
      "title": "pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI",
      "abstract": "Vision-Language Navigation VLN requires large-scale trajectory instruction data from private indoor environments, raising significant privacy concerns. Federated Learning FL mitigates this by keeping data on-device, but vanilla FL struggles under VLNs' extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal. This paper proposes pFedNavi, a structure-aware and dynamically adaptive personalized federated learning framework tailored for VLN. Our key idea is to personalize where it matters: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients, and performs fine-grained parameter fusion on the selected components (e.g., the encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization. We evaluate pFedNavi on two standard VLN benchmarks, R2R and RxR, using both ResNet and CLIP visual representations. Across all metrics, pFedNavi consistently outperforms the FedAvg-based VLN baseline, achieving up to 7.5% improvement in navigation success rate and up to 7.8% gain in trajectory fidelity, while converging 1.38x faster under non-IID conditions.",
      "authors": [
        "Qingqian Yang",
        "Hao Wang",
        "Sai Qian Zhang",
        "Jian Li",
        "Yang Hua",
        "Miao Pan",
        "Tao Song",
        "Zhengwei Qi",
        "Haibing Guan"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-16 02:18:09+00:00",
      "link": "https://arxiv.org/pdf/2602.14401v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14386v1",
      "title": "Beyond Token-Level Policy Gradients for Complex Reasoning with Large Language Models",
      "abstract": "Existing policy-gradient methods for auto-regressive language models typically select subsequent tokens one at a time as actions in the policy. While effective for many generation tasks, such an approach may not fully capture the structure of complex reasoning tasks, where a single semantic decision is often realized across multiple tokens--for example, when defining variables or composing equations. This introduces a potential mismatch between token-level optimization and the inherently block-level nature of reasoning in these settings. To bridge this gap, we propose Multi-token Policy Gradient Optimization (MPO), a framework that treats sequences of K consecutive tokens as unified semantic actions. This block-level perspective enables our method to capture the compositional structure of reasoning trajectories and supports optimization over coherent, higher-level objectives. Experiments on mathematical reasoning and coding benchmarks show that MPO outperforms standard token-level policy gradient baselines, highlight the limitations of token-level policy gradients for complex reasoning, motivating future research to look beyond token-level granularity for reasoning-intensive language tasks.",
      "authors": [
        "Mufan Xu",
        "Kehai Chen",
        "Xuefeng Bai",
        "Zhengyu Niu",
        "Muyun Yang",
        "Tiejun Zhao",
        "Min Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 01:28:38+00:00",
      "link": "https://arxiv.org/pdf/2602.14386v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14384v1",
      "title": "M-CODE: Materials Categorization via Ontology, Dimensionality and Evolution",
      "abstract": "The rapid advancement of artificial intelligence in materials science requires data standards and data management practices that can capture the complexity of real-world structures, including surfaces, interfaces, defects, and dimensionality reduction. We present M-CODE - Materials Categorization via Ontology, Dimensionality and Evolution - a compact categorization system that links materials-science-specific terminology to a set of reusable concepts as building blocks and provenance-aware transformations. M-CODE classifies structures by dimensionality, structural complexity (from pristine to compound pristine, defective, and processed), and variants that capture common structure creation and evolution approaches. A practical implementation of the categorization is provided in an open-source codebase that includes JSON schemas, examples, and Python and TypeScript types/interfaces, designed to support reproducible dataset generation, validation, and community contributions.",
      "authors": [
        "Vsevolod Biryukov",
        "Kamal Choudhary",
        "Timur Bazhirov"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.DL",
        "physics.comp-ph"
      ],
      "published": "2026-02-16 01:18:15+00:00",
      "link": "https://arxiv.org/pdf/2602.14384v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14367v1",
      "title": "InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem",
      "abstract": "The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.",
      "authors": [
        "Shuofei Qiao",
        "Yunxiang Wei",
        "Xuehai Wang",
        "Bin Wu",
        "Boyang Xue",
        "Ningyu Zhang",
        "Hossein A. Rahmani",
        "Yanshan Wang",
        "Qiang Zhang",
        "Keyan Ding",
        "Jeff Z. Pan",
        "Huajun Chen",
        "Emine Yilmaz"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2026-02-16 00:40:31+00:00",
      "link": "https://arxiv.org/pdf/2602.14367v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14351v1",
      "title": "WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control",
      "abstract": "Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.",
      "authors": [
        "Mehran Aghabozorgi",
        "Alireza Moazeni",
        "Yanshu Zhang",
        "Ke Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-15 23:53:16+00:00",
      "link": "https://arxiv.org/pdf/2602.14351v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14344v1",
      "title": "Zero-Shot Instruction Following in RL via Structured LTL Representations",
      "abstract": "We study instruction following in multi-task reinforcement learning, where an agent must zero-shot execute novel tasks not seen during training. In this setting, linear temporal logic (LTL) has recently been adopted as a powerful framework for specifying structured, temporally extended tasks. While existing approaches successfully train generalist policies, they often struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications. In this work, we address these concerns with a novel approach to learn structured task representations that facilitate training and generalisation. Our method conditions the policy on sequences of Boolean formulae constructed from a finite automaton of the task. We propose a hierarchical neural architecture to encode the logical structure of these formulae, and introduce an attention mechanism that enables the policy to reason about future subgoals. Experiments in a variety of complex environments demonstrate the strong generalisation capabilities and superior performance of our approach.",
      "authors": [
        "Mathias Jackermeier",
        "Mattia Giuri",
        "Jacques Cloete",
        "Alessandro Abate"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-15 23:22:50+00:00",
      "link": "https://arxiv.org/pdf/2602.14344v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14342v1",
      "title": "High-accuracy log-concave sampling with stochastic queries",
      "abstract": "We show that high-accuracy guarantees for log-concave sampling -- that is, iteration and query complexities which scale as $\\mathrm{poly}\\log(1/δ)$, where $δ$ is the desired target accuracy -- are achievable using stochastic gradients with subexponential tails. Notably, this exhibits a separation with the problem of convex optimization, where stochasticity (even additive Gaussian noise) in the gradient oracle incurs $\\mathrm{poly}(1/δ)$ queries. We also give an information-theoretic argument that light-tailed stochastic gradients are necessary for high accuracy: for example, in the bounded variance case, we show that the minimax-optimal query complexity scales as $Θ(1/δ)$. Our framework also provides similar high accuracy guarantees under stochastic zeroth order (value) queries.",
      "authors": [
        "Fan Chen",
        "Sinho Chewi",
        "Constantinos Daskalakis",
        "Alexander Rakhlin"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "cs.DS",
        "cs.LG",
        "math.PR"
      ],
      "published": "2026-02-15 23:19:07+00:00",
      "link": "https://arxiv.org/pdf/2602.14342v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14335v1",
      "title": "Predicting New Concept-Object Associations in Astronomy by Mining the Literature",
      "abstract": "We construct a concept-object knowledge graph from the full astro-ph corpus through July 2025. Using an automated pipeline, we extract named astrophysical objects from OCR-processed papers, resolve them to SIMBAD identifiers, and link them to scientific concepts annotated in the source corpus. We then test whether historical graph structure can forecast new concept-object associations before they appear in print. Because the concepts are derived from clustering and therefore overlap semantically, we apply an inference-time concept-similarity smoothing step uniformly to all methods. Across four temporal cutoffs on a physically meaningful subset of concepts, an implicit-feedback matrix factorization model (alternating least squares, ALS) with smoothing outperforms the strongest neighborhood baseline (KNN using text-embedding concept similarity) by 16.8% on NDCG@100 (0.144 vs 0.123) and 19.8% on Recall@100 (0.175 vs 0.146), and exceeds the best recency heuristic by 96% and 88%, respectively. These results indicate that historical literature encodes predictive structure not captured by global heuristics or local neighborhood voting, suggesting a path toward tools that could help triage follow-up targets for scarce telescope time.",
      "authors": [
        "Jinchu Li",
        "Yuan-Sen Ting",
        "Alberto Accomazzi",
        "Tirthankar Ghosal",
        "Nesar Ramachandra"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM",
        "cs.IR"
      ],
      "published": "2026-02-15 23:07:10+00:00",
      "link": "https://arxiv.org/pdf/2602.14335v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14320v2",
      "title": "Catalytic Tree Evaluation From Matching Vectors",
      "abstract": "We give new algorithms for tree evaluation (S. Cook et al. TOCT 2012) in the catalytic-computing model (Buhrman et al. STOC 2014). Two existing approaches aim to solve tree evaluation in low space: on the one hand, J. Cook and Mertz (STOC 2024) give an algorithm for TreeEval running in super-logarithmic space $O(\\log n\\log\\log n)$ and super-polynomial time $n^{O(\\log\\log n)}$. On the other hand, a simple reduction from TreeEval to circuit evaluation, combined with the result of Buhrman et al. (STOC 2014), gives a catalytic algorithm for TreeEval running in logarithmic $O(\\log n)$ free space and polynomial time, but with polynomial catalytic space.   We show that the latter result can be improved. We give a catalytic algorithm for TreeEval with logarithmic $O(\\log n)$ free space, polynomial runtime, and subpolynomial $2^{\\log^εn}$ catalytic space (for any $ε> 0$). Our result opens a new line of attack on putting TreeEval in logspace, and immediately implies an improved simulation of time by catalytic space, by the reduction of Williams (STOC 2025).   Our catalytic TreeEval algorithm is inspired by a connection to matching-vector families and private information retrieval, and improved constructions of (uniform) matching-vector families would imply improvements to our algorithm.",
      "authors": [
        "Alexandra Henzinger",
        "Edward Pyne",
        "Seyoon Ragavan"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS"
      ],
      "published": "2026-02-15 22:04:34+00:00",
      "link": "https://arxiv.org/pdf/2602.14320v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14302v1",
      "title": "Floe: Federated Specialization for Real-Time LLM-SLM Inference",
      "abstract": "Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.",
      "authors": [
        "Chunlin Tian",
        "Kahou Tam",
        "Yebo Wu",
        "Shuaihang Zhong",
        "Li Li",
        "Nicholas D. Lane",
        "Chengzhong Xu"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "published": "2026-02-15 20:28:38+00:00",
      "link": "https://arxiv.org/pdf/2602.14302v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14301v1",
      "title": "DeepFusion: Accelerating MoE Training via Federated Knowledge Distillation from Heterogeneous Edge Devices",
      "abstract": "Recent Mixture-of-Experts (MoE)-based large language models (LLMs) such as Qwen-MoE and DeepSeek-MoE are transforming generative AI in natural language processing. However, these models require vast and diverse training data. Federated learning (FL) addresses this challenge by leveraging private data from heterogeneous edge devices for privacy-preserving MoE training. Nonetheless, traditional FL approaches require devices to host local MoE models, which is impractical for resource-constrained devices due to large model sizes. To address this, we propose DeepFusion, the first scalable federated MoE training framework that enables the fusion of heterogeneous on-device LLM knowledge via federated knowledge distillation, yielding a knowledge-abundant global MoE model. Specifically, DeepFusion features each device to independently configure and train an on-device LLM tailored to its own needs and hardware limitations. Furthermore, we propose a novel View-Aligned Attention (VAA) module that integrates multi-stage feature representations from the global MoE model to construct a predictive perspective aligned with on-device LLMs, thereby enabling effective cross-architecture knowledge distillation. By explicitly aligning predictive perspectives, VAA resolves the view-mismatch problem in traditional federated knowledge distillation, which arises from heterogeneity in model architectures and prediction behaviors between on-device LLMs and the global MoE model. Experiments with industry-level MoE models (Qwen-MoE and DeepSeek-MoE) and real-world datasets (medical and finance) demonstrate that DeepFusion achieves performance close to centralized MoE training. Compared with key federated MoE baselines, DeepFusion reduces communication costs by up to 71% and improves token perplexity by up to 5.28%.",
      "authors": [
        "Songyuan Li",
        "Jia Hu",
        "Ahmed M. Abdelmoniem",
        "Geyong Min",
        "Haojun Huang",
        "Jiwei Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2026-02-15 20:25:50+00:00",
      "link": "https://arxiv.org/pdf/2602.14301v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14295v1",
      "title": "Machine Learning as a Tool (MLAT): A Framework for Integrating Statistical ML Models as Callable Tools within LLM Agent Workflows",
      "abstract": "We introduce Machine Learning as a Tool (MLAT), a design pattern in which pre-trained statistical machine learning models are exposed as callable tools within large language model (LLM) agent workflows. This allows an orchestrating agent to invoke quantitative predictions when needed and reason about their outputs in context. Unlike conventional pipelines that treat ML inference as a static preprocessing step, MLAT positions the model as a first-class tool alongside web search, database queries, and APIs, enabling the LLM to decide when and how to use it based on conversational context.   To validate MLAT, we present PitchCraft, a pilot production system that converts discovery call recordings into professional proposals with ML-predicted pricing. The system uses two agents: a Research Agent that gathers prospect intelligence via parallel tool calls, and a Draft Agent that invokes an XGBoost pricing model as a tool call and generates a complete proposal through structured outputs. The pricing model, trained on 70 examples combining real and human-verified synthetic data, achieves R^2 = 0.807 on held-out data with a mean absolute error of 3688 USD. The system reduces proposal generation time from multiple hours to under 10 minutes.   We describe the MLAT framework, structured output architecture, training methodology under extreme data scarcity, and sensitivity analysis demonstrating meaningful learned relationships. MLAT generalizes to domains requiring quantitative estimation combined with contextual reasoning.",
      "authors": [
        "Edwin Chen",
        "Zulekha Bibi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-15 20:00:28+00:00",
      "link": "https://arxiv.org/pdf/2602.14295v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14293v1",
      "title": "KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning",
      "abstract": "Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.",
      "authors": [
        "Kris Shengjun Dong",
        "Sahil Modi",
        "Dima Nikiforov",
        "Sana Damani",
        "Edward Lin",
        "Siva Kumar Sastry Hari",
        "Christos Kozyrakis"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-15 19:48:43+00:00",
      "link": "https://arxiv.org/pdf/2602.14293v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14291v1",
      "title": "Bengali-Loop: Community Benchmarks for Long-Form Bangla ASR and Speaker Diarization",
      "abstract": "Bengali (Bangla) remains under-resourced in long-form speech technology despite its wide use. We present Bengali-Loop, two community benchmarks to address this gap: (1) a long-form ASR corpus of 191 recordings (158.6 hours, 792k words) from 11 YouTube channels, collected via a reproducible subtitle-extraction pipeline and human-in-the-loop transcript verification; and (2) a speaker diarization corpus of 24 recordings (22 hours, 5,744 annotated segments) with fully manual speaker-turn labels in CSV format. Both benchmarks target realistic multi-speaker, long-duration content (e.g., Bangla drama/natok). We establish baselines (Tugstugi: 34.07% WER; pyannote.audio: 40.08% DER) and provide standardized evaluation protocols (WER/CER, DER), annotation rules, and data formats to support reproducible benchmarking and future model development for Bangla long-form ASR and diarization.",
      "authors": [
        "H. M. Shadman Tabib",
        "Istiak Ahmmed Rifti",
        "Abdullah Muhammed Amimul Ehsan",
        "Somik Dasgupta",
        "Md Zim Mim Siddiqee Sowdha",
        "Abrar Jahin Sarker",
        "Md. Rafiul Islam Nijamy",
        "Tanvir Hossain",
        "Mst. Metaly Khatun",
        "Munzer Mahmood",
        "Rakesh Debnath",
        "Gourab Biswas",
        "Asif Karim",
        "Wahid Al Azad Navid",
        "Masnoon Muztahid",
        "Fuad Ahmed Udoy",
        "Shahad Shahriar Rahman",
        "Md. Tashdiqur Rahman Shifat",
        "Most. Sonia Khatun",
        "Mushfiqur Rahman",
        "Md. Miraj Hasan",
        "Anik Saha",
        "Mohammad Ninad Mahmud Nobo",
        "Soumik Bhattacharjee",
        "Tusher Bhomik",
        "Ahmmad Nur Swapnil",
        "Shahriar Kabir"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "published": "2026-02-15 19:48:21+00:00",
      "link": "https://arxiv.org/pdf/2602.14291v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14289v1",
      "title": "Parallel Sparse and Data-Sparse Factorization-based Linear Solvers",
      "abstract": "Efficient solutions of large-scale, ill-conditioned and indefinite algebraic equations are ubiquitously needed in numerous computational fields, including multiphysics simulations, machine learning, and data science. Because of their robustness and accuracy, direct solvers are crucial components in building a scalable solver toolchain. In this article, we will review recent advances of sparse direct solvers along two axes: 1) reducing communication and latency costs in both task- and data-parallel settings, and 2) reducing computational complexity via low-rank and other compression techniques such as hierarchical matrix algebra. In addition to algorithmic principles, we also illustrate the key parallelization challenges and best practices to deliver high speed and reliability on modern heterogeneous parallel machines.",
      "authors": [
        "Xiaoye Sherry Li",
        "Yang Liu"
      ],
      "primary_category": "cs.MS",
      "categories": [
        "cs.MS",
        "cs.DC",
        "math.NA"
      ],
      "published": "2026-02-15 19:40:14+00:00",
      "link": "https://arxiv.org/pdf/2602.14289v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14285v1",
      "title": "FMMD: A multimodal open peer review dataset based on F1000Research",
      "abstract": "Automated scholarly paper review (ASPR) has entered the coexistence phase with traditional peer review, where artificial intelligence (AI) systems are increasingly incorporated into real-world manuscript evaluation. In parallel, research on automated and AI-assisted peer review has proliferated. Despite this momentum, empirical progress remains constrained by several critical limitations in existing datasets. While reviewers routinely evaluate figures, tables, and complex layouts to assess scientific claims, most existing datasets remain overwhelmingly text-centric. This bias is reinforced by a narrow focus on data from computer science venues. Furthermore, these datasets lack precise alignment between reviewer comments and specific manuscript versions, obscuring the iterative relationship between peer review and manuscript evolution. In response, we introduce FMMD, a multimodal and multidisciplinary open peer review dataset curated from F1000Research. The dataset bridges the current gap by integrating manuscript-level visual and structural data with version-specific reviewer reports and editorial decisions. By providing explicit alignment between reviewer comments and the exact article iteration under review, FMMD enables fine-grained analysis of the peer review lifecycle across diverse scientific domains. FMMD supports tasks such as multimodal issue detection and multimodal review comment generation. It provides a comprehensive empirical resource for the development of peer review research.",
      "authors": [
        "Zhenzhen Zhuang",
        "Yuqing Fu",
        "Jing Zhu",
        "Zhangping Zhou",
        "Jialiang Lin"
      ],
      "primary_category": "cs.DL",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-15 19:36:05+00:00",
      "link": "https://arxiv.org/pdf/2602.14285v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14284v1",
      "title": "Benchmarking AI Performance on End-to-End Data Science Projects",
      "abstract": "Data science is an integrated workflow of technical, analytical, communication, and ethical skills, but current AI benchmarks focus mostly on constituent parts. We test whether AI models can generate end-to-end data science projects. To do this we create a benchmark of 40 end-to-end data science projects with associated rubric evaluations. We use these to build an automated grading pipeline that systematically evaluates the data science projects produced by generative AI models. We find the extent to which generative AI models can complete end-to-end data science projects varies considerably by model. Most recent models did well on structured tasks, but there were considerable differences on tasks that needed judgment. These findings suggest that while AI models could approximate entry-level data scientists on routine tasks, they require verification.",
      "authors": [
        "Evelyn Hughes",
        "Rohan Alexander"
      ],
      "primary_category": "stat.OT",
      "categories": [
        "stat.OT",
        "cs.CY"
      ],
      "published": "2026-02-15 19:16:04+00:00",
      "link": "https://arxiv.org/pdf/2602.14284v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14280v1",
      "title": "Fast Compute for ML Optimization",
      "abstract": "We study optimization for losses that admit a variance-mean scale-mixture representation. Under this representation, each EM iteration is a weighted least squares update in which latent variables determine observation and parameter weights; these play roles analogous to Adam's second-moment scaling and AdamW's weight decay, but are derived from the model. The resulting Scale Mixture EM (SM-EM) algorithm removes user-specified learning-rate and momentum schedules. On synthetic ill-conditioned logistic regression benchmarks with $p \\in \\{20, \\ldots, 500\\}$, SM-EM with Nesterov acceleration attains up to $13\\times$ lower final loss than Adam tuned by learning-rate grid search. For a 40-point regularization path, sharing sufficient statistics across penalty values yields a $10\\times$ runtime reduction relative to the same tuned-Adam protocol. For the base (non-accelerated) algorithm, EM monotonicity guarantees nonincreasing objective values; adding Nesterov extrapolation trades this guarantee for faster empirical convergence.",
      "authors": [
        "Nick Polson",
        "Vadim Sokolov"
      ],
      "primary_category": "stat.CO",
      "categories": [
        "stat.CO",
        "cs.LG"
      ],
      "published": "2026-02-15 19:09:58+00:00",
      "link": "https://arxiv.org/pdf/2602.14280v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14279v1",
      "title": "Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions",
      "abstract": "Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multi-turn interactions in natural language, most existing elicitation methods optimize what to ask with a fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, a multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose a theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries a small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including a >12% relative gain on CES at a 10% respondent budget.",
      "authors": [
        "Ruomeng Ding",
        "Tianwei Gao",
        "Thomas P. Zollo",
        "Eitan Bachmat",
        "Richard Zemel",
        "Zhun Deng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.SI"
      ],
      "published": "2026-02-15 19:05:34+00:00",
      "link": "https://arxiv.org/pdf/2602.14279v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14275v1",
      "title": "Reverse N-Wise Output-Oriented Testing for AI/ML and Quantum Computing Systems",
      "abstract": "Artificial intelligence/machine learning (AI/ML) systems and emerging quantum computing software present unprecedented testing challenges characterized by high-dimensional/continuous input spaces, probabilistic/non-deterministic output distributions, behavioral correctness defined exclusively over observable prediction behaviors and measurement outcomes, and critical quality dimensions, trustworthiness, fairness, calibration, robustness, error syndrome patterns, that manifest through complex multi-way interactions among semantically meaningful output properties rather than deterministic input-output mappings. This paper introduces reverse n-wise output testing, a mathematically principled paradigm inversion that constructs covering arrays directly over domain-specific output equivalence classes, ML confidence calibration buckets, decision boundary regions, fairness partitions, embedding clusters, ranking stability bands, quantum measurement outcome distributions (0-dominant, 1-dominant, superposition collapse), error syndrome patterns (bit-flip, phase-flip, correlated errors), then solves the computationally challenging black-box inverse mapping problem via gradient-free metaheuristic optimization to synthesize input feature configurations or quantum circuit parameters capable of eliciting targeted behavioral signatures from opaque models. The framework delivers synergistic benefits across both domains: explicit customer-centric prediction/measurement coverage guarantees, substantial improvements in fault detection rates for ML calibration/boundary failures and quantum error syndromes, enhanced test suite efficiency, and structured MLOps/quantum validation pipelines with automated partition discovery from uncertainty analysis and coverage drift monitoring.",
      "authors": [
        "Lamine Rihani"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-15 18:57:11+00:00",
      "link": "https://arxiv.org/pdf/2602.14275v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14274v1",
      "title": "Integrating Unstructured Text into Causal Inference: Empirical Evidence from Real Data",
      "abstract": "Causal inference, a critical tool for informing business decisions, traditionally relies heavily on structured data. However, in many real-world scenarios, such data can be incomplete or unavailable. This paper presents a framework that leverages transformer-based language models to perform causal inference using unstructured text. We demonstrate the effectiveness of our framework by comparing causal estimates derived from unstructured text against those obtained from structured data across population, group, and individual levels. Our findings show consistent results between the two approaches, validating the potential of unstructured text in causal inference tasks. Our approach extends the applicability of causal inference methods to scenarios where only textual data is available, enabling data-driven business decision-making when structured tabular data is scarce.",
      "authors": [
        "Boning Zhou",
        "Ziyu Wang",
        "Han Hong",
        "Haoqi Hu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-15 18:55:03+00:00",
      "link": "https://arxiv.org/pdf/2602.14274v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14272v1",
      "title": "Radial-VCReg: More Informative Representation Learning Through Radial Gaussianization",
      "abstract": "Self-supervised learning aims to learn maximally informative representations, but explicit information maximization is hindered by the curse of dimensionality. Existing methods like VCReg address this by regularizing first and second-order feature statistics, which cannot fully achieve maximum entropy. We propose Radial-VCReg, which augments VCReg with a radial Gaussianization loss that aligns feature norms with the Chi distribution-a defining property of high-dimensional Gaussians. We prove that Radial-VCReg transforms a broader class of distributions towards normality compared to VCReg and show on synthetic and real-world datasets that it consistently improves performance by reducing higher-order dependencies and promoting more diverse and informative representations.",
      "authors": [
        "Yilun Kuang",
        "Yash Dagade",
        "Deep Chakraborty",
        "Erik Learned-Miller",
        "Randall Balestriero",
        "Tim G. J. Rudner",
        "Yann LeCun"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-15 18:50:52+00:00",
      "link": "https://arxiv.org/pdf/2602.14272v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14263v1",
      "title": "Towards a Hybrid Quantum-Classical Computing Framework for Database Optimization Problems in Real Time Setup",
      "abstract": "Quantum computing has shown promise for solving complex optimization problems in databases, such as join ordering and index selection. Prior work often submits formulated problems directly to black-box quantum or quantum-inspired solvers with the expectation of directly obtaining a good final solution. Due to the black-box nature of these solvers, users cannot perform fine-grained control over the solving procedure to balance the accuracy and efficiency, which in turn limits flexibility in real-time settings where most database problems arise. Moreover, it leads to limited potential for handling large-scale database optimization problems. In this paper, we propose a vision for the first real-time quantum-augmented database system, enabling transparent solutions for database optimization problems. We develop two complementary scalability strategies to address large-scale challenges, overcomplexity, and oversizing that exceed hardware limits. We integrate our approach with a database query optimizer as a preliminary prototype, evaluating on real-world workload, achieving up to 14x improvement over the classical query optimizer. We also achieve both better efficiency and solution quality than a black-box quantum solver.",
      "authors": [
        "Hanwen Liu",
        "Ibrahim Sabek"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-15 18:25:07+00:00",
      "link": "https://arxiv.org/pdf/2602.14263v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14252v1",
      "title": "GRAIL: Goal Recognition Alignment through Imitation Learning",
      "abstract": "Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.",
      "authors": [
        "Osher Elhadad",
        "Felipe Meneguzzi",
        "Reuth Mirsky"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "published": "2026-02-15 17:45:03+00:00",
      "link": "https://arxiv.org/pdf/2602.14252v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14251v1",
      "title": "Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection",
      "abstract": "Tabular anomaly detection is often handled by single detectors or static ensembles, even though strong performance on tabular data typically comes from heterogeneous model families (e.g., tree ensembles, deep tabular networks, and tabular foundation models) that frequently disagree under distribution shift, missingness, and rare-anomaly regimes. We propose MAD, a Multi-Agent Debating framework that treats this disagreement as a first-class signal and resolves it through a mathematically grounded coordination layer. Each agent is a machine learning (ML)-based detector that produces a normalized anomaly score, confidence, and structured evidence, augmented by a large language model (LLM)-based critic. A coordinator converts these messages into bounded per-agent losses and updates agent influence via an exponentiated-gradient rule, yielding both a final debated anomaly score and an auditable debate trace. MAD is a unified agentic framework that can recover existing approaches, such as mixture-of-experts gating and learning-with-expert-advice aggregation, by restricting the message space and synthesis operator. We establish regret guarantees for the synthesized losses and show how conformal calibration can wrap the debated score to control false positives under exchangeability. Experiments on diverse tabular anomaly benchmarks show improved robustness over baselines and clearer traces of model disagreement",
      "authors": [
        "Pinqiao Wang",
        "Sheng Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-15 17:44:32+00:00",
      "link": "https://arxiv.org/pdf/2602.14251v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14244v1",
      "title": "Federated Ensemble Learning with Progressive Model Personalization",
      "abstract": "Federated Learning provides a privacy-preserving paradigm for distributed learning, but suffers from statistical heterogeneity across clients. Personalized Federated Learning (PFL) mitigates this issue by considering client-specific models. A widely adopted approach in PFL decomposes neural networks into a shared feature extractor and client-specific heads. While effective, this design induces a fundamental tradeoff: deep or expressive shared components hinder personalization, whereas large local heads exacerbate overfitting under limited per-client data. Most existing methods rely on rigid, shallow heads, and therefore fail to navigate this tradeoff in a principled manner. In this work, we propose a boosting-inspired framework that enables a smooth control of this tradeoff. Instead of training a single personalized model, we construct an ensemble of $T$ models for each client. Across boosting iterations, the depth of the personalized component are progressively increased, while its effective complexity is systematically controlled via low-rank factorization or width shrinkage. This design simultaneously limits overfitting and substantially reduces per-client bias by allowing increasingly expressive personalization. We provide theoretical analysis that establishes generalization bounds with favorable dependence on the average local sample size and the total number of clients. Specifically, we prove that the complexity of the shared layers is effectively suppressed, while the dependence on the boosting horizon $T$ is controlled through parameter reduction. Notably, we provide a novel nonlinear generalization guarantee for decoupled PFL models. Extensive experiments on benchmark and real-world datasets (e.g., EMNIST, CIFAR-10/100, and Sent140) demonstrate that the proposed framework consistently outperforms state-of-the-art PFL methods under heterogeneous data distributions.",
      "authors": [
        "Ala Emrani",
        "Amir Najafi",
        "Abolfazl Motahari"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-15 17:35:52+00:00",
      "link": "https://arxiv.org/pdf/2602.14244v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14243v1",
      "title": "Graph Homomorphisms and Universal Algebra",
      "abstract": "Constraint satisfaction problems are computational problems that naturally appear in many areas of theoretical computer science. One of the central themes is their computational complexity, and in particular the border between polynomial-time tractability and NP-hardness. In this course we introduce the universal-algebraic approach to study the computational complexity of finite-domain CSPs. The course covers in particular the cyclic terms and bounded width theorems. To keep the presentation accessible, we start the course in the tangible setting of directed graphs and graph homomorphism problems.",
      "authors": [
        "Manuel Bodirsky"
      ],
      "primary_category": "cs.CC",
      "categories": [
        "cs.CC",
        "cs.DM",
        "math.LO",
        "math.RA"
      ],
      "published": "2026-02-15 17:35:37+00:00",
      "link": "https://arxiv.org/pdf/2602.14243v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14238v1",
      "title": "We can still parse using syntactic rules",
      "abstract": "This research introduces a new parsing approach, based on earlier syntactic work on context free grammar (CFG) and generalized phrase structure grammar (GPSG). The approach comprises both a new parsing algorithm and a set of syntactic rules and features that overcome the limitations of CFG. It also generates both dependency and constituency parse trees, while accommodating noise and incomplete parses. The system was tested on data from Universal Dependencies, showing a promising average Unlabeled Attachment Score (UAS) of 54.5% in the development dataset (7 corpora) and 53.8% in the test set (12 corpora). The system also provides multiple parse hypotheses, allowing further reranking to improve parsing accuracy. This approach also leverages much of the theoretical syntactic work since the 1950s to be used within a computational context. The application of this approach provides a transparent and interpretable NLP model to process language input.",
      "authors": [
        "Ghaly Hussein"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-15 17:16:32+00:00",
      "link": "https://arxiv.org/pdf/2602.14238v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14234v1",
      "title": "REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents",
      "abstract": "Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.",
      "authors": [
        "Zheng Chu",
        "Xiao Wang",
        "Jack Hong",
        "Huiming Fan",
        "Yuqi Huang",
        "Yue Yang",
        "Guohai Xu",
        "Chenxiao Zhao",
        "Cheng Xiang",
        "Shengchao Hu",
        "Dongdong Kuang",
        "Ming Liu",
        "Bing Qin",
        "Xing Yu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-15 17:04:46+00:00",
      "link": "https://arxiv.org/pdf/2602.14234v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14231v1",
      "title": "Robust multi-task boosting using clustering and local ensembling",
      "abstract": "Multi-Task Learning (MTL) aims to boost predictive performance by sharing information across related tasks, yet conventional methods often suffer from negative transfer when unrelated or noisy tasks are forced to share representations. We propose Robust Multi-Task Boosting using Clustering and Local Ensembling (RMB-CLE), a principled MTL framework that integrates error-based task clustering with local ensembling. Unlike prior work that assumes fixed clusters or hand-crafted similarity metrics, RMB-CLE derives inter-task similarity directly from cross-task errors, which admit a risk decomposition into functional mismatch and irreducible noise, providing a theoretically grounded mechanism to prevent negative transfer. Tasks are grouped adaptively via agglomerative clustering, and within each cluster, a local ensemble enables robust knowledge sharing while preserving task-specific patterns. Experiments show that RMB-CLE recovers ground-truth clusters in synthetic data and consistently outperforms multi-task, single-task, and pooling-based ensemble methods across diverse real-world and synthetic benchmarks. These results demonstrate that RMB-CLE is not merely a combination of clustering and boosting but a general and scalable framework that establishes a new basis for robust multi-task learning.",
      "authors": [
        "Seyedsaman Emami",
        "Daniel Hernández-Lobato",
        "Gonzalo Martínez-Muñoz"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-15 16:59:23+00:00",
      "link": "https://arxiv.org/pdf/2602.14231v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14208v1",
      "title": "Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws",
      "abstract": "Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in Li et al. (2025a) provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism -- the fast catch-up effect -- which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that large batches can be safely deferred to late training without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments -- covering both Dense and MoE architectures with up to 1.1B parameters and 1T tokens -- validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.",
      "authors": [
        "Jinbo Wang",
        "Binghui Li",
        "Zhanpeng Zhou",
        "Mingze Wang",
        "Yuxuan Sun",
        "Jiaqi Zhang",
        "Xunliang Cai",
        "Lei Wu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "published": "2026-02-15 16:06:45+00:00",
      "link": "https://arxiv.org/pdf/2602.14208v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14200v1",
      "title": "TS-Haystack: A Multi-Scale Retrieval Benchmark for Time Series Language Models",
      "abstract": "Time Series Language Models (TSLMs) are emerging as unified models for reasoning over continuous signals in natural language. However, long-context retrieval remains a major limitation: existing models are typically trained and evaluated on short sequences, while real-world time-series sensor streams can span millions of datapoints. This mismatch requires precise temporal localization under strict computational constraints, a regime that is not captured by current benchmarks. We introduce TS-Haystack, a long-context temporal retrieval benchmark comprising ten task types across four categories: direct retrieval, temporal reasoning, multi-step reasoning and contextual anomaly. The benchmark uses controlled needle insertion by embedding short activity bouts into longer longitudinal accelerometer recordings, enabling systematic evaluation across context lengths ranging from seconds to 2 hours per sample. We hypothesize that existing TSLM time series encoders overlook temporal granularity as context length increases, creating a task-dependent effect: compression aids classification but impairs retrieval of localized events. Across multiple model and encoding strategies, we observe a consistent divergence between classification and retrieval behavior. Learned latent compression preserves or improves classification accuracy at compression ratios up to 176$\\times$, but retrieval performance degrades with context length, incurring in the loss of temporally localized information. These results highlight the importance of architectural designs that decouple sequence length from computational complexity while preserving temporal fidelity.",
      "authors": [
        "Nicolas Zumarraga",
        "Thomas Kaar",
        "Ning Wang",
        "Maxwell A. Xu",
        "Max Rosenblattl",
        "Markus Kreft",
        "Kevin O'Sullivan",
        "Paul Schmiedmayer",
        "Patrick Langer",
        "Robert Jakob"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-15 15:50:02+00:00",
      "link": "https://arxiv.org/pdf/2602.14200v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14189v1",
      "title": "Knowing When Not to Answer: Abstention-Aware Scientific Reasoning",
      "abstract": "Large language models are increasingly used to answer and verify scientific claims, yet existing evaluations typically assume that a model must always produce a definitive answer. In scientific settings, however, unsupported or uncertain conclusions can be more harmful than abstaining. We study this problem through an abstention-aware verification framework that decomposes scientific claims into minimal conditions, audits each condition against available evidence using natural language inference (NLI), and selectively decides whether to support, refute, or abstain. We evaluate this framework across two complementary scientific benchmarks: SciFact and PubMedQA, covering both closed-book and open-domain evidence settings. Experiments are conducted with six diverse language models, including encoder-decoder, open-weight chat models, and proprietary APIs. Across all benchmarks and models, we observe that raw accuracy varies only modestly across architectures, while abstention plays a critical role in controlling error. In particular, confidence-based abstention substantially reduces risk at moderate coverage levels, even when absolute accuracy improvements are limited. Our results suggest that in scientific reasoning tasks, the primary challenge is not selecting a single best model, but rather determining when available evidence is sufficient to justify an answer. This work highlights abstention-aware evaluation as a practical and model-agnostic lens for assessing scientific reliability, and provides a unified experimental basis for future work on selective reasoning in scientific domains. Code is available at https://github.com/sabdaljalil2000/ai4science .",
      "authors": [
        "Samir Abdaljalil",
        "Erchin Serpedin",
        "Hasan Kurban"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-15 15:29:43+00:00",
      "link": "https://arxiv.org/pdf/2602.14189v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14188v1",
      "title": "GPT-5 vs Other LLMs in Long Short-Context Performance",
      "abstract": "With the significant expansion of the context window in Large Language Models (LLMs), these models are theoretically capable of processing millions of tokens in a single pass. However, research indicates a significant gap between this theoretical capacity and the practical ability of models to robustly utilize information within long contexts, especially in tasks that require a comprehensive understanding of numerous details. This paper evaluates the performance of four state-of-the-art models (Grok-4, GPT-4, Gemini 2.5, and GPT-5) on long short-context tasks. For this purpose, three datasets were used: two supplementary datasets for retrieving culinary recipes and math problems, and a primary dataset of 20K social media posts for depression detection. The results show that as the input volume on the social media dataset exceeds 5K posts (70K tokens), the performance of all models degrades significantly, with accuracy dropping to around 50-53% for 20K posts. Notably, in the GPT-5 model, despite the sharp decline in accuracy, its precision remained high at approximately 95%, a feature that could be highly effective for sensitive applications like depression detection. This research also indicates that the \"lost in the middle\" problem has been largely resolved in newer models. This study emphasizes the gap between the theoretical capacity and the actual performance of models on complex, high-volume data tasks and highlights the importance of metrics beyond simple accuracy for practical applications.",
      "authors": [
        "Nima Esmi",
        "Maryam Nezhad-Moghaddam",
        "Fatemeh Borhani",
        "Asadollah Shahbahrami",
        "Amin Daemdoost",
        "Georgi Gaydadjiev"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "published": "2026-02-15 15:26:25+00:00",
      "link": "https://arxiv.org/pdf/2602.14188v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14169v1",
      "title": "Deep Dense Exploration for LLM Reinforcement Learning via Pivot-Driven Resampling",
      "abstract": "Effective exploration is a key challenge in reinforcement learning for large language models: discovering high-quality trajectories within a limited sampling budget from the vast natural language sequence space. Existing methods face notable limitations: GRPO samples exclusively from the root, saturating high-probability trajectories while leaving deep, error-prone states under-explored. Tree-based methods blindly disperse budgets across trivial or unrecoverable states, causing sampling dilution that fails to uncover rare correct suffixes and destabilizes local baselines. To address this, we propose Deep Dense Exploration (DDE), a strategy that focuses exploration on $\\textit{pivots}$-deep, recoverable states within unsuccessful trajectories. We instantiate DDE with DEEP-GRPO, which introduces three key innovations: (1) a lightweight data-driven utility function that automatically balances recoverability and depth bias to identify pivot states; (2) local dense resampling at each pivot to increase the probability of discovering correct subsequent trajectories; and (3) a dual-stream optimization objective that decouples global policy learning from local corrective updates. Experiments on mathematical reasoning benchmarks demonstrate that our method consistently outperforms GRPO, tree-based methods, and other strong baselines.",
      "authors": [
        "Yiran Guo",
        "Zhongjian Qiao",
        "Yingqi Xie",
        "Jie Liu",
        "Dan Ye",
        "Ruiqing Zhang",
        "Shuang Qiu",
        "Lijie Xu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-15 14:44:15+00:00",
      "link": "https://arxiv.org/pdf/2602.14169v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14162v1",
      "title": "Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering",
      "abstract": "Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this \"pre-ingestion\" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is \"Index for locating, not understanding\"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the \"QA accuracy\" problem into a \"page localization\" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.",
      "authors": [
        "Tao Xu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.IR"
      ],
      "published": "2026-02-15 14:23:50+00:00",
      "link": "https://arxiv.org/pdf/2602.14162v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14159v1",
      "title": "Synergistic Intra- and Cross-Layer Regularization Losses for MoE Expert Specialization",
      "abstract": "Sparse Mixture-of-Experts (MoE) models scale Transformers efficiently but suffer from expert overlap -- redundant representations across experts and routing ambiguity, resulting in severely underutilized model capacity. While architectural solutions like DeepSeekMoE promote specialization, they require substantial structural modifications and rely solely on intra-layer signals. In this paper, we propose two plug-and-play regularization losses that enhance MoE specialization and routing efficiency without modifying router or model architectures. First, an intra-layer specialization loss penalizes cosine similarity between experts' SwiGLU activations on identical tokens, encouraging experts to specialize in complementary knowledge. Second, a cross-layer coupling loss maximizes joint Top-$k$ routing probabilities across adjacent layers, establishing coherent expert pathways through network depth while reinforcing intra-layer expert specialization. Both losses are orthogonal to the standard load-balancing loss and compatible with both the shared-expert architecture in DeepSeekMoE and vanilla top-$k$ MoE architectures. We implement both losses as a drop-in Megatron-LM module. Extensive experiments across pre-training, fine-tuning, and zero-shot benchmarks demonstrate consistent task gains, higher expert specialization, and lower-entropy routing; together, these improvements translate into faster inference via more stable expert pathways.",
      "authors": [
        "Rizhen Hu",
        "Yuan Cao",
        "Boao Kong",
        "Mou Sun",
        "Kun Yuan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-15 14:19:12+00:00",
      "link": "https://arxiv.org/pdf/2602.14159v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14158v1",
      "title": "A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT, LLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical Query Processing",
      "abstract": "Large language models (LLMs) show promise for healthcare question answering, but clinical use is limited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose a multi-agent medical QA framework that combines complementary LLMs with evidence retrieval, uncertainty estimation, and bias checks to improve answer reliability. Our approach has two phases. First, we fine-tune three representative LLM families (GPT, LLaMA, and DeepSeek R1) on MedQuAD-derived medical QA data (20k+ question-answer pairs across multiple NIH domains) and benchmark generation quality. DeepSeek R1 achieves the strongest scores (ROUGE-1 0.536 +- 0.04; ROUGE-2 0.226 +-0.03; BLEU 0.098 -+ 0.018) and substantially outperforms the specialised biomedical baseline BioGPT in zero-shot evaluation. Second, we implement a modular multi-agent pipeline in which a Clinical Reasoning agent (fine-tuned LLaMA) produces structured explanations, an Evidence Retrieval agent queries PubMed to ground responses in recent literature, and a Refinement agent (DeepSeek R1) improves clarity and factual consistency; an optional human validation path is triggered for high-risk or high-uncertainty cases. Safety mechanisms include Monte Carlo dropout and perplexity-based uncertainty scoring, plus lexical and sentiment-based bias detection supported by LIME/SHAP-based analyses. In evaluation, the full system achieves 87% accuracy with relevance around 0.80, and evidence augmentation reduces uncertainty (perplexity 4.13) compared to base responses, with mean end-to-end latency of 36.5 seconds under the reported configuration. Overall, the results indicate that agent specialisation and verification layers can mitigate key single-model limitations and provide a practical, extensible design for evidence-based and bias-aware medical AI.",
      "authors": [
        "Naeimeh Nourmohammadi",
        "Md Meem Hossain",
        "The Anh Han",
        "Safina Showkat Ara",
        "Zia Ush Shamszaman"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2026-02-15 14:17:27+00:00",
      "link": "https://arxiv.org/pdf/2602.14158v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14154v1",
      "title": "A Penalty Approach for Differentiation Through Black-Box Quadratic Programming Solvers",
      "abstract": "Differentiating through the solution of a quadratic program (QP) is a central problem in differentiable optimization. Most existing approaches differentiate through the Karush--Kuhn--Tucker (KKT) system, but their computational cost and numerical robustness can degrade at scale. To address these limitations, we propose dXPP, a penalty-based differentiation framework that decouples QP solving from differentiation. In the solving step (forward pass), dXPP is solver-agnostic and can leverage any black-box QP solver. In the differentiation step (backward pass), we map the solution to a smooth approximate penalty problem and implicitly differentiate through it, requiring only the solution of a much smaller linear system in the primal variables. This approach bypasses the difficulties inherent in explicit KKT differentiation and significantly improves computational efficiency and robustness. We evaluate dXPP on various tasks, including randomly generated QPs, large-scale sparse projection problems, and a real-world multi-period portfolio optimization task. Empirical results demonstrate that dXPP is competitive with KKT-based differentiation methods and achieves substantial speedups on large-scale problems.",
      "authors": [
        "Yuxuan Linghu",
        "Zhiyuan Liu",
        "Qi Deng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-02-15 14:05:36+00:00",
      "link": "https://arxiv.org/pdf/2602.14154v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14130v1",
      "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
      "abstract": "Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provided with rich context, the space of future generations becomes strongly constrained, and the generation process is effectively governed by near-deterministic dynamics. Recent approaches such as test-time scaling and context adaptation improve performance but do not fundamentally alter this constraint. To address this issue, we propose Algebraic Quantum Intelligence (AQI) as a computational framework that enables systematic expansion of semantic space. AQI is formulated as a noncommutative algebraic structure inspired by quantum theory, allowing properties such as order dependence, interference, and uncertainty to be implemented in a controlled and designable manner. Semantic states are represented as vectors in a Hilbert space, and their evolution is governed by C-values computed from noncommutative operators, thereby ensuring the coexistence and expansion of multiple future semantic possibilities. In this study, we implement AQI by extending a transformer-based LLM with more than 600 specialized operators. We evaluate the resulting system on creative reasoning benchmarks spanning ten domains under an LLM-as-a-judge protocol. The results show that AQI consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance. These findings demonstrate that noncommutative algebraic dynamics can serve as a practical and reproducible foundation for machine creativity. Notably, this architecture has already been deployed in real-world enterprise environments.",
      "authors": [
        "Kazuo Yano",
        "Jonghyeok Lee",
        "Tae Ishitomi",
        "Hironobu Kawaguchi",
        "Akira Koyama",
        "Masakuni Ota",
        "Yuki Ota",
        "Nobuo Sato",
        "Keita Shimada",
        "Sho Takematsu",
        "Ayaka Tobinai",
        "Satomi Tsuji",
        "Kazunori Yanagi",
        "Keiko Yano",
        "Manabu Harada",
        "Yuki Matsuda",
        "Kazunori Matsumoto",
        "Kenichi Matsumura",
        "Hamae Matsuo",
        "Yumi Miyazaki",
        "Kotaro Murai",
        "Tatsuya Ohshita",
        "Marie Seki",
        "Shun Tanoue",
        "Tatsuki Terakado",
        "Yuko Ichimaru",
        "Mirei Saito",
        "Akihiro Otsuka",
        "Koji Ara"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-15 13:02:57+00:00",
      "link": "https://arxiv.org/pdf/2602.14130v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14111v1",
      "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?",
      "abstract": "Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only $9\\%$ of true features despite achieving $71\\%$ explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.",
      "authors": [
        "Anton Korznikov",
        "Andrey Galichin",
        "Alexey Dontsov",
        "Oleg Rogov",
        "Ivan Oseledets",
        "Elena Tutubalina"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-15 11:53:55+00:00",
      "link": "https://arxiv.org/pdf/2602.14111v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14102v1",
      "title": "DALL: Data Labeling via Data Programming and Active Learning Enhanced by Large Language Models",
      "abstract": "Deep learning models for natural language processing rely heavily on high-quality labeled datasets. However, existing labeling approaches often struggle to balance label quality with labeling cost. To address this challenge, we propose DALL, a text labeling framework that integrates data programming, active learning, and large language models. DALL introduces a structured specification that allows users and large language models to define labeling functions via configuration, rather than code. Active learning identifies informative instances for review, and the large language model analyzes these instances to help users correct labels and to refine or suggest labeling functions. We implement DALL as an interactive labeling system for text labeling tasks. Comparative, ablation, and usability studies demonstrate DALL's efficiency, the effectiveness of its modules, and its usability.",
      "authors": [
        "Guozheng Li",
        "Ao Wang",
        "Shaoxiang Wang",
        "Yu Zhang",
        "Pengcheng Cao",
        "Yang Bai",
        "Chi Harold Liu"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-15 11:36:15+00:00",
      "link": "https://arxiv.org/pdf/2602.14102v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14089v1",
      "title": "TabTracer: Monte Carlo Tree Search for Complex Table Reasoning with Large Language Models",
      "abstract": "Large language models (LLMs) have emerged as powerful tools for natural language table reasoning, where there are two main categories of methods. Prompt-based approaches rely on language-only inference or one-pass program generation without step-level verification. Agent-based approaches use tools in a closed loop, but verification is often local and backtracking is limited, allowing errors to propagate and increasing cost. Moreover, they rely on chain- or beam-style trajectories that are typically combinatorially redundant, leading to high token costs. In this paper, we propose TabTracer, an agentic framework that coordinates multi-step tool calls over intermediate table states, with explicit state tracking for verification and rollback. First, it enforces step-level verification with typed operations and lightweight numeric and format checks to provide reliable rewards and suppress hallucinations. Second, execution-feedback Monte Carlo Tree Search maintains a search tree of candidate table states and uses backpropagated reflection scores to guide UCB1 selection and rollback via versioned snapshots. Third, it reduces redundancy with budget-aware pruning, deduplication, and state hashing with a monotonicity gate to cut token cost. Comprehensive evaluation on TabFact, WikiTQ, and CRT datasets shows that TabTracer outperforms state-of-the-art baselines by up to 6.7% in accuracy while reducing token consumption by 59--84%.",
      "authors": [
        "Zhizhao Luo",
        "Zhaojing Luo",
        "Meihui Zhang",
        "Rui Mao"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "published": "2026-02-15 10:39:43+00:00",
      "link": "https://arxiv.org/pdf/2602.14089v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14086v1",
      "title": "Neural Optimal Transport in Hilbert Spaces: Characterizing Spurious Solutions and Gaussian Smoothing",
      "abstract": "We study Neural Optimal Transport in infinite-dimensional Hilbert spaces. In non-regular settings, Semi-dual Neural OT often generates spurious solutions that fail to accurately capture target distributions. We analytically characterize this spurious solution problem using the framework of regular measures, which generalize Lebesgue absolute continuity in finite dimensions. To resolve ill-posedness, we extend the semi-dual framework via a Gaussian smoothing strategy based on Brownian motion. Our primary theoretical contribution proves that under a regular source measure, the formulation is well-posed and recovers a unique Monge map. Furthermore, we establish a sharp characterization for the regularity of smoothed measures, proving that the success of smoothing depends strictly on the kernel of the covariance operator. Empirical results on synthetic functional data and time-series datasets demonstrate that our approach effectively suppresses spurious solutions and outperforms existing baselines.",
      "authors": [
        "Jae-Hwan Choi",
        "Jiwoo Yoon",
        "Dohyun Kwon",
        "Jaewoong Choi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-15 10:27:09+00:00",
      "link": "https://arxiv.org/pdf/2602.14086v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14081v1",
      "title": "CCiV: A Benchmark for Structure, Rhythm and Quality in LLM-Generated Chinese \\textit{Ci} Poetry",
      "abstract": "The generation of classical Chinese \\textit{Ci} poetry, a form demanding a sophisticated blend of structural rigidity, rhythmic harmony, and artistic quality, poses a significant challenge for large language models (LLMs). To systematically evaluate and advance this capability, we introduce \\textbf{C}hinese \\textbf{Ci}pai \\textbf{V}ariants (\\textbf{CCiV}), a benchmark designed to assess LLM-generated \\textit{Ci} poetry across these three dimensions: structure, rhythm, and quality. Our evaluation of 17 LLMs on 30 \\textit{Cipai} reveals two critical phenomena: models frequently generate valid but unexpected historical variants of a poetic form, and adherence to tonal patterns is substantially harder than structural rules. We further show that form-aware prompting can improve structural and tonal control for stronger models, while potentially degrading weaker ones. Finally, we observe weak and inconsistent alignment between formal correctness and literary quality in our sample. CCiV highlights the need for variant-aware evaluation and more holistic constrained creative generation methods.",
      "authors": [
        "Shangqing Zhao",
        "Yupei Ren",
        "Yuhao Zhou",
        "Xiaopeng Bai",
        "Man Lan"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-15 10:19:02+00:00",
      "link": "https://arxiv.org/pdf/2602.14081v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14078v1",
      "title": "Policy Gradient with Adaptive Entropy Annealing for Continual Fine-Tuning",
      "abstract": "Despite their success, large pretrained vision models remain vulnerable to catastrophic forgetting when adapted to new tasks in class-incremental settings. Parameter-efficient fine-tuning (PEFT) alleviates this by restricting trainable parameters, yet most approaches still rely on cross-entropy (CE) loss, a surrogate for the 0-1 loss, to learn from new data. We revisit this choice and revive the true objective (0-1 loss) through a reinforcement learning perspective. By formulating classification as a one-step Markov Decision Process, we derive an Expected Policy Gradient (EPG) method that directly minimizes misclassification error with a low-variance gradient estimation. Our analysis shows that CE can be interpreted as EPG with an additional sample-weighting mechanism: CE encourages exploration by emphasizing low-confidence samples, while EPG prioritizes high-confidence ones. Building on this insight, we propose adaptive entropy annealing (aEPG), a training strategy that transitions from exploratory (CE-like) to exploitative (EPG-like) learning. aEPG-based methods outperform CE-based methods across diverse benchmarks and with various PEFT modules. More broadly, we evaluate various entropy regularization methods and demonstrate that lower entropy of the output prediction distribution enhances adaptation in pretrained vision models.",
      "authors": [
        "Yaqian Zhang",
        "Bernhard Pfahringer",
        "Eibe Frank",
        "Albert Bifet"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-15 10:05:03+00:00",
      "link": "https://arxiv.org/pdf/2602.14078v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14077v1",
      "title": "GTS: Inference-Time Scaling of Latent Reasoning with a Learnable Gaussian Thought Sampler",
      "abstract": "Inference-time scaling (ITS) in latent reasoning models typically introduces stochasticity through heuristic perturbations, such as dropout or fixed Gaussian noise. While these methods increase trajectory diversity, their exploration behavior is not explicitly modeled and can be inefficient under finite sampling budgets. We observe that stronger perturbations do not necessarily translate into more effective candidate trajectories, as unguided noise may disrupt internal decision structure rather than steer it. To provide a more structured alternative, we model latent thought exploration as conditional sampling from learnable densities and instantiate this idea as a Gaussian Thought Sampler (GTS). GTS predicts context-dependent perturbation distributions over continuous reasoning states and is trained with GRPO-style policy optimization while keeping the backbone frozen. Experiments on GSM8K with two latent reasoning architectures show that GTS achieves more reliable inference-time scaling than heuristic baselines. These findings indicate that improving latent ITS requires structured and optimizable exploration mechanisms rather than simply amplifying stochasticity.",
      "authors": [
        "Minghan Wang",
        "Ye Bai",
        "Thuy-Trang Vu",
        "Ehsan Shareghi",
        "Gholamreza Haffari"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-15 09:57:47+00:00",
      "link": "https://arxiv.org/pdf/2602.14077v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14073v2",
      "title": "Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework",
      "abstract": "Most vision-language models (VLMs) are trained on English-centric data, limiting their performance in other languages and cultural contexts. This restricts their usability for non-English-speaking users and hinders the development of multimodal systems that reflect diverse linguistic and cultural realities. In this work, we reproduce and adapt the LLaVA-Next methodology to create a set of Polish VLMs. We rely on a fully automated pipeline for translating and filtering existing multimodal datasets, and complement this with synthetic Polish data for OCR and culturally specific tasks. Despite relying almost entirely on automatic translation and minimal manual intervention to the training data, our approach yields strong results: we observe a +9.5% improvement over LLaVA-1.6-Vicuna-13B on a Polish-adapted MMBench, along with higher-quality captions in generative evaluations, as measured by human annotators in terms of linguistic correctness. These findings highlight that large-scale automated translation, combined with lightweight filtering, can effectively bootstrap high-quality multimodal models for low-resource languages. Some challenges remain, particularly in cultural coverage and evaluation. To facilitate further research, we make our models and evaluation dataset publicly available.",
      "authors": [
        "Grzegorz Statkiewicz",
        "Alicja Dobrzeniecka",
        "Karolina Seweryn",
        "Aleksandra Krasnodębska",
        "Karolina Piosek",
        "Katarzyna Bogusz",
        "Sebastian Cygert",
        "Wojciech Kusa"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-15 09:54:40+00:00",
      "link": "https://arxiv.org/pdf/2602.14073v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14069v1",
      "title": "Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric",
      "abstract": "Scalar reward models compress multi-dimensional human preferences into a single opaque score, creating an information bottleneck that often leads to brittleness and reward hacking in open-ended alignment. We argue that robust alignment for non-verifiable tasks is fundamentally a principle generalization problem: reward should not be a learned function internalized into a judge, but an explicit reasoning process executed under inspectable principles. To operationalize this view, we present the Open Rubric System (OpenRS), a plug-and-play, rubrics-based LLM-as-a-Judge framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components when ground-truth or programmatic checks are available. OpenRS uses an explicit meta-rubric -- a constitution-like specification that governs how rubrics are instantiated, weighted, and enforced -- and instantiates adaptive rubrics on the fly by conditioning on the semantic differences between two candidate responses. It then performs criterion-wise pairwise comparisons and aggregates criterion-level preferences externally, avoiding pointwise weighted scalarization while improving discriminability in open-ended settings. To keep principles consistent yet editable across various domains, we introduce a two-level meta-rubric refinement pipeline (automated evolutionary refinement for general principles and a reproducible human-in-the-loop procedure for domain principles), complemented with pointwise verifiable rubrics that act as both guardrails against degenerate behaviors and a source of verifiable reward for objective sub-tasks. Finally, we instantiate OpenRS as reward supervision in pairwise RL training.",
      "authors": [
        "Ruipeng Jia",
        "Yunyi Yang",
        "Yuxin Wu",
        "Yongbo Gai",
        "Siyuan Tao",
        "Mengyu Zhou",
        "Jianhe Lin",
        "Xiaoxi Jiang",
        "Guanjun Jiang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-15 09:39:39+00:00",
      "link": "https://arxiv.org/pdf/2602.14069v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14065v1",
      "title": "REAL: Resolving Knowledge Conflicts in Knowledge-Intensive Visual Question Answering via Reasoning-Pivot Alignment",
      "abstract": "Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation. Extensive experiments across diverse benchmarks demonstrate that REAL significantly enhances discrimination accuracy and achieves state-of-the-art performance, validating the effectiveness of our pivot-driven resolution paradigm.",
      "authors": [
        "Kai Ye",
        "Xianwei Mao",
        "Sheng Zhou",
        "Zirui Shao",
        "Ye Mo",
        "Liangliang Liu",
        "Haikuan Huang",
        "Bin Li",
        "Jiajun Bu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-15 09:29:53+00:00",
      "link": "https://arxiv.org/pdf/2602.14065v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14062v1",
      "title": "From Scarcity to Scale: A Release-Level Analysis of the Pashto Common Voice Dataset",
      "abstract": "Large, openly licensed speech datasets are essential for building automatic speech recognition (ASR) systems, yet many widely spoken languages remain underrepresented in public resources. Pashto, spoken by more than 60 million people, has historically lacked large-scale openly licensed speech data suitable for modern ASR development.   This paper presents a release-level analysis of the Pashto component of the Mozilla Common Voice corpus, focusing on version 24.0 (December 2025) and contextualizing trends across major releases. We document rapid growth from 1.49 recorded hours in mid-2023 to 2,768.7 total hours in 2025, including 975.89 validated hours available for supervised ASR training.   Beyond scale, we analyze validation throughput, contributor participation inequality, demographic metadata completeness, and sentence-level concentration in the validated subset. We find that participation is extremely concentrated (Gini = 0.941), age representation is strongly skewed toward young adults, and 41.97\\% of clips lack self-reported gender labels, limiting subgroup auditing based on metadata. At the textual level, prompt reuse is moderate: 35.88\\% of unique sentences account for 50\\% of validated clips, suggesting that structural concentration is driven primarily by uneven contributor activity rather than dominance of a small prompt set.   These results provide a quantitative audit of a rapidly scaling low-resource speech corpus and highlight practical priorities for improving dataset maturity, including expanded validation capacity and broader demographic participation.",
      "authors": [
        "Jandad Jahani",
        "Mursal Dawodi",
        "Jawid Ahmad Baktash"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "published": "2026-02-15 09:22:48+00:00",
      "link": "https://arxiv.org/pdf/2602.14062v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14060v1",
      "title": "LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts",
      "abstract": "We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language models are trained as domain experts, LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications.",
      "authors": [
        "Yang Liu",
        "Jiaye Yang",
        "Weikang Li",
        "Jiahui Liang",
        "Yang Li",
        "Lingyong Yan"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-15 09:18:22+00:00",
      "link": "https://arxiv.org/pdf/2602.14060v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14054v1",
      "title": "LogitsCoder: Towards Efficient Chain-of-Thought Path Search via Logits Preference Decoding for Code Generation",
      "abstract": "Code generation remains a challenging task that requires precise and structured reasoning. Existing Test Time Scaling (TTS) methods, including structured tree search, have made progress in exploring reasoning paths but still face two major challenges: (1) underthinking, where reasoning chains tend to be shallow and fail to capture the full complexity of problems; and (2) overthinking, where overly verbose reasoning leads to inefficiency and increased computational costs. To address these issues, we propose LogitsCoder, a novel framework that enhances chain-of-thought reasoning through lightweight, logit-level control mechanisms for code generation. LogitsCoder iteratively generates and refines reasoning steps by first steering token selection toward statistically preferred patterns via Logits Preference Decoding, then selecting and aggregating diverse reasoning paths using Logits Rank Based Path Selection and Thoughts Aggregation. This results in coherent and effective reasoning chains that balance depth and efficiency. Extensive experiments demonstrate that LogitsCoder produces more efficient and higher-quality reasoning chains, leading to superior code generation performance compared to baseline methods.",
      "authors": [
        "Jizheng Chen",
        "Weiming Zhang",
        "Xinyi Dai",
        "Weiwen Liu",
        "Kounianhua Du",
        "Yasheng Wang",
        "Ruiming Tang",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-15 08:52:19+00:00",
      "link": "https://arxiv.org/pdf/2602.14054v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14044v1",
      "title": "Context Shapes LLMs Retrieval-Augmented Fact-Checking Effectiveness",
      "abstract": "Large language models (LLMs) show strong reasoning abilities across diverse tasks, yet their performance on extended contexts remains inconsistent. While prior research has emphasized mid-context degradation in question answering, this study examines the impact of context in LLM-based fact verification. Using three datasets (HOVER, FEVEROUS, and ClimateFEVER) and five open-source models accross different parameters sizes (7B, 32B and 70B parameters) and model families (Llama-3.1, Qwen2.5 and Qwen3), we evaluate both parametric factual knowledge and the impact of evidence placement across varying context lengths. We find that LLMs exhibit non-trivial parametric knowledge of factual claims and that their verification accuracy generally declines as context length increases. Similarly to what has been shown in previous works, in-context evidence placement plays a critical role with accuracy being consistently higher when relevant evidence appears near the beginning or end of the prompt and lower when placed mid-context. These results underscore the importance of prompt structure in retrieval-augmented fact-checking systems.",
      "authors": [
        "Pietro Bernardelle",
        "Stefano Civelli",
        "Kevin Roitero",
        "Gianluca Demartini"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-15 08:15:13+00:00",
      "link": "https://arxiv.org/pdf/2602.14044v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14041v1",
      "title": "BitDance: Scaling Autoregressive Generative Models with Binary Tokens",
      "abstract": "We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to $2^{256}$ states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.",
      "authors": [
        "Yuang Ai",
        "Jiaming Han",
        "Shaobin Zhuang",
        "Weijia Mao",
        "Xuefeng Hu",
        "Ziyan Yang",
        "Zhenheng Yang",
        "Huaibo Huang",
        "Xiangyu Yue",
        "Hao Chen"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-15 08:09:05+00:00",
      "link": "https://arxiv.org/pdf/2602.14041v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14040v1",
      "title": "Explainability-Inspired Layer-Wise Pruning of Deep Neural Networks for Efficient Object Detection",
      "abstract": "Deep neural networks (DNNs) have achieved remarkable success in object detection tasks, but their increasing complexity poses significant challenges for deployment on resource-constrained platforms. While model compression techniques such as pruning have emerged as essential tools, traditional magnitude-based pruning methods do not necessarily align with the true functional contribution of network components to task-specific performance. In this work, we present an explainability-inspired, layer-wise pruning framework tailored for efficient object detection. Our approach leverages a SHAP-inspired gradient--activation attribution to estimate layer importance, providing a data-driven proxy for functional contribution rather than relying solely on static weight magnitudes. We conduct comprehensive experiments across diverse object detection architectures, including ResNet-50, MobileNetV2, ShuffleNetV2, Faster R-CNN, RetinaNet, and YOLOv8, evaluating performance on the Microsoft COCO 2017 validation set. The results show that the proposed attribution-inspired pruning consistently identifies different layers as least important compared to L1-norm-based methods, leading to improved accuracy--efficiency trade-offs. Notably, for ShuffleNetV2, our method yields a 10\\% empirical increase in inference speed, whereas L1-pruning degrades performance by 13.7\\%. For RetinaNet, the proposed approach preserves the baseline mAP (0.151) with negligible impact on inference speed, while L1-pruning incurs a 1.3\\% mAP drop for a 6.2\\% speed increase. These findings highlight the importance of data-driven layer importance assessment and demonstrate that explainability-inspired compression offers a principled direction for deploying deep neural networks on edge and resource-constrained platforms while preserving both performance and interpretability.",
      "authors": [
        "Abhinav Shukla",
        "Nachiket Tapas"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-15 08:07:19+00:00",
      "link": "https://arxiv.org/pdf/2602.14040v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14039v1",
      "title": "Geometry-Preserving Aggregation for Mixture-of-Experts Embedding Models",
      "abstract": "Mixture-of-Experts (MoE) embedding models combine expert outputs using weighted linear summation, implicitly assuming a linear subspace structure in the embedding space. This assumption is shown to be inconsistent with the geometry of expert representations. Geometric analysis of a modern MoE embedding model reveals that expert outputs lie on a shared hyperspherical manifold characterized by tightly concentrated norms and substantial angular separation. Under this geometry, linear aggregation induces inward collapse toward the manifold interior, distorting vector magnitude and direction and reducing embedding comparability. To address this inconsistency, Spherical Barycentric Aggregation (SBA) is introduced as a geometry-preserving aggregation operator that separates radial and angular components to maintain hyperspherical structure while remaining fully compatible with existing routing mechanisms. Experiments on selected tasks from the Massive Text Embedding Benchmark (MTEB), including semantic similarity, clustering, and duplicate question detection, demonstrate consistent performance improvements with identical training cost and full stability. Additional geometric analyses confirm that SBA prevents aggregation-induced collapse and preserves hyperspherical consistency, highlighting the importance of geometry-aware aggregation in MoE embedding architectures.",
      "authors": [
        "Sajjad Kachuee",
        "Mohammad Sharifkhani"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-15 08:00:56+00:00",
      "link": "https://arxiv.org/pdf/2602.14039v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14037v1",
      "title": "Affine Rank Minimization is ER Complete",
      "abstract": "We study the decision problem Affine Rank Minimization, denoted ARM(k). The input consists of rational matrices A_1,...,A_q in Q^{m x n} and rational scalars b_1,...,b_q in Q. The question is whether there exists a real matrix X in R^{m x n} such that trace(A_l^T X) = b_l for all l in {1,...,q} and rank(X) <= k. We first prove membership: for every fixed k >= 1, ARM(k) lies in the existential theory of the reals by giving an explicit existential encoding of the rank constraint using a constant-size factorization witness. We then prove existential-theory-of-reals hardness via a polynomial-time many-one reduction from ETR to ARM(k), where the target instance uses only affine equalities together with a single global constraint rank(X) <= k. The reduction compiles an ETR formula into an arithmetic circuit in gate-equality normal form and assigns each circuit quantity to a designated entry of X. Affine semantics (constants, copies, addition, and negation) are enforced by linear constraints, while multiplicative semantics are enforced by constant-size rank-forcing gadgets. Soundness is certified by a fixed-rank gauge submatrix that removes factorization ambiguity. We prove a composition lemma showing that gadgets can be embedded without unintended interactions, yielding global soundness and completeness while preserving polynomial bounds on dimension and bit-length. Consequently, ARM(k) is complete for the existential theory of the reals; in particular, ARM(3) is complete. This shows that feasibility of purely affine constraints under a fixed constant rank bound captures the full expressive power of real algebraic feasibility.",
      "authors": [
        "Angshul Majumdar"
      ],
      "primary_category": "cs.CC",
      "categories": [
        "cs.CC"
      ],
      "published": "2026-02-15 07:54:59+00:00",
      "link": "https://arxiv.org/pdf/2602.14037v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14029v1",
      "title": "Why Self-Training Helps and Hurts: Denoising vs. Signal Forgetting",
      "abstract": "Iterative self-training (self-distillation) repeatedly refits a model on pseudo-labels generated by its own predictions. We study this procedure in overparameterized linear regression: an initial estimator is trained on noisy labels, and each subsequent iterate is trained on fresh covariates with noiseless pseudo-labels from the previous model. In the high-dimensional regime, we derive deterministic-equivalent recursions for the prediction risk and effective noise across iterations, and prove that the empirical quantities concentrate sharply around these limits. The recursion separates two competing forces: a systematic component that grows with iteration due to progressive signal forgetting, and a stochastic component that decays due to denoising via repeated data-dependent projections. Their interaction yields a $U$-shaped test-risk curve and an optimal early-stopping time. In spiked covariance models, iteration further acts as an iteration-dependent spectral filter that preserves strong eigendirections while suppressing weaker ones, inducing an implicit form of soft feature selection distinct from ridge regression. Finally, we propose an iterated generalized cross-validation criterion and prove its uniform consistency for estimating the risk along the self-training trajectory, enabling fully data-driven selection of the stopping time and regularization. Experiments on synthetic covariances validate the theory and illustrate the predicted denoising-forgetting trade-off.",
      "authors": [
        "Mingqi Wu",
        "Archer Y. Yang",
        "Qiang Sun"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "published": "2026-02-15 07:28:12+00:00",
      "link": "https://arxiv.org/pdf/2602.14029v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14028v1",
      "title": "GRRM: Group Relative Reward Modeling for Machine Translation",
      "abstract": "While Group Relative Policy Optimization (GRPO) offers a powerful framework for LLM post-training, its effectiveness in open-ended domains like Machine Translation hinges on accurate intra-group ranking. We identify that standard Scalar Quality Metrics (SQM) fall short in this context; by evaluating candidates in isolation, they lack the comparative context necessary to distinguish fine-grained linguistic nuances. To address this, we introduce the Group Quality Metric (GQM) paradigm and instantiate it via the Group Relative Reward Model (GRRM). Unlike traditional independent scorers, GRRM processes the entire candidate group jointly, leveraging comparative analysis to rigorously resolve relative quality and adaptive granularity. Empirical evaluations confirm that GRRM achieves competitive ranking accuracy among all baselines. Building on this foundation, we integrate GRRM into the GRPO training loop to optimize the translation policy. Experimental results demonstrate that our framework not only improves general translation quality but also unlocks reasoning capabilities comparable to state-of-the-art reasoning models. We release codes, datasets, and model checkpoints at https://github.com/NJUNLP/GRRM.",
      "authors": [
        "Sen Yang",
        "Shanbo Cheng",
        "Lu Xu",
        "Jianbing Zhang",
        "Shujian Huang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-15 07:22:27+00:00",
      "link": "https://arxiv.org/pdf/2602.14028v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14024v1",
      "title": "EIDOS: Latent-Space Predictive Learning for Time Series Foundation Models",
      "abstract": "Most time series foundation models are pretrained by directly predicting future observations, which often yields weakly structured latent representations that capture surface noise rather than coherent and predictable temporal dynamics. In this work, we introduce EIDOS, a foundation model family that shifts pretraining from future value prediction to latent-space predictive learning. We train a causal Transformer to predict the evolution of latent representations, encouraging the emergence of structured and temporally coherent latent states. To ensure stable targets for latent-space learning, we design a lightweight aggregation branch to construct target representations. EIDOS is optimized via a joint objective that integrates latent-space alignment, observational grounding to anchor representations to the input signal, and direct forecasting supervision. On the GIFT-Eval benchmark, EIDOS mitigates structural fragmentation in the representation space and achieves state-of-the-art performance. These results demonstrate that constraining models to learn predictable latent dynamics is a principled step toward more robust and reliable time series foundation models.",
      "authors": [
        "Xinxing Zhou",
        "Qingren Yao",
        "Yiji Zhao",
        "Chenghao Liu",
        "Flora Salim",
        "Xiaojie Yuan",
        "Yanlong Wen",
        "Ming Jin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-15 07:07:20+00:00",
      "link": "https://arxiv.org/pdf/2602.14024v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14020v1",
      "title": "Computable Bernstein Certificates for Cross-Fitted Clipped Covariance Estimation",
      "abstract": "We study operator-norm covariance estimation from heavy-tailed samples that may include a small fraction of arbitrary outliers. A simple and widely used safeguard is \\emph{Euclidean norm clipping}, but its accuracy depends critically on an unknown clipping level. We propose a cross-fitted clipped covariance estimator equipped with \\emph{fully computable} Bernstein-type deviation certificates, enabling principled data-driven tuning via a selector (\\emph{MinUpper}) that balances certified stochastic error and a robust hold-out proxy for clipping bias. The resulting procedure adapts to intrinsic complexity measures such as effective rank under mild tail regularity and retains meaningful guarantees under only finite fourth moments. Experiments on contaminated spiked-covariance benchmarks illustrate stable performance and competitive accuracy across regimes.",
      "authors": [
        "Even He",
        "Zaizai Yan"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-15 06:53:40+00:00",
      "link": "https://arxiv.org/pdf/2602.14020v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14011v1",
      "title": "KoopGen: Koopman Generator Networks for Representing and Predicting Dynamical Systems with Continuous Spectra",
      "abstract": "Representing and predicting high-dimensional and spatiotemporally chaotic dynamical systems remains a fundamental challenge in dynamical systems and machine learning. Although data-driven models can achieve accurate short-term forecasts, they often lack stability, interpretability, and scalability in regimes dominated by broadband or continuous spectra. Koopman-based approaches provide a principled linear perspective on nonlinear dynamics, but existing methods rely on restrictive finite-dimensional assumptions or explicit spectral parameterizations that degrade in high-dimensional settings. Against these issues, we introduce KoopGen, a generator-based neural Koopman framework that models dynamics through a structured, state-dependent representation of Koopman generators. By exploiting the intrinsic Cartesian decomposition into skew-adjoint and self-adjoint components, KoopGen separates conservative transport from irreversible dissipation while enforcing exact operator-theoretic constraints during learning. Across systems ranging from nonlinear oscillators to high-dimensional chaotic and spatiotemporal dynamics, KoopGen improves prediction accuracy and stability, while clarifying which components of continuous-spectrum dynamics admit interpretable and learnable representations.",
      "authors": [
        "Liangyu Su",
        "Jun Shu",
        "Rui Liu",
        "Deyu Meng",
        "Zongben Xu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-15 06:32:23+00:00",
      "link": "https://arxiv.org/pdf/2602.14011v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14002v1",
      "title": "The Sufficiency-Conciseness Trade-off in LLM Self-Explanation from an Information Bottleneck Perspective",
      "abstract": "Large Language Models increasingly rely on self-explanations, such as chain of thought reasoning, to improve performance on multi step question answering. While these explanations enhance accuracy, they are often verbose and costly to generate, raising the question of how much explanation is truly necessary. In this paper, we examine the trade-off between sufficiency, defined as the ability of an explanation to justify the correct answer, and conciseness, defined as the reduction in explanation length. Building on the information bottleneck principle, we conceptualize explanations as compressed representations that retain only the information essential for producing correct answers.To operationalize this view, we introduce an evaluation pipeline that constrains explanation length and assesses sufficiency using multiple language models on the ARC Challenge dataset. To broaden the scope, we conduct experiments in both English, using the original dataset, and Persian, as a resource-limited language through translation. Our experiments show that more concise explanations often remain sufficient, preserving accuracy while substantially reducing explanation length, whereas excessive compression leads to performance degradation.",
      "authors": [
        "Ali Zahedzadeh",
        "Behnam Bahrak"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-15 05:57:39+00:00",
      "link": "https://arxiv.org/pdf/2602.14002v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13987v1",
      "title": "ATTest: Agent-Driven Tensor Testing for Deep Learning Library Modules",
      "abstract": "The unit testing of Deep Learning (DL) libraries is challenging due to complex numerical semantics and implicit tensor constraints. Traditional Search-Based Software Testing (SBST) often suffers from semantic blindness, failing to satisfy the constraints of high-dimensional tensors, whereas Large Language Models (LLMs) struggle with cross-file context and unstable code modifications. This paper proposes ATTest, an agent-driven tensor testing framework for module-level unit test generation. ATTest orchestrates a seven-stage pipeline, which encompasses constraint extraction and an iterative \"generation-validation-repair\" loop, to maintain testing stability and mitigate context-window saturation. An evaluation on PyTorch and TensorFlow demonstrates that ATTest significantly outperforms state-of-the-art baselines such as PynguinML, achieving an average branch coverage of 55.60% and 54.77%, respectively. The results illustrate how agent-driven workflows bridge the semantic gap in numerical libraries while ensuring auditable test synthesis. Source code: https://github.com/iSEngLab/ATTest.git",
      "authors": [
        "Zhengyu Zhan",
        "Ye Shang",
        "Jiawei Liu",
        "Chunrong Fang",
        "Quanjun Zhang",
        "Zhenyu Chen"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-15 04:47:58+00:00",
      "link": "https://arxiv.org/pdf/2602.13987v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13985v1",
      "title": "Bridging AI and Clinical Reasoning: Abductive Explanations for Alignment on Critical Symptoms",
      "abstract": "Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.",
      "authors": [
        "Belona Sonna",
        "Alban Grastien"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-15 04:27:59+00:00",
      "link": "https://arxiv.org/pdf/2602.13985v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13980v1",
      "title": "Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking",
      "abstract": "Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\\% in F1 score and 40.7\\% in EM score on QA tasks at the $64\\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\\%.",
      "authors": [
        "Guojie Liu",
        "Yiqi Wang",
        "Yanfeng Yang",
        "Wenqi Fan",
        "Songlei Jian",
        "Jianfeng Zhang",
        "Jie Yu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-15 03:58:13+00:00",
      "link": "https://arxiv.org/pdf/2602.13980v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13964v2",
      "title": "HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam",
      "abstract": "Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy. Our construction follows a two-stage validation-and-repair workflow resulting in a certified benchmark. In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks, yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs, model-assisted auditing, and final adjudication, resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified",
      "authors": [
        "Weiqi Zhai",
        "Zhihai Wang",
        "Jinghang Wang",
        "Boyu Yang",
        "Xiaogang Li",
        "Xiang Xu",
        "Bohan Wang",
        "Peng Wang",
        "Xingzhe Wu",
        "Anfeng Li",
        "Qiyuan Feng",
        "Yuhao Zhou",
        "Shoulin Han",
        "Wenjie Luo",
        "Yiyuan Li",
        "Yaxuan Wang",
        "Ruixian Luo",
        "Guojie Lin",
        "Peiyao Xiao",
        "Chengliang Xu",
        "Ben Wang",
        "Zeyu Wang",
        "Zichao Chen",
        "Jianan Ye",
        "Yijie Hu",
        "Jialong Chen",
        "Zongwen Shen",
        "Yuliang Xu",
        "An Yang",
        "Bowen Yu",
        "Dayiheng Liu",
        "Junyang Lin",
        "Hu Wei",
        "Que Shen",
        "Bing Zhao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-15 02:50:15+00:00",
      "link": "https://arxiv.org/pdf/2602.13964v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13962v1",
      "title": "CodeGlance: Understanding Code Reasoning Challenges in LLMs through Multi-Dimensional Feature Analysis",
      "abstract": "In modern software development, developers frequently need to understand code behavior at a glance -- whether reviewing pull requests, debugging issues, or navigating unfamiliar codebases. This ability to reason about dynamic program behavior is fundamental to effective software engineering and increasingly supported by Large Language Models (LLMs). However, existing studies on code reasoning focus primarily on isolated code snippets, overlooking the complexity of real-world scenarios involving external API interactions and unfamiliar functions. This gap hinders our understanding of what truly makes code reasoning challenging for LLMs across diverse programming contexts.   We present CodeGlance, a multi-dimensional benchmark investigating code reasoning challenges across three realistic scenarios: intrinsic logic reasoning, API interaction reasoning, and unseen function reasoning. Through systematic evaluation of 7 state-of-the-art LLMs, we reveal that unseen function reasoning poses significant challenges especially for smaller models, with Qwen2.5-3b achieving only 6.0\\% accuracy on unseen functions compared to 37.5\\% on familiar APIs. We identify critical code complexity features -- including execution trace length, API invocation count, and control flow complexity -- that significantly impact code reasoning difficulty across scenarios. We further investigate how common augmentation strategies, including CoT, document retrieval, and code search, can improve reasoning performance, finding that their effectiveness varies substantially depending on whether challenges stem from logical complexity or knowledge gaps. These findings provide actionable guidance for developing more capable code reasoning systems and deploying LLM-based programming assistants in real-world software development.",
      "authors": [
        "Yunkun Wang",
        "Xuanhe Zhang",
        "Junxiao Han",
        "Chen Zhi",
        "Shuiguang Deng"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-15 02:46:51+00:00",
      "link": "https://arxiv.org/pdf/2602.13962v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13961v1",
      "title": "MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars",
      "abstract": "Data-driven approaches like deep learning are rapidly advancing planetary science, particularly in Mars exploration. Despite recent progress, most existing benchmarks remain confined to closed-set supervised visual tasks and do not support text-guided retrieval for geospatial discovery. We introduce MarsRetrieval, a retrieval benchmark for evaluating vision-language models for Martian geospatial discovery. MarsRetrieval includes three tasks: (1) paired image-text retrieval, (2) landform retrieval, and (3) global geo-localization, covering multiple spatial scales and diverse geomorphic origins. We propose a unified retrieval-centric protocol to benchmark multimodal embedding architectures, including contrastive dual-tower encoders and generative vision-language models. Our evaluation shows MarsRetrieval is challenging: even strong foundation models often fail to capture domain-specific geomorphic distinctions. We further show that domain-specific fine-tuning is critical for generalizable geospatial discovery in planetary settings. Our code is available at https://github.com/ml-stat-Sustech/MarsRetrieval",
      "authors": [
        "Shuoyuan Wang",
        "Yiran Wang",
        "Hongxin Wei"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "astro-ph.IM",
        "cs.CL"
      ],
      "published": "2026-02-15 02:41:56+00:00",
      "link": "https://arxiv.org/pdf/2602.13961v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13960v1",
      "title": "Steady-State Behavior of Constant-Stepsize Stochastic Approximation: Gaussian Approximation and Tail Bounds",
      "abstract": "Constant-stepsize stochastic approximation (SA) is widely used in learning for computational efficiency. For a fixed stepsize, the iterates typically admit a stationary distribution that is rarely tractable. Prior work shows that as the stepsize $α\\downarrow 0$, the centered-and-scaled steady state converges weakly to a Gaussian random vector. However, for fixed $α$, this weak convergence offers no usable error bound for approximating the steady-state by its Gaussian limit. This paper provides explicit, non-asymptotic error bounds for fixed $α$. We first prove general-purpose theorems that bound the Wasserstein distance between the centered-scaled steady state and an appropriate Gaussian distribution, under regularity conditions for drift and moment conditions for noise. To ensure broad applicability, we cover both i.i.d. and Markovian noise models. We then instantiate these theorems for three representative SA settings: (1) stochastic gradient descent (SGD) for smooth strongly convex objectives, (2) linear SA, and (3) contractive nonlinear SA. We obtain dimension- and stepsize-dependent, explicit bounds in Wasserstein distance of order $α^{1/2}\\log(1/α)$ for small $α$. Building on the Wasserstein approximation error, we further derive non-uniform Berry--Esseen-type tail bounds that compare the steady-state tail probability to Gaussian tails. We achieve an explicit error term that decays in both the deviation level and stepsize $α$. We adapt the same analysis for SGD beyond strongly convexity and study general convex objectives. We identify a non-Gaussian (Gibbs) limiting law under the correct scaling, which is validated numerically, and provide a corresponding pre-limit Wasserstein error bound.",
      "authors": [
        "Zedong Wang",
        "Yuyang Wang",
        "Ijay Narang",
        "Felix Wang",
        "Yuzhou Wang",
        "Siva Theja Maguluri"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.PR"
      ],
      "published": "2026-02-15 02:34:50+00:00",
      "link": "https://arxiv.org/pdf/2602.13960v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13958v1",
      "title": "Chemical Language Models for Natural Products: A State-Space Model Approach",
      "abstract": "Language models are widely used in chemistry for molecular property prediction and small-molecule generation, yet Natural Products (NPs) remain underexplored despite their importance in drug discovery. To address this gap, we develop NP-specific chemical language models (NPCLMs) by pre-training state-space models (Mamba and Mamba-2) and comparing them with transformer baselines (GPT). Using a dataset of about 1M NPs, we present the first systematic comparison of selective state-space models and transformers for NP-focused tasks, together with eight tokenization strategies including character-level, Atom-in-SMILES (AIS), byte-pair encoding (BPE), and NP-specific BPE. We evaluate molecule generation (validity, uniqueness, novelty) and property prediction (membrane permeability, taste, anti-cancer activity) using MCC and AUC-ROC. Mamba generates 1-2 percent more valid and unique molecules than Mamba-2 and GPT, with fewer long-range dependency errors, while GPT yields slightly more novel structures. For property prediction, Mamba variants outperform GPT by 0.02-0.04 MCC under random splits, while scaffold splits show comparable performance. Results demonstrate that domain-specific pre-training on about 1M NPs can match models trained on datasets over 100 times larger.",
      "authors": [
        "Ho-Hsuan Wang",
        "Afnan Sultan",
        "Andrea Volkamer",
        "Dietrich Klakow"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-15 02:26:36+00:00",
      "link": "https://arxiv.org/pdf/2602.13958v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13937v1",
      "title": "A Multi-Agent Framework for Code-Guided, Modular, and Verifiable Automated Machine Learning",
      "abstract": "Automated Machine Learning (AutoML) has revolutionized the development of data-driven solutions; however, traditional frameworks often function as \"black boxes\", lacking the flexibility and transparency required for complex, real-world engineering tasks. Recent Large Language Model (LLM)-based agents have shifted toward code-driven approaches. However, they frequently suffer from hallucinated logic and logic entanglement, where monolithic code generation leads to unrecoverable runtime failures. In this paper, we present iML, a novel multi-agent framework designed to shift AutoML from black-box prompting to a code-guided, modular, and verifiable architectural paradigm. iML introduces three main ideas: (1) Code-Guided Planning, which synthesizes a strategic blueprint grounded in autonomous empirical profiling to eliminate hallucination; (2) Code-Modular Implementation, which decouples preprocessing and modeling into specialized components governed by strict interface contracts; and (3) Code-Verifiable Integration, which enforces physical feasibility through dynamic contract verification and iterative self-correction. We evaluate iML across MLE-BENCH and the newly introduced iML-BENCH, comprising a diverse range of real-world Kaggle competitions. The experimental results show iML's superiority over state-of-the-art agents, achieving a valid submission rate of 85% and a competitive medal rate of 45% on MLE-BENCH, with an average standardized performance score (APS) of 0.77. On iML-BENCH, iML significantly outperforms the other approaches by 38%-163% in APS. Furthermore, iML maintains a robust 70% success rate even under stripped task descriptions, effectively filling information gaps through empirical profiling. These results highlight iML's potential to bridge the gap between stochastic generation and reliable engineering, marking a meaningful step toward truly AutoML.",
      "authors": [
        "Dat Le",
        "Duc-Cuong Le",
        "Anh-Son Nguyen",
        "Tuan-Dung Bui",
        "Thu-Trang Nguyen",
        "Son Nguyen",
        "Hieu Dinh Vo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.SE"
      ],
      "published": "2026-02-15 00:20:58+00:00",
      "link": "https://arxiv.org/pdf/2602.13937v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13935v1",
      "title": "Statistical Early Stopping for Reasoning Models",
      "abstract": "While LLMs have seen substantial improvement in reasoning capabilities, they also sometimes overthink, generating unnecessary reasoning steps, particularly under uncertainty, given ill-posed or ambiguous queries. We introduce statistically principled early stopping methods that monitor uncertainty signals during generation to mitigate this issue. Our first approach is parametric: it models inter-arrival times of uncertainty keywords as a renewal process and applies sequential testing for stopping. Our second approach is nonparametric and provides finite-sample guarantees on the probability of halting too early on well-posed queries. We conduct empirical evaluations on reasoning tasks across several domains and models. Our results indicate that uncertainty-aware early stopping can improve both efficiency and reliability in LLM reasoning, and we observe especially significant gains for math reasoning.",
      "authors": [
        "Yangxinyu Xie",
        "Tao Wang",
        "Soham Mallick",
        "Yan Sun",
        "Georgy Noarov",
        "Mengxin Yu",
        "Tanwi Mallick",
        "Weijie J. Su",
        "Edgar Dobriban"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-15 00:14:53+00:00",
      "link": "https://arxiv.org/pdf/2602.13935v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13934v1",
      "title": "Why Code, Why Now: Learnability, Computability, and the Real Limits of Machine Learning",
      "abstract": "Code generation has progressed more reliably than reinforcement learning, largely because code has an information structure that makes it learnable. Code provides dense, local, verifiable feedback at every token, whereas most reinforcement learning problems do not. This difference in feedback quality is not binary but graded. We propose a five-level hierarchy of learnability based on information structure and argue that the ceiling on ML progress depends less on model size than on whether a task is learnable at all. The hierarchy rests on a formal distinction among three properties of computational problems (expressibility, computability, and learnability). We establish their pairwise relationships, including where implications hold and where they fail, and present a unified template that makes the structural differences explicit. The analysis suggests why supervised learning on code scales predictably while reinforcement learning does not, and why the common assumption that scaling alone will solve remaining ML challenges warrants scrutiny.",
      "authors": [
        "Zhimin Zhao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-15 00:14:31+00:00",
      "link": "https://arxiv.org/pdf/2602.13934v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13933v1",
      "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
      "abstract": "Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical details required for complex reasoning, while retaining raw text introduces unnecessary computational overhead for simple queries. The crux lies in the limitations of monolithic memory representations and static retrieval mechanisms, which fail to emulate the flexible and proactive memory scheduling capabilities observed in humans, thus struggling to adapt to diverse problem scenarios. Inspired by the principle of cognitive economy, we propose HyMem, a hybrid memory architecture that enables dynamic on-demand scheduling through multi-granular memory representations. HyMem adopts a dual-granular storage scheme paired with a dynamic two-tier retrieval system: a lightweight module constructs summary-level context for efficient response generation, while an LLM-based deep module is selectively activated only for complex queries, augmented by a reflection mechanism for iterative reasoning refinement. Experiments show that HyMem achieves strong performance on both the LOCOMO and LongMemEval benchmarks, outperforming full-context while reducing computational cost by 92.6\\%, establishing a state-of-the-art balance between efficiency and performance in long-term memory management.",
      "authors": [
        "Xiaochen Zhao",
        "Kaikai Wang",
        "Xiaowen Zhang",
        "Chen Yao",
        "Aili Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-15 00:06:19+00:00",
      "link": "https://arxiv.org/pdf/2602.13933v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13921v1",
      "title": "GREPO: A Benchmark for Graph Neural Networks on Repository-Level Bug Localization",
      "abstract": "Repository-level bug localization-the task of identifying where code must be modified to fix a bug-is a critical software engineering challenge. Standard Large Language Modles (LLMs) are often unsuitable for this task due to context window limitations that prevent them from processing entire code repositories. As a result, various retrieval methods are commonly used, including keyword matching, text similarity, and simple graph-based heuristics such as Breadth-First Search. Graph Neural Networks (GNNs) offer a promising alternative due to their ability to model complex, repository-wide dependencies; however, their application has been hindered by the lack of a dedicated benchmark. To address this gap, we introduce GREPO, the first GNN benchmark for repository-scale bug localization tasks. GREPO comprises 86 Python repositories and 47294 bug-fixing tasks, providing graph-based data structures ready for direct GNN processing. Our evaluation of various GNN architectures shows outstanding performance compared to established information retrieval baselines. This work highlights the potential of GNNs for bug localization and established GREPO as a foundation resource for future research, The code is available at https://github.com/qingpingmo/GREPO.",
      "authors": [
        "Juntong Wang",
        "Libin Chen",
        "Xiyuan Wang",
        "Shijia Kang",
        "Haotong Yang",
        "Da Zheng",
        "Muhan Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "published": "2026-02-14 23:22:15+00:00",
      "link": "https://arxiv.org/pdf/2602.13921v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13910v1",
      "title": "Sufficient Conditions for Stability of Minimum-Norm Interpolating Deep ReLU Networks",
      "abstract": "Algorithmic stability is a classical framework for analyzing the generalization error of learning algorithms. It predicts that an algorithm has small generalization error if it is insensitive to small perturbations in the training set such as the removal or replacement of a training point. While stability has been demonstrated for numerous well-known algorithms, this framework has had limited success in analyses of deep neural networks. In this paper we study the algorithmic stability of deep ReLU homogeneous neural networks that achieve zero training error using parameters with the smallest $L_2$ norm, also known as the minimum-norm interpolation, a phenomenon that can be observed in overparameterized models trained by gradient-based algorithms. We investigate sufficient conditions for such networks to be stable. We find that 1) such networks are stable when they contain a (possibly small) stable sub-network, followed by a layer with a low-rank weight matrix, and 2) such networks are not guaranteed to be stable even when they contain a stable sub-network, if the following layer is not low-rank. The low-rank assumption is inspired by recent empirical and theoretical results which demonstrate that training deep neural networks is biased towards low-rank weight matrices, for minimum-norm interpolation and weight-decay regularization.",
      "authors": [
        "Ouns El Harzli",
        "Yoonsoo Nam",
        "Ilja Kuzborskij",
        "Bernardo Cuenca Grau",
        "Ard A. Louis"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-14 22:20:44+00:00",
      "link": "https://arxiv.org/pdf/2602.13910v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13906v1",
      "title": "Quantifying Normality: Convergence Rate to Gaussian Limit for Stochastic Approximation and Unadjusted OU Algorithm",
      "abstract": "Stochastic approximation (SA) is a method for finding the root of an operator perturbed by noise. There is a rich literature establishing the asymptotic normality of rescaled SA iterates under fairly mild conditions. However, these asymptotic results do not quantify the accuracy of the Gaussian approximation in finite time. In this paper, we establish explicit non-asymptotic bounds on the Wasserstein distance between the distribution of the rescaled iterate at time k and the asymptotic Gaussian limit for various choices of step-sizes including constant and polynomially decaying. As an immediate consequence, we obtain tail bounds on the error of SA iterates at any time.   We obtain the sharp rates by first studying the convergence rate of the discrete Ornstein-Uhlenbeck (O-U) process driven by general noise, whose stationary distribution is identical to the limiting Gaussian distribution of the rescaled SA iterates. We believe that this is of independent interest, given its connection to sampling literature. The analysis involves adapting Stein's method for Gaussian approximation to handle the matrix weighted sum of i.i.d. random variables. The desired finite-time bounds for SA are obtained by characterizing the error dynamics between the rescaled SA iterate and the discrete time O-U process and combining it with the convergence rate of the latter process.",
      "authors": [
        "Shaan Ul Haque",
        "Zedong Wang",
        "Zixuan Zhang",
        "Siva Theja Maguluri"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-14 21:55:57+00:00",
      "link": "https://arxiv.org/pdf/2602.13906v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13905v1",
      "title": "Pre-Editorial Normalization for Automatically Transcribed Medieval Manuscripts in Old French and Latin",
      "abstract": "Recent advances in Automatic Text Recognition (ATR) have improved access to historical archives, yet a methodological divide persists between palaeographic transcriptions and normalized digital editions. While ATR models trained on more palaeographically-oriented datasets such as CATMuS have shown greater generalizability, their raw outputs remain poorly compatible with most readers and downstream NLP tools, thus creating a usability gap. On the other hand, ATR models trained to produce normalized outputs have been shown to struggle to adapt to new domains and tend to over-normalize and hallucinate. We introduce the task of Pre-Editorial Normalization (PEN), which consists in normalizing graphemic ATR output according to editorial conventions, which has the advantage of keeping an intermediate step with palaeographic fidelity while providing a normalized version for practical usability. We present a new dataset derived from the CoMMA corpus and aligned with digitized Old French and Latin editions using passim. We also produce a manually corrected gold-standard evaluation set. We benchmark this resource using ByT5-based sequence-to-sequence models on normalization and pre-annotation tasks. Our contributions include the formal definition of PEN, a 4.66M-sample silver training corpus, a 1.8k-sample gold evaluation set, and a normalization model achieving a 6.7% CER, substantially outperforming previous models for this task.",
      "authors": [
        "Thibault Clérice",
        "Rachel Bawden",
        "Anthony Glaise",
        "Ariane Pinche",
        "David Smith"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-14 21:55:30+00:00",
      "link": "https://arxiv.org/pdf/2602.13905v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13891v1",
      "title": "GSRM: Generative Speech Reward Model for Speech RLHF",
      "abstract": "Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions. Experiments show that GSRM substantially outperforms existing speech naturalness predictors, achieving model-human correlation of naturalness score prediction that approaches human inter-rater consistency. We further show how GSRM can improve the naturalness of speech LLM generations by serving as an effective verifier for online RLHF.",
      "authors": [
        "Maohao Shen",
        "Tejas Jayashankar",
        "Osama Hanna",
        "Naoyuki Kanda",
        "Yancheng Wang",
        "Kateřina Žmolíková",
        "Ruiming Xie",
        "Niko Moritz",
        "Anfeng Xu",
        "Yashesh Gaur",
        "Gregory Wornell",
        "Qing He",
        "Jilong Wu"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "published": "2026-02-14 21:22:55+00:00",
      "link": "https://arxiv.org/pdf/2602.13891v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13890v1",
      "title": "Evaluating Prompt Engineering Techniques for RAG in Small Language Models: A Multi-Hop QA Approach",
      "abstract": "Retrieval Augmented Generation (RAG) is a powerful approach for enhancing the factual grounding of language models by integrating external knowledge. While widely studied for large language models, the optimization of RAG for Small Language Models (SLMs) remains a critical research gap, particularly in complex, multi-hop question-answering tasks that require sophisticated reasoning. In these systems, prompt template design is a crucial yet under-explored factor influencing performance. This paper presents a large-scale empirical study to investigate this factor, evaluating 24 different prompt templates on the HotpotQA dataset. The set includes a standard RAG prompt, nine well-formed techniques from the literature, and 14 novel hybrid variants, all tested on two prominent SLMs: Qwen2.5-3B Instruct and Gemma3-4B-It. Our findings, based on a test set of 18720 instances, reveal significant performance gains of up to 83% on Qwen2.5 and 84.5% on Gemma3-4B-It, yielding an improvement of up to 6% for both models compared to the Standard RAG prompt. This research also offers concrete analysis and actionable recommendations for designing effective and efficient prompts for SLM-based RAG systems, practically for deployment in resource-constrained environments.",
      "authors": [
        "Amir Hossein Mohammadi",
        "Ali Moeinian",
        "Zahra Razavizade",
        "Afsaneh Fatemi",
        "Reza Ramezani"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-14 21:17:44+00:00",
      "link": "https://arxiv.org/pdf/2602.13890v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13873v1",
      "title": "Ambient Physics: Training Neural PDE Solvers with Partial Observations",
      "abstract": "In many scientific settings, acquiring complete observations of PDE coefficients and solutions can be expensive, hazardous, or impossible. Recent diffusion-based methods can reconstruct fields given partial observations, but require complete observations for training. We introduce Ambient Physics, a framework for learning the joint distribution of coefficient-solution pairs directly from partial observations, without requiring a single complete observation. The key idea is to randomly mask a subset of already-observed measurements and supervise on them, so the model cannot distinguish \"truly unobserved\" from \"artificially unobserved\", and must produce plausible predictions everywhere. Ambient Physics achieves state-of-the-art reconstruction performance. Compared with prior diffusion-based methods, it achieves a 62.51$\\%$ reduction in average overall error while using 125$\\times$ fewer function evaluations. We also identify a \"one-point transition\": masking a single already-observed point enables learning from partial observations across architectures and measurement patterns. Ambient Physics thus enables scientific progress in settings where complete observations are unavailable.",
      "authors": [
        "Harris Abdul Majid",
        "Giannis Daras",
        "Francesco Tudisco",
        "Steven McDonagh"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-14 20:23:58+00:00",
      "link": "https://arxiv.org/pdf/2602.13873v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13871v1",
      "title": "Ensemble-Conditional Gaussian Processes (Ens-CGP): Representation, Geometry, and Inference",
      "abstract": "We formulate Ensemble-Conditional Gaussian Processes (Ens-CGP), a finite-dimensional synthesis that centers ensemble-based inference on the conditional Gaussian law. Conditional Gaussian processes (CGP) arise directly from Gaussian processes under conditioning and, in linear-Gaussian settings, define the full posterior distribution for a Gaussian prior and linear observations. Classical Kalman filtering is a recursive algorithm that computes this same conditional law under dynamical assumptions; the conditional Gaussian law itself is therefore the underlying representational object, while the filter is one computational realization. In this sense, CGP provides the probabilistic foundation for Kalman-type methods as well as equivalent formulations as a strictly convex quadratic program (MAP estimation), RKHS-regularized regression, and classical regularization. Ens-CGP is the ensemble instantiation of this object, obtained by treating empirical ensemble moments as a (possibly low-rank) Gaussian prior and performing exact conditioning. By separating representation (GP -> CGP -> Ens-CGP) from computation (Kalman filters, EnKF variants, and iterative ensemble schemes), the framework links an earlier-established representational foundation for inference to ensemble-derived priors and clarifies the relationships among probabilistic, variational, and ensemble perspectives.",
      "authors": [
        "Sai Ravela",
        "Jae Deok Kim",
        "Kenneth Gee",
        "Xingjian Yan",
        "Samson Mercier",
        "Lubna Albarghouty",
        "Anamitra Saha"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "cs.IT",
        "cs.LG",
        "math.OC",
        "stat.AP",
        "stat.ML"
      ],
      "published": "2026-02-14 20:00:43+00:00",
      "link": "https://arxiv.org/pdf/2602.13871v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13870v1",
      "title": "ADAB: Arabic Dataset for Automated Politeness Benchmarking -- A Large-Scale Resource for Computational Sociopragmatics",
      "abstract": "The growing importance of culturally-aware natural language processing systems has led to an increasing demand for resources that capture sociopragmatic phenomena across diverse languages. Nevertheless, Arabic-language resources for politeness detection remain under-explored, despite the rich and complex politeness expressions embedded in Arabic communication. In this paper, we introduce ADAB (Arabic Politeness Dataset), a new annotated Arabic dataset collected from four online platforms, including social media, e-commerce, and customer service domains, covering Modern Standard Arabic and multiple dialects (Gulf, Egyptian, Levantine, and Maghrebi). The dataset was annotated based on Arabic linguistic traditions and pragmatic theory, resulting in three classes: polite, impolite, and neutral. It contains 10,000 samples with linguistic feature annotations across 16 politeness categories and achieves substantial inter-annotator agreement (kappa = 0.703). We benchmark 40 model configurations, including traditional machine learning, transformer-based models, and large language models. The dataset aims to support research on politeness-aware Arabic NLP.",
      "authors": [
        "Hend Al-Khalifa",
        "Nadia Ghezaiel",
        "Maria Bounnit",
        "Hend Hamed Alhazmi",
        "Noof Abdullah Alfear",
        "Reem Fahad Alqifari",
        "Ameera Masoud Almasoud",
        "Sharefah Ahmed Al-Ghamdi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-14 19:58:53+00:00",
      "link": "https://arxiv.org/pdf/2602.13870v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13867v1",
      "title": "Bridging the Multilingual Safety Divide: Efficient, Culturally-Aware Alignment for Global South Languages",
      "abstract": "Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a handful of high-resource languages, implicitly assuming safety and factuality ''transfer'' across languages. Evidence increasingly shows they do not. We synthesize recent findings indicating that (i) safety guardrails weaken sharply on low-resource and code-mixed inputs, (ii) culturally harmful behavior can persist even when standard toxicity scores look acceptable, and (iii) English-only knowledge edits and safety patches often fail to carry over to low-resource languages. In response, we outline a practical agenda for researchers and students in the Global South: parameter-efficient safety steering, culturally grounded evaluation and preference data, and participatory workflows that empower local communities to define and mitigate harm. Our aim is to make multilingual safety a core requirement-not an add-on-for equitable AI in underrepresented regions.",
      "authors": [
        "Somnath Banerjee",
        "Rima Hazra",
        "Animesh Mukherjee"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-14 19:56:40+00:00",
      "link": "https://arxiv.org/pdf/2602.13867v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13864v1",
      "title": "Evolving Multi-Channel Confidence-Aware Activation Functions for Missing Data with Channel Propagation",
      "abstract": "Learning in the presence of missing data can result in biased predictions and poor generalizability, among other difficulties, which data imputation methods only partially address. In neural networks, activation functions significantly affect performance yet typical options (e.g., ReLU, Swish) operate only on feature values and do not account for missingness indicators or confidence scores. We propose Three-Channel Evolved Activations (3C-EA), which we evolve using Genetic Programming to produce multivariate activation functions f(x, m, c) in the form of trees that take (i) the feature value x, (ii) a missingness indicator m, and (iii) an imputation confidence score c. To make these activations useful beyond the input layer, we introduce ChannelProp, an algorithm that deterministically propagates missingness and confidence values via linear layers based on weight magnitudes, retaining reliability signals throughout the network. We evaluate 3C-EA and ChannelProp on datasets with natural and injected (MCAR/MAR/MNAR) missingness at multiple rates under identical preprocessing and splits. Results indicate that integrating missingness and confidence inputs into the activation search improves classification performance under missingness.",
      "authors": [
        "Naeem Shahabi Sani",
        "Ferial Najiantabriz",
        "Shayan Shafaei",
        "Dean F. Hougen"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "published": "2026-02-14 19:52:10+00:00",
      "link": "https://arxiv.org/pdf/2602.13864v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13860v1",
      "title": "Tutoring Large Language Models to be Domain-adaptive, Precise, and Safe",
      "abstract": "The overarching research direction of this work is the development of a ''Responsible Intelligence'' framework designed to reconcile the immense generative power of Large Language Models (LLMs) with the stringent requirements of real-world deployment. As these models become a transformative force in artificial intelligence, there is an urgent need to move beyond general-purpose architectures toward systems that are contextually aware, inherently safer, and deeply respectful of global cultural nuances. This research navigates three interconnected threads: domain adaptation to ensure technical precision, ethical rigor to mitigate adversarial vulnerabilities, and cultural/multilingual alignment to promote global inclusivity. The methodological trajectory moves from classical supervised adaptation for task-specific demands to decoding-time alignment for safety, finally leveraging human feedback and preference modeling to achieve sociolinguistic acuity.",
      "authors": [
        "Somnath Banerjee"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-14 19:45:13+00:00",
      "link": "https://arxiv.org/pdf/2602.13860v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13855v1",
      "title": "From Fluent to Verifiable: Claim-Level Auditability for Deep Research Agents",
      "abstract": "A deep research agent produces a fluent scientific report in minutes; a careful reader then tries to verify the main claims and discovers the real cost is not reading, but tracing: which sentence is supported by which passage, what was ignored, and where evidence conflicts. We argue that as research generation becomes cheap, auditability becomes the bottleneck, and the dominant risk shifts from isolated factual errors to scientifically styled outputs whose claim-evidence links are weak, missing, or misleading. This perspective proposes claim-level auditability as a first-class design and evaluation target for deep research agents, distills recurring long-horizon failure modes (objective drift, transient constraints, and unverifiable inference), and introduces the Auditable Autonomous Research (AAR) standard, a compact measurement framework that makes auditability testable via provenance coverage, provenance soundness, contradiction transparency, and audit effort. We then argue for semantic provenance with protocolized validation: persistent, queryable provenance graphs that encode claim--evidence relations (including conflicts) and integrate continuous validation during synthesis rather than after publication, with practical instrumentation patterns to support deployment at scale.",
      "authors": [
        "Razeen A Rasheed",
        "Somnath Banerjee",
        "Animesh Mukherjee",
        "Rima Hazra"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.MA"
      ],
      "published": "2026-02-14 19:39:15+00:00",
      "link": "https://arxiv.org/pdf/2602.13855v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13848v1",
      "title": "Testing For Distribution Shifts with Conditional Conformal Test Martingales",
      "abstract": "We propose a sequential test for detecting arbitrary distribution shifts that allows conformal test martingales (CTMs) to work under a fixed, reference-conditional setting. Existing CTM detectors construct test martingales by continually growing a reference set with each incoming sample, using it to assess how atypical the new sample is relative to past observations. While this design yields anytime-valid type-I error control, it suffers from test-time contamination: after a change, post-shift observations enter the reference set and dilute the evidence for distribution shift, increasing detection delay and reducing power.   In contrast, our method avoids contamination by design by comparing each new sample to a fixed null reference dataset. Our main technical contribution is a robust martingale construction that remains valid conditional on the null reference data, achieved by explicitly accounting for the estimation error in the reference distribution induced by the finite reference set. This yields anytime-valid type-I error control together with guarantees of asymptotic power one and bounded expected detection delay. Empirically, our method detects shifts faster than standard CTMs, providing a powerful and reliable distribution-shift detector.",
      "authors": [
        "Shalev Shaer",
        "Yarin Bar",
        "Drew Prinster",
        "Yaniv Romano"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-14 18:47:26+00:00",
      "link": "https://arxiv.org/pdf/2602.13848v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13847v2",
      "title": "Causally constrained reduced-order neural models of complex turbulent dynamical systems",
      "abstract": "We introduce a flexible framework based on response theory and score matching to suppress spurious, noncausal dependencies in reduced-order neural emulators of turbulent systems, focusing on climate dynamics as a proof-of-concept. We showcase the approach using the stochastic Charney-DeVore model as a relevant prototype for low-frequency atmospheric variability. We show that the resulting causal constraints enhance neural emulators' ability to respond to both weak and strong external forcings, despite being trained exclusively on unforced data. The approach is broadly applicable to modeling complex turbulent dynamical systems in reduced spaces and can be readily integrated into general neural network architectures.",
      "authors": [
        "Fabrizio Falasca",
        "Laure Zanna"
      ],
      "primary_category": "nlin.CD",
      "categories": [
        "nlin.CD",
        "cond-mat.stat-mech",
        "cs.LG",
        "physics.ao-ph"
      ],
      "published": "2026-02-14 18:43:52+00:00",
      "link": "https://arxiv.org/pdf/2602.13847v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13836v1",
      "title": "Speculative Decoding with a Speculative Vocabulary",
      "abstract": "Speculative decoding has rapidly emerged as a leading approach for accelerating language model (LM) inference, as it offers substantial speedups while yielding identical outputs. This relies upon a small draft model, tasked with predicting the outputs of the target model. State-of-the-art speculative decoding methods use a draft model consisting of a single decoder layer and output embedding matrix, with the latter dominating drafting time for the latest LMs. Recent work has sought to address this output distribution bottleneck by reducing the vocabulary of the draft model. Although this can improve throughput, it compromises speculation effectiveness when the target token is out-of-vocabulary. In this paper, we argue for vocabulary speculation as an alternative to a reduced vocabulary. We propose SpecVocab, an efficient and effective method that selects a vocabulary subset per decoding step. Across a variety of tasks, we demonstrate that SpecVocab can achieve a higher acceptance length than state-of-the-art speculative decoding approach, EAGLE-3. Notably, this yields up to an 8.1% increase in average throughput over EAGLE-3.",
      "authors": [
        "Miles Williams",
        "Young D. Kwon",
        "Rui Li",
        "Alexandros Kouris",
        "Stylianos I. Venieris"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-14 16:10:00+00:00",
      "link": "https://arxiv.org/pdf/2602.13836v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13830v1",
      "title": "A Tale of Two Graphs: Separating Knowledge Exploration from Outline Structure for Open-Ended Deep Research",
      "abstract": "Open-Ended Deep Research (OEDR) pushes LLM agents beyond short-form QA toward long-horizon workflows that iteratively search, connect, and synthesize evidence into structured reports. However, existing OEDR agents largely follow either linear ``search-then-generate'' accumulation or outline-centric planning. The former suffers from lost-in-the-middle failures as evidence grows, while the latter relies on the LLM to implicitly infer knowledge gaps from the outline alone, providing weak supervision for identifying missing relations and triggering targeted exploration. We present DualGraph memory, an architecture that separates what the agent knows from how it writes. DualGraph maintains two co-evolving graphs: an Outline Graph (OG), and a Knowledge Graph (KG), a semantic memory that stores fine-grained knowledge units, including core entities, concepts, and their relations. By analyzing the KG topology together with structural signals from the OG, DualGraph generates targeted search queries, enabling more efficient and comprehensive iterative knowledge-driven exploration and refinement. Across DeepResearch Bench, DeepResearchGym, and DeepConsult, DualGraph consistently outperforms state-of-the-art baselines in report depth, breadth, and factual grounding; for example, it reaches a 53.08 RACE score on DeepResearch Bench with GPT-5. Moreover, ablation studies confirm the central role of the dual-graph design.",
      "authors": [
        "Zhuofan Shi",
        "Ming Ma",
        "Zekun Yao",
        "Fangkai Yang",
        "Jue Zhang",
        "Dongge Han",
        "Victor Rühle",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-14 15:54:38+00:00",
      "link": "https://arxiv.org/pdf/2602.13830v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13823v1",
      "title": "Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings",
      "abstract": "Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.",
      "authors": [
        "Haonan Jiang",
        "Yuji Wang",
        "Yongjie Zhu",
        "Xin Lu",
        "Wenyu Qin",
        "Meng Wang",
        "Pengfei Wan",
        "Yansong Tang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-14 15:35:03+00:00",
      "link": "https://arxiv.org/pdf/2602.13823v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13817v1",
      "title": "What happens when reviewers receive AI feedback in their reviews?",
      "abstract": "AI is reshaping academic research, yet its role in peer review remains polarising and contentious. Advocates see its potential to reduce reviewer burden and improve quality, while critics warn of risks to fairness, accountability, and trust. At ICLR 2025, an official AI feedback tool was deployed to provide reviewers with post-review suggestions. We studied this deployment through surveys and interviews, investigating how reviewers engaged with the tool and perceived its usability and impact. Our findings surface both opportunities and tensions when AI augments in peer review. This work contributes the first empirical evidence of such an AI tool in a live review process, documenting how reviewers respond to AI-generated feedback in a high-stakes review context. We further offer design implications for AI-assisted reviewing that aim to enhance quality while safeguarding human expertise, agency, and responsibility.",
      "authors": [
        "Shiping Chen",
        "Shu Zhong",
        "Duncan P. Brumby",
        "Anna L. Cox"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2026-02-14 15:22:33+00:00",
      "link": "https://arxiv.org/pdf/2602.13817v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13816v1",
      "title": "The acquisition of English irregular inflections by Yemeni L1 Arabic learners: A Universal Grammar approach",
      "abstract": "This study examines the acquisition of English irregular inflections by Yemeni learners of English as a second language (L2), utilizing a Universal Grammar (UG) approach. Within the UG approach, the study considers Feature Reassembly Hypothesis (FRH) (Lardiere, 2008, 2009) part of UG, focusing on the roles of first language (L1) transfer and L2 developmental influence. It analyzes learner errors across two developmental stages. Stage 1 data reveal a dominant influence of L1 transfer, particularly in phonological and structural mismatches, while stage 2 data demonstrate increased learner sensitivity to UG properties and morphological reconfiguration toward the target language. Findings reveal that errors in irregular inflectional morphology are attributed to both interlingual and intralingual sources, with overgeneralization of L2 rules as a common developmental strategy. Statistical analysis, including a one-way ANOVA, indicates significant improvement in the production of well-formed irregular inflections from stage 1 to stage 2, underscoring learners' continued access to UG. However, persistent difficulties with consonant change, zero-morpheme, and -a plural inflections suggest that limited exposure, ineffective input modeling, and insufficient instructional quality constrain full UG access. The study concludes that while L1 transfer and L2 developmental factors influence initial stages of acquisition, appropriate linguistic input and instruction are critical for facilitating UG-driven feature reassembly in adult L2 learners.",
      "authors": [
        "Muneef Y. Alsawsh",
        "Mohammed Q. Shormani"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-14 15:16:59+00:00",
      "link": "https://arxiv.org/pdf/2602.13816v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13813v1",
      "title": "Pawsterior: Variational Flow Matching for Structured Simulation-Based Inference",
      "abstract": "We introduce Pawsterior, a variational flow-matching framework for improved and extended simulation-based inference (SBI). Many SBI problems involve posteriors constrained by structured domains, such as bounded physical parameters or hybrid discrete-continuous variables, yet standard flow-matching methods typically operate in unconstrained spaces. This mismatch leads to inefficient learning and difficulty respecting physical constraints. Our contributions are twofold. First, generalizing the geometric inductive bias of CatFlow, we formalize endpoint-induced affine geometric confinement, a principle that incorporates domain geometry directly into the inference process via a two-sided variational model. This formulation improves numerical stability during sampling and leads to consistently better posterior fidelity, as demonstrated by improved classifier two-sample test performance across standard SBI benchmarks. Second, and more importantly, our variational parameterization enables SBI tasks involving discrete latent structure (e.g., switching systems) that are fundamentally incompatible with conventional flow-matching approaches. By addressing both geometric constraints and discrete latent structure, Pawsterior extends flow-matching to a broader class of structured SBI problems that were previously inaccessible.",
      "authors": [
        "Jorge Carrasco-Pollo",
        "Floor Eijkelboom",
        "Jan-Willem van de Meent"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-14 14:54:44+00:00",
      "link": "https://arxiv.org/pdf/2602.13813v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13812v2",
      "title": "DTBench: A Synthetic Benchmark for Document-to-Table Extraction",
      "abstract": "Document-to-table (Doc2Table) extraction derives structured tables from unstructured documents under a target schema, enabling reliable and verifiable SQL-based data analytics. Although large language models (LLMs) have shown promise in flexible information extraction, their ability to produce precisely structured tables remains insufficiently understood, particularly for indirect extraction that requires complex capabilities such as reasoning and conflict resolution. Existing benchmarks neither explicitly distinguish nor comprehensively cover the diverse capabilities required in Doc2Table extraction. We argue that a capability-aware benchmark is essential for systematic evaluation. However, constructing such benchmarks using human-annotated document-table pairs is costly, difficult to scale, and limited in capability coverage. To address this, we adopt a reverse Table2Doc paradigm and design a multi-agent synthesis workflow to generate documents from ground-truth tables. Based on this approach, we present DTBench, a synthetic benchmark that adopts a proposed two-level taxonomy of Doc2Table capabilities, covering 5 major categories and 13 subcategories. We evaluate several mainstream LLMs on DTBench, and demonstrate substantial performance gaps across models, as well as persistent challenges in reasoning, faithfulness, and conflict resolution. DTBench provides a comprehensive testbed for data generation and evaluation, facilitating future research on Doc2Table extraction. The benchmark is publicly available at https://github.com/ZJU-DAILY/DTBench.",
      "authors": [
        "Yuxiang Guo",
        "Zhuoran Du",
        "Nan Tang",
        "Kezheng Tang",
        "Congcong Ge",
        "Yunjun Gao"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2026-02-14 14:52:36+00:00",
      "link": "https://arxiv.org/pdf/2602.13812v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13807v1",
      "title": "AnomaMind: Agentic Time Series Anomaly Detection with Tool-Augmented Reasoning",
      "abstract": "Time series anomaly detection is critical in many real-world applications, where effective solutions must localize anomalous regions and support reliable decision-making under complex settings. However, most existing methods frame anomaly detection as a purely discriminative prediction task with fixed feature inputs, rather than an evidence-driven diagnostic process. As a result, they often struggle when anomalies exhibit strong context dependence or diverse patterns. We argue that these limitations stem from the lack of adaptive feature preparation, reasoning-aware detection, and iterative refinement during inference. To address these challenges, we propose AnomaMind, an agentic time series anomaly detection framework that reformulates anomaly detection as a sequential decision-making process. AnomaMind operates through a structured workflow that progressively localizes anomalous intervals in a coarse-to-fine manner, augments detection through multi-turn tool interactions for adaptive feature preparation, and refines anomaly decisions via self-reflection. The workflow is supported by a set of reusable tool engines, enabling context-aware diagnostic analysis. A key design of AnomaMind is an explicitly designed hybrid inference mechanism for tool-augmented anomaly detection. In this mechanism, general-purpose models are responsible for autonomous tool interaction and self-reflective refinement, while core anomaly detection decisions are learned through reinforcement learning under verifiable workflow-level feedback, enabling task-specific optimization within a flexible reasoning framework. Extensive experiments across diverse settings demonstrate that AnomaMind consistently improves anomaly detection performance. The code is available at https://anonymous.4open.science/r/AnomaMind.",
      "authors": [
        "Xiaoyu Tao",
        "Yuchong Wu",
        "Mingyue Cheng",
        "Ze Guo",
        "Tian Gao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-14 14:35:34+00:00",
      "link": "https://arxiv.org/pdf/2602.13807v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13792v1",
      "title": "StackingNet: Collective Inference Across Independent AI Foundation Models",
      "abstract": "Artificial intelligence built on large foundation models has transformed language understanding, vision and reasoning, yet these systems remain isolated and cannot readily share their capabilities. Integrating the complementary strengths of such independent foundation models is essential for building trustworthy intelligent systems. Despite rapid progress in individual model design, there is no established approach for coordinating such black-box heterogeneous models. Here we show that coordination can be achieved through a meta-ensemble framework termed StackingNet, which draws on principles of collective intelligence to combine model predictions during inference. StackingNet improves accuracy, reduces bias, enables reliability ranking, and identifies or prunes models that degrade performance, all operating without access to internal parameters or training data. Across tasks involving language comprehension, visual estimation, and academic paper rating, StackingNet consistently improves accuracy, robustness, and fairness, compared with individual models and classic ensembles. By turning diversity from a source of inconsistency into collaboration, StackingNet establishes a practical foundation for coordinated artificial intelligence, suggesting that progress may emerge from not only larger single models but also principled cooperation among many specialized ones.",
      "authors": [
        "Siyang Li",
        "Chenhao Liu",
        "Dongrui Wu",
        "Zhigang Zeng",
        "Lieyun Ding"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-14 14:12:43+00:00",
      "link": "https://arxiv.org/pdf/2602.13792v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13791v1",
      "title": "MechPert: Mechanistic Consensus as an Inductive Bias for Unseen Perturbation Prediction",
      "abstract": "Predicting transcriptional responses to unseen genetic perturbations is essential for understanding gene regulation and prioritizing large-scale perturbation experiments. Existing approaches either rely on static, potentially incomplete knowledge graphs, or prompt language models for functionally similar genes, retrieving associations shaped by symmetric co-occurrence in scientific text rather than directed regulatory logic. We introduce MechPert, a lightweight framework that encourages LLM agents to generate directed regulatory hypotheses rather than relying solely on functional similarity. Multiple agents independently propose candidate regulators with associated confidence scores; these are aggregated through a consensus mechanism that filters spurious associations, producing weighted neighborhoods for downstream prediction. We evaluate MechPert on Perturb-seq benchmarks across four human cell lines. For perturbation prediction in low-data regimes ($N=50$ observed perturbations), MechPert improves Pearson correlation by up to 10.5\\% over similarity-based baselines. For experimental design, MechPert-selected anchor genes outperform standard network centrality heuristics by up to 46\\% in well-characterized cell lines.",
      "authors": [
        "Marc Boubnovski Martell",
        "Josefa Lia Stoisser",
        "Lawrence Phillips",
        "Aditya Misra",
        "Robert Kitchen",
        "Jesper Ferkinghoff-Borg",
        "Jialin Yu",
        "Philip Torr",
        "Kaspar Märten"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-14 14:12:38+00:00",
      "link": "https://arxiv.org/pdf/2602.13791v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13790v1",
      "title": "How Do Lexical Senses Correspond Between Spoken German and German Sign Language?",
      "abstract": "Sign language lexicographers construct bilingual dictionaries by establishing word-to-sign mappings, where polysemous and homonymous words corresponding to different signs across contexts are often underrepresented. A usage-based approach examining how word senses map to signs can identify such novel mappings absent from current dictionaries, enriching lexicographic resources. We address this by analyzing German and German Sign Language (Deutsche Gebärdensprache, DGS), manually annotating 1,404 word use-to-sign ID mappings derived from 32 words from the German Word Usage Graph (D-WUG) and 49 signs from the Digital Dictionary of German Sign Language (DW-DGS). We identify three correspondence types: Type 1 (one-to-many), Type 2 (many-to-one), and Type 3 (one-to-one), plus No Match cases. We evaluate computational methods: Exact Match (EM) and Semantic Similarity (SS) using SBERT embeddings. SS substantially outperforms EM overall 88.52% vs. 71.31%), with dramatic gains for Type 1 (+52.1 pp). Our work establishes the first annotated dataset for cross-modal sense correspondence and reveals which correspondence patterns are computationally identifiable. Our code and dataset are made publicly available.",
      "authors": [
        "Melis Çelikkol",
        "Wei Zhao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-14 14:12:25+00:00",
      "link": "https://arxiv.org/pdf/2602.13790v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13783v1",
      "title": "MEMTS: Internalizing Domain Knowledge via Parameterized Memory for Retrieval-Free Domain Adaptation of Time Series Foundation Models",
      "abstract": "While Time Series Foundation Models (TSFMs) have demonstrated exceptional performance in generalized forecasting, their performance often degrades significantly when deployed in real-world vertical domains characterized by temporal distribution shifts and domain-specific periodic structures. Current solutions are primarily constrained by two paradigms: Domain-Adaptive Pretraining (DAPT), which improves short-term domain fitting but frequently disrupts previously learned global temporal patterns due to catastrophic forgetting; and Retrieval-Augmented Generation (RAG), which incorporates external knowledge but introduces substantial retrieval overhead. This creates a severe scalability bottleneck that fails to meet the high-efficiency requirements of real-time stream processing. To break this impasse, we propose Memory for Time Series (MEMTS), a lightweight and plug-and-play method for retrieval-free domain adaptation in time series forecasting. The key component of MEMTS is a Knowledge Persistence Module (KPM), which internalizes domain-specific temporal dynamics, such as recurring seasonal patterns and trends into a compact set of learnable latent prototypes. In doing so, it transforms fragmented historical observations into continuous, parameterized knowledge representations. This paradigm shift enables MEMTS to achieve accurate domain adaptation with constant-time inference and near-zero latency, while effectively mitigating catastrophic forgetting of general temporal patterns, all without requiring any architectural modifications to the frozen TSFM backbone. Extensive experiments on multiple datasets demonstrate the SOTA performance of MEMTS.",
      "authors": [
        "Xiaoyun Yu",
        "Li fan",
        "Xiangfei Qiu",
        "Nanqing Dong",
        "Yonggui Huang",
        "Honggang Qi",
        "Geguang Pu",
        "Wanli Ouyang",
        "Xi Chen",
        "Jilin Hu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-14 14:00:06+00:00",
      "link": "https://arxiv.org/pdf/2602.13783v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13780v1",
      "title": "Foundation Model-Driven Semantic Change Detection in Remote Sensing Imagery",
      "abstract": "Remote sensing (RS) change detection methods can extract critical information on surface dynamics and are an essential means for humans to understand changes in the earth's surface and environment. Among these methods, semantic change detection (SCD) can more effectively interpret the multi-class information contained in bi-temporal RS imagery, providing semantic-level predictions that support dynamic change monitoring. However, due to the limited semantic understanding capability of the model and the inherent complexity of the SCD tasks, existing SCD methods face significant challenges in both performance and paradigm complexity. In this paper, we propose PerASCD, a SCD method driven by RS foundation model PerA, designed to enhance the multi-scale semantic understanding and overall performance. We introduce a modular Cascaded Gated Decoder (CG-Decoder) that simplifies complex SCD decoding pipelines while promoting effective multi-level feature interaction and fusion. In addition, we propose a Soft Semantic Consistency Loss (SSCLoss) to mitigate the numerical instability commonly encountered during SCD training. We further explore the applicability of multiple existing RS foundation models on the SCD task when equipped with the proposed decoder. Experimental results demonstrate that our decoder not only effectively simplifies the paradigm of SCD, but also achieves seamless adaptation across various vision encoders. Our method achieves state-of-the-art (SOTA) performance on two public benchmark datasets, validating its effectiveness. The code is available at https://github.com/SathShen/PerASCD.git.",
      "authors": [
        "Hengtong Shen",
        "Li Yan",
        "Hong Xie",
        "Yaxuan Wei",
        "Xinhao Li",
        "Wenfei Shen",
        "Peixian Lv",
        "Fei Tan"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-14 13:56:31+00:00",
      "link": "https://arxiv.org/pdf/2602.13780v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13773v1",
      "title": "On Representation Redundancy in Large-Scale Instruction Tuning Data Selection",
      "abstract": "Data quality is a crucial factor in large language models training. While prior work has shown that models trained on smaller, high-quality datasets can outperform those trained on much larger but noisy or low-quality corpora, systematic methods for industrial-scale data selection in instruction tuning remain underexplored. In this work, we study instruction-tuning data selection through the lens of semantic representation similarity and identify a key limitation of state-of-the-art LLM encoders: they produce highly redundant semantic embeddings. To mitigate this redundancy, we propose Compressed Representation Data Selection (CRDS), a novel framework with two variants. CRDS-R applies Rademacher random projection followed by concatenation of transformer hidden-layer representations, while CRDS-W employs whitening-based dimensionality reduction to improve representational quality. Experimental results demonstrate that both variants substantially enhance data quality and consistently outperform state-of-the-art representation-based selection methods. Notably, CRDS-W achieves strong performance using only 3.5% of the data, surpassing the full-data baseline by an average of 0.71% across four datasets. Our code is available at https://github.com/tdano1/CRDS.",
      "authors": [
        "Youwei Shu",
        "Shaomian Zheng",
        "Dingnan Jin",
        "Wenjie Qu",
        "Ziyao Guo",
        "Qing Cui",
        "Jun Zhou",
        "Jiaheng Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-14 13:35:34+00:00",
      "link": "https://arxiv.org/pdf/2602.13773v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13771v1",
      "title": "SRA: Semantic Relation-Aware Flowchart Question Answering",
      "abstract": "Flowchart Question Answering (FlowchartQA) is a multi-modal task that automatically answers questions conditioned on graphic flowcharts. Current studies convert flowcharts into interlanguages (e.g., Graphviz) for Question Answering (QA), which effectively bridge modal gaps between questions and flowcharts. More importantly, they reveal the link relations between nodes in the flowchart, facilitating a shallow relation reasoning during tracing answers. However, the existing interlanguages still lose sight of intricate semantic/logic relationships such as Conditional and Causal relations. This hinders the deep reasoning for complex questions. To address the issue, we propose a novel Semantic Relation-Aware (SRA) FlowchartQA approach. It leverages Large Language Model (LLM) to detect the discourse semantic relations between nodes, by which a link-based interlanguage is upgraded to the semantic relation based interlanguage. In addition, we conduct an interlanguage-controllable reasoning process. In this process, the question intention is analyzed with the aim to determine the depth of reasoning (Shallow or Deep reasoning), as well as the well-matched interlanguage. We experiment on the benchmark dataset FlowVQA. The test results show that SRA yields widespread improvements when upgrading different interlanguages like Graphviz, Mermaid and Plantuml",
      "authors": [
        "Xinyu Li",
        "Bowei Zou",
        "Yuchong Chen",
        "Yifan Fan",
        "Yu Hong"
      ],
      "primary_category": "cs.MM",
      "categories": [
        "cs.MM"
      ],
      "published": "2026-02-14 13:34:01+00:00",
      "link": "https://arxiv.org/pdf/2602.13771v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13769v1",
      "title": "OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery",
      "abstract": "Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.",
      "authors": [
        "Qi Liu",
        "Wanjing Ma"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.NE"
      ],
      "published": "2026-02-14 13:32:03+00:00",
      "link": "https://arxiv.org/pdf/2602.13769v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13759v1",
      "title": "Discrete Double-Bracket Flows for Isotropic-Noise Invariant Eigendecomposition",
      "abstract": "We study matrix-free eigendecomposition under a matrix-vector product (MVP) oracle, where each step observes a covariance operator $C_k = C_{sig} + σ_k^2 I + E_k$. Standard stochastic approximation methods either use fixed steps that couple stability to $\\|C_k\\|_2$, or adapt steps in ways that slow down due to vanishing updates. We introduce a discrete double-bracket flow whose generator is invariant to isotropic shifts, yielding pathwise invariance to $σ_k^2 I$ at the discrete-time level. The resulting trajectory and a maximal stable step size $η_{max} \\propto 1/\\|C_e\\|_2^2$ depend only on the trace-free covariance $C_e$. We establish global convergence via strict-saddle geometry for the diagonalization objective and an input-to-state stability analysis, with sample complexity scaling as $O(\\|C_e\\|_2^2 / (Δ^2 ε))$ under trace-free perturbations. An explicit characterization of degenerate blocks yields an accelerated $O(\\log(1/ζ))$ saddle-escape rate and a high-probability finite-time convergence guarantee.",
      "authors": [
        "ZhiMing Li",
        "JiaHe Feng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.NA",
        "math.OC"
      ],
      "published": "2026-02-14 13:09:29+00:00",
      "link": "https://arxiv.org/pdf/2602.13759v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13758v1",
      "title": "OmniScience: A Large-scale Multi-modal Dataset for Scientific Image Understanding",
      "abstract": "Multimodal Large Language Models demonstrate strong performance on natural image understanding, yet exhibit limited capability in interpreting scientific images, including but not limited to schematic diagrams, experimental characterizations, and analytical charts. This limitation is particularly pronounced in open-source MLLMs. The gap largely stems from existing datasets with limited domain coverage, coarse structural annotations, and weak semantic grounding. We introduce OmniScience, a large-scale, high-fidelity multi-modal dataset comprising 1.5 million figure-caption-context triplets, spanning more than 10 major scientific disciplines. To obtain image caption data with higher information density and accuracy for multi-modal large-model training, we develop a dynamic model-routing re-captioning pipeline that leverages state-of-the-art multi-modal large language models to generate dense, self-contained descriptions by jointly synthesizing visual features, original figure captions, and corresponding in-text references authored by human scientists. The pipeline is further reinforced with rigorous quality filtering and alignment with human expert judgments, ensuring both factual accuracy and semantic completeness, and boosts the image-text multi-modal similarity score from 0.769 to 0.956. We further propose a caption QA protocol as a proxy task for evaluating visual understanding. Under this setting, Qwen2.5-VL-3B model finetuned on OmniScience show substantial gains over baselines, achieving a gain of 0.378 on MM-MT-Bench and a gain of 0.140 on MMMU.",
      "authors": [
        "Haoyi Tao",
        "Chaozheng Huang",
        "Nan Wang",
        "Han Lyu",
        "Linfeng Zhang",
        "Guolin Ke",
        "Xi Fang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-14 13:08:13+00:00",
      "link": "https://arxiv.org/pdf/2602.13758v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13748v1",
      "title": "RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction",
      "abstract": "Multimedia Event Extraction (MEE) aims to identify events and their arguments from documents that contain both text and images. It requires grounding event semantics across different modalities. Progress in MEE is limited by the lack of annotated training data. M2E2 is the only established benchmark, but it provides annotations only for evaluation. This makes direct supervised training impractical. Existing methods mainly rely on cross-modal alignment or inference-time prompting with Vision--Language Models (VLMs). These approaches do not explicitly learn structured event representations and often produce weak argument grounding in multimodal settings. To address these limitations, we propose RMPL, a Relation-aware Multi-task Progressive Learning framework for MEE under low-resource conditions. RMPL incorporates heterogeneous supervision from unimodal event extraction and multimedia relation extraction with stage-wise training. The model is first trained with a unified schema to learn shared event-centric representations across modalities. It is then fine-tuned for event mention identification and argument role extraction using mixed textual and visual data. Experiments on the M2E2 benchmark with multiple VLMs show consistent improvements across different modality settings.",
      "authors": [
        "Yongkang Jin",
        "Jianwen Luo",
        "Jingjing Wang",
        "Jianmin Yao",
        "Yu Hong"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "published": "2026-02-14 12:43:25+00:00",
      "link": "https://arxiv.org/pdf/2602.13748v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13746v1",
      "title": "Data-driven Bi-level Optimization of Thermal Power Systems with embedded Artificial Neural Networks",
      "abstract": "Industrial thermal power systems have coupled performance variables with hierarchical order of importance, making their simultaneous optimization computationally challenging or infeasible. This barrier limits the integrated and computationally scaleable operation optimization of industrial thermal power systems. To address this issue for large-scale engineering systems, we present a fully machine learning-powered bi-level optimization framework for data-driven optimization of industrial thermal power systems. The objective functions of upper and lower levels are approximated by artificial neural network (ANN) models and the lower-level problem is analytically embedded through Karush-Kuhn-Tucker (KKT) optimality conditions. The reformulated single level optimization framework integrating ANN models and KKT constraints (ANN-KKT) is validated on benchmark problems and on real-world power generation operation of 660 MW coal power plant and 395 MW gas turbine system. The results reveal a comparable solutions obtained from the proposed ANN-KKT framework to the bi-level solutions of the benchmark problems. Marginal computational time requirement (0.22 to 0.88 s) to compute optimal solutions yields 583 MW (coal) and 402 MW (gas turbine) of power output at optimal turbine heat rate of 7337 kJ/kWh and 7542 kJ/kWh, respectively. In addition, the method expands to delineate a feasible and robust operating envelope that accounts for uncertainty in operating variables while maximizing thermal efficiency in various scenarios. These results demonstrate that ANN-KKT offers a scalable and computationally efficient route for hierarchical, data-driven optimization of industrial thermal power systems, achieving energy-efficient operations of large-scale engineering systems and contributing to industry 5.0.",
      "authors": [
        "Talha Ansar",
        "Muhammad Mujtaba Abbas",
        "Ramit Debnath",
        "Vivek Dua",
        "Waqar Muhammad Ashraf"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-14 12:32:38+00:00",
      "link": "https://arxiv.org/pdf/2602.13746v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13738v1",
      "title": "OneLatent: Single-Token Compression for Visual Latent Reasoning",
      "abstract": "Chain-of-thought (CoT) prompting improves reasoning but often increases inference cost by one to two orders of magnitude. To address these challenges, we present \\textbf{OneLatent}, a framework that compresses intermediate reasoning into a single latent token via supervision from rendered CoT images and DeepSeek-OCR hidden states. By rendering textual steps into images, we obtain a deterministic supervision signal that can be inspected and audited without requiring the model to output verbose textual rationales. Across benchmarks, OneLatent reduces average output length by $11\\times$ with only a $2.21\\%$ average accuracy drop relative to textual CoT, while improving output token contribution (OTC) by $6.8\\times$. On long-chain logical reasoning, OneLatent reaches $99.80\\%$ on ProntoQA and $97.80\\%$ on ProsQA with one latent token, with compression up to $87.4\\times$, supporting compression-constrained generalization.",
      "authors": [
        "Bo Lv",
        "Yasheng Sun",
        "Junjie Wang",
        "Haoxiang Shi"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-14 12:03:28+00:00",
      "link": "https://arxiv.org/pdf/2602.13738v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13730v1",
      "title": "Discrete Gene Crossover Accelerates Solution Discovery in Quality-Diversity Algorithms",
      "abstract": "Quality-Diversity (QD) algorithms aim to discover diverse, high-performing solutions across behavioral niches. However, QD search often stagnates as incremental variation operators struggle to propagate building blocks across large populations. Existing mutation operators rely on gradual variation to solutions, limiting their ability to efficiently explore regions of the search space distant from parent solutions or to spread beneficial genetic material through the population. We propose a mutation operator which augments variation-based operators with discrete, gene-level crossover, enabling rapid recombination of elite genetic material. This crossover mechanism mirrors the biological principle of meiosis and facilitates both the direct transfer of genetic material and the exploration of novel genotype configurations beyond the existing elite hypervolume. We evaluate operators on three locomotion environments, demonstrating improvements in QD score, coverage, and max fitness, with particularly strong performance in later stages of optimization once building blocks have been established in the archive. These results show that the addition of a discrete crossover mutation provides a complementary exploration mechanism that sustains quality-diversity growth beyond the performance demonstrated by existing operators.",
      "authors": [
        "Joshua Hutchinson",
        "J. Michael Herrmann",
        "Simón C. Smith"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-14 11:44:21+00:00",
      "link": "https://arxiv.org/pdf/2602.13730v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13717v1",
      "title": "Artificial Intelligence in Secondary Education: Educational Affordances and Constraints of ChatGPT-4o Use",
      "abstract": "The purpose of this study was to examine, from the perspective of secondary education students, the educational affordances and constraints of using Artificial Intelligence (AI) in teaching and learning. The sample consisted of 45 students from the 2nd year of General Lyceum (11th grade, ages 16-17) in Greece, who, after becoming familiarized with ChatGPT-4o and completing six activities, filled in an open-ended questionnaire related to the research purpose. Open, axial, and selective coding of the data revealed that students recognize five educational affordances: the creation of new knowledge building on prior knowledge, immediate feedback, friendly interaction through messaging, ease and speed of access to information, and skills development. Concurrently, three main constraints were identified: content reliability, anxiety about AI use, and privacy concerns. The study concludes that students are positive toward AI use in education.",
      "authors": [
        "Tryfon Sivenas",
        "Panagiota Maragkaki"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY"
      ],
      "published": "2026-02-14 10:44:00+00:00",
      "link": "https://arxiv.org/pdf/2602.13717v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13713v1",
      "title": "On Theoretically-Driven LLM Agents for Multi-Dimensional Discourse Analysis",
      "abstract": "Identifying the strategic uses of reformulation in discourse remains a key challenge for computational argumentation. While LLMs can detect surface-level similarity, they often fail to capture the pragmatic functions of rephrasing, such as its role within rhetorical discourse. This paper presents a comparative multi-agent framework designed to quantify the benefits of incorporating explicit theoretical knowledge for this task. We utilise an dataset of annotated political debates to establish a new standard encompassing four distinct rephrase functions: Deintensification, Intensification, Specification, Generalisation, and Other, which covers all remaining types (D-I-S-G-O). We then evaluate two parallel LLM-based agent systems: one enhanced by argumentation theory via Retrieval-Augmented Generation (RAG), and an identical zero-shot baseline. The results reveal a clear performance gap: the RAG-enhanced agents substantially outperform the baseline across the board, with particularly strong advantages in detecting Intensification and Generalisation context, yielding an overall Macro F1-score improvement of nearly 30\\%. Our findings provide evidence that theoretical grounding is not only beneficial but essential for advancing beyond mere paraphrase detection towards function-aware analysis of argumentative discourse. This comparative multi-agent architecture represents a step towards scalable, theoretically informed computational tools capable of identifying rhetorical strategies in contemporary discourse.",
      "authors": [
        "Maciej Uberna",
        "Michał Wawer",
        "Jarosław A. Chudziak",
        "Marcin Koszowy"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-14 10:30:39+00:00",
      "link": "https://arxiv.org/pdf/2602.13713v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13710v1",
      "title": "HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models",
      "abstract": "Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existing methods fail to narrow the distribution gap between binarized and full-precision weights, causing quantization errors to accumulate under long-horizon closed-loop execution and severely degrade actions. To fill this gap, we propose HBVLA, a VLA-tailored binarization framework. First, we use a policy-aware enhanced Hessian to identify weights that are truly critical for action generation. Then, we employ a sparse orthogonal transform for non-salient weights to induce a low-entropy intermediate state. Finally, we quantize both salient and non-salient weights in the Harr domain with group-wise 1-bit quantization. We have evaluated our approach on different VLAs: on LIBERO, quantized OpenVLA-OFT retains 92.2% of full-precision performance; on SimplerEnv, quantized CogAct retains 93.6%, significantly outperforming state-of-the-art binarization methods. We further validate our method on real-world evaluation suite and the results show that HBVLA incurs only marginal success-rate degradation compared to the full-precision model, demonstrating robust deployability under tight hardware constraints. Our work provides a practical foundation for ultra-low-bit quantization of VLAs, enabling more reliable deployment on hardware-limited robotic platforms.",
      "authors": [
        "Xin Yan",
        "Zhenglin Wan",
        "Feiyang Ye",
        "Xingrui Yu",
        "Hangyu Du",
        "Yang You",
        "Ivor Tsang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-14 10:23:45+00:00",
      "link": "https://arxiv.org/pdf/2602.13710v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13704v1",
      "title": "Pailitao-VL: Unified Embedding and Reranker for Real-Time Multi-Modal Industrial Search",
      "abstract": "In this work, we presented Pailitao-VL, a comprehensive multi-modal retrieval system engineered for high-precision, real-time industrial search. We here address three critical challenges in the current SOTA solution: insufficient retrieval granularity, vulnerability to environmental noise, and prohibitive efficiency-performance gap. Our primary contribution lies in two fundamental paradigm shifts. First, we transitioned the embedding paradigm from traditional contrastive learning to an absolute ID-recognition task. Through anchoring instances to a globally consistent latent space defined by billions of semantic prototypes, we successfully overcome the stochasticity and granularity bottlenecks inherent in existing embedding solutions. Second, we evolved the generative reranker from isolated pointwise evaluation to the compare-and-calibrate listwise policy. By synergizing chunk-based comparative reasoning with calibrated absolute relevance scoring, the system achieves nuanced discriminative resolution while circumventing the prohibitive latency typically associated with conventional reranking methods. Extensive offline benchmarks and online A/B tests on Alibaba e-commerce platform confirm that Pailitao-VL achieves state-of-the-art performance and delivers substantial business impact. This work demonstrates a robust and scalable path for deploying advanced MLLM-based retrieval architectures in demanding, large-scale production environments.",
      "authors": [
        "Lei Chen",
        "Chen Ju",
        "Xu Chen",
        "Zhicheng Wang",
        "Yuheng Jiao",
        "Hongfeng Zhan",
        "Zhaoyang Li",
        "Shihao Xu",
        "Zhixiang Zhao",
        "Tong Jia",
        "Jinsong Lan",
        "Xiaoyong Zhu",
        "Bo Zheng"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-14 10:13:48+00:00",
      "link": "https://arxiv.org/pdf/2602.13704v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13695v1",
      "title": "Can a Lightweight Automated AI Pipeline Solve Research-Level Mathematical Problems?",
      "abstract": "Large language models (LLMs) have recently achieved remarkable success in generating rigorous mathematical proofs, with \"AI for Math\" emerging as a vibrant field of research. While these models have mastered competition-level benchmarks like the International Mathematical Olympiad and show promise in research applications through auto-formalization, their deployment via lightweight, natural-language pipelines for research problems remains underexplored. In this work, we demonstrate that next-generation models (e.g., Gemini 3 Pro, GPT-5.2 Pro), when integrated into a streamlined automated pipeline optimized for citation-based verification, can solve sophisticated research-grade problems. We evaluate our pipeline on two novel datasets: (1) the ICCM problem sets (comparable to the S.-T. Yau College Student Mathematics Contest) proposed by leading mathematicians, and (2) the \"First Proof\" problem set, consisting of previously unpublished research questions. Our pipeline generated candidate proofs for all problems in the first two ICCM sets and the \"First Proof\" set. The solutions for the first two ICCM sets and Problem 4 of the \"First Proof\" set have been fully verified by our team. All generated proofs have been submitted to the official organization, and our generated results are publicly available. We plan to open-source the complete pipeline methodology in due course.",
      "authors": [
        "Lve Meng",
        "Weilong Zhao",
        "Yanzhi Zhang",
        "Haoxiang Guan",
        "Jiyan He"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "math.AC",
        "math.CO",
        "math.CT"
      ],
      "published": "2026-02-14 09:36:24+00:00",
      "link": "https://arxiv.org/pdf/2602.13695v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13691v1",
      "title": "PhGPO: Pheromone-Guided Policy Optimization for Long-Horizon Tool Planning",
      "abstract": "Recent advancements in Large Language Model (LLM) agents have demonstrated strong capabilities in executing complex tasks through tool use. However, long-horizon multi-step tool planning is challenging, because the exploration space suffers from a combinatorial explosion. In this scenario, even when a correct tool-use path is found, it is usually considered an immediate reward for current training, which would not provide any reusable information for subsequent training. In this paper, we argue that historically successful trajectories contain reusable tool-transition patterns, which can be leveraged throughout the whole training process. Inspired by ant colony optimization where historically successful paths can be reflected by the pheromone, we propose Pheromone-Guided Policy Optimization (PhGPO), which learns a trajectory-based transition pattern (i.e., pheromone) from historical trajectories and then uses the learned pheromone to guide policy optimization. This learned pheromone provides explicit and reusable guidance that steers policy optimization toward historically successful tool transitions, thereby improving long-horizon tool planning. Comprehensive experimental results demonstrate the effectiveness of our proposed PhGPO.",
      "authors": [
        "Yu Li",
        "Guangfeng Cai",
        "Shengtian Yang",
        "Han Luo",
        "Shuo Han",
        "Xu He",
        "Dong Li",
        "Lei Feng"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-14 09:24:55+00:00",
      "link": "https://arxiv.org/pdf/2602.13691v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13690v1",
      "title": "Physics Aware Neural Networks: Denoising for Magnetic Navigation",
      "abstract": "Magnetic-anomaly navigation, leveraging small-scale variations in the Earth's magnetic field, is a promising alternative when GPS is unavailable or compromised. Airborne systems face a key challenge in extracting geomagnetic field data: the aircraft itself induces magnetic noise. Although the classical Tolles-Lawson model addresses this, it inadequately handles stochastically corrupted magnetic data required for navigation. To address stochastic noise, we propose a framework based on two physics-based constraints: divergence-free vector field and E(3)-equivariance. These ensure the learned magnetic field obeys Maxwell's equations and that outputs transform correctly with sensor position/orientation. The divergence-free constraint is implemented by training a neural network to output a vector potential $A$, with the magnetic field defined as its curl. For E(3)-equivariance, we use tensor products of geometric tensors representable via spherical harmonics with known rotational transformations. Enforcing physical consistency and restricting the admissible function space acts as an implicit regularizer that improves spatio-temporal performance. We present ablation studies evaluating each constraint alone and jointly across CNNs, MLPs, Liquid Time Constant models, and Contiformers. Continuous-time dynamics and long-term memory are critical for modelling magnetic time series; the Contiformer architecture, which provides both, outperforms state-of-the-art methods. To mitigate data scarcity, we generate synthetic datasets using the World Magnetic Model (WMM) with time-series conditional GANs, producing realistic, temporally consistent magnetic sequences across varied trajectories and environments. Experiments show that embedding these constraints significantly improves predictive accuracy and physical plausibility, outperforming classical and unconstrained deep learning approaches.",
      "authors": [
        "Aritra Das",
        "Yashas Shende",
        "Muskaan Chugh",
        "Reva Laxmi Chauhan",
        "Arghya Pathak",
        "Debayan Gupta"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-14 09:23:57+00:00",
      "link": "https://arxiv.org/pdf/2602.13690v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13684v1",
      "title": "On the Sparsifiability of Correlation Clustering: Approximation Guarantees under Edge Sampling",
      "abstract": "Correlation Clustering (CC) is a fundamental unsupervised learning primitive whose strongest LP-based approximation guarantees require $Θ(n^3)$ triangle inequality constraints and are prohibitive at scale. We initiate the study of \\emph{sparsification--approximation trade-offs} for CC, asking how much edge information is needed to retain LP-based guarantees. We establish a structural dichotomy between pseudometric and general weighted instances. On the positive side, we prove that the VC dimension of the clustering disagreement class is exactly $n{-}1$, yielding additive $\\varepsilon$-coresets of optimal size $\\tilde{O}(n/\\varepsilon^2)$; that at most $\\binom{n}{2}$ triangle inequalities are active at any LP vertex, enabling an exact cutting-plane solver; and that a sparsified variant of LP-PIVOT, which imputes missing LP marginals via triangle inequalities, achieves a robust $\\frac{10}{3}$-approximation (up to an additive term controlled by an empirically computable imputation-quality statistic $\\overlineΓ_w$) once $\\tildeΘ(n^{3/2})$ edges are observed, a threshold we prove is sharp. On the negative side, we show via Yao's minimax principle that without pseudometric structure, any algorithm observing $o(n)$ uniformly random edges incurs an unbounded approximation ratio, demonstrating that the pseudometric condition governs not only tractability but also the robustness of CC to incomplete information.",
      "authors": [
        "Ibne Farabi Shihab",
        "Sanjeda Akter",
        "Anuj Sharma"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-14 09:12:15+00:00",
      "link": "https://arxiv.org/pdf/2602.13684v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13680v1",
      "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
      "abstract": "Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that integrates Sliding Window Attention (SWA) with non-linear Test-Time Training (TTT) memory networks. \\textsc{AllMem} enables models to effectively scale to ultra-long contexts while mitigating catastrophic forgetting. This approach not only overcomes the representation constraints typical of linear memory models but also significantly reduces the computational and memory footprint during long-sequence inference. Furthermore, we implement a Memory-Efficient Fine-Tuning strategy to replace standard attention layers in pre-trained models with memory-augmented sliding window layers. This framework facilitates the efficient transformation of any off-the-shelf pre-trained LLM into an \\textsc{AllMem}-based architecture. Empirical evaluations confirm that our 4k window model achieves near-lossless performance on 37k LongBench with a marginal 0.83 drop compared to full attention. Furthermore, on InfiniteBench at a 128k context, our 8k window variant outperforms full attention, which validates the effectiveness of our parameterized memory in mitigating noise and maintaining robust long-range modeling without the prohibitive costs of global attention.",
      "authors": [
        "Ziming Wang",
        "Xiang Wang",
        "Kailong Peng",
        "Lang Qin",
        "Juan Gabriel Kostelec",
        "Christos Sourmpis",
        "Axel Laborieux",
        "Qinghai Guo"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-14 09:04:28+00:00",
      "link": "https://arxiv.org/pdf/2602.13680v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13675v1",
      "title": "Transferable XAI: Relating Understanding Across Domains with Explanation Transfer",
      "abstract": "Current Explainable AI (XAI) focuses on explaining a single application, but when encountering related applications, users may rely on their prior understanding from previous explanations. This leads to either overgeneralization and AI overreliance, or burdensome independent memorization. Indeed, related decision tasks can share explanatory factors, but with some notable differences; e.g., body mass index (BMI) affects the risks for heart disease and diabetes at the same rate, but chest pain is more indicative of heart disease. Similarly, models using different attributes for the same task still share signals; e.g., temperature and pressure affect air pollution but in opposite directions due to the ideal gas law. Leveraging transfer of learning, we propose Transferable XAI to enable users to transfer understanding across related domains by explaining the relationship between domain explanations using a general affine transformation framework applied to linear factor explanations. The framework supports explanation transfer across various domain types: translation for data subspace (subsuming prior work on Incremental XAI), scaling for decision task, and mapping for attributes. Focusing on task and attributes domain types, in formative and summative user studies, we investigated how well participants could understand AI decisions from one domain to another. Compared to single-domain and domain-independent explanations, Transferable XAI was the most helpful for understanding the second domain, leading to the best decision faithfulness, factor recall, and ability to relate explanations between domains. This framework contributes to improving the reusability of explanations across related AI applications by explaining factor relationships between subspaces, tasks, and attributes.",
      "authors": [
        "Fei Wang",
        "Yifan Zhang",
        "Brian Y. Lim"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2026-02-14 08:55:54+00:00",
      "link": "https://arxiv.org/pdf/2602.13675v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13671v1",
      "title": "MAS-on-the-Fly: Dynamic Adaptation of LLM-based Multi-Agent Systems at Test Time",
      "abstract": "Large Language Model (LLM)-based multi-agent systems (MAS) have emerged as a promising paradigm for solving complex tasks. However, existing works often rely on manual designs or \"one-size-fits-all\" automation, lacking dynamic adaptability after deployment. Inspired by how biological systems adapt, we introduce MASFly, a novel multi-agent framework enabling dynamic adaptation at test time. To adapt system generation, MASFly employs a retrieval-augmented SOP instantiation mechanism that leverages a self-constructed repository of successful collaboration patterns, enabling the LLM to assemble customized MASs for new queries. For adaptive execution, MASFly incorporates an experience-guided supervision mechanism, where a dedicated Watcher agent monitors system behaviors with reference to a personalized experience pool and provides real-time interventions. Extensive experiments demonstrate that MASFly achieves state-of-the-art performance, most notably a 61.7% success rate on the TravelPlanner benchmark, while exhibiting strong task adaptability and robustness.",
      "authors": [
        "Guangyi Liu",
        "Haojun Lin",
        "Huan Zeng",
        "Heng Wang",
        "Quanming Yao"
      ],
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "published": "2026-02-14 08:38:13+00:00",
      "link": "https://arxiv.org/pdf/2602.13671v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13670v1",
      "title": "Advancing Analytic Class-Incremental Learning through Vision-Language Calibration",
      "abstract": "Class-incremental learning (CIL) with pre-trained models (PTMs) faces a critical trade-off between efficient adaptation and long-term stability. While analytic learning enables rapid, recursive closed-form updates, its efficacy is often compromised by accumulated errors and feature incompatibility. In this paper, we first conduct a systematic study to dissect the failure modes of PTM-based analytic CIL, identifying representation rigidity as the primary bottleneck. Motivated by these insights, we propose \\textbf{VILA}, a novel dual-branch framework that advances analytic CIL via a two-level vision-language calibration strategy. Specifically, we coherently fuse plastic, task-adapted features with a frozen, universal semantic anchor at the feature level through geometric calibration, and leverage cross-modal priors at the decision level to rectify prediction bias. This confluence maintains analytic-learning's extreme efficiency while overcoming its inherent brittleness. Extensive experiments across eight benchmarks demonstrate that VILA consistently yields superior performance, particularly in fine-grained and long-sequence scenarios. Our framework harmonizes high-fidelity prediction with the simplicity of analytic learning. Our code is available at https://github.com/byzhaoAI/VILA",
      "authors": [
        "Binyu Zhao",
        "Wei Zhang",
        "Xingrui Yu",
        "Zhaonian Zou",
        "Ivor Tsang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-14 08:32:51+00:00",
      "link": "https://arxiv.org/pdf/2602.13670v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13665v1",
      "title": "HyFunc: Accelerating LLM-based Function Calls for Agentic AI through Hybrid-Model Cascade and Dynamic Templating",
      "abstract": "While agentic AI systems rely on LLMs to translate user intent into structured function calls, this process is fraught with computational redundancy, leading to high inference latency that hinders real-time applications. This paper identifies and addresses three key redundancies: (1) the redundant processing of a large library of function descriptions for every request; (2) the redundant use of a large, slow model to generate an entire, often predictable, token sequence; and (3) the redundant generation of fixed, boilerplate parameter syntax. We introduce HyFunc, a novel framework that systematically eliminates these inefficiencies. HyFunc employs a hybrid-model cascade where a large model distills user intent into a single \"soft token.\" This token guides a lightweight retriever to select relevant functions and directs a smaller, prefix-tuned model to generate the final call, thus avoiding redundant context processing and full-sequence generation by the large model. To eliminate syntactic redundancy, our \"dynamic templating\" technique injects boilerplate parameter syntax on-the-fly within an extended vLLM engine. To avoid potential limitations in generalization, we evaluate HyFunc on an unseen benchmark dataset, BFCL. Experimental results demonstrate that HyFunc achieves an excellent balance between efficiency and performance. It achieves an inference latency of 0.828 seconds, outperforming all baseline models, and reaches a performance of 80.1%, surpassing all models with a comparable parameter scale. These results suggest that HyFunc offers a more efficient paradigm for agentic AI. Our code is publicly available at https://github.com/MrBlankness/HyFunc.",
      "authors": [
        "Weibin Liao",
        "Jian-guang Lou",
        "Haoyi Xiong"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-14 08:19:54+00:00",
      "link": "https://arxiv.org/pdf/2602.13665v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13660v1",
      "title": "Optimized Certainty Equivalent Risk-Controlling Prediction Sets",
      "abstract": "In safety-critical applications such as medical image segmentation, prediction systems must provide reliability guarantees that extend beyond conventional expected loss control. While risk-controlling prediction sets (RCPS) offer probabilistic guarantees on the expected risk, they fail to capture tail behavior and worst-case scenarios that are crucial in high-stakes settings. This paper introduces optimized certainty equivalent RCPS (OCE-RCPS), a novel framework that provides high-probability guarantees on general optimized certainty equivalent (OCE) risk measures, including conditional value-at-risk (CVaR) and entropic risk. OCE-RCPS leverages upper confidence bounds to identify prediction set parameters that satisfy user-specified risk tolerance levels with provable reliability. We establish theoretical guarantees showing that OCE-RCPS satisfies the desired probabilistic constraint for loss functions such as miscoverage and false negative rate. Experiments on image segmentation demonstrate that OCE-RCPS consistently meets target satisfaction rates across various risk measures and reliability configurations, while OCE-CRC fails to provide probabilistic guarantees.",
      "authors": [
        "Jiayi Huang",
        "Amirmohammad Farzaneh",
        "Osvaldo Simeone"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "published": "2026-02-14 08:03:27+00:00",
      "link": "https://arxiv.org/pdf/2602.13660v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13651v1",
      "title": "Cumulative Utility Parity for Fair Federated Learning under Intermittent Client Participation",
      "abstract": "In real-world federated learning (FL) systems, client participation is intermittent, heterogeneous, and often correlated with data characteristics or resource constraints. Existing fairness approaches in FL primarily focus on equalizing loss or accuracy conditional on participation, implicitly assuming that clients have comparable opportunities to contribute over time. However, when participation itself is uneven, these objectives can lead to systematic under-representation of intermittently available clients, even if per-round performance appears fair. We propose cumulative utility parity, a fairness principle that evaluates whether clients receive comparable long-term benefit per participation opportunity, rather than per training round. To operationalize this notion, we introduce availability-normalized cumulative utility, which disentangles unavoidable physical constraints from avoidable algorithmic bias arising from scheduling and aggregation. Experiments on temporally skewed, non-IID federated benchmarks demonstrate that our approach substantially improves long-term representation parity, while maintaining near-perfect performance.",
      "authors": [
        "Stefan Behfar",
        "Richard Mortier"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-14 07:42:37+00:00",
      "link": "https://arxiv.org/pdf/2602.13651v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13650v1",
      "title": "KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination",
      "abstract": "We introduce KorMedMCQA-V, a Korean medical licensing-exam-style multimodal multiple-choice question answering benchmark for evaluating vision-language models (VLMs). The dataset consists of 1,534 questions with 2,043 associated images from Korean Medical Licensing Examinations (2012-2023), with about 30% containing multiple images requiring cross-image evidence integration. Images cover clinical modalities including X-ray, computed tomography (CT), electrocardiography (ECG), ultrasound, endoscopy, and other medical visuals. We benchmark over 50 VLMs across proprietary and open-source categories-spanning general-purpose, medical-specialized, and Korean-specialized families-under a unified zero-shot evaluation protocol. The best proprietary model (Gemini-3.0-Pro) achieves 96.9% accuracy, the best open-source model (Qwen3-VL-32B-Thinking) 83.7%, and the best Korean-specialized model (VARCO-VISION-2.0-14B) only 43.2%. We further find that reasoning-oriented model variants gain up to +20 percentage points over instruction-tuned counterparts, medical domain specialization yields inconsistent gains over strong general-purpose baselines, all models degrade on multi-image questions, and performance varies notably across imaging modalities. By complementing the text-only KorMedMCQA benchmark, KorMedMCQA-V forms a unified evaluation suite for Korean medical reasoning across text-only and multimodal conditions. The dataset is available via Hugging Face Datasets: https://huggingface.co/datasets/seongsubae/KorMedMCQA-V.",
      "authors": [
        "Byungjin Choi",
        "Seongsu Bae",
        "Sunjun Kweon",
        "Edward Choi"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-14 07:42:04+00:00",
      "link": "https://arxiv.org/pdf/2602.13650v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13647v1",
      "title": "PT-RAG: Structure-Fidelity Retrieval-Augmented Generation for Academic Papers",
      "abstract": "Retrieval-augmented generation (RAG) is increasingly applied to question-answering over long academic papers, where accurate evidence allocation under a fixed token budget is critical. Existing approaches typically flatten academic papers into unstructured chunks during preprocessing, which destroys the native hierarchical structure. This loss forces retrieval to operate in a disordered space, thereby producing fragmented contexts, misallocating tokens to non-evidential regions under finite token budgets, and increasing the reasoning burden for downstream language models. To address these issues, we propose PT-RAG, an RAG framework that treats the native hierarchical structure of academic papers as a low-entropy retrieval prior. PT-RAG first inherits the native hierarchy to construct a structure-fidelity PaperTree index, which prevents entropy increase at the source. It then designs a path-guided retrieval mechanism that aligns query semantics to relevant sections and selects high relevance root-to-leaf paths under a fixed token budget, yielding compact, coherent, and low-entropy retrieval contexts. In contrast to existing RAG approaches, PT-RAG avoids entropy increase caused by destructive preprocessing and provides a native low-entropy structural basis for subsequent retrieval. To assess this design, we introduce entropy-based structural diagnostics that quantify retrieval fragmentation and evidence allocation accuracy. On three academic question-answering benchmarks, PT-RAG achieves consistently lower section entropy and evidence alignment cross entropy than strong baselines, indicating reduced context fragmentation and more precise allocation to evidential regions. These structural advantages directly translate into higher answer quality.",
      "authors": [
        "Rui Yu",
        "Tianyi Wang",
        "Ruixia Liu",
        "Yinglong Wang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-14 07:40:09+00:00",
      "link": "https://arxiv.org/pdf/2602.13647v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13629v2",
      "title": "Search in Transition: A Study of University Students Perspectives on Using LLMs and Traditional Search Engines in English Test Problem Solving for Higher Study",
      "abstract": "As Artificial Intelligence (AI) becomes increasingly integrated into education, university students preparing for English language tests are frequently shifting between traditional search engines like Google and large language models (LLMs) to assist with problem-solving. This study explores students perceptions of these tools, particularly in terms of usability, efficiency, and how they fit into English test preparation practices. Using a mixed-methods design, we collected survey data from 140 university students across various academic fields and conducted in-depth interviews with 20 participants. Quantitative analyses, including ANOVA and chi-square tests, were applied to assess differences in perceived efficiency, satisfaction, and overall tool preference. The qualitative results reveal that students strategically alternate between GPT and Google based on task requirements. Google is primarily used for accessing reliable, multi-source information and verifying rules, whereas GPT is favored for summarizing content, providing explanations, paraphrasing, and drafting responses for English test tasks. Since neither tool independently satisfies all aspects of English language test preparation, students expressed a clear preference for an integrated approach. In response, this study proposes a prototype chatbot embedded within a search interface, combining GPTs interactive capabilities with Googles credibility to enhance test preparation and reduce cognitive load.",
      "authors": [
        "Tarek Rahman",
        "Md Shaharia Hossen",
        "Mark Protik Mondol",
        "Jannatun Noor Mukta"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-14 06:39:16+00:00",
      "link": "https://arxiv.org/pdf/2602.13629v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13619v1",
      "title": "Locally Private Parametric Methods for Change-Point Detection",
      "abstract": "We study parametric change-point detection, where the goal is to identify distributional changes in time series, under local differential privacy. In the non-private setting, we derive improved finite-sample accuracy guarantees for a change-point detection algorithm based on the generalized log-likelihood ratio test, via martingale methods. In the private setting, we propose two locally differentially private algorithms based on randomized response and binary mechanisms, and analyze their theoretical performance. We derive bounds on detection accuracy and validate our results through empirical evaluation. Our results characterize the statistical cost of local differential privacy in change-point detection and show how privacy degrades performance relative to a non-private benchmark. As part of this analysis, we establish a structural result for strong data processing inequalities (SDPI), proving that SDPI coefficients for Rényi divergences and their symmetric variants (Jeffreys-Rényi divergences) are achieved by binary input distributions. These results on SDPI coefficients are also of independent interest, with applications to statistical estimation, data compression, and Markov chain mixing.",
      "authors": [
        "Anuj Kumar Yadav",
        "Cemre Cadir",
        "Yanina Shkel",
        "Michael Gastpar"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.IT",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2026-02-14 06:11:54+00:00",
      "link": "https://arxiv.org/pdf/2602.13619v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13598v1",
      "title": "Designing Health Technologies for Immigrant Communities: Exploring Healthcare Providers' Communication Strategies with Patients",
      "abstract": "Patient-provider communication is an important aspect of successful healthcare, as it can directly lead to positive health outcomes. Previous studies examined factors that facilitate communication between healthcare providers and patients in socially marginalized communities, especially developing countries, and applied identified factors to technology development. However, there is limited understanding of how providers work with patients from immigrant populations in a developed country. By conducting semi-structured interviews with 15 providers working with patients from an immigrant community with unique cultural characteristics, we identified providers' effective communication strategies, including acknowledgment, community involvement, gradual care, and adaptive communication practices (i.e., adjusting the communication style). Based on our findings, we highlight cultural competence and discuss design implications for technologies to support health communication in immigrant communities. Our suggestions propose approaches for HCI researchers to identify practical, contextualized cultural competence for their health technology design.",
      "authors": [
        "Zhanming Chen",
        "Alisha Ghaju",
        "May Hang",
        "Juan F. Maestre",
        "Ji Youn Shin"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.CY"
      ],
      "published": "2026-02-14 04:36:42+00:00",
      "link": "https://arxiv.org/pdf/2602.13598v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13594v1",
      "title": "Hippocampus: An Efficient and Scalable Memory Module for Agentic AI",
      "abstract": "Agentic AI require persistent memory to store user-specific histories beyond the limited context window of LLMs. Existing memory systems use dense vector databases or knowledge-graph traversal (or hybrid), incurring high retrieval latency and poor storage scalability. We introduce Hippocampus, an agentic memory management system that uses compact binary signatures for semantic search and lossless token-ID streams for exact content reconstruction. Its core is a Dynamic Wavelet Matrix (DWM) that compresses and co-indexes both streams to support ultra-fast search in the compressed domain, thus avoiding costly dense-vector or graph computations. This design scales linearly with memory size, making it suitable for long-horizon agentic deployments. Empirically, our evaluation shows that Hippocampus reduces end-to-end retrieval latency by up to 31$\\times$ and cuts per-query token footprint by up to 14$\\times$, while maintaining accuracy on both LoCoMo and LongMemEval benchmarks.",
      "authors": [
        "Yi Li",
        "Lianjie Cao",
        "Faraz Ahmed",
        "Puneet Sharma",
        "Bingzhe Li"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-14 04:25:20+00:00",
      "link": "https://arxiv.org/pdf/2602.13594v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13586v1",
      "title": "Interpretable clustering via optimal multiway-split decision trees",
      "abstract": "Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability. Our method yields multiway-split decision trees with concise decision rules while maintaining competitive performance across various evaluation metrics.",
      "authors": [
        "Hayato Suzuki",
        "Shunnosuke Ikeda",
        "Yuichi Takano"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-14 04:08:52+00:00",
      "link": "https://arxiv.org/pdf/2602.13586v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13585v1",
      "title": "Diff-Aid: Inference-time Adaptive Interaction Denoising for Rectified Text-to-Image Generation",
      "abstract": "Recent text-to-image (T2I) diffusion models have achieved remarkable advancement, yet faithfully following complex textual descriptions remains challenging due to insufficient interactions between textual and visual features. Prior approaches enhance such interactions via architectural design or handcrafted textual condition weighting, but lack flexibility and overlook the dynamic interactions across different blocks and denoising stages. To provide a more flexible and efficient solution to this problem, we propose Diff-Aid, a lightweight inference-time method that adaptively adjusts per-token text and image interactions across transformer blocks and denoising timesteps. Beyond improving generation quality, Diff-Aid yields interpretable modulation patterns that reveal how different blocks, timesteps, and textual tokens contribute to semantic alignment during denoising. As a plug-and-play module, Diff-Aid can be seamlessly integrated into downstream applications for further improvement, including style LoRAs, controllable generation, and zero-shot editing. Experiments on strong baselines (SD 3.5 and FLUX) demonstrate consistent improvements in prompt adherence, visual quality, and human preference across various metrics. Our code and models will be released.",
      "authors": [
        "Binglei Li",
        "Mengping Yang",
        "Zhiyu Tan",
        "Junping Zhang",
        "Hao Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-14 04:08:05+00:00",
      "link": "https://arxiv.org/pdf/2602.13585v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13583v1",
      "title": "Differentiable Rule Induction from Raw Sequence Inputs",
      "abstract": "Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging neural networks to improve robustness and scalability. However, most differentiable ILP methods rely on symbolic datasets, facing challenges when learning directly from raw data. Specifically, they struggle with explicit label leakage: The inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels. In this work, we address this issue by integrating a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage. The learned rules effectively describe raw data through its features. We demonstrate that our method intuitively and precisely learns generalized rules from time series and image data.",
      "authors": [
        "Kun Gao",
        "Katsumi Inoue",
        "Yongzhi Cao",
        "Hanpin Wang",
        "Feng Yang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-14 03:54:08+00:00",
      "link": "https://arxiv.org/pdf/2602.13583v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13575v1",
      "title": "Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment",
      "abstract": "Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models. Results demonstrate a clear performance hierarchy: point-based methods < static pairwise training < Elo-Evolve across Alpaca Eval 2.0 and MT-Bench, validating the progressive benefits of pairwise comparison and dynamic opponent selection for LLM alignment.",
      "authors": [
        "Jing Zhao",
        "Ting Zhen",
        "Junwei bao",
        "Hongfei Jiang",
        "Yang song"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-14 03:18:52+00:00",
      "link": "https://arxiv.org/pdf/2602.13575v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13571v1",
      "title": "LLM-Confidence Reranker: A Training-Free Approach for Enhancing Retrieval-Augmented Generation Systems",
      "abstract": "Large language models (LLMs) have revolutionized natural language processing, yet hallucinations in knowledge-intensive tasks remain a critical challenge. Retrieval-augmented generation (RAG) addresses this by integrating external knowledge, but its efficacy depends on accurate document retrieval and ranking. Although existing rerankers demonstrate effectiveness, they frequently necessitate specialized training, impose substantial computational expenses, and fail to fully exploit the semantic capabilities of LLMs, particularly their inherent confidence signals. We propose the LLM-Confidence Reranker (LCR), a training-free, plug-and-play algorithm that enhances reranking in RAG systems by leveraging black-box LLM confidence derived from Maximum Semantic Cluster Proportion (MSCP). LCR employs a two-stage process: confidence assessment via multinomial sampling and clustering, followed by binning and multi-level sorting based on query and document confidence thresholds. This approach prioritizes relevant documents while preserving original rankings for high-confidence queries, ensuring robustness. Evaluated on BEIR and TREC benchmarks with BM25 and Contriever retrievers, LCR--using only 7--9B-parameter pre-trained LLMs--consistently improves NDCG@5 by up to 20.6% across pre-trained LLM and fine-tuned Transformer rerankers, without degradation. Ablation studies validate the hypothesis that LLM confidence positively correlates with document relevance, elucidating LCR's mechanism. LCR offers computational efficiency, parallelism for scalability, and broad compatibility, mitigating hallucinations in applications like medical diagnosis.",
      "authors": [
        "Zhipeng Song",
        "Xiangyu Kong",
        "Xinrui Bao",
        "Yizhi Zhou",
        "Jiulong Jiao",
        "Sitong Liu",
        "Yuhang Zhou",
        "Heng Qi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-14 03:12:05+00:00",
      "link": "https://arxiv.org/pdf/2602.13571v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13567v1",
      "title": "DistillLens: Symmetric Knowledge Distillation Through Logit Lens",
      "abstract": "Standard Knowledge Distillation (KD) compresses Large Language Models (LLMs) by optimizing final outputs, yet it typically treats the teacher's intermediate layer's thought process as a black box. While feature-based distillation attempts to bridge this gap, existing methods (e.g., MSE and asymmetric KL divergence) ignore the rich uncertainty profiles required for the final output. In this paper, we introduce DistillLens, a framework that symmetrically aligns the evolving thought processes of student and teacher models. By projecting intermediate hidden states into the vocabulary space via the Logit Lens, we enforce structural alignment using a symmetric divergence objective. Our analysis proves that this constraint imposes a dual-sided penalty, preventing both overconfidence and underconfidence while preserving the high-entropy information conduits essential for final deduction. Extensive experiments on GPT-2 and Llama architectures demonstrate that DistillLens consistently outperforms standard KD and feature-transfer baselines on diverse instruction-following benchmarks. The code is available at https://github.com/manishdhakal/DistillLens.",
      "authors": [
        "Manish Dhakal",
        "Uthman Jinadu",
        "Anjila Budathoki",
        "Rajshekhar Sunderraman",
        "Yi Ding"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-14 03:01:12+00:00",
      "link": "https://arxiv.org/pdf/2602.13567v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13562v1",
      "title": "Mitigating the Safety-utility Trade-off in LLM Alignment via Adaptive Safe Context Learning",
      "abstract": "While reasoning models have achieved remarkable success in complex reasoning tasks, their increasing power necessitates stringent safety measures. For safety alignment, the core challenge lies in the inherent trade-off between safety and utility. However, prevailing alignment strategies typically construct CoT training data with explicit safety rules via context distillation. This approach inadvertently limits reasoning capabilities by creating a rigid association between rule memorization and refusal. To mitigate the safety-utility trade-off, we propose the Adaptive Safe Context Learning (ASCL) framework to improve the reasoning given proper context. ASCL formulates safety alignment as a multi-turn tool-use process, empowering the model to autonomously decide when to consult safety rules and how to generate the ongoing reasoning. Furthermore, to counteract the preference for rule consultation during RL, we introduce Inverse Frequency Policy Optimization (IFPO) to rebalance advantage estimates. By decoupling rule retrieval and subsequent reasoning, our method achieves higher overall performance compared to baselines.",
      "authors": [
        "Yanbo Wang",
        "Minzheng Wang",
        "Jian Liang",
        "Lu Wang",
        "Yongcan Yu",
        "Ran He"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-14 02:37:36+00:00",
      "link": "https://arxiv.org/pdf/2602.13562v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13550v1",
      "title": "Out-of-Support Generalisation via Weight Space Sequence Modelling",
      "abstract": "As breakthroughs in deep learning transform key industries, models are increasingly required to extrapolate on datapoints found outside the range of the training set, a challenge we coin as out-of-support (OoS) generalisation. However, neural networks frequently exhibit catastrophic failure on OoS samples, yielding unrealistic but overconfident predictions. We address this challenge by reformulating the OoS generalisation problem as a sequence modelling task in the weight space, wherein the training set is partitioned into concentric shells corresponding to discrete sequential steps. Our WeightCaster framework yields plausible, interpretable, and uncertainty-aware predictions without necessitating explicit inductive biases, all the while maintaining high computational efficiency. Emprical validation on a synthetic cosine dataset and real-world air quality sensor readings demonstrates performance competitive or superior to the state-of-the-art. By enhancing reliability beyond in-distribution scenarios, these results hold significant implications for the wider adoption of artificial intelligence in safety-critical applications.",
      "authors": [
        "Roussel Desmond Nzoyem"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-14 01:51:54+00:00",
      "link": "https://arxiv.org/pdf/2602.13550v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13543v1",
      "title": "LiveNewsBench: Evaluating LLM Web Search Capabilities with Freshly Curated News",
      "abstract": "Large Language Models (LLMs) with agentic web search capabilities show strong potential for tasks requiring real-time information access and complex fact retrieval, yet evaluating such systems remains challenging. We introduce \\bench, a rigorous and regularly updated benchmark designed to assess the agentic web search abilities of LLMs. \\bench automatically generates fresh question-answer pairs from recent news articles, ensuring that questions require information beyond an LLM's training data and enabling clear separation between internal knowledge and search capability. The benchmark features intentionally difficult questions requiring multi-hop search queries, page visits, and reasoning, making it well-suited for evaluating agentic search behavior. Our automated data curation and question generation pipeline enables frequent benchmark updates and supports construction of a large-scale training dataset for agentic web search models, addressing the scarcity of such data in the research community. To ensure reliable evaluation, we include a subset of human-verified samples in the test set. We evaluate a broad range of systems using \\bench, including commercial and open-weight LLMs as well as LLM-based web search APIs. The leaderboard, datasets, and code are publicly available at livenewsbench.com.",
      "authors": [
        "Yunfan Zhang",
        "Kathleen McKeown",
        "Smaranda Muresan"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-14 01:18:51+00:00",
      "link": "https://arxiv.org/pdf/2602.13543v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13540v1",
      "title": "On Calibration of Large Language Models: From Response To Capability",
      "abstract": "Large language models (LLMs) are widely deployed as general-purpose problem solvers, making accurate confidence estimation critical for reliable use. Prior work on LLM calibration largely focuses on response-level confidence, which estimates the correctness of a single generated output. However, this formulation is misaligned with many practical settings where the central question is how likely a model is to solve a query overall. We show that this mismatch results from the stochastic nature of modern LLM decoding, under which single-response correctness fails to reflect underlying model capability. To address this issue, we introduce capability calibration, which targets the model's expected accuracy on a query. We formally distinguish capability calibration from response calibration and show that the two differ both theoretically and empirically. We establish an empirical evaluation setup and study a range of confidence estimation methods. Our results demonstrate that capability-calibrated confidence improves pass@$k$ prediction and inference budget allocation, establishing a foundation with potential for diverse applications.",
      "authors": [
        "Sin-Han Yang",
        "Cheng-Kuang Wu",
        "Chieh-Yen Lin",
        "Yun-Nung Chen",
        "Hung-yi Lee",
        "Shao-Hua Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-14 01:07:45+00:00",
      "link": "https://arxiv.org/pdf/2602.13540v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13536v1",
      "title": "Robustness Verification of Binary Neural Networks: An Ising and Quantum-Inspired Framework",
      "abstract": "Binary neural networks (BNNs) are increasingly deployed in edge computing applications due to their low hardware complexity and high energy efficiency. However, verifying the robustness of BNNs against input perturbations, including adversarial attacks, remains computationally challenging because the underlying decision problem is inherently combinatorial. In this paper, we propose an Ising- and quantum-inspired framework for BNN robustness verification. We show that, for a broad class of BNN architectures, robustness verification can be formulated as a Quadratic Constrained Boolean Optimization (QCBO) problem and subsequently transformed into a Quadratic Unconstrained Boolean Optimization (QUBO) instance amenable to Ising and quantum-inspired solvers. We demonstrate the feasibility of this formulation on binarized MNIST by solving the resulting QUBOs with a free energy machine (FEM) solver and simulated annealing. We also show the deployment of this framework on quantum annealing and digital annealing platforms. Our results highlight the potential of quantum-inspired computing and Ising computing as a pathway toward trustworthy AI systems.",
      "authors": [
        "Rahul Singh",
        "Seyran Saeedi",
        "Zheng Zhang"
      ],
      "primary_category": "cs.ET",
      "categories": [
        "cs.ET"
      ],
      "published": "2026-02-14 00:28:51+00:00",
      "link": "https://arxiv.org/pdf/2602.13536v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13532v1",
      "title": "Fast Swap-Based Element Selection for Multiplication-Free Dimension Reduction",
      "abstract": "In this paper, we propose a fast algorithm for element selection, a multiplication-free form of dimension reduction that produces a dimension-reduced vector by simply selecting a subset of elements from the input. Dimension reduction is a fundamental technique for reducing unnecessary model parameters, mitigating overfitting, and accelerating training and inference. A standard approach is principal component analysis (PCA), but PCA relies on matrix multiplications; on resource-constrained systems, the multiplication count itself can become a bottleneck. Element selection eliminates this cost because the reduction consists only of selecting elements, and thus the key challenge is to determine which elements should be retained. We evaluate a candidate subset through the minimum mean-squared error of linear regression that predicts a target vector from the selected elements, where the target may be, for example, a one-hot label vector in classification. When an explicit target is unavailable, the input itself can be used as the target, yielding a reconstruction-based criterion. The resulting optimization is combinatorial, and exhaustive search is impractical. To address this, we derive an efficient formula for the objective change caused by swapping a selected and an unselected element, using the matrix inversion lemma, and we perform a swap-based local search that repeatedly applies objective-decreasing swaps until no further improvement is possible. Experiments on MNIST handwritten-digit images demonstrate the effectiveness of the proposed method.",
      "authors": [
        "Nobutaka Ono"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "eess.AS",
        "eess.IV",
        "eess.SP"
      ],
      "published": "2026-02-14 00:11:43+00:00",
      "link": "https://arxiv.org/pdf/2602.13532v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13531v1",
      "title": "QuaRK: A Quantum Reservoir Kernel for Time Series Learning",
      "abstract": "Quantum reservoir computing offers a promising route for time series learning by modelling sequential data via rich quantum dynamics while the only training required happens at the level of a lightweight classical readout. However, studies featuring efficient and implementable quantum reservoir architectures along with model learning guarantees remain scarce in the literature. To close this gap, we introduce QuaRK, an end-to-end framework that couples a hardware-realistic quantum reservoir featurizer with a kernel-based readout scheme. Given a sequence of sample points, the reservoir injects the points one after the other to yield a compact feature vector from efficiently measured k-local observables using classical shadow tomography, after which a classical kernel-based readout learns the target mapping with explicit regularization and fast optimization. The resulting pipeline exposes clear computational knobs -- circuit width and depth as well as the measurement budget -- while preserving the flexibility of kernel methods to model nonlinear temporal functionals and being scalable to high-dimensional data. We further provide learning-theoretic generalization guarantees for dependent temporal data, linking design and resource choices to finite-sample performance, thereby offering principled guidance for building reliable temporal learners. Empirical experiments validate QuaRK and illustrate the predicted interpolation and generalization behaviours on synthetic beta-mixing time series tasks.",
      "authors": [
        "Abdallah Aaraba",
        "Soumaya Cherkaoui",
        "Ola Ahmad",
        "Shengrui Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "quant-ph"
      ],
      "published": "2026-02-14 00:04:52+00:00",
      "link": "https://arxiv.org/pdf/2602.13531v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13529v1",
      "title": "SecureGate: Learning When to Reveal PII Safely via Token-Gated Dual-Adapters for Federated LLMs",
      "abstract": "Federated learning (FL) enables collaborative training across organizational silos without sharing raw data, making it attractive for privacy-sensitive applications. With the rapid adoption of large language models (LLMs), federated fine-tuning of generative LLMs has gained attention as a way to leverage distributed data while preserving confidentiality. However, this setting introduces fundamental challenges: (i) privacy leakage of personally identifiable information (PII) due to LLM memorization, and (ii) a persistent tension between global generalization and local utility under heterogeneous data. Existing defenses, such as data sanitization and differential privacy, reduce leakage but often degrade downstream performance. We propose SecureGate, a privacy-aware federated fine-tuning framework for LLMs that provides fine-grained privacy control without sacrificing utility. SecureGate employs a dual-adapter LoRA architecture: a secure adapter that learns sanitized, globally shareable representations, and a revealing adapter that captures sensitive, organization-specific knowledge. A token-controlled gating module selectively activates these adapters at inference time, enabling controlled information disclosure without retraining. Extensive experiments across multiple LLMs and real-world datasets show that SecureGate improves task utility while substantially reducing PII leakage, achieving up to a 31.66X reduction in inference attack accuracy and a 17.07X reduction in extraction recall for unauthorized requests. Additionally, it maintains 100% routing reliability to the correct adapter and incurs only minimal computational and communication overhead.",
      "authors": [
        "Mohamed Shaaban",
        "Mohamed Elmahallawy"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "published": "2026-02-13 23:53:32+00:00",
      "link": "https://arxiv.org/pdf/2602.13529v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13521v2",
      "title": "Arming Data Agents with Tribal Knowledge",
      "abstract": "Natural language to SQL (NL2SQL) translation enables non-expert users to query relational databases through natural language. Recently, NL2SQL agents, powered by the reasoning capabilities of Large Language Models (LLMs), have significantly advanced NL2SQL translation. Nonetheless, NL2SQL agents still make mistakes when faced with large-scale real-world databases because they lack knowledge of how to correctly leverage the underlying data (e.g., knowledge about the intent of each column) and form misconceptions about the data when querying it, leading to errors. Prior work has studied generating facts about the database to provide more context to NL2SQL agents, but such approaches simply restate database contents without addressing the agent's misconceptions. In this paper, we propose Tk-Boost, a bolt-on framework for augmenting any NL2SQL agent with tribal knowledge: knowledge that corrects the agent's misconceptions in querying the database accumulated through experience using the database. To accumulate experience, Tk-Boost first asks the NL2SQL agent to answer a few queries on the database, identifies the agent's misconceptions by analyzing its mistakes on the database, and generates tribal knowledge to address them. To enable accurate retrieval, Tk-Boost indexes this knowledge with applicability conditions that specify the query features for which the knowledge is useful. When answering new queries, Tk-Boost uses this knowledge to provide feedback to the NL2SQL agent, resolving the agent's misconceptions during SQL generation, and thus improving the agent's accuracy. Extensive experiments across the BIRD and Spider 2.0 benchmarks with various NL2SQL agents shows Tk-Boost improves NL2SQL agents accuracy by up to 16.9% on Spider 2.0 and 13.7% on BIRD",
      "authors": [
        "Shubham Agarwal",
        "Asim Biswal",
        "Sepanta Zeighami",
        "Alvin Cheung",
        "Joseph Gonzalez",
        "Aditya G. Parameswaran"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "published": "2026-02-13 23:19:07+00:00",
      "link": "https://arxiv.org/pdf/2602.13521v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13517v1",
      "title": "Think Deep, Not Just Long: Measuring LLM Reasoning Effort via Deep-Thinking Tokens",
      "abstract": "Large language models (LLMs) have demonstrated impressive reasoning capabilities by scaling test-time compute via long Chain-of-Thought (CoT). However, recent findings suggest that raw token counts are unreliable proxies for reasoning quality: increased generation length does not consistently correlate with accuracy and may instead signal \"overthinking,\" leading to performance degradation. In this work, we quantify inference-time effort by identifying deep-thinking tokens -- tokens where internal predictions undergo significant revisions in deeper model layers prior to convergence. Across four challenging mathematical and scientific benchmarks (AIME 24/25, HMMT 25, and GPQA-diamond) and a diverse set of reasoning-focused models (GPT-OSS, DeepSeek-R1, and Qwen3), we show that deep-thinking ratio (the proportion of deep-thinking tokens in a generated sequence) exhibits a robust and consistently positive correlation with accuracy, substantially outperforming both length-based and confidence-based baselines. Leveraging this insight, we introduce Think@n, a test-time scaling strategy that prioritizes samples with high deep-thinking ratios. We demonstrate that Think@n matches or exceeds standard self-consistency performance while significantly reducing inference costs by enabling the early rejection of unpromising generations based on short prefixes.",
      "authors": [
        "Wei-Lin Chen",
        "Liqian Peng",
        "Tian Tan",
        "Chao Zhao",
        "Blake JianHang Chen",
        "Ziqian Lin",
        "Alec Go",
        "Yu Meng"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 23:07:37+00:00",
      "link": "https://arxiv.org/pdf/2602.13517v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13513v2",
      "title": "Learning Gradient Flow: Using Equation Discovery to Accelerate Engineering Optimization",
      "abstract": "In this work, we investigate the use of data-driven equation discovery for dynamical systems to model and forecast continuous-time dynamics of unconstrained optimization problems. To avoid expensive evaluations of the objective function and its gradient, we leverage trajectory data on the optimization variables to learn the continuous-time dynamics associated with gradient descent, Newton's method, and ADAM optimization. The discovered gradient flows are then solved as a surrogate for the original optimization problem. To this end, we introduce the Learned Gradient Flow (LGF) optimizer, which is equipped to build surrogate models of variable polynomial order in full- or reduced-dimensional spaces at user-defined intervals in the optimization process. We demonstrate the efficacy of this approach on several standard problems from engineering mechanics and scientific machine learning, including two inverse problems, structural topology optimization, and two forward solves with different discretizations. Our results suggest that the learned gradient flows can significantly expedite convergence by capturing critical features of the optimization trajectory while avoiding expensive evaluations of the objective and its gradient.",
      "authors": [
        "Grant Norman",
        "Conor Rowan",
        "Kurt Maute",
        "Alireza Doostan"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.CE",
        "cs.LG",
        "math.DS",
        "math.NA"
      ],
      "published": "2026-02-13 22:44:33+00:00",
      "link": "https://arxiv.org/pdf/2602.13513v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13510v1",
      "title": "Stochastic variance reduced extragradient methods for solving hierarchical variational inequalities",
      "abstract": "We are concerned with optimization in a broad sense through the lens of solving variational inequalities (VIs) -- a class of problems that are so general that they cover as particular cases minimization of functions, saddle-point (minimax) problems, Nash equilibrium problems, and many others. The key challenges in our problem formulation are the two-level hierarchical structure and finite-sum representation of the smooth operators in each level. For this setting, we are the first to prove convergence rates and complexity statements for variance-reduced stochastic algorithms approaching the solution of hierarchical VIs in Euclidean and Bregman setups.",
      "authors": [
        "Pavel Dvurechensky",
        "Andrea Ebner",
        "Johannes Carl Schnebel",
        "Shimrit Shtern",
        "Mathias Staudigl"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.GT",
        "cs.LG"
      ],
      "published": "2026-02-13 22:38:29+00:00",
      "link": "https://arxiv.org/pdf/2602.13510v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13506v1",
      "title": "$γ$-weakly $θ$-up-concavity: Linearizable Non-Convex Optimization with Applications to DR-Submodular and OSS Functions",
      "abstract": "Optimizing monotone non-convex functions is a fundamental challenge across machine learning and combinatorial optimization. We introduce and study $γ$-weakly $θ$-up-concavity, a novel first-order condition that characterizes a broad class of such functions. This condition provides a powerful unifying framework, strictly generalizing both DR-submodular functions and One-Sided Smooth (OSS) functions. Our central theoretical contribution demonstrates that $γ$-weakly $θ$-up-concave functions are upper-linearizable: for any feasible point, we can construct a linear surrogate whose gains provably approximate the original non-linear objective. This approximation holds up to a constant factor, namely the approximation coefficient, dependent solely on $γ$, $θ$, and the geometry of the feasible set. This linearizability yields immediate and unified approximation guarantees for a wide range of problems. Specifically, we obtain unified approximation guarantees for offline optimization as well as static and dynamic regret bounds in online settings via standard reductions to linear optimization. Moreover, our framework recovers the optimal approximation coefficient for DR-submodular maximization and significantly improves existing approximation coefficients for OSS optimization, particularly over matroid constraints.",
      "authors": [
        "Mohammad Pedramfar",
        "Vaneet Aggarwal"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "published": "2026-02-13 22:34:44+00:00",
      "link": "https://arxiv.org/pdf/2602.13506v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13504v1",
      "title": "From Perceptions To Evidence: Detecting AI-Generated Content In Turkish News Media With A Fine-Tuned Bert Classifier",
      "abstract": "The rapid integration of large language models into newsroom workflows has raised urgent questions about the prevalence of AI-generated content in online media. While computational studies have begun to quantify this phenomenon in English-language outlets, no empirical investigation exists for Turkish news media, where existing research remains limited to qualitative interviews with journalists or fake news detection. This study addresses that gap by fine-tuning a Turkish-specific BERT model (dbmdz/bert-base-turkish-cased) on a labeled dataset of 3,600 articles from three major Turkish outlets with distinct editorial orientations for binary classification of AI-rewritten content. The model achieves 0.9708 F1 score on the held-out test set with symmetric precision and recall across both classes. Subsequent deployment on over 3,500 unseen articles spanning between 2023 and 2026 reveals consistent cross-source and temporally stable classification patterns, with mean prediction confidence exceeding 0.96 and an estimated 2.5 percentage of examined news content rewritten or revised by LLMs on average. To the best of our knowledge, this is the first study to move beyond self-reported journalist perceptions toward empirical, data-driven measurement of AI usage in Turkish news media.",
      "authors": [
        "Ozancan Ozdemir"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-13 22:29:00+00:00",
      "link": "https://arxiv.org/pdf/2602.13504v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13498v1",
      "title": "TrasMuon: Trust-Region Adaptive Scaling for Orthogonalized Momentum Optimizers",
      "abstract": "Muon-style optimizers leverage Newton-Schulz (NS) iterations to orthogonalize updates, yielding update geometries that often outperform Adam-series methods. However, this orthogonalization discards magnitude information, rendering training sensitive to step-size hyperparameters and vulnerable to high-energy bursts. To mitigate this, we introduce TrasMuon (\\textbf{T}rust \\textbf{R}egion \\textbf{A}daptive \\textbf{S}caling \\textbf{Muon}). TrasMuon preserves the near-isometric geometry of Muon while stabilizing magnitudes through (i) global RMS calibration and (ii) energy-based trust-region clipping. We demonstrate that while reintroducing adaptive scaling improves optimization efficiency, it typically exacerbates instability due to high-energy outliers. TrasMuon addresses this by defining a trust region based on relative energy ratios, confining updates to a stable zone. Empirical experiments on vision and language models demonstrate that TrasMuon converges faster than baselines. Furthermore, experiments without warmup stages confirm TrasMuon's superior stability and robustness.",
      "authors": [
        "Peng Cheng",
        "Jiucheng Zang",
        "Qingnan Li",
        "Liheng Ma",
        "Yufei Cui",
        "Yingxue Zhang",
        "Boxing Chen",
        "Ming Jian",
        "Wen Tong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 22:11:59+00:00",
      "link": "https://arxiv.org/pdf/2602.13498v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13486v1",
      "title": "Preventing Rank Collapse in Federated Low-Rank Adaptation with Client Heterogeneity",
      "abstract": "Federated low-rank adaptation (FedLoRA) has facilitated communication-efficient and privacy-preserving fine-tuning of foundation models for downstream tasks. In practical federated learning scenarios, client heterogeneity in system resources and data distributions motivates heterogeneous LoRA ranks across clients. We identify a previously overlooked phenomenon in heterogeneous FedLoRA, termed rank collapse, where the energy of the global update concentrates on the minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations. Through theoretical analysis, we reveal the root cause of rank collapse: a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, which systematically suppresses higher-rank updates at a geometric rate over rounds. Motivated by this insight, we propose raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and then aggregates each partition weighted by its effective client contributions. Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.",
      "authors": [
        "Fei Wu",
        "Jia Hu",
        "Geyong Min",
        "Shiqiang Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "published": "2026-02-13 21:42:06+00:00",
      "link": "https://arxiv.org/pdf/2602.13486v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13485v1",
      "title": "Federated Learning of Nonlinear Temporal Dynamics with Graph Attention-based Cross-Client Interpretability",
      "abstract": "Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem relate to others. This is challenging in decentralized settings where raw measurements cannot be shared and client observations are heterogeneous. In practical deployments each subsystem (client) operates a fixed proprietary model that cannot be modified or retrained, limiting existing approaches. Nonlinear dynamics further make cross client temporal interdependencies difficult to interpret because they are embedded in nonlinear state transition functions. We present a federated framework for learning temporal interdependencies across clients under these constraints. Each client maps high dimensional local observations to low dimensional latent states using a nonlinear state space model. A central server learns a graph structured neural state transition model over the communicated latent states using a Graph Attention Network. For interpretability we relate the Jacobian of the learned server side transition model to attention coefficients, providing the first interpretable characterization of cross client temporal interdependencies in decentralized nonlinear systems. We establish theoretical convergence guarantees to a centralized oracle and validate the framework through synthetic experiments demonstrating convergence, interpretability, scalability and privacy. Additional real world experiments show performance comparable to decentralized baselines.",
      "authors": [
        "Ayse Tursucular",
        "Ayush Mohanty",
        "Nazal Mohamed",
        "Nagi Gebraeel"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-13 21:41:52+00:00",
      "link": "https://arxiv.org/pdf/2602.13485v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13484v1",
      "title": "How to Train Your Filter: Should You Learn, Stack or Adapt?",
      "abstract": "Filters are ubiquitous in computer science, enabling space-efficient approximate membership testing. Since Bloom filters were introduced in 1970, decades of work improved their space efficiency and performance. Recently, three new paradigms have emerged offering orders-of-magnitude improvements in false positive rates (FPRs) by using information beyond the input set: (1) learned filters train a model to distinguish (non)members, (2) stacked filters use negative workload samples to build cascading layers, and (3) adaptive filters update internal representation in response to false positive feedback. Yet each paradigm targets specific use cases, introduces complex configuration tuning, and has been evaluated in isolation. This results in unclear trade-offs and a gap in understanding how these approaches compare and when each is most appropriate. This paper presents the first comprehensive evaluation of learned, stacked, and adaptive filters across real-world datasets and query workloads. Our results reveal critical trade-offs: (1) Learned filters achieve up to 10^2 times lower FPRs but exhibit high variance and lack robustness under skewed or dynamic workloads. Critically, model inference overhead leads to up to 10^4 times slower query latencies than stacked or adaptive filters. (2) Stacked filters reliably achieve up to 10^3 times lower FPRs on skewed workloads but require workload knowledge. (3) Adaptive filters are robust across settings, achieving up to 10^3 times lower FPRs under adversarial queries without workload assumptions. Based on our analysis, learned filters suit stable workloads where input features enable effective model training and space constraints are paramount, stacked filters excel when reliable query distributions are known, and adaptive filters are most generalizable, providing robust theoretically bound guarantees even in dynamic or adversarial environments.",
      "authors": [
        "Diandre Miguel Sabale",
        "Wolfgang Gatterbauer",
        "Prashant Pandey"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS"
      ],
      "published": "2026-02-13 21:41:44+00:00",
      "link": "https://arxiv.org/pdf/2602.13484v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13483v1",
      "title": "Finding Highly Interpretable Prompt-Specific Circuits in Language Models",
      "abstract": "Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco & Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms. Despite this variation, prompts cluster into prompt families with similar circuits, and we propose a representative circuit for each family as a practical unit of analysis. Finally, we develop an automated interpretability pipeline that uses ACC++ signals to surface human-interpretable features and assemble mechanistic explanations for prompt families behavior. Together, our results recast circuits as a meaningful object of study by shifting the unit of analysis from tasks to prompts, enabling scalable circuit descriptions in the presence of prompt-specific mechanisms.",
      "authors": [
        "Gabriel Franco",
        "Lucas M. Tassis",
        "Azalea Rohr",
        "Mark Crovella"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 21:41:17+00:00",
      "link": "https://arxiv.org/pdf/2602.13483v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13479v1",
      "title": "GLIMPSE : Real-Time Text Recognition and Contextual Understanding for VQA in Wearables",
      "abstract": "Video Large Language Models (Video LLMs) have shown remarkable progress in understanding and reasoning about visual content, particularly in tasks involving text recognition and text-based visual question answering (Text VQA). However, deploying Text VQA on wearable devices faces a fundamental tension: text recognition requires high-resolution video, but streaming high-quality video drains battery and causes thermal throttling. Moreover, existing models struggle to maintain coherent temporal context when processing text across multiple frames in real-time streams. We observe that text recognition and visual reasoning have asymmetric resolution requirements - OCR needs fine detail while scene understanding tolerates coarse features. We exploit this asymmetry with a hybrid architecture that performs selective high-resolution OCR on-device while streaming low-resolution video for visual context. On a benchmark of text-based VQA samples across five task categories, our system achieves 72% accuracy at 0.49x the power consumption of full-resolution streaming, enabling sustained VQA sessions on resource-constrained wearables without sacrificing text understanding quality.",
      "authors": [
        "Akhil Ramachandran",
        "Ankit Arun",
        "Ashish Shenoy",
        "Abhay Harpale",
        "Srihari Jayakumar",
        "Debojeet Chatterjee",
        "Mohsen Moslehpour",
        "Pierce Chuang",
        "Yichao Lu",
        "Vikas Bhardwaj",
        "Peyman Heidari"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "published": "2026-02-13 21:35:01+00:00",
      "link": "https://arxiv.org/pdf/2602.13479v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13473v1",
      "title": "NeuroWeaver: An Autonomous Evolutionary Agent for Exploring the Programmatic Space of EEG Analysis Pipelines",
      "abstract": "Although foundation models have demonstrated remarkable success in general domains, the application of these models to electroencephalography (EEG) analysis is constrained by substantial data requirements and high parameterization. These factors incur prohibitive computational costs, thereby impeding deployment in resource-constrained clinical environments. Conversely, general-purpose automated machine learning frameworks are often ill-suited for this domain, as exploration within an unbounded programmatic space fails to incorporate essential neurophysiological priors and frequently yields solutions that lack scientific plausibility. To address these limitations, we propose NeuroWeaver, a unified autonomous evolutionary agent designed to generalize across diverse EEG datasets and tasks by reformulating pipeline engineering as a discrete constrained optimization problem. Specifically, we employ a Domain-Informed Subspace Initialization to confine the search to neuroscientifically plausible manifolds, coupled with a Multi-Objective Evolutionary Optimization that dynamically balances performance, novelty, and efficiency via self-reflective refinement. Empirical evaluations across five heterogeneous benchmarks demonstrate that NeuroWeaver synthesizes lightweight solutions that consistently outperform state-of-the-art task-specific methods and achieve performance comparable to large-scale foundation models, despite utilizing significantly fewer parameters.",
      "authors": [
        "Guoan Wang",
        "Shihao Yang",
        "Jun-En Ding",
        "Hao Zhu",
        "Feng Liu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-13 21:26:43+00:00",
      "link": "https://arxiv.org/pdf/2602.13473v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13466v1",
      "title": "Language Model Memory and Memory Models for Language",
      "abstract": "The ability of machine learning models to store input information in hidden layer vector embeddings, analogous to the concept of `memory', is widely employed but not well characterized. We find that language model embeddings typically contain relatively little input information regardless of data and compute scale during training. In contrast, embeddings from autoencoders trained for input regeneration are capable of nearly perfect memory formation. The substitution of memory embeddings for token sequences leads to substantial computational efficiencies, motivating the introduction of a parallelizable encoder-decoder memory model architecture. Upon causal training these models contain information-poor embeddings incapable of arbitrary information access, but by combining causal and information retention objective functions they learn to form and decode information-rich memories. Training can be further streamlined by freezing a high fidelity encoder followed by a curriculum training approach where decoders first learn to process memories and then learn to additionally predict next tokens. We introduce the perspective that next token prediction training alone is poorly suited for accurate memory formation as the objective itself is non-invertible, motivating the use of combined objective functions for models where the entire input is not exposed.",
      "authors": [
        "Benjamin L. Badger"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-13 21:16:10+00:00",
      "link": "https://arxiv.org/pdf/2602.13466v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13455v1",
      "title": "Using Machine Learning to Enhance the Detection of Obfuscated Abusive Words in Swahili: A Focus on Child Safety",
      "abstract": "The rise of digital technology has dramatically increased the potential for cyberbullying and online abuse, necessitating enhanced measures for detection and prevention, especially among children. This study focuses on detecting abusive obfuscated language in Swahili, a low-resource language that poses unique challenges due to its limited linguistic resources and technological support. Swahili is chosen due to its popularity and being the most widely spoken language in Africa, with over 16 million native speakers and upwards of 100 million speakers in total, spanning regions in East Africa and some parts of the Middle East.   We employed machine learning models including Support Vector Machines (SVM), Logistic Regression, and Decision Trees, optimized through rigorous parameter tuning and techniques like Synthetic Minority Over-sampling Technique (SMOTE) to handle data imbalance. Our analysis revealed that, while these models perform well in high-dimensional textual data, our dataset's small size and imbalance limit our findings' generalizability. Precision, recall, and F1 scores were thoroughly analyzed, highlighting the nuanced performance of each model in detecting obfuscated language.   This research contributes to the broader discourse on ensuring safer online environments for children, advocating for expanded datasets and advanced machine-learning techniques to improve the effectiveness of cyberbullying detection systems. Future work will focus on enhancing data robustness, exploring transfer learning, and integrating multimodal data to create more comprehensive and culturally sensitive detection mechanisms.",
      "authors": [
        "Phyllis Nabangi",
        "Abdul-Jalil Zakaria",
        "Jema David Ndibwile"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "published": "2026-02-13 21:02:14+00:00",
      "link": "https://arxiv.org/pdf/2602.13455v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13452v1",
      "title": "LLM-Powered Automatic Translation and Urgency in Crisis Scenarios",
      "abstract": "Large language models (LLMs) are increasingly proposed for crisis preparedness and response, particularly for multilingual communication. However, their suitability for high-stakes crisis contexts remains insufficiently evaluated. This work examines the performance of state-of-the-art LLMs and machine translation systems in crisis-domain translation, with a focus on preserving urgency, which is a critical property for effective crisis communication and triaging. Using multilingual crisis data and a newly introduced urgency-annotated dataset covering over 32 languages, we show that both dedicated translation models and LLMs exhibit substantial performance degradation and instability. Crucially, even linguistically adequate translations can distort perceived urgency, and LLM-based urgency classifications vary widely depending on the language of the prompt and input. These findings highlight significant risks in deploying general-purpose language technologies for crisis communication and underscore the need for crisis-aware evaluation frameworks.",
      "authors": [
        "Belu Ticona",
        "Antonis Anastasopoulos"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-13 20:56:06+00:00",
      "link": "https://arxiv.org/pdf/2602.13452v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13449v1",
      "title": "An Algebraic Rigidity Framework for Order-Oblivious Deterministic Black-Box PIT of ROABPs",
      "abstract": "Deterministic black-box polynomial identity testing (PIT) for read-once oblivious algebraic branching programs (ROABPs) is a central open problem in algebraic complexity, particularly in the absence of variable ordering. Prior deterministic algorithms either rely on order information or incur significant overhead through combinatorial isolation techniques.   In this paper, we introduce an algebraic rigidity framework for ROABPs based on the internal structure of their associated matrix word algebras. We show that nonzero width-$w$ ROABPs induce word algebras whose effective algebraic degrees of freedom collapse to dimension at most $w^2$, independent of the number of variables. This rigidity enables deterministic witness construction via intrinsic algebraic invariants, bypassing rank concentration, isolation lemmas, and probabilistic tools used in previous work.Thus, we obtain the first order-oblivious deterministic black-box PIT algorithm for ROABPs, running in quasi-polynomial time $n\\cdot(wd)^{O(w^2)}$. This establishes that algebraic rigidity alone suffices to derandomize PIT in this model, without assuming ordering information.   The framework further isolates a single remaining obstacle to full polynomial-time complexity. We formulate a Modular Stability Conjecture, asserting that width-$w$ ROABPs are stable under hashing into cyclic quotient rings $\\mathbb{K}[λ]/< λ^r-1 >$ once the modulus exceeds a polynomial threshold in $w$ and the individual degree. This conjecture arises naturally from the low-dimensional coefficient structure revealed by rigidity and is supported by extensive empirical evidence.   Assuming the conjecture, our methods yield a fully polynomial-time deterministic black-box PIT algorithm for ROABPs, matching the complexity of the best-known white-box algorithms and reducing the black-box problem to a concrete algebraic stability question.",
      "authors": [
        "Shalender Singh",
        "Vishnupriya Singh"
      ],
      "primary_category": "cs.CC",
      "categories": [
        "cs.CC",
        "math.CO"
      ],
      "published": "2026-02-13 20:51:11+00:00",
      "link": "https://arxiv.org/pdf/2602.13449v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13421v1",
      "title": "Metabolic cost of information processing in Poisson variational autoencoders",
      "abstract": "Computation in biological systems is fundamentally energy-constrained, yet standard theories of computation treat energy as freely available. Here, we argue that variational free energy minimization under a Poisson assumption offers a principled path toward an energy-aware theory of computation. Our key observation is that the Kullback-Leibler (KL) divergence term in the Poisson free energy objective becomes proportional to the prior firing rates of model neurons, yielding an emergent metabolic cost term that penalizes high baseline activity. This structure couples an abstract information-theoretic quantity -- the *coding rate* -- to a concrete biophysical variable -- the *firing rate* -- which enables a trade-off between coding fidelity and energy expenditure. Such a coupling arises naturally in the Poisson variational autoencoder (P-VAE) -- a brain-inspired generative model that encodes inputs as discrete spike counts and recovers a spiking form of *sparse coding* as a special case -- but is absent from standard Gaussian VAEs. To demonstrate that this metabolic cost structure is unique to the Poisson formulation, we compare the P-VAE against Grelu-VAE, a Gaussian VAE with ReLU rectification applied to latent samples, which controls for the non-negativity constraint. Across a systematic sweep of the KL term weighting coefficient $β$ and latent dimensionality, we find that increasing $β$ monotonically increases sparsity and reduces average spiking activity in the P-VAE. In contrast, Grelu-VAE representations remain unchanged, confirming that the effect is specific to Poisson statistics rather than a byproduct of non-negative representations. These results establish Poisson variational inference as a promising foundation for a resource-constrained theory of computation.",
      "authors": [
        "Hadi Vafaii",
        "Jacob L. Yates"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.AI",
        "q-bio.NC"
      ],
      "published": "2026-02-13 19:46:11+00:00",
      "link": "https://arxiv.org/pdf/2602.13421v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13419v1",
      "title": "Protect$^*$: Steerable Retrosynthesis through Neuro-Symbolic State Encoding",
      "abstract": "Large Language Models (LLMs) have shown remarkable potential in scientific domains like retrosynthesis; yet, they often lack the fine-grained control necessary to navigate complex problem spaces without error. A critical challenge is directing an LLM to avoid specific, chemically sensitive sites on a molecule - a task where unconstrained generation can lead to invalid or undesirable synthetic pathways. In this work, we introduce Protect$^*$, a neuro-symbolic framework that grounds the generative capabilities of Large Language Models (LLMs) in rigorous chemical logic. Our approach combines automated rule-based reasoning - using a comprehensive database of 55+ SMARTS patterns and 40+ characterized protecting groups - with the generative intuition of neural models. The system operates via a hybrid architecture: an ``automatic mode'' where symbolic logic deterministically identifies and guards reactive sites, and a ``human-in-the-loop mode'' that integrates expert strategic constraints. Through ``active state tracking,'' we inject hard symbolic constraints into the neural inference process via a dedicated protection state linked to canonical atom maps. We demonstrate this neuro-symbolic approach through case studies on complex natural products, including the discovery of a novel synthetic pathway for Erythromycin B, showing that grounding neural generation in symbolic logic enables reliable, expert-level autonomy.",
      "authors": [
        "Shreyas Vinaya Sathyanarayana",
        "Shah Rahil Kirankumar",
        "Sharanabasava D. Hiremath",
        "Bharath Ramsundar"
      ],
      "primary_category": "q-bio.QM",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "q-bio.BM"
      ],
      "published": "2026-02-13 19:41:55+00:00",
      "link": "https://arxiv.org/pdf/2602.13419v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13415v1",
      "title": "The Rise of AI Search: Implications for Information Markets and Human Judgement at Scale",
      "abstract": "We executed 24,000 search queries in 243 countries, generating 2.8 million AI and traditional search results in 2024 and 2025. We found a rapid global expansion of AI search and key trends that reflect important, previously hidden, policy decisions by AI companies that impact human exposure to AI search worldwide. From 2024 to 2025, overall exposure to Google AI Overviews (AIO) expanded from 7 to 229 countries, with surprising exclusions like France, Turkey, China and Cuba, which do not receive AI search results, even today. While only 1% of Covid search queries were answered by AI in 2024, over 66% of Covid queries were answered by AI in 2025 -- a 5600% increase signaling a clear policy shift on this critical health topic. Our results also show AI search surfaces significantly fewer long tail information sources, lower response variety, and significantly more low credibility and right- and center-leaning information sources, compared to traditional search, impacting the economic incentives to produce new information, market concentration in information production, and human judgment and decision-making at scale. The social and economic implications of these rapid changes in our information ecosystem necessitate a global debate about corporate and governmental policy related to AI search.",
      "authors": [
        "Sinan Aral",
        "Haiwen Li",
        "Rui Zuo"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY"
      ],
      "published": "2026-02-13 19:35:56+00:00",
      "link": "https://arxiv.org/pdf/2602.13415v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13414v1",
      "title": "FUTON: Fourier Tensor Network for Implicit Neural Representations",
      "abstract": "Implicit neural representations (INRs) have emerged as powerful tools for encoding signals, yet dominant MLP-based designs often suffer from slow convergence, overfitting to noise, and poor extrapolation. We introduce FUTON (Fourier Tensor Network), which models signals as generalized Fourier series whose coefficients are parameterized by a low-rank tensor decomposition. FUTON implicitly expresses signals as weighted combinations of orthonormal, separable basis functions, combining complementary inductive biases: Fourier bases capture smoothness and periodicity, while the low-rank parameterization enforces low-dimensional spectral structure. We provide theoretical guarantees through a universal approximation theorem and derive an inference algorithm with complexity linear in the spectral resolution and the input dimension. On image and volume representation, FUTON consistently outperforms state-of-the-art MLP-based INRs while training 2--5$\\times$ faster. On inverse problems such as image denoising and super-resolution, FUTON generalizes better and converges faster.",
      "authors": [
        "Pooya Ashtari",
        "Pourya Behmandpoor",
        "Nikos Deligiannis",
        "Aleksandra Pizurica"
      ],
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2026-02-13 19:31:44+00:00",
      "link": "https://arxiv.org/pdf/2602.13414v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13413v1",
      "title": "Why is Normalization Preferred? A Worst-Case Complexity Theory for Stochastically Preconditioned SGD under Heavy-Tailed Noise",
      "abstract": "We develop a worst-case complexity theory for stochastically preconditioned stochastic gradient descent (SPSGD) and its accelerated variants under heavy-tailed noise, a setting that encompasses widely used adaptive methods such as Adam, RMSProp, and Shampoo. We assume the stochastic gradient noise has a finite $p$-th moment for some $p \\in (1,2]$, and measure convergence after $T$ iterations. While clipping and normalization are parallel tools for stabilizing training of SGD under heavy-tailed noise, there is a fundamental separation in their worst-case properties in stochastically preconditioned settings. We demonstrate that normalization guarantees convergence to a first-order stationary point at rate $\\mathcal{O}(T^{-\\frac{p-1}{3p-2}})$ when problem parameters are known, and $\\mathcal{O}(T^{-\\frac{p-1}{2p}})$ when problem parameters are unknown, matching the optimal rates for normalized SGD, respectively. In contrast, we prove that clipping may fail to converge in the worst case due to the statistical dependence between the stochastic preconditioner and the gradient estimates. To enable the analysis, we develop a novel vector-valued Burkholder-type inequality that may be of independent interest. These results provide a theoretical explanation for the empirical preference for normalization over clipping in large-scale model training.",
      "authors": [
        "Yuchen Fang",
        "James Demmel",
        "Javad Lavaei"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.NA",
        "math.OC",
        "stat.ML"
      ],
      "published": "2026-02-13 19:29:17+00:00",
      "link": "https://arxiv.org/pdf/2602.13413v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13402v1",
      "title": "InfoCIR: Multimedia Analysis for Composed Image Retrieval",
      "abstract": "Composed Image Retrieval (CIR) allows users to search for images by combining a reference image with a text prompt that describes desired modifications. While vision-language models like CLIP have popularized this task by embedding multiple modalities into a joint space, developers still lack tools that reveal how these multimodal prompts interact with embedding spaces and why small wording changes can dramatically alter the results. We present InfoCIR, a visual analytics system that closes this gap by coupling retrieval, explainability, and prompt engineering in a single, interactive dashboard. InfoCIR integrates a state-of-the-art CIR back-end (SEARLE arXiv:2303.15247) with a six-panel interface that (i) lets users compose image + text queries, (ii) projects the top-k results into a low-dimensional space using Uniform Manifold Approximation and Projection (UMAP) for spatial reasoning, (iii) overlays similarity-based saliency maps and gradient-derived token-attribution bars for local explanation, and (iv) employs an LLM-powered prompt enhancer that generates counterfactual variants and visualizes how these changes affect the ranking of user-selected target images. A modular architecture built on Plotly-Dash allows new models, datasets, and attribution methods to be plugged in with minimal effort. We argue that InfoCIR helps diagnose retrieval failures, guides prompt enhancement, and accelerates insight generation during model development. All source code allowing for a reproducible demo is available at https://github.com/giannhskp/InfoCIR.",
      "authors": [
        "Ioannis Dravilas",
        "Ioannis Kapetangeorgis",
        "Anastasios Latsoudis",
        "Conor McCarthy",
        "Gonçalo Marcelino",
        "Marcel Worring"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.IR",
        "cs.MM"
      ],
      "published": "2026-02-13 19:08:30+00:00",
      "link": "https://arxiv.org/pdf/2602.13402v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13195v1",
      "title": "Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision",
      "abstract": "Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., \"left-most apple\") and overlooks functional and physical reasoning (e.g., \"where can I safely store the knife?\"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/",
      "authors": [
        "Aadarsh Sahoo",
        "Georgia Gkioxari"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-13 18:58:30+00:00",
      "link": "https://arxiv.org/pdf/2602.13195v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13194v2",
      "title": "Semantic Chunking and the Entropy of Natural Language",
      "abstract": "The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.",
      "authors": [
        "Weishun Zhong",
        "Doron Sivan",
        "Tankut Can",
        "Mikhail Katkov",
        "Misha Tsodyks"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI"
      ],
      "published": "2026-02-13 18:58:10+00:00",
      "link": "https://arxiv.org/pdf/2602.13194v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13191v1",
      "title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models",
      "abstract": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\\%$ and token usage by up to $93\\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.",
      "authors": [
        "Sayan Deb Sarkar",
        "Rémi Pautrat",
        "Ondrej Miksik",
        "Marc Pollefeys",
        "Iro Armeni",
        "Mahdi Rad",
        "Mihai Dusmanu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-13 18:57:31+00:00",
      "link": "https://arxiv.org/pdf/2602.13191v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13179v1",
      "title": "Fix Before Search: Benchmarking Agentic Query Visual Pre-processing in Multimodal Retrieval-augmented Generation",
      "abstract": "Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a key paradigm for grounding MLLMs with external knowledge. While query pre-processing (e.g., rewriting) is standard in text-based RAG, existing MRAG pipelines predominantly treat visual inputs as static and immutable, implicitly assuming they are noise-free. However, real-world visual queries are often ``imperfect'' -- suffering from geometric distortions, quality degradation, or semantic ambiguity -- leading to catastrophic retrieval failures. To address this gap, we propose V-QPP-Bench, the first comprehensive benchmark dedicated to Visual Query Pre-processing (V-QPP). We formulate V-QPP as an agentic decision-making task where MLLMs must autonomously diagnose imperfections and deploy perceptual tools to refine queries. Our extensive evaluation across 46,700 imperfect queries and diverse MRAG paradigms reveals three critical insights: (1) Vulnerability -- visual imperfections severely degrade both retrieval recall and end-to-end MRAG performance; (2) Restoration Potential \\& Bottleneck -- while oracle preprocessing recovers near-perfect performance, off-the-shelf MLLMs struggle with tool selection and parameter prediction without specialized training; and (3) Training Enhancement -- supervised fine-tuning enables compact models to achieve comparable or superior performance to larger proprietary models, demonstrating the benchmark's value for developing robust MRAG systems The code is available at https://github.com/phycholosogy/VQQP_Bench",
      "authors": [
        "Jiankun Zhang",
        "Shenglai Zeng",
        "Kai Guo",
        "Xinnan Dai",
        "Hui Liu",
        "Jiliang Tang",
        "Yi Chang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-13 18:39:48+00:00",
      "link": "https://arxiv.org/pdf/2602.13179v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13177v1",
      "title": "Improved Regret Guarantees for Online Mirror Descent using a Portfolio of Mirror Maps",
      "abstract": "OMD and its variants give a flexible framework for OCO where the performance depends crucially on the choice of the mirror map. While the geometries underlying OPGD and OEG, both special cases of OMD, are well understood, it remains a challenging open question on how to construct an optimal mirror map for any given constrained set and a general family of loss functions, e.g., sparse losses. Motivated by parameterizing a near-optimal set of mirror maps, we consider a simpler question: is it even possible to obtain polynomial gains in regret by using mirror maps for geometries that interpolate between $L_1$ and $L_2$, which may not be possible by restricting to only OEG ($L_1$) or OPGD ($L_2$).   Our main result answers this question positively. We show that mirror maps based on block norms adapt better to the sparsity of loss functions, compared to previous $L_p$ (for $p \\in [1, 2]$) interpolations. In particular, we construct a family of online convex optimization instances in $\\mathbb{R}^d$, where block norm-based mirror maps achieve a provable polynomial (in $d$) improvement in regret over OEG and OPGD for sparse loss functions. We then turn to the setting in which the sparsity level of the loss functions is unknown. In this case, the choice of geometry itself becomes an online decision problem. We first show that naively switching between OEG and OPGD can incur linear regret, highlighting the intrinsic difficulty of geometry selection. To overcome this issue, we propose a meta-algorithm based on multiplicative weights that dynamically selects among a family of uniform block norms. We show that this approach effectively tunes OMD to the sparsity of the losses, yielding adaptive regret guarantees. Overall, our results demonstrate that online mirror-map selection can significantly enhance the ability of OMD to exploit sparsity in online convex optimization.",
      "authors": [
        "Swati Gupta",
        "Jai Moondra",
        "Mohit Singh"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.DS",
        "cs.LG"
      ],
      "published": "2026-02-13 18:37:26+00:00",
      "link": "https://arxiv.org/pdf/2602.13177v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13174v1",
      "title": "Learning functional components of PDEs from data using neural networks",
      "abstract": "Partial differential equations often contain unknown functions that are difficult or impossible to measure directly, hampering our ability to derive predictions from the model. Workflows for recovering scalar PDE parameters from data are well studied: here we show how similar workflows can be used to recover functions from data. Specifically, we embed neural networks into the PDE and show how, as they are trained on data, they can approximate unknown functions with arbitrary accuracy. Using nonlocal aggregation-diffusion equations as a case study, we recover interaction kernels and external potentials from steady state data. Specifically, we investigate how a wide range of factors, such as the number of available solutions, their properties, sampling density, and measurement noise, affect our ability to successfully recover functions. Our approach is advantageous because it can utilise standard parameter-fitting workflows, and in that the trained PDE can be treated as a normal PDE for purposes such as generating system predictions.",
      "authors": [
        "Torkel E. Loman",
        "Yurij Salmaniw",
        "Antonio Leon Villares",
        "Jose A. Carrillo",
        "Ruth E. Baker"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.AP"
      ],
      "published": "2026-02-13 18:32:33+00:00",
      "link": "https://arxiv.org/pdf/2602.13174v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13165v1",
      "title": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures",
      "abstract": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \\textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency.",
      "authors": [
        "Asmit Kumar Singh",
        "Haozhe Wang",
        "Laxmi Naga Santosh Attaluri",
        "Tak Chiam",
        "Weihua Zhu"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-13 18:25:00+00:00",
      "link": "https://arxiv.org/pdf/2602.13165v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13377v1",
      "title": "A Survey of Code Review Benchmarks and Evaluation Practices in Pre-LLM and LLM Era",
      "abstract": "Code review is a critical practice in modern software engineering, helping developers detect defects early, improve code quality, and facilitate knowledge sharing. With the rapid advancement of large language models (LLMs), a growing body of work has explored automated support for code review. However, progress in this area is hindered by the lack of a systematic understanding of existing benchmarks and evaluation practices. Current code review datasets are scattered, vary widely in design, and provide limited insight into what review capabilities are actually being assessed. In this paper, we present a comprehensive survey of code review benchmarks spanning both the Pre-LLM and LLM eras (2015--2025). We analyze 99 research papers (58 Pre-LLM era and 41 LLM era) and extract key metadata, including datasets, evaluation metrics, data sources, and target tasks. Based on this analysis, we propose a multi-level taxonomy that organizes code review research into five domains and 18 fine-grained tasks. Our study reveals a clear shift toward end-to-end generative peer review, increasing multilingual coverage, and a decline in standalone change understanding tasks. We further identify limitations of current benchmarks and outline future directions, including broader task coverage, dynamic runtime evaluation, and taxonomy-guided fine-grained assessment. This survey provides a structured foundation for developing more realistic and comprehensive benchmarks for LLM-based code review.",
      "authors": [
        "Taufiqul Islam Khan",
        "Shaowei Wang",
        "Haoxiang Zhang",
        "Tse-Hsun Chen"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-13 18:19:38+00:00",
      "link": "https://arxiv.org/pdf/2602.13377v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13155v1",
      "title": "Learning to Approximate Uniform Facility Location via Graph Neural Networks",
      "abstract": "There has been a growing interest in using neural networks, especially message-passing neural networks (MPNNs), to solve hard combinatorial optimization problems heuristically. However, existing learning-based approaches for hard combinatorial optimization tasks often rely on supervised training data, reinforcement learning, or gradient estimators, leading to significant computational overhead, unstable training, or a lack of provable performance guarantees. In contrast, classical approximation algorithms offer such performance guarantees under worst-case inputs but are non-differentiable and unable to adaptively exploit structural regularities in natural input distributions. We address this dichotomy with the fundamental example of Uniform Facility Location (UniFL), a variant of the combinatorial facility location problem with applications in clustering, data summarization, logistics, and supply chain design. We develop a fully differentiable MPNN model that embeds approximation-algorithmic principles while avoiding the need for solver supervision or discrete relaxations. Our approach admits provable approximation and size generalization guarantees to much larger instances than seen during training. Empirically, we show that our approach outperforms standard non-learned approximation algorithms in terms of solution quality, closing the gap with computationally intensive integer linear programming approaches. Overall, this work provides a step toward bridging learning-based methods and approximation algorithms for discrete optimization.",
      "authors": [
        "Chendi Qian",
        "Christopher Morris",
        "Stefanie Jegelka",
        "Christian Sohler"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DS",
        "cs.NE",
        "stat.ML"
      ],
      "published": "2026-02-13 18:08:23+00:00",
      "link": "https://arxiv.org/pdf/2602.13155v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13139v1",
      "title": "OpenLID-v3: Improving the Precision of Closely Related Language Identification -- An Experience Report",
      "abstract": "Language identification (LID) is an essential step in building high-quality multilingual datasets from web data. Existing LID tools (such as OpenLID or GlotLID) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages. In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters, and introducing a special label for marking noise. We call this extended system OpenLID-v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages. OpenLID-v3 is available on https://huggingface.co/HPLT/OpenLID-v3.",
      "authors": [
        "Mariia Fedorova",
        "Nikolay Arefyev",
        "Maja Buljan",
        "Jindřich Helcl",
        "Stephan Oepen",
        "Egil Rønningstad",
        "Yves Scherrer"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 17:47:08+00:00",
      "link": "https://arxiv.org/pdf/2602.13139v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13128v1",
      "title": "Eventizing Traditionally Opaque Binary Neural Networks as 1-safe Petri net Models",
      "abstract": "Binary Neural Networks (BNNs) offer a low-complexity and energy-efficient alternative to traditional full-precision neural networks by constraining their weights and activations to binary values. However, their discrete, highly non-linear behavior makes them difficult to explain, validate and formally verify. As a result, BNNs remain largely opaque, limiting their suitability in safety-critical domains, where causal transparency and behavioral guarantees are essential. In this work, we introduce a Petri net (PN)-based framework that captures the BNN's internal operations as event-driven processes. By \"eventizing\" their operations, we expose their causal relationships and dependencies for a fine-grained analysis of concurrency, ordering, and state evolution. Here, we construct modular PN blueprints for core BNN components including activation, gradient computation and weight updates, and compose them into a complete system-level model. We then validate the composed PN against a reference software-based BNN, verify it against reachability and structural checks to establish 1-safeness, deadlock-freeness, mutual exclusion and correct-by-construction causal sequencing, before we assess its scalability and complexity at segment, component, and system levels using the automated measurement tools in Workcraft. Overall, this framework enables causal introspection of transparent and event-driven BNNs that are amenable to formal reasoning and verification.",
      "authors": [
        "Mohamed Tarraf",
        "Alex Chan",
        "Alex Yakovlev",
        "Rishad Shafik"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 17:25:47+00:00",
      "link": "https://arxiv.org/pdf/2602.13128v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13123v1",
      "title": "From sunblock to softblock: Analyzing the correlates of neology in published writing and on social media",
      "abstract": "Living languages are shaped by a host of conflicting internal and external evolutionary pressures. While some of these pressures are universal across languages and cultures, others differ depending on the social and conversational context: language use in newspapers is subject to very different constraints than language use on social media. Prior distributional semantic work on English word emergence (neology) identified two factors correlated with creation of new words by analyzing a corpus consisting primarily of historical published texts (Ryskina et al., 2020, arXiv:2001.07740). Extending this methodology to contextual embeddings in addition to static ones and applying it to a new corpus of Twitter posts, we show that the same findings hold for both domains, though the topic popularity growth factor may contribute less to neology on Twitter than in published writing. We hypothesize that this difference can be explained by the two domains favouring different neologism formation mechanisms.",
      "authors": [
        "Maria Ryskina",
        "Matthew R. Gormley",
        "Kyle Mahowald",
        "David R. Mortensen",
        "Taylor Berg-Kirkpatrick",
        "Vivek Kulkarni"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 17:19:28+00:00",
      "link": "https://arxiv.org/pdf/2602.13123v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13120v1",
      "title": "Scoped MSO, Register Automata, and Expressions: Equivalence over Data Words",
      "abstract": "This paper establishes logical and expression-based characterizations for the class of languages recognized by nondeterministic register automata with guessing (NRA) over infinite alphabets. We introduce Scoped MSO, a logic featuring a novel segment modality and syntactic restrictions on data comparisons. We prove this logic is expressively equivalent to NRA over data domains where ``strong guessing'' can be eliminated. Furthermore, we define Data-Regular Expressions, a minimalist regular-expression calculus built from quantifier-free regions and equipped with $k$-contracting concatenation, and demonstrate its equivalence to NRA over arbitrary relational structures. Together, these formalisms provide a robust descriptive theory for register automata, bridging the gap between automata, logic, and expressions.",
      "authors": [
        "Radosław Piórkowski"
      ],
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO"
      ],
      "published": "2026-02-13 17:17:51+00:00",
      "link": "https://arxiv.org/pdf/2602.13120v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13376v1",
      "title": "An Online Reference-Free Evaluation Framework for Flowchart Image-to-Code Generation",
      "abstract": "Vision-Language Models (VLMs) are increasingly used in document processing pipelines to convert flowchart images into structured code (e.g., Mermaid). In production, these systems process arbitrary inputs for which no ground-truth code exists, making output quality difficult to assess. We propose a reference-free evaluation framework that monitors flowchart image-to-code generation quality at inference time, using only the input image and the generated output. The framework introduces two automated metrics: $\\text{Recall}{\\text{OCR}}$, which estimates content coverage by extracting text from the input image via OCR as a proxy reference, and $\\text{Precision}{\\text{VE}}$, which detects hallucinated elements through Visual Entailment against the original image. Their harmonic mean, $\\text{F1}{\\text{OCR-VE}}$, provides a unified quality score. Validation on the FlowVQA dataset shows strong agreement with ground-truth metrics (average Pearson's $r = 0.97$, $0.91$, and $0.94$ for Recall, Precision, and F1, respectively), confirming the framework's reliability as a practical, reference-free alternative for continuous quality monitoring in production settings.",
      "authors": [
        "Giang Son Nguyen",
        "Zi Pong Lim",
        "Sarthak Ketanbhai Modi",
        "Yon Shin Teo",
        "Wenya Wang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-13 17:16:03+00:00",
      "link": "https://arxiv.org/pdf/2602.13376v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13110v1",
      "title": "SCOPE: Selective Conformal Optimized Pairwise LLM Judging",
      "abstract": "Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $α$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies, providing a stronger selection signal that enables SCOPE to consistently meet the target risk level while retaining good coverage across judge scales. In particular, at $α= 0.10$, \\textsc{Scope} consistently satisfies the risk bound across all benchmarks and judge scales (empirical risk $\\approx 0.097$ to $0.099$), while retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and $0.98$ on RewardBench with Qwen-32B. Compared to naïve baselines, \\textsc{Scope} accepts up to $2.4\\times$ more judgments on MT-Bench with Qwen-7B under the same target risk constraint, demonstrating that BPE enables reliable and high-coverage LLM-based evaluation.",
      "authors": [
        "Sher Badshah",
        "Ali Emami",
        "Hassan Sajjad"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-13 17:10:43+00:00",
      "link": "https://arxiv.org/pdf/2602.13110v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13106v1",
      "title": "Which Algorithms Can Graph Neural Networks Learn?",
      "abstract": "In recent years, there has been growing interest in understanding neural architectures' ability to learn to execute discrete algorithms, a line of work often referred to as neural algorithmic reasoning. The goal is to integrate algorithmic reasoning capabilities into larger neural pipelines. Many such architectures are based on (message-passing) graph neural networks (MPNNs), owing to their permutation equivariance and ability to deal with sparsity and variable-sized inputs. However, existing work is either largely empirical and lacks formal guarantees or it focuses solely on expressivity, leaving open the question of when and how such architectures generalize beyond a finite training set. In this work, we propose a general theoretical framework that characterizes the sufficient conditions under which MPNNs can learn an algorithm from a training set of small instances and provably approximate its behavior on inputs of arbitrary size. Our framework applies to a broad class of algorithms, including single-source shortest paths, minimum spanning trees, and general dynamic programming problems, such as the $0$-$1$ knapsack problem. In addition, we establish impossibility results for a wide range of algorithmic tasks, showing that standard MPNNs cannot learn them, and we derive more expressive MPNN-like architectures that overcome these limitations. Finally, we refine our analysis for the Bellman-Ford algorithm, yielding a substantially smaller required training set and significantly extending the recent work of Nerem et al. [2025] by allowing for a differentiable regularization loss. Empirical results largely support our theoretical findings.",
      "authors": [
        "Solveig Wittig",
        "Antonis Vasileiou",
        "Robert R. Nerem",
        "Timo Stoll",
        "Floris Geerts",
        "Yusu Wang",
        "Christopher Morris"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS",
        "cs.NE"
      ],
      "published": "2026-02-13 17:09:50+00:00",
      "link": "https://arxiv.org/pdf/2602.13106v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13104v2",
      "title": "Random Forests as Statistical Procedures: Design, Variance, and Dependence",
      "abstract": "Random forests are widely used prediction procedures, yet are typically described algorithmically rather than as statistical designs acting on a fixed set of covariates. We develop a finite-sample, design-based formulation of random forests in which each tree is an explicit randomized conditional regression function. This perspective yields an exact variance identity for the forest predictor that separates finite-aggregation variability from a structural dependence term that persists even under infinite aggregation. We further decompose both single-tree dispersion and inter-tree covariance using the laws of total variance and covariance, isolating two fundamental design mechanisms-reuse of training observations and alignment of data-adaptive partitions. These mechanisms induce a strict covariance floor, demonstrating that predictive variability cannot be eliminated by increasing the number of trees alone. The resulting framework clarifies how resampling, feature-level randomization, and split selection govern resolution, tree variability, and dependence, and establishes random forests as explicit finite-sample statistical designs whose behavior is determined by their underlying randomized construction.",
      "authors": [
        "Nathaniel S. O'Connell"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "published": "2026-02-13 17:08:43+00:00",
      "link": "https://arxiv.org/pdf/2602.13104v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13103v2",
      "title": "R-Diverse: Mitigating Diversity Illusion in Self-Play LLM Training",
      "abstract": "Self-play bootstraps LLM reasoning through an iterative Challenger-Solver loop: the Challenger is trained to generate questions that target the Solver's capabilities, and the Solver is optimized on the generated data to expand its reasoning skills. However, existing frameworks like R-Zero often exhibit non-sustained improvement, where early gains degrade as self-play continues. We identify a key failure mode, Diversity Illusion, where the Solver's training signals appear diverse yet collapse into recurring underlying patterns. It manifests as (1) Local Diversity Illusion, where diversity is enforced only within-batch, inducing cross-iteration mode cycling; and (2) Surface Diversity Illusion, where questions vary superficially but require near-identical reasoning skills. To mitigate them, we propose R-Diverse with two aligned innovations: Memory-Augmented Penalty (MAP), which uses a persistent memory bank to discourage recycling across iterations, and Skill-Aware Measurement (SAM), which evaluates diversity by the reasoning skills exercised rather than surface variation of questions. Across 10 math and general reasoning benchmarks, R-Diverse sustains gains over more iterations and consistently outperforms prior self-play methods. Code is available at https://github.com/Gengsheng-Li/R-Diverse.",
      "authors": [
        "Gengsheng Li",
        "Jinghan He",
        "Shijie Wang",
        "Dan Zhang",
        "Ruiqi Liu",
        "Renrui Zhang",
        "Zijun Yao",
        "Junfeng Fang",
        "Haiyun Guo",
        "Jinqiao Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 17:07:42+00:00",
      "link": "https://arxiv.org/pdf/2602.13103v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13102v1",
      "title": "Towards interpretable models for language proficiency assessment: Predicting the CEFR level of Estonian learner texts",
      "abstract": "Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9. Additional evaluation on an earlier exam sample revealed that the writings have become more complex over a 7-10-year period, while accuracy still reached 0.8 with some feature sets. The results have been implemented in the writing evaluation module of an Estonian open-source language learning environment.",
      "authors": [
        "Kais Allkivi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 17:06:17+00:00",
      "link": "https://arxiv.org/pdf/2602.13102v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13098v1",
      "title": "Barron-Wiener-Laguerre models",
      "abstract": "We propose a probabilistic extension of Wiener-Laguerre models for causal operator learning. Classical Wiener-Laguerre models parameterize stable linear dynamics using orthonormal Laguerre bases and apply a static nonlinear map to the resulting features. While structurally efficient and interpretable, they provide only deterministic point estimates. We reinterpret the nonlinear component through the lens of Barron function approximation, viewing two-layer networks, random Fourier features, and extreme learning machines as discretizations of integral representations over parameter measures. This perspective naturally admits Bayesian inference on the nonlinear map and yields posterior predictive uncertainty. By combining Laguerre-parameterized causal dynamics with probabilistic Barron-type nonlinear approximators, we obtain a structured yet expressive class of causal operators equipped with uncertainty quantification. The resulting framework bridges classical system identification and modern measure-based function approximation, providing a principled approach to time-series modeling and nonlinear systems identification.",
      "authors": [
        "Rahul Manavalan",
        "Filip Tronarp"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "cs.LG"
      ],
      "published": "2026-02-13 17:02:48+00:00",
      "link": "https://arxiv.org/pdf/2602.13098v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13093v2",
      "title": "Consistency of Large Reasoning Models Under Multi-Turn Attacks",
      "abstract": "Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.",
      "authors": [
        "Yubo Li",
        "Ramayya Krishnan",
        "Rema Padman"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-13 16:58:47+00:00",
      "link": "https://arxiv.org/pdf/2602.13093v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13091v1",
      "title": "Universal Transformation of One-Class Classifiers for Unsupervised Anomaly Detection",
      "abstract": "Detecting anomalies in images and video is an essential task for multiple real-world problems, including industrial inspection, computer-assisted diagnosis, and environmental monitoring. Anomaly detection is typically formulated as a one-class classification problem, where the training data consists solely of nominal values, leaving methods built on this assumption susceptible to training label noise. We present a dataset folding method that transforms an arbitrary one-class classifier-based anomaly detector into a fully unsupervised method. This is achieved by making a set of key weak assumptions: that anomalies are uncommon in the training dataset and generally heterogeneous. These assumptions enable us to utilize multiple independently trained instances of a one-class classifier to filter the training dataset for anomalies. This transformation requires no modifications to the underlying anomaly detector; the only changes are algorithmically selected data subsets used for training. We demonstrate that our method can transform a wide variety of one-class classifier anomaly detectors for both images and videos into unsupervised ones. Our method creates the first unsupervised logical anomaly detectors by transforming existing methods. We also demonstrate that our method achieves state-of-the-art performance for unsupervised anomaly detection on the MVTec AD, ViSA, and MVTec Loco AD datasets. As improvements to one-class classifiers are made, our method directly transfers those improvements to the unsupervised domain, linking the domains.",
      "authors": [
        "Declan McIntosh",
        "Alexandra Branzan Albu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-13 16:54:12+00:00",
      "link": "https://arxiv.org/pdf/2602.13091v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13087v1",
      "title": "EXCODER: EXplainable Classification Of DiscretE time series Representations",
      "abstract": "Deep learning has significantly improved time series classification, yet the lack of explainability in these models remains a major challenge. While Explainable AI (XAI) techniques aim to make model decisions more transparent, their effectiveness is often hindered by the high dimensionality and noise present in raw time series data. In this work, we investigate whether transforming time series into discrete latent representations-using methods such as Vector Quantized Variational Autoencoders (VQ-VAE) and Discrete Variational Autoencoders (DVAE)-not only preserves but enhances explainability by reducing redundancy and focusing on the most informative patterns. We show that applying XAI methods to these compressed representations leads to concise and structured explanations that maintain faithfulness without sacrificing classification performance. Additionally, we propose Similar Subsequence Accuracy (SSA), a novel metric that quantitatively assesses the alignment between XAI-identified salient subsequences and the label distribution in the training data. SSA provides a systematic way to validate whether the features highlighted by XAI methods are truly representative of the learned classification patterns. Our findings demonstrate that discrete latent representations not only retain the essential characteristics needed for classification but also offer a pathway to more compact, interpretable, and computationally efficient explanations in time series analysis.",
      "authors": [
        "Yannik Hahn",
        "Antonin Königsfeld",
        "Hasan Tercan",
        "Tobias Meisen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 16:47:45+00:00",
      "link": "https://arxiv.org/pdf/2602.13087v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13075v1",
      "title": "Unified Multi-Domain Graph Pre-training for Homogeneous and Heterogeneous Graphs via Domain-Specific Expert Encoding",
      "abstract": "Graph pre-training has achieved remarkable success in recent years, delivering transferable representations for downstream adaptation. However, most existing methods are designed for either homogeneous or heterogeneous graphs, thereby hindering unified graph modeling across diverse graph types. This separation contradicts real-world applications, where mixed homogeneous and heterogeneous graphs are ubiquitous, and distribution shifts between upstream pre-training and downstream deployment are common. In this paper, we empirically demonstrate that a balanced mixture of homogeneous and heterogeneous graph pre-training benefits downstream tasks and propose a unified multi-domain \\textbf{G}raph \\textbf{P}re-training method across \\textbf{H}omogeneous and \\textbf{H}eterogeneous graphs ($\\mathbf{GPH^{2}}$). To address the lack of a unified encoder for homogeneous and heterogeneous graphs, we propose a Unified Multi-View Graph Construction that simultaneously encodes both without explicit graph-type-specific designs. To cope with the increased cross-domain distribution discrepancies arising from mixed graphs, we introduce domain-specific expert encoding. Each expert is independently pre-trained on a single graph to capture domain-specific knowledge, thereby shielding the pre-training encoder from the adverse effects of cross-domain discrepancies. For downstream tasks, we further design a Task-oriented Expert Fusion Strategy that adaptively integrates multiple experts based on their discriminative strengths. Extensive experiments on mixed graphs demonstrate that $\\text{GPH}^{2}$ enables stable transfer across graph types and domains, significantly outperforming existing graph pre-training methods.",
      "authors": [
        "Chundong Liang",
        "Yongqi Huang",
        "Dongxiao He",
        "Peiyuan Li",
        "Yawen Li",
        "Di Jin",
        "Weixiong Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 16:34:55+00:00",
      "link": "https://arxiv.org/pdf/2602.13075v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13073v1",
      "title": "LCSB: Layer-Cyclic Selective Backpropagation for Memory-Efficient On-Device LLM Fine-Tuning",
      "abstract": "Memory-efficient backpropagation (MeBP) has enabled first-order fine-tuning of large language models (LLMs) on mobile devices with less than 1GB memory. However, MeBP requires backward computation through all transformer layers at every step, where weight decompression alone accounts for 32--42% of backward time. We propose Layer-Cyclic Selective Backpropagation (LCSB), which computes gradients for only a subset of layers per step. Our key insight is that residual connections guarantee gradient flow through identity paths, while AdamW momentum provides implicit updates for non-selected layers. We interpret LCSB as Block Coordinate Descent on the LoRA parameter space, providing theoretical justification for convergence. LCSB achieves up to 1.40$\\times$ speedup with less than 2\\% quality degradation across five models and three tasks. Surprisingly, in 4-bit quantized settings, LCSB exhibits superior stability: a 3B model that completely diverges under full backpropagation converges smoothly with LCSB, suggesting an implicit regularization effect from selective gradient computation.",
      "authors": [
        "Juneyoung Park",
        "Eunbeen Yoon",
        "Seongwan Kim. Jaeho Lee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-13 16:32:53+00:00",
      "link": "https://arxiv.org/pdf/2602.13073v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13069v1",
      "title": "Memory-Efficient Structured Backpropagation for On-Device LLM Fine-Tuning",
      "abstract": "On-device fine-tuning enables privacy-preserving personalization of large language models, but mobile devices impose severe memory constraints, typically 6--12GB shared across all workloads. Existing approaches force a trade-off between exact gradients with high memory (MeBP) and low memory with noisy estimates (MeZO). We propose Memory-efficient Structured Backpropagation (MeSP), which bridges this gap by manually deriving backward passes that exploit LoRA's low-rank structure. Our key insight is that the intermediate projection $h = xA$ can be recomputed during backward at minimal cost since rank $r \\ll d_{in}$, eliminating the need to store it. MeSP achieves 49\\% average memory reduction compared to MeBP on Qwen2.5 models (0.5B--3B) while computing mathematically identical gradients. Our analysis also reveals that MeZO's gradient estimates show near-zero correlation with true gradients (cosine similarity $\\approx$0.001), explaining its slow convergence. MeSP reduces peak memory from 361MB to 136MB for Qwen2.5-0.5B, enabling fine-tuning scenarios previously infeasible on memory-constrained devices.",
      "authors": [
        "Juneyoung Park",
        "Yuri Hong",
        "Seongwan Kim",
        "Jaeho Lee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-13 16:24:33+00:00",
      "link": "https://arxiv.org/pdf/2602.13069v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13061v1",
      "title": "Diverging Flows: Detecting Extrapolations in Conditional Generation",
      "abstract": "The ability of Flow Matching (FM) to model complex conditional distributions has established it as the state-of-the-art for prediction tasks (e.g., robotics, weather forecasting). However, deployment in safety-critical settings is hindered by a critical extrapolation hazard: driven by smoothness biases, flow models yield plausible outputs even for off-manifold conditions, resulting in silent failures indistinguishable from valid predictions. In this work, we introduce Diverging Flows, a novel approach that enables a single model to simultaneously perform conditional generation and native extrapolation detection by structurally enforcing inefficient transport for off-manifold inputs. We evaluate our method on synthetic manifolds, cross-domain style transfer, and weather temperature forecasting, demonstrating that it achieves effective detection of extrapolations without compromising predictive fidelity or inference latency. These results establish Diverging Flows as a robust solution for trustworthy flow models, paving the way for reliable deployment in domains such as medicine, robotics, and climate science.",
      "authors": [
        "Constantinos Tsakonas",
        "Serena Ivaldi",
        "Jean-Baptiste Mouret"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-02-13 16:15:58+00:00",
      "link": "https://arxiv.org/pdf/2602.13061v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13059v1",
      "title": "TraceBack: Multi-Agent Decomposition for Fine-Grained Table Attribution",
      "abstract": "Question answering (QA) over structured tables requires not only accurate answers but also transparency about which cells support them. Existing table QA systems rarely provide fine-grained attribution, so even correct answers often lack verifiable grounding, limiting trust in high-stakes settings. We address this with TraceBack, a modular multi-agent framework for scalable, cell-level attribution in single-table QA. TraceBack prunes tables to relevant rows and columns, decomposes questions into semantically coherent sub-questions, and aligns each answer span with its supporting cells, capturing both explicit and implicit evidence used in intermediate reasoning steps. To enable systematic evaluation, we release CITEBench, a benchmark with phrase-to-cell annotations drawn from ToTTo, FetaQA, and AITQA. We further propose FairScore, a reference-less metric that compares atomic facts derived from predicted cells and answers to estimate attribution precision and recall without human cell labels. Experiments show that TraceBack substantially outperforms strong baselines across datasets and granularities, while FairScore closely tracks human judgments and preserves relative method rankings, supporting interpretable and scalable evaluation of table-based QA.",
      "authors": [
        "Tejas Anvekar",
        "Junha Park",
        "Rajat Jha",
        "Devanshu Gupta",
        "Poojah Ganesan",
        "Puneeth Mathur",
        "Vivek Gupta"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 16:13:36+00:00",
      "link": "https://arxiv.org/pdf/2602.13059v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13052v1",
      "title": "Quantization-Aware Collaborative Inference for Large Embodied AI Models",
      "abstract": "Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents. To address this issue, we investigate quantization-aware collaborative inference (co-inference) for embodied AI systems. First, we develop a tractable approximation for quantization-induced inference distortion. Based on this approximation, we derive lower and upper bounds on the quantization rate-inference distortion function, characterizing its dependence on LAIM statistics, including the quantization bit-width. Next, we formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints, aiming to minimize the distortion upper bound while ensuring tightness through the corresponding lower bound. Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the proposed joint design. Particularly, simulations and real-world testbed experiments demonstrate the effectiveness of the proposed joint design in balancing inference quality, latency, and energy consumption in edge embodied AI systems.",
      "authors": [
        "Zhonghao Lyu",
        "Ming Xiao",
        "Mikael Skoglund",
        "Merouane Debbah",
        "H. Vincent Poor"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "published": "2026-02-13 16:08:19+00:00",
      "link": "https://arxiv.org/pdf/2602.13052v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13047v1",
      "title": "Can we trust AI to detect healthy multilingual English speakers among the cognitively impaired cohort in the UK? An investigation using real-world conversational speech",
      "abstract": "Conversational speech often reveals early signs of cognitive decline, such as dementia and MCI. In the UK, one in four people belongs to an ethnic minority, and dementia prevalence is expected to rise most rapidly among Black and Asian communities. This study examines the trustworthiness of AI models, specifically the presence of bias, in detecting healthy multilingual English speakers among the cognitively impaired cohort, to make these tools clinically beneficial. For experiments, monolingual participants were recruited nationally (UK), and multilingual speakers were enrolled from four community centres in Sheffield and Bradford. In addition to a non-native English accent, multilinguals spoke Somali, Chinese, or South Asian languages, who were further divided into two Yorkshire accents (West and South) to challenge the efficiency of the AI tools thoroughly. Although ASR systems showed no significant bias across groups, classification and regression models using acoustic and linguistic features exhibited bias against multilingual speakers, particularly in memory, fluency, and reading tasks. This bias was more pronounced when models were trained on the publicly available DementiaBank dataset. Moreover, multilinguals were more likely to be misclassified as having cognitive decline. This study is the first of its kind to discover that, despite their strong overall performance, current AI models show bias against multilingual individuals from ethnic minority backgrounds in the UK, and they are also more likely to misclassify speakers with a certain accent (South Yorkshire) as living with a more severe cognitive decline. In this pilot study, we conclude that the existing AI tools are therefore not yet reliable for diagnostic use in these populations, and we aim to address this in future work by developing more generalisable, bias-mitigated models.",
      "authors": [
        "Madhurananda Pahar",
        "Caitlin Illingworth",
        "Dorota Braun",
        "Bahman Mirheidari",
        "Lise Sproson",
        "Daniel Blackburn",
        "Heidi Christensen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-13 16:03:37+00:00",
      "link": "https://arxiv.org/pdf/2602.13047v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13046v1",
      "title": "Classification of Local Optimization Problems in Directed Cycles",
      "abstract": "We present a complete classification of the distributed computational complexity of local optimization problems in directed cycles for both the deterministic and the randomized LOCAL model. We show that for any local optimization problem $Π$ (that can be of the form min-sum, max-sum, min-max, or max-min, for any local cost or utility function over some finite alphabet), and for any \\emph{constant} approximation ratio $α$, the task of finding an $α$-approximation of $Π$ in directed cycles has one of the following complexities:   1. $O(1)$ rounds in deterministic LOCAL, $O(1)$ rounds in randomized LOCAL,   2. $Θ(\\log^* n)$ rounds in deterministic LOCAL, $O(1)$ rounds in randomized LOCAL,   3. $Θ(\\log^* n)$ rounds in deterministic LOCAL, $Θ(\\log^* n)$ rounds in randomized LOCAL,   4. $Θ(n)$ rounds in deterministic LOCAL, $Θ(n)$ rounds in randomized LOCAL.   Moreover, for any given $Π$ and $α$, we can determine the complexity class automatically, with an efficient (centralized, sequential) meta-algorithm, and we can also efficiently synthesize an asymptotically optimal distributed algorithm.   Before this work, similar results were only known for local search problems (e.g., locally checkable labeling problems). The family of local optimization problems is a strict generalization of local search problems, and it contains numerous commonly studied distributed tasks, such as the problems of finding approximations of the maximum independent set, minimum vertex cover, minimum dominating set, and minimum vertex coloring.",
      "authors": [
        "Thomas Boudier",
        "Fabian Kuhn",
        "Augusto Modanese",
        "Ronja Stimpert",
        "Jukka Suomela"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.CC",
        "cs.FL"
      ],
      "published": "2026-02-13 16:03:14+00:00",
      "link": "https://arxiv.org/pdf/2602.13046v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13042v1",
      "title": "GPTZero: Robust Detection of LLM-Generated Texts",
      "abstract": "While historical considerations surrounding text authenticity revolved primarily around plagiarism, the advent of large language models (LLMs) has introduced a new challenge: distinguishing human-authored from AI-generated text. This shift raises significant concerns, including the undermining of skill evaluations, the mass-production of low-quality content, and the proliferation of misinformation. Addressing these issues, we introduce GPTZero a state-of-the-art industrial AI detection solution, offering reliable discernment between human and LLM-generated text. Our key contributions include: introducing a hierarchical, multi-task architecture enabling a flexible taxonomy of human and AI texts, demonstrating state-of-the-art accuracy on a variety of domains with granular predictions, and achieving superior robustness to adversarial attacks and paraphrasing via multi-tiered automated red teaming. GPTZero offers accurate and explainable detection, and educates users on its responsible use, ensuring fair and transparent assessment of text.",
      "authors": [
        "George Alexandru Adam",
        "Alexander Cui",
        "Edwin Thomas",
        "Emily Napier",
        "Nazar Shmatko",
        "Jacob Schnell",
        "Jacob Junqi Tian",
        "Alekhya Dronavalli",
        "Edward Tian",
        "Dongwon Lee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 15:53:45+00:00",
      "link": "https://arxiv.org/pdf/2602.13042v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13040v1",
      "title": "TCRL: Temporal-Coupled Adversarial Training for Robust Constrained Reinforcement Learning in Worst-Case Scenarios",
      "abstract": "Constrained Reinforcement Learning (CRL) aims to optimize decision-making policies under constraint conditions, making it highly applicable to safety-critical domains such as autonomous driving, robotics, and power grid management. However, existing robust CRL approaches predominantly focus on single-step perturbations and temporally independent adversarial models, lacking explicit modeling of robustness against temporally coupled perturbations. To tackle these challenges, we propose TCRL, a novel temporal-coupled adversarial training framework for robust constrained reinforcement learning (TCRL) in worst-case scenarios. First, TCRL introduces a worst-case-perceived cost constraint function that estimates safety costs under temporally coupled perturbations without the need to explicitly model adversarial attackers. Second, TCRL establishes a dual-constraint defense mechanism on the reward to counter temporally coupled adversaries while maintaining reward unpredictability. Experimental results demonstrate that TCRL consistently outperforms existing methods in terms of robustness against temporally coupled perturbation attacks across a variety of CRL tasks.",
      "authors": [
        "Wentao Xu",
        "Zhongming Yao",
        "Weihao Li",
        "Zhenghang Song",
        "Yumeng Song",
        "Tianyi Li",
        "Yushuai Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 15:48:20+00:00",
      "link": "https://arxiv.org/pdf/2602.13040v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13035v1",
      "title": "Look Inward to Explore Outward: Learning Temperature Policy from LLM Internal States via Hierarchical RL",
      "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) trains large language models (LLMs) from sampled trajectories, making decoding strategy a core component of learning rather than a purely inference-time choice. Sampling temperature directly controls the exploration--exploitation trade-off by modulating policy entropy, yet existing methods rely on static values or heuristic adaptations that are decoupled from task-level rewards. We propose Introspective LLM, a hierarchical reinforcement learning framework that learns to control sampling temperature during generation. At each decoding step, the model selects a temperature based on its hidden state and samples the next token from the resulting distribution. Temperature and token policies are jointly optimized from downstream rewards using a coordinate ascent scheme. Experiments on mathematical reasoning benchmarks show that learned temperature policies outperform fixed and heuristic baselines, while exhibiting interpretable exploration behaviors aligned with reasoning uncertainty.",
      "authors": [
        "Yixiao Zhou",
        "Yang Li",
        "Dongzhou Cheng",
        "Hehe Fan",
        "Yu Cheng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-13 15:42:59+00:00",
      "link": "https://arxiv.org/pdf/2602.13035v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13024v1",
      "title": "FedHENet: A Frugal Federated Learning Framework for Heterogeneous Environments",
      "abstract": "Federated Learning (FL) enables collaborative training without centralizing data, essential for privacy compliance in real-world scenarios involving sensitive visual information. Most FL approaches rely on expensive, iterative deep network optimization, which still risks privacy via shared gradients. In this work, we propose FedHENet, extending the FedHEONN framework to image classification. By using a fixed, pre-trained feature extractor and learning only a single output layer, we avoid costly local fine-tuning. This layer is learned by analytically aggregating client knowledge in a single round of communication using homomorphic encryption (HE). Experiments show that FedHENet achieves competitive accuracy compared to iterative FL baselines while demonstrating superior stability performance and up to 70\\% better energy efficiency. Crucially, our method is hyperparameter-free, removing the carbon footprint associated with hyperparameter tuning in standard FL. Code available in https://github.com/AlejandroDopico2/FedHENet/",
      "authors": [
        "Alejandro Dopico-Castro",
        "Oscar Fontenla-Romero",
        "Bertha Guijarro-Berdiñas",
        "Amparo Alonso-Betanzos",
        "Iván Pérez Digón"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-13 15:30:18+00:00",
      "link": "https://arxiv.org/pdf/2602.13024v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13021v2",
      "title": "Prior-Guided Symbolic Regression: Towards Scientific Consistency in Equation Discovery",
      "abstract": "Symbolic Regression (SR) aims to discover interpretable equations from observational data, with the potential to reveal underlying principles behind natural phenomena. However, existing approaches often fall into the Pseudo-Equation Trap: producing equations that fit observations well but remain inconsistent with fundamental scientific principles. A key reason is that these approaches are dominated by empirical risk minimization, lacking explicit constraints to ensure scientific consistency. To bridge this gap, we propose PG-SR, a prior-guided SR framework built upon a three-stage pipeline consisting of warm-up, evolution, and refinement. Throughout the pipeline, PG-SR introduces a prior constraint checker that explicitly encodes domain priors as executable constraint programs, and employs a Prior Annealing Constrained Evaluation (PACE) mechanism during the evolution stage to progressively steer discovery toward scientifically consistent regions. Theoretically, we prove that PG-SR reduces the Rademacher complexity of the hypothesis space, yielding tighter generalization bounds and establishing a guarantee against pseudo-equations. Experimentally, PG-SR outperforms state-of-the-art baselines across diverse domains, maintaining robustness to varying prior quality, noisy data, and data scarcity.",
      "authors": [
        "Jing Xiao",
        "Xinhai Chen",
        "Jiaming Peng",
        "Qinglin Wang",
        "Menghan Jia",
        "Zhiquan Lai",
        "Guangping Yu",
        "Dongsheng Li",
        "Tiejun Li",
        "Jie Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 15:26:21+00:00",
      "link": "https://arxiv.org/pdf/2602.13021v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13017v1",
      "title": "Synaptic Activation and Dual Liquid Dynamics for Interpretable Bio-Inspired Models",
      "abstract": "In this paper, we present a unified framework for various bio-inspired models to better understand their structural and functional differences. We show that liquid-capacitance-extended models lead to interpretable behavior even in dense, all-to-all recurrent neural network (RNN) policies. We further demonstrate that incorporating chemical synapses improves interpretability and that combining chemical synapses with synaptic activation yields the most accurate and interpretable RNN models. To assess the accuracy and interpretability of these RNN policies, we consider the challenging lane-keeping control task and evaluate performance across multiple metrics, including turn-weighted validation loss, neural activity during driving, absolute correlation between neural activity and road trajectory, saliency maps of the networks' attention, and the robustness of their saliency maps measured by the structural similarity index.",
      "authors": [
        "Mónika Farsang",
        "Radu Grosu"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-13 15:23:37+00:00",
      "link": "https://arxiv.org/pdf/2602.13017v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13015v1",
      "title": "Multimodal Classification via Total Correlation Maximization",
      "abstract": "Multimodal learning integrates data from diverse sensors to effectively harness information from different modalities. However, recent studies reveal that joint learning often overfits certain modalities while neglecting others, leading to performance inferior to that of unimodal learning. Although previous efforts have sought to balance modal contributions or combine joint and unimodal learning, thereby mitigating the degradation of weaker modalities with promising outcomes, few have examined the relationship between joint and unimodal learning from an information-theoretic perspective. In this paper, we theoretically analyze modality competition and propose a method for multimodal classification by maximizing the total correlation between multimodal features and labels. By maximizing this objective, our approach alleviates modality competition while capturing inter-modal interactions via feature alignment. Building on Mutual Information Neural Estimation (MINE), we introduce Total Correlation Neural Estimation (TCNE) to derive a lower bound for total correlation. Subsequently, we present TCMax, a hyperparameter-free loss function that maximizes total correlation through variational bound optimization. Extensive experiments demonstrate that TCMax outperforms state-of-the-art joint and unimodal learning approaches. Our code is available at https://github.com/hubaak/TCMax.",
      "authors": [
        "Feng Yu",
        "Xiangyu Wu",
        "Yang Yang",
        "Jianfeng Lu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-13 15:21:45+00:00",
      "link": "https://arxiv.org/pdf/2602.13015v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13004v1",
      "title": "Uncertainty in Federated Granger Causality: From Origins to Systemic Consequences",
      "abstract": "Granger Causality (GC) provides a rigorous framework for learning causal structures from time-series data. Recent federated variants of GC have targeted distributed infrastructure applications (e.g., smart grids) with distributed clients that generate high-dimensional data bound by data-sovereignty constraints. However, Federated GC algorithms only yield deterministic point estimates of causality and neglect uncertainty. This paper establishes the first methodology for rigorously quantifying uncertainty and its propagation within federated GC frameworks. We systematically classify sources of uncertainty, explicitly differentiating aleatoric (data noise) from epistemic (model variability) effects. We derive closed-form recursions that model the evolution of uncertainty through client-server interactions and identify four novel cross-covariance components that couple data uncertainties with model parameter uncertainties across the federated architecture. We also define rigorous convergence conditions for these uncertainty recursions and obtain explicit steady-state variances for both server and client model parameters. Our convergence analysis demonstrates that steady-state variances depend exclusively on client data statistics, thus eliminating dependence on initial epistemic priors and enhancing robustness. Empirical evaluations on synthetic benchmarks and real-world industrial datasets demonstrate that explicitly characterizing uncertainty significantly improves the reliability and interpretability of federated causal inference.",
      "authors": [
        "Ayush Mohanty",
        "Nazal Mohamed",
        "Nagi Gebraeel"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-13 15:12:18+00:00",
      "link": "https://arxiv.org/pdf/2602.13004v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12989v1",
      "title": "Evaluating the Homogeneity of Keyphrase Prediction Models",
      "abstract": "Keyphrases which are useful in several NLP and IR applications are either extracted from text or predicted by generative models. Contrarily to keyphrase extraction approaches, keyphrase generation models can predict keyphrases that do not appear in a document's text called `absent keyphrases`. This ability means that keyphrase generation models can associate a document to a notion that is not explicitly mentioned in its text. Intuitively, this suggests that for two documents treating the same subjects, a keyphrase generation model is more likely to be homogeneous in their indexing i.e. predict the same keyphrase for both documents, regardless of those keyphrases appearing in their respective text or not; something a keyphrase extraction model would fail to do. Yet, homogeneity of keyphrase prediction models is not covered by current benchmarks. In this work, we introduce a method to evaluate the homogeneity of keyphrase prediction models and study if absent keyphrase generation capabilities actually help the model to be more homogeneous. To our surprise, we show that keyphrase extraction methods are competitive with generative models, and that the ability to generate absent keyphrases can actually have a negative impact on homogeneity. Our data, code and prompts are available on huggingface and github.",
      "authors": [
        "Maël Houbre",
        "Florian Boudin",
        "Beatrice Daille"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 15:00:35+00:00",
      "link": "https://arxiv.org/pdf/2602.12989v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12986v1",
      "title": "A two-step approach for speech enhancement in low-SNR scenarios using cyclostationary beamforming and DNNs",
      "abstract": "Deep Neural Networks (DNNs) often struggle to suppress noise at low signal-to-noise ratios (SNRs). This paper addresses speech enhancement in scenarios dominated by harmonic noise and proposes a framework that integrates cyclostationarity-aware preprocessing with lightweight DNN-based denoising. A cyclic minimum power distortionless response (cMPDR) spectral beamformer is used as a preprocessing block. It exploits the spectral correlations of cyclostationary noise to suppress harmonic components prior to learning-based enhancement and does not require modifications to the DNN architecture. The proposed pipeline is evaluated in a single-channel setting using two DNN architectures: a simple and lightweight convolutional recurrent neural network (CRNN), and a state-of-the-art model, namely ultra-low complexity network (ULCNet). Experiments on synthetic data and real-world recordings dominated by rotating machinery noise demonstrate consistent improvements over end-to-end DNN baselines, particularly at low SNRs. Remarkably, a parameter-efficient CRNN with cMPDR preprocessing surpasses the performance of the larger ULCNet operating on raw or Wiener-filtered inputs. These results indicate that explicitly incorporating cyclostationarity as a signal prior is more effective than increasing model capacity alone for suppressing harmonic interference.",
      "authors": [
        "Giovanni Bologni",
        "Nicolás Arrieta Larraza",
        "Richard Heusdens",
        "Richard C. Hendriks"
      ],
      "primary_category": "eess.AS",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "published": "2026-02-13 14:59:43+00:00",
      "link": "https://arxiv.org/pdf/2602.12986v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12984v1",
      "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents",
      "abstract": "Scientific reasoning inherently demands integrating sophisticated toolkits to navigate domain-specific knowledge. Yet, current benchmarks largely overlook agents' ability to orchestrate tools for such rigorous workflows. To bridge this gap, we introduce SciAgentGym, a scalable interactive environment featuring 1,780 domain-specific tools across four natural science disciplines, supported by a robust execution infrastructure. Complementing this, we present SciAgentBench, a tiered evaluation suite designed to stress-test agentic capabilities from elementary actions to long-horizon workflows. Our evaluation identifies a critical bottleneck: state-of-the-art models struggle with complex scientific tool-use. Even for a leading model like GPT-5, success rates drop sharply from 60.6% to 30.9% as interaction horizons extend, primarily due to failures in multi-step workflow execution. To address this, we propose SciForge, a data synthesis method that models the tool action space as a dependency graph to generate logic-aware training trajectories. By fine-tuning on these trajectories, our SciAgent-8B outperforms the significantly larger Qwen3-VL-235B-Instruct while exhibiting positive cross-domain transfer of scientific tool-use capabilities. These results underscore the promising potential of next-generation autonomous scientific agents.",
      "authors": [
        "Yujiong Shen",
        "Yajie Yang",
        "Zhiheng Xi",
        "Binze Hu",
        "Huayu Sha",
        "Jiazheng Zhang",
        "Qiyuan Peng",
        "Junlin Shang",
        "Jixuan Huang",
        "Yutao Fan",
        "Jingqi Tong",
        "Shihan Dou",
        "Ming Zhang",
        "Lei Bai",
        "Zhenfei Yin",
        "Tao Gui",
        "Xingjun Ma",
        "Qi Zhang",
        "Xuanjing Huang",
        "Yu-Gang Jiang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 14:58:18+00:00",
      "link": "https://arxiv.org/pdf/2602.12984v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12982v1",
      "title": "Multi-Dimensional Visual Data Recovery: Scale-Aware Tensor Modeling and Accelerated Randomized Computation",
      "abstract": "The recently proposed fully-connected tensor network (FCTN) decomposition has demonstrated significant advantages in correlation characterization and transpositional invariance, and has achieved notable achievements in multi-dimensional data processing and analysis. However, existing multi-dimensional data recovery methods leveraging FCTN decomposition still have room for further enhancement, particularly in computational efficiency and modeling capability. To address these issues, we first propose a FCTN-based generalized nonconvex regularization paradigm from the perspective of gradient mapping. Then, reliable and scalable multi-dimensional data recovery models are investigated, where the model formulation is shifted from unquantized observations to coarse-grained quantized observations. Based on the alternating direction method of multipliers (ADMM) framework, we derive efficient optimization algorithms with convergence guarantees to solve the formulated models. To alleviate the computational bottleneck encountered when processing large-scale multi-dimensional data, fast and efficient randomized compression algorithms are devised in virtue of sketching techniques in numerical linear algebra. These dimensionality-reduction techniques serve as the computational acceleration core of our proposed algorithm framework. Theoretical results on approximation error upper bounds and convergence analysis for the proposed method are derived. Extensive numerical experiments illustrate the effectiveness and superiority of the proposed algorithm over other state-of-the-art methods in terms of quantitative metrics, visual quality, and running time.",
      "authors": [
        "Wenjin Qin",
        "Hailin Wang",
        "Jiangjun Peng",
        "Jianjun Wang",
        "Tingwen Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 14:56:37+00:00",
      "link": "https://arxiv.org/pdf/2602.12982v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12976v1",
      "title": "Drift-Aware Variational Autoencoder-based Anomaly Detection with Two-level Ensembling",
      "abstract": "In today's digital world, the generation of vast amounts of streaming data in various domains has become ubiquitous. However, many of these data are unlabeled, making it challenging to identify events, particularly anomalies. This task becomes even more formidable in nonstationary environments where model performance can deteriorate over time due to concept drift. To address these challenges, this paper presents a novel method, VAE++ESDD, which employs incremental learning and two-level ensembling: an ensemble of Variational AutoEncoder(VAEs) for anomaly prediction, along with an ensemble of concept drift detectors. Each drift detector utilizes a statistical-based concept drift mechanism. To evaluate the effectiveness of VAE++ESDD, we conduct a comprehensive experimental study using real-world and synthetic datasets characterized by severely or extremely low anomalous rates and various drift characteristics. Our study reveals that the proposed method significantly outperforms both strong baselines and state-of-the-art methods.",
      "authors": [
        "Jin Li",
        "Kleanthis Malialis",
        "Christos G. Panayiotou",
        "Marios M. Polycarpou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 14:53:56+00:00",
      "link": "https://arxiv.org/pdf/2602.12976v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12975v1",
      "title": "Extending confidence calibration to generalised measures of variation",
      "abstract": "We propose the Variation Calibration Error (VCE) metric for assessing the calibration of machine learning classifiers. The metric can be viewed as an extension of the well-known Expected Calibration Error (ECE) which assesses the calibration of the maximum probability or confidence. Other ways of measuring the variation of a probability distribution exist which have the advantage of taking into account the full probability distribution, for example the Shannon entropy. We show how the ECE approach can be extended from assessing confidence calibration to assessing the calibration of any metric of variation. We present numerical examples upon synthetic predictions which are perfectly calibrated by design, demonstrating that, in this scenario, the VCE has the desired property of approaching zero as the number of data samples increases, in contrast to another entropy-based calibration metric (the UCE) which has been proposed in the literature.",
      "authors": [
        "Andrew Thompson",
        "Vivek Desai"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 14:49:31+00:00",
      "link": "https://arxiv.org/pdf/2602.12975v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12972v1",
      "title": "Jointly Optimizing Debiased CTR and Uplift for Coupons Marketing: A Unified Causal Framework",
      "abstract": "In online advertising, marketing interventions such as coupons introduce significant confounding bias into Click-Through Rate (CTR) prediction. Observed clicks reflect a mixture of users' intrinsic preferences and the uplift induced by these interventions. This causes conventional models to miscalibrate base CTRs, which distorts downstream ranking and billing decisions. Furthermore, marketing interventions often operate as multi-valued treatments with varying magnitudes, introducing additional complexity to CTR prediction.   To address these issues, we propose the \\textbf{Uni}fied \\textbf{M}ulti-\\textbf{V}alued \\textbf{T}reatment Network (UniMVT). Specifically, UniMVT disentangles confounding factors from treatment-sensitive representations, enabling a full-space counterfactual inference module to jointly reconstruct the debiased base CTR and intensity-response curves. To handle the complexity of multi-valued treatments, UniMVT employs an auxiliary intensity estimation task to capture treatment propensities and devise a unit uplift objective that normalizes the intervention effect. This ensures comparable estimation across the continuous coupon-value spectrum. UniMVT simultaneously achieves debiased CTR prediction for accurate system calibration and precise uplift estimation for incentive allocation. Extensive experiments on synthetic and industrial datasets demonstrate UniMVT's superiority in both predictive accuracy and calibration. Furthermore, real-world A/B tests confirm that UniMVT significantly improves business metrics through more effective coupon distribution.",
      "authors": [
        "Siyun Yang",
        "Shixiao Yang",
        "Jian Wang",
        "Di Fan",
        "Kehe Cai",
        "Haoyan Fu",
        "Jiaming Zhang",
        "Wenjin Wu",
        "Peng Jiang"
      ],
      "primary_category": "cs.SI",
      "categories": [
        "cs.SI",
        "cs.LG"
      ],
      "published": "2026-02-13 14:46:20+00:00",
      "link": "https://arxiv.org/pdf/2602.12972v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12968v1",
      "title": "RGAlign-Rec: Ranking-Guided Alignment for Latent Query Reasoning in Recommendation Systems",
      "abstract": "Proactive intent prediction is a critical capability in modern e-commerce chatbots, enabling \"zero-query\" recommendations by anticipating user needs from behavioral and contextual signals. However, existing industrial systems face two fundamental challenges: (1) the semantic gap between discrete user features and the semantic intents within the chatbot's Knowledge Base, and (2) the objective misalignment between general-purpose LLM outputs and task-specific ranking utilities. To address these issues, we propose RGAlign-Rec, a closed-loop alignment framework that integrates an LLM-based semantic reasoner with a Query-Enhanced (QE) ranking model. We also introduce Ranking-Guided Alignment (RGA), a multi-stage training paradigm that utilizes downstream ranking signals as feedback to refine the LLM's latent reasoning. Extensive experiments on a large-scale industrial dataset from Shopee demonstrate that RGAlign-Rec achieves a 0.12% gain in GAUC, leading to a significant 3.52% relative reduction in error rate, and a 0.56% improvement in Recall@3. Online A/B testing further validates the cumulative effectiveness of our framework: the Query-Enhanced model (QE-Rec) initially yields a 0.98% improvement in CTR, while the subsequent Ranking-Guided Alignment stage contributes an additional 0.13% gain. These results indicate that ranking-aware alignment effectively synchronizes semantic reasoning with ranking objectives, significantly enhancing both prediction accuracy and service quality in real-world proactive recommendation systems.",
      "authors": [
        "Junhua Liu",
        "Yang Jihao",
        "Cheng Chang",
        "Kunrong LI",
        "Bin Fu",
        "Kwan Hui Lim"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-13 14:38:02+00:00",
      "link": "https://arxiv.org/pdf/2602.12968v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12966v1",
      "title": "ProbeLLM: Automating Principled Diagnosis of LLM Failures",
      "abstract": "Understanding how and why large language models (LLMs) fail is becoming a central challenge as models rapidly evolve and static evaluations fall behind. While automated probing has been enabled by dynamic test generation, existing approaches often discover isolated failure cases, lack principled control over exploration, and provide limited insight into the underlying structure of model weaknesses. We propose ProbeLLM, a benchmark-agnostic automated probing framework that elevates weakness discovery from individual failures to structured failure modes. ProbeLLM formulates probing as a hierarchical Monte Carlo Tree Search, explicitly allocating limited probing budgets between global exploration of new failure regions and local refinement of recurring error patterns. By restricting probing to verifiable test cases and leveraging tool-augmented generation and verification, ProbeLLM grounds failure discovery in reliable evidence. Discovered failures are further consolidated into interpretable failure modes via failure-aware embeddings and boundary-aware induction. Across diverse benchmarks and LLMs, ProbeLLM reveals substantially broader, cleaner, and more fine-grained failure landscapes than static benchmarks and prior automated methods, supporting a shift from case-centric evaluation toward principled weakness discovery.",
      "authors": [
        "Yue Huang",
        "Zhengzhe Jiang",
        "Yuchen Ma",
        "Yu Jiang",
        "Xiangqi Wang",
        "Yujun Zhou",
        "Yuexing Hao",
        "Kehan Guo",
        "Pin-Yu Chen",
        "Stefan Feuerriegel",
        "Xiangliang Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.SE"
      ],
      "published": "2026-02-13 14:33:13+00:00",
      "link": "https://arxiv.org/pdf/2602.12966v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12962v1",
      "title": "TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design",
      "abstract": "Recent studies have extensively explored NPU architectures for accelerating AI inference in on-device environments, which are inherently resource-constrained. Meanwhile, transformer-based large language models (LLMs) have become dominant, with rapidly increasing model sizes but low degree of parameter reuse compared to conventional CNNs, making end-to-end execution on resource-limited devices extremely challenging. To address these challenges, we propose TriGen, a novel NPU architecture tailored for resource-constrained environments through software-hardware co-design. Firstly, TriGen adopts low-precision computation using microscaling (MX) to enable additional optimization opportunities while preserving accuracy, and resolves the issues that arise by employing such precision. Secondly, to jointly optimize both nonlinear and linear operations, TriGen eliminates the need for specialized hardware for essential nonlinear operations by using fast and accurate LUT, thereby maximizing performance gains and reducing hardware-cost in on-device environments, and finally, by taking practical hardware constraints into account, further employs scheduling techniques to maximize computational utilization even under limited on-chip memory capacity. We evaluate the performance of TriGen on various LLMs and show that TriGen achieves an average 2.73x performance speedup and 52% less memory transfer over the baseline NPU design with negligible accuracy loss.",
      "authors": [
        "Jonghun Lee",
        "Junghoon Lee",
        "Hyeonjin Kim",
        "Seoho Jeon",
        "Jisup Yoon",
        "Hyunbin Park",
        "Meejeong Park",
        "Heonjae Ha"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "published": "2026-02-13 14:28:31+00:00",
      "link": "https://arxiv.org/pdf/2602.12962v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12961v1",
      "title": "Ca-MCF: Category-level Multi-label Causal Feature selection",
      "abstract": "Multi-label causal feature selection has attracted extensive attention in recent years. However, current methods primarily operate at the label level, treating each label variable as a monolithic entity and overlooking the fine-grained causal mechanisms unique to individual categories. To address this, we propose a Category-level Multi-label Causal Feature selection method named Ca-MCF. Ca-MCF utilizes label category flattening to decompose label variables into specific category nodes, enabling precise modeling of causal structures within the label space. Furthermore, we introduce an explanatory competition-based category-aware recovery mechanism that leverages the proposed Specific Category-Specific Mutual Information (SCSMI) and Distinct Category-Specific Mutual Information (DCSMI) to salvage causal features obscured by label correlations. The method also incorporates structural symmetry checks and cross-dimensional redundancy removal to ensure the robustness and compactness of the identified Markov Blankets. Extensive experiments across seven real-world datasets demonstrate that Ca-MCF significantly outperforms state-of-the-art benchmarks, achieving superior predictive accuracy with reduced feature dimensionality.",
      "authors": [
        "Wanfu Gao",
        "Yanan Wang",
        "Yonghao Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 14:26:47+00:00",
      "link": "https://arxiv.org/pdf/2602.12961v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12957v1",
      "title": "Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding",
      "abstract": "Document parsing is a fundamental task in multimodal understanding, supporting a wide range of downstream applications such as information extraction and intelligent document analysis. Benefiting from strong semantic modeling and robust generalization, VLM-based end-to-end approaches have emerged as the mainstream paradigm in recent years. However, these models often suffer from substantial inference latency, as they must auto-regressively generate long token sequences when processing long-form documents. In this work, motivated by the extremely long outputs and complex layout structures commonly found in document parsing, we propose a training-free and highly efficient acceleration method. Inspired by speculative decoding, we employ a lightweight document parsing pipeline as a draft model to predict batches of future tokens, while the more accurate VLM verifies these draft predictions in parallel. Moreover, we further exploit the layout-structured nature of documents by partitioning each page into independent regions, enabling parallel decoding of each region using the same draft-verify strategy. The final predictions are then assembled according to the natural reading order. Experimental results demonstrate the effectiveness of our approach: on the general-purpose OmniDocBench, our method provides a 2.42x lossless acceleration for the dots.ocr model, and achieves up to 4.89x acceleration on long-document parsing tasks. We will release our code to facilitate reproducibility and future research.",
      "authors": [
        "Wenhui Liao",
        "Hongliang Li",
        "Pengyu Xie",
        "Xinyu Cai",
        "Yufan Shen",
        "Yi Xin",
        "Qi Qin",
        "Shenglong Ye",
        "Tianbin Li",
        "Ming Hu",
        "Junjun He",
        "Yihao Liu",
        "Wenhai Wang",
        "Min Dou",
        "Bin Fu",
        "Botian Shi",
        "Yu Qiao",
        "Lianwen Jin"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-13 14:22:10+00:00",
      "link": "https://arxiv.org/pdf/2602.12957v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12952v1",
      "title": "Transporting Task Vectors across Different Architectures without Training",
      "abstract": "Adapting large pre-trained models to downstream tasks often produces task-specific parameter updates that are expensive to relearn for every model variant. While recent work has shown that such updates can be transferred between models with identical architectures, transferring them across models of different widths remains largely unexplored. In this work, we introduce Theseus, a training-free method for transporting task-specific updates across heterogeneous models. Rather than matching parameters directly, we characterize a task update by the functional effect it induces on intermediate representations. We formalize task-vector transport as a functional matching problem on observed activations and show that, after aligning representation spaces via orthogonal Procrustes analysis, it admits a stable closed-form solution that preserves the geometry of the update. We evaluate Theseus on vision and language models across different widths, showing consistent improvements over strong baselines without additional training or backpropagation. Our results show that task updates can be meaningfully transferred across architectures when task identity is defined functionally rather than parametrically.",
      "authors": [
        "Filippo Rinaldi",
        "Aniello Panariello",
        "Giacomo Salici",
        "Angelo Porrello",
        "Simone Calderara"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-13 14:16:34+00:00",
      "link": "https://arxiv.org/pdf/2602.12952v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12941v1",
      "title": "JARVIS: An Evidence-Grounded Retrieval System for Interpretable Deceptive Reviews Adjudication",
      "abstract": "Deceptive reviews, refer to fabricated feedback designed to artificially manipulate the perceived quality of products. Within modern e-commerce ecosystems, these reviews remain a critical governance challenge. Despite advances in review-level and graph-based detection methods, two pivotal limitations remain: inadequate generalization and lack of interpretability. To address these challenges, we propose JARVIS, a framework providing Judgment via Augmented Retrieval and eVIdence graph Structures. Starting from the review to be evaluated, it retrieves semantically similar evidence via hybrid dense-sparse multimodal retrieval, expands relational signals through shared entities, and constructs a heterogeneous evidence graph. Large language model then performs evidence-grounded adjudication to produce interpretable risk assessments. Offline experiments demonstrate that JARVIS enhances performance on our constructed review dataset, achieving a precision increase from 0.953 to 0.988 and a recall boost from 0.830 to 0.901. In the production environment, our framework achieves a 27% increase in the recall volume and reduces manual inspection time by 75%. Furthermore, the adoption rate of the model-generated analysis reaches 96.4%.",
      "authors": [
        "Nan Lu",
        "Leyang Li",
        "Yurong Hu",
        "Rui Lin",
        "Shaoyi Xu"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-13 13:57:45+00:00",
      "link": "https://arxiv.org/pdf/2602.12941v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12936v1",
      "title": "Unleashing MLLMs on the Edge: A Unified Framework for Cross-Modal ReID via Adaptive SVD Distillation",
      "abstract": "Practical cloud-edge deployment of Cross-Modal Re-identification (CM-ReID) faces challenges due to maintaining a fragmented ecosystem of specialized cloud models for diverse modalities. While Multi-Modal Large Language Models (MLLMs) offer strong unification potential, existing approaches fail to adapt them into a single end-to-end backbone and lack effective knowledge distillation strategies for edge deployment. To address these limitations, we propose MLLMEmbed-ReID, a unified framework based on a powerful cloud-edge architecture. First, we adapt a foundational MLLM into a state-of-the-art cloud model. We leverage instruction-based prompting to guide the MLLM in generating a unified embedding space across RGB, infrared, sketch, and text modalities. This model is then trained efficiently with a hierarchical Low-Rank Adaptation finetuning (LoRA-SFT) strategy, optimized under a holistic cross-modal alignment objective. Second, to deploy its knowledge onto an edge-native student, we introduce a novel distillation strategy motivated by the low-rank property in the teacher's feature space. To prioritize essential information, this method employs a Principal Component Mapping loss, while relational structures are preserved via a Feature Relation loss. Our lightweight edge-based model achieves state-of-the-art performance on multiple visual CM-ReID benchmarks, while its cloud-based counterpart excels across all CM-ReID benchmarks. The MLLMEmbed-ReID framework thus presents a complete and effective solution for deploying unified MLLM-level intelligence on resource-constrained devices. The code and models will be open-sourced soon.",
      "authors": [
        "Hongbo Jiang",
        "Jie Li",
        "Xinqi Cai",
        "Tianyu Xie",
        "Yunhang Shen",
        "Pingyang Dai",
        "Liujuan Cao"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-13 13:48:08+00:00",
      "link": "https://arxiv.org/pdf/2602.12936v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12921v1",
      "title": "When Words Don't Mean What They Say: Figurative Understanding in Bengali Idioms",
      "abstract": "Figurative language understanding remains a significant challenge for Large Language Models (LLMs), especially for low-resource languages. To address this, we introduce a new idiom dataset, a large-scale, culturally-grounded corpus of 10,361 Bengali idioms. Each idiom is annotated under a comprehensive 19-field schema, established and refined through a deliberative expert consensus process, that captures its semantic, syntactic, cultural, and religious dimensions, providing a rich, structured resource for computational linguistics. To establish a robust benchmark for Bangla figurative language understanding, we evaluate 30 state-of-the-art multilingual and instruction-tuned LLMs on the task of inferring figurative meaning. Our results reveal a critical performance gap, with no model surpassing 50% accuracy, a stark contrast to significantly higher human performance (83.4%). This underscores the limitations of existing models in cross-linguistic and cultural reasoning. By releasing the new idiom dataset and benchmark, we provide foundational infrastructure for advancing figurative language understanding and cultural grounding in LLMs for Bengali and other low-resource languages.",
      "authors": [
        "Adib Sakhawat",
        "Shamim Ara Parveen",
        "Md Ruhul Amin",
        "Shamim Al Mahmud",
        "Md Saiful Islam",
        "Tahera Khatun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 13:26:11+00:00",
      "link": "https://arxiv.org/pdf/2602.12921v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12911v1",
      "title": "ViMedCSS: A Vietnamese Medical Code-Switching Speech Dataset & Benchmark",
      "abstract": "Code-switching (CS), which is when Vietnamese speech uses English words like drug names or procedures, is a common phenomenon in Vietnamese medical communication. This creates challenges for Automatic Speech Recognition (ASR) systems, especially in low-resource languages like Vietnamese. Current most ASR systems struggle to recognize correctly English medical terms within Vietnamese sentences, and no benchmark addresses this challenge. In this paper, we construct a 34-hour \\textbf{Vi}etnamese \\textbf{Med}ical \\textbf{C}ode-\\textbf{S}witching \\textbf{S}peech dataset (ViMedCSS) containing 16,576 utterances. Each utterance includes at least one English medical term drawn from a curated bilingual lexicon covering five medical topics. Using this dataset, we evaluate several state-of-the-art ASR models and examine different specific fine-tuning strategies for improving medical term recognition to investigate the best approach to solve in the dataset. Experimental results show that Vietnamese-optimized models perform better on general segments, while multilingual pretraining helps capture English insertions. The combination of both approaches yields the best balance between overall and code-switched accuracy. This work provides the first benchmark for Vietnamese medical code-switching and offers insights into effective domain adaptation for low-resource, multilingual ASR systems.",
      "authors": [
        "Tung X. Nguyen",
        "Nhu Vo",
        "Giang-Son Nguyen",
        "Duy Mai Hoang",
        "Chien Dinh Huynh",
        "Inigo Jauregi Unanue",
        "Massimo Piccardi",
        "Wray Buntine",
        "Dung D. Le"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 13:17:16+00:00",
      "link": "https://arxiv.org/pdf/2602.12911v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13367v1",
      "title": "Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts",
      "abstract": "We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.",
      "authors": [
        "Chen Yang",
        "Guangyue Peng",
        "Jiaying Zhu",
        "Ran Le",
        "Ruixiang Feng",
        "Tao Zhang",
        "Xiyun Xu",
        "Yang Song",
        "Yiming Jia",
        "Yuntao Wen",
        "Yunzhi Xu",
        "Zekai Wang",
        "Zhenwei An",
        "Zhicong Sun",
        "Zongchao Chen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-13 13:10:46+00:00",
      "link": "https://arxiv.org/pdf/2602.13367v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12889v1",
      "title": "BaziQA-Benchmark: Evaluating Symbolic and Temporally Compositional Reasoning in Large Language Models",
      "abstract": "We present BaziQA-Benchmark, a standardized benchmark for evaluating symbolic and temporally compositional reasoning in large language models. The benchmark is derived from 200 professionally curated, multiple-choice problems from the Global Fortune-teller Competition (2021--2025), where each instance requires structured inference over a fixed symbolic chart and interacting temporal conditions. Unlike anecdotal or prompt-driven evaluations, BaziQA-Benchmark enables objective scoring and controlled comparison across years, domains, and model families. We evaluate contemporary language models under a multi-turn setting and analyze performance variation across temporal difficulty, reasoning domains, and inference protocols.To further probe reasoning behavior, we introduce a lightweight Structured Reasoning Protocol that constrains inference order without adding domain knowledge. Results show that models consistently outperform chance but remain far from saturation, exhibiting pronounced sensitivity to temporal composition and reasoning order, as well as systematic failures on precise temporal localization and multi-condition symbolic judgments.",
      "authors": [
        "Jiangxi Chen",
        "Qian Liu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 12:45:42+00:00",
      "link": "https://arxiv.org/pdf/2602.12889v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12878v1",
      "title": "Understanding Cultural Alignment in Multilingual LLMs via Natural Debate Statements",
      "abstract": "In this work we investigate the sociocultural values learned by large language models (LLMs). We introduce a novel open-access dataset, Sociocultural Statements, constructed from natural debate statements using a multi-step methodology. The dataset is synthetically labeled to enable the quantization of sociocultural norms and beliefs that LLMs exhibit in their responses to these statements, according to the Hofstede cultural dimensions. We verify the accuracy of synthetic labels using human quality control on a representative sample. We conduct a comparative analysis between two groups of LLMs developed in different countries (U.S. and China), and use as a comparative baseline patterns observed in human measurements. Using this new dataset and the analysis above, we found that culturally-distinct LLMs reflect the values and norms of the countries in which they were developed, highlighting their inability to adapt to the sociocultural backgrounds of their users.",
      "authors": [
        "Vlad-Andrei Negru",
        "Camelia Lemnaru",
        "Mihai Surdeanu",
        "Rodica Potolea"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY"
      ],
      "published": "2026-02-13 12:27:54+00:00",
      "link": "https://arxiv.org/pdf/2602.12878v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12876v1",
      "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
      "abstract": "Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-$V^3$, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.",
      "authors": [
        "Huanyao Zhang",
        "Jiepeng Zhou",
        "Bo Li",
        "Bowen Zhou",
        "Yanzhe Dan",
        "Haishan Lu",
        "Zhiyong Cao",
        "Jiaoyang Chen",
        "Yuqian Han",
        "Zinan Sheng",
        "Zhengwei Tao",
        "Hao Liang",
        "Jialong Wu",
        "Yang Shi",
        "Yuanpeng He",
        "Jiaye Lin",
        "Qintong Zhang",
        "Guochen Yan",
        "Runhao Zhao",
        "Zhengpin Li",
        "Xiaohan Yu",
        "Lang Mei",
        "Chong Chen",
        "Wentao Zhang",
        "Bin Cui"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-13 12:25:13+00:00",
      "link": "https://arxiv.org/pdf/2602.12876v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12871v1",
      "title": "MentalBench: A Benchmark for Evaluating Psychiatric Diagnostic Capability of Large Language Models",
      "abstract": "We introduce MentalBench, a benchmark for evaluating psychiatric diagnostic decision-making in large language models (LLMs). Existing mental health benchmarks largely rely on social media data, limiting their ability to assess DSM-grounded diagnostic judgments. At the core of MentalBench is MentalKG, a psychiatrist-built and validated knowledge graph encoding DSM-5 diagnostic criteria and differential diagnostic rules for 23 psychiatric disorders. Using MentalKG as a golden-standard logical backbone, we generate 24,750 synthetic clinical cases that systematically vary in information completeness and diagnostic complexity, enabling low-noise and interpretable evaluation. Our experiments show that while state-of-the-art LLMs perform well on structured queries probing DSM-5 knowledge, they struggle to calibrate confidence in diagnostic decision-making when distinguishing between clinically overlapping disorders. These findings reveal evaluation gaps not captured by existing benchmarks.",
      "authors": [
        "Hoyun Song",
        "Migyeong Kang",
        "Jisu Shin",
        "Jihyun Kim",
        "Chanbi Park",
        "Hangyeol Yoo",
        "Jihyun An",
        "Alice Oh",
        "Jinyoung Han",
        "KyungTae Lim"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 12:21:33+00:00",
      "link": "https://arxiv.org/pdf/2602.12871v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12869v1",
      "title": "X-VORTEX: Spatio-Temporal Contrastive Learning for Wake Vortex Trajectory Forecasting",
      "abstract": "Wake vortices are strong, coherent air turbulences created by aircraft, and they pose a major safety and capacity challenge for air traffic management. Tracking how vortices move, weaken, and dissipate over time from LiDAR measurements is still difficult because scans are sparse, vortex signatures fade as the flow breaks down under atmospheric turbulence and instabilities, and point-wise annotation is prohibitively expensive. Existing approaches largely treat each scan as an independent, fully supervised segmentation problem, which overlooks temporal structure and does not scale to the vast unlabeled archives collected in practice. We present X-VORTEX, a spatio-temporal contrastive learning framework grounded in Augmentation Overlap Theory that learns physics-aware representations from unlabeled LiDAR point cloud sequences. X-VORTEX addresses two core challenges: sensor sparsity and time-varying vortex dynamics. It constructs paired inputs from the same underlying flight event by combining a weakly perturbed sequence with a strongly augmented counterpart produced via temporal subsampling and spatial masking, encouraging the model to align representations across missing frames and partial observations. Architecturally, a time-distributed geometric encoder extracts per-scan features and a sequential aggregator models the evolving vortex state across variable-length sequences. We evaluate on a real-world dataset of over one million LiDAR scans. X-VORTEX achieves superior vortex center localization while using only 1% of the labeled data required by supervised baselines, and the learned representations support accurate trajectory forecasting.",
      "authors": [
        "Zhan Qu",
        "Michael Färber"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-13 12:20:44+00:00",
      "link": "https://arxiv.org/pdf/2602.12869v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12851v1",
      "title": "Chimera: Neuro-Symbolic Attention Primitives for Trustworthy Dataplane Intelligence",
      "abstract": "Deploying expressive learning models directly on programmable dataplanes promises line-rate, low-latency traffic analysis but remains hindered by strict hardware constraints and the need for predictable, auditable behavior. Chimera introduces a principled framework that maps attention-oriented neural computations and symbolic constraints onto dataplane primitives, enabling trustworthy inference within the match-action pipeline. Chimera combines a kernelized, linearized attention approximation with a two-layer key-selection hierarchy and a cascade fusion mechanism that enforces hard symbolic guarantees while preserving neural expressivity. The design includes a hardware-aware mapping protocol and a two-timescale update scheme that together permit stable, line-rate operation under realistic dataplane budgets. The paper presents the Chimera architecture, a hardware mapping strategy, and empirical evidence showing that neuro-symbolic attention primitives can achieve high-fidelity inference within the resource envelope of commodity programmable switches.",
      "authors": [
        "Rong Fu",
        "Wenxin Zhang",
        "Xiaowen Ma",
        "Kun Liu",
        "Wangyu Wu",
        "Ziyu Kong",
        "Jia Yee Tan",
        "Tailong Luo",
        "Xianda Li",
        "Zeli Su",
        "Youjin Wang",
        "Yongtai Liu",
        "Simon Fong"
      ],
      "primary_category": "cs.NI",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "published": "2026-02-13 11:55:06+00:00",
      "link": "https://arxiv.org/pdf/2602.12851v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12846v1",
      "title": "Amortized Reasoning Tree Search: Decoupling Proposal and Decision in Large Language Models",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has established itself as the dominant paradigm for instilling rigorous reasoning capabilities in Large Language Models. While effective at amplifying dominant behaviors, we identify a critical pathology in this alignment process: the systematic suppression of valid but rare (low-likelihood under the base model distribution) reasoning paths. We theoretically characterize this phenomenon as a \"Normalization Squeeze,\" where the interplay between mode-seeking policy gradients and finite sampling acts as a high-pass likelihood filter, driving the probability of rare correct traces to statistical extinction. To counteract this collapse without discarding the base model's latent diversity, we propose Amortized Reasoning Tree Search (ARTS). Unlike standard approaches that force internalization via parameter updates, ARTS prioritizes deliberation by decoupling generation from verification. We introduce a Flow Matching objective that repurposes the verifier to estimate the conservation of probability flow, enabling robust navigation through sparse, high-entropy search spaces where traditional discriminative objectives fail. Extensive experiments on the MATH-500 benchmark demonstrate that ARTS achieves a performance of 74.6% (BoN@16), effectively matching fully fine-tuned policies (74.7%) without modifying the generative backbone. Crucially, on the long-tail subset where coupled RL optimization collapses to 0% pass@k, ARTS uniquely recovers significant performance, suggesting that disentangling verification from generation offers a more robust pathway for solving complex reasoning tasks.",
      "authors": [
        "Zesheng Hong",
        "Jiadong Yu",
        "Hui Pan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 11:52:50+00:00",
      "link": "https://arxiv.org/pdf/2602.12846v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12843v1",
      "title": "Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation",
      "abstract": "Radiological diagnosis is a perceptual process in which careful visual inspection and language reasoning are repeatedly interleaved. Most medical large vision language models (LVLMs) perform visual inspection only once and then rely on text-only chain-of-thought (CoT) reasoning, which operates purely in the linguistic space and is prone to hallucination. Recent methods attempt to mitigate this issue by introducing visually related coordinates, such as bounding boxes. However, these remain a pseudo-visual solution: coordinates are still text and fail to preserve rich visual details like texture and density. Motivated by the interleaved nature of radiological diagnosis, we introduce MMRad-IVL-22K, the first large-scale dataset designed for natively interleaved visual language reasoning in chest X-ray interpretation. MMRad-IVL-22K reflects a repeated cycle of reasoning and visual inspection workflow of radiologists, in which visual rationales complement textual descriptions and ground each step of the reasoning process. MMRad-IVL-22K comprises 21,994 diagnostic traces, enabling systematic scanning across 35 anatomical regions. Experimental results on advanced closed-source LVLMs demonstrate that report generation guided by multimodal CoT significantly outperforms that guided by text-only CoT in clinical accuracy and report quality (e.g., 6\\% increase in the RadGraph metric), confirming that high-fidelity interleaved vision language evidence is a non-substitutable component of reliable medical AI. Furthermore, benchmarking across seven state-of-the-art open-source LVLMs demonstrates that models fine-tuned on MMRad-IVL-22K achieve superior reasoning consistency and report quality compared with both general-purpose and medical-specific LVLMs. The project page is available at https://github.com/qiuzyc/thinking_like_a_radiologist.",
      "authors": [
        "Yichen Zhao",
        "Zelin Peng",
        "Piao Yang",
        "Xiaokang Yang",
        "Wei Shen"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-13 11:49:32+00:00",
      "link": "https://arxiv.org/pdf/2602.12843v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13362v1",
      "title": "Nonparametric Distribution Regression Re-calibration",
      "abstract": "A key challenge in probabilistic regression is ensuring that predictive distributions accurately reflect true empirical uncertainty. Minimizing overall prediction error often encourages models to prioritize informativeness over calibration, producing narrow but overconfident predictions. However, in safety-critical settings, trustworthy uncertainty estimates are often more valuable than narrow intervals. Realizing the problem, several recent works have focused on post-hoc corrections; however, existing methods either rely on weak notions of calibration (such as PIT uniformity) or impose restrictive parametric assumptions on the nature of the error. To address these limitations, we propose a novel nonparametric re-calibration algorithm based on conditional kernel mean embeddings, capable of correcting calibration error without restrictive modeling assumptions. For efficient inference with real-valued targets, we introduce a novel characteristic kernel over distributions that can be evaluated in $\\mathcal{O}(n \\log n)$ time for empirical distributions of size $n$. We demonstrate that our method consistently outperforms prior re-calibration approaches across a diverse set of regression benchmarks and model classes.",
      "authors": [
        "Ádám Jung",
        "Domokos M. Kelen",
        "András A. Benczúr"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-13 11:48:43+00:00",
      "link": "https://arxiv.org/pdf/2602.13362v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12833v1",
      "title": "TRACE: Temporal Reasoning via Agentic Context Evolution for Streaming Electronic Health Records (EHRs)",
      "abstract": "Large Language Models (LLMs) encode extensive medical knowledge but struggle to apply it reliably to longitudinal patient trajectories, where evolving clinical states, irregular timing, and heterogeneous events degrade performance over time. Existing adaptation strategies rely on fine-tuning or retrieval-based augmentation, which introduce computational overhead, privacy constraints, or instability under long contexts. We introduce TRACE (Temporal Reasoning via Agentic Context Evolution), a framework that enables temporal clinical reasoning with frozen LLMs by explicitly structuring and maintaining context rather than extending context windows or updating parameters. TRACE operates over a dual-memory architecture consisting of a static Global Protocol encoding institutional clinical rules and a dynamic Individual Protocol tracking patient-specific state. Four agentic components, Router, Reasoner, Auditor, and Steward, coordinate over this structured memory to support temporal inference and state evolution. The framework maintains bounded inference cost via structured state compression and selectively audits safety-critical clinical decisions. Evaluated on longitudinal clinical event streams from MIMIC-IV, TRACE significantly improves next-event prediction accuracy, protocol adherence, and clinical safety over long-context and retrieval-augmented baselines, while producing interpretable and auditable reasoning traces.",
      "authors": [
        "Zhan Qu",
        "Michael Färber"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2026-02-13 11:39:19+00:00",
      "link": "https://arxiv.org/pdf/2602.12833v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12828v1",
      "title": "GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories",
      "abstract": "Predicting future clinical events from longitudinal electronic health records (EHRs) is challenging due to sparse multi-type clinical events, hierarchical medical vocabularies, and the tendency of large language models (LLMs) to hallucinate when reasoning over long structured histories. We study next-visit event prediction, which aims to forecast a patient's upcoming clinical events based on prior visits. We propose GRAIL, a framework that models longitudinal EHRs using structured geometric representations and structure-aware retrieval. GRAIL constructs a unified clinical graph by combining deterministic coding-system hierarchies with data-driven temporal associations across event types, embeds this graph in hyperbolic space, and summarizes each visit as a probabilistic Central Event that denoises sparse observations. At inference time, GRAIL retrieves a structured set of clinically plausible future events aligned with hierarchical and temporal progression, and optionally refines their ranking using an LLM as a constrained inference-time reranker. Experiments on MIMIC-IV show that GRAIL consistently improves multi-type next-visit prediction and yields more hierarchy-consistent forecasts.",
      "authors": [
        "Zhan Qu",
        "Michael Färber"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 11:30:37+00:00",
      "link": "https://arxiv.org/pdf/2602.12828v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12819v1",
      "title": "WISE: A Multimodal Search Engine for Visual Scenes, Audio, Objects, Faces, Speech, and Metadata",
      "abstract": "In this paper, we present WISE, an open-source audiovisual search engine which integrates a range of multimodal retrieval capabilities into a single, practical tool accessible to users without machine learning expertise. WISE supports natural-language and reverse-image queries at both the scene level (e.g. empty street) and object level (e.g. horse) across images and videos; face-based search for specific individuals; audio retrieval of acoustic events using text (e.g. wood creak) or an audio file; search over automatically transcribed speech; and filtering by user-provided metadata. Rich insights can be obtained by combining queries across modalities -- for example, retrieving German trains from a historical archive by applying the object query \"train\" and the metadata query \"Germany\", or searching for a face in a place. By employing vector search techniques, WISE can scale to support efficient retrieval over millions of images or thousands of hours of video. Its modular architecture facilitates the integration of new models. WISE can be deployed locally for private or sensitive collections, and has been applied to various real-world use cases. Our code is open-source and available at https://gitlab.com/vgg/wise/wise.",
      "authors": [
        "Prasanna Sridhar",
        "Horace Lee",
        "David M. S. Pinto",
        "Andrew Zisserman",
        "Abhishek Dutta"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.CV"
      ],
      "published": "2026-02-13 11:03:06+00:00",
      "link": "https://arxiv.org/pdf/2602.12819v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12806v1",
      "title": "RAT-Bench: A Comprehensive Benchmark for Text Anonymization",
      "abstract": "Data containing personal information is increasingly used to train, fine-tune, or query Large Language Models (LLMs). Text is typically scrubbed of identifying information prior to use, often with tools such as Microsoft's Presidio or Anthropic's PII purifier. These tools have traditionally been evaluated on their ability to remove specific identifiers (e.g., names), yet their effectiveness at preventing re-identification remains unclear. We introduce RAT-Bench, a comprehensive benchmark for text anonymization tools based on re-identification risk. Using U.S. demographic statistics, we generate synthetic text containing various direct and indirect identifiers across domains, languages, and difficulty levels. We evaluate a range of NER- and LLM-based text anonymization tools and, based on the attributes an LLM-based attacker is able to correctly infer from the anonymized text, we report the risk of re-identification in the U.S. population, while properly accounting for the disparate impact of identifiers. We find that, while capabilities vary widely, even the best tools are far from perfect in particular when direct identifiers are not written in standard ways and when indirect identifiers enable re-identification. Overall we find LLM-based anonymizers, including new iterative anonymizers, to provide a better privacy-utility trade-off albeit at a higher computational cost. Importantly, we also find them to work well across languages. We conclude with recommendations for future anonymization tools and will release the benchmark and encourage community efforts to expand it, in particular to other geographies.",
      "authors": [
        "Nataša Krčo",
        "Zexi Yao",
        "Matthieu Meeus",
        "Yves-Alexandre de Montjoye"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "published": "2026-02-13 10:41:44+00:00",
      "link": "https://arxiv.org/pdf/2602.12806v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13359v1",
      "title": "The Speed-up Factor: A Quantitative Multi-Iteration Active Learning Performance Metric",
      "abstract": "Machine learning models excel with abundant annotated data, but annotation is often costly and time-intensive. Active learning (AL) aims to improve the performance-to-annotation ratio by using query methods (QMs) to iteratively select the most informative samples. While AL research focuses mainly on QM development, the evaluation of this iterative process lacks appropriate performance metrics. This work reviews eight years of AL evaluation literature and formally introduces the speed-up factor, a quantitative multi-iteration QM performance metric that indicates the fraction of samples needed to match random sampling performance. Using four datasets from diverse domains and seven QMs of various types, we empirically evaluate the speed-up factor and compare it with state-of-the-art AL performance metrics. The results confirm the assumptions underlying the speed-up factor, demonstrate its accuracy in capturing the described fraction, and reveal its superior stability across iterations.",
      "authors": [
        "Hannes Kath",
        "Thiago S. Gouvêa",
        "Daniel Sonntag"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 10:33:02+00:00",
      "link": "https://arxiv.org/pdf/2602.13359v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12798v1",
      "title": "Can Neural Networks Provide Latent Embeddings for Telemetry-Aware Greedy Routing?",
      "abstract": "Telemetry-Aware routing promises to increase efficacy and responsiveness to traffic surges in computer networks. Recent research leverages Machine Learning to deal with the complex dependency between network state and routing, but sacrifices explainability of routing decisions due to the black-box nature of the proposed neural routing modules. We propose \\emph{Placer}, a novel algorithm using Message Passing Networks to transform network states into latent node embeddings. These embeddings facilitate quick greedy next-hop routing without directly solving the all-pairs shortest paths problem, and let us visualize how certain network events shape routing decisions.",
      "authors": [
        "Andreas Boltres",
        "Niklas Freymuth",
        "Gerhard Neumann"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "published": "2026-02-13 10:31:09+00:00",
      "link": "https://arxiv.org/pdf/2602.12798v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12783v1",
      "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise",
      "abstract": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.",
      "authors": [
        "Yuejie Li",
        "Ke Yang",
        "Yueying Hua",
        "Berlin Chen",
        "Jianhao Nie",
        "Yueping He",
        "Caixin Kang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-13 10:08:27+00:00",
      "link": "https://arxiv.org/pdf/2602.12783v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12779v1",
      "title": "iRULER: Intelligible Rubric-Based User-Defined LLM Evaluation for Revision",
      "abstract": "Large Language Models (LLMs) have become indispensable for evaluating writing. However, text feedback they provide is often unintelligible, generic, and not specific to user criteria. Inspired by structured rubrics in education and intelligible AI explanations, we propose iRULER following identified design guidelines to \\textit{scaffold} the review process by \\textit{specific} criteria, providing \\textit{justification} for score selection, and offering \\textit{actionable} revisions to target different quality levels. To \\textit{qualify} user-defined criteria, we recursively used iRULER with a rubric-of-rubrics to iteratively \\textit{refine} rubrics. In controlled experiments on writing revision and rubric creation, iRULER most improved validated LLM-judged review scores and was perceived as most helpful and aligned compared to read-only rubric and text-based LLM feedback. Qualitative findings further support how iRULER satisfies the design guidelines for user-defined feedback. This work contributes interactive rubric tools for intelligible LLM-based review and revision of writing, and user-defined rubric creation.",
      "authors": [
        "Jingwen Bai",
        "Wei Soon Cheong",
        "Philippe Muller",
        "Brian Y Lim"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-13 10:02:25+00:00",
      "link": "https://arxiv.org/pdf/2602.12779v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12778v1",
      "title": "Aspect-Based Sentiment Analysis for Future Tourism Experiences: A BERT-MoE Framework for Persian User Reviews",
      "abstract": "This study advances aspect-based sentiment analysis (ABSA) for Persian-language user reviews in the tourism domain, addressing challenges of low-resource languages. We propose a hybrid BERT-based model with Top-K routing and auxiliary losses to mitigate routing collapse and improve efficiency. The pipeline includes: (1) overall sentiment classification using BERT on 9,558 labeled reviews, (2) multi-label aspect extraction for six tourism-related aspects (host, price, location, amenities, cleanliness, connectivity), and (3) integrated ABSA with dynamic routing. The dataset consists of 58,473 preprocessed reviews from the Iranian accommodation platform Jabama, manually annotated for aspects and sentiments. The proposed model achieves a weighted F1-score of 90.6% for ABSA, outperforming baseline BERT (89.25%) and a standard hybrid approach (85.7%). Key efficiency gains include a 39% reduction in GPU power consumption compared to dense BERT, supporting sustainable AI deployment in alignment with UN SDGs 9 and 12. Analysis reveals high mention rates for cleanliness and amenities as critical aspects. This is the first ABSA study focused on Persian tourism reviews, and we release the annotated dataset to facilitate future multilingual NLP research in tourism.",
      "authors": [
        "Hamidreza Kazemi Taskooh",
        "Taha Zare Harofte"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-13 10:01:33+00:00",
      "link": "https://arxiv.org/pdf/2602.12778v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12759v1",
      "title": "Towards a Diagnostic and Predictive Evaluation Methodology for Sequence Labeling Tasks",
      "abstract": "Standard evaluation in NLP typically indicates that system A is better on average than system B, but it provides little info on how to improve performance and, what is worse, it should not come as a surprise if B ends up being better than A on outside data. We propose an evaluation methodology for sequence labeling tasks grounded on error analysis that provides both quantitative and qualitative information on where systems must be improved and predicts how models will perform on a different distribution. The key is to create test sets that, contrary to common practice, do not rely on gathering large amounts of real-world in-distribution scraped data, but consists in handcrafting a small set of linguistically motivated examples that exhaustively cover the range of span attributes (such as shape, length, casing, sentence position, etc.) a system may encounter in the wild. We demonstrate this methodology on a benchmark for anglicism identification in Spanish. Our methodology provides results that are diagnostic (because they help identify systematic weaknesses in performance), actionable (because they can inform which model is better suited for a given scenario) and predictive: our method predicts model performance on external datasets with a median correlation of 0.85.",
      "authors": [
        "Elena Alvarez-Mellado",
        "Julio Gonzalo"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 09:39:10+00:00",
      "link": "https://arxiv.org/pdf/2602.12759v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12756v1",
      "title": "Closing the Loop: A Control-Theoretic Framework for Provably Stable Time Series Forecasting with LLMs",
      "abstract": "Large Language Models (LLMs) have recently shown exceptional potential in time series forecasting, leveraging their inherent sequential reasoning capabilities to model complex temporal dynamics. However, existing approaches typically employ a naive autoregressive generation strategy. We identify a critical theoretical flaw in this paradigm: during inference, the model operates in an open-loop manner, consuming its own generated outputs recursively. This leads to inevitable error accumulation (exposure bias), where minor early deviations cascade into significant trajectory drift over long horizons. In this paper, we reformulate autoregressive forecasting through the lens of control theory, proposing \\textbf{F-LLM} (Feedback-driven LLM), a novel closed-loop framework. Unlike standard methods that passively propagate errors, F-LLM actively stabilizes the trajectory via a learnable residual estimator (Observer) and a feedback controller. Furthermore, we provide a theoretical guarantee that our closed-loop mechanism ensures uniformly bounded error, provided the base model satisfies a local Lipschitz constraint. Extensive experiments demonstrate that F-LLM significantly mitigates error propagation, achieving good performance on time series benchmarks.",
      "authors": [
        "Xingyu Zhang",
        "Hanyun Du",
        "Zeen Song",
        "Jianqi Zhang",
        "Changwen Zheng",
        "Wenwen Qiang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 09:35:12+00:00",
      "link": "https://arxiv.org/pdf/2602.12756v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12753v1",
      "title": "Hierarchical Successor Representation for Robust Transfer",
      "abstract": "The successor representation (SR) provides a powerful framework for decoupling predictive dynamics from rewards, enabling rapid generalisation across reward configurations. However, the classical SR is limited by its inherent policy dependence: policies change due to ongoing learning, environmental non-stationarities, and changes in task demands, making established predictive representations obsolete. Furthermore, in topologically complex environments, SRs suffer from spectral diffusion, leading to dense and overlapping features that scale poorly. Here we propose the Hierarchical Successor Representation (HSR) for overcoming these limitations. By incorporating temporal abstractions into the construction of predictive representations, HSR learns stable state features which are robust to task-induced policy changes. Applying non-negative matrix factorisation (NMF) to the HSR yields a sparse, low-rank state representation that facilitates highly sample-efficient transfer to novel tasks in multi-compartmental environments. Further analysis reveals that HSR-NMF discovers interpretable topological structures, providing a policy-agnostic hierarchical map that effectively bridges model-free optimality and model-based flexibility. Beyond providing a useful basis for task-transfer, we show that HSR's temporally extended predictive structure can also be leveraged to drive efficient exploration, effectively scaling to large, procedurally generated environments.",
      "authors": [
        "Changmin Yu",
        "Máté Lengyel"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 09:32:26+00:00",
      "link": "https://arxiv.org/pdf/2602.12753v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12747v1",
      "title": "From Guidelines to Practice: Evaluating the Reproducibility of Methods in Computational Social Science",
      "abstract": "Reproducibility remains a central challenge in computational social science, where complex workflows, evolving software ecosystems, and inconsistent documentation hinder researchers ability to re-execute published methods. This study presents a systematic evaluation of reproducibility across three conditions: uncurated documentation, curated documentation, and curated documentation paired with a preset execution environment. Using 47 usability test sessions, we combine behavioral performance indicators (success rates, task time, and error profiles) with questionnaire data and thematic analysis to identify technical and conceptual barriers to reproducibility.   Curated documentation substantially reduced repository-level errors and improved users ability to interpret method outputs. Standardizing the execution environment further improved reproducibility, yielding the highest success rate and shortest task completion times. Across conditions, participants frequently relied on AI tools for troubleshooting, often enabling independent resolution of issues without facilitator intervention.   Our findings demonstrate that reproducibility barriers are multi-layered and require coordinated improvements in documentation quality, environment stability, and conceptual clarity. We discuss implications for the design of reproducibility platforms and infrastructure in computational social science.",
      "authors": [
        "Fakhri Momeni",
        "Sarah Sajid",
        "Johannes Kiesel"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-13 09:23:29+00:00",
      "link": "https://arxiv.org/pdf/2602.12747v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12746v1",
      "title": "Lamer-SSL: Layer-aware Mixture of LoRA Experts for Continual Multilingual Expansion of Self-supervised Models without Forgetting",
      "abstract": "Despite their impressive performance, self-supervised speech models often struggle to generalize to new languages and tend to forget previously acquired knowledge during continual training. To address this, we propose Lamer-SSL, a parameter-efficient framework that integrates a Layer-Aware MixturE of LoRA Experts (Lamer) module with a replay strategy. The Lamer module enables flexible balancing between shared and language-specific representations, while layer-aware expert allocation assigns more experts to deeper layers where semantic information is richer. Meanwhile, the replay strategy retains prior knowledge using minimal data, mitigating forgetting during continual training. Experiments on automatic speech recognition (ASR) and language identification (LID) demonstrate that Lamer-SSL extends self-supervised models to new languages effectively while maintaining strong performance on previously learned languages with only 2.14% parameters being trainable.",
      "authors": [
        "Jing Xu",
        "Minglin Wu",
        "Xueyuan Chen",
        "Xixin Wu",
        "Helen Meng"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "published": "2026-02-13 09:22:22+00:00",
      "link": "https://arxiv.org/pdf/2602.12746v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12744v1",
      "title": "Adaptive Structured Pruning of Convolutional Neural Networks for Time Series Classification",
      "abstract": "Deep learning models for Time Series Classification (TSC) have achieved strong predictive performance but their high computational and memory requirements often limit deployment on resource-constrained devices. While structured pruning can address these issues by removing redundant filters, existing methods typically rely on manually tuned hyperparameters such as pruning ratios which limit scalability and generalization across datasets. In this work, we propose Dynamic Structured Pruning (DSP), a fully automatic, structured pruning framework for convolution-based TSC models. DSP introduces an instance-wise sparsity loss during training to induce channel-level sparsity, followed by a global activation analysis to identify and prune redundant filters without needing any predefined pruning ratio. This work tackles computational bottlenecks of deep TSC models for deployment on resource-constrained devices. We validate DSP on 128 UCR datasets using two different deep state-of-the-art architectures: LITETime and InceptionTime. Our approach achieves an average compression of 58% for LITETime and 75% for InceptionTime architectures while maintaining classification accuracy. Redundancy analyses confirm that DSP produces compact and informative representations, offering a practical path for scalable and efficient deep TSC deployment.",
      "authors": [
        "Javidan Abdullayev",
        "Maxime Devanne",
        "Cyril Meyer",
        "Ali Ismail-Fawaz",
        "Jonathan Weber",
        "Germain Forestier"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 09:18:59+00:00",
      "link": "https://arxiv.org/pdf/2602.12744v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12727v1",
      "title": "Training Dense Retrievers with Multiple Positive Passages",
      "abstract": "Modern knowledge-intensive systems, such as retrieval-augmented generation (RAG), rely on effective retrievers to establish the performance ceiling for downstream modules. However, retriever training has been bottlenecked by sparse, single-positive annotations, which lead to false-negative noise and suboptimal supervision. While the advent of large language models (LLMs) makes it feasible to collect comprehensive multi-positive relevance labels at scale, the optimal strategy for incorporating these dense signals into training remains poorly understood. In this paper, we present a systematic study of multi-positive optimization objectives for retriever training. We unify representative objectives, including Joint Likelihood (JointLH), Summed Marginal Likelihood (SumMargLH), and Log-Sum-Exp Pairwise (LSEPair) loss, under a shared contrastive learning framework. Our theoretical analysis characterizes their distinct gradient behaviors, revealing how each allocates probability mass across positive document sets. Empirically, we conduct extensive evaluations on Natural Questions, MS MARCO, and the BEIR benchmark across two realistic regimes: homogeneous LLM-annotated data and heterogeneous mixtures of human and LLM labels. Our results show that LSEPair consistently achieves superior robustness and performance across settings, while JointLH and SumMargLH exhibit high sensitivity to the quality of positives. Furthermore, we find that the simple strategy of random sampling (Rand1LH) serves as a reliable baseline. By aligning theoretical insights with empirical findings, we provide practical design principles for leveraging dense, LLM-augmented supervision to enhance retriever effectiveness.",
      "authors": [
        "Benben Wang",
        "Minghao Tang",
        "Hengran Zhang",
        "Jiafeng Guo",
        "Keping Bi"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-13 08:56:52+00:00",
      "link": "https://arxiv.org/pdf/2602.12727v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12723v2",
      "title": "Towards explainable reference-free speech intelligibility evaluation of people with pathological speech",
      "abstract": "Objective assessment of speech that reflects meaningful changes in communication is crucial for clinical decision making and reproducible research. While existing objective assessments, particularly reference-based approaches, can capture intelligibility changes, they are often hindered by lack of explainability and the need for labor-intensive manual transcriptions. To address these issues, this work proposes the reference-free, explainable ASR Inconsistency Score. We evaluate this method on pathological speech in Dutch, Spanish and English, and compare its performance to a reference-based Word Error Rate (WER) baseline. Our results demonstrate that the ASR Inconsistency Score achieves a high correlation with expert perceptual ratings, with performance closely matching, and in one case exceeding, a standard reference-based Word Error Rate (WER) baseline.",
      "authors": [
        "Bence Mark Halpern",
        "Thomas Tienkamp",
        "Defne Abur",
        "Tomoki Toda"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD"
      ],
      "published": "2026-02-13 08:53:40+00:00",
      "link": "https://arxiv.org/pdf/2602.12723v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12714v1",
      "title": "ADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools -- From Consensus Learning to Ambiguity-Driven Emotion Reasoning",
      "abstract": "Speech Large Language Models (SLLMs) enable high-level emotion reasoning but often produce ungrounded, text-biased judgments without verifiable acoustic evidence. In contrast, self-supervised speech encoders such as WavLM provide strong acoustic representations yet remain opaque discriminative models with limited interpretability. To bridge this gap, we introduce ADEPT (Agentic Decoding of Emotion via Evidence Probing Tools), a framework that reframes emotion recognition as a multi-turn inquiry process rather than a single-pass prediction. ADEPT transforms an SLLM into an agent that maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools within a structured pipeline of candidate generation, evidence collection, and adjudication. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning. Since human affect exhibits inherent complexity and frequent co-occurrence of emotions, we treat minority annotations as informative perceptual signals rather than discarding them as noise. Finally, we integrate Group Relative Policy Optimization (GRPO) with an Evidence Trust Gate to explicitly couple tool-usage behaviors with prediction quality and enforce evidence-grounded reasoning. Experiments show that ADEPT improves primary emotion accuracy in most settings while substantially improving minor emotion characterization, producing explanations grounded in auditable acoustic and semantic evidence.",
      "authors": [
        "Esther Sun",
        "Bo-Hao Su",
        "Abinay Reddy Naini",
        "Shinji Watanabe",
        "Carlos Busso"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 08:33:37+00:00",
      "link": "https://arxiv.org/pdf/2602.12714v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12709v1",
      "title": "ReFilter: Improving Robustness of Retrieval-Augmented Generation via Gated Filter",
      "abstract": "Retrieval-augmented generation (RAG) has become a dominant paradigm for grounding large language models (LLMs) with external evidence in knowledge-intensive question answering. A core design choice is how to fuse retrieved samples into the LLMs, where existing internal fusion approaches broadly fall into query-based fusion, parametric fusion, and latent-based fusion. Despite their effectiveness at modest retrieval scales, these methods often fail to scale gracefully as the number of retrieved candidates k increases: Larger k improves evidence coverage, yet realistic top-k retrieval inevitably contains irrelevant or redundant content and increases the inference cost.   To address these limitations, we propose ReFilter, a novel latent-based fusion framework that performs token-level filtering and fusion. ReFilter consists of three key components: a context encoder for encoding context features, a gated filter for weighting each token, and a token fusion module for integrating the weighted token feature into the LLM's hidden states. Our experiments across four general-domain QA benchmarks show that ReFilter consistently achieves the best average performance under both in-domain adaptation and out-of-domain transfer. ReFilter further generalizes to five biomedical QA benchmarks in zero-shot transfer without domain fine-tuning, reaching 70.01% average accuracy with Qwen2.5-14B-Instruct.",
      "authors": [
        "Yixin Chen",
        "Ying Xiong",
        "Shangyu Wu",
        "Xiangrui Ke",
        "Nan Guan",
        "Chun Jason Xue"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 08:25:26+00:00",
      "link": "https://arxiv.org/pdf/2602.12709v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12708v1",
      "title": "Mixture of Predefined Experts: Maximizing Data Usage on Vertical Federated Learning",
      "abstract": "Vertical Federated Learning (VFL) has emerged as a critical paradigm for collaborative model training in privacy-sensitive domains such as finance and healthcare. However, most existing VFL frameworks rely on the idealized assumption of full sample alignment across participants, a premise that rarely holds in real-world scenarios. To bridge this gap, this work introduces Split-MoPE, a novel framework that integrates Split Learning with a specialized Mixture of Predefined Experts (MoPE) architecture. Unlike standard Mixture of Experts (MoE), where routing is learned dynamically, MoPE uses predefined experts to process specific data alignments, effectively maximizing data usage during both training and inference without requiring full sample overlap. By leveraging pretrained encoders for target data domains, Split-MoPE achieves state-of-the-art performance in a single communication round, significantly reducing the communication footprint compared to multi-round end-to-end training. Furthermore, unlike existing proposals that address sample misalignment, this novel architecture provides inherent robustness against malicious or noisy participants and offers per-sample interpretability by quantifying each collaborator's contribution to each prediction. Extensive evaluations on vision (CIFAR-10/100) and tabular (Breast Cancer Wisconsin) datasets demonstrate that Split-MoPE consistently outperforms state-of-the-art systems such as LASER and Vertical SplitNN, particularly in challenging scenarios with high data missingness.",
      "authors": [
        "Jon Irureta",
        "Gorka Azkune",
        "Jon Imaz",
        "Aizea Lojo",
        "Javier Fernandez-Marques"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 08:21:20+00:00",
      "link": "https://arxiv.org/pdf/2602.12708v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12706v1",
      "title": "Physics-Informed Laplace Neural Operator for Solving Partial Differential Equations",
      "abstract": "Neural operators have emerged as fast surrogate solvers for parametric partial differential equations (PDEs). However, purely data-driven models often require extensive training data and can generalize poorly, especially in small-data regimes and under unseen (out-of-distribution) input functions that are not represented in the training data. To address these limitations, we propose the Physics-Informed Laplace Neural Operator (PILNO), which enhances the Laplace Neural Operator (LNO) by embedding governing physics into training through PDE, boundary condition, and initial condition residuals. To improve expressivity, we first introduce an Advanced LNO (ALNO) backbone that retains a pole-residue transient representation while replacing the steady-state branch with an FNO-style Fourier multiplier. To make physics-informed training both data-efficient and robust, PILNO further leverages (i) virtual inputs: an unlabeled ensemble of input functions spanning a broad spectral range that provides abundant physics-only supervision and explicitly targets out-of-distribution (OOD) regimes; and (ii) temporal-causality weighting: a time-decaying reweighting of the physics residual that prioritizes early-time dynamics and stabilizes optimization for time-dependent PDEs. Across four representative benchmarks -- Burgers' equation, Darcy flow, a reaction-diffusion system, and a forced KdV equation -- PILNO consistently improves accuracy in small-data settings (e.g., N_train <= 27), reduces run-to-run variability across random seeds, and achieves stronger OOD generalization than purely data-driven baselines.",
      "authors": [
        "Heechang Kim",
        "Qianying Cao",
        "Hyomin Shin",
        "Seungchul Lee",
        "George Em Karniadakis",
        "Minseok Choi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 08:19:40+00:00",
      "link": "https://arxiv.org/pdf/2602.12706v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12705v2",
      "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs",
      "abstract": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.",
      "authors": [
        "Baorong Shi",
        "Bo Cui",
        "Boyuan Jiang",
        "Deli Yu",
        "Fang Qian",
        "Haihua Yang",
        "Huichao Wang",
        "Jiale Chen",
        "Jianfei Pan",
        "Jieqiong Cao",
        "Jinghao Lin",
        "Kai Wu",
        "Lin Yang",
        "Shengsheng Yao",
        "Tao Chen",
        "Xiaojun Xiao",
        "Xiaozhong Ji",
        "Xu Wang",
        "Yijun He",
        "Zhixiong Yang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "published": "2026-02-13 08:19:38+00:00",
      "link": "https://arxiv.org/pdf/2602.12705v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12704v1",
      "title": "QTabGAN: A Hybrid Quantum-Classical GAN for Tabular Data Synthesis",
      "abstract": "Synthesizing realistic tabular data is challenging due to heterogeneous feature types and high dimensionality. We introduce QTabGAN, a hybrid quantum-classical generative adversarial framework for tabular data synthesis. QTabGAN is especially designed for settings where real data are scarce or restricted by privacy constraints. The model exploits the expressive power of quantum circuits to learn complex data distributions, which are then mapped to tabular features using classical neural networks. We evaluate QTabGAN on multiple classification and regression datasets and benchmark it against leading state-of-the-art generative models. Experiments show that QTabGAN achieves up to 54.07% improvement across various classification datasets and evaluation metrics, thus establishing a scalable quantum approach to tabular data synthesis and highlighting its potential for quantum-assisted generative modelling.",
      "authors": [
        "Subhangi Kumari",
        "Rakesh Achutha",
        "Vignesh Sivaraman"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "quant-ph"
      ],
      "published": "2026-02-13 08:17:28+00:00",
      "link": "https://arxiv.org/pdf/2602.12704v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12703v1",
      "title": "SWING: Unlocking Implicit Graph Representations for Graph Random Features",
      "abstract": "We propose SWING: Space Walks for Implicit Network Graphs, a new class of algorithms for computations involving Graph Random Features on graphs given by implicit representations (i-graphs), where edge-weights are defined as bi-variate functions of feature vectors in the corresponding nodes. Those classes of graphs include several prominent examples, such as: $ε$-neighborhood graphs, used on regular basis in machine learning. Rather than conducting walks on graphs' nodes, those methods rely on walks in continuous spaces, in which those graphs are embedded. To accurately and efficiently approximate original combinatorial calculations, SWING applies customized Gumbel-softmax sampling mechanism with linearized kernels, obtained via random features coupled with importance sampling techniques. This algorithm is of its own interest. SWING relies on the deep connection between implicitly defined graphs and Fourier analysis, presented in this paper. SWING is accelerator-friendly and does not require input graph materialization. We provide detailed analysis of SWING and complement it with thorough experiments on different classes of i-graphs.",
      "authors": [
        "Alessandro Manenti",
        "Avinava Dubey",
        "Arijit Sehanobish",
        "Cesare Alippi",
        "Krzysztof Choromanski"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 08:12:38+00:00",
      "link": "https://arxiv.org/pdf/2602.12703v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12693v1",
      "title": "Leverage-Weighted Conformal Prediction",
      "abstract": "Split conformal prediction provides distribution-free prediction intervals with finite-sample marginal coverage, but produces constant-width intervals that overcover in low-variance regions and undercover in high-variance regions. Existing adaptive methods require training auxiliary models. We propose Leverage-Weighted Conformal Prediction (LWCP), which weights nonconformity scores by a function of the statistical leverage -- the diagonal of the hat matrix -- deriving adaptivity from the geometry of the design matrix rather than from auxiliary model fitting. We prove that LWCP preserves finite-sample marginal validity for any weight function; achieves asymptotically optimal conditional coverage at essentially no width cost when heteroscedasticity factors through leverage; and recovers the form and width of classical prediction intervals under Gaussian assumptions while retaining distribution-free guarantees. We further establish that randomized leverage approximations preserve coverage exactly with controlled width perturbation, and that vanilla CP suffers a persistent, sample-size-independent conditional coverage gap that LWCP eliminates. The method requires no hyperparameters beyond the choice of weight function and adds negligible computational overhead to vanilla CP. Experiments on synthetic and real data confirm the theoretical predictions, demonstrating substantial reductions in conditional coverage disparity across settings.",
      "authors": [
        "Shreyas Fadnavis"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 07:49:39+00:00",
      "link": "https://arxiv.org/pdf/2602.12693v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12687v1",
      "title": "Trust the uncertain teacher: distilling dark knowledge via calibrated uncertainty",
      "abstract": "The core of knowledge distillation lies in transferring the teacher's rich 'dark knowledge'-subtle probabilistic patterns that reveal how classes are related and the distribution of uncertainties. While this idea is well established, teachers trained with conventional cross-entropy often fail to preserve such signals. Their distributions collapse into sharp, overconfident peaks that appear decisive but are in fact brittle, offering little beyond the hard label or subtly hindering representation-level transfer. This overconfidence is especially problematic in high-cardinality tasks, where the nuances among many plausible classes matter most for guiding a compact student. Moreover, such brittle targets reduce robustness under distribution shift, leaving students vulnerable to miscalibration in real-world conditions. To address this limitation, we revisit distillation from a distributional perspective and propose Calibrated Uncertainty Distillation (CUD), a framework designed to make dark knowledge more faithfully accessible. Instead of uncritically adopting the teacher's overconfidence, CUD encourages teachers to reveal uncertainty where it is informative and guides students to learn from targets that are calibrated rather than sharpened certainty. By directly shaping the teacher's predictive distribution before transfer, our approach balances accuracy and calibration, allowing students to benefit from both confident signals on easy cases and structured uncertainty on hard ones. Across diverse benchmarks, CUD yields students that are not only more accurate, but also more calibrated under shift and more reliable on ambiguous, long-tail inputs.",
      "authors": [
        "Jeonghyun Kim",
        "SooKyung Kim",
        "Richeng Xuan",
        "Hyunsoo Cho"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 07:43:19+00:00",
      "link": "https://arxiv.org/pdf/2602.12687v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12681v1",
      "title": "Fool Me If You Can: On the Robustness of Binary Code Similarity Detection Models against Semantics-preserving Transformations",
      "abstract": "Binary code analysis plays an essential role in cybersecurity, facilitating reverse engineering to reveal the inner workings of programs in the absence of source code. Traditional approaches, such as static and dynamic analysis, extract valuable insights from stripped binaries, but often demand substantial expertise and manual effort. Recent advances in deep learning have opened promising opportunities to enhance binary analysis by capturing latent features and disclosing underlying code semantics. Despite the growing number of binary analysis models based on machine learning, their robustness to adversarial code transformations at the binary level remains underexplored. We evaluate the robustness of deep learning models for the task of binary code similarity detection (BCSD) under semantics-preserving transformations. The unique nature of machine instructions presents distinct challenges compared to the typical input perturbations found in other domains. We introduce asmFooler, a system that evaluates the resilience of BCSD models using a diverse set of adversarial code transformations that preserve functional semantics. We construct a dataset of 9,565 binary variants from 620 baseline samples by applying eight semantics-preserving transformations across six representative BCSD models. Our major findings highlight several key insights: i) model robustness relies on the processing pipeline, including code pre-processing, architecture, and feature selection; ii) adversarial transformation effectiveness is bounded by a budget shaped by model-specific constraints like input size and instruction expressive capacity; iii) well-crafted transformations can be highly effective with minimal perturbations; and iv) such transformations efficiently disrupt model decisions (e.g., misleading to false positives or false negatives) by focusing on semantically significant instructions.",
      "authors": [
        "Jiyong Uhm",
        "Minseok Kim",
        "Michalis Polychronakis",
        "Hyungjoon Koo"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published": "2026-02-13 07:23:15+00:00",
      "link": "https://arxiv.org/pdf/2602.12681v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12680v1",
      "title": "A Regularization-Sharpness Tradeoff for Linear Interpolators",
      "abstract": "The rule of thumb regarding the relationship between the bias-variance tradeoff and model size plays a key role in classical machine learning, but is now well-known to break down in the overparameterized setting as per the double descent curve. In particular, minimum-norm interpolating estimators can perform well, suggesting the need for new tradeoff in these settings. Accordingly, we propose a regularization-sharpness tradeoff for overparameterized linear regression with an $\\ell^p$ penalty. Inspired by the interpolating information criterion, our framework decomposes the selection penalty into a regularization term (quantifying the alignment of the regularizer and the interpolator) and a geometric sharpness term on the interpolating manifold (quantifying the effect of local perturbations), yielding a tradeoff analogous to bias-variance. Building on prior analyses that established this information criterion for ridge regularizers, this work first provides a general expression of the interpolating information criterion for $\\ell^p$ regularizers where $p \\ge 2$. Subsequently, we extend this to the LASSO interpolator with $\\ell^1$ regularizer, which induces stronger sparsity. Empirical results on real-world datasets with random Fourier features and polynomials validate our theory, demonstrating how the tradeoff terms can distinguish performant linear interpolators from weaker ones.",
      "authors": [
        "Qingyi Hu",
        "Liam Hodgkinson"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-13 07:21:08+00:00",
      "link": "https://arxiv.org/pdf/2602.12680v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12674v1",
      "title": "$\\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models",
      "abstract": "Knowledge Distillation (KD) for Large Language Models (LLMs) has become increasingly important as models grow in size and complexity. While existing distillation approaches focus on imitating teacher behavior, they often overlook the original learning environment that shaped the teacher's knowledge. Inspired by the experiential learning theory and inverse reinforcement learning, we propose Experiential Knowledge Distillation ($\\mathcal{X}$-KD), a novel and general framework that enables student models to learn in the teacher's original learning environment. $\\mathcal{X}$-KD adopts the Approximated Variational Reward Imitation Learning (AVRIL) framework to jointly model the teacher's original reward function and perform policy distillation, encouraging consistency between the student policy and the original reward function. Our derivation demonstrates that $\\mathcal{X}$-KD follows the supervised learning framework and applies to both sequence-level and divergence-based distillation methods, underlining the simplicity and flexibility of our approach. Empirical results show that $\\mathcal{X}$-KD outperforms the generalized KD and MiniLLM baselines on abstractive summarization, machine translation, and arithmetic reasoning tasks. Additionally, $\\mathcal{X}$-KD achieves better performance-diversity trade-off and data efficiency than baseline KD approaches.",
      "authors": [
        "Yuang Cai",
        "Yuyu Yuan"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 07:15:10+00:00",
      "link": "https://arxiv.org/pdf/2602.12674v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12665v1",
      "title": "Evaluating Robustness of Reasoning Models on Parameterized Logical Problems",
      "abstract": "Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.",
      "authors": [
        "Naïm Es-sebbani",
        "Esteban Marquer",
        "Yakoub Salhi",
        "Zied Bouraoui"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-13 06:54:25+00:00",
      "link": "https://arxiv.org/pdf/2602.12665v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15902v1",
      "title": "Doc-to-LoRA: Learning to Instantly Internalize Contexts",
      "abstract": "Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into model parameters, per-prompt distillation is impractical due to training costs and latency. To address these limitations, we propose Doc-to-LoRA (D2L), a lightweight hypernetwork that meta-learns to perform approximate CD within a single forward pass. Given an unseen prompt, D2L generates a LoRA adapter for a target LLM, enabling subsequent queries to be answered without re-consuming the original context, reducing latency and KV-cache memory consumption during inference of the target LLM. On a long-context needle-in-a-haystack task, D2L successfully learns to map contexts into adapters that store the needle information, achieving near-perfect zero-shot accuracy at sequence lengths exceeding the target LLM's native context window by more than 4x. On real-world QA datasets with limited compute, D2L outperforms standard CD while significantly reducing peak memory consumption and update latency. We envision that D2L can facilitate rapid adaptation of LLMs, opening up the possibility of frequent knowledge updates and personalized chat behavior.",
      "authors": [
        "Rujikorn Charakorn",
        "Edoardo Cetin",
        "Shinnosuke Uesaka",
        "Robert Tjarko Lange"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-13 06:54:20+00:00",
      "link": "https://arxiv.org/pdf/2602.15902v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12660v1",
      "title": "Learning Ordinal Probabilistic Reward from Preferences",
      "abstract": "Reward models are crucial for aligning large language models (LLMs) with human values and intentions. Existing approaches follow either Generative (GRMs) or Discriminative (DRMs) paradigms, yet both suffer from limitations: GRMs typically demand costly point-wise supervision, while DRMs produce uncalibrated relative scores that lack probabilistic interpretation. To address these challenges, we introduce a novel reward modeling paradigm: Probabilistic Reward Model (PRM). Instead of modeling reward as a deterministic scalar, our approach treats it as a random variable, learning a full probability distribution for the quality of each response. To make this paradigm practical, we present its closed-form, discrete realization: the Ordinal Probabilistic Reward Model (OPRM), which discretizes the quality score into a finite set of ordinal ratings. Building on OPRM, we propose a data-efficient training strategy called Region Flooding Tuning (RgFT). It enables rewards to better reflect absolute text quality by incorporating quality-level annotations, which guide the model to concentrate the probability mass within corresponding rating sub-regions. Experiments on various reward model benchmarks show that our method improves accuracy by $\\textbf{2.9%}\\sim\\textbf{7.4%}$ compared to prior reward models, demonstrating strong performance and data efficiency. Analysis of the score distribution provides evidence that our method captures not only relative rankings but also absolute quality.",
      "authors": [
        "Longze Chen",
        "Lu Wang",
        "Renke Shan",
        "Ze Gong",
        "Run Luo",
        "Jiaming Li",
        "Jing Luo",
        "Qiyao Wang",
        "Min Yang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 06:43:02+00:00",
      "link": "https://arxiv.org/pdf/2602.12660v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12649v1",
      "title": "Multi-Task Learning with Additive U-Net for Image Denoising and Classification",
      "abstract": "We investigate additive skip fusion in U-Net architectures for image denoising and denoising-centric multi-task learning (MTL). By replacing concatenative skips with gated additive fusion, the proposed Additive U-Net (AddUNet) constrains shortcut capacity while preserving fixed feature dimensionality across depth. This structural regularization induces controlled encoder-decoder information flow and stabilizes joint optimization. Across single-task denoising and joint denoising-classification settings, AddUNet achieves competitive reconstruction performance with improved training stability. In MTL, learned skip weights exhibit systematic task-aware redistribution: shallow skips favor reconstruction, while deeper features support discrimination. Notably, reconstruction remains robust even under limited classification capacity, indicating implicit task decoupling through additive fusion. These findings show that simple constraints on skip connections act as an effective architectural regularizer for stable and scalable multi-task learning without increasing model complexity.",
      "authors": [
        "Vikram Lakkavalli",
        "Neelam Sinha"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-13 06:15:32+00:00",
      "link": "https://arxiv.org/pdf/2602.12649v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12642v1",
      "title": "Beyond Normalization: Rethinking the Partition Function as a Difficulty Scheduler for RLVR",
      "abstract": "Reward-maximizing RL methods enhance the reasoning performance of LLMs, but often reduce the diversity among outputs. Recent works address this issue by adopting GFlowNets, training LLMs to match a target distribution while jointly learning its partition function. In contrast to prior works that treat this partition function solely as a normalizer, we reinterpret it as a per-prompt expected-reward (i.e., online accuracy) signal, leveraging this unused information to improve sample efficiency. Specifically, we first establish a theoretical relationship between the partition function and per-prompt accuracy estimates. Building on this key insight, we propose Partition Function-Guided RL (PACED-RL), a post-training framework that leverages accuracy estimates to prioritize informative question prompts during training, and further improves sample efficiency through an accuracy estimate error-prioritized replay. Crucially, both components reuse information already produced during GFlowNet training, effectively amortizing the compute overhead into the existing optimization process. Extensive experiments across diverse benchmarks demonstrate strong performance improvements over GRPO and prior GFlowNet approaches, highlighting PACED-RL as a promising direction for a more sample efficient distribution-matching training for LLMs.",
      "authors": [
        "Dohyung Kim",
        "Minbeom Kim",
        "Jeonghye Kim",
        "Sangmook Lee",
        "Sojeong Rhee",
        "Kyomin Jung"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-13 06:04:14+00:00",
      "link": "https://arxiv.org/pdf/2602.12642v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12640v1",
      "title": "ImageRAGTurbo: Towards One-step Text-to-Image Generation with Retrieval-Augmented Diffusion Models",
      "abstract": "Diffusion models have emerged as the leading approach for text-to-image generation. However, their iterative sampling process, which gradually morphs random noise into coherent images, introduces significant latency that limits their applicability. While recent few-step diffusion models reduce the number of sampling steps to as few as one to four steps, they often compromise image quality and prompt alignment, especially in one-step generation. Additionally, these models require computationally expensive training procedures. To address these limitations, we propose ImageRAGTurbo, a novel approach to efficiently finetune few-step diffusion models via retrieval augmentation. Given a text prompt, we retrieve relevant text-image pairs from a database and use them to condition the generation process. We argue that such retrieved examples provide rich contextual information to the UNet denoiser that helps reduce the number of denoising steps without compromising image quality. Indeed, our initial investigations show that using the retrieved content to edit the denoiser's latent space ($\\mathcal{H}$-space) without additional finetuning already improves prompt fidelity. To further improve the quality of the generated images, we augment the UNet denoiser with a trainable adapter in the $\\mathcal{H}$-space, which efficiently blends the retrieved content with the target prompt using a cross-attention mechanism. Experimental results on fast text-to-image generation demonstrate that our approach produces high-fidelity images without compromising latency compared to existing methods.",
      "authors": [
        "Peijie Qiu",
        "Hariharan Ramshankar",
        "Arnau Ramisa",
        "René Vidal",
        "Amit Kumar K C",
        "Vamsi Salaka",
        "Rahul Bhagat"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-13 05:59:57+00:00",
      "link": "https://arxiv.org/pdf/2602.12640v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12639v1",
      "title": "CLASE: A Hybrid Method for Chinese Legalese Stylistic Evaluation",
      "abstract": "Legal text generated by large language models (LLMs) can usually achieve reasonable factual accuracy, but it frequently fails to adhere to the specialised stylistic norms and linguistic conventions of legal writing. In order to improve stylistic quality, a crucial first step is to establish a reliable evaluation method. However, having legal experts manually develop such a metric is impractical, as the implicit stylistic requirements in legal writing practice are difficult to formalise into explicit rubrics. Meanwhile, existing automatic evaluation methods also fall short: reference-based metrics conflate semantic accuracy with stylistic fidelity, and LLM-as-a-judge evaluations suffer from opacity and inconsistency. To address these challenges, we introduce CLASE (Chinese LegAlese Stylistic Evaluation), a hybrid evaluation method that focuses on the stylistic performance of legal text. The method incorporates a hybrid scoring mechanism that combines 1) linguistic feature-based scores and 2) experience-guided LLM-as-a-judge scores. Both the feature coefficients and the LLM scoring experiences are learned from contrastive pairs of authentic legal documents and their LLM-restored counterparts. This hybrid design captures both surface-level features and implicit stylistic norms in a transparent, reference-free manner. Experiments on 200 Chinese legal documents show that CLASE achieves substantially higher alignment with human judgments than traditional metrics and pure LLM-as-a-judge methods. Beyond improved alignment, CLASE provides interpretable score breakdowns and suggestions for improvements, offering a scalable and practical solution for professional stylistic evaluation in legal text generation (Code and data for CLASE is available at: https://github.com/rexera/CLASE).",
      "authors": [
        "Yiran Rex Ma",
        "Yuxiao Ye",
        "Huiyuan Xie"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-13 05:51:56+00:00",
      "link": "https://arxiv.org/pdf/2602.12639v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12635v1",
      "title": "Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats",
      "abstract": "As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.",
      "authors": [
        "Pengxiang Zhao",
        "Hui-Ling Zhen",
        "Xing Li",
        "Han Bao",
        "Weizhe Lin",
        "Zhiyuan Yang",
        "Ziwei Yu",
        "Xin Wang",
        "Mingxuan Yuan",
        "Xianzhi Yu",
        "Zhenhua Dong"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-13 05:41:31+00:00",
      "link": "https://arxiv.org/pdf/2602.12635v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13352v1",
      "title": "Using Deep Learning to Generate Semantically Correct Hindi Captions",
      "abstract": "Automated image captioning using the content from the image is very appealing when done by harnessing the capability of computer vision and natural language processing. Extensive research has been done in the field with a major focus on the English language which gives the scope for further developments in the same with consideration of popular foreign languages. This research utilizes distinct models for translating the image caption into Hindi, the fourth most popular language across the world. Exploring the multi-modal architectures this research comprises local visual features, global visual features, attention mechanisms, and pre-trained models. Using google cloud translator on the image dataset from Flickr8k, Hindi image descriptions have been generated. Pre-trained CNNs like VGG16, ResNet50, and Inception V3 helped in retrieving image characteristics, while the uni-directional and bi-directional techniques of text encoding are used for the text encoding process. An additional Attention layer helps to generate a weight vector and, by multiplying it, combine image characteristics from each time step into a sentence-level feature vector. Bilingual evaluation understudy scores are used to compare the research outcome. Many experiments that serve as a baseline are done for the comparative analysis of the research. An image with a score of BLEU-1 is considered sufficient, whereas one with a score of BLEU-4 is considered to have fluid image captioning. For both BLEU scores, the attention-based bidirectional LSTM with VGG16 produced the best results of 0.59 and 0.19 respectively. The experiments conclude that researchs ability to produce relevant, semantically accurate image captions in Hindi. The research accomplishes the goals and future research can be guided by this research model.",
      "authors": [
        "Wasim Akram Khan",
        "Anil Kumar Vuppala"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-13 05:06:30+00:00",
      "link": "https://arxiv.org/pdf/2602.13352v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12622v1",
      "title": "Efficient Personalized Federated PCA with Manifold Optimization for IoT Anomaly Detection",
      "abstract": "Internet of things (IoT) networks face increasing security threats due to their distributed nature and resource constraints. Although federated learning (FL) has gained prominence as a privacy-preserving framework for distributed IoT environments, current federated principal component analysis (PCA) methods lack the integration of personalization and robustness, which are critical for effective anomaly detection. To address these limitations, we propose an efficient personalized federated PCA (FedEP) method for anomaly detection in IoT networks. The proposed model achieves personalization through introducing local representations with the $\\ell_1$-norm for element-wise sparsity, while maintaining robustness via enforcing local models with the $\\ell_{2,1}$-norm for row-wise sparsity. To solve this non-convex problem, we develop a manifold optimization algorithm based on the alternating direction method of multipliers (ADMM) with rigorous theoretical convergence guarantees. Experimental results confirm that the proposed FedEP outperforms the state-of-the-art FedPG, achieving excellent F1-scores and accuracy in various IoT security scenarios. Our code will be available at \\href{https://github.com/xianchaoxiu/FedEP}{https://github.com/xianchaoxiu/FedEP}.",
      "authors": [
        "Xianchao Xiu",
        "Chenyi Huang",
        "Wei Zhang",
        "Wanquan Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 04:58:50+00:00",
      "link": "https://arxiv.org/pdf/2602.12622v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12618v1",
      "title": "Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models",
      "abstract": "Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are incompatible with FlashAttention. We take a different approach: rather than identifying unimportant tokens, we treat the LLM itself as the optimal guide for compression. Observing that deeper layers naturally transmit vision-to-text information, we introduce Attention-Driven Self-Compression (ADSC), a simple, broadly applicable method that progressively reduces vision tokens using only the LLM's attention mechanism. Our method applies uniform token downsampling at selected layers, forming bottlenecks that encourage the model to reorganize and compress information into the remaining tokens. It requires no score computation, auxiliary modules, or attention modification, and remains fully compatible with FlashAttention. Applied to LLaVA-1.5, ADSC reduces FLOPs by 53.7% and peak KV-cache memory by 56.7%, while preserving 98.2% of the original model performance. Across multiple benchmarks, it outperforms prior pruning approaches in both efficiency and accuracy. Crucially, under high compression ratios, our method remains robust while heuristic-based techniques degrade sharply.",
      "authors": [
        "Omer Faruk Deniz",
        "Ruiyu Mao",
        "Ruochen Li",
        "Yapeng Tian",
        "Latifur Khan"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-13 04:49:27+00:00",
      "link": "https://arxiv.org/pdf/2602.12618v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12613v1",
      "title": "Coden: Efficient Temporal Graph Neural Networks for Continuous Prediction",
      "abstract": "Temporal Graph Neural Networks (TGNNs) are pivotal in processing dynamic graphs. However, existing TGNNs primarily target one-time predictions for a given temporal span, whereas many practical applications require continuous predictions, that predictions are issued frequently over time. Directly adapting existing TGNNs to continuous-prediction scenarios introduces either significant computational overhead or prediction quality issues especially for large graphs. This paper revisits the challenge of { continuous predictions} in TGNNs, and introduces {\\sc Coden}, a TGNN model designed for efficient and effective learning on dynamic graphs. {\\sc Coden} innovatively overcomes the key complexity bottleneck in existing TGNNs while preserving comparable predictive accuracy. Moreover, we further provide theoretical analyses that substantiate the effectiveness and efficiency of {\\sc Coden}, and clarify its duality relationship with both RNN-based and attention-based models. Our evaluations across five dynamic datasets show that {\\sc Coden} surpasses existing performance benchmarks in both efficiency and effectiveness, establishing it as a superior solution for continuous prediction in evolving graph environments.",
      "authors": [
        "Zulun Zhu",
        "Siqiang Luo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 04:39:42+00:00",
      "link": "https://arxiv.org/pdf/2602.12613v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12612v1",
      "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback",
      "abstract": "Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space target to open-ended program spaces, they primarily rely on scalar metrics (e.g., NDCG, Hit Ratio) that fail to provide qualitative insights into model failures or directional guidance for improvement. To address this, we propose Self-EvolveRec, a novel framework that establishes a directional feedback loop by integrating a User Simulator for qualitative critiques and a Model Diagnosis Tool for quantitative internal verification. Furthermore, we introduce a Diagnosis Tool - Model Co-Evolution strategy to ensure that evaluation criteria dynamically adapt as the recommendation architecture evolves. Extensive experiments demonstrate that Self-EvolveRec significantly outperforms state-of-the-art NAS and LLM-driven code evolution baselines in both recommendation performance and user satisfaction. Our code is available at https://github.com/Sein-Kim/self_evolverec.",
      "authors": [
        "Sein Kim",
        "Sangwu Park",
        "Hongseok Kang",
        "Wonjoong Kim",
        "Jimin Seo",
        "Yeonjun In",
        "Kanghoon Yoon",
        "Chanyoung Park"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-13 04:38:32+00:00",
      "link": "https://arxiv.org/pdf/2602.12612v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12606v1",
      "title": "RelBench v2: A Large-Scale Benchmark and Repository for Relational Data",
      "abstract": "Relational deep learning (RDL) has emerged as a powerful paradigm for learning directly on relational databases by modeling entities and their relationships across multiple interconnected tables. As this paradigm evolves toward larger models and relational foundation models, scalable and realistic benchmarks are essential for enabling systematic evaluation and progress. In this paper, we introduce RelBench v2, a major expansion of the RelBench benchmark for RDL. RelBench v2 adds four large-scale relational datasets spanning scholarly publications, enterprise resource planning, consumer platforms, and clinical records, increasing the benchmark to 11 datasets comprising over 22 million rows across 29 tables. We further introduce autocomplete tasks, a new class of predictive objectives that require models to infer missing attribute values directly within relational tables while respecting temporal constraints, expanding beyond traditional forecasting tasks constructed via SQL queries. In addition, RelBench v2 expands beyond its native datasets by integrating external benchmarks and evaluation frameworks: we translate event streams from the Temporal Graph Benchmark into relational schemas for unified relational-temporal evaluation, interface with ReDeLEx to provide uniform access to 70+ real-world databases suitable for pretraining, and incorporate 4DBInfer datasets and tasks to broaden multi-table prediction coverage. Experimental results demonstrate that RDL models consistently outperform single-table baselines across autocomplete, forecasting, and recommendation tasks, highlighting the importance of modeling relational structure explicitly.",
      "authors": [
        "Justin Gu",
        "Rishabh Ranjan",
        "Charilaos Kanatsoulis",
        "Haiming Tang",
        "Martin Jurkovic",
        "Valter Hudovernik",
        "Mark Znidar",
        "Pranshu Chaturvedi",
        "Parth Shroff",
        "Fengyu Li",
        "Jure Leskovec"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 04:23:48+00:00",
      "link": "https://arxiv.org/pdf/2602.12606v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12605v1",
      "title": "Block-Sample MAC-Bayes Generalization Bounds",
      "abstract": "We present a family of novel block-sample MAC-Bayes bounds (mean approximately correct). While PAC-Bayes bounds (probably approximately correct) typically give bounds for the generalization error that hold with high probability, MAC-Bayes bounds have a similar form but bound the expected generalization error instead. The family of bounds we propose can be understood as a generalization of an expectation version of known PAC-Bayes bounds. Compared to standard PAC-Bayes bounds, the new bounds contain divergence terms that only depend on subsets (or \\emph{blocks}) of the training data. The proposed MAC-Bayes bounds hold the promise of significantly improving upon the tightness of traditional PAC-Bayes and MAC-Bayes bounds. This is illustrated with a simple numerical example in which the original PAC-Bayes bound is vacuous regardless of the choice of prior, while the proposed family of bounds are finite for appropriate choices of the block size. We also explore the question whether high-probability versions of our MAC-Bayes bounds (i.e., PAC-Bayes bounds of a similar form) are possible. We answer this question in the negative with an example that shows that in general, it is not possible to establish a PAC-Bayes bound which (a) vanishes with a rate faster than $\\mathcal{O}(1/\\log n)$ whenever the proposed MAC-Bayes bound vanishes with rate $\\mathcal{O}(n^{-1/2})$ and (b) exhibits a logarithmic dependence on the permitted error probability.",
      "authors": [
        "Matthias Frey",
        "Jingge Zhu",
        "Michael C. Gastpar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "published": "2026-02-13 04:23:12+00:00",
      "link": "https://arxiv.org/pdf/2602.12605v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12601v1",
      "title": "HyperMLP: An Integrated Perspective for Sequence Modeling",
      "abstract": "Self-attention is often viewed as probabilistic query-key lookup, motivating designs that preserve normalized attention scores and fixed positional semantics. We advocate a simpler and more unified perspective: an autoregressive attention head can be viewed as a dynamic two-layer MLP whose weights are instantiated from the context history. From this view, attention scores form an ever-growing hidden representation, and standard MLP activations such as ReLU or GLU naturally implement input-conditioned selection over a context-dependent memory pool rather than a probability distribution. Based on this formulation, we introduce HyperMLP and HyperGLU, which learn dynamic mixing in both feature space and sequence space, using a reverse-offset (lag) layout to align temporal mixing with autoregressive semantics. We provide theoretical characterizations of the expressivity and implications of this structure, and empirically show that HyperMLP/HyperGLU consistently outperform strong softmax-attention baselines under matched parameter budgets.",
      "authors": [
        "Jiecheng Lu",
        "Shihao Yang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "published": "2026-02-13 04:20:10+00:00",
      "link": "https://arxiv.org/pdf/2602.12601v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12593v1",
      "title": "RQ-GMM: Residual Quantized Gaussian Mixture Model for Multimodal Semantic Discretization in CTR Prediction",
      "abstract": "Multimodal content is crucial for click-through rate (CTR) prediction. However, directly incorporating continuous embeddings from pre-trained models into CTR models yields suboptimal results due to misaligned optimization objectives and convergence speed inconsistency during joint training. Discretizing embeddings into semantic IDs before feeding them into CTR models offers a more effective solution, yet existing methods suffer from limited codebook utilization, reconstruction accuracy, and semantic discriminability. We propose RQ-GMM (Residual Quantized Gaussian Mixture Model), which introduces probabilistic modeling to better capture the statistical structure of multimodal embedding spaces. Through Gaussian Mixture Models combined with residual quantization, RQ-GMM achieves superior codebook utilization and reconstruction accuracy. Experiments on public datasets and online A/B tests on a large-scale short-video platform serving hundreds of millions of users demonstrate substantial improvements: RQ-GMM yields a 1.502% gain in Advertiser Value over strong baselines. The method has been fully deployed, serving daily recommendations for hundreds of millions of users.",
      "authors": [
        "Ziye Tong",
        "Jiahao Liu",
        "Weimin Zhang",
        "Hongji Ruan",
        "Derick Tang",
        "Zhanpeng Zeng",
        "Qinsong Zeng",
        "Peng Zhang",
        "Tun Lu",
        "Ning Gu"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-13 04:11:24+00:00",
      "link": "https://arxiv.org/pdf/2602.12593v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12592v1",
      "title": "Power Interpretable Causal ODE Networks: A Unified Model for Explainable Anomaly Detection and Root Cause Analysis in Power Systems",
      "abstract": "Anomaly detection and root cause analysis (RCA) are critical for ensuring the safety and resilience of cyber-physical systems such as power grids. However, existing machine learning models for time series anomaly detection often operate as black boxes, offering only binary outputs without any explanation, such as identifying anomaly type and origin. To address this challenge, we propose Power Interpretable Causality Ordinary Differential Equation (PICODE) Networks, a unified, causality-informed architecture that jointly performs anomaly detection along with the explanation why it is detected as an anomaly, including root cause localization, anomaly type classification, and anomaly shape characterization. Experimental results in power systems demonstrate that PICODE achieves competitive detection performance while offering improved interpretability and reduced reliance on labeled data or external causal graphs. We provide theoretical results demonstrating the alignment between the shape of anomaly functions and the changes in the weights of the extracted causal graphs.",
      "authors": [
        "Yue Sun",
        "Likai Wang",
        "Rick S. Blum",
        "Parv Venkitasubramaniam"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 04:06:47+00:00",
      "link": "https://arxiv.org/pdf/2602.12592v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12587v1",
      "title": "Multi-Head Attention as a Source of Catastrophic Forgetting in MoE Transformers",
      "abstract": "Mixture-of-Experts (MoE) architectures are often considered a natural fit for continual learning because sparse routing should localize updates and reduce interference, yet MoE Transformers still forget substantially even with sparse, well-balanced expert utilization. We attribute this gap to a pre-routing bottleneck: multi-head attention concatenates head-specific signals into a single post-attention router input, forcing routing to act on co-occurring feature compositions rather than separable head channels. We show that this router input simultaneously encodes multiple separately decodable semantic and structural factors with uneven head support, and that different feature compositions induce weakly aligned parameter-gradient directions; as a result, routing maps many distinct compositions to the same route. We quantify this collision effect via a route-wise effective composition number $N_{eff}$ and find that higher $N_{eff}$ is associated with larger old-task loss increases after continual training. Motivated by these findings, we propose MH-MoE, which performs head-wise routing over sub-representations to increase routing granularity and reduce composition collisions. On TRACE with Qwen3-0.6B/8B, MH-MoE effectively mitigates forgetting, reducing BWT on Qwen3-0.6B from 11.2% (LoRAMoE) to 4.5%.",
      "authors": [
        "Anrui Chen",
        "Ruijun Huang",
        "Xin Zhang",
        "Fang Dong",
        "Hengjie Cao",
        "Zhendong Huang",
        "Yifeng Yang",
        "Mengyi Chen",
        "Jixian Zhou",
        "Mingzhi Dong",
        "Yujiang Wang",
        "Jinlong Hou",
        "Qin Lv",
        "Robert P. Dick",
        "Yuan Cheng",
        "Tun Lu",
        "Fan Yang",
        "Li Shang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 03:59:09+00:00",
      "link": "https://arxiv.org/pdf/2602.12587v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12575v1",
      "title": "Discovering Semantic Latent Structures in Psychological Scales: A Response-Free Pathway to Efficient Simplification",
      "abstract": "Psychological scale refinement traditionally relies on response-based methods such as factor analysis, item response theory, and network psychometrics to optimize item composition. Although rigorous, these approaches require large samples and may be constrained by data availability and cross-cultural comparability. Recent advances in natural language processing suggest that the semantic structure of questionnaire items may encode latent construct organization, offering a complementary response-free perspective. We introduce a topic-modeling framework that operationalizes semantic latent structure for scale simplification. Items are encoded using contextual sentence embeddings and grouped via density-based clustering to discover latent semantic factors without predefining their number. Class-based term weighting derives interpretable topic representations that approximate constructs and enable merging of semantically adjacent clusters. Representative items are selected using membership criteria within an integrated reduction pipeline. We benchmarked the framework across DASS, IPIP, and EPOCH, evaluating structural recovery, internal consistency, factor congruence, correlation preservation, and reduction efficiency. The proposed method recovered coherent factor-like groupings aligned with established constructs. Selected items reduced scale length by 60.5% on average while maintaining psychometric adequacy. Simplified scales showed high concordance with original factor structures and preserved inter-factor correlations, indicating that semantic latent organization provides a response-free approximation of measurement structure. Our framework formalizes semantic structure as an inspectable front-end for scale construction and reduction. To facilitate adoption, we provide a visualization-supported tool enabling one-click semantic analysis and structured simplification.",
      "authors": [
        "Bo Wang",
        "Yuxuan Zhang",
        "Yueqin Hu",
        "Hanchao Hou",
        "Kaiping Peng",
        "Shiguang Ni"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-13 03:37:15+00:00",
      "link": "https://arxiv.org/pdf/2602.12575v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12574v1",
      "title": "Monte Carlo Tree Search with Reasoning Path Refinement for Small Language Models in Conversational Text-to-NoSQL",
      "abstract": "NoSQL databases have been widely adopted in big data analytics, geospatial applications, and healthcare services, due to their flexibility and scalability. However, querying NoSQL databases requires specialized technical expertise, creating a high barrier for users. While recent studies have explored text-to-NoSQL problem, they primarily focus on single-turn interactions, ignoring the conversational nature of real-world queries. To bridge this gap, we introduce the Conversational Text-to-NoSQL task, which generates NoSQL queries given a natural language question, a NoSQL database, and the dialogue history. To address this task, we propose Stage-MCTS, a framework that endows small language models (SLMs) with NoSQL-specific reasoning capabilities by formulating query generation as a search problem. The framework employs Monte Carlo Tree Search (MCTS) guided by a rule-based reward to produce stepwise reasoning data, followed by progressive supervised fine-tuning (SFT) and self-training strategies. We further construct CoNoSQL, a cross-domain dataset with over 2,000 dialogues and 150 databases, to support evaluation. Experiments demonstrate that our approach outperforms state-of-the-art large reasoning models, improving execution value match (EVM) accuracy by up to 7.93%.",
      "authors": [
        "Xubang Xiong",
        "Raymond Chi-Wing Wong",
        "Yuanfeng Song"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "published": "2026-02-13 03:35:38+00:00",
      "link": "https://arxiv.org/pdf/2602.12574v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12569v1",
      "title": "Editable XAI: Toward Bidirectional Human-AI Alignment with Co-Editable Explanations of Interpretable Attributes",
      "abstract": "While Explainable AI (XAI) helps users understand AI decisions, misalignment in domain knowledge can lead to disagreement. This inconsistency hinders understanding, and because explanations are often read-only, users lack the control to improve alignment. We propose making XAI editable, allowing users to write rules to improve control and gain deeper understanding through the generation effect of active learning. We developed CoExplain, leveraging a neural network for universal representation and symbolic rules for intuitive reasoning on interpretable attributes. CoExplain explains the neural network with a faithful proxy decision tree, parses user-written rules as an equivalent neural network graph, and collaboratively optimizes the decision tree. In a user study (N=43), CoExplain and manually editable XAI improved user understanding and model alignment compared to read-only XAI. CoExplain was easier to use with fewer edits and less time. This work contributes Editable XAI for bidirectional AI alignment, improving understanding and control.",
      "authors": [
        "Haoyang Chen",
        "Jingwen Bai",
        "Fang Tian",
        "Brian Y Lim"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-13 03:27:11+00:00",
      "link": "https://arxiv.org/pdf/2602.12569v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12567v1",
      "title": "Fractional Order Federated Learning for Battery Electric Vehicle Energy Consumption Modeling",
      "abstract": "Federated learning on connected electric vehicles (BEVs) faces severe instability due to intermittent connectivity, time-varying client participation, and pronounced client-to-client variation induced by diverse operating conditions. Conventional FedAvg and many advanced methods can suffer from excessive drift and degraded convergence under these realistic constraints. This work introduces Fractional-Order Roughness-Informed Federated Averaging (FO-RI-FedAvg), a lightweight and modular extension of FedAvg that improves stability through two complementary client-side mechanisms: (i) adaptive roughness-informed proximal regularization, which dynamically tunes the pull toward the global model based on local loss-landscape roughness, and (ii) non-integer-order local optimization, which incorporates short-term memory to smooth conflicting update directions. The approach preserves standard FedAvg server aggregation, adds only element-wise operations with amortizable overhead, and allows independent toggling of each component. Experiments on two real-world BEV energy prediction datasets, VED and its extended version eVED, show that FO-RI-FedAvg achieves improved accuracy and more stable convergence compared to strong federated baselines, particularly under reduced client participation.",
      "authors": [
        "Mohammad Partohaghighi",
        "Roummel Marcia",
        "Bruce J. West",
        "YangQuan Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 03:26:15+00:00",
      "link": "https://arxiv.org/pdf/2602.12567v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12564v1",
      "title": "CAPTS: Channel-Aware, Preference-Aligned Trigger Selection for Multi-Channel Item-to-Item Retrieval",
      "abstract": "Large-scale industrial recommender systems commonly adopt multi-channel retrieval for candidate generation, combining direct user-to-item (U2I) retrieval with two-hop user-to-item-to-item (U2I2I) pipelines. In U2I2I, the system selects a small set of historical interactions as triggers to seed downstream item-to-item (I2I) retrieval across multiple channels. In production, triggers are often selected using rule-based policies or learned scorers and tuned in a channel-by-channel manner. However, these practices face two persistent challenges: biased value attribution that values triggers by on-trigger feedback rather than their downstream utility as retrieval seeds, and uncoordinated multi-channel routing where channels select triggers independently under a shared quota, increasing cross-channel overlap. To address these challenges, we propose Channel-Aware, Preference-Aligned Trigger Selection (CAPTS), a unified and flexible framework that treats multi-channel trigger selection as a learnable routing problem. CAPTS introduces a Value Attribution Module (VAM) that provides look-ahead supervision by crediting each trigger with the subsequent engagement generated by items retrieved from it on each I2I channel, and a Channel-Adaptive Trigger Routing (CATR) module that coordinates trigger-to-channel assignment to maximize the overall value of multi-channel retrieval. Extensive offline experiments and large-scale online A/B tests on Kwai, Kuaishou's international short-video platform, show that CAPTS consistently improves multi-channel recall offline and delivers a +0.351% lift in average time spent per device online.",
      "authors": [
        "Xiaoyou Zhou",
        "Yuqi Liu",
        "Zhao Liu",
        "Xiao Lv",
        "Bo Chen",
        "Ruiming Tang",
        "Guorui Zhou"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-13 03:23:12+00:00",
      "link": "https://arxiv.org/pdf/2602.12564v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12556v1",
      "title": "SD-MoE: Spectral Decomposition for Effective Expert Specialization",
      "abstract": "Mixture-of-Experts (MoE) architectures scale Large Language Models via expert specialization induced by conditional computation. In practice, however, expert specialization often fails: some experts become functionally similar, while others functioning as de facto shared experts, limiting the effective capacity and model performance. In this work, we analysis from a spectral perspective on parameter and gradient spaces, uncover that (1) experts share highly overlapping dominant spectral components in their parameters, (2) dominant gradient subspaces are strongly aligned across experts, driven by ubiquitous low-rank structure in human corpus, and (3) gating mechanisms preferentially route inputs along these dominant directions, further limiting specialization. To address this, we propose Spectral-Decoupled MoE (SD-MoE), which decomposes both parameter and gradient in the spectral space. SD-MoE improves performance across downstream tasks, enables effective expert specialization, incurring minimal additional computation, and can be seamlessly integrated into a wide range of existing MoE architectures, including Qwen and DeepSeek.",
      "authors": [
        "Ruijun Huang",
        "Fang Dong",
        "Xin Zhang",
        "Hengjie Cao",
        "Zhendong Huang",
        "Anrui Chen",
        "Jixian Zhou",
        "Mengyi Chen",
        "Yifeng Yang",
        "Mingzhi Dong",
        "Yujiang Wang",
        "Jinlong Hou",
        "Qin Lv",
        "Robert P. Dick",
        "Yuan Cheng",
        "Fan Yang",
        "Tun Lu",
        "Chun Zhang",
        "Li Shang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 03:07:26+00:00",
      "link": "https://arxiv.org/pdf/2602.12556v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12546v1",
      "title": "Decoder-only Conformer with Modality-aware Sparse Mixtures of Experts for ASR",
      "abstract": "We present a decoder-only Conformer for automatic speech recognition (ASR) that processes speech and text in a single stack without external speech encoders or pretrained large language models (LLM). The model uses a modality-aware sparse mixture of experts (MoE): disjoint expert pools for speech and text with hard routing and top-1 selection, embedded in hybrid-causality Conformer blocks (bidirectional for speech, causal for text). Training combines CTC on speech positions with label-smoothed cross-entropy for text generation. Our 113M-parameter model consistently improves WER over a 139M AED baseline on Librispeech (2.8% vs. 3.2% test-clean; 5.6% vs. 6.0% test-other). On Common Voice 16.1 with a single multilingual model across five languages, our approach reduces average WER from 12.2% to 10.6%. To our knowledge, this is the first randomly initialized decoder-only ASR that surpasses strong AED baselines via modality-aware routing and sparse MoE, achieving better accuracy with fewer active parameters and without alignment/adaptation modules.",
      "authors": [
        "Jaeyoung Lee",
        "Masato Mimura"
      ],
      "primary_category": "eess.AS",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "published": "2026-02-13 02:53:54+00:00",
      "link": "https://arxiv.org/pdf/2602.12546v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12542v1",
      "title": "Exploring Accurate and Transparent Domain Adaptation in Predictive Healthcare via Concept-Grounded Orthogonal Inference",
      "abstract": "Deep learning models for clinical event prediction on electronic health records (EHR) often suffer performance degradation when deployed under different data distributions. While domain adaptation (DA) methods can mitigate such shifts, its \"black-box\" nature prevents widespread adoption in clinical practice where transparency is essential for trust and safety. We propose ExtraCare to decompose patient representations into invariant and covariant components. By supervising these two components and enforcing their orthogonality during training, our model preserves label information while exposing domain-specific variation at the same time for more accurate predictions than most feature alignment models. More importantly, it offers human-understandable explanations by mapping sparse latent dimensions to medical concepts and quantifying their contributions via targeted ablations. ExtraCare is evaluated on two real-world EHR datasets across multiple domain partition settings, demonstrating superior performance along with enhanced transparency, as evidenced by its accurate predictions and explanations from extensive case studies.",
      "authors": [
        "Pengfei Hu",
        "Chang Lu",
        "Feifan Liu",
        "Yue Ning"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-13 02:46:50+00:00",
      "link": "https://arxiv.org/pdf/2602.12542v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12540v1",
      "title": "Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting",
      "abstract": "Autonomous driving, as an agent operating in the physical world, requires the fundamental capability to build \\textit{world models} that capture how the environment evolves spatiotemporally in order to support long-term planning. At the same time, scalability demands learning such models in a self-supervised manner; \\textit{joint-embedding predictive architecture (JEPA)} enables learning world models via leveraging large volumes of unlabeled data without relying on expensive human annotations. In this paper, we propose \\textbf{AD-LiST-JEPA}, a self-supervised world model for autonomous driving that predicts future spatiotemporal evolution from LiDAR data using a JEPA framework. We evaluate the quality of the learned representations through a downstream LiDAR-based occupancy completion and forecasting (OCF) task, which jointly assesses perception and prediction. Proof of concept experiments show better OCF performance with pretrained encoder after JEPA-based world model learning.",
      "authors": [
        "Haoran Zhu",
        "Anna Choromanska"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2026-02-13 02:42:21+00:00",
      "link": "https://arxiv.org/pdf/2602.12540v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12537v1",
      "title": "News Harvesting from Google News combining Web Scraping, LLM Metadata Extraction and SCImago Media Rankings enrichment: a case study of IFMIF-DONES",
      "abstract": "This study develops and evaluates a systematic methodology for constructing news datasets from Google News, combining automated web scraping, large language model (LLM)-based metadata extraction, and SCImago Media Rankings enrichment. Using the IFMIF-DONES fusion energy project as a case study, we implemented a five-stage data collection pipeline across 81 region-language combinations, yielding 1,482 validated records after a 56% noise reduction. Results are compared against two licensed press databases: MyNews (2,280 records) and ProQuest Newsstream Collection (148 records). Overlap analysis reveals high complementarity, with 76% of Google News records exclusive to this platform. The dataset captures content types absent from proprietary databases, including specialized outlets, institutional communications, and social media posts. However, significant methodological challenges emerge: temporal instability requiring synchronic collection, a 100-result cap per query demanding multi-stage strategies, and unexpected noise including academic PDFs, false positives, and pornographic content infiltrating results through black hat SEO techniques. LLM-assisted extraction proved effective for structured articles but exhibited systematic hallucination patterns requiring validation protocols. We conclude that Google News offers valuable complementary coverage for communication research but demands substantial methodological investment, multi-source triangulation, and robust filtering mechanisms to ensure dataset integrity.",
      "authors": [
        "Victor Herrero-Solana"
      ],
      "primary_category": "cs.DL",
      "categories": [
        "cs.DL"
      ],
      "published": "2026-02-13 02:34:26+00:00",
      "link": "https://arxiv.org/pdf/2602.12537v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12534v1",
      "title": "Linear Regression with Unknown Truncation Beyond Gaussian Features",
      "abstract": "In truncated linear regression, samples $(x,y)$ are shown only when the outcome $y$ falls inside a certain survival set $S^\\star$ and the goal is to estimate the unknown $d$-dimensional regressor $w^\\star$. This problem has a long history of study in Statistics and Machine Learning going back to the works of (Galton, 1897; Tobin, 1958) and more recently in, e.g., (Daskalakis et al., 2019; 2021; Lee et al., 2023; 2024). Despite this long history, however, most prior works are limited to the special case where $S^\\star$ is precisely known. The more practically relevant case, where $S^\\star$ is unknown and must be learned from data, remains open: indeed, here the only available algorithms require strong assumptions on the distribution of the feature vectors (e.g., Gaussianity) and, even then, have a $d^{\\mathrm{poly} (1/\\varepsilon)}$ run time for achieving $\\varepsilon$ accuracy.   In this work, we give the first algorithm for truncated linear regression with unknown survival set that runs in $\\mathrm{poly} (d/\\varepsilon)$ time, by only requiring that the feature vectors are sub-Gaussian. Our algorithm relies on a novel subroutine for efficiently learning unions of a bounded number of intervals using access to positive examples (without any negative examples) under a certain smoothness condition. This learning guarantee adds to the line of works on positive-only PAC learning and may be of independent interest.",
      "authors": [
        "Alexandros Kouridakis",
        "Anay Mehrotra",
        "Alkis Kalavasis",
        "Constantine Caramanis"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.DS",
        "cs.LG",
        "math.ST"
      ],
      "published": "2026-02-13 02:29:54+00:00",
      "link": "https://arxiv.org/pdf/2602.12534v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12530v1",
      "title": "Reasoning to Rank: An End-to-End Solution for Exploiting Large Language Models for Recommendation",
      "abstract": "Recommender systems are tasked to infer users' evolving preferences and rank items aligned with their intents, which calls for in-depth reasoning beyond pattern-based scoring. Recent efforts start to leverage large language models (LLMs) for recommendation, but how to effectively optimize the model for improved recommendation utility is still under explored. In this work, we propose Reasoning to Rank, an end-to-end training framework that internalizes recommendation utility optimization into the learning of step-by-step reasoning in LLMs. To avoid position bias in LLM reasoning and enable direct optimization of the reasoning process, our framework performs reasoning at the user-item level and employs reinforcement learning for end-to-end training of the LLM. Experiments on three Amazon datasets and a large-scale industrial dataset showed consistent gains over strong conventional and LLM-based solutions. Extensive in-depth analyses validate the necessity of the key components in the proposed framework and shed lights on the future developments of this line of work.",
      "authors": [
        "Kehan Zheng",
        "Deyao Hong",
        "Qian Li",
        "Jun Zhang",
        "Huan Yu",
        "Jie Jiang",
        "Hongning Wang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-13 02:22:48+00:00",
      "link": "https://arxiv.org/pdf/2602.12530v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12528v1",
      "title": "DiffuRank: Effective Document Reranking with Diffusion Language Models",
      "abstract": "Recent advances in large language models (LLMs) have inspired new paradigms for document reranking. While this paradigm better exploits the reasoning and contextual understanding capabilities of LLMs, most existing LLM-based rerankers rely on autoregressive generation, which limits their efficiency and flexibility. In particular, token-by-token decoding incurs high latency, while the fixed left-to-right generation order causes early prediction errors to propagate and is difficult to revise. To address these limitations, we explore the use of diffusion language models (dLLMs) for document reranking and propose DiffuRank, a reranking framework built upon dLLMs. Unlike autoregressive models, dLLMs support more flexible decoding and generation processes that are not constrained to a left-to-right order, and enable parallel decoding, which may lead to improved efficiency and controllability. Specifically, we investigate three reranking strategies based on dLLMs: (1) a pointwise approach that uses dLLMs to estimate the relevance of each query-document pair; (2) a logit-based listwise approach that prompts dLLMs to jointly assess the relevance of multiple documents and derives ranking lists directly from model logits; and (3) a permutation-based listwise approach that adapts the canonical decoding process of dLLMs to the reranking tasks. For each approach, we design corresponding training methods to fully exploit the advantages of dLLMs. We evaluate both zero-shot and fine-tuned reranking performance on multiple benchmarks. Experimental results show that dLLMs achieve performance comparable to, and in some cases exceeding, that of autoregressive LLMs with similar model sizes. These findings demonstrate the promise of diffusion-based language models as a compelling alternative to autoregressive architectures for document reranking.",
      "authors": [
        "Qi Liu",
        "Kun Ai",
        "Jiaxin Mao",
        "Yanzhao Zhang",
        "Mingxin Li",
        "Dingkun Long",
        "Pengjun Xie",
        "Fengbin Zhu",
        "Ji-Rong Wen"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "published": "2026-02-13 02:18:14+00:00",
      "link": "https://arxiv.org/pdf/2602.12528v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12510v1",
      "title": "Visual RAG Toolkit: Scaling Multi-Vector Visual Retrieval with Training-Free Pooling and Multi-Stage Search",
      "abstract": "Multi-vector visual retrievers (e.g., ColPali-style late interaction models) deliver strong accuracy, but scale poorly because each page yields thousands of vectors, making indexing and search increasingly expensive. We present Visual RAG Toolkit, a practical system for scaling visual multi-vector retrieval with training-free, model-aware pooling and multi-stage retrieval. Motivated by Matryoshka Embeddings, our method performs static spatial pooling - including a lightweight sliding-window averaging variant - over patch embeddings to produce compact tile-level and global representations for fast candidate generation, followed by exact MaxSim reranking using full multi-vector embeddings.   Our design yields a quadratic reduction in vector-to-vector comparisons by reducing stored vectors per page from thousands to dozens, notably without requiring post-training, adapters, or distillation. Across experiments with interaction-style models such as ColPali and ColSmol-500M, we observe that over the limited ViDoRe v2 benchmark corpus 2-stage retrieval typically preserves NDCG and Recall @ 5/10 with minimal degradation, while substantially improving throughput (approximately 4x QPS); with sensitivity mainly at very large k. The toolkit additionally provides robust preprocessing - high resolution PDF to image conversion, optional margin/empty-region cropping and token hygiene (indexing only visual tokens) - and a reproducible evaluation pipeline, enabling rapid exploration of two-, three-, and cascaded retrieval variants. By emphasizing efficiency at common cutoffs (e.g., k <= 10), the toolkit lowers hardware barriers and makes state-of-the-art visual retrieval more accessible in practice.",
      "authors": [
        "Ara Yeroyan"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-13 01:27:39+00:00",
      "link": "https://arxiv.org/pdf/2602.12510v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12498v1",
      "title": "Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models",
      "abstract": "Negation is a fundamental linguistic operation in clinical reporting, yet vision-language models (VLMs) frequently fail to distinguish affirmative from negated medical statements. To systematically characterize this limitation, we introduce a radiology-specific diagnostic benchmark that evaluates polarity sensitivity under controlled clinical conditions, revealing that common medical VLMs consistently confuse negated and non-negated findings. To enable learning beyond simple condition absence, we further construct a contextual clinical negation dataset that encodes structured claims and supports attribute-level negations involving location and severity. Building on these resources, we propose Negation-Aware Selective Training (NAST), an interpretability-guided adaptation method that uses causal tracing effects (CTEs) to modulate layer-wise gradient updates during fine-tuning. Rather than applying uniform learning rates, NAST scales each layer's update according to its causal contribution to negation processing, transforming mechanistic interpretability signals into a principled optimization rule. Experiments demonstrate improved discrimination of affirmative and negated clinical statements without degrading general vision-language alignment, highlighting the value of causal interpretability for targeted model adaptation in safety-critical medical settings. Code and resources are available at https://github.com/healthylaife/NAST.",
      "authors": [
        "Ali Abbasi",
        "Mehdi Taghipour",
        "Rahmatollah Beheshti"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-13 00:44:26+00:00",
      "link": "https://arxiv.org/pdf/2602.12498v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12499v1",
      "title": "A Theoretical Analysis of Mamba's Training Dynamics: Filtering Relevant Features for Generalization in State Space Models",
      "abstract": "The recent empirical success of Mamba and other selective state space models (SSMs) has renewed interest in non-attention architectures for sequence modeling, yet their theoretical foundations remain underexplored. We present a first-step analysis of generalization and learning dynamics for a simplified but representative Mamba block: a single-layer, single-head selective SSM with input-dependent gating, followed by a two-layer MLP trained via gradient descent (GD). Our study adopts a structured data model with tokens that include both class-relevant and class-irrelevant patterns under token-level noise and examines two canonical regimes: majority-voting and locality-structured data sequences. We prove that the model achieves guaranteed generalization by establishing non-asymptotic sample complexity and convergence rate bounds, which improve as the effective signal increases and the noise decreases. Furthermore, we show that the gating vector aligns with class-relevant features while ignoring irrelevant ones, thereby formalizing a feature-selection role similar to attention but realized through selective recurrence. Numerical experiments on synthetic data justify our theoretical results. Overall, our results provide principled insight into when and why Mamba-style selective SSMs learn efficiently, offering a theoretical counterpoint to Transformer-centric explanations.",
      "authors": [
        "Mugunthan Shandirasegaran",
        "Hongkang Li",
        "Songyang Zhang",
        "Meng Wang",
        "Shuai Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-13 00:44:26+00:00",
      "link": "https://arxiv.org/pdf/2602.12499v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12482v1",
      "title": "Geometric separation and constructive universal approximation with two hidden layers",
      "abstract": "We give a geometric construction of neural networks that separate disjoint compact subsets of $\\Bbb R^n$, and use it to obtain a constructive universal approximation theorem. Specifically, we show that networks with two hidden layers and either a sigmoidal activation (i.e., strictly monotone bounded continuous) or the ReLU activation can approximate any real-valued continuous function on an arbitrary compact set $K\\subset\\Bbb R^n$ to any prescribed accuracy in the uniform norm. For finite $K$, the construction simplifies and yields a sharp depth-2 (single hidden layer) approximation result.",
      "authors": [
        "Chanyoung Sung"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.CA"
      ],
      "published": "2026-02-12 23:46:11+00:00",
      "link": "https://arxiv.org/pdf/2602.12482v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12471v1",
      "title": "Tight Bounds for Logistic Regression with Large Stepsize Gradient Descent in Low Dimension",
      "abstract": "We consider the optimization problem of minimizing the logistic loss with gradient descent to train a linear model for binary classification with separable data. With a budget of $T$ iterations, it was recently shown that an accelerated $1/T^2$ rate is possible by choosing a large step size $η= Θ(γ^2 T)$ (where $γ$ is the dataset's margin) despite the resulting non-monotonicity of the loss. In this paper, we provide a tighter analysis of gradient descent for this problem when the data is two-dimensional: we show that GD with a sufficiently large learning rate $η$ finds a point with loss smaller than $\\mathcal{O}(1/(ηT))$, as long as $T \\geq Ω(n/γ+ 1/γ^2)$, where $n$ is the dataset size. Our improved rate comes from a tighter bound on the time $τ$ that it takes for GD to transition from unstable (non-monotonic loss) to stable (monotonic loss), via a fine-grained analysis of the oscillatory dynamics of GD in the subspace orthogonal to the max-margin classifier. We also provide a lower bound of $τ$ matching our upper bound up to logarithmic factors, showing that our analysis is tight.",
      "authors": [
        "Michael Crawshaw",
        "Mingrui Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 22:58:18+00:00",
      "link": "https://arxiv.org/pdf/2602.12471v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12470v1",
      "title": "Designing RNAs with Language Models",
      "abstract": "RNA design, the task of finding a sequence that folds into a target secondary structure, has broad biological and biomedical impact but remains computationally challenging due to the exponentially large sequence space and exponentially many competing folds. Traditional approaches treat it as an optimization problem, relying on per-instance heuristics or constraint-based search. We instead reframe RNA design as conditional sequence generation and introduce a reusable neural approximator, instantiated as an autoregressive language model (LM), that maps target structures directly to sequences. We first train our model in a supervised setting on random-induced structure-sequence pairs, and then use reinforcement learning (RL) to optimize end-to-end metrics. We also propose methods to select a small subset for RL that greatly improves RL efficiency and quality. Across four datasets, our approach outperforms state-of-the-art systems on key metrics such as Boltzmann probability while being 1.7x faster, establishing conditional LM generation as a scalable, task-agnostic alternative to per-instance optimization for RNA design. Our code and data are available at https://github.com/KuNyaa/RNA-Design-LM.",
      "authors": [
        "Milan Gautam",
        "Ning Dai",
        "Tianshuo Zhou",
        "Bowen Xie",
        "David Mathews",
        "Liang Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 22:57:09+00:00",
      "link": "https://arxiv.org/pdf/2602.12470v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12469v1",
      "title": "Regularized Meta-Learning for Improved Generalization",
      "abstract": "Deep ensemble methods often improve predictive performance, yet they suffer from three practical limitations: redundancy among base models that inflates computational cost and degrades conditioning, unstable weighting under multicollinearity, and overfitting in meta-learning pipelines. We propose a regularized meta-learning framework that addresses these challenges through a four-stage pipeline combining redundancy-aware projection, statistical meta-feature augmentation, and cross-validated regularized meta-models (Ridge, Lasso, and ElasticNet). Our multi-metric de-duplication strategy removes near-collinear predictors using correlation and MSE thresholds ($τ_{\\text{corr}}=0.95$), reducing the effective condition number of the meta-design matrix while preserving predictive diversity. Engineered ensemble statistics and interaction terms recover higher-order structure unavailable to raw prediction columns. A final inverse-RMSE blending stage mitigates regularizer-selection variance. On the Playground Series S6E1 benchmark (100K samples, 72 base models), the proposed framework achieves an out-of-fold RMSE of 8.582, improving over simple averaging (8.894) and conventional Ridge stacking (8.627), while matching greedy hill climbing (8.603) with substantially lower runtime (4 times faster). Conditioning analysis shows a 53.7\\% reduction in effective matrix condition number after redundancy projection. Comprehensive ablations demonstrate consistent contributions from de-duplication, statistical meta-features, and meta-ensemble blending. These results position regularized meta-learning as a stable and deployment-efficient stacking strategy for high-dimensional ensemble systems.",
      "authors": [
        "Noor Islam S. Mohammad",
        "Md Muntaqim Meherab"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 22:55:32+00:00",
      "link": "https://arxiv.org/pdf/2602.12469v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12468v1",
      "title": "Continuous Diffusion Models Can Obey Formal Syntax",
      "abstract": "Diffusion language models offer a promising alternative to autoregressive models due to their global, non-causal generation process, but their continuous latent dynamics make discrete constraints -- e.g., the output should be a JSON file that matches a given schema -- difficult to impose. We introduce a training-free guidance method for steering continuous diffusion language models to satisfy formal syntactic constraints expressed using regular expressions. Our approach constructs an analytic score estimating the probability that a latent state decodes to a valid string accepted by a given regular expression, and uses its gradient to guide sampling, without training auxiliary classifiers. The denoising process targets the base model conditioned on syntactic validity.   We implement our method in Diffinity on top of the PLAID diffusion model and evaluate it on 180 regular-expression constraints over JSON and natural-language benchmarks. Diffinity achieves 68-96\\% constraint satisfaction while incurring only a small perplexity cost relative to unconstrained sampling, outperforming autoregressive constrained decoding in both constraint satisfaction and output quality.",
      "authors": [
        "Jinwoo Kim",
        "Taylor Berg-Kirkpatrick",
        "Loris D'Antoni"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.FL"
      ],
      "published": "2026-02-12 22:55:05+00:00",
      "link": "https://arxiv.org/pdf/2602.12468v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12445v1",
      "title": "RBCorr: Response Bias Correction in Language Models",
      "abstract": "Language models (LMs) are known to be prone to response biases, which present as option preference biases in fixed-response questions. It is therefore imperative to develop low-cost and effective response bias correction methods to improve LM performance and enable more accurate evaluations of model abilities. Here, we propose a simple response bias correction strategy ($\\texttt{RBCorr}$) and test it on 12 open-weight language models using yes-no, entailment, and multiple choice questions. We show that response bias is prevalent in LMs pre-correction and that $\\texttt{RBCorr}$ effectively eliminates bias and boosts model performance. We also explore the generalizability of bias behavior across models, datasets, and prompt formats, showing that LogProbs-based correction is highly dependent on all three of these aspects. Overall, $\\texttt{RBCorr}$ is an easy-to-use method that can boost the performance of smaller LMs and ensure that LM performance on closed-response benchmarks aligns more closely with their true capabilities.",
      "authors": [
        "Om Bhatt",
        "Anna A. Ivanova"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-12 22:05:04+00:00",
      "link": "https://arxiv.org/pdf/2602.12445v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12443v1",
      "title": "SHAPR: A Solo Human-Centred and AI-Assisted Practice Framework for Research Software Development",
      "abstract": "Research software has become a central vehicle for inquiry and learning in many Higher Degree Research (HDR) contexts, where solo researchers increasingly develop software-based artefacts as part of their research methodology. At the same time, generative artificial intelligence is reshaping development practice, offering powerful forms of assistance while introducing new challenges for accountability, reflection, and methodological rigour. Although Action Design Research (ADR) provides a well-established foundation for studying and constructing socio-technical artefacts, it offers limited guidance on how its principles can be operationalised in the day-to-day practice of solo, AI-assisted research software development. This paper proposes the SHAPR framework (Solo, Human-centred, AI-assisted PRactice) as a practice-level operational framework that complements ADR by translating its high-level principles into actionable guidance for contemporary research contexts. SHAPR supports the enactment of ADR Building-Intervention-Evaluation cycles by making explicit the roles, artefacts, reflective practices, and lightweight governance mechanisms required to sustain human accountability and learning in AI-assisted development. The contribution of the paper is conceptual: SHAPR itself is treated as the primary design artefact and unit of analysis and is evaluated formatively through reflective analysis of its internal coherence, alignment with ADR principles, and applicability to solo research practice. By explicitly linking research software development, Human-AI collaboration, and reflective learning, this study contributes to broader discussions on how SHAPR can support both knowledge production and HDR researcher training.",
      "authors": [
        "Ka Ching Chan"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.HC"
      ],
      "published": "2026-02-12 22:02:39+00:00",
      "link": "https://arxiv.org/pdf/2602.12443v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12429v1",
      "title": "Stabilizing Native Low-Rank LLM Pretraining",
      "abstract": "Foundation models have achieved remarkable success, yet their growing parameter counts pose significant computational and memory challenges. Low-rank factorization offers a promising route to reduce training and inference costs, but the community lacks a stable recipe for training models from scratch using exclusively low-rank weights while matching the performance of the dense model. We demonstrate that Large Language Models (LLMs) can be trained from scratch using exclusively low-rank factorized weights for all non-embedding matrices without auxiliary \"full-rank\" guidance required by prior methods. While native low-rank training often suffers from instability and loss spikes, we identify uncontrolled growth in the spectral norm (largest singular value) of the weight matrix update as the dominant factor. To address this, we introduce Spectron: Spectral renormalization with orthogonalization, which dynamically bounds the resultant weight updates based on the current spectral norms of the factors. Our method enables stable, end-to-end factorized training with negligible overhead. Finally, we establish compute-optimal scaling laws for natively low-rank transformers, demonstrating predictable power-law behavior and improved inference efficiency relative to dense models.",
      "authors": [
        "Paul Janson",
        "Edouard Oyallon",
        "Eugene Belilovsky"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 21:33:14+00:00",
      "link": "https://arxiv.org/pdf/2602.12429v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13348v1",
      "title": "Exploring the Performance of ML/DL Architectures on the MNIST-1D Dataset",
      "abstract": "Small datasets like MNIST have historically been instrumental in advancing machine learning research by providing a controlled environment for rapid experimentation and model evaluation. However, their simplicity often limits their utility for distinguishing between advanced neural network architectures. To address these challenges, Greydanus et al. introduced the MNIST-1D dataset, a one-dimensional adaptation of MNIST designed to explore inductive biases in sequential data. This dataset maintains the advantages of small-scale datasets while introducing variability and complexity that make it ideal for studying advanced architectures.   In this paper, we extend the exploration of MNIST-1D by evaluating the performance of Residual Networks (ResNet), Temporal Convolutional Networks (TCN), and Dilated Convolutional Neural Networks (DCNN). These models, known for their ability to capture sequential patterns and hierarchical features, were implemented and benchmarked alongside previously tested architectures such as logistic regression, MLPs, CNNs, and GRUs. Our experimental results demonstrate that advanced architectures like TCN and DCNN consistently outperform simpler models, achieving near-human performance on MNIST-1D. ResNet also shows significant improvements, highlighting the importance of leveraging inductive biases and hierarchical feature extraction in small structured datasets.   Through this study, we validate the utility of MNIST-1D as a robust benchmark for evaluating machine learning architectures under computational constraints. Our findings emphasize the role of architectural innovations in improving model performance and offer insights into optimizing deep learning models for resource-limited environments.",
      "authors": [
        "Michael Beebe",
        "GodsGift Uzor",
        "Manasa Chepuri",
        "Divya Sree Vemula",
        "Angel Ayala"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-12 21:31:20+00:00",
      "link": "https://arxiv.org/pdf/2602.13348v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12424v1",
      "title": "RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty",
      "abstract": "Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability to effectively distinguish models' capabilities. To address this limitation, we propose RankLLM, a novel framework designed to quantify both question difficulty and model competency. RankLLM introduces difficulty as the primary criterion for differentiation, enabling a more fine-grained evaluation of LLM capabilities. RankLLM's core mechanism facilitates bidirectional score propagation between models and questions. The core intuition of RankLLM is that a model earns a competency score when it correctly answers a question, while a question's difficulty score increases when it challenges a model. Using this framework, we evaluate 30 models on 35,550 questions across multiple domains. RankLLM achieves 90% agreement with human judgments and consistently outperforms strong baselines such as IRT. It also exhibits strong stability, fast convergence, and high computational efficiency, making it a practical solution for large-scale, difficulty-aware LLM evaluation.",
      "authors": [
        "Ziqian Zhang",
        "Xingjian Hu",
        "Yue Huang",
        "Kai Zhang",
        "Ruoxi Chen",
        "Yixin Liu",
        "Qingsong Wen",
        "Kaidi Xu",
        "Xiangliang Zhang",
        "Neil Zhenqiang Gong",
        "Lichao Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-12 21:28:46+00:00",
      "link": "https://arxiv.org/pdf/2602.12424v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12414v1",
      "title": "propella-1: Multi-Property Document Annotation for LLM Data Curation at Scale",
      "abstract": "Since FineWeb-Edu, data curation for LLM pretraining has predominantly relied on single scalar quality scores produced by small classifiers. A single score conflates multiple quality dimensions, prevents flexible filtering, and offers no interpretability. We introduce propella-1, a family of small multilingual LLMs (0.6B, 1.7B, 4B parameters) that annotate text documents across 18 properties organized into six categories: core content, classification, quality and value, audience and purpose, safety and compliance, and geographic relevance. The models support 57 languages and produce structured JSON annotations conforming to a predefined schema. Evaluated against a frontier commercial LLM as a reference annotator, the 4B model achieves higher agreement than much larger general-purpose models. We release propella-annotations, a dataset of over three billion document annotations covering major pretraining corpora including data from FineWeb-2, FinePDFs, HPLT 3.0, and Nemotron-CC. Using these annotations, we present a multi-dimensional compositional analysis of widely used pretraining datasets, revealing substantial differences in quality, reasoning depth, and content composition that single-score approaches cannot capture. All model weights and annotations are released under permissive, commercial-use licenses.",
      "authors": [
        "Maximilian Idahl",
        "Benedikt Droste",
        "Björn Plüster",
        "Jan Philipp Harries"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 21:13:08+00:00",
      "link": "https://arxiv.org/pdf/2602.12414v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12413v1",
      "title": "Soft Contamination Means Benchmarks Test Shallow Generalization",
      "abstract": "If LLM training data is polluted with benchmark test data, then benchmark performance gives biased estimates of out-of-distribution (OOD) generalization. Typical decontamination filters use n-gram matching which fail to detect semantic duplicates: sentences with equivalent (or near-equivalent) content that are not close in string space. We study this soft contamination of training data by semantic duplicates. Among other experiments, we embed the Olmo3 training corpus and find that: 1) contamination remains widespread, e.g. we find semantic duplicates for 78% of CodeForces and exact duplicates for 50% of ZebraLogic problems; 2) including semantic duplicates of benchmark data in training does improve benchmark performance; and 3) when finetuning on duplicates of benchmark datapoints, performance also improves on truly-held-out datapoints from the same benchmark. We argue that recent benchmark gains are thus confounded: the prevalence of soft contamination means gains reflect both genuine capability improvements and the accumulation of test data and effective test data in growing training corpora.",
      "authors": [
        "Ari Spiesberger",
        "Juan J. Vazquez",
        "Nicky Pochinkov",
        "Tomáš Gavenčiak",
        "Peli Grietzer",
        "Gavin Leech",
        "Nandi Schoots"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 21:12:53+00:00",
      "link": "https://arxiv.org/pdf/2602.12413v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12410v1",
      "title": "Conference Proceedings of the Inaugural Conference of the International Society for Tractography (IST 2025 Bordeaux)",
      "abstract": "This collection comprises the abstracts presented during poster, power pitch and oral sessions at the Inaugural Conference of the International Society for Tractography (IST Conference 2025), held in Bordeaux, France, from October 13-16, 2025. The conference was designed to foster meaningful exchange and collaboration between disparate fields. The overall focus was on advancing research, innovation, and community in the common fields of interest: neuroanatomy, tractography methods and scientific/clinical applications of tractography. The included abstracts cover the latest advancements in tractography, Diffusion MRI, and related fields including new work on; neurological and psychiatric disorders, deep brain stimulation targeting, and brain development. This landmark event brought together world-leading experts to discuss critical challenges and chart the future direction of the field.",
      "authors": [
        "Flavio Dell Acqua",
        "Maxime Descoteaux",
        "Graham Little",
        "Laurent Petit",
        "Dogu Baran Aydogan",
        "Stephanie Forkel",
        "Alexander Leemans",
        "Simona Schiavi",
        "Michel Thiebaut de Schotten"
      ],
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.NC"
      ],
      "published": "2026-02-12 21:07:41+00:00",
      "link": "https://arxiv.org/pdf/2602.12410v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12403v1",
      "title": "MonoLoss: A Training Objective for Interpretable Monosemantic Representations",
      "abstract": "Sparse autoencoders (SAEs) decompose polysemantic neural representations, where neurons respond to multiple unrelated concepts, into monosemantic features that capture single, interpretable concepts. However, standard training objectives only weakly encourage this decomposition, and existing monosemanticity metrics require pairwise comparisons across all dataset samples, making them inefficient during training and evaluation. We study a recent MonoScore metric and derive a single-pass algorithm that computes exactly the same quantity, but with a cost that grows linearly, rather than quadratically, with the number of dataset images. On OpenImagesV7, we achieve up to a 1200x speedup wall-clock speedup in evaluation and 159x during training, while adding only ~4% per-epoch overhead. This allows us to treat MonoScore as a training signal: we introduce the Monosemanticity Loss (MonoLoss), a plug-in objective that directly rewards semantically consistent activations for learning interpretable monosemantic representations. Across SAEs trained on CLIP, SigLIP2, and pretrained ViT features, using BatchTopK, TopK, and JumpReLU SAEs, MonoLoss increases MonoScore for most latents. MonoLoss also consistently improves class purity (the fraction of a latent's activating images belonging to its dominant class) across all encoder and SAE combinations, with the largest gain raising baseline purity from 0.152 to 0.723. Used as an auxiliary regularizer during ResNet-50 and CLIP-ViT-B/32 finetuning, MonoLoss yields up to 0.6\\% accuracy gains on ImageNet-1K and monosemantic activating patterns on standard benchmark datasets. The code is publicly available at https://github.com/AtlasAnalyticsLab/MonoLoss.",
      "authors": [
        "Ali Nasiri-Sarvi",
        "Anh Tien Nguyen",
        "Hassan Rivaz",
        "Dimitris Samaras",
        "Mahdi S. Hosseini"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-12 20:53:41+00:00",
      "link": "https://arxiv.org/pdf/2602.12403v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12391v1",
      "title": "High-dimensional Level Set Estimation with Trust Regions and Double Acquisition Functions",
      "abstract": "Level set estimation (LSE) classifies whether an unknown function's value exceeds a specified threshold for given inputs, a fundamental problem in many real-world applications. In active learning settings with limited initial data, we aim to iteratively acquire informative points to construct an accurate classifier for this task. In high-dimensional spaces, this becomes challenging where the search volume grows exponentially with increasing dimensionality. We propose TRLSE, an algorithm for high-dimensional LSE, which identifies and refines regions near the threshold boundary with dual acquisition functions operating at both global and local levels. We provide a theoretical analysis of TRLSE's accuracy and show its superior sample efficiency against existing methods through extensive evaluations on multiple synthetic and real-world LSE problems.",
      "authors": [
        "Giang Ngo",
        "Dat Phan Trong",
        "Dang Nguyen",
        "Sunil Gupta"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 20:36:04+00:00",
      "link": "https://arxiv.org/pdf/2602.12391v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12390v1",
      "title": "Rational Neural Networks have Expressivity Advantages",
      "abstract": "We study neural networks with trainable low-degree rational activation functions and show that they are more expressive and parameter-efficient than modern piecewise-linear and smooth activations such as ELU, LeakyReLU, LogSigmoid, PReLU, ReLU, SELU, CELU, Sigmoid, SiLU, Mish, Softplus, Tanh, Softmin, Softmax, and LogSoftmax. For an error target of $\\varepsilon>0$, we establish approximation-theoretic separations: Any network built from standard fixed activations can be uniformly approximated on compact domains by a rational-activation network with only $\\mathrm{poly}(\\log\\log(1/\\varepsilon))$ overhead in size, while the converse provably requires $Ω(\\log(1/\\varepsilon))$ parameters in the worst case. This exponential gap persists at the level of full networks and extends to gated activations and transformer-style nonlinearities. In practice, rational activations integrate seamlessly into standard architectures and training pipelines, allowing rationals to match or outperform fixed activations under identical architectures and optimizers.",
      "authors": [
        "Maosen Tang",
        "Alex Townsend"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "published": "2026-02-12 20:33:42+00:00",
      "link": "https://arxiv.org/pdf/2602.12390v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12389v1",
      "title": "Evolving Beyond Snapshots: Harmonizing Structure and Sequence via Entity State Tuning for Temporal Knowledge Graph Forecasting",
      "abstract": "Temporal knowledge graph (TKG) forecasting requires predicting future facts by jointly modeling structural dependencies within each snapshot and temporal evolution across snapshots. However, most existing methods are stateless: they recompute entity representations at each timestamp from a limited query window, leading to episodic amnesia and rapid decay of long-term dependencies. To address this limitation, we propose Entity State Tuning (EST), an encoder-agnostic framework that endows TKG forecasters with persistent and continuously evolving entity states. EST maintains a global state buffer and progressively aligns structural evidence with sequential signals via a closed-loop design. Specifically, a topology-aware state perceiver first injects entity-state priors into structural encoding. Then, a unified temporal context module aggregates the state-enhanced events with a pluggable sequence backbone. Subsequently, a dual-track evolution mechanism writes the updated context back to the global entity state memory, balancing plasticity against stability. Experiments on multiple benchmarks show that EST consistently improves diverse backbones and achieves state-of-the-art performance, highlighting the importance of state persistence for long-horizon TKG forecasting. The code is published at https://github.com/yuanwuyuan9/Evolving-Beyond-Snapshots",
      "authors": [
        "Siyuan Li",
        "Yunjia Wu",
        "Yiyong Xiao",
        "Pingyang Huang",
        "Peize Li",
        "Ruitong Liu",
        "Yan Wen",
        "Te Sun",
        "Fangyi Pei"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-12 20:33:35+00:00",
      "link": "https://arxiv.org/pdf/2602.12389v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12384v2",
      "title": "Why Deep Jacobian Spectra Separate: Depth-Induced Scaling and Singular-Vector Alignment",
      "abstract": "Understanding why gradient-based training in deep networks exhibits strong implicit bias remains challenging, in part because tractable singular-value dynamics are typically available only for balanced deep linear models. We propose an alternative route based on two theoretically grounded and empirically testable signatures of deep Jacobians: depth-induced exponential scaling of ordered singular values and strong spectral separation. Adopting a fixed-gates view of piecewise-linear networks, where Jacobians reduce to products of masked linear maps within a single activation region, we prove the existence of Lyapunov exponents governing the top singular values at initialization, give closed-form expressions in a tractable masked model, and quantify finite-depth corrections. We further show that sufficiently strong separation forces singular-vector alignment in matrix products, yielding an approximately shared singular basis for intermediate Jacobians. Together, these results motivate an approximation regime in which singular-value dynamics become effectively decoupled, mirroring classical balanced deep-linear analyses without requiring balancing. Experiments in fixed-gates settings validate the predicted scaling, alignment, and resulting dynamics, supporting a mechanistic account of emergent low-rank Jacobian structure as a driver of implicit bias.",
      "authors": [
        "Nathanaël Haas",
        "François Gatine",
        "Augustin M Cosse",
        "Zied Bouraoui"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 20:27:59+00:00",
      "link": "https://arxiv.org/pdf/2602.12384v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12379v1",
      "title": "Deep Doubly Debiased Longitudinal Effect Estimation with ICE G-Computation",
      "abstract": "Estimating longitudinal treatment effects is essential for sequential decision-making but is challenging due to treatment-confounder feedback. While Iterative Conditional Expectation (ICE) G-computation offers a principled approach, its recursive structure suffers from error propagation, corrupting the learned outcome regression models. We propose D3-Net, a framework that mitigates error propagation in ICE training and then applies a robust final correction. First, to interrupt error propagation during learning, we train the ICE sequence using Sequential Doubly Robust (SDR) pseudo-outcomes, which provide bias-corrected targets for each regression. Second, we employ a multi-task Transformer with a covariate simulator head for auxiliary supervision, regularizing representations against corruption by noisy pseudo-outcomes, and a target network to stabilize training dynamics. For the final estimate, we discard the SDR correction and instead use the uncorrected nuisance models to perform Longitudinal Targeted Minimum Loss-Based Estimation (LTMLE) on the original outcomes. This second-stage, targeted debiasing ensures robustness and optimal finite-sample properties. Comprehensive experiments demonstrate that our model, D3-Net, robustly reduces bias and variance across different horizons, counterfactuals, and time-varying confoundings, compared to existing state-of-the-art ICE-based estimators.",
      "authors": [
        "Wenxin Chen",
        "Weishen Pan",
        "Kyra Gan",
        "Fei Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 20:16:27+00:00",
      "link": "https://arxiv.org/pdf/2602.12379v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12368v1",
      "title": "A Machine Learning Approach to the Nirenberg Problem",
      "abstract": "This work introduces the Nirenberg Neural Network: a numerical approach to the Nirenberg problem of prescribing Gaussian curvature on $S^2$ for metrics that are pointwise conformal to the round metric. Our mesh-free physics-informed neural network (PINN) approach directly parametrises the conformal factor globally and is trained with a geometry-aware loss enforcing the curvature equation. Additional consistency checks were performed via the Gauss-Bonnet theorem, and spherical-harmonic expansions were fit to the learnt models to provide interpretability.   For prescribed curvatures with known realisability, the neural network achieves very low losses ($10^{-7} - 10^{-10}$), while unrealisable curvatures yield significantly higher losses. This distinction enables the assessment of unknown cases, separating likely realisable functions from non-realisable ones. The current capabilities of the Nirenberg Neural Network demonstrate that neural solvers can serve as exploratory tools in geometric analysis, offering a quantitative computational perspective on longstanding existence questions.",
      "authors": [
        "Gianfranco Cortés",
        "Maria Esteban-Casadevall",
        "Yueqing Feng",
        "Jonas Henkel",
        "Edward Hirst",
        "Tancredi Schettini Gherardini",
        "Alexander G. Stapleton"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "hep-th",
        "math.AP",
        "math.DG"
      ],
      "published": "2026-02-12 19:58:11+00:00",
      "link": "https://arxiv.org/pdf/2602.12368v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13345v1",
      "title": "BLUEPRINT Rebuilding a Legacy: Multimodal Retrieval for Complex Engineering Drawings and Documents",
      "abstract": "Decades of engineering drawings and technical records remain locked in legacy archives with inconsistent or missing metadata, making retrieval difficult and often manual. We present Blueprint, a layout-aware multimodal retrieval system designed for large-scale engineering repositories. Blueprint detects canonical drawing regions, applies region-restricted VLM-based OCR, normalizes identifiers (e.g., DWG, part, facility), and fuses lexical and dense retrieval with a lightweight region-level reranker. Deployed on ~770k unlabeled files, it automatically produces structured metadata suitable for cross-facility search.   We evaluate Blueprint on a 5k-file benchmark with 350 expert-curated queries using pooled, graded (0/1/2) relevance judgments. Blueprint delivers a 10.1% absolute gain in Success@3 and an 18.9% relative improvement in nDCG@3 over the strongest vision-language baseline}, consistently outperforming across vision, text, and multimodal intents. Oracle ablations reveal substantial headroom under perfect region detection and OCR. We release all queries, runs, annotations, and code to facilitate reproducible evaluation on legacy engineering archives.",
      "authors": [
        "Ethan Seefried",
        "Ran Eldegaway",
        "Sanjay Das",
        "Nathaniel Blanchard",
        "Tirthankar Ghosal"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IR",
        "cs.MA"
      ],
      "published": "2026-02-12 19:48:44+00:00",
      "link": "https://arxiv.org/pdf/2602.13345v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12360v1",
      "title": "Predicting Dynamic Map States from Limited Field-of-View Sensor Data",
      "abstract": "When autonomous systems are deployed in real-world scenarios, sensors are often subject to limited field-of-view (FOV) constraints, either naturally through system design, or through unexpected occlusions or sensor failures. In conditions where a large FOV is unavailable, it is important to be able to infer information about the environment and predict the state of nearby surroundings based on available data to maintain safe and accurate operation. In this work, we explore the effectiveness of deep learning for dynamic map state prediction based on limited FOV time series data. We show that by representing dynamic sensor data in a simple single-image format that captures both spatial and temporal information, we can effectively use a wide variety of existing image-to-image learning models to predict map states with high accuracy in a diverse set of sensing scenarios.",
      "authors": [
        "Knut Peterson",
        "David Han"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-12 19:36:49+00:00",
      "link": "https://arxiv.org/pdf/2602.12360v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12356v1",
      "title": "A Theoretical Framework for Adaptive Utility-Weighted Benchmarking",
      "abstract": "Benchmarking has long served as a foundational practice in machine learning and, increasingly, in modern AI systems such as large language models, where shared tasks, metrics, and leaderboards offer a common basis for measuring progress and comparing approaches. As AI systems are deployed in more varied and consequential settings, though, there is growing value in complementing these established practices with a more holistic conceptualization of what evaluation should represent. Of note, recognizing the sociotechnical contexts in which these systems operate invites an opportunity for a deeper view of how multiple stakeholders and their unique priorities might inform what we consider meaningful or desirable model behavior. This paper introduces a theoretical framework that reconceptualizes benchmarking as a multilayer, adaptive network linking evaluation metrics, model components, and stakeholder groups through weighted interactions. Using conjoint-derived utilities and a human-in-the-loop update rule, we formalize how human tradeoffs can be embedded into benchmark structure and how benchmarks can evolve dynamically while preserving stability and interpretability. The resulting formulation generalizes classical leaderboards as a special case and provides a foundation for building evaluation protocols that are more context aware, resulting in new robust tools for analyzing the structural properties of benchmarks, which opens a path toward more accountable and human-aligned evaluation.",
      "authors": [
        "Philip Waggoner"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 19:33:47+00:00",
      "link": "https://arxiv.org/pdf/2602.12356v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12350v1",
      "title": "Completeness in the Polynomial Hierarchy and PSPACE for many natural problems derived from NP",
      "abstract": "Many natural optimization problems derived from $\\sf NP$ admit bilevel and multilevel extensions in which decisions are made sequentially by multiple players with conflicting objectives, as in interdiction, adversarial selection, and adjustable robust optimization. Such problems are naturally modeled by alternating quantifiers and, therefore, lie beyond $\\sf NP$, typically in the polynomial hierarchy or $\\sf PSPACE$. Despite extensive study of these problem classes, relatively few natural completeness results are known at these higher levels. We introduce a general framework for proving completeness in the polynomial hierarchy and $\\sf PSPACE$ for problems derived from $\\sf NP$. Our approach is based on a refinement of $\\sf NP$, which we call $\\sf NP$ with solutions ($\\sf NP$-$\\sf S$), in which solutions are explicit combinatorial objects, together with a restricted class of reductions -- solution-embedding reductions -- that preserve solution structure. We define $\\sf NP$-$\\sf S$-completeness and show that a large collection of classical $\\sf NP$-complete problems, including Clique, Vertex Cover, Knapsack, and Traveling Salesman, are $\\sf NP$-$\\sf S$-complete.   Using this framework, we establish general meta-theorems showing that if a problem is $\\sf NP$-$\\sf S$-complete, then its natural two-level extensions are $Σ_2^p$-complete, its three-level extensions are $Σ_3^p$-complete, and its $k$-level extensions are $Σ_k^p$-complete. When the number of levels is unbounded, the resulting problems are $\\sf PSPACE$-complete. Our results subsume nearly all previously known completeness results for multilevel optimization problems derived from $\\sf NP$ and yield many new ones simultaneously, demonstrating that high computational complexity is a generic feature of multilevel extensions of $\\sf NP$-complete problems.",
      "authors": [
        "Christoph Grüne",
        "Berit Johannes",
        "James B. Orlin",
        "Lasse Wulf"
      ],
      "primary_category": "cs.CC",
      "categories": [
        "cs.CC",
        "math.CO"
      ],
      "published": "2026-02-12 19:17:33+00:00",
      "link": "https://arxiv.org/pdf/2602.12350v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12349v1",
      "title": "Variational Green's Functions for Volumetric PDEs",
      "abstract": "Green's functions characterize the fundamental solutions of partial differential equations; they are essential for tasks ranging from shape analysis to physical simulation, yet they remain computationally prohibitive to evaluate on arbitrary geometric discretizations. We present Variational Green's Function (VGF), a method that learns a smooth, differentiable representation of the Green's function for linear self-adjoint PDE operators, including the Poisson, the screened Poisson, and the biharmonic equations. To resolve the sharp singularities characteristic of the Green's functions, our method decomposes the Green's function into an analytic free-space component, and a learned corrector component. Our method leverages a variational foundation to impose Neumann boundary conditions naturally, and imposes Dirichlet boundary conditions via a projective layer on the output of the neural field. The resulting Green's functions are fast to evaluate, differentiable with respect to source application, and can be conditioned on other signals parameterizing our geometry.",
      "authors": [
        "Joao Teixeira",
        "Eitan Grinspun",
        "Otman Benchekroun"
      ],
      "primary_category": "cs.GR",
      "categories": [
        "cs.GR",
        "cs.LG"
      ],
      "published": "2026-02-12 19:12:44+00:00",
      "link": "https://arxiv.org/pdf/2602.12349v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12342v1",
      "title": "Intrinsic Credit Assignment for Long Horizon Interaction",
      "abstract": "How can we train agents to navigate uncertainty over long horizons? In this work, we propose ΔBelief-RL, which leverages a language model's own intrinsic beliefs to reward intermediate progress. Our method utilizes the change in the probability an agent assigns to the target solution for credit assignment. By training on synthetic interaction data, ΔBelief-RL teaches information-seeking capabilities that consistently outperform purely outcome-based rewards for Reinforcement Learning, with improvements generalizing to out-of-distribution applications ranging from customer service to personalization. Notably, the performance continues to improve as we scale test-time interactions beyond the training horizon, with interaction-efficiency increasing even on Pass@k metrics. Overall, our work introduces a scalable training strategy for navigating uncertainty over a long-horizon, by enabling credit assignment to intermediate actions via intrinsic ΔBelief rewards.",
      "authors": [
        "Ilze Amanda Auzina",
        "Joschka Strüber",
        "Sergio Hernández-Gutiérrez",
        "Shashwat Goel",
        "Ameya Prabhu",
        "Matthias Bethge"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 19:00:54+00:00",
      "link": "https://arxiv.org/pdf/2602.12342v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12278v1",
      "title": "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers",
      "abstract": "Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.",
      "authors": [
        "David Jiahao Fu",
        "Lam Thanh Do",
        "Jiayu Li",
        "Kevin Chen-Chuan Chang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-12 18:59:35+00:00",
      "link": "https://arxiv.org/pdf/2602.12278v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12276v1",
      "title": "Agentic Test-Time Scaling for WebAgents",
      "abstract": "Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.",
      "authors": [
        "Nicholas Lee",
        "Lutfi Eren Erdogan",
        "Chris Joseph John",
        "Surya Krishnapillai",
        "Michael W. Mahoney",
        "Kurt Keutzer",
        "Amir Gholami"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-12 18:58:30+00:00",
      "link": "https://arxiv.org/pdf/2602.12276v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12275v1",
      "title": "On-Policy Context Distillation for Language Models",
      "abstract": "Context distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. We demonstrate the effectiveness of OPCD on two important applications: experiential knowledge distillation, where models extract and consolidate transferable knowledge from their historical solution traces, and system prompt distillation, where models internalize beneficial behaviors encoded in optimized prompts. Across mathematical reasoning, text-based games, and domain-specific tasks, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities. We further show that OPCD enables effective cross-size distillation, where smaller student models can internalize experiential knowledge from larger teachers.",
      "authors": [
        "Tianzhu Ye",
        "Li Dong",
        "Xun Wu",
        "Shaohan Huang",
        "Furu Wei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 18:58:28+00:00",
      "link": "https://arxiv.org/pdf/2602.12275v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12273v1",
      "title": "Learning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs",
      "abstract": "We propose an optimization-informed deep neural network approach, named iUzawa-Net, aiming for the first solver that enables real-time solutions for a class of nonsmooth optimal control problems of linear partial differential equations (PDEs). The iUzawa-Net unrolls an inexact Uzawa method for saddle point problems, replacing classical preconditioners and PDE solvers with specifically designed learnable neural networks. We prove universal approximation properties and establish the asymptotic $\\varepsilon$-optimality for the iUzawa-Net, and validate its promising numerical efficiency through nonsmooth elliptic and parabolic optimal control problems. Our techniques offer a versatile framework for designing and analyzing various optimization-informed deep learning approaches to optimal control and other PDE-constrained optimization problems. The proposed learning-to-control approach synergizes model-based optimization algorithms and data-driven deep learning techniques, inheriting the merits of both methodologies.",
      "authors": [
        "Yongcun Song",
        "Xiaoming Yuan",
        "Hangrui Yue",
        "Tianyou Zeng"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-02-12 18:57:43+00:00",
      "link": "https://arxiv.org/pdf/2602.12273v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12267v1",
      "title": "Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data",
      "abstract": "Self-supervised learning (SSL) is a powerful paradigm for learning from unlabeled time-series data. However, popular methods such as masked autoencoders (MAEs) rely on reconstructing inputs from a fixed, predetermined masking ratio. Instead of this static design, we propose treating the corruption level as a new degree of freedom for representation learning, enhancing flexibility and performance. To achieve this, we introduce the Flow-Guided Neural Operator (FGNO), a novel framework combining operator learning with flow matching for SSL training. FGNO learns mappings in functional spaces by using Short-Time Fourier Transform to unify different time resolutions. We extract a rich hierarchy of features by tapping into different network layers and flow times that apply varying strengths of noise to the input data. This enables the extraction of versatile representations, from low-level patterns to high-level global features, using a single model adaptable to specific tasks. Unlike prior generative SSL methods that use noisy inputs during inference, we propose using clean inputs for representation extraction while learning representations with noise; this eliminates randomness and boosts accuracy. We evaluate FGNO across three biomedical domains, where it consistently outperforms established baselines. Our method yields up to 35% AUROC gains in neural signal decoding (BrainTreeBank), 16% RMSE reductions in skin temperature prediction (DREAMT), and over 20% improvement in accuracy and macro-F1 on SleepEDF under low-data regimes. These results highlight FGNO's robustness to data scarcity and its superior capacity to learn expressive representations for diverse time series.",
      "authors": [
        "Duy Nguyen",
        "Jiachen Yao",
        "Jiayun Wang",
        "Julius Berner",
        "Animashree Anandkumar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 18:54:57+00:00",
      "link": "https://arxiv.org/pdf/2602.12267v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12262v2",
      "title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization",
      "abstract": "Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.",
      "authors": [
        "Tunyu Zhang",
        "Xinxi Zhang",
        "Ligong Han",
        "Haizhou Shi",
        "Xiaoxiao He",
        "Zhuowei Li",
        "Hao Wang",
        "Kai Xu",
        "Akash Srivastava",
        "Hao Wang",
        "Vladimir Pavlovic",
        "Dimitris N. Metaxas"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-12 18:52:35+00:00",
      "link": "https://arxiv.org/pdf/2602.12262v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12259v1",
      "title": "Think like a Scientist: Physics-guided LLM Agent for Equation Discovery",
      "abstract": "Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.",
      "authors": [
        "Jianke Yang",
        "Ohm Venkatachalam",
        "Mohammad Kianezhad",
        "Sharvaree Vadgama",
        "Rose Yu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-12 18:49:27+00:00",
      "link": "https://arxiv.org/pdf/2602.12259v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12251v1",
      "title": "A technical curriculum on language-oriented artificial intelligence in translation and specialised communication",
      "abstract": "This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.",
      "authors": [
        "Ralph Krüger"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "published": "2026-02-12 18:37:23+00:00",
      "link": "https://arxiv.org/pdf/2602.12251v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12247v2",
      "title": "ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction",
      "abstract": "Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.",
      "authors": [
        "Nick Ferguson",
        "Josh Pennington",
        "Narek Beghian",
        "Aravind Mohan",
        "Douwe Kiela",
        "Sheshansh Agrawal",
        "Thien Hang Nguyen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 18:31:37+00:00",
      "link": "https://arxiv.org/pdf/2602.12247v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12245v1",
      "title": "Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces",
      "abstract": "Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.",
      "authors": [
        "Anthony Kobanda",
        "Waris Radji"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 18:30:27+00:00",
      "link": "https://arxiv.org/pdf/2602.12245v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12243v1",
      "title": "Federated Gaussian Process Learning via Pseudo-Representations for Large-Scale Multi-Robot Systems",
      "abstract": "Multi-robot systems require scalable and federated methods to model complex environments under computational and communication constraints. Gaussian Processes (GPs) offer robust probabilistic modeling, but suffer from cubic computational complexity, limiting their applicability in large-scale deployments. To address this challenge, we introduce the pxpGP, a novel distributed GP framework tailored for both centralized and decentralized large-scale multi-robot networks. Our approach leverages sparse variational inference to generate a local compact pseudo-representation. We introduce a sparse variational optimization scheme that bounds local pseudo-datasets and formulate a global scaled proximal-inexact consensus alternating direction method of multipliers (ADMM) with adaptive parameter updates and warm-start initialization. Experiments on synthetic and real-world datasets demonstrate that pxpGP and its decentralized variant, dec-pxpGP, outperform existing distributed GP methods in hyperparameter estimation and prediction accuracy, particularly in large-scale networks.",
      "authors": [
        "Sanket A. Salunkhe",
        "George P. Kontoudis"
      ],
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA"
      ],
      "published": "2026-02-12 18:28:27+00:00",
      "link": "https://arxiv.org/pdf/2602.12243v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12237v1",
      "title": "Olmix: A Framework for Data Mixing Throughout LM Development",
      "abstract": "Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.",
      "authors": [
        "Mayee F. Chen",
        "Tyler Murray",
        "David Heineman",
        "Matt Jordan",
        "Hannaneh Hajishirzi",
        "Christopher Ré",
        "Luca Soldaini",
        "Kyle Lo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-12 18:16:05+00:00",
      "link": "https://arxiv.org/pdf/2602.12237v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12236v1",
      "title": "Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision",
      "abstract": "Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47\\%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.",
      "authors": [
        "Anika Tabassum Meem",
        "Muntasir Hossain Nadid",
        "Md Zesun Ahmed Mia"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-12 18:15:32+00:00",
      "link": "https://arxiv.org/pdf/2602.12236v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12235v2",
      "title": "Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation",
      "abstract": "Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define token overflow as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.",
      "authors": [
        "Julia Belikova",
        "Danila Rozhevskii",
        "Dennis Svirin",
        "Konstantin Polev",
        "Alexander Panchenko"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 18:15:08+00:00",
      "link": "https://arxiv.org/pdf/2602.12235v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12318v1",
      "title": "Abstractive Red-Teaming of Language Model Character",
      "abstract": "We want language model assistants to conform to a character specification, which asserts how the model should act across diverse user interactions. While models typically follow these character specifications, they can occasionally violate them in large-scale deployments. In this work, we aim to identify types of queries that are likely to produce such character violations at deployment, using much less than deployment-level compute. To do this, we introduce abstractive red-teaming, where we search for natural-language query categories, e.g. \"The query is in Chinese. The query asks about family roles,\" that routinely elicit violations. These categories abstract over the many possible variants of a query which could appear in the wild. We introduce two algorithms for efficient category search against a character-trait-specific reward model: one based on reinforcement learning on a category generator LLM, and another which leverages a strong LLM to iteratively synthesize categories from high-scoring queries. Across a 12-principle character specification and 7 target models, we find that our algorithms consistently outperform baselines, and generate qualitatively interesting categories; for example, queries which ask Llama-3.1-8B-Instruct to predict the future lead to responses saying that AI will dominate humanity, and queries that ask GPT-4.1-Mini for essential prison survival items lead to enthusiastic recommendation of illegal weapons. Overall, we believe our results represent an important step towards realistic pre-deployment auditing of language model character.",
      "authors": [
        "Nate Rahn",
        "Allison Qi",
        "Avery Griffin",
        "Jonathan Michala",
        "Henry Sleight",
        "Erik Jones"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 18:12:12+00:00",
      "link": "https://arxiv.org/pdf/2602.12318v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12233v1",
      "title": "Categorical Flow Maps",
      "abstract": "We introduce Categorical Flow Maps, a flow-matching method for accelerated few-step generation of categorical data via self-distillation. Building on recent variational formulations of flow matching and the broader trend towards accelerated inference in diffusion and flow-based models, we define a flow map towards the simplex that transports probability mass toward a predicted endpoint, yielding a parametrisation that naturally constrains model predictions. Since our trajectories are continuous rather than discrete, Categorical Flow Maps can be trained with existing distillation techniques, as well as a new objective based on endpoint consistency. This continuous formulation also automatically unlocks test-time inference: we can directly reuse existing guidance and reweighting techniques in the categorical setting to steer sampling toward downstream objectives. Empirically, we achieve state-of-the-art few-step results on images, molecular graphs, and text, with strong performance even in single-step generation.",
      "authors": [
        "Daan Roos",
        "Oscar Davis",
        "Floor Eijkelboom",
        "Michael Bronstein",
        "Max Welling",
        "İsmail İlkan Ceylan",
        "Luca Ambrogioni",
        "Jan-Willem van de Meent"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 18:10:46+00:00",
      "link": "https://arxiv.org/pdf/2602.12233v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12222v1",
      "title": "Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training",
      "abstract": "Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \\textbf{\\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \\textbf{\\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \\textbf{\\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT",
      "authors": [
        "Miaosen Zhang",
        "Yishan Liu",
        "Shuxia Lin",
        "Xu Yang",
        "Qi Dai",
        "Chong Luo",
        "Weihao Jiang",
        "Peng Hou",
        "Anxiang Zeng",
        "Xin Geng",
        "Baining Guo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-12 17:59:58+00:00",
      "link": "https://arxiv.org/pdf/2602.12222v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12218v1",
      "title": "The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics",
      "abstract": "Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $ρ> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($ρ\\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.",
      "authors": [
        "Christian Internò",
        "Jumpei Yamaguchi",
        "Loren Amdahl-Culleton",
        "Markus Olhofer",
        "David Klindt",
        "Barbara Hammer"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 17:56:07+00:00",
      "link": "https://arxiv.org/pdf/2602.12218v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12206v1",
      "title": "Making the complete OpenAIRE citation graph easily accessible through compact data representation",
      "abstract": "The OpenAIRE graph contains a large citation graph dataset, with over 200 million publications and over 2 billion citations. The current graph is available as a dump with metadata which uncompressed totals ~TB. This makes it hard to process on conventional computers. To make this network more available for the community we provide a processed OpenAIRE graph which is downscaled to 32GB, while preserving the full graph structure. Apart from this we offer the processed data in very simple format, which allows further straightforward manipulation. We also provide a python pipeline, which can be used to process the next releases of the OpenAIRE graph.",
      "authors": [
        "Joakim Skarding",
        "Pavel Sanda"
      ],
      "primary_category": "cs.SI",
      "categories": [
        "cs.SI",
        "cs.DL"
      ],
      "published": "2026-02-12 17:44:36+00:00",
      "link": "https://arxiv.org/pdf/2602.12206v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12205v2",
      "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
      "abstract": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.",
      "authors": [
        "Dianyi Wang",
        "Ruihang Li",
        "Feng Han",
        "Chaofan Ma",
        "Wei Song",
        "Siyuan Wang",
        "Yibin Wang",
        "Yi Xin",
        "Hongjian Liu",
        "Zhixiong Zhang",
        "Shengyuan Ding",
        "Tianhang Wang",
        "Zhenglin Cheng",
        "Tao Lin",
        "Cheng Jin",
        "Kaicheng Yu",
        "Jingjing Chen",
        "Wenjie Wang",
        "Zhongyu Wei",
        "Jiaqi Wang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-12 17:44:24+00:00",
      "link": "https://arxiv.org/pdf/2602.12205v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12203v1",
      "title": "ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images",
      "abstract": "Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.",
      "authors": [
        "Mathieu Sibue",
        "Andres Muñoz Garza",
        "Samuel Mensah",
        "Pranav Shetty",
        "Zhiqiang Ma",
        "Xiaomo Liu",
        "Manuela Veloso"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 17:38:57+00:00",
      "link": "https://arxiv.org/pdf/2602.12203v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12192v1",
      "title": "Query-focused and Memory-aware Reranker for Long Context Processing",
      "abstract": "Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.",
      "authors": [
        "Yuqing Li",
        "Jiangnan Li",
        "Mo Yu",
        "Guoxuan Ding",
        "Zheng Lin",
        "Weiping Wang",
        "Jie Zhou"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 17:23:38+00:00",
      "link": "https://arxiv.org/pdf/2602.12192v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12187v1",
      "title": "SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization",
      "abstract": "Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.",
      "authors": [
        "Sunghwan Kim",
        "Wooseok Jeong",
        "Serin Kim",
        "Sangam Lee",
        "Dongha Lee"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-12 17:18:00+00:00",
      "link": "https://arxiv.org/pdf/2602.12187v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12177v1",
      "title": "EO-VAE: Towards A Multi-sensor Tokenizer for Earth Observation Data",
      "abstract": "State-of-the-art generative image and video models rely heavily on tokenizers that compress high-dimensional inputs into more efficient latent representations. While this paradigm has revolutionized RGB generation, Earth observation (EO) data presents unique challenges due to diverse sensor specifications and variable spectral channels. We propose EO-VAE, a multi-sensor variational autoencoder designed to serve as a foundational tokenizer for the EO domain. Unlike prior approaches that train separate tokenizers for each modality, EO-VAE utilizes a single model to encode and reconstruct flexible channel combinations via dynamic hypernetworks. Our experiments on the TerraMesh dataset demonstrate that EO-VAE achieves superior reconstruction fidelity compared to the TerraMind tokenizers, establishing a robust baseline for latent generative modeling in remote sensing.",
      "authors": [
        "Nils Lehmann",
        "Yi Wang",
        "Zhitong Xiong",
        "Xiaoxiang Zhu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-12 17:09:14+00:00",
      "link": "https://arxiv.org/pdf/2602.12177v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12172v1",
      "title": "Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation",
      "abstract": "Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline -- Knowledge Identifier, Organizer, and Adapter (IOA) -- that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom's Mastery Learning Principles and Vygotsky's Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model's performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1/3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7% of teacher performance on DollyEval while using less than 1/10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2% improvement on MATH and 22.3% on HumanEval compared with state-of-the-art baselines.",
      "authors": [
        "Bowei He",
        "Yankai Chen",
        "Xiaokun Zhang",
        "Linghe Kong",
        "Philip S. Yu",
        "Xue Liu",
        "Chen Ma"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-12 17:00:36+00:00",
      "link": "https://arxiv.org/pdf/2602.12172v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12170v1",
      "title": "Statistical Parsing for Logical Information Retrieval",
      "abstract": "In previous work (Coppola, 2024) we introduced the Quantified Boolean Bayesian Network (QBBN), a logical graphical model that implements the forward fragment of natural deduction (Prawitz, 1965) as a probabilistic factor graph. That work left two gaps: no negation/backward reasoning, and no parser for natural language.   This paper addresses both gaps across inference, semantics, and syntax. For inference, we extend the QBBN with NEG factors enforcing P(x) + P(neg x) = 1, enabling contrapositive reasoning (modus tollens) via backward lambda messages, completing Prawitz's simple elimination rules. The engine handles 44/44 test cases spanning 22 reasoning patterns. For semantics, we present a typed logical language with role-labeled predicates, modal quantifiers, and three tiers of expressiveness following Prawitz: first-order quantification, propositions as arguments, and predicate quantification via lambda abstraction. For syntax, we present a typed slot grammar that deterministically compiles sentences to logical form (33/33 correct, zero ambiguity). LLMs handle disambiguation (95% PP attachment accuracy) but cannot produce structured parses directly (12.4% UAS), confirming grammars are necessary. The architecture: LLM preprocesses, grammar parses, LLM reranks, QBBN infers.   We argue this reconciles formal semantics with Sutton's \"bitter lesson\" (2019): LLMs eliminate the annotation bottleneck that killed formal NLP, serving as annotator while the QBBN serves as verifier. Code: https://github.com/gregorycoppola/world",
      "authors": [
        "Greg Coppola"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 16:57:25+00:00",
      "link": "https://arxiv.org/pdf/2602.12170v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12164v1",
      "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
      "abstract": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.",
      "authors": [
        "Xiaohan He",
        "Shiyang Feng",
        "Songtao Huang",
        "Lei Bai",
        "Bin Wang",
        "Bo Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 16:46:00+00:00",
      "link": "https://arxiv.org/pdf/2602.12164v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12158v1",
      "title": "SafeNeuron: Neuron-Level Safety Alignment for Large Language Models",
      "abstract": "Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model's internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities. Furthermore, our layer-wise analysis reveals that safety behaviors are governed by stable and shared internal representations. Overall, SafeNeuron provides an interpretable and robust perspective for model alignment.",
      "authors": [
        "Zhaoxin Wang",
        "Jiaming Liang",
        "Fengbin Zhu",
        "Weixiang Zhao",
        "Junfeng Fang",
        "Jiayi Ji",
        "Handing Wang",
        "Tat-Seng Chua"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 16:40:05+00:00",
      "link": "https://arxiv.org/pdf/2602.12158v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12146v1",
      "title": "Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning",
      "abstract": "Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data formats. Recent advancements in deep learning have opened new avenues for compression; however, many existing approaches depend on dense vector representations that obscure the underlying token structure. To address these limitations, we propose a novel lossless compression method that leverages Reinforcement Learning applied to a T5 language model architecture. This approach enables the compression of data into sequences of tokens rather than traditional vector representations. Unlike auto-encoders, which typically encode information into continuous latent spaces, our method preserves the token-based structure, aligning more closely with the original data format. This preservation allows for higher compression ratios while maintaining semantic integrity. By training the model using an off-policy Reinforcement Learning algorithm, we optimize sequence length to minimize redundancy and enhance compression efficiency. Our method introduces an efficient and adaptive data compression system built upon advanced Reinforcement Learning techniques, functioning independently of external grammatical or world knowledge. This approach shows significant improvements in compression ratios compared to conventional methods. By leveraging the latent information within language models, our system effectively compresses data without requiring explicit content understanding, paving the way for more robust and practical compression solutions across various applications.",
      "authors": [
        "Mahdi Khodabandeh",
        "Ghazal Shabani",
        "Arash Yousefi Jordehi",
        "Seyed Abolghasem Mirroshandel"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IT"
      ],
      "published": "2026-02-12 16:30:55+00:00",
      "link": "https://arxiv.org/pdf/2602.12146v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12143v1",
      "title": "STAR : Bridging Statistical and Agentic Reasoning for Large Model Performance Prediction",
      "abstract": "As comprehensive large model evaluation becomes prohibitively expensive, predicting model performance from limited observations has become essential. However, existing statistical methods struggle with pattern shifts, data sparsity, and lack of explanation, while pure LLM methods remain unreliable. We propose STAR, a framework that bridges data-driven STatistical expectations with knowledge-driven Agentic Reasoning. STAR leverages specialized retrievers to gather external knowledge and embeds semantic features into Constrained Probabilistic Matrix Factorization (CPMF) to generate statistical expectations with uncertainty. A reasoning module guided by Expectation Violation Theory (EVT) then refines predictions through intra-family analysis, cross-model comparison, and credibility-aware aggregation, producing adjustments with traceable explanations. Extensive experiments show that STAR consistently outperforms all baselines on both score-based and rank-based metrics, delivering a 14.46% gain in total score over the strongest statistical method under extreme sparsity, with only 1--2 observed scores per test model.",
      "authors": [
        "Xiaoxiao Wang",
        "Chunxiao Li",
        "Junying Wang",
        "Yijin Guo",
        "Zijian Chen",
        "Chunyi Li",
        "Xiaohong Liu",
        "Zicheng Zhang",
        "Guangtao Zhai"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-12 16:30:07+00:00",
      "link": "https://arxiv.org/pdf/2602.12143v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12137v1",
      "title": "CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes",
      "abstract": "City councils play a crucial role in local governance, directly influencing citizens' daily lives through decisions made during municipal meetings. These deliberations are formally documented in meeting minutes, which serve as official records of discussions, decisions, and voting outcomes. Despite their importance, municipal meeting records have received little attention in Information Retrieval (IR) and Natural Language Processing (NLP), largely due to the lack of annotated datasets, which ultimately limit the development of computational models. To address this gap, we introduce CitiLink-Minutes, a multilayer dataset of 120 European Portuguese municipal meeting minutes from six municipalities. Unlike prior annotated datasets of parliamentary or video records, CitiLink-Minutes provides multilayer annotations and structured linkage of official written minutes. The dataset contains over one million tokens, with all personal identifiers de-identified. Each minute was manually annotated by two trained annotators and curated by an experienced linguist across three complementary dimensions: (1) metadata, (2) subjects of discussion, and (3) voting outcomes, totaling over 38,000 individual annotations. Released under FAIR principles and accompanied by baseline results on metadata extraction, topic classification, and vote labeling, CitiLink-Minutes demonstrates its potential for downstream NLP and IR tasks, while promoting transparent access to municipal decisions.",
      "authors": [
        "Ricardo Campos",
        "Ana Filipa Pacheco",
        "Ana Luísa Fernandes",
        "Inês Cantante",
        "Rute Rebouças",
        "Luís Filipe Cunha",
        "José Miguel Isidro",
        "José Pedro Evans",
        "Miguel Marques",
        "Rodrigo Batista",
        "Evelin Amorim",
        "Alípio Jorge",
        "Nuno Guimarães",
        "Sérgio Nunes",
        "António Leal",
        "Purificação Silvano"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 16:22:55+00:00",
      "link": "https://arxiv.org/pdf/2602.12137v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12135v2",
      "title": "WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End Spoken Dialogue Models",
      "abstract": "With the rapid integration of advanced reasoning capabilities into spoken dialogue models, the field urgently demands benchmarks that transcend simple interactions to address real-world complexity. However, current evaluations predominantly adhere to text-generation standards, overlooking the unique audio-centric characteristics of paralinguistics and colloquialisms, alongside the cognitive depth required by modern agents. To bridge this gap, we introduce WavBench, a comprehensive benchmark designed to evaluate realistic conversational abilities where prior works fall short. Uniquely, WavBench establishes a tripartite framework: 1) Pro subset, designed to rigorously challenge reasoning-enhanced models with significantly increased difficulty; 2) Basic subset, defining a novel standard for spoken colloquialism that prioritizes \"listenability\" through natural vocabulary, linguistic fluency, and interactive rapport, rather than rigid written accuracy; and 3) Acoustic subset, covering explicit understanding, generation, and implicit dialogue to rigorously evaluate comprehensive paralinguistic capabilities within authentic real-world scenarios. Through evaluating five state-of-the-art models, WavBench offers critical insights into the intersection of complex problem-solving, colloquial delivery, and paralinguistic fidelity, guiding the evolution of robust spoken dialogue models. The benchmark dataset and evaluation toolkit are available at https://naruto-2024.github.io/wavbench.github.io/.",
      "authors": [
        "Yangzhuo Li",
        "Shengpeng Ji",
        "Yifu Chen",
        "Tianle Liang",
        "Haorong Ying",
        "Yule Wang",
        "Junbo Li",
        "Jun Fang",
        "Zhou Zhao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 16:22:11+00:00",
      "link": "https://arxiv.org/pdf/2602.12135v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12132v1",
      "title": "A Rule-based Computational Model for Gaidhlig Morphology",
      "abstract": "Language models and software tools are essential to support the continuing vitality of lesser-used languages; however, currently popular neural models require considerable data for training, which normally is not available for such low-resource languages. This paper describes work-in-progress to construct a rule-based model of Gaidhlig morphology using data from Wiktionary, arguing that rule-based systems effectively leverage limited sample data, support greater interpretability, and provide insights useful in the design of teaching materials. The use of SQL for querying the occurrence of different lexical patterns is investigated, and a declarative rule-base is presented that allows Python utilities to derive inflected forms of Gaidhlig words. This functionality could be used to support educational tools that teach or explain language patterns, for example, or to support higher level tools such as rule-based dependency parsers. This approach adds value to the data already present in Wiktionary by adapting it to new use-cases.",
      "authors": [
        "Peter J Barclay"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 16:20:17+00:00",
      "link": "https://arxiv.org/pdf/2602.12132v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12129v1",
      "title": "Towards Personalized Bangla Book Recommendation: A Large-Scale Multi-Entity Book Graph Dataset",
      "abstract": "Personalized book recommendation in Bangla literature has been constrained by the lack of structured, large-scale, and publicly available datasets. This work introduces RokomariBG, a large-scale, multi-entity heterogeneous book graph dataset designed to support research on personalized recommendation in a low-resource language setting. The dataset comprises 127,302 books, 63,723 users, 16,601 authors, 1,515 categories, 2,757 publishers, and 209,602 reviews, connected through eight relation types and organized as a comprehensive knowledge graph.   To demonstrate the utility of the dataset, we provide a systematic benchmarking study on the Top-N recommendation task, evaluating a diverse set of representative recommendation models, including classical collaborative filtering methods, matrix factorization models, content-based approaches, graph neural networks, a hybrid matrix factorization model with side information, and a neural two-tower retrieval architecture. The benchmarking results highlight the importance of leveraging multi-relational structure and textual side information, with neural retrieval models achieving the strongest performance (NDCG@10 = 0.204). Overall, this work establishes a foundational benchmark and a publicly available resource for Bangla book recommendation research, enabling reproducible evaluation and future studies on recommendation in low-resource cultural domains. The dataset and code are publicly available at https://github.com/backlashblitz/Bangla-Book-Recommendation-Dataset",
      "authors": [
        "Rahin Arefin Ahmed",
        "Md. Anik Chowdhury",
        "Sakil Ahmed Sheikh Reza",
        "Devnil Bhattacharjee",
        "Muhammad Abdullah Adnan",
        "Nafis Sadeq"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "published": "2026-02-12 16:18:55+00:00",
      "link": "https://arxiv.org/pdf/2602.12129v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12125v1",
      "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
      "abstract": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.",
      "authors": [
        "Wenkai Yang",
        "Weijie Liu",
        "Ruobing Xie",
        "Kai Yang",
        "Saiyong Yang",
        "Yankai Lin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-12 16:14:29+00:00",
      "link": "https://arxiv.org/pdf/2602.12125v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12123v1",
      "title": "Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning",
      "abstract": "Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.   Meta-Sel constructs a meta-dataset by sampling pairs from the training split and using class agreement as supervision, then trains a calibrated logistic regressor on two inexpensive meta-features: TF--IDF cosine similarity and a length-compatibility ratio. At inference time, the selector performs a single vectorized scoring pass over the full candidate pool and returns the top-k demonstrations, requiring no model fine-tuning, no online exploration, and no additional LLM calls. This yields deterministic rankings and makes the selection mechanism straightforward to audit via interpretable feature weights.   Beyond proposing Meta-Sel, we provide a broad empirical study of demonstration selection, benchmarking 12 methods -- spanning prompt engineering baselines, heuristic selection, reinforcement learning, and influence-based approaches -- across four intent datasets and five open-source LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing methods, is particularly effective for smaller models where selection quality can partially compensate for limited model capacity, and maintains competitive selection-time overhead.",
      "authors": [
        "Xubin Wang",
        "Weijia Jia"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-12 16:11:29+00:00",
      "link": "https://arxiv.org/pdf/2602.12123v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12120v1",
      "title": "Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models",
      "abstract": "Many universities face increasing financial pressure and rely on accurate forecasts of commencing enrolments. However, enrolment forecasting in higher education is often data-sparse; annual series are short and affected by reporting changes and regime shifts. Popular classical approaches can be unreliable, as parameter estimation and model selection are unstable with short samples, and structural breaks degrade extrapolation. Recently, TSFMs have provided zero-shot priors, delivering strong gains in annual, data-sparse institutional forecasting under leakage-disciplined covariate construction. We benchmark multiple TSFM families in a zero-shot setting and test a compact, leakage-safe covariate set and introduce the Institutional Operating Conditions Index (IOCI), a transferable 0-100 regime covariate derived from time-stamped documentary evidence available at each forecast origin, alongside Google Trends demand proxies with stabilising feature engineering. Using an expanding-window backtest with strict vintage alignment, covariate-conditioned TSFMs perform on par with classical benchmarks without institution-specific training, with performance differences varying by cohort and model.",
      "authors": [
        "Jittarin Jetwiriyanon",
        "Teo Susnjak",
        "Surangika Ranathunga"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 16:10:42+00:00",
      "link": "https://arxiv.org/pdf/2602.12120v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12117v1",
      "title": "KAN-FIF: Spline-Parameterized Lightweight Physics-based Tropical Cyclone Estimation on Meteorological Satellite",
      "abstract": "Tropical cyclones (TC) are among the most destructive natural disasters, causing catastrophic damage to coastal regions through extreme winds, heavy rainfall, and storm surges. Timely monitoring of tropical cyclones is crucial for reducing loss of life and property, yet it is hindered by the computational inefficiency and high parameter counts of existing methods on resource-constrained edge devices. Current physics-guided models suffer from linear feature interactions that fail to capture high-order polynomial relationships between TC attributes, leading to inflated model sizes and hardware incompatibility. To overcome these challenges, this study introduces the Kolmogorov-Arnold Network-based Feature Interaction Framework (KAN-FIF), a lightweight multimodal architecture that integrates MLP and CNN layers with spline-parameterized KAN layers. For Maximum Sustained Wind (MSW) prediction, experiments demonstrate that the KAN-FIF framework achieves a $94.8\\%$ reduction in parameters (0.99MB vs 19MB) and $68.7\\%$ faster inference per sample (2.3ms vs 7.35ms) compared to baseline model Phy-CoCo, while maintaining superior accuracy with $32.5\\%$ lower MAE. The offline deployment experiment of the FY-4 series meteorological satellite processor on the Qingyun-1000 development board achieved a 14.41ms per-sample inference latency with the KAN-FIF framework, demonstrating promising feasibility for operational TC monitoring and extending deployability to edge-device AI applications. The code is released at https://github.com/Jinglin-Zhang/KAN-FIF.",
      "authors": [
        "Jiakang Shen",
        "Qinghui Chen",
        "Runtong Wang",
        "Chenrui Xu",
        "Jinglin Zhang",
        "Cong Bai",
        "Feng Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 16:07:39+00:00",
      "link": "https://arxiv.org/pdf/2602.12117v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12116v1",
      "title": "P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling",
      "abstract": "Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.",
      "authors": [
        "Pinyi Zhang",
        "Ting-En Lin",
        "Yuchuan Wu",
        "Jingyang Chen",
        "Zongqi Wang",
        "Hua Yang",
        "Ze Xu",
        "Fei Huang",
        "Kai Zhang",
        "Yongbin Li"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 16:07:22+00:00",
      "link": "https://arxiv.org/pdf/2602.12116v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12112v1",
      "title": "Few-Shot Design Optimization by Exploiting Auxiliary Information",
      "abstract": "Many real-world design problems involve optimizing an expensive black-box function $f(x)$, such as hardware design or drug discovery. Bayesian Optimization has emerged as a sample-efficient framework for this problem. However, the basic setting considered by these methods is simplified compared to real-world experimental setups, where experiments often generate a wealth of useful information. We introduce a new setting where an experiment generates high-dimensional auxiliary information $h(x)$ along with the performance measure $f(x)$; moreover, a history of previously solved tasks from the same task family is available for accelerating optimization. A key challenge of our setting is learning how to represent and utilize $h(x)$ for efficiently solving new optimization tasks beyond the task history. We develop a novel approach for this setting based on a neural model which predicts $f(x)$ for unseen designs given a few-shot context containing observations of $h(x)$. We evaluate our method on two challenging domains, robotic hardware design and neural network hyperparameter tuning, and introduce a novel design problem and large-scale benchmark for the former. On both domains, our method utilizes auxiliary feedback effectively to achieve more accurate few-shot prediction and faster optimization of design tasks, significantly outperforming several methods for multi-task optimization.",
      "authors": [
        "Arjun Mani",
        "Carl Vondrick",
        "Richard Zemel"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 16:03:46+00:00",
      "link": "https://arxiv.org/pdf/2602.12112v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12105v1",
      "title": "Iskra: A System for Inverse Geometry Processing",
      "abstract": "We propose a system for differentiating through solutions to geometry processing problems. Our system differentiates a broad class of geometric algorithms, exploiting existing fast problem-specific schemes common to geometry processing, including local-global and ADMM solvers. It is compatible with machine learning frameworks, opening doors to new classes of inverse geometry processing applications. We marry the scatter-gather approach to mesh processing with tensor-based workflows and rely on the adjoint method applied to user-specified imperative code to generate an efficient backward pass behind the scenes. We demonstrate our approach by differentiating through mean curvature flow, spectral conformal parameterization, geodesic distance computation, and as-rigid-as-possible deformation, examining usability and performance on these applications. Our system allows practitioners to differentiate through existing geometry processing algorithms without needing to reformulate them, resulting in low implementation effort, fast runtimes, and lower memory requirements than differentiable optimization tools not tailored to geometry processing.",
      "authors": [
        "Ana Dodik",
        "Ahmed H. Mahmoud",
        "Justin Solomon"
      ],
      "primary_category": "cs.GR",
      "categories": [
        "cs.GR",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-12 15:59:06+00:00",
      "link": "https://arxiv.org/pdf/2602.12105v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12084v1",
      "title": "Computing Distinguishing Formulae for Threshold-Based Behavioural Distances",
      "abstract": "Behavioural distances generally offer more fine-grained means of comparing quantitative systems than two-valued behavioural equivalences. They often relate to quantitative modalities, which generate quantitative modal logics that characterize a given behavioural distance in terms of the induced logical distance. We develop a unified framework for behavioural distances and logics induced by a special type of modalities that lift two-valued predicates to quantitative predicates. A typical example is the probability operator, which maps a two-valued predicate $A$ to a quantitative predicate on probability distributions assigning to each distribution the respective probability of $A$. Correspondingly, the prototypical example of our framework is $ε$-bisimulation distance of Markov chains, which has recently been shown to coincide with the behavioural distance induced by the popular Lévy-Prokhorov distance on distributions. Other examples include behavioural distance on metric transition systems and Hausdorff behavioural distance on fuzzy transition systems. Our main generic results concern the polynomial-time extraction of distinguishing formulae in two characteristic modal logics: A two-valued logic with a notion of satisfaction up to $ε$, and a quantitative modal logic. These results instantiate to new results in many of the mentioned examples. Notably, we obtain polynomial-time extraction of distinguishing formulae for $ε$-bisimulation distance of Markov chains in a quantitative logic featuring a `generally' modality used in probabilistic knowledge representation.",
      "authors": [
        "Jonas Forster",
        "Lutz Schröder",
        "Paul Wild",
        "Barbara König",
        "Pedro Nora"
      ],
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO"
      ],
      "published": "2026-02-12 15:39:30+00:00",
      "link": "https://arxiv.org/pdf/2602.12084v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12082v1",
      "title": "Empirical Gaussian Processes",
      "abstract": "Gaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space. We study Empirical GPs, a principled framework for constructing flexible, data-driven GP priors that overcome these limitations. Rather than relying on standard parametric kernels, we estimate the mean and covariance functions empirically from a corpus of historical observations, enabling the prior to reflect rich, non-trivial covariance structures present in the data. Theoretically, we show that the resulting model converges to the GP that is closest (in KL-divergence sense) to the real data generating process. Practically, we formulate the problem of learning the GP prior from independent datasets as likelihood estimation and derive an Expectation-Maximization algorithm with closed-form updates, allowing the model handle heterogeneous observation locations across datasets. We demonstrate that Empirical GPs achieve competitive performance on learning curve extrapolation and time series forecasting benchmarks.",
      "authors": [
        "Jihao Andreas Lin",
        "Sebastian Ament",
        "Louis C. Tiao",
        "David Eriksson",
        "Maximilian Balandat",
        "Eytan Bakshy"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-12 15:39:08+00:00",
      "link": "https://arxiv.org/pdf/2602.12082v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12078v1",
      "title": "Tiny Recursive Reasoning with Mamba-2 Attention Hybrid",
      "abstract": "Recent work on recursive reasoning models like TRM demonstrates that tiny networks (7M parameters) can achieve strong performance on abstract reasoning tasks through latent recursion -- iterative refinement in hidden representation space without emitting intermediate tokens. This raises a natural question about operator choice: Mamba-2's state space recurrence is itself a form of iterative refinement, making it a natural candidate for recursive reasoning -- but does introducing Mamba-2 into the recursive scaffold preserve reasoning capability? We investigate this by replacing the Transformer blocks in TRM with Mamba-2 hybrid operators while maintaining parameter parity (6.83M vs 6.86M parameters). On ARC-AGI-1, we find that the hybrid improves pass@2 (the official metric) by +2.0\\% (45.88\\% vs 43.88\\%) and consistently outperforms at higher K values (+4.75\\% at pass@100), whilst maintaining pass@1 parity. This suggests improved candidate coverage -- the model generates correct solutions more reliably -- with similar top-1 selection. Our results validate that Mamba-2 hybrid operators preserve reasoning capability within the recursive scaffold, establishing SSM-based operators as viable candidates in the recursive operator design space and taking a first step towards understanding the best mixing strategies for recursive reasoning.",
      "authors": [
        "Wenlong Wang",
        "Fergal Reid"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-12 15:36:32+00:00",
      "link": "https://arxiv.org/pdf/2602.12078v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12064v1",
      "title": "DIVER: A Robust Text-to-SQL System with Dynamic Interactive Value Linking and Evidence Reasoning",
      "abstract": "In the era of large language models, Text-to-SQL, as a natural language interface for databases, is playing an increasingly important role. The sota Text-to-SQL models have achieved impressive accuracy, but their performance critically relies on expert-written evidence, which typically clarifies schema and value linking that existing models struggle to identify. Such limitations stem from the ambiguity of user queries and, more importantly, the complexity of comprehending large-scale and dynamic database values. Consequently, in real-world scenarios where expert assistance is unavailable, existing methods suffer a severe performance collapse, with execution accuracy dropping by over 10%. This underscores their lack of robustness. To address this, we propose DIVER, a robust system that automates evidence reasoning with dynamic interactive value linking. It leverages a compatible toolbox containing diverse tools to probe the database. Then, restricted by a structured workspace (CoTF, Chain of Thoughts and Facts), it reflects based on probe results and selects a new tool for next round of probing. Through this automatically iterative process, DIVER identifies schema and value linking missed by existing methods. Based on these accurate linkings, DIVER is able to infer correct usage of SQL functions and formulas and generate high-quality evidence, achieving robust Text-to-SQL without expert assistance. Extensive experiments demonstrate that: 1) The DIVER system significantly enhances the robustness of various Text-to-SQL models, improving performance by up to 10.82% in Execution Accuracy (EX) and 16.09% in Valid Efficiency Score (VES). 2) Our dynamic interactive value linking significantly improves the robustness of existing systems and the accuracy of schema and value linking, especially when confronted with challenges posed by large-scale, dynamic database values.",
      "authors": [
        "Yafeng Nan",
        "Haifeng Sun",
        "Zirui Zhuang",
        "Qi Qi",
        "Guojun Chu",
        "Jianxin Liao",
        "Dan Pei",
        "Jingyu Wang"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-12 15:23:06+00:00",
      "link": "https://arxiv.org/pdf/2602.12064v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12056v1",
      "title": "LawThinker: A Deep Research Legal Agent in Dynamic Environments",
      "abstract": "Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent .",
      "authors": [
        "Xinyu Yang",
        "Chenlong Deng",
        "Tongyu Wen",
        "Binyu Xie",
        "Zhicheng Dou"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 15:19:11+00:00",
      "link": "https://arxiv.org/pdf/2602.12056v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12049v1",
      "title": "Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards",
      "abstract": "Large language models (LLMs) have demonstrated strong code generation capabilities, yet the runtime performance of generated code is not guaranteed, and there have been few attempts to train LLMs using runtime performance as a reward in the HPC domain. We propose an online reinforcement learning approach that executes LLM-generated code on a supercomputer and directly feeds back the measured runtime performance (GFLOPS) as a reward. We further introduce a Staged Quality-Diversity (SQD) algorithm that progressively varies the permitted optimization techniques on a per-problem basis, enabling the model to learn code optimization from diverse perspectives. We build a distributed system connecting a GPU training cluster with a CPU benchmarking cluster, and train Qwen2.5 Coder 14B on a double-precision matrix multiplication task using Group Relative Policy Optimization (GRPO). Through two experiments, we show that reinforcement learning combining runtime performance feedback with staged optimization can improve the HPC code generation capability of LLMs.",
      "authors": [
        "Ryo Mikasa",
        "Shun-ichiro Hayashi",
        "Daichi Mukunoki",
        "Tetsuya Hoshino",
        "Takahiro Katagiri"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 15:12:59+00:00",
      "link": "https://arxiv.org/pdf/2602.12049v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12041v1",
      "title": "Compress, Cross and Scale: Multi-Level Compression Cross Networks for Efficient Scaling in Recommender Systems",
      "abstract": "Modeling high-order feature interactions efficiently is a central challenge in click-through rate and conversion rate prediction. Modern industrial recommender systems are predominantly built upon deep learning recommendation models, where the interaction backbone plays a critical role in determining both predictive performance and system efficiency. However, existing interaction modules often struggle to simultaneously achieve strong interaction capacity, high computational efficiency, and good scalability, resulting in limited ROI when models are scaled under strict production constraints. In this work, we propose MLCC, a structured feature interaction architecture that organizes feature crosses through hierarchical compression and dynamic composition, which can efficiently capture high-order feature dependencies while maintaining favorable computational complexity. We further introduce MC-MLCC, a Multi-Channel extension that decomposes feature interactions into parallel subspaces, enabling efficient horizontal scaling with improved representation capacity and significantly reduced parameter growth. Extensive experiments on three public benchmarks and a large-scale industrial dataset show that our proposed models consistently outperform strong DLRM-style baselines by up to 0.52 AUC, while reducing model parameters and FLOPs by up to 26$\\times$ under comparable performance. Comprehensive scaling analyses demonstrate stable and predictable scaling behavior across embedding dimension, head number, and channel count, with channel-based scaling achieving substantially better efficiency than conventional embedding inflation. Finally, online A/B testing on a real-world advertising platform validates the practical effectiveness of our approach, which has been widely adopted in Bilibili advertising system under strict latency and resource constraints.",
      "authors": [
        "Heng Yu",
        "Xiangjun Zhou",
        "Jie Xia",
        "Heng Zhao",
        "Anxin Wu",
        "Yu Zhao",
        "Dongying Kong"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-12 15:06:46+00:00",
      "link": "https://arxiv.org/pdf/2602.12041v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12039v2",
      "title": "The Implicit Bias of Logit Regularization",
      "abstract": "Logit regularization, the addition of a convex penalty directly in logit space, is widely used in modern classifiers, with label smoothing as a prominent example. While such methods often improve calibration and generalization, their mechanism remains under-explored. In this work, we analyze a general class of such logit regularizers in the context of linear classification, and demonstrate that they induce an implicit bias of logit clustering around finite per-sample targets. For Gaussian data, or whenever logits are sufficiently clustered, we prove that logit clustering drives the weight vector to align exactly with Fisher's Linear Discriminant. To demonstrate the consequences, we study a simple signal-plus-noise model in which this transition has dramatic effects: Logit regularization halves the critical sample complexity and induces grokking in the small-noise limit, while making generalization robust to noise. Our results extend the theoretical understanding of label smoothing and highlight the efficacy of a broader class of logit-regularization methods.",
      "authors": [
        "Alon Beck",
        "Yohai Bar Sinai",
        "Noam Levi"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-12 15:06:08+00:00",
      "link": "https://arxiv.org/pdf/2602.12039v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12021v1",
      "title": "Improved state mixing in higher-order and block diagonal linear recurrent networks",
      "abstract": "Linear recurrent networks (LRNNs) and linear state space models (SSMs) promise computational and memory efficiency on long-sequence modeling tasks, yet their diagonal state transitions limit expressivity. Dense and nonlinear architectures (e.g., LSTMs) on the other hand are provably more expressive, but computationally costly. Here, we explore how expressivity in LRNNs can be increased via richer state mixing across time and channels while maintaining competitive efficiency. Specifically, we introduce two structured LRNN architectures: (i) Higher-order Linear Recurrent Units (H-LRU), which generalize first-order recurrence to higher order, mixing multiple past states, and (ii) Block-Diagonal LRUs (BD-LRU), which enable dense intra-block channel mixing. Per-channel (H-LRU) or per-row (BD-LRU) L1-normalization of selective gates stabilizes training and allows for scaling window/block sizes. A parallel-scan implementation of the proposed architectures keeps the throughput competitive with diagonal LRNNs for moderate orders (H-LRU) and block sizes (BD-LRU). In synthetic sequence modeling tasks, the performance of BD-LRU matches or exceeds those of linear SSMs (Mamba), low-rank LRNNs (DeltaNet) and LSTM baselines, while H-LRU is found to be the most parameter-efficient in compression task. In both synthetic sequence modeling and language modeling, our results indicate that the structure of state mixing rather than width alone shapes expressivity of LRNNs, offering a practical route to closing the efficiency-expressivity gap in linear sequence models.",
      "authors": [
        "Igor Dubinin",
        "Antonio Orvieto",
        "Felix Effenberger"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 14:51:59+00:00",
      "link": "https://arxiv.org/pdf/2602.12021v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12018v1",
      "title": "Artificial intelligence is creating a new global linguistic hierarchy",
      "abstract": "Artificial intelligence (AI) has the potential to transform healthcare, education, governance and socioeconomic equity, but its benefits remain concentrated in a small number of languages (Bender, 2019; Blasi et al., 2022; Joshi et al., 2020; Ranathunga and de Silva, 2022; Young, 2015). Language AI - the technologies that underpin widely-used conversational systems such as ChatGPT - could provide major benefits if available in people's native languages, yet most of the world's 7,000+ linguistic communities currently lack access and face persistent digital marginalization. Here we present a global longitudinal analysis of social, economic and infrastructural conditions across languages to assess systemic inequalities in language AI. We first analyze the existence of AI resources for 6003 languages. We find that despite efforts of the community to broaden the reach of language technologies (Bapna et al., 2022; Costa-Jussà et al., 2022), the dominance of a handful of languages is exacerbating disparities on an unprecedented scale, with divides widening exponentially rather than narrowing. Further, we contrast the longitudinal diffusion of AI with that of earlier IT technologies, revealing a distinctive hype-driven pattern of spread. To translate our findings into practical insights and guide prioritization efforts, we introduce the Language AI Readiness Index (EQUATE), which maps the state of technological, socio-economic, and infrastructural prerequisites for AI deployment across languages. The index highlights communities where capacity exists but remains underutilized, and provides a framework for accelerating more equitable diffusion of language AI. Our work contributes to setting the baseline for a transition towards more sustainable and equitable language technologies.",
      "authors": [
        "Giulia Occhini",
        "Kumiko Tanaka-Ishii",
        "Anna Barford",
        "Refael Tikochinski",
        "Songbo Hu",
        "Roi Reichart",
        "Yijie Zhou",
        "Hannah Claus",
        "Ulla Petti",
        "Ivan Vulić",
        "Ramit Debnath",
        "Anna Korhonen"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.CL"
      ],
      "published": "2026-02-12 14:50:44+00:00",
      "link": "https://arxiv.org/pdf/2602.12018v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12015v1",
      "title": "Disentangling Ambiguity from Instability in Large Language Models: A Clinical Text-to-SQL Case Study",
      "abstract": "Deploying large language models for clinical Text-to-SQL requires distinguishing two qualitatively different causes of output diversity: (i) input ambiguity that should trigger clarification, and (ii) model instability that should trigger human review. We propose CLUES, a framework that models Text-to-SQL as a two-stage process (interpretations --> answers) and decomposes semantic uncertainty into an ambiguity score and an instability score. The instability score is computed via the Schur complement of a bipartite semantic graph matrix. Across AmbigQA/SituatedQA (gold interpretations) and a clinical Text-to-SQL benchmark (known interpretations), CLUES improves failure prediction over state-of-the-art Kernel Language Entropy. In deployment settings, it remains competitive while providing a diagnostic decomposition unavailable from a single score. The resulting uncertainty regimes map to targeted interventions - query refinement for ambiguity, model improvement for instability. The high-ambiguity/high-instability regime contains 51% of errors while covering 25% of queries, enabling efficient triage.",
      "authors": [
        "Angelo Ziletti",
        "Leonardo D'Ambrosi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 14:46:20+00:00",
      "link": "https://arxiv.org/pdf/2602.12015v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12013v1",
      "title": "InjectRBP: Steering Large Language Model Reasoning Behavior via Pattern Injection",
      "abstract": "Reasoning can significantly enhance the performance of Large Language Models. While recent studies have exploited behavior-related prompts adjustment to enhance reasoning, these designs remain largely intuitive and lack a systematic analysis of the underlying behavioral patterns. Motivated by this, we investigate how models' reasoning behaviors shape reasoning from the perspective of behavioral patterns. We observe that models exhibit adaptive distributions of reasoning behaviors when responding to specific types of questions, and that structurally injecting these patterns can substantially influence the quality of the models' reasoning processes and outcomes. Building on these findings, we propose two optimization methods that require no parameter updates: InjectCorrect and InjectRLOpt. InjectCorrect guides the model by imitating behavioral patterns derived from its own past correct answers. InjectRLOpt learns a value function from historical behavior-pattern data and, via our proposed Reliability-Aware Softmax Policy, generates behavioral injectant during inference to steer the reasoning process. Our experiments demonstrate that both methods can improve model performance across various reasoning tasks without requiring any modifications to model parameters, achieving gains of up to 5.34% and 8.67%, respectively.",
      "authors": [
        "Xiuping Wu",
        "Zhao Yu",
        "Yuxin Cheng",
        "Ngai Wong",
        "Liangjun Ke",
        "Tapas Mishra",
        "Konstantinos V. Katsikopoulos"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 14:44:40+00:00",
      "link": "https://arxiv.org/pdf/2602.12013v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12009v1",
      "title": "On the Sensitivity of Firing Rate-Based Federated Spiking Neural Networks to Differential Privacy",
      "abstract": "Federated Neuromorphic Learning (FNL) enables energy-efficient and privacy-preserving learning on devices without centralizing data. However, real-world deployments require additional privacy mechanisms that can significantly alter training signals. This paper analyzes how Differential Privacy (DP) mechanisms, specifically gradient clipping and noise injection, perturb firing-rate statistics in Spiking Neural Networks (SNNs) and how these perturbations are propagated to rate-based FNL coordination. On a speech recognition task under non-IID settings, ablations across privacy budgets and clipping bounds reveal systematic rate shifts, attenuated aggregation, and ranking instability during client selection. Moreover, we relate these shifts to sparsity and memory indicators. Our findings provide actionable guidance for privacy-preserving FNL, specifically regarding the balance between privacy strength and rate-dependent coordination.",
      "authors": [
        "Luiz Pereira",
        "Mirko Perkusich",
        "Dalton Valadares",
        "Kyller Gorgônio"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 14:40:25+00:00",
      "link": "https://arxiv.org/pdf/2602.12009v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12004v1",
      "title": "CSEval: A Framework for Evaluating Clinical Semantics in Text-to-Image Generation",
      "abstract": "Text-to-image generation has been increasingly applied in medical domains for various purposes such as data augmentation and education. Evaluating the quality and clinical reliability of these generated images is essential. However, existing methods mainly assess image realism or diversity, while failing to capture whether the generated images reflect the intended clinical semantics, such as anatomical location and pathology. In this study, we propose the Clinical Semantics Evaluator (CSEval), a framework that leverages language models to assess clinical semantic alignment between the generated images and their conditioning prompts. Our experiments show that CSEval identifies semantic inconsistencies overlooked by other metrics and correlates with expert judgment. CSEval provides a scalable and clinically meaningful complement to existing evaluation methods, supporting the safe adoption of generative models in healthcare.",
      "authors": [
        "Robert Cronshaw",
        "Konstantinos Vilouras",
        "Junyu Yan",
        "Yuning Du",
        "Feng Chen",
        "Steven McDonagh",
        "Sotirios A. Tsaftaris"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 14:35:31+00:00",
      "link": "https://arxiv.org/pdf/2602.12004v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11995v1",
      "title": "Momentum LMS Theory beyond Stationarity: Stability, Tracking, and Regret",
      "abstract": "In large-scale data processing scenarios, data often arrive in sequential streams generated by complex systems that exhibit drifting distributions and time-varying system parameters. This nonstationarity challenges theoretical analysis, as it violates classical assumptions of i.i.d. (independent and identically distributed) samples, necessitating algorithms capable of real-time updates without expensive retraining. An effective approach should process each sample in a single pass, while maintaining computational and memory complexities independent of the data stream length. Motivated by these challenges, this paper investigates the Momentum Least Mean Squares (MLMS) algorithm as an adaptive identification tool, leveraging its computational simplicity and online processing capabilities. Theoretically, we derive tracking performance and regret bounds for the MLMS in time-varying stochastic linear systems under various practical conditions. Unlike classical LMS, whose stability can be characterized by first-order random vector difference equations, MLMS introduces an additional dynamical state due to momentum, leading to second-order time-varying random vector difference equations whose stability analysis hinges on more complicated products of random matrices, which poses a substantially challenging problem to resolve. Experiments on synthetic and real-world data streams demonstrate that MLMS achieves rapid adaptation and robust tracking, in agreement with our theoretical results especially in nonstationary settings, highlighting its promise for modern streaming and online learning applications.",
      "authors": [
        "Yifei Jin",
        "Xin Zheng",
        "Lei Guo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 14:24:42+00:00",
      "link": "https://arxiv.org/pdf/2602.11995v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11982v1",
      "title": "Automatic Simplification of Common Vulnerabilities and Exposures Descriptions",
      "abstract": "Understanding cyber security is increasingly important for individuals and organizations. However, a lot of information related to cyber security can be difficult to understand to those not familiar with the topic. In this study, we focus on investigating how large language models (LLMs) could be utilized in automatic text simplification (ATS) of Common Vulnerability and Exposure (CVE) descriptions. Automatic text simplification has been studied in several contexts, such as medical, scientific, and news texts, but it has not yet been studied to simplify texts in the rapidly changing and complex domain of cyber security. We created a baseline for cyber security ATS and a test dataset of 40 CVE descriptions, evaluated by two groups of cyber security experts in two survey rounds. We have found that while out-of-the box LLMs can make the text appear simpler, they struggle with meaning preservation. Code and data are available at https://version.aalto.fi/gitlab/vehomav1/simplification\\_nmi.",
      "authors": [
        "Varpu Vehomäki",
        "Kimmo K. Kaski"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 14:12:58+00:00",
      "link": "https://arxiv.org/pdf/2602.11982v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11968v1",
      "title": "DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling",
      "abstract": "In this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The collection covers three time periods: 2011-2015, 2020-2021 and 2024-present (1 million documents per time period for each language). We additionally provide pre-computed word type and token embeddings and lexical substitutions for our chosen target words, while at the same time leaving it open for the other researchers to come up with their own target words using the same datasets. DHPLT aims at filling in the current lack of multilingual diachronic corpora for semantic change modelling (beyond a dozen of high-resource languages). It opens the way for a variety of new experimental setups in this field. All the resources described in this paper are available at https://data.hplt-project.org/three/diachronic/, sorted by language.",
      "authors": [
        "Mariia Fedorova",
        "Andrey Kutuzov",
        "Khonzoda Umarova"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 14:01:40+00:00",
      "link": "https://arxiv.org/pdf/2602.11968v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11966v1",
      "title": "MING: An Automated CNN-to-Edge MLIR HLS framework",
      "abstract": "Driven by the increasing demand for low-latency and real-time processing, machine learning applications are steadily migrating toward edge computing platforms, where Field-Programmable Gate Arrays (FPGAs) are widely adopted for their energy efficiency compared to CPUs and GPUs. To generate high-performance and low-power FPGA designs, several frameworks built upon High Level Synthesis (HLS) vendor tools have been proposed, among which MLIR-based frameworks are gaining significant traction due to their extensibility and ease of use. However, existing state-of-the-art frameworks often overlook the stringent resource constraints of edge devices. To address this limitation, we propose MING, an Multi-Level Intermediate Representation (MLIR)-based framework that abstracts and automates the HLS design process. Within this framework, we adopt a streaming architecture with carefully managed buffers, specifically designed to handle resource constraints while ensuring low-latency. In comparison with recent frameworks, our approach achieves on average 15x speedup for standard Convolutional Neural Network (CNN) kernels with up to four layers, and up to 200x for single-layer kernels. For kernels with larger input sizes, MING is capable of generating efficient designs that respect hardware resource constraints, whereas state-of-the-art frameworks struggle to meet.",
      "authors": [
        "Jiahong Bi",
        "Lars Schütze",
        "Jeronimo Castrillon"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR"
      ],
      "published": "2026-02-12 14:01:11+00:00",
      "link": "https://arxiv.org/pdf/2602.11966v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11961v1",
      "title": "Scaling Model and Data for Multilingual Machine Translation with Open Large Language Models",
      "abstract": "Open large language models (LLMs) have demonstrated improving multilingual capabilities in recent years. In this paper, we present a study of open LLMs for multilingual machine translation (MT) across a range of languages, and investigate the effects of model scaling and data scaling when adapting open LLMs to multilingual MT through continual pretraining and instruction finetuning. Based on the Gemma3 model family, we develop MiLMMT-46, which achieves top-tier multilingual translation performance across 46 languages. Extensive experiments show that MiLMMT-46 consistently outperforms recent state-of-the-art (SOTA) models, including Seed-X, HY-MT-1.5, and TranslateGemma, and achieves competitive performance with strong proprietary systems such as Google Translate and Gemini 3 Pro.",
      "authors": [
        "Yuzhe Shang",
        "Pengzhi Gao",
        "Wei Liu",
        "Jian Luan",
        "Jinsong Su"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 13:56:02+00:00",
      "link": "https://arxiv.org/pdf/2602.11961v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11960v1",
      "title": "Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion",
      "abstract": "This report evaluates PDF-to-Markdown conversion using recent Vision-Language Models (VLMs) on challenging French documents. Document parsing is a critical step for Retrieval-Augmented Generation (RAG) pipelines, where transcription and layout errors propagate to downstream retrieval and grounding. Existing benchmarks often emphasize English or Chinese and can over-penalize benign formatting and linearization choices (e.g., line breaks, list segmentation, alternative table renderings) that are largely irrelevant for downstream use.   We introduce a French-focused benchmark of difficult pages selected via model-disagreement sampling from a corpus of 60{,}000 documents, covering handwritten forms, complex layouts, dense tables, and graphics-rich pages. Evaluation is performed with unit-test-style checks that target concrete failure modes (text presence, reading order, and local table constraints) combined with category-specific normalization designed to discount presentation-only variance. Across 15 models, we observe substantially higher robustness for the strongest proprietary models on handwriting and forms, while several open-weights systems remain competitive on standard printed layouts.",
      "authors": [
        "Bruno Rigal",
        "Victor Dupriez",
        "Alexis Mignon",
        "Ronan Le Hy",
        "Nicolas Mery"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-12 13:55:43+00:00",
      "link": "https://arxiv.org/pdf/2602.11960v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11958v1",
      "title": "RAM-Net: Expressive Linear Attention with Selectively Addressable Memory",
      "abstract": "While linear attention architectures offer efficient inference, compressing unbounded history into a fixed-size memory inherently limits expressivity and causes information loss. To address this limitation, we introduce Random Access Memory Network (RAM-Net), a novel architecture designed to bridge the gap between the representational capacity of full attention and the memory efficiency of linear models. The core of RAM-Net maps inputs to high-dimensional sparse vectors serving as explicit addresses, allowing the model to selectively access a massive memory state. This design enables exponential state size scaling without additional parameters, which significantly mitigates signal interference and enhances retrieval fidelity. Moreover, the inherent sparsity ensures exceptional computational efficiency, as state updates are confined to minimal entries. Extensive experiments demonstrate that RAM-Net consistently surpasses state-of-the-art baselines in fine-grained long-range retrieval tasks and achieves competitive performance in standard language modeling and zero-shot commonsense reasoning benchmarks, validating its superior capability to capture complex dependencies with significantly reduced computational overhead.",
      "authors": [
        "Kaicheng Xiao",
        "Haotian Li",
        "Liran Dong",
        "Guoliang Xing"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-12 13:55:29+00:00",
      "link": "https://arxiv.org/pdf/2602.11958v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11957v1",
      "title": "Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization",
      "abstract": "Large language models (LLMs) are increasingly used to create content in regulated domains such as pharmaceuticals, where outputs must be scientifically accurate and legally compliant. Manual quality control (QC) is slow, error prone, and can become a publication bottleneck. We introduce LRBTC, a modular LLM and vision language model (VLM) driven QC architecture covering Language, Regulatory, Brand, Technical, and Content Structure checks. LRBTC combines a Student-Teacher dual model architecture, human in the loop (HITL) workflow with waterfall rule filtering to enable scalable, verifiable content validation and optimization. On AIReg-Bench, our approach achieves 83.0% F1 and 97.5% recall, reducing missed violations by 5x compared with Gemini 2.5 Pro. On CSpelling, it improves mean accuracy by 26.7%. Error analysis further reveals that while current models are strong at detecting misspellings (92.5 recall), they fail to identify complex medical grammatical (25.0 recall) and punctuation (41.7 recall) errors, highlighting a key area for future work. This work provides a practical, plug and play solution for reliable, transparent quality control of content in high stakes, compliance critical industries. We also provide access to our Demo under MIT Licenses.",
      "authors": [
        "Suyash Mishra",
        "Qiang Li",
        "Anubhav Girdhar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 13:53:29+00:00",
      "link": "https://arxiv.org/pdf/2602.11957v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11954v1",
      "title": "PAC to the Future: Zero-Knowledge Proofs of PAC Private Systems",
      "abstract": "Privacy concerns in machine learning systems have grown significantly with the increasing reliance on sensitive user data for training large-scale models. This paper introduces a novel framework combining Probably Approximately Correct (PAC) Privacy with zero-knowledge proofs (ZKPs) to provide verifiable privacy guarantees in trustless computing environments. Our approach addresses the limitations of traditional privacy-preserving techniques by enabling users to verify both the correctness of computations and the proper application of privacy-preserving noise, particularly in cloud-based systems. We leverage non-interactive ZKP schemes to generate proofs that attest to the correct implementation of PAC privacy mechanisms while maintaining the confidentiality of proprietary systems. Our results demonstrate the feasibility of achieving verifiable PAC privacy in outsourced computation, offering a practical solution for maintaining trust in privacy-preserving machine learning and database systems while ensuring computational integrity.",
      "authors": [
        "Guilhem Repetto",
        "Nojan Sheybani",
        "Gabrielle De Micheli",
        "Farinaz Koushanfar"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR"
      ],
      "published": "2026-02-12 13:49:22+00:00",
      "link": "https://arxiv.org/pdf/2602.11954v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11945v1",
      "title": "Towards Performance-Enhanced Model-Contrastive Federated Learning using Historical Information in Heterogeneous Scenarios",
      "abstract": "Federated Learning (FL) enables multiple nodes to collaboratively train a model without sharing raw data. However, FL systems are usually deployed in heterogeneous scenarios, where nodes differ in both data distributions and participation frequencies, which undermines the FL performance. To tackle the above issue, this paper proposes PMFL, a performance-enhanced model-contrastive federated learning framework using historical training information. Specifically, on the node side, we design a novel model-contrastive term into the node optimization objective by incorporating historical local models to capture stable contrastive points, thereby improving the consistency of model updates in heterogeneous data distributions.   On the server side, we utilize the cumulative participation count of each node to adaptively adjust its aggregation weight, thereby correcting the bias in the global objective caused by different node participation frequencies. Furthermore, the updated global model incorporates historical global models to reduce its fluctuations in performance between adjacent rounds. Extensive experiments demonstrate that PMFL achieves superior performance compared with existing FL methods in heterogeneous scenarios.",
      "authors": [
        "Hongliang Zhang",
        "Jiguo Yu",
        "Guijuan Wang",
        "Wenshuo Ma",
        "Tianqing He",
        "Baobao Chai",
        "Chunqiang Hu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 13:40:37+00:00",
      "link": "https://arxiv.org/pdf/2602.11945v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11940v1",
      "title": "Temporally Unified Adversarial Perturbations for Time Series Forecasting",
      "abstract": "While deep learning models have achieved remarkable success in time series forecasting, their vulnerability to adversarial examples remains a critical security concern. However, existing attack methods in the forecasting field typically ignore the temporal consistency inherent in time series data, leading to divergent and contradictory perturbation values for the same timestamp across overlapping samples. This temporally inconsistent perturbations problem renders adversarial attacks impractical for real-world data manipulation. To address this, we introduce Temporally Unified Adversarial Perturbations (TUAPs), which enforce a temporal unification constraint to ensure identical perturbations for each timestamp across all overlapping samples. Moreover, we propose a novel Timestamp-wise Gradient Accumulation Method (TGAM) that provides a modular and efficient approach to effectively generate TUAPs by aggregating local gradient information from overlapping samples. By integrating TGAM with momentum-based attack algorithms, we ensure strict temporal consistency while fully utilizing series-level gradient information to explore the adversarial perturbation space. Comprehensive experiments on three benchmark datasets and four representative state-of-the-art models demonstrate that our proposed method significantly outperforms baselines in both white-box and black-box transfer attack scenarios under TUAP constraints. Moreover, our method also exhibits superior transfer attack performance even without TUAP constraints, demonstrating its effectiveness and superiority in generating adversarial perturbations for time series forecasting models.",
      "authors": [
        "Ruixian Su",
        "Yukun Bao",
        "Xinze Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 13:37:45+00:00",
      "link": "https://arxiv.org/pdf/2602.11940v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11938v4",
      "title": "Who is the richest club in the championship? Detecting and Rewriting Underspecified Questions Improve QA Performance",
      "abstract": "Large language models (LLMs) perform well on well-posed questions, yet standard question-answering (QA) benchmarks remain far from solved. We argue that this gap is partly due to underspecified questions - queries whose interpretation cannot be uniquely determined without additional context. To test this hypothesis, we introduce an LLM-based classifier to identify underspecified questions and apply it to several widely used QA datasets, finding that 16% to over 50% of benchmark questions are underspecified and that LLMs perform significantly worse on them. To isolate the effect of underspecification, we conduct a controlled rewriting experiment that serves as an upper-bound analysis, rewriting underspecified questions into fully specified variants while holding gold answers fixed. QA performance consistently improves under this setting, indicating that many apparent QA failures stem from question underspecification rather than model limitations. Our findings highlight underspecification as an important confound in QA evaluation and motivate greater attention to question clarity in benchmark design.",
      "authors": [
        "Yunchong Huang",
        "Gianni Barlacchi",
        "Sandro Pezzelle"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 13:36:23+00:00",
      "link": "https://arxiv.org/pdf/2602.11938v4",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11937v1",
      "title": "Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration",
      "abstract": "Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality.",
      "authors": [
        "Akhiad Bercovich",
        "Nir Ailon",
        "Vladimir Anisimov",
        "Tomer Asida",
        "Nave Assaf",
        "Mohammad Dabbah",
        "Ido Galil",
        "Amnon Geifman",
        "Yonatan Geifman",
        "Izhak Golan",
        "Roi Koren",
        "Itay Levy",
        "Zach Moshe",
        "Pavlo Molchanov",
        "Najeeb Nabwani",
        "Mostofa Patwari",
        "Omri Puny",
        "Tomer Ronen",
        "Itamar Schen",
        "Elad Segal",
        "Ido Shahaf",
        "Oren Tropp",
        "Ran Zilberstein",
        "Ran El-Yaniv"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 13:36:19+00:00",
      "link": "https://arxiv.org/pdf/2602.11937v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11931v1",
      "title": "AdaptEvolve: Improving Efficiency of Evolutionary AI Agents through Adaptive Model Selection",
      "abstract": "Evolutionary agentic systems intensify the trade-off between computational efficiency and reasoning capability by repeatedly invoking large language models (LLMs) during inference. This setting raises a central question: how can an agent dynamically select an LLM that is sufficiently capable for the current generation step while remaining computationally efficient? While model cascades offer a practical mechanism for balancing this trade-off, existing routing strategies typically rely on static heuristics or external controllers and do not explicitly account for model uncertainty. We introduce AdaptEvolve: Adaptive LLM Selection for Multi-LLM Evolutionary Refinement within an evolutionary sequential refinement framework that leverages intrinsic generation confidence to estimate real-time solvability. Empirical results show that confidence-driven selection yields a favourable Pareto frontier, reducing total inference cost by an average of 37.9% across benchmarks while retaining 97.5% of the upper-bound accuracy of static large-model baselines. Our code is available at https://github.com/raypretam/adaptive_llm_selection.",
      "authors": [
        "Pretam Ray",
        "Pratik Prabhanjan Brahma",
        "Zicheng Liu",
        "Emad Barsoum"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-12 13:26:56+00:00",
      "link": "https://arxiv.org/pdf/2602.11931v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11920v1",
      "title": "Learning Conditional Averages",
      "abstract": "We introduce the problem of learning conditional averages in the PAC framework. The learner receives a sample labeled by an unknown target concept from a known concept class, as in standard PAC learning. However, instead of learning the target concept itself, the goal is to predict, for each instance, the average label over its neighborhood -- an arbitrary subset of points that contains the instance. In the degenerate case where all neighborhoods are singletons, the problem reduces exactly to classic PAC learning. More generally, it extends PAC learning to a setting that captures learning tasks arising in several domains, including explainability, fairness, and recommendation systems. Our main contribution is a complete characterization of when conditional averages are learnable, together with sample complexity bounds that are tight up to logarithmic factors. The characterization hinges on the joint finiteness of two novel combinatorial parameters, which depend on both the concept class and the neighborhood system, and are closely related to the independence number of the associated neighborhood graph.",
      "authors": [
        "Marco Bressan",
        "Nataly Brukhim",
        "Nicolo Cesa-Bianchi",
        "Emmanuel Esposito",
        "Yishay Mansour",
        "Shay Moran",
        "Maximilian Thiessen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-12 13:20:29+00:00",
      "link": "https://arxiv.org/pdf/2602.11920v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11917v1",
      "title": "AlphaPROBE: Alpha Mining via Principled Retrieval and On-graph biased evolution",
      "abstract": "Extracting signals through alpha factor mining is a fundamental challenge in quantitative finance. Existing automated methods primarily follow two paradigms: Decoupled Factor Generation, which treats factor discovery as isolated events, and Iterative Factor Evolution, which focuses on local parent-child refinements. However, both paradigms lack a global structural view, often treating factor pools as unstructured collections or fragmented chains, which leads to redundant search and limited diversity. To address these limitations, we introduce AlphaPROBE (Alpha Mining via Principled Retrieval and On-graph Biased Evolution), a framework that reframes alpha mining as the strategic navigation of a Directed Acyclic Graph (DAG). By modeling factors as nodes and evolutionary links as edges, AlphaPROBE treats the factor pool as a dynamic, interconnected ecosystem. The framework consists of two core components: a Bayesian Factor Retriever that identifies high-potential seeds by balancing exploitation and exploration through a posterior probability model, and a DAG-aware Factor Generator that leverages the full ancestral trace of factors to produce context-aware, nonredundant optimizations. Extensive experiments on three major Chinese stock market datasets against 8 competitive baselines demonstrate that AlphaPROBE significantly gains enhanced performance in predictive accuracy, return stability and training efficiency. Our results confirm that leveraging global evolutionary topology is essential for efficient and robust automated alpha discovery. We have open-sourced our implementation at https://github.com/gta0804/AlphaPROBE.",
      "authors": [
        "Taian Guo",
        "Haiyang Shen",
        "Junyu Luo",
        "Binqi Chen",
        "Hongjun Ding",
        "Jinsheng Huang",
        "Luchen Liu",
        "Yun Ma",
        "Ming Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 13:14:58+00:00",
      "link": "https://arxiv.org/pdf/2602.11917v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11908v2",
      "title": "When Should LLMs Be Less Specific? Selective Abstraction for Reliable Long-Form Text Generation",
      "abstract": "LLMs are widely used, yet they remain prone to factual errors that erode user trust and limit adoption in high-risk settings. One approach to mitigate this risk is to equip models with uncertainty estimation mechanisms that abstain when confidence is low. However, this binary \"all-or-nothing\" approach is excessively restrictive in long-form settings, often discarding valuable information. We introduce Selective Abstraction (SA), a framework that enables LLMs to trade specificity for reliability by selectively reducing the detail of uncertain content. We first formalize SA through the lenses of selective risk and coverage. We then propose Atom-wise Selective Abstraction, a claim-level instantiation that decomposes responses into atomic claims (short, self-contained statements each expressing a single fact) and replaces uncertain atoms with higher confidence, less specific abstractions. To evaluate this framework, we develop a novel end-to-end pipeline for open-ended generation that instantiates risk as factual correctness and measures coverage using an information-theoretic measure of retained information. Across six open-source models on the FactScore and LongFact-Objects benchmarks, atom-wise SA consistently outperforms existing baselines, improving the area under the risk-coverage curve (AURC) by up to 27.73% over claim removal, demonstrating that reducing specificity can boost accuracy and reliability while preserving most of their original meaning.",
      "authors": [
        "Shani Goren",
        "Ido Galil",
        "Ran El-Yaniv"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-12 13:06:14+00:00",
      "link": "https://arxiv.org/pdf/2602.11908v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11904v1",
      "title": "Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation",
      "abstract": "Software languages evolve over time for reasons such as feature additions. When grammars evolve, textual instances that originally conformed to them may become outdated. While model-driven engineering provides many techniques for co-evolving models with metamodel changes, these approaches are not designed for textual DSLs and may lose human-relevant information such as layout and comments. This study systematically evaluates the potential of large language models (LLMs) for co-evolving grammars and instances of textual DSLs. Using Claude Sonnet 4.5 and GPT-5.2 across ten case languages with ten runs each, we assess both correctness and preservation of human-oriented information. Results show strong performance on small-scale cases ($\\geq$94% precision and recall for instances requiring fewer than 20 modified lines), but performance degraded with scale: Claude maintains 85% recall at 40 lines, while GPT fails on the largest instances. Response time increases substantially with instance size, and grammar evolution complexity and deletion granularity affect performance more than change type. These findings clarify when LLM-based co-evolution is effective and where current limitations remain.",
      "authors": [
        "Weixing Zhang",
        "Bowen Jiang",
        "Yuhong Fu",
        "Anne Koziolek",
        "Regina Hebig",
        "Daniel Strüber"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published": "2026-02-12 13:01:01+00:00",
      "link": "https://arxiv.org/pdf/2602.11904v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11886v1",
      "title": "LLM-based Triplet Extraction from Financial Reports",
      "abstract": "Corporate financial reports are a valuable source of structured knowledge for Knowledge Graph construction, but the lack of annotated ground truth in this domain makes evaluation difficult. We present a semi-automated pipeline for Subject-Predicate-Object triplet extraction that uses ontology-driven proxy metrics, specifically Ontology Conformance and Faithfulness, instead of ground-truth-based evaluation. We compare a static, manually engineered ontology against a fully automated, document-specific ontology induction approach across different LLMs and two corporate annual reports. The automatically induced ontology achieves 100% schema conformance in all configurations, eliminating the ontology drift observed with the manual approach. We also propose a hybrid verification strategy that combines regex matching with an LLM-as-a-judge check, reducing apparent subject hallucination rates from 65.2% to 1.6% by filtering false positives caused by coreference resolution. Finally, we identify a systematic asymmetry between subject and object hallucinations, which we attribute to passive constructions and omitted agents in financial prose.",
      "authors": [
        "Dante Wesslund",
        "Ville Stenström",
        "Pontus Linde",
        "Alexander Holmberg"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 12:36:10+00:00",
      "link": "https://arxiv.org/pdf/2602.11886v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11881v1",
      "title": "From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders",
      "abstract": "Sparse autoencoders (SAEs) have proven effective for extracting monosemantic features from large language models (LLMs), yet these features are typically identified in isolation. However, broad evidence suggests that LLMs capture the intrinsic structure of natural language, where the phenomenon of \"feature splitting\" in particular indicates that such structure is hierarchical. To capture this, we propose the Hierarchical Sparse Autoencoder (HSAE), which jointly learns a series of SAEs and the parent-child relationships between their features. HSAE strengthens the alignment between parent and child features through two novel mechanisms: a structural constraint loss and a random feature perturbation mechanism. Extensive experiments across various LLMs and layers demonstrate that HSAE consistently recovers semantically meaningful hierarchies, supported by both qualitative case studies and rigorous quantitative metrics. At the same time, HSAE preserves the reconstruction fidelity and interpretability of standard SAEs across different dictionary sizes. Our work provides a powerful, scalable tool for discovering and analyzing the multi-scale conceptual structures embedded in LLM representations.",
      "authors": [
        "Yifan Luo",
        "Yang Zhan",
        "Jiedong Jiang",
        "Tianyang Liu",
        "Mingrui Wu",
        "Zhennan Zhou",
        "Bin Dong"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 12:30:23+00:00",
      "link": "https://arxiv.org/pdf/2602.11881v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11874v1",
      "title": "Efficient Crawling for Scalable Web Data Acquisition (Extended Version)",
      "abstract": "Journalistic fact-checking, as well as social or economic research, require analyzing high-quality statistics datasets (SDs, in short). However, retrieving SD corpora at scale may be hard, inefficient, or impossible, depending on how they are published online. To improve open statistics data accessibility, we present a focused Web crawling algorithm that retrieves as many targets, i.e., resources of certain types, as possible, from a given website, in an efficient and scalable way, by crawling (much) less than the full website. We show that optimally solving this problem is intractable, and propose an approach based on reinforcement learning, namely using sleeping bandits. We propose SB-CLASSIFIER, a crawler that efficiently learns which hyperlinks lead to pages that link to many targets, based on the paths leading to the links in their enclosing webpages. Our experiments on websites with millions of webpages show that our crawler is highly efficient, delivering high fractions of a site's targets while crawling only a small part.",
      "authors": [
        "Antoine Gauquier",
        "Ioana Manolescu",
        "Pierre Senellart"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-12 12:23:53+00:00",
      "link": "https://arxiv.org/pdf/2602.11874v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11871v1",
      "title": "DMAP: A Distribution Map for Text",
      "abstract": "Large Language Models (LLMs) are a powerful tool for statistical text analysis, with derived sequences of next-token probability distributions offering a wealth of information. Extracting this signal typically relies on metrics such as perplexity, which do not adequately account for context; how one should interpret a given next-token probability is dependent on the number of reasonable choices encoded by the shape of the conditional distribution. In this work, we present DMAP, a mathematically grounded method that maps a text, via a language model, to a set of samples in the unit interval that jointly encode rank and probability information. This representation enables efficient, model-agnostic analysis and supports a range of applications. We illustrate its utility through three case studies: (i) validation of generation parameters to ensure data integrity, (ii) examining the role of probability curvature in machine-generated text detection, and (iii) a forensic analysis revealing statistical fingerprints left in downstream models that have been subject to post-training on synthetic data. Our results demonstrate that DMAP offers a unified statistical view of text that is simple to compute on consumer hardware, widely applicable, and provides a foundation for further research into text analysis with LLMs.",
      "authors": [
        "Tom Kempton",
        "Julia Rozanova",
        "Parameswaran Kamalaruban",
        "Maeve Madigan",
        "Karolina Wresilo",
        "Yoann L. Launay",
        "David Sutton",
        "Stuart Burrell"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-12 12:21:24+00:00",
      "link": "https://arxiv.org/pdf/2602.11871v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11863v1",
      "title": "In-Context Function Learning in Large Language Models",
      "abstract": "Large language models (LLMs) can learn from a few demonstrations provided at inference time. We study this in-context learning phenomenon through the lens of Gaussian Processes (GPs). We build controlled experiments where models observe sequences of multivariate scalar-valued function samples drawn from known GP priors. We evaluate prediction error in relation to the number of demonstrations and compare against two principled references: (i) an empirical GP-regression learner that gives a lower bound on achievable error, and (ii) the expected error of a 1-nearest-neighbor (1-NN) rule, which gives a data-driven upper bound. Across model sizes, we find that LLM learning curves are strongly influenced by the function-generating kernels and approach the GP lower bound as the number of demonstrations increases. We then study the inductive biases of these models using a likelihood-based analysis. We find that LLM predictions are most likely under less smooth GP kernels. Finally, we explore whether post-training can shift these inductive biases and improve sample-efficiency on functions sampled from GPs with smoother kernels. We find that both reinforcement learning and supervised fine-tuning can effectively shift inductive biases in the direction of the training data. Together, our framework quantifies the extent to which LLMs behave like GP learners and provides tools for steering their inductive biases for continuous function learning tasks.",
      "authors": [
        "Elif Akata",
        "Konstantinos Voudouris",
        "Vincent Fortuin",
        "Eric Schulz"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 12:09:48+00:00",
      "link": "https://arxiv.org/pdf/2602.11863v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11861v1",
      "title": "A$^{2}$V-SLP: Alignment-Aware Variational Modeling for Disentangled Sign Language Production",
      "abstract": "Building upon recent structural disentanglement frameworks for sign language production, we propose A$^{2}$V-SLP, an alignment-aware variational framework that learns articulator-wise disentangled latent distributions rather than deterministic embeddings. A disentangled Variational Autoencoder (VAE) encodes ground-truth sign pose sequences and extracts articulator-specific mean and variance vectors, which are used as distributional supervision for training a non-autoregressive Transformer. Given text embeddings, the Transformer predicts both latent means and log-variances, while the VAE decoder reconstructs the final sign pose sequences through stochastic sampling at the decoding stage. This formulation maintains articulator-level representations by avoiding deterministic latent collapse through distributional latent modeling. In addition, we integrate a gloss attention mechanism to strengthen alignment between linguistic input and articulated motion. Experimental results show consistent gains over deterministic latent regression, achieving state-of-the-art back-translation performance and improved motion realism in a fully gloss-free setting.",
      "authors": [
        "Sümeyye Meryem Taşyürek",
        "Enis Mücahid İskender",
        "Hacer Yalim Keles"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-12 12:07:32+00:00",
      "link": "https://arxiv.org/pdf/2602.11861v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11843v1",
      "title": "Fast Evaluation of Truncated Neumann Series by Low-Product Radix Kernels",
      "abstract": "Truncated Neumann series $S_k(A)=I+A+\\cdots+A^{k-1}$ are used in   approximate matrix inversion and polynomial preconditioning. In dense   settings, matrix-matrix products dominate the cost of evaluating $S_k$.   Naive evaluation needs $k-1$ products, while splitting methods reduce this   to $O(\\log k)$. Repeated squaring, for example, uses $2\\log_2 k$   products, so further gains require higher-radix kernels that extend the   series by $m$ terms per update. Beyond the known radix-5 kernel, explicit   higher-radix constructions were not available, and the existence of exact   rational kernels was unclear.   We construct radix kernels for $T_m(B)=I+B+\\cdots+B^{m-1}$ and use them to   build faster series algorithms. For radix 9, we derive an exact 3-product   kernel with rational coefficients, which is the first exact construction   beyond radix 5. This kernel yields $5\\log_9 k=1.58\\log_2 k$ products, a   21% reduction from repeated squaring. For radix 15, numerical optimization   yields a 4-product kernel that matches the target through degree 14 but   has nonzero spillover (extra terms) at degrees $\\ge 15$. Because spillover   breaks the standard telescoping update, we introduce a residual-based   radix-kernel framework that accommodates approximate kernels and retains   coefficient $(μ_m+2)/\\log_2 m$. Within this framework, radix 15 attains   $6/\\log_2 15\\approx 1.54$, the best known asymptotic rate. Numerical   experiments support the predicted product-count savings and associated   runtime trends.",
      "authors": [
        "Piyush Sao"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA",
        "cs.MS"
      ],
      "published": "2026-02-12 11:37:31+00:00",
      "link": "https://arxiv.org/pdf/2602.11843v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11841v1",
      "title": "Improving Neural Retrieval with Attribution-Guided Query Rewriting",
      "abstract": "Neural retrievers are effective but brittle: underspecified or ambiguous queries can misdirect ranking even when relevant documents exist. Existing approaches address this brittleness only partially: LLMs rewrite queries without retriever feedback, and explainability methods identify misleading tokens but are used for post-hoc analysis. We close this loop and propose an attribution-guided query rewriting method that uses token-level explanations to guide query rewriting. For each query, we compute gradient-based token attributions from the retriever and then use these scores as soft guidance in a structured prompt to an LLM that clarifies weak or misleading query components while preserving intent. Evaluated on BEIR collections, the resulting rewrites consistently improve retrieval effectiveness over strong baselines, with larger gains for implicit or ambiguous information needs.",
      "authors": [
        "Moncef Garouani",
        "Josiane Mothe"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-12 11:34:06+00:00",
      "link": "https://arxiv.org/pdf/2602.11841v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11836v1",
      "title": "ULTRA:Urdu Language Transformer-based Recommendation Architecture",
      "abstract": "Urdu, as a low-resource language, lacks effective semantic content recommendation systems, particularly in the domain of personalized news retrieval. Existing approaches largely rely on lexical matching or language-agnostic techniques, which struggle to capture semantic intent and perform poorly under varying query lengths and information needs. This limitation results in reduced relevance and adaptability in Urdu content recommendation. We propose ULTRA (Urdu Language Transformer-based Recommendation Architecture),an adaptive semantic recommendation framework designed to address these challenges. ULTRA introduces a dual-embedding architecture with a query-length aware routing mechanism that dynamically distinguishes between short, intent-focused queries and longer, context-rich queries. Based on a threshold-driven decision process, user queries are routed to specialized semantic pipelines optimized for either title/headline-level or full-content/document level representations, ensuring appropriate semantic granularity during retrieval. The proposed system leverages transformer-based embeddings and optimized pooling strategies to move beyond surface-level keyword matching and enable context-aware similarity search. Extensive experiments conducted on a large-scale Urdu news corpus demonstrate that the proposed architecture consistently improves recommendation relevance across diverse query types. Results show gains in precision above 90% compared to single-pipeline baselines, highlighting the effectiveness of query-adaptive semantic alignment for low-resource languages. The findings establish ULTRA as a robust and generalizable content recommendation architecture, offering practical design insights for semantic retrieval systems in low-resource language settings.",
      "authors": [
        "Alishbah Bashir",
        "Fatima Qaiser",
        "Ijaz Hussain"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-12 11:26:46+00:00",
      "link": "https://arxiv.org/pdf/2602.11836v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11825v1",
      "title": "CAAL: Confidence-Aware Active Learning for Heteroscedastic Atmospheric Regression",
      "abstract": "Quantifying the impacts of air pollution on health and climate relies on key atmospheric particle properties such as toxicity and hygroscopicity. However, these properties typically require complex observational techniques or expensive particle-resolved numerical simulations, limiting the availability of labeled data. We therefore estimate these hard-to-measure particle properties from routinely available observations (e.g., air pollutant concentrations and meteorological conditions). Because routine observations only indirectly reflect particle composition and structure, the mapping from routine observations to particle properties is noisy and input-dependent, yielding a heteroscedastic regression setting. With a limited and costly labeling budget, the central challenge is to select which samples to measure or simulate. While active learning is a natural approach, most acquisition strategies rely on predictive uncertainty. Under heteroscedastic noise, this signal conflates reducible epistemic uncertainty with irreducible aleatoric uncertainty, causing limited budgets to be wasted in noise-dominated regions. To address this challenge, we propose a confidence-aware active learning framework (CAAL) for efficient and robust sample selection in heteroscedastic settings. CAAL consists of two components: a decoupled uncertainty-aware training objective that separately optimises the predictive mean and noise level to stabilise uncertainty estimation, and a confidence-aware acquisition function that dynamically weights epistemic uncertainty using predicted aleatoric uncertainty as a reliability signal. Experiments on particle-resolved numerical simulations and real atmospheric observations show that CAAL consistently outperforms standard AL baselines. The proposed framework provides a practical and general solution for the efficient expansion of high-cost atmospheric particle property databases.",
      "authors": [
        "Fei Jiang",
        "Jiyang Xia",
        "Junjie Yu",
        "Mingfei Sun",
        "Hugh Coe",
        "David Topping",
        "Dantong Liu",
        "Zhenhui Jessie Li",
        "Zhonghua Zheng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "physics.ao-ph"
      ],
      "published": "2026-02-12 11:09:58+00:00",
      "link": "https://arxiv.org/pdf/2602.11825v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11812v1",
      "title": "Predicting LLM Output Length via Entropy-Guided Representations",
      "abstract": "The long-tailed distribution of sequence lengths in LLM serving and reinforcement learning (RL) sampling causes significant computational waste due to excessive padding in batched inference. Existing methods rely on auxiliary models for static length prediction, but they incur high overhead, generalize poorly, and fail in stochastic \"one-to-many\" sampling scenarios. We introduce a lightweight framework that reuses the main model's internal hidden states for efficient length prediction. Our framework features two core components: 1) Entropy-Guided Token Pooling (EGTP), which uses on-the-fly activations and token entropy for highly accurate static prediction with negligible cost, and 2) Progressive Length Prediction (PLP), which dynamically estimates the remaining length at each decoding step to handle stochastic generation. To validate our approach, we build and release ForeLen, a comprehensive benchmark with long-sequence, Chain-of-Thought, and RL data. On ForeLen, EGTP achieves state-of-the-art accuracy, reducing MAE by 29.16\\% over the best baseline. Integrating our methods with a length-aware scheduler yields significant end-to-end throughput gains. Our work provides a new technical and evaluation baseline for efficient LLM inference.",
      "authors": [
        "Huanyi Xie",
        "Yubin Chen",
        "Liangyu Wang",
        "Lijie Hu",
        "Di Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 10:49:04+00:00",
      "link": "https://arxiv.org/pdf/2602.11812v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11808v1",
      "title": "Deep Kernel Fusion for Transformers",
      "abstract": "Agentic LLM inference with long contexts is increasingly limited by memory bandwidth rather than compute. In this setting, SwiGLU MLP blocks, whose large weights exceed cache capacity, become a major yet under-optimized bottleneck. We propose DeepFusionKernel, a deeply fused kernel that cuts HBM traffic and boosts cache reuse, delivering up to 13.2% speedup on H100 and 9.7% on A100 over SGLang. Integrated with SGLang and paired with a kernel scheduler, DeepFusionKernel ensures consistent accelerations over generation lengths, while remaining adaptable to diverse models, inference configurations, and hardware platforms.",
      "authors": [
        "Zixi Zhang",
        "Zhiwen Mo",
        "Yiren Zhao",
        "Robert Mullins"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 10:43:59+00:00",
      "link": "https://arxiv.org/pdf/2602.11808v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11800v1",
      "title": "Temporal Difference Learning with Constrained Initial Representations",
      "abstract": "Recently, there have been numerous attempts to enhance the sample efficiency of off-policy reinforcement learning (RL) agents when interacting with the environment, including architecture improvements and new algorithms. Despite these advances, they overlook the potential of directly constraining the initial representations of the input data, which can intuitively alleviate the distribution shift issue and stabilize training. In this paper, we introduce the Tanh function into the initial layer to fulfill such a constraint. We theoretically unpack the convergence property of the temporal difference learning with the Tanh function under linear function approximation. Motivated by theoretical insights, we present our Constrained Initial Representations framework, tagged CIR, which is made up of three components: (i) the Tanh activation along with normalization methods to stabilize representations; (ii) the skip connection module to provide a linear pathway from the shallow layer to the deep layer; (iii) the convex Q-learning that allows a more flexible value estimate and mitigates potential conservatism. Empirical results show that CIR exhibits strong performance on numerous continuous control tasks, even being competitive or surpassing existing strong baseline methods.",
      "authors": [
        "Jiafei Lyu",
        "Jingwen Yang",
        "Zhongjian Qiao",
        "Runze Liu",
        "Zeyuan Liu",
        "Deheng Ye",
        "Zongqing Lu",
        "Xiu Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 10:27:57+00:00",
      "link": "https://arxiv.org/pdf/2602.11800v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11795v1",
      "title": "A Subword Embedding Approach for Variation Detection in Luxembourgish User Comments",
      "abstract": "This paper presents an embedding-based approach to detecting variation without relying on prior normalisation or predefined variant lists. The method trains subword embeddings on raw text and groups related forms through combined cosine and n-gram similarity. This allows spelling and morphological diversity to be examined and analysed as linguistic structure rather than treated as noise. Using a large corpus of Luxembourgish user comments, the approach uncovers extensive lexical and orthographic variation that aligns with patterns described in dialectal and sociolinguistic research. The induced families capture systematic correspondences and highlight areas of regional and stylistic differentiation. The procedure does not strictly require manual annotation, but does produce transparent clusters that support both quantitative and qualitative analysis. The results demonstrate that distributional modelling can reveal meaningful patterns of variation even in ''noisy'' or low-resource settings, offering a reproducible methodological framework for studying language variety in multilingual and small-language contexts.",
      "authors": [
        "Anne-Marie Lutgen",
        "Alistair Plum",
        "Christoph Purschke"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 10:19:50+00:00",
      "link": "https://arxiv.org/pdf/2602.11795v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11794v1",
      "title": "Latent-Variable Learning of SPDEs via Wiener Chaos",
      "abstract": "We study the problem of learning the law of linear stochastic partial differential equations (SPDEs) with additive Gaussian forcing from spatiotemporal observations. Most existing deep learning approaches either assume access to the driving noise or initial condition, or rely on deterministic surrogate models that fail to capture intrinsic stochasticity. We propose a structured latent-variable formulation that requires only observations of solution realizations and learns the underlying randomly forced dynamics. Our approach combines a spectral Galerkin projection with a truncated Wiener chaos expansion, yielding a principled separation between deterministic evolution and stochastic forcing. This reduces the infinite-dimensional SPDE to a finite system of parametrized ordinary differential equations governing latent temporal dynamics. The latent dynamics and stochastic forcing are jointly inferred through variational learning, allowing recovery of stochastic structure without explicit observation or simulation of noise during training. Empirical evaluation on synthetic data demonstrates state-of-the-art performance under comparable modeling assumptions across bounded and unbounded one-dimensional spatial domains.",
      "authors": [
        "Sebastian Zeng",
        "Andreas Petersson",
        "Wolfgang Bock"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 10:19:43+00:00",
      "link": "https://arxiv.org/pdf/2602.11794v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11792v1",
      "title": "Detecting RLVR Training Data via Structural Convergence of Reasoning",
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-$k$NN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the $k$ smallest nearest-neighbor edit distances. Min-$k$NN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-$k$NN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.",
      "authors": [
        "Hongbo Zhang",
        "Yue Yang",
        "Jianhao Yan",
        "Guangsheng Bao",
        "Yue Zhang",
        "Yue Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-12 10:17:32+00:00",
      "link": "https://arxiv.org/pdf/2602.11792v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11789v1",
      "title": "Decentralized Non-convex Stochastic Optimization with Heterogeneous Variance",
      "abstract": "Decentralized optimization is critical for solving large-scale machine learning problems over distributed networks, where multiple nodes collaborate through local communication. In practice, the variances of stochastic gradient estimators often differ across nodes, yet their impact on algorithm design and complexity remains unclear. To address this issue, we propose D-NSS, a decentralized algorithm with node-specific sampling, and establish its sample complexity depending on the arithmetic mean of local standard deviations, achieving tighter bounds than existing methods that rely on the worst-case or quadratic mean. We further derive a matching sample complexity lower bound under heterogeneous variance, thereby proving the optimality of this dependence. Moreover, we extend the framework with a variance reduction technique and develop D-NSS-VR, which under the mean-squared smoothness assumption attains an improved sample complexity bound while preserving the arithmetic-mean dependence. Finally, numerical experiments validate the theoretical results and demonstrate the effectiveness of the proposed algorithms.",
      "authors": [
        "Hongxu Chen",
        "Ke Wei",
        "Luo Luo"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "published": "2026-02-12 10:13:53+00:00",
      "link": "https://arxiv.org/pdf/2602.11789v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11767v1",
      "title": "TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents",
      "abstract": "Advances in large language models (LLMs) are driving a shift toward using reinforcement learning (RL) to train agents from iterative, multi-turn interactions across tasks. However, multi-turn RL remains challenging as rewards are often sparse or delayed, and environments can be stochastic. In this regime, naive trajectory sampling can hinder exploitation and induce mode collapse. We propose TSR (Trajectory-Search Rollouts), a training-time approach that repurposes test-time scaling ideas for improved per-turn rollout generation. TSR performs lightweight tree-style search to construct high-quality trajectories by selecting high-scoring actions at each turn using task-specific feedback. This improves rollout quality and stabilizes learning while leaving the underlying optimization objective unchanged, making TSR optimizer-agnostic. We instantiate TSR with best-of-N, beam, and shallow lookahead search, and pair it with PPO and GRPO, achieving up to 15% performance gains and more stable learning on Sokoban, FrozenLake, and WebShop tasks at a one-time increase in training compute. By moving search from inference time to the rollout stage of training, TSR provides a simple and general mechanism for stronger multi-turn agent learning, complementary to existing frameworks and rejection-sampling-style selection methods.",
      "authors": [
        "Aladin Djuhera",
        "Swanand Ravindra Kadhe",
        "Farhan Ahmed",
        "Holger Boche"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-12 09:49:24+00:00",
      "link": "https://arxiv.org/pdf/2602.11767v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11761v1",
      "title": "MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling",
      "abstract": "The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involve a trade-off between memory efficiency and model performance. This paper introduces MiniCPM-SALA, a 9B-parameter hybrid architecture that integrates the high-fidelity long-context modeling of sparse attention (InfLLM-V2) with the global efficiency of linear attention (Lightning Attention). By employing a layer selection algorithm to integrate these mechanisms in a 1:3 ratio and utilizing a hybrid positional encoding (HyPE), the model maintains efficiency and performance for long-context tasks. Furthermore, we introduce a cost-effective continual training framework that transforms pre-trained Transformer-based models into hybrid models, which reduces training costs by approximately 75% compared to training from scratch. Extensive experiments show that MiniCPM-SALA maintains general capabilities comparable to full-attention models while offering improved efficiency. On a single NVIDIA A6000D GPU, the model achieves up to 3.5x the inference speed of the full-attention model at the sequence length of 256K tokens and supports context lengths of up to 1M tokens, a scale where traditional full-attention 8B models fail because of memory constraints.",
      "authors": [
        "MiniCPM Team",
        "Wenhao An",
        "Yingfa Chen",
        "Yewei Fang",
        "Jiayi Li",
        "Xin Li",
        "Yaohui Li",
        "Yishan Li",
        "Yuxuan Li",
        "Biyuan Lin",
        "Chuan Liu",
        "Hezi Liu",
        "Siyuan Liu",
        "Hongya Lyu",
        "Yinxu Pan",
        "Shixin Ren",
        "Xingyu Shen",
        "Zhou Su",
        "Haojun Sun",
        "Yangang Sun",
        "Zhen Leng Thai",
        "Xin Tian",
        "Rui Wang",
        "Xiaorong Wang",
        "Yudong Wang",
        "Bo Wu",
        "Xiaoyue Xu",
        "Dong Xu",
        "Shuaikang Xue",
        "Jiawei Yang",
        "Bowen Zhang",
        "Jinqian Zhang",
        "Letian Zhang",
        "Shengnan Zhang",
        "Xinyu Zhang",
        "Xinyuan Zhang",
        "Zhu Zhang",
        "Hengyu Zhao",
        "Jiacheng Zhao",
        "Jie Zhou",
        "Zihan Zhou",
        "Shuo Wang",
        "Chaojun Xiao",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-12 09:37:05+00:00",
      "link": "https://arxiv.org/pdf/2602.11761v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11760v1",
      "title": "Aggregate Models, Not Explanations: Improving Feature Importance Estimation",
      "abstract": "Feature-importance methods show promise in transforming machine learning models from predictive engines into tools for scientific discovery. However, due to data sampling and algorithmic stochasticity, expressive models can be unstable, leading to inaccurate variable importance estimates and undermining their utility in critical biomedical applications. Although ensembling offers a solution, deciding whether to explain a single ensemble model or aggregate individual model explanations is difficult due to the nonlinearity of importance measures and remains largely understudied. Our theoretical analysis, developed under assumptions accommodating complex state-of-the-art ML models, reveals that this choice is primarily driven by the model's excess risk. In contrast to prior literature, we show that ensembling at the model level provides more accurate variable-importance estimates, particularly for expressive models, by reducing this leading error term. We validate these findings on classical benchmarks and a large-scale proteomic study from the UK Biobank.",
      "authors": [
        "Joseph Paillard",
        "Angel Reyero Lobo",
        "Denis A. Engemann",
        "Bertrand Thirion"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-12 09:36:03+00:00",
      "link": "https://arxiv.org/pdf/2602.11760v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11756v1",
      "title": "Towards a theory of Façade-X data access: satisfiability of SPARQL basic graph patterns",
      "abstract": "Data integration is the primary use case for knowledge graphs. However, integrated data are not typically graphs but come in different formats, for example, CSV, XML, or a relational database. Façade-X is a recently proposed method for providing direct access to an open-ended set of data formats. The method includes a meta-model that specialises RDF to fit general data structures. This model allows to express SPARQL queries targeting data sources with those structures. Previous work formalised Façade-X and demonstrated how it can theoretically represent any format expressible with a context-free grammar, as well as the relational model. A reference implementation, SPARQL Anything, demonstrates the feasibility of the approach in practice. It is noteworthy that Façade-X utilises a fraction of RDF, and, consequently, not all SPARQL queries yield a solution (i.e. are satisfiable) when evaluated over a Façade-X graph. In this article, we consolidate Façade-X, and we study the satisfiability of basic graph patterns. The theory is accompanied by an algorithm for deciding the satisfiability of basic graph patterns on Façade-X data sources. Furthermore, we provide extensive experiments with a proof-of-concept implementation, demonstrating practical feasibility, including with real-world queries. Our results pave the way for studying query execution strategies for Façade-X data access with SPARQL and supporting developers to build more efficient data integration systems for knowledge graphs.",
      "authors": [
        "Luigi Asprino",
        "Enrico Daga"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-12 09:32:53+00:00",
      "link": "https://arxiv.org/pdf/2602.11756v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12937v2",
      "title": "Curriculum Learning and Pseudo-Labeling Improve the Generalization of Multi-Label Arabic Dialect Identification Models",
      "abstract": "Being modeled as a single-label classification task for a long time, recent work has argued that Arabic Dialect Identification (ADI) should be framed as a multi-label classification task. However, ADI remains constrained by the availability of single-label datasets, with no large-scale multi-label resources available for training. By analyzing models trained on single-label ADI data, we show that the main difficulty in repurposing such datasets for Multi-Label Arabic Dialect Identification (MLADI) lies in the selection of negative samples, as many sentences treated as negative could be acceptable in multiple dialects. To address these issues, we construct a multi-label dataset by generating automatic multi-label annotations using GPT-4o and binary dialect acceptability classifiers, with aggregation guided by the Arabic Level of Dialectness (ALDi). Afterward, we train a BERT-based multi-label classifier using curriculum learning strategies aligned with dialectal complexity and label cardinality. On the MLADI leaderboard, our best-performing LAHJATBERT model achieves a macro F1 of 0.69, compared to 0.55 for the strongest previously reported system. Code and data are available at https://mohamedalaa9.github.io/lahjatbert/.",
      "authors": [
        "Ali Mekky",
        "Mohamed El Zeftawy",
        "Lara Hassan",
        "Amr Keleg",
        "Preslav Nakov"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-12 09:30:55+00:00",
      "link": "https://arxiv.org/pdf/2602.12937v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11745v1",
      "title": "Text2GQL-Bench: A Text to Graph Query Language Benchmark [Experiment, Analysis & Benchmark]",
      "abstract": "Graph models are fundamental to data analysis in domains rich with complex relationships. Text-to-Graph-Query-Language (Text-to-GQL) systems act as a translator, converting natural language into executable graph queries. This capability allows Large Language Models (LLMs) to directly analyze and manipulate graph data, posi-tioning them as powerful agent infrastructures for Graph Database Management System (GDBMS). Despite recent progress, existing datasets are often limited in domain coverage, supported graph query languages, or evaluation scope. The advancement of Text-to-GQL systems is hindered by the lack of high-quality benchmark datasets and evaluation methods to systematically compare model capabilities across different graph query languages and domains. In this work, we present Text2GQL-Bench, a unified Text-to-GQL benchmark designed to address these limitations. Text2GQL-Bench couples a multi-GQL dataset that has 178,184 (Question, Query) pairs spanning 13 domains, with a scalable construction framework that generates datasets in different domains, question abstraction levels, and GQLs with heterogeneous resources. To support compre-hensive assessment, we introduce an evaluation method that goes beyond a single end-to-end metric by jointly reporting grammatical validity, similarity, semantic alignment, and execution accuracy. Our evaluation uncovers a stark dialect gap in ISO-GQL generation: even strong LLMs achieve only at most 4% execution accuracy (EX) in zero-shot settings, though a fixed 3-shot prompt raises accuracy to around 50%, the grammatical validity remains lower than 70%. Moreover, a fine-tuned 8B open-weight model reaches 45.1% EX, and 90.8% grammatical validity, demonstrating that most of the performance jump is unlocked by exposure to sufficient ISO-GQL examples.",
      "authors": [
        "Songlin Lyu",
        "Lujie Ban",
        "Zihang Wu",
        "Tianqi Luo",
        "Jirong Liu",
        "Chenhao Ma",
        "Yuyu Luo",
        "Nan Tang",
        "Shipeng Qi",
        "Heng Lin",
        "Yongchao Liu",
        "Chuntao Hong"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 09:16:44+00:00",
      "link": "https://arxiv.org/pdf/2602.11745v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11738v1",
      "title": "U-Former ODE: Fast Probabilistic Forecasting of Irregular Time Series",
      "abstract": "Probabilistic forecasting of irregularly sampled time series is crucial in domains such as healthcare and finance, yet it remains a formidable challenge. Existing Neural Controlled Differential Equation (Neural CDE) approaches, while effective at modelling continuous dynamics, suffer from slow, inherently sequential computation, which restricts scalability and limits access to global context. We introduce UFO (U-Former ODE), a novel architecture that seamlessly integrates the parallelizable, multiscale feature extraction of U-Nets, the powerful global modelling of Transformers, and the continuous-time dynamics of Neural CDEs. By constructing a fully causal, parallelizable model, UFO achieves a global receptive field while retaining strong sensitivity to local temporal dynamics. Extensive experiments on five standard benchmarks -- covering both regularly and irregularly sampled time series -- demonstrate that UFO consistently outperforms ten state-of-the-art neural baselines in predictive accuracy. Moreover, UFO delivers up to 15$\\times$ faster inference compared to conventional Neural CDEs, with consistently strong performance on long and highly multivariate sequences.",
      "authors": [
        "Ilya Kuleshov",
        "Alexander Marusov",
        "Alexey Zaytsev"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 09:05:09+00:00",
      "link": "https://arxiv.org/pdf/2602.11738v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11730v1",
      "title": "STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning",
      "abstract": "In vision-language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial-temporal video grounding (STVG). Prior approaches typically focus on enhancing visual-textual alignment or attaching auxiliary decoders. However, these strategies inevitably introduce additional trainable modules, leading to significant annotation costs and computational overhead. In this work, we propose a novel visual prompting paradigm that avoids the difficult problem of aligning coordinates across modalities. Specifically, we reformulate per-frame coordinate prediction as a compact instance-level identification problem by assigning each object a unique, temporally consistent ID. These IDs are embedded into the video as visual prompts, providing explicit and interpretable inputs to the VLMs. Furthermore, we introduce STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization. Extensive experiments on six benchmarks demonstrate the effectiveness of our approach. STVG-R1 surpasses the baseline Qwen2.5-VL-7B by a remarkable margin of 20.9% on m_IoU on the HCSTVG-v2 benchmark, establishing a new state of the art (SOTA). Surprisingly, STVG-R1 also exhibits strong zero-shot generalization to multi-object referring video object segmentation tasks, achieving a SOTA 47.3% J&F on MeViS.",
      "authors": [
        "Xiaowen Zhang",
        "Zhi Gao",
        "Licheng Jiao",
        "Lingling Li",
        "Qing Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-12 08:53:32+00:00",
      "link": "https://arxiv.org/pdf/2602.11730v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11722v1",
      "title": "PAC-Bayesian Generalization Guarantees for Fairness on Stochastic and Deterministic Classifiers",
      "abstract": "Classical PAC generalization bounds on the prediction risk of a classifier are insufficient to provide theoretical guarantees on fairness when the goal is to learn models balancing predictive risk and fairness constraints. We propose a PAC-Bayesian framework for deriving generalization bounds for fairness, covering both stochastic and deterministic classifiers. For stochastic classifiers, we derive a fairness bound using standard PAC-Bayes techniques. Whereas for deterministic classifiers, as usual PAC-Bayes arguments do not apply directly, we leverage a recent advance in PAC-Bayes to extend the fairness bound beyond the stochastic setting. Our framework has two advantages: (i) It applies to a broad class of fairness measures that can be expressed as a risk discrepancy, and (ii) it leads to a self-bounding algorithm in which the learning procedure directly optimizes a trade-off between generalization bounds on the prediction risk and on the fairness. We empirically evaluate our framework with three classical fairness measures, demonstrating not only its usefulness but also the tightness of our bounds.",
      "authors": [
        "Julien Bastian",
        "Benjamin Leblanc",
        "Pascal Germain",
        "Amaury Habrard",
        "Christine Largeron",
        "Guillaume Metzler",
        "Emilie Morvant",
        "Paul Viallard"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-12 08:49:34+00:00",
      "link": "https://arxiv.org/pdf/2602.11722v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11719v1",
      "title": "Uncertainty-aware Generative Recommendation",
      "abstract": "Generative Recommendation has emerged as a transformative paradigm, reformulating recommendation as an end-to-end autoregressive sequence generation task. Despite its promise, existing preference optimization methods typically rely on binary outcome correctness, suffering from a systemic limitation we term uncertainty blindness. This issue manifests in the neglect of the model's intrinsic generation confidence, the variation in sample learning difficulty, and the lack of explicit confidence expression, directly leading to unstable training dynamics and unquantifiable decision risks. In this paper, we propose Uncertainty-aware Generative Recommendation (UGR), a unified framework that leverages uncertainty as a critical signal for adaptive optimization. UGR synergizes three mechanisms: (1) an uncertainty-weighted reward to penalize confident errors; (2) difficulty-aware optimization dynamics to prevent premature convergence; and (3) explicit confidence alignment to empower the model with confidence expression capabilities. Extensive experiments demonstrate that UGR not only yields superior recommendation performance but also fundamentally stabilizes training, preventing the performance degradation often observed in standard methods. Furthermore, the learned confidence enables reliable downstream risk-aware applications.",
      "authors": [
        "Chenxiao Fan",
        "Chongming Gao",
        "Yaxin Gong",
        "Haoyan Liu",
        "Fuli Feng",
        "Xiangnan He"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-12 08:48:51+00:00",
      "link": "https://arxiv.org/pdf/2602.11719v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11717v1",
      "title": "Beyond Parameter Arithmetic: Sparse Complementary Fusion for Distribution-Aware Model Merging",
      "abstract": "Model merging has emerged as a promising paradigm for composing the capabilities of large language models by directly operating in weight space, enabling the integration of specialized models without costly retraining. However, existing merging methods largely rely on parameter-space heuristics, which often introduce severe interference, leading to degraded generalization and unstable generation behaviors such as repetition and incoherent outputs. In this work, we propose Sparse Complementary Fusion with reverse KL (SCF-RKL), a novel model merging framework that explicitly controls functional interference through sparse, distribution-aware updates. Instead of assuming linear additivity in parameter space, SCF-RKL measures the functional divergence between models using reverse Kullback-Leibler divergence and selectively incorporates complementary parameters. This mode-seeking, sparsity-inducing design effectively preserves stable representations while integrating new capabilities. We evaluate SCF-RKL across a wide range of model scales and architectures, covering both reasoning-focused and instruction-tuned models. Extensive experiments on 24 benchmarks spanning advanced reasoning, general reasoning and knowledge, instruction following, and safety demonstrate, vision classification that SCF-RKL consistently outperforms existing model merging methods while maintaining strong generalization and generation stability.",
      "authors": [
        "Weihong Lin",
        "Lin Sun",
        "Qilong Shi",
        "Aomufei Yuan",
        "Yuxuan Tian",
        "Zhengyang Wang",
        "Guangxiang Zhao",
        "Xiangzheng Zhang",
        "Tong Yang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 08:45:42+00:00",
      "link": "https://arxiv.org/pdf/2602.11717v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11715v1",
      "title": "DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels",
      "abstract": "Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation. This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation, spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation.",
      "authors": [
        "Haolei Bai",
        "Lingcheng Kong",
        "Xueyi Chen",
        "Jianmian Wang",
        "Zhiqiang Tao",
        "Huan Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-12 08:45:13+00:00",
      "link": "https://arxiv.org/pdf/2602.11715v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11700v1",
      "title": "TabSieve: Explicit In-Table Evidence Selection for Tabular Prediction",
      "abstract": "Tabular prediction can benefit from in-table rows as few-shot evidence, yet existing tabular models typically perform instance-wise inference and LLM-based prompting is often brittle. Models do not consistently leverage relevant rows, and noisy context can degrade performance. To address this challenge, we propose TabSieve, a select-then-predict framework that makes evidence usage explicit and auditable. Given a table and a query row, TabSieve first selects a small set of informative rows as evidence and then predicts the missing target conditioned on the selected evidence. To enable this capability, we construct TabSieve-SFT-40K by synthesizing high-quality reasoning trajectories from 331 real tables using a strong teacher model with strict filtering. Furthermore, we introduce TAB-GRPO, a reinforcement learning recipe that jointly optimizes evidence selection and prediction correctness with separate rewards, and stabilizes mixed regression and classification training via dynamic task-advantage balancing. Experiments on a held-out benchmark of 75 classification and 52 regression tables show that TabSieve consistently improves performance across shot budgets, with average gains of 2.92% on classification and 4.45% on regression over the second-best baseline. Further analysis indicates that TabSieve concentrates more attention on the selected evidence, which improves robustness to noisy context.",
      "authors": [
        "Yongyao Wang",
        "Ziqi Miao",
        "Lu Yang",
        "Haonan Jia",
        "Wenting Yan",
        "Chen Qian",
        "Lijun Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 08:28:58+00:00",
      "link": "https://arxiv.org/pdf/2602.11700v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11698v1",
      "title": "SpiralFormer: Looped Transformers Can Learn Hierarchical Dependencies via Multi-Resolution Recursion",
      "abstract": "Recursive (looped) Transformers decouple computational depth from parameter depth by repeatedly applying shared layers, providing an explicit architectural primitive for iterative refinement and latent reasoning. However, early looped Transformers often underperform non-recursive baselines of equal compute. While recent literature has introduced more effective recursion mechanisms to mitigate this gap, existing architectures still operate at a fixed, full-token resolution, neglecting the potential efficiency of computing over compressed latent representations. In this paper, we propose SpiralFormer, a looped Transformer that executes recurrence under a multi-resolution recursion schedule. We provide probing evidence that multi-resolution recursion enables the model to learn hierarchical dependencies by inducing iteration-wise functional specialization across different scales. Empirically, SpiralFormer achieves better parameter and compute efficiency than both looped and non-looped baselines across model scales from 160M to 1.4B, establishing sequence resolution as a potential axis for scaling recursive architectures.",
      "authors": [
        "Chengting Yu",
        "Xiaobo Shu",
        "Yadao Wang",
        "Yizhen Zhang",
        "Haoyi Wu",
        "You Wu",
        "Rujiao Long",
        "Ziheng Chen",
        "Yuchi Xu",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 08:23:21+00:00",
      "link": "https://arxiv.org/pdf/2602.11698v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11690v1",
      "title": "ANML: Attribution-Native Machine Learning with Guaranteed Robustness",
      "abstract": "Frontier AI systems increasingly train on specialized expert data, from clinical records to proprietary research to curated datasets, yet current training pipelines treat all samples identically. A Nobel laureate's contribution receives the same weight as an unverified submission. We introduce ANML (Attribution-Native Machine Learning), a framework that weights training samples by four quality factors: gradient-based consistency (q), verification status (v), contributor reputation (r), and temporal relevance (T). By combining what the model observes (gradient signals) with what the system knows about data provenance (external signals), ANML produces per-contributor quality weights that simultaneously improve model performance and enable downstream attribution. Across 5 datasets (178-32,561 samples), ANML achieves 33-72% error reduction over gradient-only baselines. Quality-weighted training is data-efficient: 20% high-quality data outperforms 100% uniformly weighted data by 47%. A Two-Stage Adaptive gating mechanism guarantees that ANML never underperforms the best available baseline, including under strategic joint attacks combining credential faking with gradient alignment. When per-sample detection fails against subtle corruption, contributor-level attribution provides 1.3-5.3x greater improvement than sample-level methods, with the advantage growing as corruption becomes harder to detect.",
      "authors": [
        "Oliver Zahn",
        "Matt Beton",
        "Simran Chana"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 08:12:30+00:00",
      "link": "https://arxiv.org/pdf/2602.11690v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11685v1",
      "title": "DRACO: a Cross-Domain Benchmark for Deep Research Accuracy, Completeness, and Objectivity",
      "abstract": "We present DRACO (Deep Research Accuracy, Completeness, and Objectivity), a benchmark of complex deep research tasks. These tasks, which span 10 domains and draw on information sources from 40 countries, originate from anonymized real-world usage patterns within a large-scale deep research system. Tasks are sampled from a de-identified dataset of Perplexity Deep Research requests, then filtered and augmented to ensure that the tasks are anonymized, open-ended and complex, objectively evaluable, and representative of the broad scope of real-world deep research use cases. Outputs are graded against task-specific rubrics along four dimensions: factual accuracy (accuracy), breadth and depth of analysis (including completeness), presentation quality (including objectivity), and citation quality. DRACO is publicly available at https://hf.co/datasets/perplexity-ai/draco.",
      "authors": [
        "Joey Zhong",
        "Hao Zhang",
        "Clare Southern",
        "Jeremy Yang",
        "Thomas Wang",
        "Kate Jung",
        "Shu Zhang",
        "Denis Yarats",
        "Johnny Ho",
        "Jerry Ma"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 08:07:25+00:00",
      "link": "https://arxiv.org/pdf/2602.11685v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11683v1",
      "title": "ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces",
      "abstract": "Recent work explores latent reasoning to improve reasoning efficiency by replacing explicit reasoning trajectories with continuous representations in a latent space, yet its effectiveness varies across settings. Analysis of model confidence dynamics under latent reasoning reveals that thinking trajectories ending in incorrect answers contain fewer low-confidence steps than those ending in correct answers. Meanwhile, we suggest that soft embeddings aggregated by multiple low-confidence thinking alternatives may introduce and propagate noise, leading to high confidence in unreliable reasoning trajectories. Motivated by these observations, ThinkRouter, an inference-time confidence-aware routing mechanism is proposed to avoid high confidence and noise for efficient reasoning. ThinkRouter routes thinking to the discrete token space when model confidence is low, and to the latent space otherwise. Extensive experiments on STEM reasoning and coding benchmarks across diverse large reasoning models demonstrate that ThinkRouter outperforms explicit CoT, random routing, and latent reasoning baselines in terms of accuracy, achieving an average improvement of 19.70 points in Pass@1, while reducing generation length by up to 15.55%. Further comprehensive analysis reveals that ThinkRouter can calibrate errors arising from explicit CoT and latent reasoning, and accelerates end-of-thinking token generation by globally lowering model confidence.",
      "authors": [
        "Xin Xu",
        "Tong Yu",
        "Xiang Chen",
        "Haoliang Wang",
        "Julian McAuley",
        "Saayan Mitra"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-12 08:01:01+00:00",
      "link": "https://arxiv.org/pdf/2602.11683v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11675v1",
      "title": "Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs",
      "abstract": "Machine learning systems that are \"right for the wrong reasons\" achieve high performance through shortcuts that collapse under distributional shift. We show this pathology has a precise causal origin: autoregressive training provides no gradient signal to distinguish association P(Y|X) from intervention P(Y|do(X)), a failure we formalize as Rung Collapse. When outcome-based learning reinforces correct answers obtained through incorrect causal models, the agent becomes entrenched in flawed reasoning, a phenomenon we term Aleatoric Entrenchment. We propose Epistemic Regret Minimization (ERM), a belief revision objective that penalizes errors in causal reasoning independently of task success, and embed it within a three-layer architecture with three contributions grounded in knowledge representation: (1) a Physical Grounding Theorem proving that actions satisfying actuator independence implement valid do-operations, bridging action languages and do-calculus; (2) ERM as a causal belief revision operator satisfying AGM postulates, preventing entrenchment even when the agent succeeds for the wrong reasons; and (3) a failure mode taxonomy that classifies recurring reasoning errors and injects domain-independent guards, enabling cross-domain transfer. We prove asymptotic recovery of the true interventional distribution with finite-sample bounds. Experiments on 1,360 causal trap scenarios across six frontier LLMs reveal that Rung Collapse persists even in reasoning-enhanced models (3.7% for GPT-5.2), that steerability exhibits inverse scaling where advanced models resist generic correction, and that targeted ERM feedback recovers 53-59% of entrenched errors where outcome-level feedback fails.",
      "authors": [
        "Edward Y. Chang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 07:48:21+00:00",
      "link": "https://arxiv.org/pdf/2602.11675v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11674v1",
      "title": "Benchmark Health Index: A Systematic Framework for Benchmarking the Benchmarks of LLMs",
      "abstract": "Large Language Models (LLMs) are advancing rapidly, yet the benchmarks used to measure this progress are becoming increasingly unreliable. Score inflation and selective reporting have eroded the authority of standard benchmarks, leaving the community uncertain about which evaluation results remain trustworthy. We introduce the Benchmark Health Index (BHI), a pure data-driven framework for auditing evaluation sets along three orthogonal and complementary axes: (1) Capability Discrimination, measuring how sharply a benchmark separates model performance beyond noise; (2) Anti-Saturation, estimating remaining headroom before ceiling effects erode resolution and thus the benchmark's expected longevity; and (3) Impact, quantifying influence across academic and industrial ecosystems via adoption breadth and practice-shaping power. By distilling 106 validated benchmarks from the technical reports of 91 representative models in 2025, we systematically characterize the evaluation landscape. BHI is the first framework to quantify benchmark health at a macro level, providing a principled basis for benchmark selection and enabling dynamic lifecycle management for next-generation evaluation protocols.",
      "authors": [
        "Longyuan Zhu",
        "Hairan Hua",
        "Linlin Miao",
        "Bing Zhao"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 07:47:16+00:00",
      "link": "https://arxiv.org/pdf/2602.11674v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11673v1",
      "title": "RI-Mamba: Rotation-Invariant Mamba for Robust Text-to-Shape Retrieval",
      "abstract": "3D assets have rapidly expanded in quantity and diversity due to the growing popularity of virtual reality and gaming. As a result, text-to-shape retrieval has become essential in facilitating intuitive search within large repositories. However, existing methods require canonical poses and support few object categories, limiting their real-world applicability where objects can belong to diverse classes and appear in random orientations. To address this challenge, we propose RI-Mamba, the first rotation-invariant state-space model for point clouds. RI-Mamba defines global and local reference frames to disentangle pose from geometry and uses Hilbert sorting to construct token sequences with meaningful geometric structure while maintaining rotation invariance. We further introduce a novel strategy to compute orientational embeddings and reintegrate them via feature-wise linear modulation, effectively recovering spatial context and enhancing model expressiveness. Our strategy is inherently compatible with state-space models and operates in linear time. To scale up retrieval, we adopt cross-modal contrastive learning with automated triplet generation, allowing training on diverse datasets without manual annotation. Extensive experiments demonstrate RI-Mamba's superior representational capacity and robustness, achieving state-of-the-art performance on the OmniObject3D benchmark across more than 200 object categories under arbitrary orientations. Our code will be made available at https://github.com/ndkhanh360/RI-Mamba.git.",
      "authors": [
        "Khanh Nguyen",
        "Dasith de Silva Edirimuni",
        "Ghulam Mubashar Hassan",
        "Ajmal Mian"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-12 07:46:03+00:00",
      "link": "https://arxiv.org/pdf/2602.11673v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11671v1",
      "title": "Do Not Treat Code as Natural Language: Implications for Repository-Level Code Generation and Beyond",
      "abstract": "Large language models for code (CodeLLMs) have demonstrated remarkable success in standalone code completion and generation, sometimes even surpassing human performance, yet their effectiveness diminishes in repository-level settings where cross-file dependencies and structural context are essential. Existing Retrieval-Augmented Generation (RAG) approaches often borrow strategies from NLP, relying on chunking-based indexing and similarity-based retrieval. Chunking results in the loss of coherence between code units and overlooks structural relationships, while similarity-driven methods frequently miss functionally relevant dependencies such as helper functions, classes, or global variables. To address these limitations, we present Hydra, a repository-level code generation framework that treats code as structured code rather than natural language. Our approach introduces (i) a structure-aware indexing strategy that represents repositories as hierarchical trees of functions, classes, and variables, preserving code structure and dependencies, (ii) a lightweight dependency-aware retriever (DAR) that explicitly identifies and retrieves the true dependencies required by a target function, and (iii) a hybrid retrieval mechanism that combines DAR with similarity-based retrieval to provide both essential building blocks and practical usage examples. Extensive experiments on the challenging DevEval and RepoExec benchmarks, both requiring function implementation from real-world repositories with complex large repository context, show that Hydra achieves state-of-the-art performance across open- and closed-source CodeLLMs. Notably, our method establishes a new state of the art in repository-level code generation, surpassing strongest baseline by over 5% in Pass@1 and even enabling smaller models to match or exceed the performance of much larger ones that rely on existing retrievers.",
      "authors": [
        "Minh Le-Anh",
        "Huyen Nguyen",
        "Khanh An Tran",
        "Nam Le Hai",
        "Linh Ngo Van",
        "Nghi D. Q. Bui",
        "Bach Le"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-12 07:44:00+00:00",
      "link": "https://arxiv.org/pdf/2602.11671v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11666v1",
      "title": "PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics",
      "abstract": "The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely semantic Retrieval Augmented Generation (RAG) often leads to \"context poisoning,\" where agents generate linguistically plausible but physically invalid configurations due to a fundamental Semantic-Physical Disconnect. To bridge this gap, this work introduces PhyNiKCE (Physical and Numerical Knowledgeable Context Engineering), a neurosymbolic agentic framework for trustworthy engineering. Unlike standard black-box agents, PhyNiKCE decouples neural planning from symbolic validation. It employs a Symbolic Knowledge Engine that treats simulation setup as a Constraint Satisfaction Problem, rigidly enforcing physical constraints via a Deterministic RAG Engine with specialized retrieval strategies for solvers, turbulence models, and boundary conditions. Validated through rigorous OpenFOAM experiments on practical, non-tutorial CFD tasks using Gemini-2.5-Pro/Flash, PhyNiKCE demonstrates a 96% relative improvement over state-of-the-art baselines. Furthermore, by replacing trial-and-error with knowledge-driven initialization, the framework reduced autonomous self-correction loops by 59% while simultaneously lowering LLM token consumption by 17%. These results demonstrate that decoupling neural generation from symbolic constraint enforcement significantly enhances robustness and efficiency. While validated on CFD, this architecture offers a scalable, auditable paradigm for Trustworthy Artificial Intelligence in broader industrial automation.",
      "authors": [
        "E Fan",
        "Lisong Shi",
        "Zhengtong Li",
        "Chih-yung Wen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-12 07:37:56+00:00",
      "link": "https://arxiv.org/pdf/2602.11666v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11665v1",
      "title": "Fully First-Order Algorithms for Online Bilevel Optimization",
      "abstract": "In this work, we study non-convex-strongly-convex online bilevel optimization (OBO). Existing OBO algorithms are mainly based on hypergradient descent, which requires access to a Hessian-vector product (HVP) oracle and potentially incurs high computational costs. By reformulating the original OBO problem as a single-level online problem with inequality constraints and constructing a sequence of Lagrangian function, we eliminate the need for HVPs arising from implicit differentiation. Specifically, we propose a fully first-order algorithm for OBO, and provide theoretical guarantees showing that it achieves regret of $O(1 + V_T + H_{2,T})$. Furthermore, we develop an improved variant with an adaptive inner-iteration scheme, which removes the dependence on the drift variation of the inner-level optimal solution and achieves regret of $O(\\sqrt{T} + V_T)$. This regret have the advatange when $V_{T}\\ge O(\\sqrt{T})$.",
      "authors": [
        "Tingkai Jia",
        "Cheng Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-02-12 07:36:24+00:00",
      "link": "https://arxiv.org/pdf/2602.11665v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11646v1",
      "title": "Brain Tumor Classifiers Under Attack: Robustness of ResNet Variants Against Transferable FGSM and PGD Attacks",
      "abstract": "Adversarial robustness in deep learning models for brain tumor classification remains an underexplored yet critical challenge, particularly for clinical deployment scenarios involving MRI data. In this work, we investigate the susceptibility and resilience of several ResNet-based architectures, referred to as BrainNet, BrainNeXt and DilationNet, against gradient-based adversarial attacks, namely FGSM and PGD. These models, based on ResNet, ResNeXt, and dilated ResNet variants respectively, are evaluated across three preprocessing configurations (i) full-sized augmented, (ii) shrunk augmented and (iii) shrunk non-augmented MRI datasets. Our experiments reveal that BrainNeXt models exhibit the highest robustness to black-box attacks, likely due to their increased cardinality, though they produce weaker transferable adversarial samples. In contrast, BrainNet and Dilation models are more vulnerable to attacks from each other, especially under PGD with higher iteration steps and $α$ values. Notably, shrunk and non-augmented data significantly reduce model resilience, even when the untampered test accuracy remains high, highlighting a key trade-off between input resolution and adversarial vulnerability. These results underscore the importance of jointly evaluating classification performance and adversarial robustness for reliable real-world deployment in brain MRI analysis.",
      "authors": [
        "Ryan Deem",
        "Garrett Goodman",
        "Waqas Majeed",
        "Md Abdullah Al Hafiz Khan",
        "Michail S. Alexiou"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-12 06:58:33+00:00",
      "link": "https://arxiv.org/pdf/2602.11646v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11642v1",
      "title": "Electrostatics-Inspired Surface Reconstruction (EISR): Recovering 3D Shapes as a Superposition of Poisson's PDE Solutions",
      "abstract": "Implicit shape representation, such as SDFs, is a popular approach to recover the surface of a 3D shape as the level sets of a scalar field. Several methods approximate SDFs using machine learning strategies that exploit the knowledge that SDFs are solutions of the Eikonal partial differential equation (PDEs). In this work, we present a novel approach to surface reconstruction by encoding it as a solution to a proxy PDE, namely Poisson's equation. Then, we explore the connection between Poisson's equation and physics, e.g., the electrostatic potential due to a positive charge density. We employ Green's functions to obtain a closed-form parametric expression for the PDE's solution, and leverage the linearity of our proxy PDE to find the target shape's implicit field as a superposition of solutions. Our method shows improved results in approximating high-frequency details, even with a small number of shape priors.",
      "authors": [
        "Diego Patiño",
        "Knut Peterson",
        "Kostas Daniilidis",
        "David K. Han"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-12 06:54:40+00:00",
      "link": "https://arxiv.org/pdf/2602.11642v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11641v1",
      "title": "Both Topology and Text Matter: Revisiting LLM-guided Out-of-Distribution Detection on Text-attributed Graphs",
      "abstract": "Text-attributed graphs (TAGs) associate nodes with textual attributes and graph structure, enabling GNNs to jointly model semantic and structural information. While effective on in-distribution (ID) data, GNNs often encounter out-of-distribution (OOD) nodes with unseen textual or structural patterns in real-world settings, leading to overconfident and erroneous predictions in the absence of reliable OOD detection. Early approaches address this issue from a topology-driven perspective, leveraging neighboring structures to mitigate node-level detection bias. However, these methods typically encode node texts as shallow vector features, failing to fully exploit rich semantic information. In contrast, recent LLM-based approaches generate pseudo OOD priors by leveraging textual knowledge, but they suffer from several limitations: (1) a reliability-informativeness imbalance in the synthesized OOD priors, as the generated OOD exposures either deviate from the true OOD semantics, or introduce non-negligible ID noise, all of which offers limited improvement to detection performance; (2) reliance on specialized architectures, which prevents incorporation of the extensive effective topology-level insights that have been empirically validated in prior work. To this end, we propose LG-Plug, an LLM-Guided Plug-and-play strategy for TAG OOD detection tasks. LG-Plug aligns topology and text representations to produce fine-grained node embeddings, then generates consensus-driven OOD exposure via clustered iterative LLM prompting. Moreover, it leverages lightweight in-cluster codebook and heuristic sampling reduce time cost of LLM querying. The resulting OOD exposure serves as a regularization term to separate ID and OOD nodes, enabling seamless integration with existing detectors.",
      "authors": [
        "Yinlin Zhu",
        "Di Wu",
        "Xu Wang",
        "Guocong Quan",
        "Miao Hu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 06:53:35+00:00",
      "link": "https://arxiv.org/pdf/2602.11641v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11639v1",
      "title": "PACE: Prefix-Protected and Difficulty-Aware Compression for Efficient Reasoning",
      "abstract": "Language Reasoning Models (LRMs) achieve strong performance by scaling test-time computation but often suffer from ``overthinking'', producing excessively long reasoning traces that increase latency and memory usage. Existing LRMs typically enforce conciseness with uniform length penalties, which over-compress crucial early deduction steps at the sequence level and indiscriminately penalize all queries at the group level. To solve these limitations, we propose \\textbf{\\model}, a dual-level framework for prefix-protected and difficulty-aware compression under hierarchical supervision. At the sequence level, prefix-protected optimization employs decaying mixed rollouts to maintain valid reasoning paths while promoting conciseness. At the group level, difficulty-aware penalty dynamically scales length constraints based on query complexity, maintaining exploration for harder questions while curbing redundancy on easier ones. Extensive experiments on DeepSeek-R1-Distill-Qwen (1.5B/7B) demonstrate that \\model achieves a substantial reduction in token usage (up to \\textbf{55.7\\%}) while simultaneously improving accuracy (up to \\textbf{4.1\\%}) on math benchmarks, with generalization ability to code, science, and general domains.",
      "authors": [
        "Ruixiang Feng",
        "Yuntao Wen",
        "Silin Zhou",
        "Ke Shi",
        "Yifan Wang",
        "Ran Le",
        "Zhenwei An",
        "Zongchao Chen",
        "Chen Yang",
        "Guangyue Peng",
        "Yiming Jia",
        "Dongsheng Wang",
        "Tao Zhang",
        "Lisi Chen",
        "Yang Song",
        "Shen Gao",
        "Shuo Shang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 06:43:08+00:00",
      "link": "https://arxiv.org/pdf/2602.11639v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11633v1",
      "title": "TIP: Resisting Gradient Inversion via Targeted Interpretable Perturbation in Federated Learning",
      "abstract": "Federated Learning (FL) facilitates collaborative model training while preserving data locality; however, the exchange of gradients renders the system vulnerable to Gradient Inversion Attacks (GIAs), allowing adversaries to reconstruct private training data with high fidelity. Existing defenses, such as Differential Privacy (DP), typically employ indiscriminate noise injection across all parameters, which severely degrades model utility and convergence stability. To address those limitation, we proposes Targeted Interpretable Perturbation (TIP), a novel defense framework that integrates model interpretability with frequency domain analysis. Unlike conventional methods that treat parameters uniformly, TIP introduces a dual-targeting strategy. First, leveraging Gradient-weighted Class Activation Mapping (Grad-CAM) to quantify channel sensitivity, we dynamically identify critical convolution channels that encode primary semantic features. Second, we transform these selected kernels into the frequency domain via the Discrete Fourier Transform and selectively inject calibrated perturbations into the high-frequency spectrum. By selectively perturbing high-frequency components, TIP effectively destroys the fine-grained details necessary for image reconstruction while preserving the low-frequency information crucial for model accuracy. Extensive experiments on benchmark datasets demonstrate that TIP renders reconstructed images visually unrecognizable against state-of-the-art GIAs, while maintaining global model accuracy comparable to non-private baselines, significantly outperforming existing DP-based defenses in the privacy-utility trade-off and interpretability. Code is available in https://github.com/2766733506/asldkfjssdf_arxiv",
      "authors": [
        "Jianhua Wang",
        "Yinlin Su"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 06:32:49+00:00",
      "link": "https://arxiv.org/pdf/2602.11633v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11631v1",
      "title": "Enforcing Reciprocity in Operator Learning for Seismic Wave Propagation",
      "abstract": "Accurate and efficient wavefield modeling underpins seismic structure and source studies. Traditional methods comply with physical laws but are computationally intensive. Data-driven methods, while opening new avenues for advancement, have yet to incorporate strict physical consistency. The principle of reciprocity is one of the most fundamental physical laws in wave propagation. We introduce the Reciprocity-Enforced Neural Operator (RENO), a transformer-based architecture for modeling seismic wave propagation that hard-codes the reciprocity principle. The model leverages the cross-attention mechanism and commutative operations to guarantee invariance under swapping source and receiver positions. Beyond improved physical consistency, the proposed architecture supports simultaneous realizations for multiple sources without crosstalk issues. This yields an order-of-magnitude inference speedup at a similar memory footprint over an reciprocity-unenforced neural operator on a realistic configuration. We demonstrate the functionality using the reciprocity relation for particle velocity fields under single forces. This architecture is also applicable to pressure fields under dilatational sources and travel-time fields governed by the eikonal equation, paving the way for encoding more complex reciprocity relations.",
      "authors": [
        "Caifeng Zou",
        "Yaozhong Shi",
        "Zachary E. Ross",
        "Robert W. Clayton",
        "Kamyar Azizzadenesheli"
      ],
      "primary_category": "physics.geo-ph",
      "categories": [
        "physics.geo-ph",
        "cs.LG"
      ],
      "published": "2026-02-12 06:28:14+00:00",
      "link": "https://arxiv.org/pdf/2602.11631v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11630v1",
      "title": "Neuro-Symbolic Multitasking: A Unified Framework for Discovering Generalizable Solutions to PDE Families",
      "abstract": "Solving Partial Differential Equations (PDEs) is fundamental to numerous scientific and engineering disciplines. A common challenge arises from solving the PDE families, which are characterized by sharing an identical mathematical structure but varying in specific parameters. Traditional numerical methods, such as the finite element method, need to independently solve each instance within a PDE family, which incurs massive computational cost. On the other hand, while recent advancements in machine learning PDE solvers offer impressive computational speed and accuracy, their inherent ``black-box\" nature presents a considerable limitation. These methods primarily yield numerical approximations, thereby lacking the crucial interpretability provided by analytical expressions, which are essential for deeper scientific insight. To address these limitations, we propose a neuro-assisted multitasking symbolic PDE solver framework for PDE family solving, dubbed NMIPS. In particular, we employ multifactorial optimization to simultaneously discover the analytical solutions of PDEs. To enhance computational efficiency, we devise an affine transfer method by transferring learned mathematical structures among PDEs in a family, avoiding solving each PDE from scratch. Experimental results across multiple cases demonstrate promising improvements over existing baselines, achieving up to a $\\sim$35.7% increase in accuracy while providing interpretable analytical solutions.",
      "authors": [
        "Yipeng Huang",
        "Dejun Xu",
        "Zexin Lin",
        "Zhenzhong Wang",
        "Min Jiang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 06:25:44+00:00",
      "link": "https://arxiv.org/pdf/2602.11630v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11629v1",
      "title": "GP2F: Cross-Domain Graph Prompting with Adaptive Fusion of Pre-trained Graph Neural Networks",
      "abstract": "Graph Prompt Learning (GPL) has recently emerged as a promising paradigm for downstream adaptation of pre-trained graph models, mitigating the misalignment between pre-training objectives and downstream tasks. Recently, the focus of GPL has shifted from in-domain to cross-domain scenarios, which is closer to the real world applications, where the pre-training source and downstream target often differ substantially in data distribution. However, why GPLs remain effective under such domain shifts is still unexplored. Empirically, we observe that representative GPL methods are competitive with two simple baselines in cross-domain settings: full fine-tuning (FT) and linear probing (LP), motivating us to explore a deeper understanding of the prompting mechanism. We provide a theoretical analysis demonstrating that jointly leveraging these two complementary branches yields a smaller estimation error than using either branch alone, formally proving that cross-domain GPL benefits from the integration between pre-trained knowledge and task-specific adaptation. Based on this insight, we propose GP2F, a dual-branch GPL method that explicitly instantiates the two extremes: (1) a frozen branch that retains pre-trained knowledge, and (2) an adapted branch with lightweight adapters for task-specific adaptation. We then perform adaptive fusion under topology constraints via a contrastive loss and a topology-consistent loss. Extensive experiments on cross-domain few-shot node and graph classification demonstrate that our method outperforms existing methods.",
      "authors": [
        "Dongxiao He",
        "Wenxuan Sun",
        "Yongqi Huang",
        "Jitao Zhao",
        "Di Jin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 06:25:21+00:00",
      "link": "https://arxiv.org/pdf/2602.11629v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11626v1",
      "title": "ArGEnT: Arbitrary Geometry-encoded Transformer for Operator Learning",
      "abstract": "Learning solution operators for systems with complex, varying geometries and parametric physical settings is a central challenge in scientific machine learning. In many-query regimes such as design optimization, control and inverse problems, surrogate modeling must generalize across geometries while allowing flexible evaluation at arbitrary spatial locations. In this work, we propose Arbitrary Geometry-encoded Transformer (ArGEnT), a geometry-aware attention-based architecture for operator learning on arbitrary domains. ArGEnT employs Transformer attention mechanisms to encode geometric information directly from point-cloud representations with three variants-self-attention, cross-attention, and hybrid-attention-that incorporates different strategies for incorporating geometric features. By integrating ArGEnT into DeepONet as the trunk network, we develop a surrogate modeling framework capable of learning operator mappings that depend on both geometric and non-geometric inputs without the need to explicitly parametrize geometry as a branch network input. Evaluation on benchmark problems spanning fluid dynamics, solid mechanics and electrochemical systems, we demonstrate significantly improved prediction accuracy and generalization performance compared with the standard DeepONet and other existing geometry-aware saurrogates. In particular, the cross-attention transformer variant enables accurate geometry-conditioned predictions with reduced reliance on signed distance functions. By combining flexible geometry encoding with operator-learning capabilities, ArGEnT provides a scalable surrogate modeling framework for optimization, uncertainty quantification, and data-driven modeling of complex physical systems.",
      "authors": [
        "Wenqian Chen",
        "Yucheng Fu",
        "Michael Penwarden",
        "Pratanu Roy",
        "Panos Stinis"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph",
        "physics.comp-ph",
        "physics.flu-dyn"
      ],
      "published": "2026-02-12 06:22:59+00:00",
      "link": "https://arxiv.org/pdf/2602.11626v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11623v1",
      "title": "TreeGrad-Ranker: Feature Ranking via $O(L)$-Time Gradients for Decision Trees",
      "abstract": "We revisit the use of probabilistic values, which include the well-known Shapley and Banzhaf values, to rank features for explaining the local predicted values of decision trees. The quality of feature rankings is typically assessed with the insertion and deletion metrics. Empirically, we observe that co-optimizing these two metrics is closely related to a joint optimization that selects a subset of features to maximize the local predicted value while minimizing it for the complement. However, we theoretically show that probabilistic values are generally unreliable for solving this joint optimization. Therefore, we explore deriving feature rankings by directly optimizing the joint objective. As the backbone, we propose TreeGrad, which computes the gradients of the multilinear extension of the joint objective in $O(L)$ time for decision trees with $L$ leaves; these gradients include weighted Banzhaf values. Building upon TreeGrad, we introduce TreeGrad-Ranker, which aggregates the gradients while optimizing the joint objective to produce feature rankings, and TreeGrad-Shap, a numerically stable algorithm for computing Beta Shapley values with integral parameters. In particular, the feature scores computed by TreeGrad-Ranker satisfy all the axioms uniquely characterizing probabilistic values, except for linearity, which itself leads to the established unreliability. Empirically, we demonstrate that the numerical error of Linear TreeShap can be up to $10^{15}$ times larger than that of TreeGrad-Shap when computing the Shapley value. As a by-product, we also develop TreeProb, which generalizes Linear TreeShap to support all probabilistic values. In our experiments, TreeGrad-Ranker performs significantly better on both insertion and deletion metrics. Our code is available at https://github.com/watml/TreeGrad.",
      "authors": [
        "Weida Li",
        "Yaoliang Yu",
        "Bryan Kian Hsiang Low"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 06:17:12+00:00",
      "link": "https://arxiv.org/pdf/2602.11623v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11622v1",
      "title": "Evolutionary Router Feature Generation for Zero-Shot Graph Anomaly Detection with Mixture-of-Experts",
      "abstract": "Zero-shot graph anomaly detection (GAD) has attracted increasing attention recent years, yet the heterogeneity of graph structures, features, and anomaly patterns across graphs make existing single GNN methods insufficiently expressive to model diverse anomaly mechanisms. In this regard, Mixture-of-experts (MoE) architectures provide a promising paradigm by integrating diverse GNN experts with complementary inductive biases, yet their effectiveness in zero-shot GAD is severely constrained by distribution shifts, leading to two key routing challenges. First, nodes often carry vastly different semantics across graphs, and straightforwardly performing routing based on their features is prone to generating biased or suboptimal expert assignments. Second, as anomalous graphs often exhibit pronounced distributional discrepancies, existing router designs fall short in capturing domain-invariant routing principles that generalize beyond the training graphs. To address these challenges, we propose a novel MoE framework with evolutionary router feature generation (EvoFG) for zero-shot GAD. To enhance MoE routing, we propose an evolutionary feature generation scheme that iteratively constructs and selects informative structural features via an LLM-based generator and Shapley-guided evaluation. Moreover, a memory-enhanced router with an invariant learning objective is designed to capture transferable routing patterns under distribution shifts. Extensive experiments on six benchmarks show that EvoFG consistently outperforms state-of-the-art baselines, achieving strong and stable zero-shot GAD performance.",
      "authors": [
        "Haiyang Jiang",
        "Tong Chen",
        "Xinyi Gao",
        "Guansong Pang",
        "Quoc Viet Hung Nguyen",
        "Hongzhi Yin"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-12 06:16:51+00:00",
      "link": "https://arxiv.org/pdf/2602.11622v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11609v1",
      "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
      "abstract": "We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence.   To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses.   Code, data, and package are available at https://github.com/maitrix-org/scPilot",
      "authors": [
        "Yiming Gao",
        "Zhen Wang",
        "Jefferson Chen",
        "Mark Antkowiak",
        "Mengzhou Hu",
        "JungHo Kong",
        "Dexter Pratt",
        "Jieyuan Liu",
        "Enze Ma",
        "Zhiting Hu",
        "Eric P. Xing"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "q-bio.GN"
      ],
      "published": "2026-02-12 06:04:11+00:00",
      "link": "https://arxiv.org/pdf/2602.11609v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11596v1",
      "title": "MAPLE: Modality-Aware Post-training and Learning Ecosystem",
      "abstract": "Multimodal language models now integrate text, audio, and video for unified reasoning. Yet existing RL post-training pipelines treat all input signals as equally relevant, ignoring which modalities each task actually requires. This modality-blind training inflates policy-gradient variance, slows convergence, and degrades robustness to real-world distribution shifts where signals may be missing, added, or reweighted. We introduce MAPLE, a complete modality-aware post-training and learning ecosystem comprising: (1) MAPLE-bench, the first benchmark explicitly annotating minimal signal combinations required per task; (2) MAPO, a modality-aware policy optimization framework that stratifies batches by modality requirement to reduce gradient variance from heterogeneous group advantages; (3) Adaptive weighting and curriculum scheduling that balances and prioritizes harder signal combinations. Systematic analysis across loss aggregation, clipping, sampling, and curriculum design establishes MAPO's optimal training strategy. Adaptive weighting and curriculum focused learning further boost performance across signal combinations. MAPLE narrows uni/multi-modal accuracy gaps by 30.24%, converges 3.18x faster, and maintains stability across all modality combinations under realistic reduced signal access. MAPLE constitutes a complete recipe for deployment-ready multimodal RL post-training.",
      "authors": [
        "Nikhil Verma",
        "Minjung Kim",
        "JooYoung Yoo",
        "Kyung-Min Jin",
        "Manasa Bharadwaj",
        "Kevin Ferreira",
        "Ko Keun Kim",
        "Youngjoon Kim"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 05:26:36+00:00",
      "link": "https://arxiv.org/pdf/2602.11596v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11584v1",
      "title": "Gradient Compression May Hurt Generalization: A Remedy by Synthetic Data Guided Sharpness Aware Minimization",
      "abstract": "It is commonly believed that gradient compression in federated learning (FL) enjoys significant improvement in communication efficiency with negligible performance degradation. In this paper, we find that gradient compression induces sharper loss landscapes in federated learning, particularly under non-IID data distributions, which suggests hindered generalization capability. The recently emerging Sharpness Aware Minimization (SAM) effectively searches for a flat minima by incorporating a gradient ascent step (i.e., perturbing the model with gradients) before the celebrated stochastic gradient descent. Nonetheless, the direct application of SAM in FL suffers from inaccurate estimation of the global perturbation due to data heterogeneity. Existing approaches propose to utilize the model update from the previous communication round as a rough estimate. However, its effectiveness is hindered when model update compression is incorporated. In this paper, we propose FedSynSAM, which leverages the global model trajectory to construct synthetic data and facilitates an accurate estimation of the global perturbation. The convergence of the proposed algorithm is established, and extensive experiments are conducted to validate its effectiveness.",
      "authors": [
        "Yujie Gu",
        "Richeng Jin",
        "Zhaoyang Zhang",
        "Huaiyu Dai"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 05:08:49+00:00",
      "link": "https://arxiv.org/pdf/2602.11584v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11581v1",
      "title": "Analytical Search",
      "abstract": "Analytical information needs, such as trend analysis and causal impact assessment, are prevalent across various domains including law, finance, science, and much more. However, existing information retrieval paradigms, whether based on relevance-oriented document ranking or retrieval-augmented generation (RAG) with large language models (LLMs), often struggle to meet the end-to-end requirements of such tasks at the corpus scale. They either emphasize information finding rather than end-to-end problem solving, or simply treat everything as naive question answering, offering limited control over reasoning, evidence usage, and verifiability. As a result, they struggle to support analytical queries that have diverse utility concepts and high accountability requirements.   In this paper, we propose analytical search as a distinct and emerging search paradigm designed to fulfill these analytical information needs. Analytical search reframes search as an evidence-governed, process-oriented analytical workflow that explicitly models analytical intent, retrieves evidence for fusion, and produces verifiable conclusions through structured, multi-step inference. We position analytical search in contrast to existing paradigms, and present a unified system framework that integrates query understanding, recall-oriented retrieval, reasoning-aware fusion, and adaptive verification. We also discuss potential research directions for the construction of analytical search engines. In this way, we highlight the conceptual significance and practical importance of analytical search and call on efforts toward the next generation of search engines that support analytical information needs.",
      "authors": [
        "Yiteng Tu",
        "Shuo Miao",
        "Weihang Su",
        "Yiqun Liu",
        "Qingyao Ai"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-12 05:06:29+00:00",
      "link": "https://arxiv.org/pdf/2602.11581v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11578v1",
      "title": "Quantum-Enhanced Temporal Embeddings via a Hybrid Seq2Seq Architecture",
      "abstract": "This work investigates how shallow, NISQ-compatible quantum layers can improve temporal representation learning in real-world sequential data. We develop a QLSTM Seq2Seq autoencoder in which a depth-1 variational quantum circuit is embedded inside each recurrent gate, shaping the geometry of the learned latent manifold. Evaluated on fourteen rolling S and P 500 windows from 2022 to 2025, the quantum-enhanced encoder produces smoother trajectories, clearer regime transitions, and more stable, sector-coherent clusters than a classical LSTM baseline. These geometric properties support the use of a Radial Basis Function (RBF) kernel for downstream portfolio allocation, where both RBF-Graph and RBF-DivMom strategies consistently outperform their classical counterparts in risk-adjusted terms. Analysis across periods shows that compressed manifolds favor concentrated allocation, while dispersed manifolds favor diversification, demonstrating that latent geometry serves as a regime indicator. The results highlight a practical role for shallow hybrid quantum and classical layers in NISQ-era sequence modeling, offering a reproducible pathway for improving temporal embeddings in finance and other data-limited, noise-sensitive domains.",
      "authors": [
        "Tien-Ching Hsieh",
        "Yun-Cheng Tsai",
        "Samuel Yen-Chi Chen"
      ],
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE"
      ],
      "published": "2026-02-12 04:55:33+00:00",
      "link": "https://arxiv.org/pdf/2602.11578v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12305v1",
      "title": "OptiML: An End-to-End Framework for Program Synthesis and CUDA Kernel Optimization",
      "abstract": "Generating high-performance CUDA kernels remains challenging due to the need to navigate a combinatorial space of low-level transformations under noisy and expensive hardware feedback. Although large language models can synthesize functionally correct CUDA code, achieving competitive performance requires systematic exploration and verification of optimization choices. We present OptiML, an end-to-end framework that maps either natural-language intent or input CUDA code to performance-optimized CUDA kernels by formulating kernel optimization as search under verification. OptiML consists of two decoupled stages. When the input is natural language, a Mixture-of-Thoughts generator (OptiML-G) acts as a proposal policy over kernel implementation strategies, producing an initial executable program. A search-based optimizer (OptiML-X) then refines either synthesized or user-provided kernels using Monte Carlo Tree Search over LLM-driven edits, guided by a hardware-aware reward derived from profiler feedback. Each candidate transformation is compiled, verified, and profiled with Nsight Compute, and evaluated by a composite objective that combines runtime with hardware bottleneck proxies and guardrails against regressions. We evaluate OptiML in both synthesis-and-optimize and optimization-only settings on a diverse suite of CUDA kernels. Results show that OptiML consistently discovers verified performance improvements over strong LLM baselines and produces interpretable optimization trajectories grounded in profiler evidence.",
      "authors": [
        "Arijit Bhattacharjee",
        "Heng Ping",
        "Son Vu Le",
        "Paul Bogdan",
        "Nesreen K. Ahmed",
        "Ali Jannesari"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.MA",
        "cs.SE"
      ],
      "published": "2026-02-12 04:50:19+00:00",
      "link": "https://arxiv.org/pdf/2602.12305v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11573v2",
      "title": "Fast Tuning the Index Construction Parameters of Proximity Graphs in Vector Databases",
      "abstract": "k-approximate nearest neighbor search (k-ANNS) in high-dimensional vector spaces is a fundamental problem across many fields. With the advent of vector databases and retrieval-augmented generation, k-ANNS has garnered increasing attention. Among existing methods, proximity graphs (PG) based approaches are the state-of-the-art (SOTA) methods. However, the construction parameters of PGs significantly impact their search performance. Before constructing a PG for a given dataset, it is essential to tune these parameters, which first recommends a set of promising parameters and then estimates the quality of each parameter by building the corresponding PG and then testing its k-ANNS performance. Given that the construction complexity of PGs is superlinear, building and evaluating graph indexes accounts for the primary cost of parameter tuning. Unfortunately, there is currently no method considered and optimized this process.In this paper, we introduce FastPGT, an efficient framework for tuning the PG construction parameters. FastPGT accelerates parameter estimation by building multiple PGs simultaneously, thereby reducing repeated computations. Moreover, we modify the SOTA tuning model to recommend multiple parameters at once, which can be efficiently estimated using our method of building multiple PGs simultaneously. Through extensive experiments on real-world datasets, we demonstrate that FastPGT achieves up to 2.37x speedup over the SOTA method VDTuner, without compromising tuning quality.",
      "authors": [
        "Wenyang Zhou",
        "Jiadong Xie",
        "Yingfan Liu",
        "Zhihao Yin",
        "Jeffrey Xu Yu",
        "Hui Li",
        "Zhangqian Mu",
        "Xiaotian Qiao",
        "Jiangtao Cui"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-12 04:45:43+00:00",
      "link": "https://arxiv.org/pdf/2602.11573v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11570v1",
      "title": "PRIME: A Process-Outcome Alignment Benchmark for Verifiable Reasoning in Mathematics and Engineering",
      "abstract": "While model-based verifiers are essential for scaling Reinforcement Learning with Verifiable Rewards (RLVR), current outcome-centric verification paradigms primarily focus on the consistency between the final result and the ground truth, often neglecting potential errors in the derivation process. This leads to assigning positive rewards to correct answers produced from incorrect derivations. To bridge this gap, we introduce PRIME, a benchmark for evaluating verifiers on Process-Outcome Alignment verification in Mathematics and Engineering. Curated from a comprehensive collection of college-level STEM problems, PRIME comprises 2,530 high-difficulty samples through a consistency-based filtering pipeline. Through extensive evaluation, we find that current verifiers frequently fail to detect derivation flaws. Furthermore, we propose a process-aware RLVR training paradigm utilizing verifiers selected via PRIME. This approach substantially outperforms the outcome-only verification baseline, achieving absolute performance gains of 8.29%, 9.12%, and 7.31% on AIME24, AIME25, and Beyond-AIME, respectively, for the Qwen3-14B-Base model. Finally, we demonstrate a strong linear correlation ($R^2 > 0.92$) between verifier accuracy on PRIME and RLVR training effectiveness, validating PRIME as a reliable predictor for verifier selection.",
      "authors": [
        "Xiangfeng Wang",
        "Hangyu Guo",
        "Yanlin Lai",
        "Mitt Huang",
        "Liang Zhao",
        "Chengyuan Yao",
        "Yinmin Zhang",
        "Qi Han",
        "Xiaoxiao Ren",
        "Chun Yuan",
        "Tong Xu",
        "Zheng Ge",
        "Xiangyu Zhang",
        "Daxin Jiang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 04:45:01+00:00",
      "link": "https://arxiv.org/pdf/2602.11570v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11565v1",
      "title": "Move What Matters: Parameter-Efficient Domain Adaptation via Optimal Transport Flow for Collaborative Perception",
      "abstract": "Fast domain adaptation remains a fundamental challenge for deploying multi-agent systems across diverse environments in Vehicle-to-Everything (V2X) collaborative perception. Despite the success of Parameter-Efficient Fine-Tuning (PEFT) in natural language processing and conventional vision tasks, directly applying PEFT to multi-agent settings leads to significant performance degradation and training instability. In this work, we conduct a detailed analysis and identify two key factors: (i) inter-frame redundancy in heterogeneous sensory streams, and (ii) erosion of fine-grained semantics in deep-layer representations under PEFT adaptation. To address these issues, we propose FlowAdapt, a parameter-efficient framework grounded in optimal transport theory, which minimizes information transport costs across both data distributions and network hierarchies. Specifically, we introduce a Wasserstein Greedy Sampling strategy to selectively filter redundant samples via a bounded covering radius. Furthermore, Progressive Knowledge Transfer module is designed to progressively inject compressed early-stage representations into later stages through learnable pathways, alleviating semantic degradation in late-stage adaptation. Extensive experiments on three benchmarks demonstrate that FlowAdapt achieves state-of-the-art performance with only 1% of trainable parameters, effectively bridging domain gaps with superior sample efficiency and generalization.",
      "authors": [
        "Zesheng Jia",
        "Jin Wang",
        "Siao Liu",
        "Lingzhi Li",
        "Ziyao Huang",
        "Yunjiang Xu",
        "Jianping Wang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-12 04:36:50+00:00",
      "link": "https://arxiv.org/pdf/2602.11565v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11557v1",
      "title": "The Implicit Bias of Steepest Descent with Mini-batch Stochastic Gradient",
      "abstract": "A variety of widely used optimization methods like SignSGD and Muon can be interpreted as instances of steepest descent under different norm-induced geometries. In this work, we study the implicit bias of mini-batch stochastic steepest descent in multi-class classification, characterizing how batch size, momentum, and variance reduction shape the limiting max-margin behavior and convergence rates under general entry-wise and Schatten-$p$ norms. We show that without momentum, convergence only occurs with large batches, yielding a batch-dependent margin gap but the full-batch convergence rate. In contrast, momentum enables small-batch convergence through a batch-momentum trade-off, though it slows convergence. This approach provides fully explicit, dimension-free rates that improve upon prior results. Moreover, we prove that variance reduction can recover the exact full-batch implicit bias for any batch size, albeit at a slower convergence rate. Finally, we further investigate the batch-size-one steepest descent without momentum, and reveal its convergence to a fundamentally different bias via a concrete data example, which reveals a key limitation of purely stochastic updates. Overall, our unified analysis clarifies when stochastic optimization aligns with full-batch behavior, and paves the way for perform deeper explorations of the training behavior of stochastic gradient steepest descent algorithms.",
      "authors": [
        "Jichu Li",
        "Xuan Tang",
        "Difan Zou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 04:25:38+00:00",
      "link": "https://arxiv.org/pdf/2602.11557v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11550v1",
      "title": "TS-Memory: Plug-and-Play Memory for Time Series Foundation Models",
      "abstract": "Time Series Foundation Models (TSFMs) achieve strong zero-shot forecasting through large-scale pre-training, but adapting them to downstream domains under distribution shift remains challenging. Existing solutions face a trade-off: Parametric Adaptation can cause catastrophic forgetting and requires costly multi-domain maintenance, while Non-Parametric Retrieval improves forecasts but incurs high inference latency due to datastore search. We propose Parametric Memory Distillation and implement it as TS-Memory, a lightweight memory adapter that augments frozen TSFMs. TS-Memory is trained in two stages. First, we construct an offline, leakage-safe kNN teacher that synthesizes confidence-aware quantile targets from retrieved futures. Second, we distill this retrieval-induced distributional correction into a lightweight memory adapter via confidence-gated supervision. During inference, TS-Memory fuses memory and backbone predictions with constant-time overhead, enabling retrieval-free deployment. Experiments across diverse TSFMs and benchmarks demonstrate consistent improvements in both point and probabilistic forecasting over representative adaptation methods, with efficiency comparable to the frozen backbone.",
      "authors": [
        "Sisuo Lyu",
        "Siru Zhong",
        "Tiegang Chen",
        "Weilin Ruan",
        "Qingxiang Liu",
        "Taiqiang Lv",
        "Qingsong Wen",
        "Raymond Chi-Wing Wong",
        "Yuxuan Liang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 04:16:19+00:00",
      "link": "https://arxiv.org/pdf/2602.11550v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11549v1",
      "title": "Native Reasoning Models: Training Language Models to Reason on Unverifiable Data",
      "abstract": "The prevailing paradigm for training large reasoning models--combining Supervised Fine-Tuning (SFT) with Reinforcement Learning with Verifiable Rewards (RLVR)--is fundamentally constrained by its reliance on high-quality, human-annotated reasoning data and external verifiers. This dependency incurs significant data-collection costs, risks embedding human cognitive biases, and confines the reinforcement learning stage to objectively assessable domains like mathematics and coding, leaving a wide range of unverifiable tasks beyond its scope. To overcome these limitations, we introduce NRT (Native Reasoning Training), a novel framework that cultivates complex reasoning by having the model generate its own reasoning traces using only standard question-answer pairs, thereby obviating the need for expert-written demonstrations. NRT reframes the training problem by treating the reasoning process as a latent variable. It employs a unified training objective that models reasoning as an optimization problem, intrinsically rewarding paths that increase the model's likelihood of producing the ground-truth answer. This unified perspective allows us to analyze intrinsic failure modes of prior methods, such as policy collapse, and systematically design more robust reward aggregation functions, creating a self-reinforcing feedback loop where the model learns to think in ways that resolve its own uncertainty. Empirical evaluation on Llama and Mistral model families demonstrates that NRT achieves state-of-the-art performance among verifier-free methods, significantly outperforming standard SFT baselines and prior verifier-free RL methods. Our approach yields particularly strong performance gains in complex reasoning domains and exhibits high robustness to policy collapse, offering a general, scalable path toward building more powerful and broadly applicable reasoning systems.",
      "authors": [
        "Yuanfu Wang",
        "Zhixuan Liu",
        "Xiangtian Li",
        "Chaochao Lu",
        "Chao Yang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-12 04:15:46+00:00",
      "link": "https://arxiv.org/pdf/2602.11549v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11539v1",
      "title": "Real-Time Proactive Anomaly Detection via Forward and Backward Forecast Modeling",
      "abstract": "Reactive anomaly detection methods, which are commonly deployed to identify anomalies after they occur based on observed deviations, often fall short in applications that demand timely intervention, such as industrial monitoring, finance, and cybersecurity. Proactive anomaly detection, by contrast, aims to detect early warning signals before failures fully manifest, but existing methods struggle with handling heterogeneous multivariate data and maintaining precision under noisy or unpredictable conditions. In this work, we introduce two proactive anomaly detection frameworks: the Forward Forecasting Model (FFM) and the Backward Reconstruction Model (BRM). Both models leverage a hybrid architecture combining Temporal Convolutional Networks (TCNs), Gated Recurrent Units (GRUs), and Transformer encoders to model directional temporal dynamics. FFM forecasts future sequences to anticipate disruptions, while BRM reconstructs recent history from future context to uncover early precursors. Anomalies are flagged based on forecasting error magnitudes and directional embedding discrepancies. Our models support both continuous and discrete multivariate features, enabling robust performance in real-world settings. Extensive experiments on four benchmark datasets, MSL, SMAP, SMD, and PSM, demonstrate that FFM and BRM outperform state-of-the-art baselines across detection metrics and significantly improve the timeliness of anomaly anticipation. These properties make our approach well-suited for deployment in time-sensitive domains requiring proactive monitoring.",
      "authors": [
        "Luis Olmos",
        "Rashida Hasan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 03:57:41+00:00",
      "link": "https://arxiv.org/pdf/2602.11539v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11527v1",
      "title": "CausalAgent: A Conversational Multi-Agent System for End-to-End Causal Inference",
      "abstract": "Causal inference holds immense value in fields such as healthcare, economics, and social sciences. However, traditional causal analysis workflows impose significant technical barriers, requiring researchers to possess dual backgrounds in statistics and computer science, while manually selecting algorithms, handling data quality issues, and interpreting complex results. To address these challenges, we propose CausalAgent, a conversational multi-agent system for end-to-end causal inference. The system innovatively integrates Multi-Agent Systems (MAS), Retrieval-Augmented Generation (RAG), and the Model Context Protocol (MCP) to achieve automation from data cleaning and causal structure learning to bias correction and report generation through natural language interaction. Users need only upload a dataset and pose questions in natural language to receive a rigorous, interactive analysis report. As a novel user-centered human-AI collaboration paradigm, CausalAgent explicitly models the analysis workflow. By leveraging interactive visualizations, it significantly lowers the barrier to entry for causal analysis while ensuring the rigor and interpretability of the process.",
      "authors": [
        "Jiawei Zhu",
        "Wei Chen",
        "Ruichu Cai"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 03:36:29+00:00",
      "link": "https://arxiv.org/pdf/2602.11527v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11523v1",
      "title": "Unifying Stable Optimization and Reference Regularization in RLHF",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has advanced alignment capabilities significantly but remains hindered by two core challenges: \\textbf{reward hacking} and \\textbf{stable optimization}. Current solutions independently address these issues through separate regularization strategies, specifically a KL-divergence penalty against a supervised fine-tuned model ($π_0$) to mitigate reward hacking, and policy ratio clipping towards the current policy ($π_t$) to promote stable alignment. However, the implicit trade-off arising from simultaneously regularizing towards both $π_0$ and $π_t$ remains under-explored. In this paper, we introduce a unified regularization approach that explicitly balances the objectives of preventing reward hacking and maintaining stable policy updates. Our simple yet principled alignment objective yields a weighted supervised fine-tuning loss with a superior trade-off, which demonstrably improves both alignment results and implementation complexity. Extensive experiments across diverse benchmarks validate that our method consistently outperforms RLHF and online preference learning methods, achieving enhanced alignment performance and stability.",
      "authors": [
        "Li He",
        "Qiang Qu",
        "He Zhao",
        "Stephen Wan",
        "Dadong Wang",
        "Lina Yao",
        "Tongliang Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 03:31:19+00:00",
      "link": "https://arxiv.org/pdf/2602.11523v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11518v1",
      "title": "KuaiSearch: A Large-Scale E-Commerce Search Dataset for Recall, Ranking, and Relevance",
      "abstract": "E-commerce search serves as a central interface, connecting user demands with massive product inventories and plays a vital role in our daily lives. However, in real-world applications, it faces challenges, including highly ambiguous queries, noisy product texts with weak semantic order, and diverse user preferences, all of which make it difficult to accurately capture user intent and fine-grained product semantics. In recent years, significant advances in large language models (LLMs) for semantic representation and contextual reasoning have created new opportunities to address these challenges. Nevertheless, existing e-commerce search datasets still suffer from notable limitations: queries are often heuristically constructed, cold-start users and long-tail products are filtered out, query and product texts are anonymized, and most datasets cover only a single stage of the search pipeline. Collectively, these issues constrain research on LLM-based e-commerce search. To address these challenges, we construct and release KuaiSearch. To the best of our knowledge, it is the largest e-commerce search dataset currently available. KuaiSearch is built upon real user search interactions from the Kuaishou platform, preserving authentic user queries and natural-language product texts, covering cold-start users and long-tail products, and systematically spanning three key stages of the search pipeline: recall, ranking, and relevance judgment. We conduct a comprehensive analysis of KuaiSearch from multiple perspectives, including products, users, and queries, and establish benchmark experiments across several representative search tasks. Experimental results demonstrate that KuaiSearch provides a valuable foundation for research on real-world e-commerce search.",
      "authors": [
        "Yupeng Li",
        "Ben Chen",
        "Mingyue Cheng",
        "Zhiding Liu",
        "Xuxin Zhang",
        "Chenyi Lei",
        "Wenwu Ou"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-12 03:22:05+00:00",
      "link": "https://arxiv.org/pdf/2602.11518v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11516v1",
      "title": "Human-Inspired Continuous Learning of Internal Reasoning Processes: Learning How to Think for Adaptive AI Systems",
      "abstract": "Learning internal reasoning processes is crucial for developing AI systems capable of sustained adaptation in dynamic real-world environments. However, most existing approaches primarily emphasize learning task-specific outputs or static knowledge representations, while overlooking the continuous refinement of internal reasoning structures, action scheduling policies, and learning mechanisms themselves. In this paper, we propose a human-inspired continuous learning framework that unifies reasoning, action, reflection, and verification within a sequential reasoning model enhanced by parallel learning. The framework explicitly treats internal thinking processes as primary learning objects. It systematically records internal reasoning trajectories and environmental interactions as structured learning material, enabling the system to optimize not only task-level content but also the organization, scheduling, and evolution of reasoning activities. This design realizes learning alongside processing, allowing cognitive structures to improve during execution. Furthermore, the framework supports controlled replacement of predefined logic with learned procedures and introduces a hierarchical learning-to-learn mechanism that jointly adapts task-level parameters and learning strategies. As a result, the system progressively evolves its internal cognitive architecture while preserving operational stability. Experimental results on a temperature sensor abnormality detection task show that incorporating internal-process learning reduces average runtime by 23.9%.",
      "authors": [
        "Hong Su"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 03:19:04+00:00",
      "link": "https://arxiv.org/pdf/2602.11516v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11509v1",
      "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning",
      "abstract": "Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution.",
      "authors": [
        "David Wan",
        "Han Wang",
        "Ziyang Wang",
        "Elias Stengel-Eskin",
        "Hyunji Lee",
        "Mohit Bansal"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-12 03:10:02+00:00",
      "link": "https://arxiv.org/pdf/2602.11509v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11498v1",
      "title": "Partial GFlowNet: Accelerating Convergence in Large State Spaces via Strategic Partitioning",
      "abstract": "Generative Flow Networks (GFlowNets) have shown promising potential to generate high-scoring candidates with probability proportional to their rewards. As existing GFlowNets freely explore in state space, they encounter significant convergence challenges when scaling to large state spaces. Addressing this issue, this paper proposes to restrict the exploration of actor. A planner is introduced to partition the entire state space into overlapping partial state spaces. Given their limited size, these partial state spaces allow the actor to efficiently identify subregions with higher rewards. A heuristic strategy is introduced to switch partial regions thus preventing the actor from wasting time exploring fully explored or low-reward partial regions. By iteratively exploring these partial state spaces, the actor learns to converge towards the high-reward subregions within the entire state space. Experiments on several widely used datasets demonstrate that \\modelname converges faster than existing works on large state spaces. Furthermore, \\modelname not only generates candidates with higher rewards but also significantly improves their diversity.",
      "authors": [
        "Xuan Yu",
        "Xu Wang",
        "Rui Zhu",
        "Yudong Zhang",
        "Yang Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 02:50:26+00:00",
      "link": "https://arxiv.org/pdf/2602.11498v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11494v1",
      "title": "Arbitrary Ratio Feature Compression via Next Token Prediction",
      "abstract": "Feature compression is increasingly important for improving the efficiency of downstream tasks, especially in applications involving large-scale or multi-modal data. While existing methods typically rely on dedicated models for achieving specific compression ratios, they are often limited in flexibility and generalization. In particular, retraining is necessary when adapting to a new compression ratio. To address this limitation, we propose a novel and flexible Arbitrary Ratio Feature Compression (ARFC) framework, which supports any compression ratio with a single model, eliminating the need for multiple specialized models. At its core, the Arbitrary Ratio Compressor (ARC) is an auto-regressive model that performs compression via next-token prediction. This allows the compression ratio to be controlled at inference simply by adjusting the number of generated tokens. To enhance the quality of the compressed features, two key modules are introduced. The Mixture of Solutions (MoS) module refines the compressed tokens by utilizing multiple compression results (solutions), reducing uncertainty and improving robustness. The Entity Relation Graph Constraint (ERGC) is integrated into the training process to preserve semantic and structural relationships during compression. Extensive experiments on cross-modal retrieval, image classification, and image retrieval tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches at various compression ratios. Notably, in some cases, it even surpasses the performance of the original, uncompressed features. These results validate the effectiveness and versatility of ARFC for practical, resource-constrained scenarios.",
      "authors": [
        "Yufan Liu",
        "Daoyuan Ren",
        "Zhipeng Zhang",
        "Wenyang Luo",
        "Bing Li",
        "Weiming Hu",
        "Stephen Maybank"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-12 02:38:57+00:00",
      "link": "https://arxiv.org/pdf/2602.11494v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11491v1",
      "title": "Exploring Multiple High-Scoring Subspaces in Generative Flow Networks",
      "abstract": "As a probabilistic sampling framework, Generative Flow Networks (GFlowNets) show strong potential for constructing complex combinatorial objects through the sequential composition of elementary components. However, existing GFlowNets often suffer from excessive exploration over vast state spaces, leading to over-sampling of low-reward regions and convergence to suboptimal distributions. Effectively biasing GFlowNets toward high-reward solutions remains a non-trivial challenge. In this paper, we propose CMAB-GFN, which integrates a combinatorial multi-armed bandit (CMAB) framework with GFlowNet policies. The CMAB component prunes low-quality actions, yielding compact high-scoring subspaces for exploration. Restricting GFNs to these compact high-scoring subspaces accelerates the discovery of high-value candidates, while the exploration of different subspaces ensures that diversity is not sacrificed. Experimental results on multiple tasks demonstrate that CMAB-GFN generates higher-reward candidates than existing approaches.",
      "authors": [
        "Xuan Yu",
        "Xu Wang",
        "Rui Zhu",
        "Yudong Zhang",
        "Yang Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 02:30:52+00:00",
      "link": "https://arxiv.org/pdf/2602.11491v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11476v1",
      "title": "Bounded Local Generator Classes for Deterministic State Evolution",
      "abstract": "We formalize a constructive subclass of locality-preserving deterministic operators acting on graph-indexed state systems. We define the class of Bounded Local Generator Classes (BLGC), consisting of finite-range generators operating on bounded state spaces under deterministic composition. Within this class, incremental update cost is independent of total system dimension. We prove that, under the BLGC assumptions, per-step operator work satisfies W_t = O(1) as the number of nodes M \\to \\infty, establishing a structural decoupling between global state size and incremental computational effort. The framework admits a Hilbert-space embedding in \\ell^2(V; \\mathbb{R}^d) and yields bounded operator norms on admissible subspaces. The result applies specifically to the defined subclass and does not claim universality beyond the stated locality and boundedness constraints.",
      "authors": [
        "R. Jay Martin"
      ],
      "primary_category": "cs.OS",
      "categories": [
        "cs.OS",
        "cs.DS"
      ],
      "published": "2026-02-12 01:24:27+00:00",
      "link": "https://arxiv.org/pdf/2602.11476v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11467v1",
      "title": "PRISM: A 3D Probabilistic Neural Representation for Interpretable Shape Modeling",
      "abstract": "Understanding how anatomical shapes evolve in response to developmental covariates and quantifying their spatially varying uncertainties is critical in healthcare research. Existing approaches typically rely on global time-warping formulations that ignore spatially heterogeneous dynamics. We introduce PRISM, a novel framework that bridges implicit neural representations with uncertainty-aware statistical shape analysis. PRISM models the conditional distribution of shapes given covariates, providing spatially continuous estimates of both the population mean and covariate-dependent uncertainty at arbitrary locations. A key theoretical contribution is a closed-form Fisher Information metric that enables efficient, analytically tractable local temporal uncertainty quantification via automatic differentiation. Experiments on three synthetic datasets and one clinical dataset demonstrate PRISM's strong performance across diverse tasks within a unified framework, while providing interpretable and clinically meaningful uncertainty estimates.",
      "authors": [
        "Yining Jiao",
        "Sreekalyani Bhamidi",
        "Carlton Jude Zdanski",
        "Julia S Kimbell",
        "Andrew Prince",
        "Cameron P Worden",
        "Samuel Kirse",
        "Christopher Rutter",
        "Benjamin H Shields",
        "Jisan Mahmud",
        "Marc Niethammer"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 00:55:31+00:00",
      "link": "https://arxiv.org/pdf/2602.11467v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11460v1",
      "title": "ADRD-Bench: A Preliminary LLM Benchmark for Alzheimer's Disease and Related Dementias",
      "abstract": "Large language models (LLMs) have shown great potential for healthcare applications. However, existing evaluation benchmarks provide minimal coverage of Alzheimer's Disease and Related Dementias (ADRD). To address this gap, we introduce ADRD-Bench, the first ADRD-specific benchmark dataset designed for rigorous evaluation of LLMs. ADRD-Bench has two components: 1) ADRD Unified QA, a synthesis of 1,352 questions consolidated from seven established medical benchmarks, providing a unified assessment of clinical knowledge; and 2) ADRD Caregiving QA, a novel set of 149 questions derived from the Aging Brain Care (ABC) program, a widely used, evidence-based brain health management program. Guided by a program with national expertise in comprehensive ADRD care, this new set was designed to mitigate the lack of practical caregiving context in existing benchmarks. We evaluated 33 state-of-the-art LLMs on the proposed ADRD-Bench. Results showed that the accuracy of open-weight general models ranged from 0.63 to 0.93 (mean: 0.78; std: 0.09). The accuracy of open-weight medical models ranged from 0.48 to 0.93 (mean: 0.82; std: 0.13). The accuracy of closed-source general models ranged from 0.83 to 0.91 (mean: 0.89; std: 0.03). While top-tier models achieved high accuracies (>0.9), case studies revealed that inconsistent reasoning quality and stability limit their reliability, highlighting a critical need for domain-specific improvement to enhance LLMs' knowledge and reasoning grounded in daily caregiving data. The entire dataset is available at https://github.com/IIRL-ND/ADRD-Bench.",
      "authors": [
        "Guangxin Zhao",
        "Jiahao Zheng",
        "Malaz Boustani",
        "Jarek Nabrzyski",
        "Meng Jiang",
        "Yiyu Shi",
        "Zhi Zheng"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-12 00:38:21+00:00",
      "link": "https://arxiv.org/pdf/2602.11460v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11455v1",
      "title": "Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attention connectivity and find that only a small fraction of tokens (approximately 15%) exhibit strong visual-textual coupling. These high-connectivity tokens act as anchors that ground reasoning in the image, while the majority follow linguistic patterns. During RLVR training, credit assignment naturally concentrates on these anchors, sharpening their visual grounding over time. Building on this insight, we propose Anchor-Token Reinforcement Learning (AT-RL), a lightweight framework that selectively reinforces high-connectivity tokens via graph-based clustering of attention topology. Evaluated across the series (3B-32B), AT-RL introduces only 1.2% overhead yet enables the 32B model to surpass the 72B-Instruct baseline on MathVista (80.2), with consistent gains observed across STEM, video and general tasks. Conversely, training solely on low-connectivity tokens causes severe degradation, confirming that effective multimodal RL hinges on precise credit assignment to visual anchors. Our work reveals that reasoning quality is governed not by token quantity but by the fidelity of cross-modal anchoring.",
      "authors": [
        "Zhengbo Jiao",
        "Shaobo Wang",
        "Zifan Zhang",
        "Wei Wang",
        "Bing Zhao",
        "Hu Wei",
        "Linfeng Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 00:20:54+00:00",
      "link": "https://arxiv.org/pdf/2602.11455v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11453v1",
      "title": "From Noise to Order: Learning to Rank via Denoising Diffusion",
      "abstract": "In information retrieval (IR), learning-to-rank (LTR) methods have traditionally limited themselves to discriminative machine learning approaches that model the probability of the document being relevant to the query given some feature representation of the query-document pair. In this work, we propose an alternative denoising diffusion-based deep generative approach to LTR that instead models the full joint distribution over feature vectors and relevance labels. While in the discriminative setting, an over-parameterized ranking model may find different ways to fit the training data, we hypothesize that candidate solutions that can explain the full data distribution under the generative setting produce more robust ranking models. With this motivation, we propose DiffusionRank that extends TabDiff, an existing denoising diffusion-based generative model for tabular datasets, to create generative equivalents of classical discriminative pointwise and pairwise LTR objectives. Our empirical results demonstrate significant improvements from DiffusionRank models over their discriminative counterparts. Our work points to a rich space for future research exploration on how we can leverage ongoing advancements in deep generative modeling approaches, such as diffusion, for learning-to-rank in IR.",
      "authors": [
        "Sajad Ebrahimi",
        "Bhaskar Mitra",
        "Negar Arabzadeh",
        "Ye Yuan",
        "Haolun Wu",
        "Fattane Zarrinkalam",
        "Ebrahim Bagheri"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-12 00:02:37+00:00",
      "link": "https://arxiv.org/pdf/2602.11453v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11451v1",
      "title": "LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation",
      "abstract": "Looped Transformers have emerged as an efficient and powerful class of models for reasoning in the language domain. Recent studies show that these models achieve strong performance on algorithmic and reasoning tasks, suggesting that looped architectures possess an inductive bias toward latent reasoning. However, prior approaches fix the number of loop iterations during training and inference, leaving open the question of whether these models can flexibly adapt their computational depth under variable compute budgets. We introduce LoopFormer, a looped Transformer trained on variable-length trajectories to enable budget-conditioned reasoning. Our core contribution is a shortcut-consistency training scheme that aligns trajectories of different lengths, ensuring that shorter loops yield informative representations while longer loops continue to refine them. LoopFormer conditions each loop on the current time and step size, enabling representations to evolve consistently across trajectories of varying length rather than drifting or stagnating. Empirically, LoopFormer demonstrates robust performance on language modeling and reasoning benchmarks even under aggressive compute constraints, while scaling gracefully with additional budget. These results show that looped Transformers are inherently suited for adaptive language modeling, opening a path toward controllable and budget-aware large language models.",
      "authors": [
        "Ahmadreza Jeddi",
        "Marco Ciccone",
        "Babak Taati"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 23:58:28+00:00",
      "link": "https://arxiv.org/pdf/2602.11451v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11444v1",
      "title": "Towards Reliable Machine Translation: Scaling LLMs for Critical Error Detection and Safety",
      "abstract": "Machine Translation (MT) plays a pivotal role in cross-lingual information access, public policy communication, and equitable knowledge dissemination. However, critical meaning errors, such as factual distortions, intent reversals, or biased translations, can undermine the reliability, fairness, and safety of multilingual systems. In this work, we explore the capacity of instruction-tuned Large Language Models (LLMs) to detect such critical errors, evaluating models across a range of parameters using the publicly accessible data sets. Our findings show that model scaling and adaptation strategies (zero-shot, few-shot, fine-tuning) yield consistent improvements, outperforming encoder-only baselines like XLM-R and ModernBERT. We argue that improving critical error detection in MT contributes to safer, more trustworthy, and socially accountable information systems by reducing the risk of disinformation, miscommunication, and linguistic harm, especially in high-stakes or underrepresented contexts. This work positions error detection not merely as a technical challenge, but as a necessary safeguard in the pursuit of just and responsible multilingual AI. The code will be made available at GitHub.",
      "authors": [
        "Muskaan Chopra",
        "Lorenz Sparrenberg",
        "Rafet Sifa"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-11 23:47:39+00:00",
      "link": "https://arxiv.org/pdf/2602.11444v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11443v1",
      "title": "Filtered Approximate Nearest Neighbor Search in Vector Databases: System Design and Performance Analysis",
      "abstract": "Retrieval-Augmented Generation (RAG) applications increasingly rely on Filtered Approximate Nearest Neighbor Search (FANNS) to combine semantic retrieval with metadata constraints. While algorithmic innovations for FANNS have been proposed, there remains a lack of understanding regarding how generic filtering strategies perform within Vector Databases. In this work, we systematize the taxonomy of filtering strategies and evaluate their integration into FAISS, Milvus, and pgvector. To provide a robust benchmarking framework, we introduce a new relational dataset, \\textit{MoReVec}, consisting of two tables, featuring 768-dimensional text embeddings and a rich schema of metadata attributes. We further propose the \\textit{Global-Local Selectivity (GLS)} correlation metric to quantify the relationship between filters and query vectors.   Our experiments reveal that algorithmic adaptations within the engine often override raw index performance. Specifically, we find that: (1) \\textit{Milvus} achieves superior recall stability through hybrid approximate/exact execution; (2) \\textit{pgvector}'s cost-based query optimizer frequently selects suboptimal execution plans, favoring approximate index scans even when exact sequential scans would yield perfect recall at comparable latency; and (3) partition-based indexes (IVFFlat) outperform graph-based indexes (HNSW) for low-selectivity queries. To facilitate this analysis, we extend the widely-used \\textit{ANN-Benchmarks} to support filtered vector search and make it available online. Finally, we synthesize our findings into a set of practical guidelines for selecting index types and configuring query optimizers for hybrid search workloads.",
      "authors": [
        "Abylay Amanbayev",
        "Brian Tsan",
        "Tri Dang",
        "Florin Rusu"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.IR"
      ],
      "published": "2026-02-11 23:40:26+00:00",
      "link": "https://arxiv.org/pdf/2602.11443v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11424v1",
      "title": "Gradients Must Earn Their Influence: Unifying SFT with Generalized Entropic Objectives",
      "abstract": "Standard negative log-likelihood (NLL) for Supervised Fine-Tuning (SFT) applies uniform token-level weighting. This rigidity creates a two-fold failure mode: (i) overemphasizing low-probability targets can amplify gradients on noisy supervision and disrupt robust priors, and (ii) uniform weighting provides weak sharpening when the model is already confident. Existing methods fail to resolve the resulting plasticity--stability dilemma, often suppressing necessary learning signals alongside harmful ones. To address this issue, we unify token-level SFT objectives within a generalized deformed-log family and expose a universal gate $\\times$ error gradient structure, where the gate controls how much the model trusts its current prediction. By employing the Cayley transform, we map the model's continuously evolving uncertainty onto a continuous focus trajectory, which enables seamless interpolation between scenarios involving uncertain novel concepts and those involving well-established knowledge. We then introduce Dynamic Entropy Fine-Tuning (DEFT), a parameter-free objective that modulates the trust gate using distribution concentration (Rényi-2 entropy) as a practical proxy for the model's predictive state. Extensive experiments and analyses demonstrate that DEFT achieves a better balance between exploration and exploitation, leading to improved overall performance.",
      "authors": [
        "Zecheng Wang",
        "Deyuan Liu",
        "Chunshan Li",
        "Yupeng Zhang",
        "Zhengyun Zhao",
        "Dianhui Chu",
        "Bingning Wang",
        "Dianbo Sui"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-11 22:56:43+00:00",
      "link": "https://arxiv.org/pdf/2602.11424v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11414v1",
      "title": "A physics-informed data-driven framework for modeling hyperelastic materials with progressive damage and failure",
      "abstract": "This work presents a two-stage physics-informed, data-driven constitutive modeling framework for hyperelastic soft materials undergoing progressive damage and failure. The framework is grounded in the concept of hyperelasticity with energy limiters and employs Gaussian Process Regression (GPR) to separately learn the intact (undamaged) elastic response and damage evolution directly from data. In Stage I, GPR models learn the intact hyperelastic response through volumetric and isochoric response functions (or only the isochoric response under incompressibility), ensuring energetic consistency of the intact response and satisfaction of fundamental principles such as material frame indifference and balance of angular momentum. In Stage II, damage is modeled via a separate GPR model that learns the mapping between the intact strain energy density predicted by Stage I models and a stress-reduction factor governing damage and failure, with monotonicity, non-negativity, and complete-failure constraints enforced through penalty-based optimization to ensure thermodynamic admissibility. Validation on synthetic datasets, including benchmarking against analytical constitutive models and competing data-driven approaches, demonstrates high in-distribution accuracy under uniaxial tension and robust generalization from limited training data to compression and shear modes not used during training. Application to experimental brain tissue data demonstrates the practical applicability of the framework and enables inference of damage evolution and critical failure energy. Overall, the proposed framework combines the physical consistency, interpretability, and generalizability of analytical models with the flexibility, predictive accuracy, and automation of machine learning, offering a powerful approach for modeling failure in soft materials under limited experimental data.",
      "authors": [
        "Kshitiz Upadhyay"
      ],
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE",
        "cond-mat.mtrl-sci",
        "cond-mat.soft"
      ],
      "published": "2026-02-11 22:31:49+00:00",
      "link": "https://arxiv.org/pdf/2602.11414v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11411v1",
      "title": "Improving the Robustness of Large Language Models for Code Tasks via Fine-tuning with Perturbed Data",
      "abstract": "Context: In the fast-paced evolution of software development, Large Language Models (LLMs) have become indispensable tools for tasks such as code generation, completion, analysis, and bug fixing. Ensuring the robustness of these models against potential vulnerabilities from handling diverse inputs is critical, as variations in input can lead to incorrect or insecure code outputs.   Objective: This work aims to improve the robustness of LLMs for coding-related tasks against potential adversarial inputs. Specifically, we investigate how fine-tuning LLMs with perturbed datasets impacts their robustness against input perturbations.   Method: We systematically evaluated LLM robustness by fine-tuning models using datasets perturbed at character-level, word-level, and sentence-level, comparing results against base models and models fine-tuned on unperturbed datasets.   Results: Fine-tuning LLMs with perturbed datasets significantly improves model robustness (RD usually drops around 4\\% - 6\\%), especially for models with relatively weak robustness. However, this fine-tuning process typically results in a slight performance decrease (pass@1 usually drops around 1\\% - 3\\%) compared to fine-tuning with unperturbed datasets, although occasional performance improvements are observed.   Conclusion \\& Implications: Fine-tuning LLMs for coding tasks with perturbed data effectively enhances their robustness at the cost of a minor performance reduction, emphasizing the importance of balancing the robustness and performance of LLMs for coding applications.",
      "authors": [
        "Yang Liu",
        "Armstrong Foundjem",
        "Xingfang Wu",
        "Heng Li",
        "Foutse Khomh"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-11 22:30:01+00:00",
      "link": "https://arxiv.org/pdf/2602.11411v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11410v1",
      "title": "CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer",
      "abstract": "Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.",
      "authors": [
        "David Pardoe",
        "Neil Daftary",
        "Miro Furtado",
        "Aditya Aiyer",
        "Yu Wang",
        "Liuqing Li",
        "Tao Song",
        "Lars Hertel",
        "Young Jin Yun",
        "Senthil Radhakrishnan",
        "Zhiwei Wang",
        "Tommy Li",
        "Khai Tran",
        "Ananth Nagarajan",
        "Ali Naqvi",
        "Yue Zhang",
        "Renpeng Fang",
        "Avi Romascanu",
        "Arjun Kulothungun",
        "Deepak Kumar",
        "Praneeth Boda",
        "Fedor Borisyuk",
        "Ruoyan Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 22:24:33+00:00",
      "link": "https://arxiv.org/pdf/2602.11410v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11408v1",
      "title": "GHOST: Unmasking Phantom States in Mamba2 via Grouped Hidden-state Output-aware Selection & Truncation",
      "abstract": "While Mamba2's expanded state dimension enhances temporal modeling, it incurs substantial inference overhead that saturates bandwidth during autoregressive generation. Standard pruning methods fail to address this bottleneck: unstructured sparsity leaves activations dense, magnitude-based selection ignores runtime dynamics, and gradient-based methods impose prohibitive costs. We introduce GHOST (Grouped Hidden-state Output-aware Selection and Truncation), a structured pruning framework that approximates control-theoretic balanced truncation using only forward-pass statistics. By jointly measuring controllability and observability, GHOST rivals the fidelity of gradient-based methods without requiring backpropagation. As a highlight, on models ranging from 130M to 2.7B parameters, our approach achieves a 50\\% state-dimension reduction with approximately 1 perplexity point increase on WikiText-2. Code is available at https://anonymous.4open.science/r/mamba2_ghost-7BCB/.",
      "authors": [
        "Michael Menezes",
        "Anastasios Kyrillidis"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "published": "2026-02-11 22:20:10+00:00",
      "link": "https://arxiv.org/pdf/2602.11408v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11388v1",
      "title": "Sparse Semantic Dimension as a Generalization Certificate for LLMs",
      "abstract": "Standard statistical learning theory predicts that Large Language Models (LLMs) should overfit because their parameter counts vastly exceed the number of training tokens. Yet, in practice, they generalize robustly. We propose that the effective capacity controlling generalization lies in the geometry of the model's internal representations: while the parameter space is high-dimensional, the activation states lie on a low-dimensional, sparse manifold. To formalize this, we introduce the Sparse Semantic Dimension (SSD), a complexity measure derived from the active feature vocabulary of a Sparse Autoencoder (SAE) trained on the model's layers. Treating the LLM and SAE as frozen oracles, we utilize this framework to attribute the model's generalization capabilities to the sparsity of the dictionary rather than the total parameter count. Empirically, we validate this framework on GPT-2 Small and Gemma-2B, demonstrating that our bound provides non-vacuous certificates at realistic sample sizes. Crucially, we uncover a counter-intuitive \"feature sharpness\" scaling law: despite being an order of magnitude larger, Gemma-2B requires significantly fewer calibration samples to identify its active manifold compared to GPT-2, suggesting that larger models learn more compressible, distinct semantic structures. Finally, we show that this framework functions as a reliable safety monitor: out-of-distribution inputs trigger a measurable \"feature explosion\" (a sharp spike in active features), effectively signaling epistemic uncertainty through learned feature violation. Code is available at: https://github.com/newcodevelop/sparse-semantic-dimension.",
      "authors": [
        "Dibyanayan Bandyopadhyay",
        "Asif Ekbal"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-11 21:45:18+00:00",
      "link": "https://arxiv.org/pdf/2602.11388v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11387v1",
      "title": "Provably Efficient Algorithms for S- and Non-Rectangular Robust MDPs with General Parameterization",
      "abstract": "We study robust Markov decision processes (RMDPs) with general policy parameterization under s-rectangular and non-rectangular uncertainty sets. Prior work is largely limited to tabular policies, and hence either lacks sample complexity guarantees or incurs high computational cost. Our method reduces the average reward RMDPs to entropy-regularized discounted robust MDPs, restoring strong duality and enabling tractable equilibrium computation. We prove novel Lipschitz and Lipschitz-smoothness properties for general policy parameterizations that extends to infinite state spaces. To address infinite-horizon gradient estimation, we introduce a multilevel Monte Carlo gradient estimator with $\\tilde{\\mathcal{O}}(ε^{-2})$ sample complexity, a factor of $\\mathcal{O}(ε^{-2})$ improvement over prior work. Building on this, we design a projected gradient descent algorithm for s-rectangular uncertainty ($\\mathcal{O}(ε^{-5})$) and a Frank--Wolfe algorithm for non-rectangular uncertainty ($\\mathcal{O}(ε^{-4})$ discounted, $\\mathcal{O}(ε^{-10.5})$ average reward), significantly improving prior results in both the discounted setting and average reward setting. Our work is the first one to provide sample complexity guarantees for RMDPs with general policy parameterization beyond $(s, a)$-rectangularity. It also provides the first such guarantees in the average reward setting and improves existing bounds for discounted robust MDPs.",
      "authors": [
        "Anirudh Satheesh",
        "Ziyi Chen",
        "Furong Huang",
        "Heng Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 21:44:20+00:00",
      "link": "https://arxiv.org/pdf/2602.11387v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11374v1",
      "title": "Retrieval-Aware Distillation for Transformer-SSM Hybrids",
      "abstract": "State-space models (SSMs) offer efficient sequence modeling but lag behind Transformers on benchmarks that require in-context retrieval. Prior work links this gap to a small set of attention heads, termed Gather-and-Aggregate (G&A), which SSMs struggle to reproduce. We propose *retrieval-aware distillation*, which converts a pretrained Transformer into a hybrid student by preserving only these retrieval-critical heads and distilling the rest into recurrent heads. We identify the essential heads via ablation on a synthetic retrieval task, producing a hybrid with sparse, non-uniform attention placement. We show that preserving **just 2% of attention heads recovers over 95% of teacher performance on retrieval-heavy tasks** (10 heads in a 1B model), requiring far fewer heads than hybrids that retain at least 25%. We further find that large recurrent states often compensate for missing retrieval: once retrieval is handled by these heads, the SSM backbone can be simplified with limited loss, even with an $8\\times$ reduction in state dimension. By reducing both the attention cache and the SSM state, the resulting hybrid is $5$--$6\\times$ more memory-efficient than comparable hybrids, closing the Transformer--SSM gap at a fraction of the memory cost.",
      "authors": [
        "Aviv Bick",
        "Eric P. Xing",
        "Albert Gu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 21:05:00+00:00",
      "link": "https://arxiv.org/pdf/2602.11374v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11364v1",
      "title": "The Energy of Falsehood: Detecting Hallucinations via Diffusion Model Likelihoods",
      "abstract": "Large Language Models (LLMs) frequently hallucinate plausible but incorrect assertions, a vulnerability often missed by uncertainty metrics when models are confidently wrong. We propose DiffuTruth, an unsupervised framework that reconceptualizes fact verification via non equilibrium thermodynamics, positing that factual truths act as stable attractors on a generative manifold while hallucinations are unstable. We introduce the Generative Stress Test, claims are corrupted with noise and reconstructed using a discrete text diffusion model. We define Semantic Energy, a metric measuring the semantic divergence between the original claim and its reconstruction using an NLI critic. Unlike vector space errors, Semantic Energy isolates deep factual contradictions. We further propose a Hybrid Calibration fusing this stability signal with discriminative confidence. Extensive experiments on FEVER demonstrate DiffuTruth achieves a state of the art unsupervised AUROC of 0.725, outperforming baselines by 1.5 percent through the correction of overconfident predictions. Furthermore, we show superior zero shot generalization on the multi hop HOVER dataset, outperforming baselines by over 4 percent, confirming the robustness of thermodynamic truth properties to distribution shifts.",
      "authors": [
        "Arpit Singh Gautam",
        "Kailash Talreja",
        "Saurabh Jha"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-11 20:52:16+00:00",
      "link": "https://arxiv.org/pdf/2602.11364v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11361v1",
      "title": "Finding the Cracks: Improving LLMs Reasoning with Paraphrastic Probing and Consistency Verification",
      "abstract": "Large language models have demonstrated impressive performance across a variety of reasoning tasks. However, their problem-solving ability often declines on more complex tasks due to hallucinations and the accumulation of errors within these intermediate steps. Recent work has introduced the notion of critical tokens--tokens in the reasoning process that exert significant influence on subsequent steps. Prior studies suggest that replacing critical tokens can refine reasoning trajectories. Nonetheless, reliably identifying and exploiting critical tokens remains challenging. To address this, we propose the Paraphrastic Probing and Consistency Verification~(PPCV) framework. PPCV operates in two stages. In the first stage, we roll out an initial reasoning path from the original question and then concatenate paraphrased versions of the question with this reasoning path. And we identify critical tokens based on mismatches between the predicted top-1 token and the expected token in the reasoning path. A criterion is employed to confirm the final critical token. In the second stage, we substitute critical tokens with candidate alternatives and roll out new reasoning paths for both the original and paraphrased questions. The final answer is determined by checking the consistency of outputs across these parallel reasoning processes. We evaluate PPCV on mainstream LLMs across multiple benchmarks. Extensive experiments demonstrate PPCV substantially enhances the reasoning performance of LLMs compared to baselines.",
      "authors": [
        "Weili Shi",
        "Dongliang Guo",
        "Lehan Yang",
        "Tianlong Wang",
        "Hanzhang Yuan",
        "Sheng Li"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-11 20:48:52+00:00",
      "link": "https://arxiv.org/pdf/2602.11361v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11346v1",
      "title": "Divide and Learn: Multi-Objective Combinatorial Optimization at Scale",
      "abstract": "Multi-objective combinatorial optimization seeks Pareto-optimal solutions over exponentially large discrete spaces, yet existing methods sacrifice generality, scalability, or theoretical guarantees. We reformulate it as an online learning problem over a decomposed decision space, solving position-wise bandit subproblems via adaptive expert-guided sequential construction. This formulation admits regret bounds of $O(d\\sqrt{T \\log T})$ depending on subproblem dimensionality \\(d\\) rather than combinatorial space size. On standard benchmarks, our method achieves 80--98\\% of specialized solvers performance while achieving two to three orders of magnitude improvement in sample and computational efficiency over Bayesian optimization methods. On real-world hardware-software co-design for AI accelerators with expensive simulations, we outperform competing methods under fixed evaluation budgets. The advantage grows with problem scale and objective count, establishing bandit optimization over decomposed decision spaces as a principled alternative to surrogate modeling or offline training for multi-objective optimization.",
      "authors": [
        "Esha Singh",
        "Dongxia Wu",
        "Chien-Yi Yang",
        "Tajana Rosing",
        "Rose Yu",
        "Yi-An Ma"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 20:29:35+00:00",
      "link": "https://arxiv.org/pdf/2602.11346v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11332v1",
      "title": "Sample-Free Safety Assessment of Neural Network Controllers via Taylor Methods",
      "abstract": "In recent years, artificial neural networks have been increasingly studied as feedback controllers for guidance problems. While effective in complex scenarios, they lack the verification guarantees found in classical guidance policies. Their black-box nature creates significant concerns regarding trustworthiness, limiting their adoption in safety-critical spaceflight applications. This work addresses this gap by developing a method to assess the safety of a trained neural network feedback controller via automatic domain splitting and polynomial bounding. The methodology involves embedding the trained neural network into the system's dynamical equations, rendering the closed-loop system autonomous. The system flow is then approximated by high-order Taylor polynomials, which are subsequently manipulated to construct polynomial maps that project state uncertainties onto an event manifold. Automatic domain splitting ensures the polynomials are accurate over their relevant subdomains, whilst also allowing an extensive state-space to be analysed efficiently. Utilising polynomial bounding techniques, the resulting event values may be rigorously constrained and analysed within individual subdomains, thereby establishing bounds on the range of possible closed-loop outcomes from using such neural network controllers and supporting safety assessment and informed operational decision-making in real-world missions.",
      "authors": [
        "Adam Evans",
        "Roberto Armellin"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-02-11 20:06:04+00:00",
      "link": "https://arxiv.org/pdf/2602.11332v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11325v2",
      "title": "Amortised and provably-robust simulation-based inference",
      "abstract": "Complex simulator-based models are now routinely used to perform inference across the sciences and engineering, but existing inference methods are often unable to account for outliers and other extreme values in data which occur due to faulty measurement instruments or human error. In this paper, we introduce a novel approach to simulation-based inference grounded in generalised Bayesian inference and a neural approximation of a weighted score-matching loss. This leads to a method that is both amortised and provably robust to outliers, a combination not achieved by existing approaches. Furthermore, through a carefully chosen conditional density model, we demonstrate that inference can be further simplified and performed without the need for Markov chain Monte Carlo sampling, thereby offering significant computational advantages, with complexity that is only a small fraction of that of current state-of-the-art approaches.",
      "authors": [
        "Ayush Bharti",
        "Charita Dellaporta",
        "Yuga Hikida",
        "François-Xavier Briol"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.CO",
        "stat.ME"
      ],
      "published": "2026-02-11 19:54:27+00:00",
      "link": "https://arxiv.org/pdf/2602.11325v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11322v1",
      "title": "Predictive Associative Memory: Retrieval Beyond Similarity Through Temporal Co-occurrence",
      "abstract": "Current approaches to memory in neural systems rely on similarity-based retrieval: given a query, find the most representationally similar stored state. This assumption -- that useful memories are similar memories -- fails to capture a fundamental property of biological memory: association through temporal co-occurrence. We propose Predictive Associative Memory (PAM), an architecture in which a JEPA-style predictor, trained on temporal co-occurrence within a continuous experience stream, learns to navigate the associative structure of an embedding space. We introduce an Inward JEPA that operates over stored experience (predicting associatively reachable past states) as the complement to the standard Outward JEPA that operates over incoming sensory data (predicting future states). We evaluate PAM as an associative recall system -- testing faithfulness of recall for experienced associations -- rather than as a retrieval system evaluated on generalisation to unseen associations. On a synthetic benchmark, the predictor's top retrieval is a true temporal associate 97% of the time (Association Precision@1 = 0.970); it achieves cross-boundary Recall@20 = 0.421 where cosine similarity scores zero; and it separates experienced-together from never-experienced-together states with a discrimination AUC of 0.916 (cosine: 0.789). Even restricted to cross-room pairs where embedding similarity is uninformative, the predictor achieves AUC = 0.849 (cosine: 0.503, chance). A temporal shuffle control confirms the signal is genuine temporal co-occurrence structure, not embedding geometry: shuffling collapses cross-boundary recall by 90%, replicated across training seeds. All results are stable across seeds (SD < 0.006) and query selections (SD $\\leq$ 0.012).",
      "authors": [
        "Jason Dury"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "published": "2026-02-11 19:51:22+00:00",
      "link": "https://arxiv.org/pdf/2602.11322v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11320v2",
      "title": "Efficient Analysis of the Distilled Neural Tangent Kernel",
      "abstract": "Neural tangent kernel (NTK) methods are computationally limited by the need to evaluate large Jacobians across many data points. Existing approaches reduce this cost primarily through projecting and sketching the Jacobian. We show that NTK computation can also be reduced by compressing the data dimension itself using NTK-tuned dataset distillation. We demonstrate that the neural tangent space spanned by the input data can be induced by dataset distillation, yielding a 20-100$\\times$ reduction in required Jacobian calculations. We further show that per-class NTK matrices have low effective rank that is preserved by this reduction. Building on these insights, we propose the distilled neural tangent kernel (DNTK), which combines NTK-tuned dataset distillation with state-of-the-art projection methods to reduce up NTK computational complexity by up to five orders of magnitude while preserving kernel structure and predictive performance.",
      "authors": [
        "Jamie Mahowald",
        "Brian Bell",
        "Alex Ho",
        "Michael Geyer"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 19:45:55+00:00",
      "link": "https://arxiv.org/pdf/2602.11320v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11316v1",
      "title": "Selective Prior Synchronization via SYNC Loss",
      "abstract": "Prediction under uncertainty is a critical requirement for the deep neural network to succeed responsibly. This paper focuses on selective prediction, which allows DNNs to make informed decisions about when to predict or abstain based on the uncertainty level of their predictions. Current methods are either ad-hoc such as SelectiveNet, focusing on how to modify the network architecture or objective function, or post-hoc such as softmax response, achieving selective prediction through analyzing the model's probabilistic outputs. We observe that post-hoc methods implicitly generate uncertainty information, termed the selective prior, which has traditionally been used only during inference. We argue that the selective prior provided by the selection mechanism is equally vital during the training stage. Therefore, we propose the SYNC loss which introduces a novel integration of ad-hoc and post-hoc method. Specifically, our approach incorporates the softmax response into the training process of SelectiveNet, enhancing its selective prediction capabilities by examining the selective prior. Evaluated across various datasets, including CIFAR-100, ImageNet-100, and Stanford Cars, our method not only enhances the model's generalization capabilities but also surpasses previous works in selective prediction performance, and sets new benchmarks for state-of-the-art performance.",
      "authors": [
        "Ishan Mishra",
        "Jiajie Li",
        "Deepak Mishra",
        "Jinjun Xiong"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-11 19:43:00+00:00",
      "link": "https://arxiv.org/pdf/2602.11316v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11305v1",
      "title": "Are Aligned Large Language Models Still Misaligned?",
      "abstract": "Misalignment in Large Language Models (LLMs) arises when model behavior diverges from human expectations and fails to simultaneously satisfy safety, value, and cultural dimensions, which must co-occur in real-world settings to solve a real-world query. Existing misalignment benchmarks-such as INSECURE CODE (safety-centric), VALUEACTIONLENS (value-centric), and CULTURALHERITAGE (culture centric)-rely on evaluating misalignment along individual dimensions, preventing simultaneous evaluation. To address this gap, we introduce Mis-Align Bench, a unified benchmark for analyzing misalignment across safety, value, and cultural dimensions. First we constructs SAVACU, an English misaligned-aligned dataset of 382,424 samples spanning 112 domains (or labels), by reclassifying prompts from the LLM-PROMPT-DATASET via taxonomy into 14 safety domains, 56 value domains, and 42 cultural domains using Mistral-7B-Instruct-v0.3, and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based fingerprint to avoid deduplication. Furthermore, we pairs prompts with misaligned and aligned responses via two-stage rejection sampling to enforce quality. Second we benchmarks general-purpose, fine-tuned, and open-weight LLMs, enabling systematic evaluation of misalignment under three dimensions. Empirically, single-dimension models achieve high Coverage (upto 97.6%) but incur False Failure Rate >50% and lower Alignment Score (63%-66%) under joint conditions.",
      "authors": [
        "Usman Naseem",
        "Gautam Siddharth Kashyap",
        "Rafiq Ali",
        "Ebad Shabbir",
        "Sushant Kumar Ray",
        "Abdullah Mohammad",
        "Agrima Seth"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 19:30:43+00:00",
      "link": "https://arxiv.org/pdf/2602.11305v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11304v1",
      "title": "CryptoAnalystBench: Failures in Multi-Tool Long-Form LLM Analysis",
      "abstract": "Modern analyst agents must reason over complex, high token inputs, including dozens of retrieved documents, tool outputs, and time sensitive data. While prior work has produced tool calling benchmarks and examined factuality in knowledge augmented systems, relatively little work studies their intersection: settings where LLMs must integrate large volumes of dynamic, structured and unstructured multi tool outputs. We investigate LLM failure modes in this regime using crypto as a representative high data density domain. We introduce (1) CryptoAnalystBench, an analyst aligned benchmark of 198 production crypto and DeFi queries spanning 11 categories; (2) an agentic harness equipped with relevant crypto and DeFi tools to generate responses across multiple frontier LLMs; and (3) an evaluation pipeline with citation verification and an LLM as a judge rubric spanning four user defined success dimensions: relevance, temporal relevance, depth, and data consistency. Using human annotation, we develop a taxonomy of seven higher order error types that are not reliably captured by factuality checks or LLM based quality scoring. We find that these failures persist even in state of the art systems and can compromise high stakes decisions. Based on this taxonomy, we refine the judge rubric to better capture these errors. While the judge does not align with human annotators on precise scoring across rubric iterations, it reliably identifies critical failure modes, enabling scalable feedback for developers and researchers studying analyst style agents. We release CryptoAnalystBench with annotated queries, the evaluation pipeline, judge rubrics, and the error taxonomy, and outline mitigation strategies and open challenges in evaluating long form, multi tool augmented systems.",
      "authors": [
        "Anushri Eswaran",
        "Oleg Golev",
        "Darshan Tank",
        "Sidhant Rahi",
        "Himanshu Tyagi"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published": "2026-02-11 19:29:31+00:00",
      "link": "https://arxiv.org/pdf/2602.11304v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11295v1",
      "title": "On Decision-Valued Maps and Representational Dependence",
      "abstract": "A computational engine applied to different representations of the same data can produce different discrete outcomes, with some representations preserving the result and others changing it entirely. A decision-valued map records which representations preserve the outcome and which change it, associating each member of a declared representation family with the discrete result it produces. This paper formalizes decision-valued maps and describes DecisionDB, an infrastructure that logs, replays and audits these relationships using identifiers computed from content and artifacts stored in write-once form. Deterministic replay recovers each recorded decision identifier exactly from stored artifacts, with all three identifying fields matching their persisted values. The contribution partitions representation space into persistence regions and boundaries, and treats decision reuse as a mechanically checkable condition.",
      "authors": [
        "Gil Raitses"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "published": "2026-02-11 19:11:49+00:00",
      "link": "https://arxiv.org/pdf/2602.11295v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11286v1",
      "title": "Grok in the Wild: Characterizing the Roles and Uses of Large Language Models on Social Media",
      "abstract": "xAI's large language model, Grok, is called by millions of people each week on the social media platform X. Prior work characterizing how large language models are used has focused on private, one-on-one interactions. Grok's deployment on X represents a major departure from this setting, with interactions occurring in a public social space. In this paper, we systematically sample three months of interaction data to investigate how, when, and to what effect Grok is used on X. At the platform level, we find that Grok responds to 62% of requests, that the majority (51%) are in English, and that engagement is low, with half of Grok's responses receiving 20 or fewer views after 48 hours. We also inductively build a taxonomy of 10 roles that LLMs play in mediating social interactions and use these roles to analyze 41,735 interactions with Grok on X. We find that Grok most often serves as an information provider but, in contrast to LLM use in private one-on-one settings, also takes on roles related to dispute management, such as truth arbiter, advocate, and adversary. Finally, we characterize the population of X users who prompted Grok and find that their self-expressed interests are closely related to the roles the model assumes in the corresponding interactions. Our findings provide an initial quantitative description of human-AI interactions on X, and a broader understanding of the diverse roles that large language models might play in our online social spaces.",
      "authors": [
        "Katelyn Xiaoying Mei",
        "Robert Wolfe",
        "Nicholas Weber",
        "Martin Saveski"
      ],
      "primary_category": "cs.SI",
      "categories": [
        "cs.SI",
        "cs.CY"
      ],
      "published": "2026-02-11 19:06:22+00:00",
      "link": "https://arxiv.org/pdf/2602.11286v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11151v2",
      "title": "Diffusion-Pretrained Dense and Contextual Embeddings",
      "abstract": "In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval. By leveraging bidirectional attention through diffusion-based pretraining, our models capture comprehensive bidirectional context within passages, enabling the use of mean pooling and a late chunking strategy to better preserve global context across long documents. We release two model types: pplx-embed-v1 for standard retrieval, and pplx-embed-context-v1 for contextualized embeddings that incorporate global document context into passage representations. pplx-embed-v1 achieves competitive performance on the MTEB(Multilingual, v2), MTEB(Code), MIRACL, BERGEN, and ToolRet retrieval benchmarks, while pplx-embed-context-v1 sets new records on the ConTEB benchmark. Beyond public benchmarks, pplx-embed-v1 demonstrates strong performance on our internal evaluation suite, focusing on real-world, large-scale search scenarios constructed from 1B production web pages. These results validate the models' effectiveness in production environments where retrieval quality and efficiency are critical at scale.",
      "authors": [
        "Sedigheh Eslami",
        "Maksim Gaiduk",
        "Markus Krimmel",
        "Louis Milliken",
        "Bo Wang",
        "Denis Bykov"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-02-11 18:59:08+00:00",
      "link": "https://arxiv.org/pdf/2602.11151v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11149v1",
      "title": "Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning",
      "abstract": "Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models.",
      "authors": [
        "Dawid J. Kopiczko",
        "Sagar Vaze",
        "Tijmen Blankevoort",
        "Yuki M. Asano"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 18:58:54+00:00",
      "link": "https://arxiv.org/pdf/2602.11149v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11146v1",
      "title": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
      "abstract": "Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.",
      "authors": [
        "Gongye Liu",
        "Bo Yang",
        "Yida Zhi",
        "Zhizhou Zhong",
        "Lei Ke",
        "Didan Deng",
        "Han Gao",
        "Yongxiang Huang",
        "Kaihao Zhang",
        "Hongbo Fu",
        "Wenhan Luo"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-11 18:57:29+00:00",
      "link": "https://arxiv.org/pdf/2602.11146v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15898v1",
      "title": "MultiCube-RAG for Multi-hop Question Answering",
      "abstract": "Multi-hop question answering (QA) necessitates multi-step reasoning and retrieval across interconnected subjects, attributes, and relations. Existing retrieval-augmented generation (RAG) methods struggle to capture these structural semantics accurately, resulting in suboptimal performance. Graph-based RAGs structure such information in graphs, but the resulting graphs are often noisy and computationally expensive. Moreover, most methods rely on single-step retrieval, neglecting the need for multi-hop reasoning processes. Recent training-based approaches attempt to incentivize the large language models (LLMs) for iterative reasoning and retrieval, but their training processes are prone to unstable convergence and high computational overhead. To address these limitations, we devise an ontology-based cube structure with multiple and orthogonal dimensions to model structural subjects, attributes, and relations. Built on the cube structure, we propose MultiCube-RAG, a training-free method consisting of multiple cubes for multi-step reasoning and retrieval. Each cube specializes in modeling a class of subjects, so that MultiCube-RAG flexibly selects the most suitable cubes to acquire the relevant knowledge precisely. To enhance the query-based reasoning and retrieval, our method decomposes a complex multi-hop query into a set of simple subqueries along cube dimensions and conquers each of them sequentially. Experiments on four multi-hop QA datasets show that MultiCube-RAG improves response accuracy by 8.9% over the average performance of various baselines. Notably, we also demonstrate that our method performs with greater efficiency and inherent explainability.",
      "authors": [
        "Jimeng Shi",
        "Wei Hu",
        "Runchu Tian",
        "Bowen Jin",
        "Wonbin Kweon",
        "SeongKu Kang",
        "Yunfan Kang",
        "Dingqi Ye",
        "Sizhe Zhou",
        "Shaowen Wang",
        "Jiawei Han"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 18:54:04+00:00",
      "link": "https://arxiv.org/pdf/2602.15898v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11139v1",
      "title": "TabICLv2: A better, faster, scalable, and open tabular foundation model",
      "abstract": "Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining diversity; (2) various architectural innovations, including a new scalable softmax in attention improving generalization to larger datasets without prohibitive long-sequence pretraining; and (3) optimized pretraining protocols, notably replacing AdamW with the Muon optimizer. On the TabArena and TALENT benchmarks, TabICLv2 without any tuning surpasses the performance of the current state of the art, RealTabPFN-2.5 (hyperparameter-tuned, ensembled, and fine-tuned on real data). With only moderate pretraining compute, TabICLv2 generalizes effectively to million-scale datasets under 50GB GPU memory while being markedly faster than RealTabPFN-2.5. We provide extensive ablation studies to quantify these contributions and commit to open research by first releasing inference code and model weights at https://github.com/soda-inria/tabicl, with synthetic data engine and pretraining code to follow.",
      "authors": [
        "Jingang Qu",
        "David Holzmüller",
        "Gaël Varoquaux",
        "Marine Le Morvan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 18:51:02+00:00",
      "link": "https://arxiv.org/pdf/2602.11139v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11136v2",
      "title": "FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight",
      "abstract": "As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.",
      "authors": [
        "Jiayi Zhou",
        "Yang Sheng",
        "Hantao Lou",
        "Yaodong Yang",
        "Jie Fu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-11 18:48:11+00:00",
      "link": "https://arxiv.org/pdf/2602.11136v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11133v1",
      "title": "Just on Time: Token-Level Early Stopping for Diffusion Language Models",
      "abstract": "Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model's predictions and local context to dynamically determine when individual tokens can be finalized. This yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing the total number of diffusion steps required. Across diverse benchmarks, spanning mathematical reasoning, general question answering, and scientific understanding, our approach achieves state-of-the-art efficiency gains while preserving generation quality.",
      "authors": [
        "Zahar Kohut",
        "Severyn Shykula",
        "Dmytro Khamula",
        "Mykola Vysotskyi",
        "Taras Rumezhak",
        "Volodymyr Karpiv"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-11 18:44:04+00:00",
      "link": "https://arxiv.org/pdf/2602.11133v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11123v1",
      "title": "From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent",
      "abstract": "Accelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into executable actions for database retrieval, property prediction, structure generation, and stability evaluation. Beyond automating tool invocation, MKNA autonomously extracts quantitative thresholds and chemically meaningful design motifs from literature and database evidence, enabling data-grounded hypothesis formation. Applied to the search for high-Debye-temperature ceramics, the agent identifies a literature-supported screening criterion (Theta_D > 800 K), rediscovers canonical ultra-stiff materials such as diamond, SiC, SiN, and BeO, and proposes thermodynamically stable, previously unreported Be-C-rich compounds that populate the sparsely explored 1500-1700 K regime. These results demonstrate that MKNA not only finds stable candidates but also reconstructs interpretable design heuristics, establishing a generalizable platform for autonomous, language-guided materials exploration.",
      "authors": [
        "Genmao Zhuang",
        "Amir Barati Farimani"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-02-11 18:34:24+00:00",
      "link": "https://arxiv.org/pdf/2602.11123v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11114v1",
      "title": "Learning to Compose for Cross-domain Agentic Workflow Generation",
      "abstract": "Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily on the task distribution and the available operators. Under domain shift, current systems typically rely on iterative workflow refinement to discover a feasible workflow from a large workflow space, incurring high iteration costs and yielding unstable, domain-specific behavior. In response, we internalize a decompose-recompose-decide mechanism into an open-source LLM for cross-domain workflow generation. To decompose, we learn a compact set of reusable workflow capabilities across diverse domains. To recompose, we map each input task to a sparse composition over these bases to generate a task-specific workflow in a single pass. To decide, we attribute the success or failure of workflow generation to counterfactual contributions from learned capabilities, thereby capturing which capabilities actually drive success by their marginal effects. Across stringent multi-domain, cross-domain, and unseen-domain evaluations, our 1-pass generator surpasses SOTA refinement baselines that consume 20 iterations, while substantially reducing generation latency and cost.",
      "authors": [
        "Jialiang Wang",
        "Shengxiang Xu",
        "Hanmo Liu",
        "Jiachuan Wang",
        "Yuyu Luo",
        "Shimin Di",
        "Min-Ling Zhang",
        "Lei Chen"
      ],
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "published": "2026-02-11 18:27:22+00:00",
      "link": "https://arxiv.org/pdf/2602.11114v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11107v1",
      "title": "Renet: Principled and Efficient Relaxation for the Elastic Net via Dynamic Objective Selection",
      "abstract": "We introduce Renet, a principled generalization of the Relaxed Lasso to the Elastic Net family of estimators. While, on the one hand, $\\ell_1$-regularization is a standard tool for variable selection in high-dimensional regimes and, on the other hand, the $\\ell_2$ penalty provides stability and solution uniqueness through strict convexity, the standard Elastic Net nevertheless suffers from shrinkage bias that frequently yields suboptimal prediction accuracy. We propose to address this limitation through a framework called \\textit{relaxation}. Existing relaxation implementations rely on naive linear interpolations of penalized and unpenalized solutions, which ignore the non-linear geometry that characterizes the entire regularization path and risk violating the Karush-Kuhn-Tucker conditions. Renet addresses these limitations by enforcing sign consistency through an adaptive relaxation procedure that dynamically dispatches between convex blending and efficient sub-path refitting. Furthermore, we identify and formalize a unique synergy between relaxation and the ``One-Standard-Error'' rule: relaxation serves as a robust debiasing mechanism, allowing practitioners to leverage the parsimony of the 1-SE rule without the traditional loss in predictive fidelity. Our theoretical framework incorporates automated stability safeguards for ultra-high dimensional regimes and is supported by a comprehensive benchmarking suite across 20 synthetic and real-world datasets, demonstrating that Renet consistently outperforms the standard Elastic Net and provides a more robust alternative to the Adaptive Elastic Net in high-dimensional, low signal-to-noise ratio and high-multicollinearity regimes. By leveraging an adaptive solver backend, Renet delivers these statistical gains while offering a computational profile that remains competitive with state-of-the-art coordinate descent implementations.",
      "authors": [
        "Albert Dorador"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-11 18:22:59+00:00",
      "link": "https://arxiv.org/pdf/2602.11107v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11106v2",
      "title": "TEGRA: Text Encoding With Graph and Retrieval Augmentation for Misinformation Detection",
      "abstract": "Misinformation detection is a critical task that can benefit significantly from the integration of external knowledge, much like manual fact-checking. In this work, we propose a novel method for representing textual documents that facilitates the incorporation of information from a knowledge base. Our approach, Text Encoding with Graph (TEG), processes documents by extracting structured information in the form of a graph and encoding both the text and the graph for classification purposes. Through extensive experiments, we demonstrate that this hybrid representation enhances misinformation detection performance compared to using language models alone. Furthermore, we introduce TEGRA, an extension of our framework that integrates domain-specific knowledge, further enhancing classification accuracy in most cases.",
      "authors": [
        "Géraud Faye",
        "Wassila Ouerdane",
        "Guillaume Gadek",
        "Sylvain Gatepaille",
        "Céline Hudelot"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 18:21:17+00:00",
      "link": "https://arxiv.org/pdf/2602.11106v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11249v1",
      "title": "How to check in continually over 4,000 days on an online learning platform? An empirical experience and a practical solution",
      "abstract": "The check-in service is often provided as an incentive system by online learning platforms to help users establish a learning routine and achieve accomplishment. However, according to the questionnaire conducted in this study, 82.5% of users of online English learning platforms that feature a check-in service have failed to maintain the daily check-in behavior for long-term language learning, mainly by reason of demotivation, forgetfulness, boredom, and insufficient time. As a language learner, I have an empirical experience in maintaining a record of over 4,000 daily check-ins on China's leading online English learning platform of Shanbay. In the meantime, I have been constantly exploring a practical solution to help cultivate perseverance for other users to follow through the learning routine. In this paper, I systematically introduce this practical solution, the GILT method, and its instructions. The experience and solution for perseverance development are based on Shanbay, but they can be applied to other learning platforms for different purposes.",
      "authors": [
        "Jialiang Lin"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.HC"
      ],
      "published": "2026-02-11 18:18:08+00:00",
      "link": "https://arxiv.org/pdf/2602.11249v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11097v1",
      "title": "Statistical Learning Analysis of Physics-Informed Neural Networks",
      "abstract": "We study the training and performance of physics-informed learning for initial and boundary value problems (IBVP) with physics-informed neural networks (PINNs) from a statistical learning perspective. Specifically, we restrict ourselves to parameterizations with hard initial and boundary condition constraints and reformulate the problem of estimating PINN parameters as a statistical learning problem. From this perspective, the physics penalty on the IBVP residuals can be better understood not as a regularizing term bus as an infinite source of indirect data, and the learning process as fitting the PINN distribution of residuals $p(y \\mid x, t, w) q(x, t) $ to the true data-generating distribution $δ(0) q(x, t)$ by minimizing the Kullback-Leibler divergence between the true and PINN distributions. Furthermore, this analysis show that physics-informed learning with PINNs is a singular learning problem, and we employ singular learning theory tools, namely the so-called Local Learning Coefficient (Lau et al., 2025) to analyze the estimates of PINN parameters obtained via stochastic optimization for a heat equation IBVP. Finally, we discuss implications of this analysis on the quantification of predictive uncertainty of PINNs and the extrapolation capacity of PINNs.",
      "authors": [
        "David A. Barajas-Solano"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "physics.comp-ph"
      ],
      "published": "2026-02-11 18:09:29+00:00",
      "link": "https://arxiv.org/pdf/2602.11097v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11092v1",
      "title": "MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning",
      "abstract": "Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework designed as a discovery engine for photonic and hybrid quantum machine learning. MerLin integrates optimized strong simulation of linear optical circuits into standard PyTorch and scikit learn workflows, enabling end to end differentiable training of quantum layers. MerLin is designed around systematic benchmarking and reproducibility. As an initial contribution, we reproduce eighteen state of the art photonic and hybrid QML works spanning kernel methods, reservoir computing, convolutional and recurrent architectures, generative models, and modern training paradigms. These reproductions are released as reusable, modular experiments that can be directly extended and adapted, establishing a shared experimental baseline consistent with empirical benchmarking methodologies widely adopted in modern artificial intelligence. By embedding photonic quantum models within established machine learning ecosystems, MerLin allows practitioners to leverage existing tooling for ablation studies, cross modality comparisons, and hybrid classical quantum workflows. The framework already implements hardware aware features, allowing tests on available quantum hardware while enabling exploration beyond its current capabilities, positioning MerLin as a future proof co design tool linking algorithms, benchmarks, and hardware.",
      "authors": [
        "Cassandre Notton",
        "Benjamin Stott",
        "Philippe Schoeb",
        "Anthony Walsh",
        "Grégoire Leboucher",
        "Vincent Espitalier",
        "Vassilis Apostolou",
        "Louis-Félix Vigneux",
        "Alexia Salavrakos",
        "Jean Senellart"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.PL",
        "quant-ph"
      ],
      "published": "2026-02-11 18:00:01+00:00",
      "link": "https://arxiv.org/pdf/2602.11092v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11091v1",
      "title": "Can Large Language Models Make Everyone Happy?",
      "abstract": "Misalignment in Large Language Models (LLMs) refers to the failure to simultaneously satisfy safety, value, and cultural dimensions, leading to behaviors that diverge from human expectations in real-world settings where these dimensions must co-occur. Existing benchmarks, such as SAFETUNEBED (safety-centric), VALUEBENCH (value-centric), and WORLDVIEW-BENCH (culture-centric), primarily evaluate these dimensions in isolation and therefore provide limited insight into their interactions and trade-offs. More recent efforts, including MIB and INTERPRETABILITY BENCHMARK-based on mechanistic interpretability, offer valuable perspectives on model failures; however, they remain insufficient for systematically characterizing cross-dimensional trade-offs. To address these gaps, we introduce MisAlign-Profile, a unified benchmark for measuring misalignment trade-offs inspired by mechanistic profiling. First, we construct MISALIGNTRADE, an English misaligned-aligned dataset across 112 normative domains taxonomies, including 14 safety, 56 value, and 42 cultural domains. In addition to domain labels, each prompt is classified with one of three orthogonal semantic types-object, attribute, or relations misalignment-using Gemma-2-9B-it and expanded via Qwen3-30B-A3B-Instruct-2507 with SimHash-based fingerprinting to avoid deduplication. Each prompt is paired with misaligned and aligned responses through two-stage rejection sampling to ensure quality. Second, we benchmark general-purpose, fine-tuned, and open-weight LLMs on MISALIGNTRADE-revealing 12%-34% misalignment trade-offs across dimensions.",
      "authors": [
        "Usman Naseem",
        "Gautam Siddharth Kashyap",
        "Ebad Shabbir",
        "Sushant Kumar Ray",
        "Abdullah Mohammad",
        "Rafiq Ali"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 17:57:23+00:00",
      "link": "https://arxiv.org/pdf/2602.11091v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11090v1",
      "title": "Direct Learning of Calibration-Aware Uncertainty for Neural PDE Surrogates",
      "abstract": "Neural PDE surrogates are often deployed in data-limited or partially observed regimes where downstream decisions depend on calibrated uncertainty in addition to low prediction error. Existing approaches obtain uncertainty through ensemble replication, fixed stochastic noise such as dropout, or post hoc calibration. Cross-regularized uncertainty learns uncertainty parameters during training using gradients routed through a held-out regularization split. The predictor is optimized on the training split for fit, while low-dimensional uncertainty controls are optimized on the regularization split to reduce train-test mismatch, yielding regime-adaptive uncertainty without per-regime noise tuning. The framework can learn continuous noise levels at the output head, within hidden features, or within operator-specific components such as spectral modes. We instantiate the approach in Fourier Neural Operators and evaluate on APEBench sweeps over observed fraction and training-set size. Across these sweeps, the learned predictive distributions are better calibrated on held-out splits and the resulting uncertainty fields concentrate in high-error regions in one-step spatial diagnostics.",
      "authors": [
        "Carlos Stein Brito"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "stat.CO"
      ],
      "published": "2026-02-11 17:57:20+00:00",
      "link": "https://arxiv.org/pdf/2602.11090v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11089v1",
      "title": "DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning",
      "abstract": "In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the \\emph{data recipe}, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate \\emph{end-to-end data recipe generation} for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.",
      "authors": [
        "Yicheng Chen",
        "Zerun Ma",
        "Xinchen Xie",
        "Yining Li",
        "Kai Chen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-11 17:56:15+00:00",
      "link": "https://arxiv.org/pdf/2602.11089v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11087v1",
      "title": "General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies",
      "abstract": "Offline RL algorithms aim to improve upon the behavior policy that produces the collected data while constraining the learned policy to be within the support of the dataset. However, practical offline datasets often contain examples with little diversity or limited exploration of the environment, and from multiple behavior policies with diverse expertise levels. Limited exploration can impair the offline RL algorithm's ability to estimate \\textit{Q} or \\textit{V} values, while constraining towards diverse behavior policies can be overly conservative. Such datasets call for a balance between the RL objective and behavior policy constraints. We first identify the connection between $f$-divergence and optimization constraint on the Bellman residual through a more general Linear Programming form for RL and the convex conjugate. Following this, we introduce the general flexible function formulation for the $f$-divergence to incorporate an adaptive constraint on algorithms' learning objectives based on the offline training dataset. Results from experiments on the MuJoCo, Fetch, and AdroitHand environments show the correctness of the proposed LP form and the potential of the flexible $f$-divergence in improving performance for learning from a challenging dataset when applied to a compatible constrained optimization algorithm.",
      "authors": [
        "Jianxun Wang",
        "Grant C. Forbes",
        "Leonardo Villalobos-Arias",
        "David L. Roberts"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 17:53:49+00:00",
      "link": "https://arxiv.org/pdf/2602.11087v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11081v1",
      "title": "SteuerLLM: Local specialized large language model for German tax law analysis",
      "abstract": "Large language models (LLMs) demonstrate strong general reasoning and language understanding, yet their performance degrades in domains governed by strict formal rules, precise terminology, and legally binding structure. Tax law exemplifies these challenges, as correct answers require exact statutory citation, structured legal argumentation, and numerical accuracy under rigid grading schemes. We algorithmically generate SteuerEx, the first open benchmark derived from authentic German university tax law examinations. SteuerEx comprises 115 expert-validated examination questions spanning six core tax law domains and multiple academic levels, and employs a statement-level, partial-credit evaluation framework that closely mirrors real examination practice. We further present SteuerLLM, a domain-adapted LLM for German tax law trained on a large-scale synthetic dataset generated from authentic examination material using a controlled retrieval-augmented pipeline. SteuerLLM (28B parameters) consistently outperforms general-purpose instruction-tuned models of comparable size and, in several cases, substantially larger systems, demonstrating that domain-specific data and architectural adaptation are more decisive than parameter scale for performance on realistic legal reasoning tasks. All benchmark data, training datasets, model weights, and evaluation code are released openly to support reproducible research in domain-specific legal artificial intelligence. A web-based demo of SteuerLLM is available at https://steuerllm.i5.ai.fau.de.",
      "authors": [
        "Sebastian Wind",
        "Jeta Sopa",
        "Laurin Schmid",
        "Quirin Jackl",
        "Sebastian Kiefer",
        "Fei Wu",
        "Martin Mayr",
        "Harald Köstler",
        "Gerhard Wellein",
        "Andreas Maier",
        "Soroosh Tayebi Arasteh"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-11 17:46:01+00:00",
      "link": "https://arxiv.org/pdf/2602.11081v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11074v1",
      "title": "AI Sensing and Intervention in Higher Education: Student Perceptions of Learning Impacts, Affective Responses, and Ethical Priorities",
      "abstract": "AI technologies that sense student attention and emotions to enable more personalised teaching interventions are increasingly promoted, but raise pressing questions about student learning, well-being, and ethics. In particular, students' perspectives about AI sensing-intervention in learning are often overlooked. We conducted an online mixed-method experiment with Australian university students (N=132), presenting video scenarios varying by whether sensing was used (in-use vs. not-in-use), sensing modality (gaze-based attention detection vs. facial-based emotion detection), and intervention (by digital device vs. teacher). Participants also completed pairwise ranking tasks to prioritise six core ethical concerns. Findings revealed that students valued targeted intervention but responded negatively to AI monitoring, regardless of sensing methods. Students preferred system-generated hints over teacher-initiated assistance, citing learning agency and social embarrassment concerns. Students' ethical considerations prioritised autonomy and privacy, followed by transparency, accuracy, fairness, and learning beneficence. We advocate designing customisable, social-sensitive, non-intrusive systems that preserve student control, agency, and well-being.",
      "authors": [
        "Bingyi Han",
        "Ying Ma",
        "Simon Coghlan",
        "Dana McKay",
        "George Buchanan",
        "Wally Smith"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-11 17:43:33+00:00",
      "link": "https://arxiv.org/pdf/2602.11074v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11072v1",
      "title": "Simultaneous Speech-to-Speech Translation Without Aligned Data",
      "abstract": "Simultaneous speech translation requires translating source speech into a target language in real-time while handling non-monotonic word dependencies. Traditional approaches rely on supervised training with word-level aligned data, which is difficult to collect at scale and thus depends on synthetic alignments using language-specific heuristics that are suboptimal. We propose Hibiki-Zero, which eliminates the need for word-level alignments entirely. This fundamentally simplifies the training pipeline and enables seamless scaling to diverse languages with varying grammatical structures, removing the bottleneck of designing language-specific alignment heuristics. We first train on sentence-level aligned data to learn speech translation at high latency, then apply a novel reinforcement learning strategy using GRPO to optimize latency while preserving translation quality. Hibiki-Zero achieves state-of-the-art performance in translation accuracy, latency, voice transfer, and naturalness across five X-to-English tasks. Moreover, we demonstrate that our model can be adapted to support a new input language with less than 1000h of speech. We provide examples, model weights, inference code and we release a benchmark containing 45h of multilingual data for speech translation evaluation.",
      "authors": [
        "Tom Labiausse",
        "Romain Fabre",
        "Yannick Estève",
        "Alexandre Défossez",
        "Neil Zeghidour"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "published": "2026-02-11 17:41:01+00:00",
      "link": "https://arxiv.org/pdf/2602.11072v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11062v1",
      "title": "MoToRec: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation",
      "abstract": "Graph neural networks (GNNs) have revolutionized recommender systems by effectively modeling complex user-item interactions, yet data sparsity and the item cold-start problem significantly impair performance, particularly for new items with limited or no interaction history. While multimodal content offers a promising solution, existing methods result in suboptimal representations for new items due to noise and entanglement in sparse data. To address this, we transform multimodal recommendation into discrete semantic tokenization. We present Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation (MoToRec), a framework centered on a sparsely-regularized Residual Quantized Variational Autoencoder (RQ-VAE) that generates a compositional semantic code of discrete, interpretable tokens, promoting disentangled representations. MoToRec's architecture is enhanced by three synergistic components: (1) a sparsely-regularized RQ-VAE that promotes disentangled representations, (2) a novel adaptive rarity amplification that promotes prioritized learning for cold-start items, and (3) a hierarchical multi-source graph encoder for robust signal fusion with collaborative signals. Extensive experiments on three large-scale datasets demonstrate MoToRec's superiority over state-of-the-art methods in both overall and cold-start scenarios. Our work validates that discrete tokenization provides an effective and scalable alternative for mitigating the long-standing cold-start challenge.",
      "authors": [
        "Jialin Liu",
        "Zhaorui Zhang",
        "Ray C. C. Cheung"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "published": "2026-02-11 17:31:14+00:00",
      "link": "https://arxiv.org/pdf/2602.11062v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12302v1",
      "title": "Grandes Modelos de Linguagem Multimodais (MLLMs): Da Teoria à Prática",
      "abstract": "Multimodal Large Language Models (MLLMs) combine the natural language understanding and generation capabilities of LLMs with perception skills in modalities such as image and audio, representing a key advancement in contemporary AI. This chapter presents the main fundamentals of MLLMs and emblematic models. Practical techniques for preprocessing, prompt engineering, and building multimodal pipelines with LangChain and LangGraph are also explored. For further practical study, supplementary material is publicly available online: https://github.com/neemiasbsilva/MLLMs-Teoria-e-Pratica. Finally, the chapter discusses the challenges and highlights promising trends.",
      "authors": [
        "Neemias da Silva",
        "Júlio C. W. Scholz",
        "John Harrison",
        "Marina Borges",
        "Paulo Ávila",
        "Frances A Santos",
        "Myriam Delgado",
        "Rodrigo Minetto",
        "Thiago H Silva"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "published": "2026-02-11 17:30:22+00:00",
      "link": "https://arxiv.org/pdf/2602.12302v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11057v1",
      "title": "Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models",
      "abstract": "The multi-commodity flow (MCF) problem is a fundamental topic in network flow and combinatorial optimization, with broad applications in transportation, communication, and logistics, etc. Nowadays, the rapid expansion of allocation systems has posed challenges for existing optimization engines in balancing optimality and tractability. In this paper, we present Pram, the first ML-based method that leverages the reasoning power of multimodal language models (MLMs) for addressing the trade-off dilemma -- a great need of service providers. As part of our proposal, Pram (i) quickly computes high-quality allocations by dividing the original problem into local subproblems, which are then resolved by an MLM-powered \"agent\", and (ii) ensures global consistency by harmonizing these subproblems via a multi-agent reinforcement learning algorithm. Theoretically, we show that Pram, which learns to perform gradient descent in context, provably converges to the optimum within the family of MCF problems. Empirically, on real-world datasets and public topologies, Pram achieves performance comparable to, and in some cases even surpassing, linear programming solvers (very close to the optimal solution), and substantially lower runtimes (1 to 2 orders of magnitude faster). Moreover, Pram exhibits strong robustness (<10\\% performance degradation under link failures or flow bursts), demonstrating MLM's generalization ability to unforeseen events. Pram is objective-agnostic and seamlessly integrates with mainstream allocation systems, providing a practical and scalable solution for future networks.",
      "authors": [
        "Xinyu Yuan",
        "Yan Qiao",
        "Zonghui Wang",
        "Wenzhi Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 17:24:49+00:00",
      "link": "https://arxiv.org/pdf/2602.11057v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11052v1",
      "title": "GraphSeek: Next-Generation Graph Analytics with LLMs",
      "abstract": "Graphs are foundational across domains but remain hard to use without deep expertise. LLMs promise accessible natural language (NL) graph analytics, yet they fail to process industry-scale property graphs effectively and efficiently: such datasets are large, highly heterogeneous, structurally complex, and evolve dynamically. To address this, we devise a novel abstraction for complex multi-query analytics over such graphs. Its key idea is to replace brittle generation of graph queries directly from NL with planning over a Semantic Catalog that describes both the graph schema and the graph operations. Concretely, this induces a clean separation between a Semantic Plane for LLM planning and broader reasoning, and an Execution Plane for deterministic, database-grade query execution over the full dataset and tool implementations. This design yields substantial gains in both token efficiency and task effectiveness even with small-context LLMs. We use this abstraction as the basis of the first LLM-enhanced graph analytics framework called GraphSeek. GraphSeek achieves substantially higher success rates (e.g., 86% over enhanced LangChain) and points toward the next generation of affordable and accessible graph analytics that unify LLM reasoning with database-grade execution over large and complex property graphs.",
      "authors": [
        "Maciej Besta",
        "Łukasz Jarmocik",
        "Orest Hrycyna",
        "Shachar Klaiman",
        "Konrad Mączka",
        "Robert Gerstenberger",
        "Jürgen Müller",
        "Piotr Nyczyk",
        "Hubert Niewiadomski",
        "Torsten Hoefler"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.IR"
      ],
      "published": "2026-02-11 17:20:06+00:00",
      "link": "https://arxiv.org/pdf/2602.11052v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11047v3",
      "title": "Embedding Inversion via Conditional Masked Diffusion Language Models",
      "abstract": "We frame embedding inversion as conditional masked diffusion, recovering all tokens in parallel through iterative denoising rather than sequential autoregressive generation. A masked diffusion language model is conditioned on the target embedding via adaptive layer normalization, requiring only 8 forward passes with no access to the target encoder at inference time. On 32-token sequences across three embedding models, the method achieves token recovery through parallel denoising without requiring encoder access, iterative correction, or architecture-specific alignment. Source code and live demo are available at https://github.com/jina-ai/embedding-inversion-demo.",
      "authors": [
        "Han Xiao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 17:17:13+00:00",
      "link": "https://arxiv.org/pdf/2602.11047v3",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11238v1",
      "title": "SurveyLens: A Research Discipline-Aware Benchmark for Automatic Survey Generation",
      "abstract": "The exponential growth of scientific literature has driven the evolution of Automatic Survey Generation (ASG) from simple pipelines to multi-agent frameworks and commercial Deep Research agents. However, current ASG evaluation methods rely on generic metrics and are heavily biased toward Computer Science (CS), failing to assess whether ASG methods adhere to the distinct standards of various academic disciplines. Consequently, researchers, especially those outside CS, lack clear guidance on using ASG systems to yield high-quality surveys compliant with specific discipline standards. To bridge this gap, we introduce SurveyLens, the first discipline-aware benchmark evaluating ASG methods across diverse research disciplines. We construct SurveyLens-1k, a curated dataset of 1,000 high-quality human-written surveys spanning 10 disciplines. Subsequently, we propose a dual-lens evaluation framework: (1) Discipline-Aware Rubric Evaluation, which utilizes LLMs with human preference-aligned weights to assess adherence to domain-specific writing standards; and (2) Canonical Alignment Evaluation to rigorously measure content coverage and synthesis quality against human-written survey papers. We conduct extensive experiments by evaluating 11 state-of-the-art ASG methods on SurveyLens, including Vanilla LLMs, ASG systems, and Deep Research agents. Our analysis reveals the distinct strengths and weaknesses of each paradigm across fields, providing essential guidance for selecting tools tailored to specific disciplinary requirements.",
      "authors": [
        "Beichen Guo",
        "Zhiyuan Wen",
        "Jia Gu",
        "Senzhang Wang",
        "Haochen Shi",
        "Ruosong Yang",
        "Shuaiqi Liu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 17:16:43+00:00",
      "link": "https://arxiv.org/pdf/2602.11238v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11044v1",
      "title": "Language Model Inversion through End-to-End Differentiation",
      "abstract": "Despite emerging research on Language Models (LM), few approaches analyse the invertibility of LMs. That is, given a LM and a desirable target output sequence of tokens, determining what input prompts would yield the target output remains an open problem. We formulate this problem as a classical gradient-based optimisation. First, we propose a simple algorithm to achieve end-to-end differentiability of a given (frozen) LM and then find optimised prompts via gradient descent. Our central insight is to view LMs as functions operating on sequences of distributions over tokens (rather than the traditional view as functions on sequences of tokens). Our experiments and ablations demonstrate that our DLM-powered inversion can reliably and efficiently optimise prompts of lengths $10$ and $80$ for targets of length $20$, for several white-box LMs (out-of-the-box).",
      "authors": [
        "Kevin Yandoka Denamganaï",
        "Kartic Subr"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-11 17:14:41+00:00",
      "link": "https://arxiv.org/pdf/2602.11044v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11040v1",
      "title": "Learning Page Order in Shuffled WOO Releases",
      "abstract": "We investigate document page ordering on 5,461 shuffled WOO documents (Dutch freedom of information releases) using page embeddings. These documents are heterogeneous collections such as emails, legal texts, and spreadsheets compiled into single PDFs, where semantic ordering signals are unreliable. We compare five methods, including pointer networks, seq2seq transformers, and specialized pairwise ranking models. The best performing approach successfully reorders documents up to 15 pages, with Kendall's tau ranging from 0.95 for short documents (2-5 pages) to 0.72 for 15 page documents. We observe two unexpected failures: seq2seq transformers fail to generalize on long documents (Kendall's tau drops from 0.918 on 2-5 pages to 0.014 on 21-25 pages), and curriculum learning underperforms direct training by 39% on long documents. Ablation studies suggest learned positional encodings are one contributing factor to seq2seq failure, though the degradation persists across all encoding variants, indicating multiple interacting causes. Attention pattern analysis reveals that short and long documents require fundamentally different ordering strategies, explaining why curriculum learning fails. Model specialization achieves substantial improvements on longer documents (+0.21 tau).",
      "authors": [
        "Efe Kahraman",
        "Giulio Tosato"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-11 17:11:27+00:00",
      "link": "https://arxiv.org/pdf/2602.11040v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11022v1",
      "title": "Information Abstraction for Data Transmission Networks based on Large Language Models",
      "abstract": "Biological systems, particularly the human brain, achieve remarkable energy efficiency by abstracting information across multiple hierarchical levels. In contrast, modern artificial intelligence and communication systems often consume significant energy overheads in transmitting low-level data, with limited emphasis on abstraction. Despite its implicit importance, a formal and computational theory of information abstraction remains absent. In this work, we introduce the Degree of Information Abstraction (DIA), a general metric that quantifies how well a representation compresses input data while preserving task-relevant semantics. We derive a tractable information-theoretic formulation of DIA and propose a DIA-based information abstraction framework. As a case study, we apply DIA to a large language model (LLM)-guided video transmission task, where abstraction-aware encoding significantly reduces transmission volume by $99.75\\%$, while maintaining semantic fidelity. Our results suggest that DIA offers a principled tool for rebalancing energy and information in intelligent systems and opens new directions in neural network design, neuromorphic computing, semantic communication, and joint sensing-communication architectures.",
      "authors": [
        "Haoyuan Zhu",
        "Haonan Hu",
        "Jie Zhang"
      ],
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT"
      ],
      "published": "2026-02-11 16:48:31+00:00",
      "link": "https://arxiv.org/pdf/2602.11022v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11021v1",
      "title": "ContactGaussian-WM: Learning Physics-Grounded World Model from Videos",
      "abstract": "Developing world models that understand complex physical interactions is essential for advancing robotic planning and simulation.However, existing methods often struggle to accurately model the environment under conditions of data scarcity and complex contact-rich dynamic motion.To address these challenges, we propose ContactGaussian-WM, a differentiable physics-grounded rigid-body world model capable of learning intricate physical laws directly from sparse and contact-rich video sequences.Our framework consists of two core components: (1) a unified Gaussian representation for both visual appearance and collision geometry, and (2) an end-to-end differentiable learning framework that differentiates through a closed-form physics engine to infer physical properties from sparse visual observations.Extensive simulations and real-world evaluations demonstrate that ContactGaussian-WM outperforms state-of-the-art methods in learning complex scenarios, exhibiting robust generalization capabilities.Furthermore, we showcase the practical utility of our framework in downstream applications, including data synthesis and real-time MPC.",
      "authors": [
        "Meizhong Wang",
        "Wanxin Jin",
        "Kun Cao",
        "Lihua Xie",
        "Yiguang Hong"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-11 16:48:13+00:00",
      "link": "https://arxiv.org/pdf/2602.11021v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11015v1",
      "title": "CVPL: A Geometric Framework for Post-Hoc Linkage Risk Assessment in Protected Tabular Data",
      "abstract": "Formal privacy metrics provide compliance-oriented guarantees but often fail to quantify actual linkability in released datasets. We introduce CVPL (Cluster-Vector-Projection Linkage), a geometric framework for post-hoc assessment of linkage risk between original and protected tabular data. CVPL represents linkage analysis as an operator pipeline comprising blocking, vectorization, latent projection, and similarity evaluation, yielding continuous, scenario-dependent risk estimates rather than binary compliance verdicts. We formally define CVPL under an explicit threat model and introduce threshold-aware risk surfaces, R(lambda, tau), that capture the joint effects of protection strength and attacker strictness. We establish a progressive blocking strategy with monotonicity guarantees, enabling anytime risk estimation with valid lower bounds. We demonstrate that the classical Fellegi-Sunter linkage emerges as a special case of CVPL under restrictive assumptions, and that violations of these assumptions can lead to systematic over-linking bias. Empirical validation on 10,000 records across 19 protection configurations demonstrates that formal k-anonymity compliance may coexist with substantial empirical linkability, with a significant portion arising from non-quasi-identifier behavioral patterns. CVPL provides interpretable diagnostics identifying which features drive linkage feasibility, supporting privacy impact assessment, protection mechanism comparison, and utility-risk trade-off analysis.",
      "authors": [
        "Valery Khvatov",
        "Alexey Neyman"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published": "2026-02-11 16:39:07+00:00",
      "link": "https://arxiv.org/pdf/2602.11015v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11005v1",
      "title": "Interpretable Vision Transformers in Monocular Depth Estimation via SVDA",
      "abstract": "Monocular depth estimation is a central problem in computer vision with applications in robotics, AR, and autonomous driving, yet the self-attention mechanisms that drive modern Transformer architectures remain opaque. We introduce SVD-Inspired Attention (SVDA) into the Dense Prediction Transformer (DPT), providing the first spectrally structured formulation of attention for dense prediction tasks. SVDA decouples directional alignment from spectral modulation by embedding a learnable diagonal matrix into normalized query-key interactions, enabling attention maps that are intrinsically interpretable rather than post-hoc approximations. Experiments on KITTI and NYU-v2 show that SVDA preserves or slightly improves predictive accuracy while adding only minor computational overhead. More importantly, SVDA unlocks six spectral indicators that quantify entropy, rank, sparsity, alignment, selectivity, and robustness. These reveal consistent cross-dataset and depth-wise patterns in how attention organizes during training, insights that remain inaccessible in standard Transformers. By shifting the role of attention from opaque mechanism to quantifiable descriptor, SVDA redefines interpretability in monocular depth estimation and opens a principled avenue toward transparent dense prediction models.",
      "authors": [
        "Vasileios Arampatzakis",
        "George Pavlidis",
        "Nikolaos Mitianoudis",
        "Nikos Papamarkos"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-11 16:27:15+00:00",
      "link": "https://arxiv.org/pdf/2602.11005v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11004v1",
      "title": "Enhancing Predictability of Multi-Tenant DNN Inference for Autonomous Vehicles' Perception",
      "abstract": "Autonomous vehicles (AVs) rely on sensors and deep neural networks (DNNs) to perceive their surrounding environment and make maneuver decisions in real time. However, achieving real-time DNN inference in the AV's perception pipeline is challenging due to the large gap between the computation requirement and the AV's limited resources. Most, if not all, of existing studies focus on optimizing the DNN inference time to achieve faster perception by compressing the DNN model with pruning and quantization. In contrast, we present a Predictable Perception system with DNNs (PP-DNN) that reduce the amount of image data to be processed while maintaining the same level of accuracy for multi-tenant DNNs by dynamically selecting critical frames and regions of interest (ROIs). PP-DNN is based on our key insight that critical frames and ROIs for AVs vary with the AV's surrounding environment. However, it is challenging to identify and use critical frames and ROIs in multi-tenant DNNs for predictable inference. Given image-frame streams, PP-DNN leverages an ROI generator to identify critical frames and ROIs based on the similarities of consecutive frames and traffic scenarios. PP-DNN then leverages a FLOPs predictor to predict multiply-accumulate operations (MACs) from the dynamic critical frames and ROIs. The ROI scheduler coordinates the processing of critical frames and ROIs with multiple DNN models. Finally, we design a detection predictor for the perception of non-critical frames. We have implemented PP-DNN in an ROS-based AV pipeline and evaluated it with the BDD100K and the nuScenes dataset. PP-DNN is observed to significantly enhance perception predictability, increasing the number of fusion frames by up to 7.3x, reducing the fusion delay by >2.6x and fusion-delay variations by >2.3x, improving detection completeness by 75.4% and the cost-effectiveness by up to 98% over the baseline.",
      "authors": [
        "Liangkai Liu",
        "Kang G. Shin",
        "Jinkyu Lee",
        "Chengmo Yang",
        "Weisong Shi"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO",
        "eess.SY"
      ],
      "published": "2026-02-11 16:25:10+00:00",
      "link": "https://arxiv.org/pdf/2602.11004v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11000v1",
      "title": "Fine-Tuning GPT-5 for GPU Kernel Generation",
      "abstract": "Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code generation, they face significant challenges in GPU code generation because of the scarcity of high-quality labeled training data, compiler biases when generating synthetic solutions, and limited generalization across hardware generations. This precludes supervised fine-tuning (SFT) as a scalable methodology for improving current LLMs. In contrast, reinforcement learning (RL) offers a data-efficient and adaptive alternative but requires access to relevant tools, careful selection of training problems, and a robust evaluation environment. We present Makora's environment and tools for reinforcement learning finetuning of frontier models and report our results from fine-tuning GPT-5 for Triton code generation. In the single-attempt setting, our fine-tuned model improves kernel correctness from 43.7% to 77.0% (+33.3 percentage points) and increases the fraction of problems outperforming TorchInductor from 14.8% to 21.8% (+7 percentage points) compared to baseline GPT-5, while exceeding prior state-of-the-art models on KernelBench. When integrated into a full coding agent, it is able to solve up to 97.4% of problems in an expanded KernelBench suite, outperforming the PyTorch TorchInductor compiler on 72.9% of problems with a geometric mean speedup of 2.12x. Our work demonstrates that targeted post-training with reinforcement learning can unlock LLM capabilities in highly specialized technical domains where traditional supervised learning is limited by data availability, opening new pathways for AI-assisted accelerator programming.",
      "authors": [
        "Ali Tehrani",
        "Yahya Emara",
        "Essam Wissam",
        "Wojciech Paluch",
        "Waleed Atallah",
        "Łukasz Dudziak",
        "Mohamed S. Abdelfattah"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-11 16:22:54+00:00",
      "link": "https://arxiv.org/pdf/2602.11000v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10996v1",
      "title": "The emergence of numerical representations in communicating artificial agents",
      "abstract": "Human languages provide efficient systems for expressing numerosities, but whether the sheer pressure to communicate is enough for numerical representations to arise in artificial agents, and whether the emergent codes resemble human numerals at all, remains an open question. We study two neural network-based agents that must communicate numerosities in a referential game using either discrete tokens or continuous sketches, thus exploring both symbolic and iconic representations. Without any pre-defined numeric concepts, the agents achieve high in-distribution communication accuracy in both communication channels and converge on high-precision symbol-meaning mappings. However, the emergent code is non-compositional: the agents fail to derive systematic messages for unseen numerosities, typically reusing the symbol of the highest trained numerosity (discrete), or collapsing extrapolated values onto a single sketch (continuous). We conclude that the communication pressure alone suffices for precise transmission of learned numerosities, but additional pressures are needed to yield compositional codes and generalisation abilities.",
      "authors": [
        "Daniela Mihai",
        "Lucas Weber",
        "Francesca Franzon"
      ],
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-11 16:21:43+00:00",
      "link": "https://arxiv.org/pdf/2602.10996v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10995v1",
      "title": "A Human-Centric Framework for Data Attribution in Large Language Models",
      "abstract": "In the current Large Language Model (LLM) ecosystem, creators have little agency over how their data is used, and LLM users may find themselves unknowingly plagiarizing existing sources. Attribution of LLM-generated text to LLM input data could help with these challenges, but so far we have more questions than answers: what elements of LLM outputs require attribution, what goals should it serve, how should it be implemented?   We contribute a human-centric data attribution framework, which situates the attribution problem within the broader data economy. Specific use cases for attribution, such as creative writing assistance or fact-checking, can be specified via a set of parameters (including stakeholder objectives and implementation criteria). These criteria are up for negotiation by the relevant stakeholder groups: creators, LLM users, and their intermediaries (publishers, platforms, AI companies). The outcome of domain-specific negotiations can be implemented and tested for whether the stakeholder goals are achieved. The proposed approach provides a bridge between methodological NLP work on data attribution, governance work on policy interventions, and economic analysis of creator incentives for a sustainable equilibrium in the data economy.",
      "authors": [
        "Amelie Wührl",
        "Mattes Ruckdeschel",
        "Kyle Lo",
        "Anna Rogers"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY"
      ],
      "published": "2026-02-11 16:20:50+00:00",
      "link": "https://arxiv.org/pdf/2602.10995v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10994v1",
      "title": "Interpretable Vision Transformers in Image Classification via SVDA",
      "abstract": "Vision Transformers (ViTs) have achieved state-of-the-art performance in image classification, yet their attention mechanisms often remain opaque and exhibit dense, non-structured behaviors. In this work, we adapt our previously proposed SVD-Inspired Attention (SVDA) mechanism to the ViT architecture, introducing a geometrically grounded formulation that enhances interpretability, sparsity, and spectral structure. We apply the use of interpretability indicators -- originally proposed with SVDA -- to monitor attention dynamics during training and assess structural properties of the learned representations. Experimental evaluations on four widely used benchmarks -- CIFAR-10, FashionMNIST, CIFAR-100, and ImageNet-100 -- demonstrate that SVDA consistently yields more interpretable attention patterns without sacrificing classification accuracy. While the current framework offers descriptive insights rather than prescriptive guidance, our results establish SVDA as a comprehensive and informative tool for analyzing and developing structured attention models in computer vision. This work lays the foundation for future advances in explainable AI, spectral diagnostics, and attention-based model compression.",
      "authors": [
        "Vasileios Arampatzakis",
        "George Pavlidis",
        "Nikolaos Mitianoudis",
        "Nikos Papamarkos"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-11 16:20:32+00:00",
      "link": "https://arxiv.org/pdf/2602.10994v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10991v2",
      "title": "Implementation of Polynomial NP-Complete Algorithms Based on the NP Verifier Simulation Framework",
      "abstract": "While prior work established a verifier-based polynomial-time framework for NP, explicit deterministic machines for concrete NP-complete problems have remained elusive. In this paper, we construct fully specified deterministic Turing Machines (DTMs) for \\textsc{SAT} and \\textsc{Subset-Sum} within an improved NP verifier simulation framework. A key contribution of this work is the development of a functional implementation that bridges the gap between theoretical proofs and executable software. Our improved feasible-graph construction yields a genuine reduction in the asymptotic polynomial degree, while optimized edge-extension mechanisms significantly improve practical execution speed. We show that these machines generate valid witnesses, extending the framework to deterministic \\textsc{FNP} computation without increasing complexity. The complete Python implementation behaves in accordance with the predicted polynomial-time bounds, and the source code along with sample instances are available in a public online repository.",
      "authors": [
        "Changryeol Lee"
      ],
      "primary_category": "cs.CC",
      "categories": [
        "cs.CC",
        "cs.FL"
      ],
      "published": "2026-02-11 16:17:36+00:00",
      "link": "https://arxiv.org/pdf/2602.10991v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10971v1",
      "title": "A Jointly Efficient and Optimal Algorithm for Heteroskedastic Generalized Linear Bandits with Adversarial Corruptions",
      "abstract": "We consider the problem of heteroskedastic generalized linear bandits (GLBs) with adversarial corruptions, which subsumes various stochastic contextual bandit settings, including heteroskedastic linear bandits and logistic/Poisson bandits. We propose HCW-GLB-OMD, which consists of two components: an online mirror descent (OMD)-based estimator and Hessian-based confidence weights to achieve corruption robustness. This is computationally efficient in that it only requires ${O}(1)$ space and time complexity per iteration. Under the self-concordance assumption on the link function, we show a regret bound of $\\tilde{O}\\left( d \\sqrt{\\sum_t g(τ_t) \\dotμ_{t,\\star}} + d^2 g_{\\max} κ+ d κC \\right)$, where $\\dotμ_{t,\\star}$ is the slope of $μ$ around the optimal arm at time $t$, $g(τ_t)$'s are potentially exogenously time-varying dispersions (e.g., $g(τ_t) = σ_t^2$ for heteroskedastic linear bandits, $g(τ_t) = 1$ for Bernoulli and Poisson), $g_{\\max} = \\max_{t \\in [T]} g(τ_t)$ is the maximum dispersion, and $C \\geq 0$ is the total corruption budget of the adversary. We complement this with a lower bound of $\\tildeΩ(d \\sqrt{\\sum_t g(τ_t) \\dotμ_{t,\\star}} + d C)$, unifying previous problem-specific lower bounds. Thus, our algorithm achieves, up to a $κ$-factor in the corruption term, instance-wise minimax optimality simultaneously across various instances of heteroskedastic GLBs with adversarial corruptions.",
      "authors": [
        "Sanghwa Kim",
        "Junghyun Lee",
        "Se-Young Yun"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-11 16:01:06+00:00",
      "link": "https://arxiv.org/pdf/2602.10971v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10965v1",
      "title": "MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs",
      "abstract": "Knowledge editing (KE) enables precise modifications to factual content in large language models (LLMs). Existing KE methods are largely designed for dense architectures, limiting their applicability to the increasingly prevalent sparse Mixture-of-Experts (MoE) models that underpin modern scalable LLMs. Although MoEs offer strong efficiency and capacity scaling, naively adapting dense-model editors is both computationally costly and prone to routing distribution shifts that undermine stability and consistency. To address these challenges, we introduce MoEEdit, the first routing-stable framework for parameter-modifying knowledge editing in MoE LLMs. Our method reparameterizes expert updates via per-expert null-space projections that keep router inputs invariant and thereby suppress routing shifts. The resulting block-structured optimization is solved efficiently with a block coordinate descent (BCD) solver. Experiments show that MoEEdit attains state-of-the-art efficacy and generalization while preserving high specificity and routing stability, with superior compute and memory efficiency. These results establish a robust foundation for scalable, precise knowledge editing in sparse LLMs and underscore the importance of routing-stable interventions.",
      "authors": [
        "Yupu Gu",
        "Rongzhe Wei",
        "Andy Zhu",
        "Pan Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 15:56:30+00:00",
      "link": "https://arxiv.org/pdf/2602.10965v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10953v1",
      "title": "Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models",
      "abstract": "Diffusion Language Models (DLMs) generate text by iteratively denoising a masked sequence, repeatedly deciding which positions to commit at each step. Standard decoding follows a greedy rule: unmask the most confident positions, yet this local choice can lock the model into a suboptimal unmasking order, especially on reasoning-heavy prompts. We present SOAR, a training-free decoding algorithm that adapts its behavior to the model's uncertainty. When confidence is low, SOAR briefly widens the search over alternative unmasking decisions to avoid premature commitments; when confidence is high, it collapses the search and decodes many positions in parallel to reduce the number of denoising iterations. Across mathematical reasoning and code generation benchmarks (GSM8K, MBPP, HumanEval) on Dream-7B and LLaDA-8B, SOAR improves generation quality while maintaining competitive inference speed, offering a practical way to balance quality and efficiency in DLM decoding.",
      "authors": [
        "Mingyu Cao",
        "Alvaro Correia",
        "Christos Louizos",
        "Shiwei Liu",
        "Lu Yin"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-11 15:41:09+00:00",
      "link": "https://arxiv.org/pdf/2602.10953v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11229v1",
      "title": "Latent Generative Solvers for Generalizable Long-Term Physics Simulation",
      "abstract": "We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns probabilistic latent dynamics with a Transformer trained by flow matching. Our key mechanism is an uncertainty knob that perturbs latent inputs during training and inference, teaching the solver to correct off-manifold rollout drift and stabilizing autoregressive prediction. We further use flow forcing to update a system descriptor (context) from model-generated trajectories, aligning train/test conditioning and improving long-term stability. We pretrain on a curated corpus of $\\sim$2.5M trajectories at $128^2$ resolution spanning 12 PDE families. LGS matches strong deterministic neural-operator baselines on short horizons while substantially reducing rollout drift on long horizons. Learning in latent space plus efficient architectural choices yields up to \\textbf{70$\\times$} lower FLOPs than non-generative baselines, enabling scalable pretraining. We also show efficient adaptation to an out-of-distribution $256^2$ Kolmogorov flow dataset under limited finetuning budgets. Overall, LGS provides a practical route toward generalizable, uncertainty-aware neural PDE solvers that are more reliable for long-term forecasting and downstream scientific workflows.",
      "authors": [
        "Zituo Chen",
        "Haixu Wu",
        "Sili Deng"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-11 15:34:52+00:00",
      "link": "https://arxiv.org/pdf/2602.11229v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15897v1",
      "title": "Mitigating Gradient Inversion Risks in Language Models via Token Obfuscation",
      "abstract": "Training and fine-tuning large-scale language models largely benefit from collaborative learning, but the approach has been proven vulnerable to gradient inversion attacks (GIAs), which allow adversaries to reconstruct private training data from shared gradients. Existing defenses mainly employ gradient perturbation techniques, e.g., noise injection or gradient pruning, to disrupt GIAs' direct mapping from gradient space to token space. However, these methods often fall short due to the retention of semantics similarity across gradient, embedding, and token spaces. In this work, we propose a novel defense mechanism named GHOST (gradient shield with obfuscated tokens), a token-level obfuscation mechanism that neutralizes GIAs by decoupling the inherent connections across gradient, embedding, and token spaces. GHOST is built upon an important insight: due to the large scale of the token space, there exist semantically distinct yet embedding-proximate tokens that can serve as the shadow substitutes of the original tokens, which enables a semantic disconnection in the token space while preserving the connection in the embedding and gradient spaces. GHOST comprises a searching step, which identifies semantically distinct candidate tokens using a multi-criteria searching process, and a selection step, which selects optimal shadow tokens to ensure minimal disruption to features critical for training by preserving alignment with the internal outputs produced by original tokens. Evaluation across diverse model architectures (from BERT to Llama) and datasets demonstrates the remarkable effectiveness of GHOST in protecting privacy (as low as 1% in recovery rate) and preserving utility (up to 0.92 in classification F1 and 5.45 in perplexity), in both classification and generation tasks against state-of-the-art GIAs and adaptive attack scenarios.",
      "authors": [
        "Xinguo Feng",
        "Zhongkui Ma",
        "Zihan Wang",
        "Alsharif Abuadbba",
        "Guangdong Bai"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 14:49:31+00:00",
      "link": "https://arxiv.org/pdf/2602.15897v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10911v1",
      "title": "Tuning the burn-in phase in training recurrent neural networks improves their performance",
      "abstract": "Training recurrent neural networks (RNNs) with standard backpropagation through time (BPTT) can be challenging, especially in the presence of long input sequences. A practical alternative to reduce computational and memory overhead is to perform BPTT repeatedly over shorter segments of the training data set, corresponding to truncated BPTT. In this paper, we examine the training of RNNs when using such a truncated learning approach for time series tasks. Specifically, we establish theoretical bounds on the accuracy and performance loss when optimizing over subsequences instead of the full data sequence. This reveals that the burn-in phase of the RNN is an important tuning knob in its training, with significant impact on the performance guarantees. We validate our theoretical results through experiments on standard benchmarks from the fields of system identification and time series forecasting. In all experiments, we observe a strong influence of the burn-in phase on the training process, and proper tuning can lead to a reduction of the prediction error on the training and test data of more than 60% in some cases.",
      "authors": [
        "Julian D. Schiller",
        "Malte Heinrich",
        "Victor G. Lopez",
        "Matthias A. Müller"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "eess.SY",
        "math.OC"
      ],
      "published": "2026-02-11 14:48:07+00:00",
      "link": "https://arxiv.org/pdf/2602.10911v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10908v1",
      "title": "SoftMatcha 2: A Fast and Soft Pattern Matcher for Trillion-Scale Corpora",
      "abstract": "We present an ultra-fast and flexible search algorithm that enables search over trillion-scale natural language corpora in under 0.3 seconds while handling semantic variations (substitution, insertion, and deletion). Our approach employs string matching based on suffix arrays that scales well with corpus size. To mitigate the combinatorial explosion induced by the semantic relaxation of queries, our method is built on two key algorithmic ideas: fast exact lookup enabled by a disk-aware design, and dynamic corpus-aware pruning. We theoretically show that the proposed method suppresses exponential growth in the search space with respect to query length by leveraging statistical properties of natural language. In experiments on FineWeb-Edu (Lozhkov et al., 2024) (1.4T tokens), we show that our method achieves significantly lower search latency than existing methods: infini-gram (Liu et al., 2024), infini-gram mini (Xu et al., 2025), and SoftMatcha (Deguchi et al., 2025). As a practical application, we demonstrate that our method identifies benchmark contamination in training corpora, unidentified by existing approaches. We also provide an online demo of fast, soft search across corpora in seven languages.",
      "authors": [
        "Masataka Yoneda",
        "Yusuke Matsushita",
        "Go Kamoda",
        "Kohei Suenaga",
        "Takuya Akiba",
        "Masaki Waga",
        "Sho Yokoi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-11 14:40:15+00:00",
      "link": "https://arxiv.org/pdf/2602.10908v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10905v1",
      "title": "Natural Hypergradient Descent: Algorithm Design, Convergence Analysis, and Parallel Implementation",
      "abstract": "In this work, we propose Natural Hypergradient Descent (NHGD), a new method for solving bilevel optimization problems. To address the computational bottleneck in hypergradient estimation--namely, the need to compute or approximate Hessian inverse--we exploit the statistical structure of the inner optimization problem and use the empirical Fisher information matrix as an asymptotically consistent surrogate for the Hessian. This design enables a parallel optimize-and-approximate framework in which the Hessian-inverse approximation is updated synchronously with the stochastic inner optimization, reusing gradient information at negligible additional cost. Our main theoretical contribution establishes high-probability error bounds and sample complexity guarantees for NHGD that match those of state-of-the-art optimize-then-approximate methods, while significantly reducing computational time overhead. Empirical evaluations on representative bilevel learning tasks further demonstrate the practical advantages of NHGD, highlighting its scalability and effectiveness in large-scale machine learning settings.",
      "authors": [
        "Deyi Kong",
        "Zaiwei Chen",
        "Shuzhong Zhang",
        "Shancong Mou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "published": "2026-02-11 14:31:33+00:00",
      "link": "https://arxiv.org/pdf/2602.10905v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10891v1",
      "title": "Interactive LLM-assisted Curriculum Learning for Multi-Task Evolutionary Policy Search",
      "abstract": "Multi-task policy search is a challenging problem because policies are required to generalize beyond training cases. Curriculum learning has proven to be effective in this setting, as it introduces complexity progressively. However, designing effective curricula is labor-intensive and requires extensive domain expertise. LLM-based curriculum generation has only recently emerged as a potential solution, but was limited to operate in static, offline modes without leveraging real-time feedback from the optimizer. Here we propose an interactive LLM-assisted framework for online curriculum generation, where the LLM adaptively designs training cases based on real-time feedback from the evolutionary optimization process. We investigate how different feedback modalities, ranging from numeric metrics alone to combinations with plots and behavior visualizations, influence the LLM ability to generate meaningful curricula. Through a 2D robot navigation case study, tackled with genetic programming as optimizer, we evaluate our approach against static LLM-generated curricula and expert-designed baselines. We show that interactive curriculum generation outperforms static approaches, with multimodal feedback incorporating both progression plots and behavior visualizations yielding performance competitive with expert-designed curricula. This work contributes to understanding how LLMs can serve as interactive curriculum designers for embodied AI systems, with potential extensions to broader evolutionary robotics applications.",
      "authors": [
        "Berfin Sakallioglu",
        "Giorgia Nadizar",
        "Eric Medvet"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "published": "2026-02-11 14:21:52+00:00",
      "link": "https://arxiv.org/pdf/2602.10891v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10886v1",
      "title": "The CLEF-2026 FinMMEval Lab: Multilingual and Multimodal Evaluation of Financial AI Systems",
      "abstract": "We present the setup and the tasks of the FinMMEval Lab at CLEF 2026, which introduces the first multilingual and multimodal evaluation framework for financial Large Language Models (LLMs). While recent advances in financial natural language processing have enabled automated analysis of market reports, regulatory documents, and investor communications, existing benchmarks remain largely monolingual, text-only, and limited to narrow subtasks. FinMMEval 2026 addresses this gap by offering three interconnected tasks that span financial understanding, reasoning, and decision-making: Financial Exam Question Answering, Multilingual Financial Question Answering (PolyFiQA), and Financial Decision Making. Together, these tasks provide a comprehensive evaluation suite that measures models' ability to reason, generalize, and act across diverse languages and modalities. The lab aims to promote the development of robust, transparent, and globally inclusive financial AI systems, with datasets and evaluation resources publicly released to support reproducible research.",
      "authors": [
        "Zhuohan Xie",
        "Rania Elbadry",
        "Fan Zhang",
        "Georgi Georgiev",
        "Xueqing Peng",
        "Lingfei Qian",
        "Jimin Huang",
        "Dimitar Dimitrov",
        "Vanshikaa Jani",
        "Yuyang Dai",
        "Jiahui Geng",
        "Yuxia Wang",
        "Ivan Koychev",
        "Veselin Stoyanov",
        "Preslav Nakov"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ],
      "published": "2026-02-11 14:14:06+00:00",
      "link": "https://arxiv.org/pdf/2602.10886v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10885v1",
      "title": "Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics",
      "abstract": "Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \\textbf{RLCER} (\\textbf{R}einforcement \\textbf{L}earning with \\textbf{C}oT Supervision via Self-\\textbf{E}volving \\textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.",
      "authors": [
        "Leheng Sheng",
        "Wenchang Ma",
        "Ruixin Hong",
        "Xiang Wang",
        "An Zhang",
        "Tat-Seng Chua"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-11 14:13:46+00:00",
      "link": "https://arxiv.org/pdf/2602.10885v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10881v1",
      "title": "Diagnosing Structural Failures in LLM-Based Evidence Extraction for Meta-Analysis",
      "abstract": "Systematic reviews and meta-analyses rely on converting narrative articles into structured, numerically grounded study records. Despite rapid advances in large language models (LLMs), it remains unclear whether they can meet the structural requirements of this process, which hinge on preserving roles, methods, and effect-size attribution across documents rather than on recognizing isolated entities. We propose a structural, diagnostic framework that evaluates LLM-based evidence extraction as a progression of schema-constrained queries with increasing relational and numerical complexity, enabling precise identification of failure points beyond atom-level extraction. Using a manually curated corpus spanning five scientific domains, together with a unified query suite and evaluation protocol, we evaluate two state-of-the-art LLMs under both per-document and long-context, multi-document input regimes. Across domains and models, performance remains moderate for single-property queries but degrades sharply once tasks require stable binding between variables, roles, statistical methods, and effect sizes. Full meta-analytic association tuples are extracted with near-zero reliability, and long-context inputs further exacerbate these failures. Downstream aggregation amplifies even minor upstream errors, rendering corpus-level statistics unreliable. Our analysis shows that these limitations stem not from entity recognition errors, but from systematic structural breakdowns, including role reversals, cross-analysis binding drift, instance compression in dense result sections, and numeric misattribution, indicating that current LLMs lack the structural fidelity, relational binding, and numerical grounding required for automated meta-analysis. The code and data are publicly available at GitHub (https://github.com/zhiyintan/LLM-Meta-Analysis).",
      "authors": [
        "Zhiyin Tan",
        "Jennifer D'Souza"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-11 14:09:43+00:00",
      "link": "https://arxiv.org/pdf/2602.10881v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10880v1",
      "title": "Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation",
      "abstract": "Vision-Language Models (VLMs) have shown promise in generating plotting code from chart images, yet achieving structural fidelity remains challenging. Existing approaches largely rely on supervised fine-tuning, encouraging surface-level token imitation rather than faithful modeling of underlying chart structure, which often leads to hallucinated or semantically inconsistent outputs. We propose Chart Specification, a structured intermediate representation that shifts training from text imitation to semantically grounded supervision. Chart Specification filters syntactic noise to construct a structurally balanced training set and supports a Spec-Align Reward that provides fine-grained, verifiable feedback on structural correctness, enabling reinforcement learning to enforce consistent plotting logic. Experiments on three public benchmarks show that our method consistently outperforms prior approaches. With only 3K training samples, we achieve strong data efficiency, surpassing leading baselines by up to 61.7% on complex benchmarks, and scaling to 4K samples establishes new state-of-the-art results across all evaluated metrics. Overall, our results demonstrate that precise structural supervision offers an efficient pathway to high-fidelity chart-to-code generation. Code and dataset are available at: https://github.com/Mighten/chart-specification-paper",
      "authors": [
        "Minggui He",
        "Mingchen Dai",
        "Jian Zhang",
        "Yilun Liu",
        "Shimin Tao",
        "Pufan Zeng",
        "Osamu Yoshie",
        "Yuya Ieiri"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-11 14:08:06+00:00",
      "link": "https://arxiv.org/pdf/2602.10880v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10878v1",
      "title": "Simple generators of rational function fields",
      "abstract": "Consider a subfield of the field of rational functions in several indeterminates. We present an algorithm that, given a set of generators of such a subfield, finds a simple generating set. We provide an implementation of the algorithm and show that it improves upon the state of the art both in efficiency and the quality of the results. Furthermore, we demonstrate the utility of simplified generators through several case studies from different application domains, such as structural parameter identifiability. The main algorithmic novelties include performing only partial Gröbner basis computation via sparse interpolation and efficient search for polynomials of a fixed degree in a subfield of the rational function field.",
      "authors": [
        "Alexander Demin",
        "Gleb Pogudin"
      ],
      "primary_category": "cs.SC",
      "categories": [
        "cs.SC",
        "cs.MS",
        "eess.SY",
        "math.AC",
        "math.DS"
      ],
      "published": "2026-02-11 14:07:00+00:00",
      "link": "https://arxiv.org/pdf/2602.10878v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10875v1",
      "title": "Stride-Net: Fairness-Aware Disentangled Representation Learning for Chest X-Ray Diagnosis",
      "abstract": "Deep neural networks for chest X-ray classification achieve strong average performance, yet often underperform for specific demographic subgroups, raising critical concerns about clinical safety and equity. Existing debiasing methods frequently yield inconsistent improvements across datasets or attain fairness by degrading overall diagnostic utility, treating fairness as a post hoc constraint rather than a property of the learned representation. In this work, we propose Stride-Net (Sensitive Attribute Resilient Learning via Disentanglement and Learnable Masking with Embedding Alignment), a fairness-aware framework that learns disease-discriminative yet demographically invariant representations for chest X-ray analysis. Stride-Net operates at the patch level, using a learnable stride-based mask to select label-aligned image regions while suppressing sensitive attribute information through adversarial confusion loss. To anchor representations in clinical semantics and discourage shortcut learning, we further enforce semantic alignment between image features and BioBERT-based disease label embeddings via Group Optimal Transport. We evaluate Stride-Net on the MIMIC-CXR and CheXpert benchmarks across race and intersectional race-gender subgroups. Across architectures including ResNet and Vision Transformers, Stride-Net consistently improves fairness metrics while matching or exceeding baseline accuracy, achieving a more favorable accuracy-fairness trade-off than prior debiasing approaches. Our code is available at https://github.com/Daraksh/Fairness_StrideNet.",
      "authors": [
        "Darakshan Rashid",
        "Raza Imam",
        "Dwarikanath Mahapatra",
        "Brejesh Lall"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-11 14:04:52+00:00",
      "link": "https://arxiv.org/pdf/2602.10875v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10870v1",
      "title": "FedPS: Federated data Preprocessing via aggregated Statistics",
      "abstract": "Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for model performance but is largely overlooked in FL research. In practical FL systems, privacy constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, a unified framework for federated data preprocessing based on aggregated statistics. FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling, encoding, discretization, and missing-value imputation, and extend preprocessing-related models such as k-Means, k-Nearest Neighbors, and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments.",
      "authors": [
        "Xuefeng Xu",
        "Graham Cormode"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 13:58:55+00:00",
      "link": "https://arxiv.org/pdf/2602.10870v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10867v1",
      "title": "Deep Learning of Compositional Targets with Hierarchical Spectral Methods",
      "abstract": "Why depth yields a genuine computational advantage over shallow methods remains a central open question in learning theory. We study this question in a controlled high-dimensional Gaussian setting, focusing on compositional target functions. We analyze their learnability using an explicit three-layer fitting model trained via layer-wise spectral estimators. Although the target is globally a high-degree polynomial, its compositional structure allows learning to proceed in stages: an intermediate representation reveals structure that is inaccessible at the input level. This reduces learning to simpler spectral estimation problems, well studied in the context of multi-index models, whereas any shallow estimator must resolve all components simultaneously. Our analysis relies on Gaussian universality, leading to sharp separations in sample complexity between two and three-layer learning strategies.",
      "authors": [
        "Hugo Tabanelli",
        "Yatin Dandi",
        "Luca Pesce",
        "Florent Krzakala"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-11 13:54:20+00:00",
      "link": "https://arxiv.org/pdf/2602.10867v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10854v1",
      "title": "Automated Model Design using Gated Neuron Selection in Telecom",
      "abstract": "The telecommunications industry is experiencing rapid growth in adopting deep learning for critical tasks such as traffic prediction, signal strength prediction, and quality of service optimisation. However, designing neural network architectures for these applications remains challenging and time-consuming, particularly when targeting compact models suitable for resource-constrained network environments. Therefore, there is a need for automating the model design process to create high-performing models efficiently. This paper introduces TabGNS (Tabular Gated Neuron Selection), a novel gradient-based Neural Architecture Search (NAS) method specifically tailored for tabular data in telecommunications networks. We evaluate TabGNS across multiple telecommunications and generic tabular datasets, demonstrating improvements in prediction performance while reducing the architecture size by 51-82% and reducing the search time by up to 36x compared to state-of-the-art tabular NAS methods. Integrating TabGNS into the model lifecycle management enables automated design of neural networks throughout the lifecycle, accelerating deployment of ML solutions in telecommunications networks.",
      "authors": [
        "Adam Orucu",
        "Marcus Medhage",
        "Farnaz Moradi",
        "Andreas Johnsson",
        "Sarunas Girdzijauskas"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 13:40:48+00:00",
      "link": "https://arxiv.org/pdf/2602.10854v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10847v1",
      "title": "Enhancing Multivariate Time Series Forecasting with Global Temporal Retrieval",
      "abstract": "Multivariate time series forecasting (MTSF) plays a vital role in numerous real-world applications, yet existing models remain constrained by their reliance on a limited historical context. This limitation prevents them from effectively capturing global periodic patterns that often span cycles significantly longer than the input horizon - despite such patterns carrying strong predictive signals. Naive solutions, such as extending the historical window, lead to severe drawbacks, including overfitting, prohibitive computational costs, and redundant information processing. To address these challenges, we introduce the Global Temporal Retriever (GTR), a lightweight and plug-and-play module designed to extend any forecasting model's temporal awareness beyond the immediate historical context. GTR maintains an adaptive global temporal embedding of the entire cycle and dynamically retrieves and aligns relevant global segments with the input sequence. By jointly modeling local and global dependencies through a 2D convolution and residual fusion, GTR effectively bridges short-term observations with long-term periodicity without altering the host model architecture. Extensive experiments on six real-world datasets demonstrate that GTR consistently delivers state-of-the-art performance across both short-term and long-term forecasting scenarios, while incurring minimal parameter and computational overhead. These results highlight GTR as an efficient and general solution for enhancing global periodicity modeling in MTSF tasks. Code is available at this repository: https://github.com/macovaseas/GTR.",
      "authors": [
        "Fanpu Cao",
        "Lu Dai",
        "Jindong Han",
        "Hui Xiong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 13:33:33+00:00",
      "link": "https://arxiv.org/pdf/2602.10847v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10845v1",
      "title": "SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy",
      "abstract": "Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical \"structural resolution mismatch,\" failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data.",
      "authors": [
        "Xuecheng Zou",
        "Yu Tang",
        "Bingbing Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-11 13:31:58+00:00",
      "link": "https://arxiv.org/pdf/2602.10845v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10833v1",
      "title": "Training-Induced Bias Toward LLM-Generated Content in Dense Retrieval",
      "abstract": "Dense retrieval is a promising approach for acquiring relevant context or world knowledge in open-domain natural language processing tasks and is now widely used in information retrieval applications. However, recent reports claim a broad preference for text generated by large language models (LLMs). This bias is called \"source bias\", and it has been hypothesized that lower perplexity contributes to this effect. In this study, we revisit this claim by conducting a controlled evaluation to trace the emergence of such preferences across training stages and data sources. Using parallel human- and LLM-generated counterparts of the SciFact and Natural Questions (NQ320K) datasets, we compare unsupervised checkpoints with models fine-tuned using in-domain human text, in-domain LLM-generated text, and MS MARCO. Our results show the following: 1) Unsupervised retrievers do not exhibit a uniform pro-LLM preference. The direction and magnitude depend on the dataset. 2) Across the settings tested, supervised fine-tuning on MS MARCO consistently shifts the rankings toward LLM-generated text. 3) In-domain fine-tuning produces dataset-specific and inconsistent shifts in preference. 4) Fine-tuning on LLM-generated corpora induces a pronounced pro-LLM bias. Finally, a retriever-centric perplexity probe involving the reattachment of a language modeling head to the fine-tuned dense retriever encoder indicates agreement with relevance near chance, thereby weakening the explanatory power of perplexity. Our study demonstrates that source bias is a training-induced phenomenon rather than an inherent property of dense retrievers.",
      "authors": [
        "William Xion",
        "Wolfgang Nejdl"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-11 13:20:25+00:00",
      "link": "https://arxiv.org/pdf/2602.10833v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10832v1",
      "title": "I can tell whether you are a Native Hawlêri Speaker! How ANN, CNN, and RNN perform in NLI-Native Language Identification",
      "abstract": "Native Language Identification (NLI) is a task in Natural Language Processing (NLP) that typically determines the native language of an author through their writing or a speaker through their speaking. It has various applications in different areas, such as forensic linguistics and general linguistics studies. Although considerable research has been conducted on NLI regarding two different languages, such as English and German, the literature indicates a significant gap regarding NLI for dialects and subdialects. The gap becomes wider in less-resourced languages such as Kurdish. This research focuses on NLI within the context of a subdialect of Sorani (Central) Kurdish. It aims to investigate the NLI for Hewlêri, a subdialect spoken in Hewlêr (Erbil), the Capital of the Kurdistan Region of Iraq. We collected about 24 hours of speech by recording interviews with 40 native or non-native Hewlêri speakers, 17 female and 23 male. We created three Neural Network-based models: Artificial Neural Network (ANN), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), which were evaluated through 66 experiments, covering various time-frames from 1 to 60 seconds, undersampling, oversampling, and cross-validation. The RNN model showed the highest accuracy of 95.92% for 5-second audio segmentation, using an 80:10:10 data splitting scheme. The created dataset is the first speech dataset for NLI on the Hewlêri subdialect in the Sorani Kurdish dialect, which can be of benefit to various research areas.",
      "authors": [
        "Hardi Garari",
        "Hossein Hassani"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 13:17:56+00:00",
      "link": "https://arxiv.org/pdf/2602.10832v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10816v1",
      "title": "Beyond Confidence: The Rhythms of Reasoning in Generative Models",
      "abstract": "Large Language Models (LLMs) exhibit impressive capabilities yet suffer from sensitivity to slight input context variations, hampering reliability. Conventional metrics like accuracy and perplexity fail to assess local prediction robustness, as normalized output probabilities can obscure the underlying resilience of an LLM's internal state to perturbations. We introduce the Token Constraint Bound ($δ_{\\mathrm{TCB}}$), a novel metric that quantifies the maximum internal state perturbation an LLM can withstand before its dominant next-token prediction significantly changes. Intrinsically linked to output embedding space geometry, $δ_{\\mathrm{TCB}}$ provides insights into the stability of the model's internal predictive commitment. Our experiments show $δ_{\\mathrm{TCB}}$ correlates with effective prompt engineering and uncovers critical prediction instabilities missed by perplexity during in-context learning and text generation. $δ_{\\mathrm{TCB}}$ offers a principled, complementary approach to analyze and potentially improve the contextual stability of LLM predictions.",
      "authors": [
        "Deyuan Liu",
        "Zecheng Wang",
        "Zhanyue Qin",
        "Zhiying Tu",
        "Dianhui Chu",
        "Dianbo Sui"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-11 12:58:23+00:00",
      "link": "https://arxiv.org/pdf/2602.10816v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15895v1",
      "title": "Understand Then Memory: A Cognitive Gist-Driven RAG Framework with Global Semantic Diffusion",
      "abstract": "Retrieval-Augmented Generation (RAG) effectively mitigates hallucinations in LLMs by incorporating external knowledge. However, the inherent discrete representation of text in existing frameworks often results in a loss of semantic integrity, leading to retrieval deviations. Inspired by the human episodic memory mechanism, we propose CogitoRAG, a RAG framework that simulates human cognitive memory processes. The core of this framework lies in the extraction and evolution of the Semantic Gist. During the offline indexing stage, CogitoRAG first deduces unstructured corpora into gist memory corpora, which are then transformed into a multi-dimensional knowledge graph integrating entities, relational facts, and memory nodes. In the online retrieval stage, the framework handles complex queries via Query Decomposition Module that breaks them into comprehensive sub-queries, mimicking the cognitive decomposition humans employ for complex information. Subsequently, Entity Diffusion Module performs associative retrieval across the graph, guided by structural relevance and an entity-frequency reward mechanism. Furthermore, we propose the CogniRank algorithm, which precisely reranks candidate passages by fusing diffusion-derived scores with semantic similarity. The final evidence is delivered to the generator in a passage-memory pairing format, providing high-density information support. Experimental results across five mainstream QA benchmarks and multi-task generation on GraphBench demonstrate that CogitoRAG significantly outperforms state-of-the-art RAG methods, showcasing superior capabilities in complex knowledge integration and reasoning.",
      "authors": [
        "Pengcheng Zhou",
        "Haochen Li",
        "Zhiqiang Nie",
        "JiaLe Chen",
        "Qing Gong",
        "Weizhen Zhang",
        "Chun Yu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-11 12:58:08+00:00",
      "link": "https://arxiv.org/pdf/2602.15895v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10802v1",
      "title": "Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act",
      "abstract": "Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance.",
      "authors": [
        "Da-Lun Chen",
        "Prasasthy Balasubramanian",
        "Lauri Lovén",
        "Susanna Pirttikangas",
        "Jaakko Sauvola",
        "Panagiotis Kostakos"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-11 12:44:03+00:00",
      "link": "https://arxiv.org/pdf/2602.10802v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10796v2",
      "title": "PRISM: Parallel Residual Iterative Sequence Model",
      "abstract": "Generative sequence modeling faces a fundamental tension between the expressivity of Transformers and the efficiency of linear sequence models. Existing efficient architectures are theoretically bounded by shallow, single-step linear updates, while powerful iterative methods like Test-Time Training (TTT) break hardware parallelism due to state-dependent gradients. We propose PRISM (Parallel Residual Iterative Sequence Model) to resolve this tension. PRISM introduces a solver-inspired inductive bias that captures key structural properties of multi-step refinement in a parallelizable form. We employ a Write-Forget Decoupling strategy that isolates non-linearity within the injection operator. To bypass the serial dependency of explicit solvers, PRISM utilizes a two-stage proxy architecture: a short-convolution anchors the initial residual using local history energy, while a learned predictor estimates the refinement updates directly from the input. This design distills structural patterns associated with iterative correction into a parallelizable feedforward operator. Theoretically, we prove that this formulation achieves Rank-$L$ accumulation, structurally expanding the update manifold beyond the single-step Rank-$1$ bottleneck. Empirically, it achieves comparable performance to explicit optimization methods while achieving 174x higher throughput.",
      "authors": [
        "Jie Jiang",
        "Ke Cheng",
        "Xin Xu",
        "Mengyang Pang",
        "Tianhao Lu",
        "Jiaheng Li",
        "Yue Liu",
        "Yuan Wang",
        "Jun Zhang",
        "Huan Yu",
        "Zhouchen Lin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 12:39:41+00:00",
      "link": "https://arxiv.org/pdf/2602.10796v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10794v1",
      "title": "Transport, Don't Generate: Deterministic Geometric Flows for Combinatorial Optimization",
      "abstract": "Recent advances in Neural Combinatorial Optimization (NCO) have been dominated by diffusion models that treat the Euclidean Traveling Salesman Problem (TSP) as a stochastic $N \\times N$ heatmap generation task. In this paper, we propose CycFlow, a framework that replaces iterative edge denoising with deterministic point transport. CycFlow learns an instance-conditioned vector field that continuously transports input 2D coordinates to a canonical circular arrangement, where the optimal tour is recovered from this $2N$ dimensional representation via angular sorting. By leveraging data-dependent flow matching, we bypass the quadratic bottleneck of edge scoring in favor of linear coordinate dynamics. This paradigm shift accelerates solving speed by up to three orders of magnitude compared to state-of-the-art diffusion baselines, while maintaining competitive optimality gaps.",
      "authors": [
        "Benjy Friedmann",
        "Nadav Dym"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 12:38:12+00:00",
      "link": "https://arxiv.org/pdf/2602.10794v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11221v1",
      "title": "The Automatic Verification of Image-Text Claims (AVerImaTeC) Shared Task",
      "abstract": "The Automatic Verification of Image-Text Claims (AVerImaTeC) shared task aims to advance system development for retrieving evidence and verifying real-world image-text claims. Participants were allowed to either employ external knowledge sources, such as web search engines, or leverage the curated knowledge store provided by the organizers. System performance was evaluated using the AVerImaTeC score, defined as a conditional verdict accuracy in which a verdict is considered correct only when the associated evidence score exceeds a predefined threshold. The shared task attracted 14 submissions during the development phase and 6 submissions during the testing phase. All participating systems in the testing phase outperformed the baseline provided. The winning team, HUMANE, achieved an AVerImaTeC score of 0.5455. This paper provides a detailed description of the shared task, presents the complete evaluation results, and discusses key insights and lessons learned.",
      "authors": [
        "Rui Cao",
        "Zhenyun Deng",
        "Yulong Chen",
        "Michael Schlichtkrull",
        "Andreas Vlachos"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 12:32:15+00:00",
      "link": "https://arxiv.org/pdf/2602.11221v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10787v1",
      "title": "VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection",
      "abstract": "Software vulnerability detection (SVD) is a critical challenge in modern systems. Large language models (LLMs) offer natural-language explanations alongside predictions, but most work focuses on binary evaluation, and explanations often lack semantic consistency with Common Weakness Enumeration (CWE) categories. We propose VulReaD, a knowledge-graph-guided approach for vulnerability reasoning and detection that moves beyond binary classification toward CWE-level reasoning. VulReaD leverages a security knowledge graph (KG) as a semantic backbone and uses a strong teacher LLM to generate CWE-consistent contrastive reasoning supervision, enabling student model training without manual annotations. Students are fine-tuned with Odds Ratio Preference Optimization (ORPO) to encourage taxonomy-aligned reasoning while suppressing unsupported explanations. Across three real-world datasets, VulReaD improves binary F1 by 8-10% and multi-class classification by 30% Macro-F1 and 18% Micro-F1 compared to state-of-the-art baselines. Results show that LLMs outperform deep learning baselines in binary detection and that KG-guided reasoning enhances CWE coverage and interpretability.",
      "authors": [
        "Samal Mukhtar",
        "Yinghua Yao",
        "Zhu Sun",
        "Mustafa Mustafa",
        "Yew Soon Ong",
        "Youcheng Sun"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR",
        "cs.IR"
      ],
      "published": "2026-02-11 12:24:51+00:00",
      "link": "https://arxiv.org/pdf/2602.10787v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10780v1",
      "title": "Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks",
      "abstract": "Machine learning models are increasingly present in our everyday lives; as a result, they become targets of adversarial attackers seeking to manipulate the systems we interact with. A well-known vulnerability is a backdoor introduced into a neural network by poisoned training data or a malicious training process. Backdoors can be used to induce unwanted behavior by including a certain trigger in the input. Existing mitigations filter training data, modify the model, or perform expensive input modifications on samples. If a vulnerable model has already been deployed, however, those strategies are either ineffective or inefficient. To address this gap, we propose our inference-time backdoor mitigation approach called FIRE (Feature-space Inference-time REpair). We hypothesize that a trigger induces structured and repeatable changes in the model's internal representation. We view the trigger as directions in the latent spaces between layers that can be applied in reverse to correct the inference mechanism. Therefore, we turn the backdoored model against itself by manipulating its latent representations and moving a poisoned sample's features along the backdoor directions to neutralize the trigger. Our evaluation shows that FIRE has low computational overhead and outperforms current runtime mitigations on image benchmarks across various attacks, datasets, and network architectures.",
      "authors": [
        "Enrico Ahlers",
        "Daniel Passon",
        "Yannic Noller",
        "Lars Grunske"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "published": "2026-02-11 12:13:25+00:00",
      "link": "https://arxiv.org/pdf/2602.10780v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10778v1",
      "title": "GoodVibe: Security-by-Vibe for LLM-Based Code Generation",
      "abstract": "Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.   We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.",
      "authors": [
        "Maximilian Thang",
        "Lichao Wu",
        "Sasha Behrouzi",
        "Mohamadreza Rostami",
        "Jona te Lintelo",
        "Stjepan Picek",
        "Ahmad-Reza Sadeghi"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR"
      ],
      "published": "2026-02-11 12:10:14+00:00",
      "link": "https://arxiv.org/pdf/2602.10778v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10770v1",
      "title": "LOREN: Low Rank-Based Code-Rate Adaptation in Neural Receivers",
      "abstract": "Neural network based receivers have recently demonstrated superior system-level performance compared to traditional receivers. However, their practicality is limited by high memory and power requirements, as separate weight sets must be stored for each code rate. To address this challenge, we propose LOREN, a Low Rank-Based Code-Rate Adaptation Neural Receiver that achieves adaptability with minimal overhead. LOREN integrates lightweight low rank adaptation adapters (LOREN adapters) into convolutional layers, freezing a shared base network while training only small adapters per code rate. An end-to-end training framework over 3GPP CDL channels ensures robustness across realistic wireless environments. LOREN achieves comparable or superior performance relative to fully retrained base neural receivers. The hardware implementation of LOREN in 22nm technology shows more than 65% savings in silicon area and up to 15% power reduction when supporting three code rates.",
      "authors": [
        "Bram Van Bolderik",
        "Vlado Menkovski",
        "Sonia Heemstra de Groot",
        "Manil Dev Gomony"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "eess.SP"
      ],
      "published": "2026-02-11 12:00:54+00:00",
      "link": "https://arxiv.org/pdf/2602.10770v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11220v1",
      "title": "Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT",
      "abstract": "Large language models (LLMs) have made rapid progress, yet adapting them to downstream scenarios still commonly relies on supervised fine-tuning (SFT). When downstream data exhibit a substantial distribution shift from the model's prior training distribution, SFT can induce catastrophic forgetting. To narrow this gap, data rewriting has been proposed as a data-centric approach that rewrites downstream training data prior to SFT. However, existing methods typically sample rewrites from a prompt-induced conditional distribution, so the resulting targets are not necessarily aligned with the model's natural QA-style generation distribution. Moreover, reliance on fixed templates can lead to diversity collapse. To address these issues, we cast data rewriting as a policy learning problem and learn a rewriting policy that better matches the backbone's QA-style generation distribution while preserving diversity. Since distributional alignment, diversity and task consistency are automatically evaluable but difficult to optimize end-to-end with differentiable objectives, we leverage reinforcement learning to optimize the rewrite distribution under reward feedback and propose an RL-based data-rewriting agent. The agent jointly optimizes QA-style distributional alignment and diversity under a hard task-consistency gate, thereby constructing a higher-quality rewritten dataset for downstream SFT. Extensive experiments show that our method achieves downstream gains comparable to standard SFT while reducing forgetting on non-downstream benchmarks by 12.34% on average. Our code is available at https://anonymous.4open.science/r/Patch-the-Prompt-Gap-4112 .",
      "authors": [
        "Jiacheng Wang",
        "Ping Jian",
        "Zhen Yang",
        "Zirong Chen",
        "Keren Liao",
        "Zhongbin Guo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-11 11:51:37+00:00",
      "link": "https://arxiv.org/pdf/2602.11220v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10754v1",
      "title": "Exploring the impact of adaptive rewiring in Graph Neural Networks",
      "abstract": "This paper explores sparsification methods as a form of regularization in Graph Neural Networks (GNNs) to address high memory usage and computational costs in large-scale graph applications. Using techniques from Network Science and Machine Learning, including Erdős-Rényi for model sparsification, we enhance the efficiency of GNNs for real-world applications. We demonstrate our approach on N-1 contingency assessment in electrical grids, a critical task for ensuring grid reliability. We apply our methods to three datasets of varying sizes, exploring Graph Convolutional Networks (GCN) and Graph Isomorphism Networks (GIN) with different degrees of sparsification and rewiring. Comparison across sparsification levels shows the potential of combining insights from both research fields to improve GNN performance and scalability. Our experiments highlight the importance of tuning sparsity parameters: while sparsity can improve generalization, excessive sparsity may hinder learning of complex patterns. Our adaptive rewiring approach, particularly when combined with early stopping, proves promising by allowing the model to adapt its connectivity structure during training. This research contributes to understanding how sparsity can be effectively leveraged in GNNs for critical applications like power grid reliability analysis.",
      "authors": [
        "Charlotte Cambier van Nooten",
        "Christos Aronis",
        "Yuliya Shapovalova",
        "Lucia Cavallaro"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY",
        "stat.ML"
      ],
      "published": "2026-02-11 11:34:43+00:00",
      "link": "https://arxiv.org/pdf/2602.10754v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10751v1",
      "title": "Predicting integers from continuous parameters",
      "abstract": "We study the problem of predicting numeric labels that are constrained to the integers or to a subrange of the integers. For example, the number of up-votes on social media posts, or the number of bicycles available at a public rental station. While it is possible to model these as continuous values, and to apply traditional regression, this approach changes the underlying distribution on the labels from discrete to continuous. Discrete distributions have certain benefits, which leads us to the question whether such integer labels can be modeled directly by a discrete distribution, whose parameters are predicted from the features of a given instance. Moreover, we focus on the use case of output distributions of neural networks, which adds the requirement that the parameters of the distribution be continuous so that backpropagation and gradient descent may be used to learn the weights of the network. We investigate several options for such distributions, some existing and some novel, and test them on a range of tasks, including tabular learning, sequential prediction and image generation. We find that overall the best performance comes from two distributions: Bitwise, which represents the target integer in bits and places a Bernoulli distribution on each, and a discrete analogue of the Laplace distribution, which uses a distribution with exponentially decaying tails around a continuous mean.",
      "authors": [
        "Bas Maat",
        "Peter Bloem"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 11:30:48+00:00",
      "link": "https://arxiv.org/pdf/2602.10751v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10748v1",
      "title": "Benchmarking Large Language Models for Knowledge Graph Validation",
      "abstract": "Knowledge Graphs (KGs) store structured factual knowledge by linking entities through relationships, crucial for many applications. These applications depend on the KG's factual accuracy, so verifying facts is essential, yet challenging. Expert manual verification is ideal but impractical on a large scale. Automated methods show promise but are not ready for real-world KGs. Large Language Models (LLMs) offer potential with their semantic understanding and knowledge access, yet their suitability and effectiveness for KG fact validation remain largely unexplored.   In this paper, we introduce FactCheck, a benchmark designed to evaluate LLMs for KG fact validation across three key dimensions: (1) LLMs internal knowledge; (2) external evidence via Retrieval-Augmented Generation (RAG); and (3) aggregated knowledge employing a multi-model consensus strategy. We evaluated open-source and commercial LLMs on three diverse real-world KGs. FactCheck also includes a RAG dataset with 2+ million documents tailored for KG fact validation. Additionally, we offer an interactive exploration platform for analyzing verification decisions.   The experimental analyses demonstrate that while LLMs yield promising results, they are still not sufficiently stable and reliable to be used in real-world KG validation scenarios. Integrating external evidence through RAG methods yields fluctuating performance, providing inconsistent improvements over more streamlined approaches -- at higher computational costs. Similarly, strategies based on multi-model consensus do not consistently outperform individual models, underscoring the lack of a one-fits-all solution. These findings further emphasize the need for a benchmark like FactCheck to systematically evaluate and drive progress on this difficult yet crucial task.",
      "authors": [
        "Farzad Shami",
        "Stefano Marchesin",
        "Gianmaria Silvello"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-11 11:24:46+00:00",
      "link": "https://arxiv.org/pdf/2602.10748v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10745v1",
      "title": "Spectral-Spatial Contrastive Learning Framework for Regression on Hyperspectral Data",
      "abstract": "Contrastive learning has demonstrated great success in representation learning, especially for image classification tasks. However, there is still a shortage in studies targeting regression tasks, and more specifically applications on hyperspectral data. In this paper, we propose a spectral-spatial contrastive learning framework for regression tasks for hyperspectral data, in a model-agnostic design allowing to enhance backbones such as 3D convolutional and transformer-based networks. Moreover, we provide a collection of transformations relevant for augmenting hyperspectral data. Experiments on synthetic and real datasets show that the proposed framework and transformations significantly improve the performance of all studied backbone models.",
      "authors": [
        "Mohamad Dhaini",
        "Paul Honeine",
        "Maxime Berar",
        "Antonin Van Exem"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-11 11:16:57+00:00",
      "link": "https://arxiv.org/pdf/2602.10745v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10743v1",
      "title": "Kalman Linear Attention: Parallel Bayesian Filtering For Efficient Language Modelling and State Tracking",
      "abstract": "State-space language models such as Mamba and gated linear attention (GLA) offer efficient alternatives to transformers due to their linear complexity and parallel training, but often lack the expressivity and robust state-tracking needed for complex reasoning. We address these limitations by reframing sequence modelling through a probabilistic lens, using Bayesian filters as a core primitive. While classical filters such as Kalman filters provide principled state estimation and uncertainty tracking, they are typically viewed as inherently sequential. We show that reparameterising the Kalman filter in information form enables its updates to be computed via an associative scan, allowing efficient parallel training. Building on this insight, we introduce the Kalman Linear Attention (KLA) layer, a neural sequence-modelling primitive that performs time-parallel probabilistic inference while maintaining explicit belief-state uncertainty. KLA offers strictly more expressive nonlinear updates and gating than GLA variants while retaining their computational advantages. On language modelling tasks, KLA matches or outperforms modern SSMs and GLAs across representative discrete token-manipulation and state-tracking benchmarks.",
      "authors": [
        "Vaisakh Shaj",
        "Cameron Barker",
        "Aidan Scannell",
        "Andras Szecsenyi",
        "Elliot J. Crowley",
        "Amos Storkey"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 11:11:45+00:00",
      "link": "https://arxiv.org/pdf/2602.10743v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10735v1",
      "title": "Calliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity",
      "abstract": "A narrated e-book combines synchronized audio with digital text, highlighting the currently spoken word or sentence during playback. This format supports early literacy and assists individuals with reading challenges, while also allowing general readers to seamlessly switch between reading and listening. With the emergence of natural-sounding neural Text-to-Speech (TTS) technology, several commercial services have been developed to leverage these technology for converting standard text e-books into high-quality narrated e-books. However, no open-source solutions currently exist to perform this task. In this paper, we present Calliope, an open-source framework designed to fill this gap. Our method leverages state-of-the-art open-source TTS to convert a text e-book into a narrated e-book in the EPUB 3 Media Overlay format. The method offers several innovative steps: audio timestamps are captured directly during TTS, ensuring exact synchronization between narration and text highlighting; the publisher's original typography, styling, and embedded media are strictly preserved; and the entire pipeline operates offline. This offline capability eliminates recurring API costs, mitigates privacy concerns, and avoids copyright compliance issues associated with cloud-based services. The framework currently supports the state-of-the-art open-source TTS systems XTTS-v2 and Chatterbox. A potential alternative approach involves first generating narration via TTS and subsequently synchronizing it with the text using forced alignment. However, while our method ensures exact synchronization, our experiments show that forced alignment introduces drift between the audio and text highlighting significant enough to degrade the reading experience. Source code and usage instructions are available at https://github.com/hugohammer/TTS-Narrated-Ebook-Creator.git.",
      "authors": [
        "Hugo L. Hammer",
        "Vajira Thambawita",
        "Pål Halvorsen"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-11 10:54:59+00:00",
      "link": "https://arxiv.org/pdf/2602.10735v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11219v1",
      "title": "Credal Concept Bottleneck Models: Structural Separation of Epistemic and Aleatoric Uncertainty",
      "abstract": "Decomposing predictive uncertainty into epistemic (model ignorance) and aleatoric (data ambiguity) components is central to reliable decision making, yet most methods estimate both from the same predictive distribution. Recent empirical and theoretical results show these estimates are typically strongly correlated, so changes in predictive spread simultaneously affect both components and blur their semantics. We propose a credal-set formulation in which uncertainty is represented as a set of predictive distributions, so that epistemic and aleatoric uncertainty correspond to distinct geometric properties: the size of the set versus the noise within its elements. We instantiate this idea in a Variational Credal Concept Bottleneck Model with two disjoint uncertainty heads trained by disjoint objectives and non-overlapping gradient paths, yielding separation by construction rather than post hoc decomposition. Across multi-annotator benchmarks, our approach reduces the correlation between epistemic and aleatoric uncertainty by over an order of magnitude compared to standard methods, while improving the alignment of epistemic uncertainty with prediction error and aleatoric uncertainty with ground-truth ambiguity.",
      "authors": [
        "Tanmoy Mukherjee",
        "Marius Kloft",
        "Pierre Marquis",
        "Zied Bouraoui"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 10:54:57+00:00",
      "link": "https://arxiv.org/pdf/2602.11219v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10732v1",
      "title": "Macaron: Controlled, Human-Written Benchmark for Multilingual and Multicultural Reasoning via Template-Filling",
      "abstract": "Multilingual benchmarks rarely test reasoning over culturally grounded premises: translated datasets keep English-centric scenarios, while culture-first datasets often lack control over the reasoning required. We propose Macaron, a template-first benchmark that factorizes reasoning type and cultural aspect across question languages. Using 100 language-agnostic templates that cover 7 reasoning types, 22 cultural aspects, native annotators create scenario-aligned English and local-language multiple-choice questions and systematically derived True/False questions. Macaron contains 11,862 instances spanning 20 countries/cultural contexts, 10 scripts, and 20 languages (including low-resource ones like Amharic, Yoruba, Zulu, Kyrgyz, and some Arabic dialects). In zero-shot evaluation of 21 multilingual LLMs, reasoning-mode models achieve the strongest performance and near-parity between English and local languages, while open-weight models degrade substantially in local languages and often approach chance on T/F tasks. Culture-grounded mathematical and counting templates are consistently the hardest. The data can be accessed here https://huggingface.co/datasets/AlaaAhmed2444/Macaron.",
      "authors": [
        "Alaa Elsetohy",
        "Sama Hadhoud",
        "Haryo Akbarianto Wibowo",
        "Chenxi Whitehouse",
        "Genta Indra Winata",
        "Fajri Koto",
        "Alham Fikri Aji"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 10:45:46+00:00",
      "link": "https://arxiv.org/pdf/2602.10732v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12300v1",
      "title": "Fast and General Automatic Differentiation for Finite-State Methods",
      "abstract": "We propose a new method, that we coined the ``morphism-trick'', to integrate custom implementations of vector-Jacobian products in automatic differentiation softwares, applicable to a wide range of semiring-based computations. Our approach leads to efficient and semiring-agnostic implementations of the backward pass of dynamic programming algorithms. For the particular case of finite-state methods, we introduce an algorithm that computes and differentiates the $\\oplus$-sum of all paths' weight of a finite-state automaton. Results show that, with minimal effort from the user, our novel library allows computing the gradient of a function w.r.t. to the weights of a finite state automaton orders of magnitude faster than state-of-the-art automatic differentiation systems. Implementations are made available via an open-source library distributed under a permissive license.",
      "authors": [
        "Lucas Ondel Yang",
        "Tina Raissi",
        "Martin Kocour",
        "Pablo Riera",
        "Caio Corro"
      ],
      "primary_category": "cs.FL",
      "categories": [
        "cs.FL"
      ],
      "published": "2026-02-11 10:36:18+00:00",
      "link": "https://arxiv.org/pdf/2602.12300v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15894v1",
      "title": "Quality-constrained Entropy Maximization Policy Optimization for LLM Diversity",
      "abstract": "Recent research indicates that while alignment methods significantly improve the quality of large language model(LLM) outputs, they simultaneously reduce the diversity of the models' output. Although some methods have been proposed to enhance LLM output diversity, they often come at the cost of reduced performance. In this work, we first theoretically demonstrate that the alignment task can be decomposed into two distributions: quality and diversity. To enhance the diversity of LLM outputs while ensuring quality, we propose the Quality-constrained Entropy Maximization Policy Optimization (QEMPO). QEMPO aims to maximize the output entropy of the policy while ensuring output quality. By adding different constraints to QEMPO, we obtain different policies. To optimize policies, we propose both online and offline training methods. Experiments validate that QEMPO achieves performance comparable to or even better than RLHF while improving output diversity.",
      "authors": [
        "Haihui Pan",
        "Yuzhong Hong",
        "Shaoke Lv",
        "Junwei Bao",
        "Hongfei Jiang",
        "Yang Song"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-11 10:28:50+00:00",
      "link": "https://arxiv.org/pdf/2602.15894v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10708v1",
      "title": "Interpretable Graph-Level Anomaly Detection via Contrast with Normal Prototypes",
      "abstract": "The task of graph-level anomaly detection (GLAD) is to identify anomalous graphs that deviate significantly from the majority of graphs in a dataset. While deep GLAD methods have shown promising performance, their black-box nature limits their reliability and deployment in real-world applications. Although some recent methods have made attempts to provide explanations for anomaly detection results, they either provide explanations without referencing normal graphs, or rely on abstract latent vectors as prototypes rather than concrete graphs from the dataset. To address these limitations, we propose Prototype-based Graph-Level Anomaly Detection (ProtoGLAD), an interpretable unsupervised framework that provides explanation for each detected anomaly by explicitly contrasting with its nearest normal prototype graph. It employs a point-set kernel to iteratively discover multiple normal prototype graphs and their associated clusters from the dataset, then identifying graphs distant from all discovered normal clusters as anomalies. Extensive experiments on multiple real-world datasets demonstrate that ProtoGLAD achieves competitive anomaly detection performance compared to state-of-the-art GLAD methods while providing better human-interpretable prototype-based explanations.",
      "authors": [
        "Qiuran Zhao",
        "Kai Ming Ting",
        "Xinpeng Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 10:11:16+00:00",
      "link": "https://arxiv.org/pdf/2602.10708v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10706v3",
      "title": "Reducing Estimation Uncertainty Using Normalizing Flows and Stratification",
      "abstract": "Estimating the expectation of a real-valued function of a random variable from sample data is a critical aspect of statistical analysis, with far-reaching implications in various applications. Current methodologies typically assume (semi-)parametric distributions such as Gaussian or mixed Gaussian, leading to significant estimation uncertainty if these assumptions do not hold. We propose a flow-based model, integrated with stratified sampling, that leverages a parametrized neural network to offer greater flexibility in modeling unknown data distributions, thereby mitigating this limitation. Our model shows a marked reduction in estimation uncertainty across multiple datasets, including high-dimensional (30 and 128) ones, outperforming crude Monte Carlo estimators and Gaussian mixture models. Reproducible code is available at https://github.com/rnoxy/flowstrat.",
      "authors": [
        "Paweł Lorek",
        "Rafał Nowak",
        "Rafał Topolnicki",
        "Tomasz Trzciński",
        "Maciej Zięba",
        "Aleksandra Krystecka"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 10:08:10+00:00",
      "link": "https://arxiv.org/pdf/2602.10706v3",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10699v2",
      "title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation",
      "abstract": "Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.",
      "authors": [
        "Jie Jiang",
        "Yangru Huang",
        "Zeyu Wang",
        "Changping Wang",
        "Yuling Xiong",
        "Jun Zhang",
        "Huan Yu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-11 09:57:36+00:00",
      "link": "https://arxiv.org/pdf/2602.10699v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10693v1",
      "title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training",
      "abstract": "Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO",
      "authors": [
        "Guobin Shen",
        "Chenxiao Zhao",
        "Xiang Cheng",
        "Lei Huang",
        "Xing Yu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 09:48:08+00:00",
      "link": "https://arxiv.org/pdf/2602.10693v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11217v1",
      "title": "The Magic Correlations: Understanding Knowledge Transfer from Pretraining to Supervised Fine-Tuning",
      "abstract": "Understanding how language model capabilities transfer from pretraining to supervised fine-tuning (SFT) is fundamental to efficient model development and data curation. In this work, we investigate four core questions: RQ1. To what extent do accuracy and confidence rankings established during pretraining persist after SFT? RQ2. Which benchmarks serve as robust cross-stage predictors and which are unreliable? RQ3. How do transfer dynamics shift with model scale? RQ4. How well does model confidence align with accuracy, as a measure of calibration quality? Does this alignment pattern transfer across training stages? We address these questions through a suite of correlation protocols applied to accuracy and confidence metrics across diverse data mixtures and model scales. Our experiments reveal that transfer reliability varies dramatically across capability categories, benchmarks, and scales -- with accuracy and confidence exhibiting distinct, sometimes opposing, scaling dynamics. These findings shed light on the complex interplay between pretraining decisions and downstream outcomes, providing actionable guidance for benchmark selection, data curation, and efficient model development.",
      "authors": [
        "Simin Fan",
        "Dimitris Paparas",
        "Natasha Noy",
        "Binbin Xiong",
        "Noveen Sachdeva",
        "Berivan Isik"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 09:46:40+00:00",
      "link": "https://arxiv.org/pdf/2602.11217v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10684v1",
      "title": "Privacy Control in Conversational LLM Platforms: A Walkthrough Study",
      "abstract": "Large language models (LLMs) are increasingly integrated into daily life through conversational interfaces, processing user data via natural language inputs and exhibiting advanced reasoning capabilities, which raises new concerns about user control over privacy. While much research has focused on potential privacy risks, less attention has been paid to the data control mechanisms these platforms provide. This study examines six conversational LLM platforms, analyzing how they define and implement features for users to access, edit, delete, and share data. Our analysis reveals an emerging paradigm of data control in conversational LLM platforms, where user data is generated and derived through interaction itself, natural language enables flexible yet often ambiguous control, and multi-user interactions with shared data raise questions of co-ownership and governance. Based on these findings, we offer practical insights for platform developers, policymakers, and researchers to design more effective and usable privacy controls in LLM-powered conversational interactions.",
      "authors": [
        "Zhuoyang Li",
        "Yanlai Wu",
        "Yao Li",
        "Xinning Gui",
        "Yuhan Luo"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-11 09:39:19+00:00",
      "link": "https://arxiv.org/pdf/2602.10684v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10680v1",
      "title": "A solvable high-dimensional model where nonlinear autoencoders learn structure invisible to PCA while test loss misaligns with generalization",
      "abstract": "Many real-world datasets contain hidden structure that cannot be detected by simple linear correlations between input features. For example, latent factors may influence the data in a coordinated way, even though their effect is invisible to covariance-based methods such as PCA. In practice, nonlinear neural networks often succeed in extracting such hidden structure in unsupervised and self-supervised learning. However, constructing a minimal high-dimensional model where this advantage can be rigorously analyzed has remained an open theoretical challenge. We introduce a tractable high-dimensional spiked model with two latent factors: one visible to covariance, and one statistically dependent yet uncorrelated, appearing only in higher-order moments. PCA and linear autoencoders fail to recover the latter, while a minimal nonlinear autoencoder provably extracts both. We analyze both the population risk, and empirical risk minimization. Our model also provides a tractable example where self-supervised test loss is poorly aligned with representation quality: nonlinear autoencoders recover latent structure that linear methods miss, even though their reconstruction loss is higher.",
      "authors": [
        "Vicente Conde Mendes",
        "Lorenzo Bardone",
        "Cédric Koller",
        "Jorge Medina Moreira",
        "Vittorio Erba",
        "Emanuele Troiani",
        "Lenka Zdeborová"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cond-mat.dis-nn",
        "cs.LG"
      ],
      "published": "2026-02-11 09:31:29+00:00",
      "link": "https://arxiv.org/pdf/2602.10680v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10670v1",
      "title": "Domain Knowledge Guided Bayesian Optimization For Autonomous Alignment Of Complex Scientific Instruments",
      "abstract": "Bayesian Optimization (BO) is a powerful tool for optimizing complex non-linear systems. However, its performance degrades in high-dimensional problems with tightly coupled parameters and highly asymmetric objective landscapes, where rewards are sparse. In such needle-in-a-haystack scenarios, even advanced methods like trust-region BO (TurBO) often lead to unsatisfactory results. We propose a domain knowledge guided Bayesian Optimization approach, which leverages physical insight to fundamentally simplify the search problem by transforming coordinates to decouple input features and align the active subspaces with the primary search axes. We demonstrate this approach's efficacy on a challenging 12-dimensional, 6-crystal Split-and-Delay optical system, where conventional approaches, including standard BO, TuRBO and multi-objective BO, consistently led to unsatisfactory results. When combined with an reverse annealing exploration strategy, this approach reliably converges to the global optimum. The coordinate transformation itself is the key to this success, significantly accelerating the search by aligning input co-ordinate axes with the problem's active subspaces. As increasingly complex scientific instruments, from large telescopes to new spectrometers at X-ray Free Electron Lasers are deployed, the demand for robust high-dimensional optimization grows. Our results demonstrate a generalizable paradigm: leveraging physical insight to transform high-dimensional, coupled optimization problems into simpler representations can enable rapid and robust automated tuning for consistent high performance while still retaining current optimization algorithms.",
      "authors": [
        "Aashwin Mishra",
        "Matt Seaberg",
        "Ryan Roussel",
        "Daniel Ratner",
        "Apurva Mehta"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math-ph"
      ],
      "published": "2026-02-11 09:15:20+00:00",
      "link": "https://arxiv.org/pdf/2602.10670v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10661v2",
      "title": "Targeted Syntactic Evaluation of Language Models on Georgian Case Alignment",
      "abstract": "This paper evaluates the performance of transformer-based language models on split-ergative case alignment in Georgian, a particularly rare system for assigning grammatical cases to mark argument roles. We focus on subject and object marking determined through various permutations of nominative, ergative, and dative noun forms. A treebank-based approach for the generation of minimal pairs using the Grew query language is implemented. We create a dataset of 370 syntactic tests made up of seven tasks containing 50-70 samples each, where three noun forms are tested in any given sample. Five encoder- and two decoder-only models are evaluated with word- and/or sentence-level accuracy metrics. Regardless of the specific syntactic makeup, models performed worst in assigning the ergative case correctly and strongest in assigning the nominative case correctly. Performance correlated with the overall frequency distribution of the three forms (NOM > DAT > ERG). Though data scarcity is a known issue for low-resource languages, we show that the highly specific role of the ergative along with a lack of available training data likely contributes to poor performance on this case. The dataset is made publicly available and the methodology provides an interesting avenue for future syntactic evaluations of languages where benchmarks are limited.",
      "authors": [
        "Daniel Gallagher",
        "Gerhard Heyer"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 09:05:02+00:00",
      "link": "https://arxiv.org/pdf/2602.10661v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11215v1",
      "title": "Charting Empirical Laws for LLM Fine-Tuning in Scientific Multi-Discipline Learning",
      "abstract": "While large language models (LLMs) have achieved strong performance through fine-tuning within individual scientific domains, their learning dynamics in multi-disciplinary contexts remains poorly understood, despite the promise of improved generalization and broader applicability through cross-domain knowledge synergy. In this work, we present the first systematic study of multi-disciplinary LLM fine-tuning, constructing a five-discipline corpus and analyzing learning patterns of full fine-tuning, LoRA, LoRA-MoE, and LoRA compositions. Particularly, our study shows that multi-disciplinary learning is substantially more variable than single-discipline training and distills four consistent empirical laws: (1) Balance-then-Diversity: low-resource disciplines degrade performance unless mitigated via diversity-aware upsampling; (2) Merge-then-Align: restoring instruction-following ability is critical for cross-discipline synergy; (3) Optimize-then-Scale: parameter scaling offers limited gains without prior design optimization; and (4) Share-then-Specialize: asymmetric LoRA-MoE yields robust gains with minimal trainable parameters via shared low-rank projection. Together, these laws form a practical recipe for principled multi-discipline fine-tuning and provide actionable guidance for developing generalizable scientific LLMs.",
      "authors": [
        "Lintao Wang",
        "Zhuqiang Lu",
        "Yilin Zhu",
        "Kun Hu",
        "Zhenfei Yin",
        "Shixiang Tang",
        "Zhiyong Wang",
        "Wanli Ouyang",
        "Xinzhu Ma"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 09:04:13+00:00",
      "link": "https://arxiv.org/pdf/2602.11215v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10656v1",
      "title": "AudioRAG: A Challenging Benchmark for Audio Reasoning and Information Retrieval",
      "abstract": "Due to recent advancements in Large Audio-Language Models (LALMs) that demonstrate remarkable performance across a range of sound-, speech- and music-related tasks, there is a growing interest in proposing benchmarks to assess these models. Existing benchmarks generally focus only on reasoning with internal knowledge, neglecting real-world scenarios that require external information grounding. To bridge this gap, we introduce AudioRAG, a novel benchmark designed to evaluate audio-based reasoning augmented by information retrieval in realistic web environments. This benchmark comprises both LLM-generated and manually curated question-answer pairs. Our evaluations reveal that even the state-of-the-art LALMs struggle to answer these questions. We therefore propose an agentic pipeline that integrates audio reasoning with retrieval-augmented generation, providing a stronger baseline for future research.",
      "authors": [
        "Jingru Lin",
        "Chen Zhang",
        "Tianrui Wang",
        "Haizhou Li"
      ],
      "primary_category": "eess.AS",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "published": "2026-02-11 09:00:02+00:00",
      "link": "https://arxiv.org/pdf/2602.10656v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10640v1",
      "title": "Beyond Kemeny Medians: Consensus Ranking Distributions Definition, Properties and Statistical Learning",
      "abstract": "In this article we develop a new method for summarizing a ranking distribution, \\textit{i.e.} a probability distribution on the symmetric group $\\mathfrak{S}_n$, beyond the classical theory of consensus and Kemeny medians. Based on the notion of \\textit{local ranking median}, we introduce the concept of \\textit{consensus ranking distribution} ($\\crd$), a sparse mixture model of Dirac masses on $\\mathfrak{S}_n$, in order to approximate a ranking distribution with small distortion from a mass transportation perspective. We prove that by choosing the popular Kendall $τ$ distance as the cost function, the optimal distortion can be expressed as a function of pairwise probabilities, paving the way for the development of efficient learning methods that do not suffer from the lack of vector space structure on $\\mathfrak{S}_n$. In particular, we propose a top-down tree-structured statistical algorithm that allows for the progressive refinement of a CRD based on ranking data, from the Dirac mass at a Kemeny median at the root of the tree to the empirical ranking data distribution itself at the end of the tree's exhaustive growth. In addition to the theoretical arguments developed, the relevance of the algorithm is empirically supported by various numerical experiments.",
      "authors": [
        "Stephan Clémençon",
        "Ekhine Irurozki"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-11 08:41:14+00:00",
      "link": "https://arxiv.org/pdf/2602.10640v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10635v1",
      "title": "OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization",
      "abstract": "To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.",
      "authors": [
        "Keane Ong",
        "Sabri Boughorbel",
        "Luwei Xiao",
        "Chanakya Ekbote",
        "Wei Dai",
        "Ao Qu",
        "Jingyao Wu",
        "Rui Mao",
        "Ehsan Hoque",
        "Erik Cambria",
        "Gianmarco Mengaldo",
        "Paul Pu Liang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-11 08:35:59+00:00",
      "link": "https://arxiv.org/pdf/2602.10635v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10632v1",
      "title": "The Neurosymbolic Frontier of Nonuniform Ellipticity: Formalizing Sharp Schauder Theory via Topos-Theoretic Reasoning Models",
      "abstract": "This white paper presents a critical synthesis of the recent breakthrough in nonuniformly elliptic regularity theory and the burgeoning field of neurosymbolic large reasoning models (LRMs). We explore the resolution of the long-standing sharp growth rate conjecture in Schauder theory, achieved by Cristiana De Filippis and Giuseppe Mingione, which identifies the exact threshold $q/p < 1 + α/n$ for gradient Hölder continuity. Central to this mathematical achievement is the ``ghost equation'' methodology, a sophisticated auxiliary derivation that bypasses the non-differentiability of classical Euler-Lagrange systems. We propose that the next era of mathematical discovery lies in the integration of these pure analytical constructs with LRMs grounded in topos theory and formal verification frameworks such as Safe and Typed Chain-of-Thought (PC-CoT). By modeling the reasoning process as a categorical colimit in a slice topos, we demonstrate how LRMs can autonomously navigate the ``Dark Side'' of the calculus of variations, providing machine-checkable proofs for regularity bounds in complex, multi-phase physical systems.",
      "authors": [
        "Suyash Mishra"
      ],
      "primary_category": "cs.SC",
      "categories": [
        "cs.SC",
        "cs.AI"
      ],
      "published": "2026-02-11 08:24:57+00:00",
      "link": "https://arxiv.org/pdf/2602.10632v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10623v1",
      "title": "Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling",
      "abstract": "Reward models learned from human preferences are central to aligning large language models (LLMs) via reinforcement learning from human feedback, yet they are often vulnerable to reward hacking due to noisy annotations and systematic biases such as response length or style. We propose Bayesian Non-Negative Reward Model (BNRM), a principled reward modeling framework that integrates non-negative factor analysis into Bradley-Terry (BT) preference model. BNRM represents rewards through a sparse, non-negative latent factor generative process that operates at two complementary levels: instance-specific latent variables induce disentangled reward representations, while sparsity over global latent factors acts as an implicit debiasing mechanism that suppresses spurious correlations. Together, this disentanglement-then-debiasing structure enables robust uncertainty-aware reward learning. To scale BNRM to modern LLMs, we develop an amortized variational inference network conditioned on deep model representations, allowing efficient end-to-end training. Extensive empirical results demonstrate that BNRM substantially mitigates reward over-optimization, improves robustness under distribution shifts, and yields more interpretable reward decompositions than strong baselines.",
      "authors": [
        "Zhibin Duan",
        "Guowei Rong",
        "Zhuo Li",
        "Bo Chen",
        "Mingyuan Zhou",
        "Dandan Guo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 08:14:11+00:00",
      "link": "https://arxiv.org/pdf/2602.10623v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10622v1",
      "title": "How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning",
      "abstract": "Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention masks within a unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention, we propose Gradient-Guided Soft Masking, a gradient-based pre-warmup applied before a linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning. Our code is available at https://github.com/JhCircle/Deepfind-GGSM.",
      "authors": [
        "Jiahao Yuan",
        "Yike Xu",
        "Jinyong Wen",
        "Baokun Wang",
        "Yang Chen",
        "Xiaotong Lin",
        "Wuliang Huang",
        "Ziyi Gao",
        "Xing Fu",
        "Yu Cheng",
        "Weiqiang Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 08:12:43+00:00",
      "link": "https://arxiv.org/pdf/2602.10622v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10613v1",
      "title": "Highly Adaptive Principal Component Regression",
      "abstract": "The Highly Adaptive Lasso (HAL) is a nonparametric regression method that achieves almost dimension-free convergence rates under minimal smoothness assumptions, but its implementation can be computationally prohibitive in high dimensions due to the large basis matrix it requires. The Highly Adaptive Ridge (HAR) has been proposed as a scalable alternative. Building on both procedures, we introduce the Principal Component based Highly Adaptive Lasso (PCHAL) and Principal Component based Highly Adaptive Ridge (PCHAR). These estimators constitute an outcome-blind dimension reduction which offer substantial gains in computational efficiency and match the empirical performances of HAL and HAR. We also uncover a striking spectral link between the leading principal components of the HAL/HAR Gram operator and a discrete sinusoidal basis, revealing an explicit Fourier-type structure underlying the PC truncation.",
      "authors": [
        "Mingxun Wang",
        "Alejandro Schuler",
        "Mark van der Laan",
        "Carlos García Meixide"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-11 08:03:17+00:00",
      "link": "https://arxiv.org/pdf/2602.10613v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10611v1",
      "title": "On the Role of Consistency Between Physics and Data in Physics-Informed Neural Networks",
      "abstract": "Physics-informed neural networks (PINNs) have gained significant attention as a surrogate modeling strategy for partial differential equations (PDEs), particularly in regimes where labeled data are scarce and physical constraints can be leveraged to regularize the learning process. In practice, however, PINNs are frequently trained using experimental or numerical data that are not fully consistent with the governing equations due to measurement noise, discretization errors, or modeling assumptions. The implications of such data-to-PDE inconsistencies on the accuracy and convergence of PINNs remain insufficiently understood. In this work, we systematically analyze how data inconsistency fundamentally limits the attainable accuracy of PINNs. We introduce the concept of a consistency barrier, defined as an intrinsic lower bound on the error that arises from mismatches between the fidelity of the data and the exact enforcement of the PDE residual. To isolate and quantify this effect, we consider the 1D viscous Burgers equation with a manufactured analytical solution, which enables full control over data fidelity and residual errors. PINNs are trained using datasets of progressively increasing numerical accuracy, as well as perfectly consistent analytical data. Results show that while the inclusion of the PDE residual allows PINNs to partially mitigate low-fidelity data and recover the dominant physical structure, the training process ultimately saturates at an error level dictated by the data inconsistency. When high-fidelity numerical data are employed, PINN solutions become indistinguishable from those trained on analytical data, indicating that the consistency barrier is effectively removed. These findings clarify the interplay between data quality and physics enforcement in PINNs providing practical guidance for the construction and interpretation of physics-informed surrogate models.",
      "authors": [
        "Nicolás Becerra-Zuniga",
        "Lucas Lacasa",
        "Eusebio Valero",
        "Gonzalo Rubio"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "physics.comp-ph",
        "stat.ML"
      ],
      "published": "2026-02-11 08:00:53+00:00",
      "link": "https://arxiv.org/pdf/2602.10611v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10609v1",
      "title": "Online Causal Kalman Filtering for Stable and Effective Policy Optimization",
      "abstract": "Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.",
      "authors": [
        "Shuo He",
        "Lang Feng",
        "Xin Cheng",
        "Lei Feng",
        "Bo An"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-11 07:57:43+00:00",
      "link": "https://arxiv.org/pdf/2602.10609v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10607v1",
      "title": "Hierarchical Zero-Order Optimization for Deep Neural Networks",
      "abstract": "Zeroth-order (ZO) optimization has long been favored for its biological plausibility and its capacity to handle non-differentiable objectives, yet its computational complexity has historically limited its application in deep neural networks. Challenging the conventional paradigm that gradients propagate layer-by-layer, we propose Hierarchical Zeroth-Order (HZO) optimization, a novel divide-and-conquer strategy that decomposes the depth dimension of the network. We prove that HZO reduces the query complexity from $O(ML^2)$ to $O(ML \\log L)$ for a network of width $M$ and depth $L$, representing a significant leap over existing ZO methodologies. Furthermore, we provide a detailed error analysis showing that HZO maintains numerical stability by operating near the unitary limit ($L_{lip} \\approx 1$). Extensive evaluations on CIFAR-10 and ImageNet demonstrate that HZO achieves competitive accuracy compared to backpropagation.",
      "authors": [
        "Sansheng Cao",
        "Zhengyu Ma",
        "Yonghong Tian"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 07:56:07+00:00",
      "link": "https://arxiv.org/pdf/2602.10607v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10606v2",
      "title": "S-GRec: Personalized Semantic-Aware Generative Recommendation with Asymmetric Advantage",
      "abstract": "Generative recommendation models sequence generation to produce items end-to-end, but training from behavioral logs often provides weak supervision on underlying user intent. Although Large Language Models (LLMs) offer rich semantic priors that could supply such supervision, direct adoption in industrial recommendation is hindered by two obstacles: semantic signals can conflict with platform business objectives, and LLM inference is prohibitively expensive at scale. This paper presents S-GRec, a semantic-aware framework that decouples an online lightweight generator from an offline LLM-based semantic judge for train-time supervision. S-GRec introduces a two-stage Personalized Semantic Judge (PSJ) that produces interpretable aspect evidence and learns user-conditional aggregation from pairwise feedback, yielding stable semantic rewards. To prevent semantic supervision from deviating from business goals, Asymmetric Advantage Policy Optimization (A2PO) anchors optimization on business rewards (e.g., eCPM) and injects semantic advantages only when they are consistent. Extensive experiments on public benchmarks and a large-scale production system validate both effectiveness and scalability, including statistically significant gains in CTR and a 1.19\\% lift in GMV in online A/B tests, without requiring real-time LLM inference.",
      "authors": [
        "Jie Jiang",
        "Hongbo Tang",
        "Wenjie Wu",
        "Yangru Huang",
        "Zhenmao Li",
        "Qian Li",
        "Changping Wang",
        "Jun Zhang",
        "Huan Yu"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-11 07:54:26+00:00",
      "link": "https://arxiv.org/pdf/2602.10606v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10605v1",
      "title": "Evaluating Numerical Accuracy in Mixed-Precision Computing by Dual-Delta Testing",
      "abstract": "Mixed-precision computing has become increasingly important in modern high-performance computing and machine learning applications. When implementing custom mixed-precision functions -- such as fused operators, optimized GPU kernels, or quantized inference paths -- it is critical to verify their numerical accuracy. Traditional approaches typically compare the custom implementation against a reference using a single error metric. However, this single-delta approach provides limited insight into whether the observed errors are inherent to the precision level or specific to the implementation. This paper introduces \\textit{Dual-Delta Testing}, a systematic methodology that evaluates two error distributions against a high-precision oracle, enabling rigorous comparison between a custom implementation and a baseline reference. We present the mathematical framework, algorithmic formulation, statistical analysis techniques, and practical examples demonstrating the methodology's effectiveness in evaluating numerical accuracy.",
      "authors": [
        "Peichen Xie"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA",
        "cs.SE"
      ],
      "published": "2026-02-11 07:54:04+00:00",
      "link": "https://arxiv.org/pdf/2602.10605v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10603v2",
      "title": "dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning",
      "abstract": "Genomic foundation models have the potential to decode DNA syntax, yet face a fundamental tradeoff in their input representation. Standard fixed-vocabulary tokenizers fragment biologically meaningful motifs such as codons and regulatory elements, while nucleotide-level models preserve biological coherence but incur prohibitive computational costs for long contexts. We introduce dnaHNet, a state-of-the-art tokenizer-free autoregressive model that segments and models genomic sequences end-to-end. Using a differentiable dynamic chunking mechanism, dnaHNet compresses raw nucleotides into latent tokens adaptively, balancing compression with predictive accuracy. Pretrained on prokaryotic genomes, dnaHNet outperforms leading architectures including StripedHyena2 in scaling and efficiency. This recursive chunking yields quadratic FLOP reductions, enabling $>3 \\times$ inference speedup over Transformers. On zero-shot tasks, dnaHNet achieves superior performance in predicting protein variant fitness and gene essentiality, while automatically discovering hierarchical biological structures without supervision. These results establish dnaHNet as a scalable, interpretable framework for next-generation genomic modeling.",
      "authors": [
        "Arnav Shah",
        "Junzhe Li",
        "Parsa Idehpour",
        "Adibvafa Fallahpour",
        "Brandon Wang",
        "Sukjun Hwang",
        "Bo Wang",
        "Patrick D. Hsu",
        "Hani Goodarzi",
        "Albert Gu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 07:48:21+00:00",
      "link": "https://arxiv.org/pdf/2602.10603v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10602v1",
      "title": "Learning Mixture Density via Natural Gradient Expectation Maximization",
      "abstract": "Mixture density networks are neural networks that produce Gaussian mixtures to represent continuous multimodal conditional densities. Standard training procedures involve maximum likelihood estimation using the negative log-likelihood (NLL) objective, which suffers from slow convergence and mode collapse. In this work, we improve the optimization of mixture density networks by integrating their information geometry. Specifically, we interpret mixture density networks as deep latent-variable models and analyze them through an expectation maximization framework, which reveals surprising theoretical connections to natural gradient descent. We then exploit such connections to derive the natural gradient expectation maximization (nGEM) objective. We show that empirically nGEM achieves up to 10$\\times$ faster convergence while adding almost zerocomputational overhead, and scales well to high-dimensional data where NLL otherwise fails.",
      "authors": [
        "Yutao Chen",
        "Jasmine Bayrooti",
        "Steven Morad"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 07:46:47+00:00",
      "link": "https://arxiv.org/pdf/2602.10602v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10598v1",
      "title": "Neuro-symbolic Action Masking for Deep Reinforcement Learning",
      "abstract": "Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.",
      "authors": [
        "Shuai Han",
        "Mehdi Dastani",
        "Shihan Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-11 07:42:53+00:00",
      "link": "https://arxiv.org/pdf/2602.10598v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10597v1",
      "title": "Llama-Polya: Instruction Tuning for Large Language Model based on Polya's Problem-solving",
      "abstract": "This paper introduces Llama-Polya, an instruction-tuned large language model that integrates Polya's four-step problem-solving framework into its dialogue structure to support mathematical reasoning. Mathematical problem-solving is central to students' success in mathematics education, yet many learners struggle to plan, justify, and verify their solutions. Although large language models (LLMs) show promise as intelligent tutors, they often lack structured pedagogical alignment grounded in established learning theories.   To address this gap, we operationalize Polya's problem-solving framework within an instruction-tuned LLM to promote metacognitive engagement and examine the effects of pedagogy-aligned fine-tuning compared to domain-only and general-purpose instruction tuning. Built on the Llama-3.1-8B architecture, Llama-Polya was fine-tuned on synthetic math problem-solving data derived from GSM8K, structured according to Polya's four stages. We developed and evaluated multiple variants-general-purpose instruct, math-domain metamath, pedagogy-aligned polya-v2, and sequential metamath+polya-v2-using both quantitative accuracy metrics and qualitative pedagogical assessments.   Results indicate that models tuned with Polya's framework and domain-specific data produced more balanced reasoning-stage distributions and fewer premature answers. Expert evaluators also observed improved pedagogical coherence and metacognitive prompting, although limitations in personalization and mathematical rigor remained. These findings suggest that pedagogy-grounded instruction tuning can enhance educational alignment and reasoning transparency in LLM-based tutoring systems.",
      "authors": [
        "Unggi Lee",
        "Yeil Jeong",
        "Chohui Lee",
        "Gyuri Byun",
        "Yunseo Lee",
        "Minji Kang",
        "Minji Jeon"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY"
      ],
      "published": "2026-02-11 07:41:49+00:00",
      "link": "https://arxiv.org/pdf/2602.10597v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10595v1",
      "title": "Roughness-Informed Federated Learning",
      "abstract": "Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet faces challenges in non-independent and identically distributed (non-IID) settings due to client drift, which impairs convergence. We propose RI-FedAvg, a novel FL algorithm that mitigates client drift by incorporating a Roughness Index (RI)-based regularization term into the local objective, adaptively penalizing updates based on the fluctuations of local loss landscapes. This paper introduces RI-FedAvg, leveraging the RI to quantify the roughness of high-dimensional loss functions, ensuring robust optimization in heterogeneous settings. We provide a rigorous convergence analysis for non-convex objectives, establishing that RI-FedAvg converges to a stationary point under standard assumptions. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that RI-FedAvg outperforms state-of-the-art baselines, including FedAvg, FedProx, FedDyn, SCAFFOLD, and DP-FedAvg, achieving higher accuracy and faster convergence in non-IID scenarios. Our results highlight RI-FedAvg's potential to enhance the robustness and efficiency of federated learning in practical, heterogeneous environments.",
      "authors": [
        "Mohammad Partohaghighi",
        "Roummel Marcia",
        "Bruce J. West",
        "YangQuan Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 07:35:45+00:00",
      "link": "https://arxiv.org/pdf/2602.10595v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10588v1",
      "title": "TRACE: Theoretical Risk Attribution under Covariate-shift Effects",
      "abstract": "When a source-trained model $Q$ is replaced by a model $\\tilde{Q}$ trained on shifted data, its performance on the source domain can change unpredictably. To address this, we study the two-model risk change, $ΔR := R_P(\\tilde{Q}) - R_P(Q)$, under covariate shift. We introduce TRACE (Theoretical Risk Attribution under Covariate-shift Effects), a framework that decomposes $|ΔR|$ into an interpretable upper bound. This decomposition disentangles the risk change into four actionable factors: two generalization gaps, a model change penalty, and a covariate shift penalty, transforming the bound into a powerful diagnostic tool for understanding why performance has changed. To make TRACE a fully computable diagnostic, we instantiate each term. The covariate shift penalty is estimated via a model sensitivity factor (from high-quantile input gradients) and a data-shift measure; we use feature-space Optimal Transport (OT) by default and provide a robust alternative using Maximum Mean Discrepancy (MMD). The model change penalty is controlled by the average output distance between the two models on the target sample. Generalization gaps are estimated on held-out data. We validate our framework in an idealized linear regression setting, showing the TRACE bound correctly captures the scaling of the true risk difference with the magnitude of the shift. Across synthetic and vision benchmarks, TRACE diagnostics are valid and maintain a strong monotonic relationship with the true performance degradation. Crucially, we derive a deployment gate score that correlates strongly with $|ΔR|$ and achieves high AUROC/AUPRC for gating decisions, enabling safe, label-efficient model replacement.",
      "authors": [
        "Hosein Anjidani",
        "S. Yahya S. R. Tehrani",
        "Mohammad Mahdi Mojahedian",
        "Mohammad Hossein Yassaee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-11 07:22:33+00:00",
      "link": "https://arxiv.org/pdf/2602.10588v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11212v1",
      "title": "Towards Compressive and Scalable Recurrent Memory",
      "abstract": "Transformers face a quadratic bottleneck in attention when scaling to long contexts. Recent approaches introduce recurrent memory to extend context beyond the current window, yet these often face a fundamental trade-off between theoretical principles and practical scalability. To address this, we introduce Elastic Memory, a novel memory architecture grounded in the HiPPO framework for online function approximation. Elastic Memory treats historical sequence as samples from continuous signals, applying optimal online compression to encode them into a fixed-size memory state. For retrieval, we propose a flexible \\textit{polynomial sampling} mechanism that reconstructs a history summary from this compressed state. Elastic Memory consistently outperformed baselines on long-context (32k+) datasets across three domains. With equal parameters, it beat Memorizing Transformer by 16x memory and outperformed Melodi at all memory sizes, even when Melodi had 30% more parameters. When scaling model size, Elastic Memory stayed ahead of all baselines and was significantly faster than Melodi at 4x size. Furthermore, its decoupled design allows for injecting inductive biases at test-time to boost performance.",
      "authors": [
        "Yunchong Song",
        "Jushi Kai",
        "Liming Lu",
        "Kaixi Qiu",
        "Zhouhan Lin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 07:21:49+00:00",
      "link": "https://arxiv.org/pdf/2602.11212v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10587v1",
      "title": "Deep Bootstrap",
      "abstract": "In this work, we propose a novel deep bootstrap framework for nonparametric regression based on conditional diffusion models. Specifically, we construct a conditional diffusion model to learn the distribution of the response variable given the covariates. This model is then used to generate bootstrap samples by pairing the original covariates with newly synthesized responses. We reformulate nonparametric regression as conditional sample mean estimation, which is implemented directly via the learned conditional diffusion model. Unlike traditional bootstrap methods that decouple the estimation of the conditional distribution, sampling, and nonparametric regression, our approach integrates these components into a unified generative framework. With the expressive capacity of diffusion models, our method facilitates both efficient sampling from high-dimensional or multimodal distributions and accurate nonparametric estimation. We establish rigorous theoretical guarantees for the proposed method. In particular, we derive optimal end-to-end convergence rates in the Wasserstein distance between the learned and target conditional distributions. Building on this foundation, we further establish the convergence guarantees of the resulting bootstrap procedure. Numerical studies demonstrate the effectiveness and scalability of our approach for complex regression tasks.",
      "authors": [
        "Jinyuan Chang",
        "Yuling Jiao",
        "Lican Kang",
        "Junjie Shi"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-11 07:20:20+00:00",
      "link": "https://arxiv.org/pdf/2602.10587v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10585v1",
      "title": "Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity",
      "abstract": "The trade-off between interpretability and accuracy remains a core challenge in machine learning. Standard Generalized Additive Models (GAMs) offer clear feature attributions but are often constrained by their strictly additive nature, which can limit predictive performance. Introducing feature interactions can boost accuracy yet may obscure individual feature contributions. To address these issues, we propose Neural Additive Experts (NAEs), a novel framework that seamlessly balances interpretability and accuracy. NAEs employ a mixture of experts framework, learning multiple specialized networks per feature, while a dynamic gating mechanism integrates information across features, thereby relaxing rigid additive constraints. Furthermore, we propose targeted regularization techniques to mitigate variance among expert predictions, facilitating a smooth transition from an exclusively additive model to one that captures intricate feature interactions while maintaining clarity in feature attributions. Our theoretical analysis and experiments on synthetic data illustrate the model's flexibility, and extensive evaluations on real-world datasets confirm that NAEs achieve an optimal balance between predictive accuracy and transparent, feature-level explanations. The code is available at https://github.com/Teddy-XiongGZ/NAE.",
      "authors": [
        "Guangzhi Xiong",
        "Sanchit Sinha",
        "Aidong Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 07:19:25+00:00",
      "link": "https://arxiv.org/pdf/2602.10585v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10584v1",
      "title": "When Gradient Clipping Becomes a Control Mechanism for Differential Privacy in Deep Learning",
      "abstract": "Privacy-preserving training on sensitive data commonly relies on differentially private stochastic optimization with gradient clipping and Gaussian noise. The clipping threshold is a critical control knob: if set too small, systematic over-clipping induces optimization bias; if too large, injected noise dominates updates and degrades accuracy. Existing adaptive clipping methods often depend on per-example gradient norm statistics, adding computational overhead and introducing sensitivity to datasets and architectures. We propose a control-driven clipping strategy that adapts the threshold using a lightweight, weight-only spectral diagnostic computed from model parameters. At periodic probe steps, the method analyzes a designated weight matrix via spectral decomposition and estimates a heavy-tailed spectral indicator associated with training stability. This indicator is smoothed over time and fed into a bounded feedback controller that updates the clipping threshold multiplicatively in the log domain. Because the controller uses only parameters produced during privacy-preserving training, the resulting threshold updates are post-processing and do not increase privacy loss beyond that of the underlying DP optimizer under standard composition accounting.",
      "authors": [
        "Mohammad Partohaghighi",
        "Roummel Marcia",
        "Bruce J. West",
        "YangQuan Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 07:18:25+00:00",
      "link": "https://arxiv.org/pdf/2602.10584v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10583v1",
      "title": "Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets",
      "abstract": "Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines.",
      "authors": [
        "Bo Xue",
        "Yunchong Song",
        "Fanghao Shao",
        "Xuekai Zhu",
        "Lin Chen",
        "Luoyi Fu",
        "Xinbing Wang",
        "Zhouhan Lin"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-11 07:17:41+00:00",
      "link": "https://arxiv.org/pdf/2602.10583v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10576v1",
      "title": "LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization",
      "abstract": "Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.",
      "authors": [
        "Boxiao Wang",
        "Kai Li",
        "Tianyi Liu",
        "Chen Li",
        "Junzhe Wang",
        "Yifan Zhang",
        "Jian Cheng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 07:02:23+00:00",
      "link": "https://arxiv.org/pdf/2602.10576v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10545v1",
      "title": "$μ$pscaling small models: Principled warm starts and hyperparameter transfer",
      "abstract": "Modern large-scale neural networks are often trained and released in multiple sizes to accommodate diverse inference budgets. To improve efficiency, recent work has explored model upscaling: initializing larger models from trained smaller ones in order to transfer knowledge and accelerate convergence. However, this method can be sensitive to hyperparameters that need to be tuned at the target upscaled model size, which is prohibitively costly to do directly. It remains unclear whether the most common workaround -- tuning on smaller models and extrapolating via hyperparameter scaling laws -- is still sound when using upscaling. We address this with principled approaches to upscaling with respect to model widths and efficiently tuning hyperparameters in this setting. First, motivated by $μ$P and any-dimensional architectures, we introduce a general upscaling method applicable to a broad range of architectures and optimizers, backed by theory guaranteeing that models are equivalent to their widened versions and allowing for rigorous analysis of infinite-width limits. Second, we extend the theory of $μ$Transfer to a hyperparameter transfer technique for models upscaled using our method and empirically demonstrate that this method is effective on realistic datasets and architectures.",
      "authors": [
        "Yuxin Ma",
        "Nan Chen",
        "Mateo Díaz",
        "Soufiane Hayou",
        "Dmitriy Kunisky",
        "Soledad Villar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-02-11 05:37:22+00:00",
      "link": "https://arxiv.org/pdf/2602.10545v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10541v1",
      "title": "Solving PDEs in One Shot via Fourier Features with Exact Analytical Derivatives",
      "abstract": "Recent random feature methods for solving partial differential equations (PDEs) reduce computational cost compared to physics-informed neural networks (PINNs) but still rely on iterative optimization or expensive derivative computation. We observe that sinusoidal random Fourier features possess a cyclic derivative structure: the derivative of any order of $\\sin(\\mathbf{W}\\cdot\\mathbf{x}+b)$ is a single sinusoid with a monomial prefactor, computable in $O(1)$ operations. Alternative activations such as $\\tanh$, used in prior one-shot methods like PIELM, lack this property: their higher-order derivatives grow as $O(2^n)$ terms, requiring automatic differentiation for operator assembly. We propose FastLSQ, which combines frozen random Fourier features with analytical operator assembly to solve linear PDEs via a single least-squares call, and extend it to nonlinear PDEs via Newton--Raphson iteration where each linearized step is a FastLSQ solve. On a benchmark of 17 PDEs spanning 1 to 6 dimensions, FastLSQ achieves relative $L^2$ errors of $10^{-7}$ in 0.07\\,s on linear problems, three orders of magnitude more accurate and significantly faster than state-of-the-art iterative PINN solvers, and $10^{-8}$ to $10^{-9}$ on nonlinear problems via Newton iteration in under 9s.",
      "authors": [
        "Antonin Sulc"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA",
        "cs.LG"
      ],
      "published": "2026-02-11 05:28:58+00:00",
      "link": "https://arxiv.org/pdf/2602.10541v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10539v1",
      "title": "What Makes Value Learning Efficient in Residual Reinforcement Learning?",
      "abstract": "Residual reinforcement learning (RL) enables stable online refinement of expressive pretrained policies by freezing the base and learning only bounded corrections. However, value learning in residual RL poses unique challenges that remain poorly understood. In this work, we identify two key bottlenecks: cold start pathology, where the critic lacks knowledge of the value landscape around the base policy, and structural scale mismatch, where the residual contribution is dwarfed by the base action. Through systematic investigation, we uncover the mechanisms underlying these bottlenecks, revealing that simple yet principled solutions suffice: base-policy transitions serve as an essential value anchor for implicit warmup, and critic normalization effectively restores representation sensitivity for discerning value differences. Based on these insights, we propose DAWN (Data-Anchored Warmup and Normalization), a minimal approach targeting efficient value learning in residual RL. By addressing these bottlenecks, DAWN demonstrates substantial efficiency gains across diverse benchmarks, policy architectures, and observation modalities.",
      "authors": [
        "Guozheng Ma",
        "Lu Li",
        "Haoyu Wang",
        "Zixuan Liu",
        "Pierre-Luc Bacon",
        "Dacheng Tao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 05:25:39+00:00",
      "link": "https://arxiv.org/pdf/2602.10539v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10538v2",
      "title": "Why Agentic Theorem Prover Works: A Statistical Provability Theory of Mathematical Reasoning Models",
      "abstract": "Agentic theorem provers -- pipelines that couple a mathematical reasoning model with library retrieval, subgoal-decomposition/search planner, and a proof assistant verifier -- have recently achieved striking empirical success, yet it remains unclear which components drive performance and why such systems work at all despite classical hardness of proof search. We propose a distributional viewpoint and introduce \\textbf{statistical provability}, defined as the finite-horizon success probability of reaching a verified proof, averaged over an instance distribution, and formalize modern theorem-proving pipelines as time-bounded MDPs. Exploiting Bellman structure, we prove existence of optimal policies under mild regularity, derive provability certificates via sub-/super-solution inequalities, and bound the performance gap of score-guided planning (greedy/top-\\(k\\)/beam/rollouts) in terms of approximation error, sequential statistical complexity, representation geometry (metric entropy/doubling structure), and action-gap margin tails. Together, our theory provides a principled, component-sensitive explanation of when and why agentic theorem provers succeed on biased real-world problem distributions, while clarifying limitations in worst-case or adversarial regimes.",
      "authors": [
        "Sho Sonoda",
        "Shunta Akiyama",
        "Yuya Uezato"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-11 05:22:24+00:00",
      "link": "https://arxiv.org/pdf/2602.10538v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10532v1",
      "title": "Statistical Inference and Learning for Shapley Additive Explanations (SHAP)",
      "abstract": "The SHAP (short for Shapley additive explanation) framework has become an essential tool for attributing importance to variables in predictive tasks. In model-agnostic settings, SHAP uses the concept of Shapley values from cooperative game theory to fairly allocate credit to the features in a vector $X$ based on their contribution to an outcome $Y$. While the explanations offered by SHAP are local by nature, learners often need global measures of feature importance in order to improve model explainability and perform feature selection. The most common approach for converting these local explanations into global ones is to compute either the mean absolute SHAP or mean squared SHAP. However, despite their ubiquity, there do not exist approaches for performing statistical inference on these quantities.   In this paper, we take a semi-parametric approach for calibrating confidence in estimates of the $p$th powers of Shapley additive explanations. We show that, by treating the SHAP curve as a nuisance function that must be estimated from data, one can reliably construct asymptotically normal estimates of the $p$th powers of SHAP. When $p \\geq 2$, we show a de-biased estimator that combines U-statistics with Neyman orthogonal scores for functionals of nested regressions is asymptotically normal. When $1 \\leq p < 2$ (and the hence target parameter is not twice differentiable), we construct de-biased U-statistics for a smoothed alternative. In particular, we show how to carefully tune the temperature parameter of the smoothing function in order to obtain inference for the true, unsmoothed $p$th power. We complement these results by presenting a Neyman orthogonal loss that can be used to learn the SHAP curve via empirical risk minimization and discussing excess risk guarantees for commonly used function classes.",
      "authors": [
        "Justin Whitehouse",
        "Ayush Sawarni",
        "Vasilis Syrgkanis"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "published": "2026-02-11 05:01:47+00:00",
      "link": "https://arxiv.org/pdf/2602.10532v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10531v2",
      "title": "From Collapse to Improvement: Statistical Perspectives on the Evolutionary Dynamics of Iterative Training on Contaminated Sources",
      "abstract": "The problem of model collapse has presented new challenges in iterative training of generative models, where such training with synthetic data leads to an overall degradation of performance. This paper looks at the problem from a statistical viewpoint, illustrating that one can actually hope for improvement when models are trained on data contaminated with synthetic samples, as long as there is some amount of fresh information from the true target distribution. In particular, we consider iterative training on samples sourced from a mixture of the true target and synthetic distributions. We analyze the entire iterative evolution in a next-token prediction language model, capturing how the interplay between the mixture weights and the sample size controls the overall long-term performance. With non-trivial mixture weight of the true distribution, even if it decays over time, simply training the model in a contamination-agnostic manner with appropriate sample sizes can avoid collapse and even recover the true target distribution under certain conditions. Simulation studies support our findings and also show that such behavior is more general for other classes of models.",
      "authors": [
        "Soham Bakshi",
        "Sunrit Chakraborty"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "published": "2026-02-11 05:01:46+00:00",
      "link": "https://arxiv.org/pdf/2602.10531v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10530v1",
      "title": "Generalized Robust Adaptive-Bandwidth Multi-View Manifold Learning in High Dimensions with Noise",
      "abstract": "Multiview datasets are common in scientific and engineering applications, yet existing fusion methods offer limited theoretical guarantees, particularly in the presence of heterogeneous and high-dimensional noise. We propose Generalized Robust Adaptive-Bandwidth Multiview Diffusion Maps (GRAB-MDM), a new kernel-based diffusion geometry framework for integrating multiple noisy data sources. The key innovation of GRAB-MDM is a {view}-dependent bandwidth selection strategy that adapts to the geometry and noise level of each view, enabling a stable and principled construction of multiview diffusion operators. Under a common-manifold model, we establish asymptotic convergence results and show that the adaptive bandwidths lead to provably robust recovery of the shared intrinsic structure, even when noise levels and sensor dimensions differ across views. Numerical experiments demonstrate that GRAB-MDM significantly improves robustness and embedding quality compared with fixed-bandwidth and equal-bandwidth baselines, and usually outperform existing algorithms. The proposed framework offers a practical and theoretically grounded solution for multiview sensor fusion in high-dimensional noisy environments.",
      "authors": [
        "Xiucai Ding",
        "Chao Shen",
        "Hau-Tieng Wu"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "published": "2026-02-11 05:01:10+00:00",
      "link": "https://arxiv.org/pdf/2602.10530v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10527v1",
      "title": "AI-PACE: A Framework for Integrating AI into Medical Education",
      "abstract": "The integration of artificial intelligence (AI) into healthcare is accelerating, yet medical education has not kept pace with these technological advancements. This paper synthesizes current knowledge on AI in medical education through a comprehensive analysis of the literature, identifying key competencies, curricular approaches, and implementation strategies. The aim is highlighting the critical need for structured AI education across the medical learning continuum and offer a framework for curriculum development. The findings presented suggest that effective AI education requires longitudinal integration throughout medical training, interdisciplinary collaboration, and balanced attention to both technical fundamentals and clinical applications. This paper serves as a foundation for medical educators seeking to prepare future physicians for an AI-enhanced healthcare environment.",
      "authors": [
        "Scott P. McGrath",
        "Katherine K. Kim",
        "Karnjit Johl",
        "Haibo Wang",
        "Nick Anderson"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "published": "2026-02-11 04:52:26+00:00",
      "link": "https://arxiv.org/pdf/2602.10527v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10522v1",
      "title": "Consistency Meets Verification: Enhancing Test Generation Quality in Large Language Models Without Ground-Truth Solutions",
      "abstract": "Large Language Models (LLMs) have significantly advanced automated test generation, yet existing methods often rely on ground-truth code for verification, risking bug propagation and limiting applicability in test-driven development. We present ConVerTest, a novel two-stage pipeline for synthesizing reliable tests without requiring prior code implementations. ConVerTest integrates three core strategies: (i) Self-Consistency(SC) to generate convergent test cases via majority voting; (ii) Chain-of-Verification (CoVe) for iterative, reasoning-guided code refinement; and (iii) a Dual Execution Agreement to crossvalidate code and tests through consensus. Experiments on BIGCODEBENCH and LESS BASIC PYTHON PROBLEMS (LBPP) benchmarks demonstrate that ConVerTest improves test validity, line coverage, and mutation scores by up to 39%, 28%, and 18% respectively over baselines. Our findings highlight ConVerTest as a robust solution for mitigating hallucinations and enhancing the reliability of autonomous software testing agents.",
      "authors": [
        "Hamed Taherkhani",
        "Alireza DaghighFarsoodeh",
        "Mohammad Chowdhury",
        "Hung Viet Pham",
        "Hadi Hemmati"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-11 04:40:38+00:00",
      "link": "https://arxiv.org/pdf/2602.10522v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10520v2",
      "title": "Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models",
      "abstract": "Looped Language Models (LoopLMs) perform multi-step latent reasoning prior to token generation and outperform conventional LLMs on reasoning benchmarks at smaller parameter budgets. However, attempts to further improve LoopLM reasoning with reinforcement learning have failed - standard objectives such as Group Relative Policy Optimization (GRPO) only assign credit to the final latent state, creating a fundamental mismatch with the model's internal computation. To resolve this, we introduce RLTT (Reward Latent Thought Trajectories), a reinforcement learning framework which distributes reward across the full latent reasoning trajectory. RLTT provides dense, trajectory-level credit assignment without relying on external verifiers and can directly replace GRPO with negligible overhead. Across extensive experiments with Ouro-2.6B-Thinking under identical training and inference conditions, RLTT yields substantial improvements over GRPO on challenging mathematical reasoning benchmarks, improving accuracy by +14.4% on MATH-500, +16.6% on AIME24, and +10.0% on BeyondAIME. Despite being trained exclusively on mathematics, RLTT also transfers effectively to non-mathematical reasoning benchmarks, demonstrating the effectiveness of trajectory-level credit assignment for reinforcement learning in LoopLMs.",
      "authors": [
        "Jonathan Williams",
        "Esin Tureci"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 04:39:42+00:00",
      "link": "https://arxiv.org/pdf/2602.10520v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10518v1",
      "title": "MapVerse: A Benchmark for Geospatial Question Answering on Diverse Real-World Maps",
      "abstract": "Maps are powerful carriers of structured and contextual knowledge, encompassing geography, demographics, infrastructure, and environmental patterns. Reasoning over such knowledge requires models to integrate spatial relationships, visual cues, real-world context, and domain-specific expertise-capabilities that current large language models (LLMs) and vision-language models (VLMs) still struggle to exhibit consistently. Yet, datasets used to benchmark VLMs on map-based reasoning remain narrow in scope, restricted to specific domains, and heavily reliant on artificially generated content (outputs from LLMs or pipeline-based methods), offering limited depth for evaluating genuine geospatial reasoning. To address this gap, we present MapVerse, a large-scale benchmark built on real-world maps. It comprises 11,837 human-authored question-answer pairs across 1,025 maps, spanning ten diverse map categories and multiple question categories for each. The dataset provides a rich setting for evaluating map reading, interpretation, and multimodal reasoning. We evaluate ten state-of-the-art models against our benchmark to establish baselines and quantify reasoning gaps. Beyond overall performance, we conduct fine-grained categorical analyses to assess model inference across multiple dimensions and investigate the visual factors shaping reasoning outcomes. Our findings reveal that while current VLMs perform competitively on classification-style tasks, both open- and closed-source models fall short on advanced tasks requiring complex spatial reasoning.",
      "authors": [
        "Sharat Bhat",
        "Harshita Khandelwal",
        "Tushar Kataria",
        "Vivek Gupta"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-11 04:36:14+00:00",
      "link": "https://arxiv.org/pdf/2602.10518v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10512v1",
      "title": "Don't Eliminate Cut: Exponential Separations in LLM-Based Theorem Proving",
      "abstract": "We develop a theoretical analysis of LLM-guided formal theorem proving in interactive proof assistants (e.g., Lean) by modeling tactic proposal as a stochastic policy in a finite-horizon deterministic MDP. To capture modern representation learning, we treat the state and action spaces as general compact metric spaces and assume Lipschitz policies. To explain the gap between worst-case hardness and empirical success, we introduce problem distributions generated by a reference policy $q$, including a latent-variable model in which proofs exhibit reusable cut/lemma/sketch structure represented by a proof DAG. Under a top-$k$ search protocol and Tsybakov-type margin conditions, we derive lower bounds on finite-horizon success probability that decompose into search and learning terms, with learning controlled by sequential Rademacher/covering complexity. Our main separation result shows that when cut elimination expands a DAG of depth $D$ into a cut-free tree of size $Ω(Λ^D)$ while the cut-aware hierarchical process has size $O(λ^D)$ with $λ\\llΛ$, a flat (cut-free) learner provably requires exponentially more data than a cut-aware hierarchical learner. This provides a principled justification for subgoal decomposition in recent agentic theorem provers.",
      "authors": [
        "Sho Sonoda",
        "Shunta Akiyama",
        "Yuya Uezato"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.LO",
        "stat.ML"
      ],
      "published": "2026-02-11 04:24:09+00:00",
      "link": "https://arxiv.org/pdf/2602.10512v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10506v1",
      "title": "Learning Structure-Semantic Evolution Trajectories for Graph Domain Adaptation",
      "abstract": "Graph Domain Adaptation (GDA) aims to bridge distribution shifts between domains by transferring knowledge from well-labeled source graphs to given unlabeled target graphs. One promising recent approach addresses graph transfer by discretizing the adaptation process, typically through the construction of intermediate graphs or stepwise alignment procedures. However, such discrete strategies often fail in real-world scenarios, where graph structures evolve continuously and nonlinearly, making it difficult for fixed-step alignment to approximate the actual transformation process. To address these limitations, we propose \\textbf{DiffGDA}, a \\textbf{Diff}usion-based \\textbf{GDA} method that models the domain adaptation process as a continuous-time generative process. We formulate the evolution from source to target graphs using stochastic differential equations (SDEs), enabling the joint modeling of structural and semantic transitions. To guide this evolution, a domain-aware network is introduced to steer the generative process toward the target domain, encouraging the diffusion trajectory to follow an optimal adaptation path. We theoretically show that the diffusion process converges to the optimal solution bridging the source and target domains in the latent space. Extensive experiments on 14 graph transfer tasks across 8 real-world datasets demonstrate DiffGDA consistently outperforms state-of-the-art baselines.",
      "authors": [
        "Wei Chen",
        "Xingyu Guo",
        "Shuang Li",
        "Yan Zhong",
        "Zhao Zhang",
        "Fuzhen Zhuang",
        "Hongrui Liu",
        "Libang Zhang",
        "Guo Ye",
        "Huimei He"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 04:11:04+00:00",
      "link": "https://arxiv.org/pdf/2602.10506v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10496v2",
      "title": "Low-Dimensional Execution Manifolds in Transformer Learning Dynamics: Evidence from Modular Arithmetic Tasks",
      "abstract": "We investigate the geometric structure of learning dynamics in overparameterized transformer models through carefully controlled modular arithmetic tasks. Our primary finding is that despite operating in high-dimensional parameter spaces ($d=128$), transformer training trajectories rapidly collapse onto low-dimensional execution manifolds of dimension $3$--$4$. This dimensional collapse is robust across random seeds and moderate task difficulties, though the orientation of the manifold in parameter space varies between runs. We demonstrate that this geometric structure underlies several empirically observed phenomena: (1) sharp attention concentration emerges as saturation along routing coordinates within the execution manifold, (2) SGD commutators are preferentially aligned with the execution subspace (up to $10\\times$ random baseline) early in training, with $>92\\%$ of non-commutativity confined to orthogonal staging directions and this alignment decreasing as training converges, and (3) sparse autoencoders capture auxiliary routing structure but fail to isolate execution itself, which remains distributed across the low-dimensional manifold. Our results suggest a unifying geometric framework for understanding transformer learning, where the vast majority of parameters serve to absorb optimization interference while core computation occurs in a dramatically reduced subspace. These findings have implications for interpretability, training curriculum design, and understanding the role of overparameterization in neural network learning.",
      "authors": [
        "Yongzhong Xu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 03:57:46+00:00",
      "link": "https://arxiv.org/pdf/2602.10496v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10489v1",
      "title": "Learning Adaptive Distribution Alignment with Neural Characteristic Function for Graph Domain Adaptation",
      "abstract": "Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs but is challenged by complex, multi-faceted distributional shifts. Existing methods attempt to reduce distributional shifts by aligning manually selected graph elements (e.g., node attributes or structural statistics), which typically require manually designed graph filters to extract relevant features before alignment. However, such approaches are inflexible: they rely on scenario-specific heuristics, and struggle when dominant discrepancies vary across transfer scenarios. To address these limitations, we propose \\textbf{ADAlign}, an Adaptive Distribution Alignment framework for GDA. Unlike heuristic methods, ADAlign requires no manual specification of alignment criteria. It automatically identifies the most relevant discrepancies in each transfer and aligns them jointly, capturing the interplay between attributes, structures, and their dependencies. This makes ADAlign flexible, scenario-aware, and robust to diverse and dynamically evolving shifts. To enable this adaptivity, we introduce the Neural Spectral Discrepancy (NSD), a theoretically principled parametric distance that provides a unified view of cross-graph shifts. NSD leverages neural characteristic function in the spectral domain to encode feature-structure dependencies of all orders, while a learnable frequency sampler adaptively emphasizes the most informative spectral components for each task via minimax paradigm. Extensive experiments on 10 datasets and 16 transfer tasks show that ADAlign not only outperforms state-of-the-art baselines but also achieves efficiency gains with lower memory usage and faster training.",
      "authors": [
        "Wei Chen",
        "Xingyu Guo",
        "Shuang Li",
        "Zhao Zhang",
        "Yan Zhong",
        "Fuzhen Zhuang",
        "Deqing wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 03:48:04+00:00",
      "link": "https://arxiv.org/pdf/2602.10489v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10485v1",
      "title": "Abstraction Generation for Generalized Planning with Pretrained Large Language Models",
      "abstract": "Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.",
      "authors": [
        "Zhenhe Cui",
        "Huaxiang Xia",
        "Hangjun Shen",
        "Kailun Luo",
        "Yong He",
        "Wei Liang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-11 03:44:15+00:00",
      "link": "https://arxiv.org/pdf/2602.10485v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10480v2",
      "title": "Neuro-Symbolic Synergy for Interactive World Modeling",
      "abstract": "Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical consistency but lack semantic expressivity. To bridge this gap, we propose Neuro-Symbolic Synergy (NeSyS), a framework that integrates the probabilistic semantic priors of LLMs with executable symbolic rules to achieve both expressivity and robustness. NeSyS alternates training between the two models using trajectories inadequately explained by the other. Unlike rule-based prompting, the symbolic WM directly constrains the LLM by modifying its output probability distribution. The neural WM is fine-tuned only on trajectories not covered by symbolic rules, reducing training data by 50% without loss of accuracy. Extensive experiments on three distinct interactive environments, i.e., ScienceWorld, Webshop, and Plancraft, demonstrate NeSyS's consistent advantages over baselines in both WM prediction accuracy and data efficiency.",
      "authors": [
        "Hongyu Zhao",
        "Siyu Zhou",
        "Haolin Yang",
        "Zengyi Qin",
        "Tianyi Zhou"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 03:36:18+00:00",
      "link": "https://arxiv.org/pdf/2602.10480v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10478v2",
      "title": "GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks",
      "abstract": "GPU memory errors are a critical threat to deep learning (DL) frameworks, leading to crashes or even security issues. We introduce GPU-Fuzz, a fuzzer locating these issues efficiently by modeling operator parameters as formal constraints. GPU-Fuzz utilizes a constraint solver to generate test cases that systematically probe error-prone boundary conditions in GPU kernels. Applied to PyTorch, TensorFlow, and PaddlePaddle, we uncovered 13 unknown bugs, demonstrating the effectiveness of GPU-Fuzz in finding memory errors.",
      "authors": [
        "Zihao Li",
        "Hongyi Lu",
        "Yanan Guo",
        "Zhenkai Zhang",
        "Shuai Wang",
        "Fengwei Zhang"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published": "2026-02-11 03:32:43+00:00",
      "link": "https://arxiv.org/pdf/2602.10478v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10476v1",
      "title": "Driving Reaction Trajectories via Latent Flow Matching",
      "abstract": "Recent advances in reaction prediction have achieved near-saturated accuracy on standard benchmarks (e.g., USPTO), yet most state-of-the-art models formulate the task as a one-shot mapping from reactants to products, offering limited insight into the underlying reaction process. Procedural alternatives introduce stepwise generation but often rely on mechanism-specific supervision, discrete symbolic edits, and computationally expensive inference. In this work, we propose LatentRxnFlow, a new reaction prediction paradigm that models reactions as continuous latent trajectories anchored at the thermodynamic product state. Built on Conditional Flow Matching, our approach learns time-dependent latent dynamics directly from standard reactant-product pairs, without requiring mechanistic annotations or curated intermediate labels. While LatentRxnFlow achieves state-of-the-art performance on USPTO benchmarks, more importantly, the continuous formulation exposes the full generative trajectory, enabling trajectory-level diagnostics that are difficult to realize with discrete or one-shot models. We show that latent trajectory analysis allows us to localize and characterize failure modes and to mitigate certain errors via gated inference. Furthermore, geometric properties of the learned trajectories provide an intrinsic signal of epistemic uncertainty, helping prioritize reliably predictable reaction outcomes and flag ambiguous cases for additional validation. Overall, LatentRxnFlow combines strong predictive accuracy with improved transparency, diagnosability, and uncertainty awareness, moving reaction prediction toward more trustworthy deployment in high-throughput discovery workflows.",
      "authors": [
        "Yili Shen",
        "Xiangliang Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 03:28:20+00:00",
      "link": "https://arxiv.org/pdf/2602.10476v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10461v1",
      "title": "Unlocked Backpropagation using Wave Scattering",
      "abstract": "Both the backpropagation algorithm in machine learning and the maximum principle in optimal control theory are posed as a two-point boundary problem, resulting in a \"forward-backward\" lock. We derive a reformulation of the maximum principle in optimal control theory as a hyperbolic initial value problem by introducing an additional \"optimization time\" dimension. We introduce counter-propagating wave variables with finite propagation speed and recast the optimization problem in terms of scattering relationships between them. This relaxation of the original problem can be interpreted as a physical system that equilibrates and changes its physical properties in order to minimize reflections. We discretize this continuum theory to derive a family of fully unlocked algorithms suitable for training neural networks. Different parameter dynamics, including gradient descent, can be derived by demanding dissipation and minimization of reflections at parameter ports. These results also imply that any physical substrate that supports the scattering and dissipation of waves can be interpreted as solving an optimization problem.",
      "authors": [
        "Christian Pehle",
        "Jean-Jacques Slotine"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "published": "2026-02-11 03:00:06+00:00",
      "link": "https://arxiv.org/pdf/2602.10461v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10457v1",
      "title": "Analyzing Fairness of Neural Network Prediction via Counterfactual Dataset Generation",
      "abstract": "Interpreting the inference-time behavior of deep neural networks remains a challenging problem. Existing approaches to counterfactual explanation typically ask: What is the closest alternative input that would alter the model's prediction in a desired way? In contrast, we explore counterfactual datasets. Rather than perturbing the input, our method efficiently finds the closest alternative training dataset, one that differs from the original dataset by changing a few labels. Training a new model on this altered dataset can then lead to a different prediction of a given test instance. This perspective provides a new way to assess fairness by directly analyzing the influence of label bias on training and inference. Our approach can be characterized as probing whether a given prediction depends on biased labels. Since exhaustively enumerating all possible alternate datasets is infeasible, we develop analysis techniques that trace how bias in the training data may propagate through the learning algorithm to the trained network. Our method heuristically ranks and modifies the labels of a bounded number of training examples to construct a counterfactual dataset, retrains the model, and checks whether its prediction on a chosen test case changes. We evaluate our approach on feedforward neural networks across over 1100 test cases from 7 widely-used fairness datasets. Results show that it modifies only a small subset of training labels, highlighting its ability to pinpoint the critical training examples that drive prediction changes. Finally, we demonstrate how our counterfactual datasets reveal connections between training examples and test cases, offering an interpretable way to probe dataset bias.",
      "authors": [
        "Brian Hyeongseok Kim",
        "Jacqueline L. Mitchell",
        "Chao Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 02:55:50+00:00",
      "link": "https://arxiv.org/pdf/2602.10457v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10454v1",
      "title": "LATA: A Tool for LLM-Assisted Translation Annotation",
      "abstract": "The construction of high-quality parallel corpora for translation research has increasingly evolved from simple sentence alignment to complex, multi-layered annotation tasks. This methodological shift presents significant challenges for structurally divergent language pairs, such as Arabic--English, where standard automated tools frequently fail to capture deep linguistic shifts or semantic nuances. This paper introduces a novel, LLM-assisted interactive tool designed to reduce the gap between scalable automation and the rigorous precision required for expert human judgment. Unlike traditional statistical aligners, our system employs a template-based Prompt Manager that leverages large language models (LLMs) for sentence segmentation and alignment under strict JSON output constraints. In this tool, automated preprocessing integrates into a human-in-the-loop workflow, allowing researchers to refine alignments and apply custom translation technique annotations through a stand-off architecture. By leveraging LLM-assisted processing, the tool balances annotation efficiency with the linguistic precision required to analyze complex translation phenomena in specialized domains.",
      "authors": [
        "Baorong Huang",
        "Ali Asiri"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 02:49:01+00:00",
      "link": "https://arxiv.org/pdf/2602.10454v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10451v1",
      "title": "A Multimodal Conditional Mixture Model with Distribution-Level Physics Priors",
      "abstract": "Many scientific and engineering systems exhibit intrinsically multimodal behavior arising from latent regime switching and non-unique physical mechanisms. In such settings, learning the full conditional distribution of admissible outcomes in a physically consistent and interpretable manner remains a challenge. While recent advances in machine learning have enabled powerful multimodal generative modeling, their integration with physics-constrained scientific modeling remains nontrivial, particularly when physical structure must be preserved or data are limited. This work develops a physics-informed multimodal conditional modeling framework based on mixture density representations. Mixture density networks (MDNs) provide an explicit and interpretable parameterization of multimodal conditional distributions. Physical knowledge is embedded through component-specific regularization terms that penalize violations of governing equations or physical laws. This formulation naturally accommodates non-uniqueness and stochasticity while remaining computationally efficient and amenable to conditioning on contextual inputs. The proposed framework is evaluated across a range of scientific problems in which multimodality arises from intrinsic physical mechanisms rather than observational noise, including bifurcation phenomena in nonlinear dynamical systems, stochastic partial differential equations, and atomistic-scale shock dynamics. In addition, the proposed method is compared with a conditional flow matching (CFM) model, a representative state-of-the-art generative modeling approach, demonstrating that MDNs can achieve competitive performance while offering a simpler and more interpretable formulation.",
      "authors": [
        "Jinkyo Han",
        "Bahador Bahmani"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "physics.comp-ph"
      ],
      "published": "2026-02-11 02:46:10+00:00",
      "link": "https://arxiv.org/pdf/2602.10451v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10450v1",
      "title": "Constructing Industrial-Scale Optimization Modeling Benchmark",
      "abstract": "Optimization modeling underpins decision-making in logistics, manufacturing, energy, and finance, yet translating natural-language requirements into correct optimization formulations and solver-executable code remains labor-intensive. Although large language models (LLMs) have been explored for this task, evaluation is still dominated by toy-sized or synthetic benchmarks, masking the difficulty of industrial problems with $10^{3}$--$10^{6}$ (or more) variables and constraints. A key bottleneck is the lack of benchmarks that align natural-language specifications with reference formulations/solver code grounded in real optimization models. To fill in this gap, we introduce MIPLIB-NL, built via a structure-aware reverse construction methodology from real mixed-integer linear programs in MIPLIB~2017. Our pipeline (i) recovers compact, reusable model structure from flat solver formulations, (ii) reverse-generates natural-language specifications explicitly tied to this recovered structure under a unified model--data separation format, and (iii) performs iterative semantic validation through expert review and human--LLM interaction with independent reconstruction checks. This yields 223 one-to-one reconstructions that preserve the mathematical content of the original instances while enabling realistic natural-language-to-optimization evaluation. Experiments show substantial performance degradation on MIPLIB-NL for systems that perform strongly on existing benchmarks, exposing failure modes invisible at toy scale.",
      "authors": [
        "Zhong Li",
        "Hongliang Lu",
        "Tao Wei",
        "Wenyu Liu",
        "Yuxuan Chen",
        "Yuan Lan",
        "Fan Zhang",
        "Zaiwen Wen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "published": "2026-02-11 02:45:31+00:00",
      "link": "https://arxiv.org/pdf/2602.10450v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10449v2",
      "title": "A Unified Theory of Random Projection for Influence Functions",
      "abstract": "Influence functions and related data attribution scores take the form of $g^{\\top}F^{-1}g^{\\prime}$, where $F\\succeq 0$ is a curvature operator. In modern overparametrized models, forming or inverting $F\\in\\mathbb{R}^{d\\times d}$ is prohibitive, motivating scalable influence computation via random projection with a sketch $P \\in \\mathbb{R}^{m\\times d}$. This practice is commonly justified via the Johnson--Lindenstrauss (JL) lemma, which ensures approximate preservation of Euclidean geometry for a fixed dataset. However, JL does not address how sketching behaves under inversion. Furthermore, there is no existing theory that explains how sketching interacts with other widely-used techniques, such as ridge regularization and structured curvature approximations.   We develop a unified theory characterizing when projection provably preserves influence functions. When $g,g^{\\prime}\\in\\text{range}(F)$, we show that: 1) Unregularized projection: exact preservation holds iff $P$ is injective on $\\text{range}(F)$, which necessitates $m\\geq \\text{rank}(F)$; 2) Regularized projection: ridge regularization fundamentally alters the sketching barrier, with approximation guarantees governed by the effective dimension of $F$ at the regularization scale; 3) Factorized influence: for Kronecker-factored curvatures $F=A\\otimes E$, the guarantees continue to hold for decoupled sketches $P=P_A\\otimes P_E$, even though such sketches exhibit row correlations that violate i.i.d. assumptions. Beyond this range-restricted setting, we analyze out-of-range test gradients and quantify a leakage term that arises when test gradients have components in $\\ker(F)$. This yields guarantees for influence queries on general test points.   Overall, this work develops a novel theory that characterizes when projection provably preserves influence and provides principled guidance for choosing the sketch size in practice.",
      "authors": [
        "Pingbang Hu",
        "Yuzheng Hu",
        "Jiaqi W. Ma",
        "Han Zhao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 02:42:04+00:00",
      "link": "https://arxiv.org/pdf/2602.10449v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10445v2",
      "title": "End-to-End Semantic ID Generation for Generative Advertisement Recommendation",
      "abstract": "Generative Recommendation (GR) has excelled by framing recommendation as next-token prediction. This paradigm relies on Semantic IDs (SIDs) to tokenize large-scale items into discrete sequences. Existing GR approaches predominantly generate SIDs via Residual Quantization (RQ), where items are encoded into embeddings and then quantized to discrete SIDs. However, this paradigm suffers from inherent limitations: 1) Objective misalignment and semantic degradation stemming from the two-stage compression; 2) Error accumulation inherent in the structure of RQ. To address these limitations, we propose UniSID, a Unified SID generation framework for generative advertisement recommendation. Specifically, we jointly optimize embeddings and SIDs in an end-to-end manner from raw advertising data, enabling semantic information to flow directly into the SID space and thus addressing the inherent limitations of the two-stage cascading compression paradigm. To capture fine-grained semantics, a multi-granularity contrastive learning strategy is introduced to align distinct items across SID levels. Finally, a summary-based ad reconstruction mechanism is proposed to encourage SIDs to capture high-level semantic information that is not explicitly present in advertising contexts. Experiments demonstrate that UniSID consistently outperforms state-of-the-art SID generation methods, yielding up to a 4.62% improvement in Hit Rate metrics across downstream advertising scenarios compared to the strongest baseline.",
      "authors": [
        "Jie Jiang",
        "Xinxun Zhang",
        "Enming Zhang",
        "Yuling Xiong",
        "Jun Zhang",
        "Jingwen Wang",
        "Huan Yu",
        "Yuxiang Wang",
        "Hao Wang",
        "Xiao Yan",
        "Jiawei Jiang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "published": "2026-02-11 02:38:26+00:00",
      "link": "https://arxiv.org/pdf/2602.10445v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10441v1",
      "title": "LakeMLB: Data Lake Machine Learning Benchmark",
      "abstract": "Modern data lakes have emerged as foundational platforms for large-scale machine learning, enabling flexible storage of heterogeneous data and structured analytics through table-oriented abstractions. Despite their growing importance, standardized benchmarks for evaluating machine learning performance in data lake environments remain scarce. To address this gap, we present LakeMLB (Data Lake Machine Learning Benchmark), designed for the most common multi-source, multi-table scenarios in data lakes. LakeMLB focuses on two representative multi-table scenarios, Union and Join, and provides three real-world datasets for each scenario, covering government open data, finance, Wikipedia, and online marketplaces. The benchmark supports three representative integration strategies: pre-training-based, data augmentation-based, and feature augmentation-based approaches. We conduct extensive experiments with state-of-the-art tabular learning methods, offering insights into their performance under complex data lake scenarios. We release both datasets and code to facilitate rigorous research on machine learning in data lake ecosystems; the benchmark is available at https://github.com/zhengwang100/LakeMLB.",
      "authors": [
        "Feiyu Pan",
        "Tianbin Zhang",
        "Aoqian Zhang",
        "Yu Sun",
        "Zheng Wang",
        "Lixing Chen",
        "Li Pan",
        "Jianhua Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 02:33:29+00:00",
      "link": "https://arxiv.org/pdf/2602.10441v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11210v1",
      "title": "SWE-MiniSandbox: Container-Free Reinforcement Learning for Building Software Engineering Agents",
      "abstract": "Reinforcement learning (RL) has become a key paradigm for training software engineering (SWE) agents, but existing pipelines typically rely on per-task containers for isolation. At scale, pre-built container images incur substantial storage overhead, slow environment setup, and require container-management privileges. We propose SWE-MiniSandbox, a lightweight, container-free method that enables scalable RL training of SWE agents without sacrificing isolation. Instead of relying on per-instance containers, SWE-MiniSandbox executes each task in an isolated workspace backed by kernel-level mechanisms, substantially reducing system overhead. It leverages lightweight environment pre-caching techniques to eliminate the need for bulky container images. As a result, our approach lowers disk usage to approximately 5\\% of that required by container-based pipelines and reduces environment preparation time to about 25\\% of the container baseline. Empirical results demonstrate that SWE-MiniSandbox achieves evaluation performance comparable to standard container-based pipelines. By removing the dependency on heavy container infrastructure, SWE-MiniSandbox offers a practical and accessible foundation for scaling RL-based SWE agents, particularly in resource-constrained research environments.",
      "authors": [
        "Danlong Yuan",
        "Wei Wu",
        "Zhengren Wang",
        "Xueliang Zhao",
        "Huishuai Zhang",
        "Dongyan Zhao"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-11 02:33:04+00:00",
      "link": "https://arxiv.org/pdf/2602.11210v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10437v2",
      "title": "Control Reinforcement Learning: Interpretable Token-Level Steering of LLMs via Sparse Autoencoder Features",
      "abstract": "Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE features for steering at each token, producing interpretable intervention logs: the learned policy identifies features that change model outputs when amplified. Adaptive Feature Masking encourages diverse feature discovery while preserving singlefeature interpretability. The framework yields new analysis capabilities: branch point tracking locates tokens where feature choice determines output correctness; critic trajectory analysis separates policy limitations from value estimation errors; layer-wise comparison reveals syntactic features in early layers and semantic features in later layers. On Gemma 2 2B across MMLU, BBQ, GSM8K, HarmBench, and XSTest, CRL achieves improvements while providing per-token intervention logs. These results establish learned feature steering as a mechanistic interpretability tool that complements static feature analysis with dynamic intervention probes",
      "authors": [
        "Seonglae Cho",
        "Zekun Wu",
        "Adriano Koshiyama"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-11 02:28:49+00:00",
      "link": "https://arxiv.org/pdf/2602.10437v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11209v1",
      "title": "SAFuzz: Semantic-Guided Adaptive Fuzzing for LLM-Generated Code",
      "abstract": "While AI-coding assistants accelerate software development, current testing frameworks struggle to keep pace with the resulting volume of AI-generated code. Traditional fuzzing techniques often allocate resources uniformly and lack semantic awareness of algorithmic vulnerability patterns, leading to inefficient resource usage and missed vulnerabilities. To address these limitations, we present a hybrid testing framework that leverages LLM-guided adaptive fuzzing to detect algorithmic vulnerabilities efficiently. Our system SAFuzz integrates prompt-based behavioral diversification, harness generation with problem-specific oracles, and an LLM-based predictor to enable adaptive resource allocation and dynamic early stopping. Evaluating SAFuzz on CSES algorithmic problems, we improve vulnerability discrimination precision from 77.9% to 85.7% and achieve a 1.71x reduction in time cost compared to SOTA GreenFuzz while maintaining comparable recall. We further observe that combining our approach with existing unit test generation methods yields complementary gains, increasing the bug detection recall from 67.3% to 79.5%.",
      "authors": [
        "Ziyi Yang",
        "Kalit Inani",
        "Keshav Kabra",
        "Vima Gupta",
        "Anand Padmanabha Iyer"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.CR"
      ],
      "published": "2026-02-11 02:19:50+00:00",
      "link": "https://arxiv.org/pdf/2602.11209v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10432v1",
      "title": "A Dual-Stream Physics-Augmented Unsupervised Architecture for Runtime Embedded Vehicle Health Monitoring",
      "abstract": "Runtime quantification of vehicle operational intensity is essential for predictive maintenance and condition monitoring in commercial and heavy-duty fleets. Traditional metrics like mileage fail to capture mechanical burden, while unsupervised deep learning models detect statistical anomalies, typically transient surface shocks, but often conflate statistical stability with mechanical rest. We identify this as a critical blind spot: high-load steady states, such as hill climbing with heavy payloads, appear statistically normal yet impose significant drivetrain fatigue. To resolve this, we propose a Dual-Stream Architecture that fuses unsupervised learning for surface anomaly detection with macroscopic physics proxies for cumulative load estimation. This approach leverages low-frequency sensor data to generate a multi-dimensional health vector, distinguishing between dynamic hazards and sustained mechanical effort. Validated on a RISC-V embedded platform, the architecture demonstrates low computational overhead, enabling comprehensive, edge-based health monitoring on resource-constrained ECUs without the latency or bandwidth costs of cloud-based monitoring.",
      "authors": [
        "Enzo Nicolas Spotorno",
        "Antonio Augusto Medeiros Frohlich"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 02:19:22+00:00",
      "link": "https://arxiv.org/pdf/2602.10432v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10422v1",
      "title": "Navigating heterogeneous protein landscapes through geometry-aware smoothing",
      "abstract": "The evolutionary fitness landscape of biological molecules is extremely sparse and heterogeneous, with functional sequences forming isolated dense ``islands'' within a vast combinatorial space of largely non-functional variants. Protein sequences, in particular, exemplify this structure, yet most generative artificial intelligence models implicitly assume a homogeneous data distribution. We show that this assumption fundamentally breaks down in heterogeneous biological sequence spaces: fixed global noise levels impose a destructive trade-off, either oversmoothing dense functional clusters or fragmenting sparse regions and producing non-functional hallucinations. To address this limitation, we introduce \\emph{Density-Dependent Smoothing} (DDS), a geometry-aware generative framework that adapts stochastic smoothing to the local density of the underlying sequence landscape. By inversely coupling diffusion noise to estimated sequence density, DDS enables gentle refinement in high-density functional regions while promoting controlled exploration across sparse regions. Implemented as a plug-in mechanism for discrete molecular sampling, DDS consistently outperforms state-of-the-art diffusion and autoregressive models across antibody repertoires, therapeutic antibody design, antimicrobial peptide generation and coronavirus antibody design. Together, these results show that fixed global smoothing assumptions fundamentally limit generative modeling in sparse biological sequence spaces, and that geometry-aware smoothing removes this constraint, enabling reliable exploration and design previously unattainable with fixed-noise generative models.",
      "authors": [
        "Srinivas Anumasa",
        "Barath Chandran",
        "Tingting Chen",
        "Nuwaisir Mohammad Rahman",
        "Yingtao Zhu",
        "Rushi Shah",
        "Hongyu He",
        "Peisong Zhang",
        "Yizhen Liao",
        "Yiming Tang",
        "Yong Shen",
        "Tianfan Fu",
        "Rui Qing",
        "Xiao Li",
        "Sebastian Maurer-Stroh",
        "Xinyi Su",
        "Zhizhuo Zhang",
        "Dianbo Liu"
      ],
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE"
      ],
      "published": "2026-02-11 02:07:08+00:00",
      "link": "https://arxiv.org/pdf/2602.10422v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10420v1",
      "title": "Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning",
      "abstract": "Flow matching has emerged as a powerful framework for generative modeling, with recent empirical successes highlighting the effectiveness of signal-space prediction ($x$-prediction). In this work, we investigate the transfer of this paradigm to binary manifolds, a fundamental setting for generative modeling of discrete data. While $x$-prediction remains effective, we identify a latent structural mismatch that arises when it is coupled with velocity-based objectives ($v$-loss), leading to a time-dependent singular weighting that amplifies gradient sensitivity to approximation errors. Motivated by this observation, we formalize prediction-loss alignment as a necessary condition for flow matching training. We prove that re-aligning the objective to the signal space ($x$-loss) eliminates the singular weighting, yielding uniformly bounded gradients and enabling robust training under uniform timestep sampling without reliance on heuristic schedules. Finally, with alignment secured, we examine design choices specific to binary data, revealing a topology-dependent distinction between probabilistic objectives (e.g., cross-entropy) and geometric losses (e.g., mean squared error). Together, these results provide theoretical foundations and practical guidelines for robust flow matching on binary -- and related discrete -- domains, positioning signal-space alignment as a key principle for robust diffusion learning.",
      "authors": [
        "Jiadong Hong",
        "Lei Liu",
        "Xinyu Bian",
        "Wenjie Wang",
        "Zhaoyang Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IT",
        "eess.IV",
        "eess.SP"
      ],
      "published": "2026-02-11 02:02:30+00:00",
      "link": "https://arxiv.org/pdf/2602.10420v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10419v1",
      "title": "Equivariant Evidential Deep Learning for Interatomic Potentials",
      "abstract": "Uncertainty quantification (UQ) is critical for assessing the reliability of machine learning interatomic potentials (MLIPs) in molecular dynamics (MD) simulations, identifying extrapolation regimes and enabling uncertainty-aware workflows such as active learning for training dataset construction. Existing UQ approaches for MLIPs are often limited by high computational cost or suboptimal performance. Evidential deep learning (EDL) provides a theoretically grounded single-model alternative that determines both aleatoric and epistemic uncertainty in a single forward pass. However, extending evidential formulations from scalar targets to vector-valued quantities such as atomic forces introduces substantial challenges, particularly in maintaining statistical self-consistency under rotational transformations. To address this, we propose \\textit{Equivariant Evidential Deep Learning for Interatomic Potentials} ($\\text{e}^2$IP), a backbone-agnostic framework that models atomic forces and their uncertainty jointly by representing uncertainty as a full $3\\times3$ symmetric positive definite covariance tensor that transforms equivariantly under rotations. Experiments on diverse molecular benchmarks show that $\\text{e}^2$IP provides a stronger accuracy-efficiency-reliability balance than the non-equivariant evidential baseline and the widely used ensemble method. It also achieves better data efficiency through the fully equivariant architecture while retaining single-model inference efficiency.",
      "authors": [
        "Zhongyao Wang",
        "Taoyong Cui",
        "Jiawen Zou",
        "Shufei Zhang",
        "Bo Yan",
        "Wanli Ouyang",
        "Weimin Tan",
        "Mao Su"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-11 02:00:25+00:00",
      "link": "https://arxiv.org/pdf/2602.10419v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13325v1",
      "title": "Graph neural networks uncover structure and functions underlying the activity of simulated neural assemblies",
      "abstract": "Graph neural networks trained to predict observable dynamics can be used to decompose the temporal activity of complex heterogeneous systems into simple, interpretable representations. Here we apply this framework to simulated neural assemblies with thousands of neurons and demonstrate that it can jointly reveal the connectivity matrix, the neuron types, the signaling functions, and in some cases hidden external stimuli. In contrast to existing machine learning approaches such as recurrent neural networks and transformers, which emphasize predictive accuracy but offer limited interpretability, our method provides both reliable forecasts of neural activity and interpretable decomposition of the mechanisms governing large neural assemblies.",
      "authors": [
        "Cédric Allier",
        "Larissa Heinrich",
        "Magdalena Schneider",
        "Stephan Saalfeld"
      ],
      "primary_category": "q-bio.NC",
      "categories": [
        "q-bio.NC",
        "cs.LG"
      ],
      "published": "2026-02-11 01:59:27+00:00",
      "link": "https://arxiv.org/pdf/2602.13325v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10414v1",
      "title": "EVOKE: Emotion Vocabulary Of Korean and English",
      "abstract": "This paper introduces EVOKE, a parallel dataset of emotion vocabulary in English and Korean. The dataset offers comprehensive coverage of emotion words in each language, in addition to many-to-many translations between words in the two languages and identification of language-specific emotion words. The dataset contains 1,427 Korean words and 1,399 English words, and we systematically annotate 819 Korean and 924 English adjectives and verbs. We also annotate multiple meanings of each word and their relationships, identifying polysemous emotion words and emotion-related metaphors. The dataset is, to our knowledge, the most comprehensive, systematic, and theory-agnostic dataset of emotion words in both Korean and English to date. It can serve as a practical tool for emotion science, psycholinguistics, computational linguistics, and natural language processing, allowing researchers to adopt different views on the resource reflecting their needs and theoretical perspectives. The dataset is publicly available at https://github.com/yoonwonj/EVOKE.",
      "authors": [
        "Yoonwon Jung",
        "Hagyeong Shin",
        "Benjamin K. Bergen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 01:54:22+00:00",
      "link": "https://arxiv.org/pdf/2602.10414v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10410v1",
      "title": "LUCID: Attention with Preconditioned Representations",
      "abstract": "Softmax-based dot-product attention is a cornerstone of Transformer architectures, enabling remarkable capabilities such as in-context learning. However, as context lengths increase, a fundamental limitation of the softmax function emerges: it tends to diffuse probability mass to irrelevant tokens degrading performance in long-sequence scenarios. Furthermore, attempts to sharpen focus by lowering softmax temperature hinder learnability due to vanishing gradients. We introduce LUCID Attention, an architectural modification that applies a preconditioner to the attention probabilities. This preconditioner, derived from exponentiated key-key similarities, minimizes overlap between the keys in a Reproducing Kernel Hilbert Space, thus allowing the query to focus on important keys among large number of keys accurately with same computational complexity as standard attention. Additionally, LUCID's preconditioning-based approach to retrieval bypasses the need for low temperature and the learnability problems associated with it. We validate our approach by training ~1 billion parameter language models evaluated on up to 128K tokens. Our results demonstrate significant gains on long-context retrieval tasks, specifically retrieval tasks from BABILong, RULER, SCROLLS and LongBench. For instance, LUCID achieves up to 18% improvement in BABILong and 14% improvement in RULER multi-needle performance compared to standard attention.",
      "authors": [
        "Sai Surya Duvvuri",
        "Nirmal Patel",
        "Nilesh Gupta",
        "Inderjit S. Dhillon"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-11 01:46:32+00:00",
      "link": "https://arxiv.org/pdf/2602.10410v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10408v1",
      "title": "Gated Removal of Normalization in Transformers Enables Stable Training and Efficient Inference",
      "abstract": "Normalization is widely viewed as essential for stabilizing Transformer training. We revisit this assumption for pre-norm Transformers and ask to what extent sample-dependent normalization is needed inside Transformer blocks. We introduce TaperNorm, a drop-in replacement for RMSNorm/LayerNorm that behaves exactly like the standard normalizer early in training and then smoothly tapers to a learned sample-independent linear/affine map. A single global gate is held at $g{=}1$ during gate warmup, used to calibrate the scaling branch via EMAs, and then cosine-decayed to $g{=}0$, at which point per-token statistics vanish and the resulting fixed scalings can be folded into adjacent linear projections. Our theoretical and empirical results isolate scale anchoring as the key role played by output normalization: as a (near) $0$-homogeneous map it removes radial gradients at the output, whereas without such an anchor cross-entropy encourages unbounded logit growth (``logit chasing''). We further show that a simple fixed-target auxiliary loss on the pre-logit residual-stream scale provides an explicit alternative anchor and can aid removal of the final normalization layer. Empirically, TaperNorm matches normalized baselines under identical setups while eliminating per-token statistics and enabling these layers to be folded into adjacent linear projections at inference. On an efficiency microbenchmark, folding internal scalings yields up to $1.22\\times$ higher throughput in last-token logits mode. These results take a step towards norm-free Transformers while identifying the special role output normalization plays.",
      "authors": [
        "Andrei Kanavalau",
        "Carmen Amo Alonso",
        "Sanjay Lall"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-11 01:40:34+00:00",
      "link": "https://arxiv.org/pdf/2602.10408v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10404v1",
      "title": "Modular Multi-Task Learning for Chemical Reaction Prediction",
      "abstract": "Adapting large language models (LLMs) trained on broad organic chemistry to smaller, domain-specific reaction datasets is a key challenge in chemical and pharmaceutical R&D. Effective specialisation requires learning new reaction knowledge while preserving general chemical understanding across related tasks. Here, we evaluate Low-Rank Adaptation (LoRA) as a parameter-efficient alternative to full fine-tuning for organic reaction prediction on limited, complex datasets. Using USPTO reaction classes and challenging C-H functionalisation reactions, we benchmark forward reaction prediction, retrosynthesis and reagent prediction. LoRA achieves accuracy comparable to full fine-tuning while effectively mitigating catastrophic forgetting and better preserving multi-task performance. Both fine-tuning approaches generalise beyond training distributions, producing plausible alternative solvent predictions. Notably, C-H functionalisation fine-tuning reveals that LoRA and full fine-tuning encode subtly different reactivity patterns, suggesting more effective reaction-specific adaptation with LoRA. As LLMs continue to scale, our results highlight the practicality of modular, parameter-efficient fine-tuning strategies for their flexible deployment for chemistry applications.",
      "authors": [
        "Jiayun Pang",
        "Ahmed M. Zaitoun",
        "Xacobe Couso Cambeiro",
        "Ivan Vulić"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-11 01:17:06+00:00",
      "link": "https://arxiv.org/pdf/2602.10404v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10388v2",
      "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs",
      "abstract": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.",
      "authors": [
        "Zhongzhi Li",
        "Xuansheng Wu",
        "Yijiang Li",
        "Lijie Hu",
        "Ninghao Liu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-11 00:23:13+00:00",
      "link": "https://arxiv.org/pdf/2602.10388v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10387v1",
      "title": "Making Databases Faster with LLM Evolutionary Sampling",
      "abstract": "Traditional query optimization relies on cost-based optimizers that estimate execution cost (e.g., runtime, memory, and I/O) using predefined heuristics and statistical models. Improving these heuristics requires substantial engineering effort, and even when implemented, these heuristics often cannot take into account semantic correlations in queries and schemas that could enable better physical plans. Using our DBPlanBench harness for the DataFusion engine, we expose the physical plan through a compact serialized representation and let the LLM propose localized edits that can be applied and executed. We then apply an evolutionary search over these edits to refine candidates across iterations. Our key insight is that LLMs can leverage semantic knowledge to identify and apply non-obvious optimizations, such as join orderings that minimize intermediate cardinalities. We obtain up to 4.78$\\times$ speedups on some queries and we demonstrate a small-to-large workflow in which optimizations found on small databases transfer effectively to larger databases.",
      "authors": [
        "Mehmet Hamza Erol",
        "Xiangpeng Hao",
        "Federico Bianchi",
        "Ciro Greco",
        "Jacopo Tagliabue",
        "James Zou"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "published": "2026-02-11 00:21:51+00:00",
      "link": "https://arxiv.org/pdf/2602.10387v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10384v2",
      "title": "When Tables Go Crazy: Evaluating Multimodal Models on French Financial Documents",
      "abstract": "Vision-language models (VLMs) perform well on many document understanding tasks, yet their reliability in specialized, non-English domains remains underexplored. This gap is especially critical in finance, where documents mix dense regulatory text, numerical tables, and visual charts, and where extraction errors can have real-world consequences. We introduce Multimodal Finance Eval, the first multimodal benchmark for evaluating French financial document understanding. The dataset contains 1,204 expert-validated questions spanning text extraction, table comprehension, chart interpretation, and multi-turn conversational reasoning, drawn from real investment prospectuses, KIDs, and PRIIPs. We evaluate six open-weight VLMs (8B-124B parameters) using an LLM-as-judge protocol. While models achieve strong performance on text and table tasks (85-90% accuracy), they struggle with chart interpretation (34-62%). Most notably, multi-turn dialogue reveals a sharp failure mode: early mistakes propagate across turns, driving accuracy down to roughly 50% regardless of model size.   These results show that current VLMs are effective for well-defined extraction tasks but remain brittle in interactive, multi-step financial analysis. Multimodal Finance Eval offers a challenging benchmark to measure and drive progress in this high-stakes setting.",
      "authors": [
        "Virginie Mouilleron",
        "Théo Lasnier",
        "Djamé Seddah"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-11 00:04:56+00:00",
      "link": "https://arxiv.org/pdf/2602.10384v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10378v1",
      "title": "Flash-SD-KDE: Accelerating SD-KDE with Tensor Cores",
      "abstract": "Score-debiased kernel density estimation (SD-KDE) achieves improved asymptotic convergence rates over classical KDE, but its use of an empirical score has made it significantly slower in practice. We show that by re-ordering the SD-KDE computation to expose matrix-multiplication structure, Tensor Cores can be used to accelerate the GPU implementation. On a 32k-sample 16-dimensional problem, our approach runs up to $47\\times$ faster than a strong SD-KDE GPU baseline and $3{,}300\\times$ faster than scikit-learn's KDE. On a larger 1M-sample 16-dimensional task evaluated on 131k queries, Flash-SD-KDE completes in $2.3$ s on a single GPU, making score-debiased density estimation practical at previously infeasible scales.",
      "authors": [
        "Elliot L. Epstein",
        "Rajat Vadiraj Dwaraknath",
        "John Winnicki"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "published": "2026-02-10 23:56:03+00:00",
      "link": "https://arxiv.org/pdf/2602.10378v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10371v1",
      "title": "Simple LLM Baselines are Competitive for Model Diffing",
      "abstract": "Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no systematic comparison of these approaches exists nor are there established evaluation criteria. We address this gap by proposing evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and use these to compare existing methods. Our results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.",
      "authors": [
        "Elias Kempf",
        "Simon Schrodi",
        "Bartosz Cywiński",
        "Thomas Brox",
        "Neel Nanda",
        "Arthur Conmy"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 23:45:26+00:00",
      "link": "https://arxiv.org/pdf/2602.10371v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10368v1",
      "title": "New Algorithms and Hardness Results for Robust Satisfiability of (Promise) CSPs",
      "abstract": "In this paper, we continue the study of robust satisfiability of promise CSPs (PCSPs), initiated in (Brakensiek, Guruswami, Sandeep, STOC 2023 / Discrete Analysis 2025), and obtain the following results:   For the PCSP 1-in-3-SAT vs NAE-SAT with negations, we prove that it is hard, under the Unique Games conjecture (UGC), to satisfy $1-Ω(1/\\log (1/ε))$ constraints in a $(1-ε)$-satisfiable instance. This shows that the exponential loss incurred by the BGS algorithm for the case of Alternating-Threshold polymorphisms is necessary, in contrast to the polynomial loss achievable for Majority polymorphisms.   For any Boolean PCSP that admits Majority polymorphisms, we give an algorithm satisfying $1-O(\\sqrtε)$ fraction of the weaker constraints when promised the existence of an assignment satisfying $1-ε$ fraction of the stronger constraints. This significantly generalizes the Charikar--Makarychev--Makarychev algorithm for 2-SAT, and matches the optimal trade-off possible under the UGC. The algorithm also extends, with the loss of an extra $\\log (1/ε)$ factor, to PCSPs on larger domains with a certain structural condition, which is implied by, e.g., a family of Plurality polymorphisms.   We prove that assuming the UGC, robust satisfiability is preserved under the addition of equality constraints. As a consequence, we can extend the rich algebraic techniques for decision/search PCSPs to robust PCSPs. The methods involve the development of a correlated and robust version of the general SDP rounding algorithm for CSPs due to (Brown-Cohen, Raghavendra, ICALP 2016), which might be of independent interest.",
      "authors": [
        "Joshua Brakensiek",
        "Lorenzo Ciardo",
        "Venkatesan Guruswami",
        "Aaron Potechin",
        "Stanislav Živný"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS",
        "cs.CC",
        "cs.LO"
      ],
      "published": "2026-02-10 23:40:41+00:00",
      "link": "https://arxiv.org/pdf/2602.10368v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10367v1",
      "title": "LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation",
      "abstract": "The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.",
      "authors": [
        "Zhiling Yan",
        "Dingjie Song",
        "Zhe Fang",
        "Yisheng Ji",
        "Xiang Li",
        "Quanzheng Li",
        "Lichao Sun"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-10 23:38:25+00:00",
      "link": "https://arxiv.org/pdf/2602.10367v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10357v1",
      "title": "Theoretical Analysis of Contrastive Learning under Imbalanced Data: From Training Dynamics to a Pruning Solution",
      "abstract": "Contrastive learning has emerged as a powerful framework for learning generalizable representations, yet its theoretical understanding remains limited, particularly under imbalanced data distributions that are prevalent in real-world applications. Such an imbalance can degrade representation quality and induce biased model behavior, yet a rigorous characterization of these effects is lacking. In this work, we develop a theoretical framework to analyze the training dynamics of contrastive learning with Transformer-based encoders under imbalanced data. Our results reveal that neuron weights evolve through three distinct stages of training, with different dynamics for majority features, minority features, and noise. We further show that minority features reduce representational capacity, increase the need for more complex architectures, and hinder the separation of ground-truth features from noise. Inspired by these neuron-level behaviors, we show that pruning restores performance degraded by imbalance and enhances feature separation, offering both conceptual insights and practical guidance. Major theoretical findings are validated through numerical experiments.",
      "authors": [
        "Haixu Liao",
        "Yating Zhou",
        "Songyang Zhang",
        "Meng Wang",
        "Shuai Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 23:06:12+00:00",
      "link": "https://arxiv.org/pdf/2602.10357v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10352v1",
      "title": "Learning Self-Interpretation from Interpretability Artifacts: Training Lightweight Adapters on Vector-Label Pairs",
      "abstract": "Self-interpretation methods prompt language models to describe their own internal states, but remain unreliable due to hyperparameter sensitivity. We show that training lightweight adapters on interpretability artifacts, while keeping the LM entirely frozen, yields reliable self-interpretation across tasks and model families. A scalar affine adapter with just $d_\\text{model}+1$ parameters suffices: trained adapters generate sparse autoencoder feature labels that outperform the training labels themselves (71% vs 63% generation scoring at 70B scale), identify topics with 94% recall@1 versus 1% for untrained baselines, and decode bridge entities in multi-hop reasoning that appear in neither prompt nor response, surfacing implicit reasoning without chain-of-thought. The learned bias vector alone accounts for 85% of improvement, and simpler adapters generalize better than more expressive alternatives. Controlling for model knowledge via prompted descriptions, we find self-interpretation gains outpace capability gains from 7B to 72B parameters. Our results demonstrate that self-interpretation improves with scale, without modifying the model being interpreted.",
      "authors": [
        "Keenan Pepper",
        "Alex McKenzie",
        "Florin Pop",
        "Stijn Servaes",
        "Martin Leitgab",
        "Mike Vaiana",
        "Judd Rosenblatt",
        "Michael S. A. Graziano",
        "Diogo de Lucena"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-10 22:50:02+00:00",
      "link": "https://arxiv.org/pdf/2602.10352v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11208v1",
      "title": "Adaptive Physics Transformer with Fused Global-Local Attention for Subsurface Energy Systems",
      "abstract": "The Earth's subsurface is a cornerstone of modern society, providing essential energy resources like hydrocarbons, geothermal, and minerals while serving as the primary reservoir for $CO_2$ sequestration. However, full physics numerical simulations of these systems are notoriously computationally expensive due to geological heterogeneity, high resolution requirements, and the tight coupling of physical processes with distinct propagation time scales. Here we propose the \\textbf{Adaptive Physics Transformer} (APT), a geometry-, mesh-, and physics-agnostic neural operator that explicitly addresses these challenges. APT fuses a graph-based encoder to extract high-resolution local heterogeneous features with a global attention mechanism to resolve long-range physical impacts. Our results demonstrate that APT outperforms state-of-the-art architectures in subsurface tasks across both regular and irregular grids with robust super-resolution capabilities. Notably, APT is the first architecture that directly learns from adaptive mesh refinement simulations. We also demonstrate APT's capability for cross-dataset learning, positioning it as a robust and scalable backbone for large-scale subsurface foundation model development.",
      "authors": [
        "Xin Ju",
        "Nok Hei",
        "Fung",
        "Yuyan Zhang",
        "Carl Jacquemyn",
        "Matthew Jackson",
        "Randolph Settgast",
        "Sally M. Benson",
        "Gege Wen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 22:48:58+00:00",
      "link": "https://arxiv.org/pdf/2602.11208v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10346v1",
      "title": "Geometry-Aware Decoding with Wasserstein-Regularized Truncation and Mass Penalties for Large Language Models",
      "abstract": "Large language models (LLMs) must balance diversity and creativity against logical coherence in open-ended generation. Existing truncation-based samplers are effective but largely heuristic, relying mainly on probability mass and entropy while ignoring semantic geometry of the token space. We present Top-W, a geometry-aware truncation rule that uses Wasserstein distance-defined over token-embedding geometry-to keep the cropped distribution close to the original, while explicitly balancing retained probability mass against the entropy of the kept set. Our theory yields a simple closed-form structure for the fixed-potential subset update: depending on the mass-entropy trade-off, the optimal crop either collapses to a single token or takes the form of a one-dimensional prefix that can be found efficiently with a linear scan. We implement Top-W using efficient geometry-based potentials (nearest-set or k-NN) and pair it with an alternating decoding routine that keeps the standard truncation-and-sampling interface unchanged. Extensive experiments on four benchmarks (GSM8K, GPQA, AlpacaEval, and MT-Bench) across three instruction-tuned models show that Top-W consistently outperforms prior state-of-the-art decoding approaches achieving up to 33.7% improvement. Moreover, we find that Top-W not only improves accuracy-focused performance, but also boosts creativity under judge-based open-ended evaluation.",
      "authors": [
        "Arash Gholami Davoodi",
        "Navid Rezazadeh",
        "Seyed Pouyan Mousavi Davoudi",
        "Pouya Pezeshkpour"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-10 22:36:48+00:00",
      "link": "https://arxiv.org/pdf/2602.10346v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10345v1",
      "title": "Identifying Evidence-Based Nudges in Biomedical Literature with Large Language Models",
      "abstract": "We present a scalable, AI-powered system that identifies and extracts evidence-based behavioral nudges from unstructured biomedical literature. Nudges are subtle, non-coercive interventions that influence behavior without limiting choice, showing strong impact on health outcomes like medication adherence. However, identifying these interventions from PubMed's 8 million+ articles is a bottleneck. Our system uses a novel multi-stage pipeline: first, hybrid filtering (keywords, TF-IDF, cosine similarity, and a \"nudge-term bonus\") reduces the corpus to about 81,000 candidates. Second, we use OpenScholar (quantized LLaMA 3.1 8B) to classify papers and extract structured fields like nudge type and target behavior in a single pass, validated against a JSON schema.   We evaluated four configurations on a labeled test set (N=197). The best setup (Title/Abstract/Intro) achieved a 67.0% F1 score and 72.0% recall, ideal for discovery. A high-precision variant using self-consistency (7 randomized passes) achieved 100% precision with 12% recall, demonstrating a tunable trade-off for high-trust use cases. This system is being integrated into Agile Nudge+, a real-world platform, to ground LLM-generated interventions in peer-reviewed evidence. This work demonstrates interpretable, domain-specific retrieval pipelines for evidence synthesis and personalized healthcare.",
      "authors": [
        "Jaydeep Chauhan",
        "Mark Seidman",
        "Pezhman Raeisian Parvari",
        "Zhi Zheng",
        "Zina Ben-Miled",
        "Cristina Barboi",
        "Andrew Gonzalez",
        "Malaz Boustani"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 22:36:07+00:00",
      "link": "https://arxiv.org/pdf/2602.10345v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10330v2",
      "title": "Efficient reduction of stellar contamination and noise in planetary transmission spectra using neural networks",
      "abstract": "Context: JWST has enabled transmission spectroscopy at unprecedented precision, but stellar heterogeneities (spots and faculae) remain a dominant contamination source that can bias atmospheric retrievals if uncorrected. Aims: We present a fast, unsupervised methodology to reduce stellar contamination and instrument-specific noise in exoplanet transmission spectra using denoising autoencoders, improving the reliability of retrieved atmospheric parameters. Methods: We design and train denoising autoencoder architectures on large synthetic datasets of terrestrial (TRAPPIST-1e analogues) and sub-Neptune (K2-18b analogues) planets. Reconstruction quality is evaluated with the $χ^2$ statistic over a wide range of signal-to-noise ratios, and atmospheric retrieval experiments on contaminated spectra are used to compare against standard correction approaches in accuracy and computational cost. Results: The autoencoders reconstruct uncontaminated spectra while preserving key molecular features, even at low S/N. In retrieval tests, pre-processing with denoising autoencoders reduces bias in inferred abundances relative to uncorrected baselines and matches the accuracy of simultaneous stellar-contamination fitting while reducing computational time by a factor of three to six. Conclusions: Denoising autoencoders provide an efficient alternative to conventional correction strategies and are promising components of future atmospheric characterization pipelines for both rocky and gaseous exoplanets.",
      "authors": [
        "David S. Duque-Castaño",
        "Lauren Flor-Torres",
        "Jorge I. Zuluaga"
      ],
      "primary_category": "astro-ph.EP",
      "categories": [
        "astro-ph.EP",
        "astro-ph.IM",
        "cs.LG"
      ],
      "published": "2026-02-10 22:07:18+00:00",
      "link": "https://arxiv.org/pdf/2602.10330v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10329v1",
      "title": "Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality",
      "abstract": "Human reasoning is shaped by resource rationality -- optimizing performance under constraints. Recently, inference-time scaling has emerged as a powerful paradigm to improve the reasoning performance of Large Language Models by expanding test-time computation. Specifically, instruction-tuned (IT) models explicitly generate long reasoning steps during inference, whereas Large Reasoning Models (LRMs) are trained by reinforcement learning to discover reasoning paths that maximize accuracy. However, it remains unclear whether resource-rationality can emerge from such scaling without explicit reward related to computational costs. We introduce a Variable Attribution Task in which models infer which variables determine outcomes given candidate variables, input-output trials, and predefined logical functions. By varying the number of candidate variables and trials, we systematically manipulate task complexity. Both models exhibit a transition from brute-force to analytic strategies as complexity increases. IT models degrade on XOR and XNOR functions, whereas LRMs remain robust. These findings suggest that models can adjust their reasoning behavior in response to task complexity, even without explicit cost-based reward. It provides compelling evidence that resource rationality is an emergent property of inference-time scaling itself.",
      "authors": [
        "Zhimin Hu",
        "Riya Roshan",
        "Sashank Varma"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-10 22:07:05+00:00",
      "link": "https://arxiv.org/pdf/2602.10329v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13322v1",
      "title": "Diagnostic Benchmarks for Invariant Learning Dynamics: Empirical Validation of the Eidos Architecture",
      "abstract": "We present the PolyShapes-Ideal (PSI) dataset, a suite of diagnostic benchmarks designed to isolate topological invariance -- the ability to maintain structural identity across affine transformations -- from the textural correlations that dominate standard vision benchmarks. Through three diagnostic probes (polygon classification under noise, zero-shot font transfer from MNIST, and geometric collapse mapping under progressive deformation), we demonstrate that the Eidos architecture achieves >99% accuracy on PSI and 81.67% zero-shot transfer across 30 unseen typefaces without pre-training. These results validate the \"Form-First\" hypothesis: generalization in structurally constrained architectures is a property of geometric integrity, not statistical scale.",
      "authors": [
        "Datorien L. Anderson"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-10 22:04:44+00:00",
      "link": "https://arxiv.org/pdf/2602.13322v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10321v1",
      "title": "Single-Turn LLM Reformulation Powered Multi-Stage Hybrid Re-Ranking for Tip-of-the-Tongue Known-Item Retrieval",
      "abstract": "Retrieving known items from vague descriptions, Tip-of-the-Tongue (ToT) retrieval, remains a significant challenge. We propose using a single call to a generic 8B-parameter LLM for query reformulation, bridging the gap between ill-formed ToT queries and specific information needs. This method is particularly effective where standard Pseudo-Relevance Feedback fails due to poor initial recall. Crucially, our LLM is not fine-tuned for ToT or specific domains, demonstrating that gains stem from our prompting strategy rather than model specialization. Rewritten queries feed a multi-stage pipeline: sparse retrieval (BM25), dense/late-interaction reranking (Contriever, E5-large-v2, ColBERTv2), monoT5 cross-encoding, and list-wise reranking (Qwen 2.5 72B). Experiments on 2025 TREC-ToT datasets show that while raw queries yield poor performance, our lightweight pre-retrieval transformation improves Recall by 20.61%. Subsequent reranking improves nDCG@10 by 33.88%, MRR by 29.92%, and MAP@10 by 29.98%, offering a cost-effective intervention that unlocks the potential of downstream rankers. Code and data: https://github.com/debayan1405/TREC-TOT-2025",
      "authors": [
        "Debayan Mukhopadhyay",
        "Utshab Kumar Ghosh",
        "Shubham Chatterjee"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-10 21:59:10+00:00",
      "link": "https://arxiv.org/pdf/2602.10321v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10303v1",
      "title": "ICODEN: Ordinary Differential Equation Neural Networks for Interval-Censored Data",
      "abstract": "Predicting time-to-event outcomes when event times are interval censored is challenging because the exact event time is unobserved. Many existing survival analysis approaches for interval-censored data rely on strong model assumptions or cannot handle high-dimensional predictors. We develop ICODEN, an ordinary differential equation-based neural network for interval-censored data that models the hazard function through deep neural networks and obtains the cumulative hazard by solving an ordinary differential equation. ICODEN does not require the proportional hazards assumption or a prespecified parametric form for the hazard function, thereby permitting flexible survival modeling. Across simulation settings with proportional or non-proportional hazards and both linear and nonlinear covariate effects, ICODEN consistently achieves satisfactory predictive accuracy and remains stable as the number of predictors increases. Applications to data from multiple phases of the Alzheimer's Disease Neuroimaging Initiative (ADNI) and to two Age-Related Eye Disease Studies (AREDS and AREDS2) for age-related macular degeneration (AMD) demonstrate ICODEN's robust prediction performance. In both applications, predicting time-to-AD or time-to-late AMD, ICODEN effectively uses hundreds to more than 1,000 SNPs and supports data-driven subgroup identification with differential progression risk profiles. These results establish ICODEN as a practical assumption-lean tool for prediction with interval-censored survival data in high-dimensional biomedical settings.",
      "authors": [
        "Haoling Wang",
        "Lang Zeng",
        "Tao Sun",
        "Youngjoo Cho",
        "Ying Ding"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "q-bio.QM",
        "stat.ML"
      ],
      "published": "2026-02-10 21:18:38+00:00",
      "link": "https://arxiv.org/pdf/2602.10303v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10300v1",
      "title": "Configuration-to-Performance Scaling Law with Neural Ansatz",
      "abstract": "Researchers build scaling laws to forecast the training performance of expensive large-scale runs with larger model size N and data size D. These laws assume that other training hyperparameters are optimally chosen, which can require significant effort and, in some cases, be impossible due to external hardware constraints. To improve predictability across a broader set of hyperparameters and enable simpler tuning at scale, we propose learning a \\textit{Configuration-to-Performance Scaling Law} (CPL): a mapping from the \\textit{full training configuration} to training performance. Because no simple functional form can express this mapping, we parameterize it with a large language model (LLM), and fit it with diverse open-source pretraining logs across multiple sources, yielding a \\textit{Neural} Configuration-to-Performance Scaling Law (NCPL). NCPL accurately predicts how training configurations influence the final pretraining loss, achieving 20-40% lower prediction error than the configuration-agnostic Chinchilla law and generalizing to runs using up to 10 x more compute than any run in the training set. It further supports joint tuning of multiple hyperparameters with performance comparable to hyperparameter scaling law baselines. Finally, NCPL naturally and effectively extends to richer prediction targets such as loss-curve prediction.",
      "authors": [
        "Huaqing Zhang",
        "Kaiyue Wen",
        "Tengyu Ma"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 21:16:59+00:00",
      "link": "https://arxiv.org/pdf/2602.10300v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10295v1",
      "title": "ECHO: An Open Research Platform for Evaluation of Chat, Human Behavior, and Outcomes",
      "abstract": "ECHO (Evaluation of Chat, Human behavior, and Outcomes) is an open research platform designed to support reproducible, mixed-method studies of human interaction with both conversational AI systems and Web search engines. It enables researchers from varying disciplines to orchestrate end-to-end experimental workflows that integrate consent and background surveys, chat-based and search-based information-seeking sessions, writing or judgment tasks, and pre- and post-task evaluations within a unified, low-coding-load framework. ECHO logs fine-grained interaction traces and participant responses, and exports structured datasets for downstream analysis. By supporting both chat and search alongside flexible evaluation instruments, ECHO lowers technical barriers for studying learning, decision making, and user experience across different information access paradigms, empowering researchers from information retrieval, HCI, and the social sciences to conduct scalable and reproducible human-centered AI evaluations.",
      "authors": [
        "Jiqun Liu",
        "Nischal Dinesh",
        "Ran Yu"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2026-02-10 21:10:38+00:00",
      "link": "https://arxiv.org/pdf/2602.10295v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13319v1",
      "title": "Situation Graph Prediction: Structured Perspective Inference for User Modeling",
      "abstract": "Perspective-Aware AI requires modeling evolving internal states--goals, emotions, contexts--not merely preferences. Progress is limited by a data bottleneck: digital footprints are privacy-sensitive and perspective states are rarely labeled. We propose Situation Graph Prediction (SGP), a task that frames perspective modeling as an inverse inference problem: reconstructing structured, ontology-aligned representations of perspective from observable multimodal artifacts. To enable grounding without real labels, we use a structure-first synthetic generation strategy that aligns latent labels and observable traces by design. As a pilot, we construct a dataset and run a diagnostic study using retrieval-augmented in-context learning as a proxy for supervision. In our study with GPT-4o, we observe a gap between surface-level extraction and latent perspective inference--indicating latent-state inference is harder than surface extraction under our controlled setting. Results suggest SGP is non-trivial and provide evidence for the structure-first data synthesis strategy.",
      "authors": [
        "Jisung Shin",
        "Daniel Platnick",
        "Marjan Alirezaie",
        "Hossein Rahnama"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "published": "2026-02-10 20:58:15+00:00",
      "link": "https://arxiv.org/pdf/2602.13319v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10282v1",
      "title": "Linear-LLM-SCM: Benchmarking LLMs for Coefficient Elicitation in Linear-Gaussian Causal Models",
      "abstract": "Large language models (LLMs) have shown potential in identifying qualitative causal relations, but their ability to perform quantitative causal reasoning -- estimating effect sizes that parametrize functional relationships -- remains underexplored in continuous domains. We introduce Linear-LLM-SCM, a plug-and-play benchmarking framework for evaluating LLMs on linear Gaussian structural causal model (SCM) parametrization when the DAG is given. The framework decomposes a DAG into local parent-child sets and prompts an LLM to produce a regression-style structural equation per node, which is aggregated and compared against available ground-truth parameters. Our experiments show several challenges in such benchmarking tasks, namely, strong stochasticity in the results in some of the models and susceptibility to DAG misspecification via spurious edges in the continuous domains. Across models, we observe substantial variability in coefficient estimates for some settings and sensitivity to structural and semantic perturbations, highlighting current limitations of LLMs as quantitative causal parameterizers. We also open-sourced the benchmarking framework so that researchers can utilize their DAGs and any off-the-shelf LLMs plug-and-play for evaluation in their domains effortlessly.",
      "authors": [
        "Kanta Yamaoka",
        "Sumantrak Mukherjee",
        "Thomas Gärtner",
        "David Antony Selby",
        "Stefan Konigorski",
        "Eyke Hüllermeier",
        "Viktor Bengs",
        "Sebastian Josef Vollmer"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 20:49:01+00:00",
      "link": "https://arxiv.org/pdf/2602.10282v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10271v3",
      "title": "MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation",
      "abstract": "Understanding multimodal long-context documents that comprise multimodal chunks such as paragraphs, figures, and tables is challenging due to (1) cross-modal heterogeneity to localize relevant information across modalities, (2) cross-page reasoning to aggregate dispersed evidence across pages. To address these challenges, we are motivated to adopt a query-centric formulation that projects cross-modal and cross-page information into a unified query representation space, with queries acting as abstract semantic surrogates for heterogeneous multimodal content. In this paper, we propose a Multimodal Long-Context Document Retrieval Augmented Generation (MLDocRAG) framework that leverages a Multimodal Chunk-Query Graph (MCQG) to organize multimodal document content around semantically rich, answerable queries. MCQG is constructed via a multimodal document expansion process that generates fine-grained queries from heterogeneous document chunks and links them to their corresponding content across modalities and pages. This graph-based structure enables selective, query-centric retrieval and structured evidence aggregation, thereby enhancing grounding and coherence in multimodal long-context question answering. Experiments on datasets MMLongBench-Doc and LongDocURL demonstrate that MLDocRAG consistently improves retrieval quality and answer accuracy, demonstrating its effectiveness for multimodal long-context understanding.",
      "authors": [
        "Yongyue Zhang",
        "Yaxiong Wu"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "published": "2026-02-10 20:29:10+00:00",
      "link": "https://arxiv.org/pdf/2602.10271v3",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10258v1",
      "title": "JAG: Joint Attribute Graphs for Filtered Nearest Neighbor Search",
      "abstract": "Despite filtered nearest neighbor search being a fundamental task in modern vector search systems, the performance of existing algorithms is highly sensitive to query selectivity and filter type. In particular, existing solutions excel either at specific filter categories (e.g., label equality) or within narrow selectivity bands (e.g., pre-filtering for low selectivity) and are therefore a poor fit for practical deployments that demand generalization to new filter types and unknown query selectivities. In this paper, we propose JAG (Joint Attribute Graphs), a graph-based algorithm designed to deliver robust performance across the entire selectivity spectrum and support diverse filter types. Our key innovation is the introduction of attribute and filter distances, which transform binary filter constraints into continuous navigational guidance. By constructing a proximity graph that jointly optimizes for both vector similarity and attribute proximity, JAG prevents navigational dead-ends and allows JAG to consistently outperform prior graph-based filtered nearest neighbor search methods. Our experimental results across five datasets and four filter types (Label, Range, Subset, Boolean) demonstrate that JAG significantly outperforms existing state-of-the-art baselines in both throughput and recall robustness.",
      "authors": [
        "Haike Xu",
        "Guy Blelloch",
        "Laxman Dhulipala",
        "Lars Gottesbüren",
        "Rajesh Jayaram",
        "Jakub Łącki"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.DB"
      ],
      "published": "2026-02-10 20:00:51+00:00",
      "link": "https://arxiv.org/pdf/2602.10258v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10253v1",
      "title": "The Complexity of Bayesian Network Learning: Revisiting the Superstructure",
      "abstract": "We investigate the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. We follow up on previous works that have analyzed the complexity of BNSL w.r.t. the so-called superstructure of the input. While known results imply that BNSL is unlikely to be fixed-parameter tractable even when parameterized by the size of a vertex cover in the superstructure, here we show that a different kind of parameterization - notably by the size of a feedback edge set - yields fixed-parameter tractability. We proceed by showing that this result can be strengthened to a localized version of the feedback edge set, and provide corresponding lower bounds that complement previous results to provide a complexity classification of BNSL w.r.t. virtually all well-studied graph parameters.   We then analyze how the complexity of BNSL depends on the representation of the input. In particular, while the bulk of past theoretical work on the topic assumed the use of the so-called non-zero representation, here we prove that if an additive representation can be used instead then BNSL becomes fixed-parameter tractable even under significantly milder restrictions to the superstructure, notably when parameterized by the treewidth alone. Last but not least, we show how our results can be extended to the closely related problem of Polytree Learning.",
      "authors": [
        "Robert Ganian",
        "Viktoriia Korchemna"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "published": "2026-02-10 19:58:24+00:00",
      "link": "https://arxiv.org/pdf/2602.10253v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10249v1",
      "title": "Modeling Programming Skills with Source Code Embeddings for Context-aware Exercise Recommendation",
      "abstract": "In this paper, we propose a context-aware recommender system that models students' programming skills using embeddings of the source code they submit throughout a course. These embeddings predict students' skills across multiple programming topics, producing profiles that are matched to the skills required by unseen homework problems. To generate recommendations, we compute the cosine similarity between student profiles and problem skill vectors, ranking exercises according to their alignment with each student's current abilities. We evaluated our approach using real data from students and exercises in an introductory programming course at our university. First, we assessed the effectiveness of our source code embeddings for predicting skills, comparing them with token-based and graph-based alternatives. Results showed that Jina embeddings outperformed TF-IDF, CodeBERT-cpp, and GraphCodeBERT across most skills. Additionally, we evaluated the system's ability to recommend exercises aligned with weekly course content by analyzing student submissions collected over seven course offerings. Our approach consistently produced more suitable recommendations than baselines based on correctness or solution time, indicating that predicted programming skills provide a stronger signal for problem recommendation.",
      "authors": [
        "Carlos Eduardo P. Silva",
        "João Pedro M. Sena",
        "Julio C. S. Reis",
        "André G. Santos",
        "Lucas N. Ferreira"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 19:51:48+00:00",
      "link": "https://arxiv.org/pdf/2602.10249v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13318v1",
      "title": "DECKBench: Benchmarking Multi-Agent Frameworks for Academic Slide Generation and Editing",
      "abstract": "Automatically generating and iteratively editing academic slide decks requires more than document summarization. It demands faithful content selection, coherent slide organization, layout-aware rendering, and robust multi-turn instruction following. However, existing benchmarks and evaluation protocols do not adequately measure these challenges. To address this gap, we introduce the Deck Edits and Compliance Kit Benchmark (DECKBench), an evaluation framework for multi-agent slide generation and editing. DECKBench is built on a curated dataset of paper to slide pairs augmented with realistic, simulated editing instructions. Our evaluation protocol systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and multi-turn instruction following. We further implement a modular multi-agent baseline system that decomposes the slide generation and editing task into paper parsing and summarization, slide planning, HTML creation, and iterative editing. Experimental results demonstrate that the proposed benchmark highlights strengths, exposes failure modes, and provides actionable insights for improving multi-agent slide generation and editing systems. Overall, this work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing. Code and data are publicly available at https://github.com/morgan-heisler/DeckBench .",
      "authors": [
        "Daesik Jang",
        "Morgan Lindsay Heisler",
        "Linzi Xing",
        "Yifei Li",
        "Edward Wang",
        "Ying Xiong",
        "Yong Zhang",
        "Zhenan Fan"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-10 19:49:06+00:00",
      "link": "https://arxiv.org/pdf/2602.13318v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10238v1",
      "title": "Learning to Evict from Key-Value Cache",
      "abstract": "The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve only as indirect proxies for a token's future utility and introduce computational overhead. We reframe KV cache eviction as a reinforcement learning (RL) problem: learning to rank tokens by their predicted usefulness for future decoding. To this end, we introduce KV Policy (KVP), a framework of lightweight per-head RL agents trained on pre-computed generation traces using only key and value vectors. Each agent learns a specialized eviction policy guided by future utility, which evaluates the quality of the ranking across all cache budgets, requiring no modifications to the underlying LLM or additional inference. Evaluated across two different model families on the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly outperforms baselines. Furthermore, zero-shot tests on standard downstream tasks (e.g., LongBench, BOOLQ, ARC) indicate that KVP generalizes well beyond its training distribution and to longer context lengths. These results demonstrate that learning to predict future token utility is a powerful and scalable paradigm for adaptive KV cache management.",
      "authors": [
        "Luca Moschella",
        "Laura Manduchi",
        "Ozan Sener"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-10 19:34:15+00:00",
      "link": "https://arxiv.org/pdf/2602.10238v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10235v1",
      "title": "Reimagining Sign Language Technologies: Analyzing Translation Work of Chinese Deaf Online Content Creators",
      "abstract": "While sign language translation systems promise to enhance deaf people's access to information and communication, they have been met with strong skepticism from deaf communities due to risks of misrepresenting and oversimplifying the richness of signed communication in technologies. This article provides empirical evidence of the complexity of translation work involved in deaf communication through interviews with 13 deaf Chinese content creators who actively produce and share sign language content on video sharing platforms with both deaf and hearing audiences. By studying this unique group of content creators, our findings highlight the nuances of sign language translation, showing how deaf creators create content with multilingualism and multiculturalism in mind, support meaning making across languages and cultures, and navigate politics involved in their translation work. Grounded in these deaf-led translation practices, we draw on the sociolinguistic concept of (trans)languaging to re-conceptualize and reimagine the design of sign language translation systems.",
      "authors": [
        "Xinru Tang",
        "Anne Marie Piper"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-10 19:27:06+00:00",
      "link": "https://arxiv.org/pdf/2602.10235v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10233v1",
      "title": "ImprovEvolve: Ask AlphaEvolve to Improve the Input Solution and Then Improvise",
      "abstract": "Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve, have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. In this article, we present ImprovEvolve, a simple yet effective technique for enhancing LLM-based evolutionary approaches such as AlphaEvolve. Given an optimization problem, the standard approach is to evolve program code that, when executed, produces a solution close to the optimum. We propose an alternative program parameterization that maintains the ability to construct optimal solutions while reducing the cognitive load on the LLM. Specifically, we evolve a program (implementing, e.g., a Python class with a prescribed interface) that provides the following functionality: (1) propose a valid initial solution, (2) improve any given solution in terms of fitness, and (3) perturb a solution with a specified intensity. The optimum can then be approached by iteratively applying improve() and perturb() with a scheduled intensity. We evaluate ImprovEvolve on challenging problems from the AlphaEvolve paper: hexagon packing in a hexagon and the second autocorrelation inequality. For hexagon packing, the evolved program achieves new state-of-the-art results for 11, 12, 15, and 16 hexagons; a lightly human-edited variant further improves results for 14, 17, and 23 hexagons. For the second autocorrelation inequality, the human-edited program achieves a new state-of-the-art lower bound of 0.96258, improving upon AlphaEvolve's 0.96102.",
      "authors": [
        "Alexey Kravatskiy",
        "Valentin Khrulkov",
        "Ivan Oseledets"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.AI",
        "math.CA",
        "math.MG",
        "math.OC"
      ],
      "published": "2026-02-10 19:23:13+00:00",
      "link": "https://arxiv.org/pdf/2602.10233v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10231v1",
      "title": "Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards",
      "abstract": "Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts.",
      "authors": [
        "Kirill Pavlenko",
        "Alexander Golubev",
        "Simon Karasik",
        "Boris Yangel"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-10 19:22:37+00:00",
      "link": "https://arxiv.org/pdf/2602.10231v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10228v1",
      "title": "PRISM: Differentially Private Synthetic Data with Structure-Aware Budget Allocation for Prediction",
      "abstract": "Differential privacy (DP) provides a mathematical guarantee limiting what an adversary can learn about any individual from released data. However, achieving this protection typically requires adding noise, and noise can accumulate when many statistics are measured. Existing DP synthetic data methods treat all features symmetrically, spreading noise uniformly even when the data will serve a specific prediction task.   We develop a prediction-centric approach operating in three regimes depending on available structural knowledge. In the causal regime, when the causal parents of $Y$ are known and distribution shift is expected, we target the parents for robustness. In the graphical regime, when a Bayesian network structure is available and the distribution is stable, the Markov blanket of $Y$ provides a sufficient feature set for optimal prediction. In the predictive regime, when no structural knowledge exists, we select features via differentially private methods without claiming to recover causal or graphical structure.   We formalize this as PRISM, a mechanism that (i) identifies a predictive feature subset according to the appropriate regime, (ii) constructs targeted summary statistics, (iii) allocates budget to minimize an upper bound on prediction error, and (iv) synthesizes data via graphical-model inference. We prove end-to-end privacy guarantees and risk bounds. Empirically, task-aware allocation improves prediction accuracy compared to generic synthesizers. Under distribution shift, targeting causal parents achieves AUC $\\approx 0.73$ while correlation-based selection collapses to chance ($\\approx 0.49$).",
      "authors": [
        "Amir Asiaee",
        "Chao Yan",
        "Zachary B. Abrams",
        "Bradley A. Malin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 19:17:55+00:00",
      "link": "https://arxiv.org/pdf/2602.10228v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10226v1",
      "title": "Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents",
      "abstract": "Optimizing large-scale machine learning systems, such as recommendation models for global video platforms, requires navigating a massive hyperparameter search space and, more critically, designing sophisticated optimizers, architectures, and reward functions to capture nuanced user behaviors. Achieving substantial improvements in these areas is a non-trivial task, traditionally relying on extensive manual iterations to test new hypotheses. We propose a self-evolving system that leverages Large Language Models (LLMs), specifically those from Google's Gemini family, to autonomously generate, train, and deploy high-performing, complex model changes within an end-to-end automated workflow. The self-evolving system is comprised of an Offline Agent (Inner Loop) that performs high-throughput hypothesis generation using proxy metrics, and an Online Agent (Outer Loop) that validates candidates against delayed north star business metrics in live production. Our agents act as specialized Machine Learning Engineers (MLEs): they exhibit deep reasoning capabilities, discovering novel improvements in optimization algorithms and model architecture, and formulating innovative reward functions that target long-term user engagement. The effectiveness of this approach is demonstrated through several successful production launches at YouTube, confirming that autonomous, LLM-driven evolution can surpass traditional engineering workflows in both development velocity and model performance.",
      "authors": [
        "Haochen Wang",
        "Yi Wu",
        "Daryl Chang",
        "Li Wei",
        "Lukasz Heldt"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-10 19:16:52+00:00",
      "link": "https://arxiv.org/pdf/2602.10226v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10217v1",
      "title": "Temper-Then-Tilt: Principled Unlearning for Generative Models through Tempering and Classifier Guidance",
      "abstract": "We study machine unlearning in large generative models by framing the task as density ratio estimation to a target distribution rather than supervised fine-tuning. While classifier guidance is a standard approach for approximating this ratio and can succeed in general, we show it can fail to faithfully unlearn with finite samples when the forget set represents a sharp, concentrated data distribution. To address this, we introduce Temper-Then-Tilt Unlearning (T3-Unlearning), which freezes the base model and applies a two-step inference procedure: (i) tempering the base distribution to flatten high-confidence spikes, and (ii) tilting the tempered distribution using a lightweight classifier trained to distinguish retain from forget samples. Our theoretical analysis provides finite-sample guarantees linking the surrogate classifier's risk to unlearning error, proving that tempering is necessary to successfully unlearn for concentrated distributions. Empirical evaluations on the TOFU benchmark show that T3-Unlearning improves forget quality and generative utility over existing baselines, while training only a fraction of the parameters with a minimal runtime.",
      "authors": [
        "Jacob L. Block",
        "Mehryar Mohri",
        "Aryan Mokhtari",
        "Sanjay Shakkottai"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 19:08:40+00:00",
      "link": "https://arxiv.org/pdf/2602.10217v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10212v1",
      "title": "Rank-Accuracy Trade-off for LoRA: A Gradient-Flow Analysis",
      "abstract": "Previous empirical studies have shown that LoRA achieves accuracy comparable to full-parameter methods on downstream fine-tuning tasks, even for rank-1 updates. By contrast, the theoretical underpinnings of the dependence of LoRA's accuracy on update rank remain relatively unexplored. In this work, we compare the accuracy of rank-r LoRA updates against full-parameter updates for fine-tuning tasks from a dynamical systems perspective. We perform gradient flow analysis in both full-rank and low-rank regimes to establish explicit relationships between rank and accuracy for two loss functions under LoRA. While gradient flow equations for LoRA are presented in prior work, we rigorously derive their form and show that they are identical for simultaneous and sequential LoRA parameter updates. We then use the resulting dynamical system equations to obtain closed-form relationships between LoRA rank and accuracy for trace-squared and Frobenius-norm low-rank approximation loss functions.",
      "authors": [
        "Michael Rushka",
        "Diego Klabjan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 19:05:27+00:00",
      "link": "https://arxiv.org/pdf/2602.10212v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10210v1",
      "title": "How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge",
      "abstract": "Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.",
      "authors": [
        "Junhong Lin",
        "Bing Zhang",
        "Song Wang",
        "Ziyan Liu",
        "Dan Gutfreund",
        "Julian Shun",
        "Yada Zhu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 19:04:01+00:00",
      "link": "https://arxiv.org/pdf/2602.10210v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10204v1",
      "title": "Adaptive Optimization via Momentum on Variance-Normalized Gradients",
      "abstract": "We introduce MVN-Grad (Momentum on Variance-Normalized Gradients), an Adam-style optimizer that improves stability and performance by combining two complementary ideas: variance-based normalization and momentum applied after normalization. MVN-Grad scales each coordinate by an exponential moving average of gradient uncertainty and applies momentum to the resulting normalized gradients, eliminating the cross-time coupling between stale momentum and a stochastic normalizer present in standard Adam-type updates. We prove that this decoupling yields strictly smaller one-step conditional update variance than momentum-then-normalize variance methods under standard noise assumptions, and that MVN-Grad is robust to outliers: it has a uniformly bounded response to single gradient spikes.   In low-variance regimes, we further show variance normalization avoids sign-type collapse associated with second-moment scaling and can yield accelerated convergence. Across CIFAR-100 image classification and GPT-style language modeling benchmarks, MVN-Grad matches or outperforms Adam, AdaBelief, and LaProp, delivering smoother training and improved generalization with no added overhead.",
      "authors": [
        "Francisco Patitucci",
        "Aryan Mokhtari"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-02-10 19:00:25+00:00",
      "link": "https://arxiv.org/pdf/2602.10204v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10195v1",
      "title": "Versor: A Geometric Sequence Architecture",
      "abstract": "A novel sequence architecture design is introduced, Versor, which uses Conformal Geometric Algebra (CGA) in place of the traditional fundamental non-linear operations to achieve structural generalization and significant performance improvements on a variety of tasks, while offering improved interpretability and efficiency. By embedding states in the $Cl_{4,1}$ manifold and evolving them via geometric transformations (rotors), Versor natively represents $SE(3)$-equivariant relationships without requiring explicit structural encoding. Versor is validated on chaotic N-body dynamics, topological reasoning, and standard multimodal benchmarks (CIFAR-10, WikiText-103), consistently outperforming Transformers, Graph Networks, and geometric baselines (GATr, EGNN). Key results include: orders of magnitude fewer parameters ($200\\times$ vs. Transformers); interpretable attention decomposing into proximity and orientational components; zero-shot scale generalization (99.3% MCC on topology vs. 50.4% for ViT); and $O(L)$ linear complexity via the novel Recursive Rotor Accumulator. In out-of-distribution tests, Versor maintains stable predictions while Transformers fail catastrophically. Custom Clifford kernels achieve up to $78\\times$ speedup, providing a scalable foundation for geometrically-aware scientific modeling.",
      "authors": [
        "Truong Minh Huy",
        "Edward Hirst"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "hep-th"
      ],
      "published": "2026-02-10 19:00:02+00:00",
      "link": "https://arxiv.org/pdf/2602.10195v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10182v1",
      "title": "Signature-Kernel Based Evaluation Metrics for Robust Probabilistic and Tail-Event Forecasting",
      "abstract": "Probabilistic forecasting is increasingly critical across high-stakes domains, from finance and epidemiology to climate science. However, current evaluation frameworks lack a consensus metric and suffer from two critical flaws: they often assume independence across time steps or variables, and they demonstrably lack sensitivity to tail events, the very occurrences that are most pivotal in real-world decision-making. To address these limitations, we propose two kernel-based metrics: the signature maximum mean discrepancy (Sig-MMD) and our novel censored Sig-MMD (CSig-MMD). By leveraging the signature kernel, these metrics capture complex inter-variate and inter-temporal dependencies and remain robust to missing data. Furthermore, CSig-MMD introduces a censoring scheme that prioritizes a forecaster's capability to predict tail events while strictly maintaining properness, a vital property for a good scoring rule. These metrics enable a more reliable evaluation of direct multi-step forecasting, facilitating the development of more robust probabilistic algorithms.",
      "authors": [
        "Benjamin R. Redhead",
        "Thomas L. Lee",
        "Peng Gu",
        "Víctor Elvira",
        "Amos Storkey"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-10 19:00:00+00:00",
      "link": "https://arxiv.org/pdf/2602.10182v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10097v1",
      "title": "Step-resolved data attribution for looped transformers",
      "abstract": "We study how individual training examples shape the internal computation of looped transformers, where a shared block is applied for $τ$ recurrent iterations to enable latent reasoning. Existing training-data influence estimators such as TracIn yield a single scalar score that aggregates over all loop iterations, obscuring when during the recurrent computation a training example matters. We introduce \\textit{Step-Decomposed Influence (SDI)}, which decomposes TracIn into a length-$τ$ influence trajectory by unrolling the recurrent computation graph and attributing influence to specific loop iterations. To make SDI practical at transformer scale, we propose a TensorSketch implementation that never materialises per-example gradients. Experiments on looped GPT-style models and algorithmic reasoning tasks show that SDI scales excellently, matches full-gradient baselines with low error and supports a broad range of data attribution and interpretability tasks with per-step insights into the latent reasoning process.",
      "authors": [
        "Georgios Kaissis",
        "David Mildenberger",
        "Juan Felipe Gomez",
        "Martin J. Menten",
        "Eleni Triantafillou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-10 18:57:53+00:00",
      "link": "https://arxiv.org/pdf/2602.10097v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10085v2",
      "title": "CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs",
      "abstract": "Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos at https://sites.google.com/view/code-sharp/homepage.",
      "authors": [
        "Richard Bornemann",
        "Pierluigi Vito Amadori",
        "Antoine Cully"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-10 18:51:39+00:00",
      "link": "https://arxiv.org/pdf/2602.10085v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10081v2",
      "title": "Anagent For Enhancing Scientific Table & Figure Analysis",
      "abstract": "In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \\& figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \\& figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 9 broad domains with 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\\uparrow 13.43\\%$ in training-free settings and $\\uparrow 42.12\\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \\& figure analysis. Our project page: https://xhguo7.github.io/Anagent/.",
      "authors": [
        "Xuehang Guo",
        "Zhiyong Lu",
        "Tom Hope",
        "Qingyun Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-10 18:46:28+00:00",
      "link": "https://arxiv.org/pdf/2602.10081v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10074v1",
      "title": "CAPID: Context-Aware PII Detection for Question-Answering Systems",
      "abstract": "Detecting personally identifiable information (PII) in user queries is critical for ensuring privacy in question-answering systems. Current approaches mainly redact all PII, disregarding the fact that some of them may be contextually relevant to the user's question, resulting in a degradation of response quality. Large language models (LLMs) might be able to help determine which PII are relevant, but due to their closed source nature and lack of privacy guarantees, they are unsuitable for sensitive data processing. To achieve privacy-preserving PII detection, we propose CAPID, a practical approach that fine-tunes a locally owned small language model (SLM) that filters sensitive information before it is passed to LLMs for QA. However, existing datasets do not capture the context-dependent relevance of PII needed to train such a model effectively. To fill this gap, we propose a synthetic data generation pipeline that leverages LLMs to produce a diverse, domain-rich dataset spanning multiple PII types and relevance levels. Using this dataset, we fine-tune an SLM to detect PII spans, classify their types, and estimate contextual relevance. Our experiments show that relevance-aware PII detection with a fine-tuned SLM substantially outperforms existing baselines in span, relevance and type accuracy while preserving significantly higher downstream utility under anonymization.",
      "authors": [
        "Mariia Ponomarenko",
        "Sepideh Abedini",
        "Masoumeh Shafieinejad",
        "D. B. Emerson",
        "Shubhankar Mohapatra",
        "Xi He"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "published": "2026-02-10 18:41:31+00:00",
      "link": "https://arxiv.org/pdf/2602.10074v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10067v3",
      "title": "Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability",
      "abstract": "Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model (when run in tandem with our probing harness), while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.",
      "authors": [
        "Aaditya Vikram Prasad",
        "Connor Watts",
        "Jack Merullo",
        "Dhruvil Gala",
        "Owen Lewis",
        "Thomas McGrath",
        "Ekdeep Singh Lubana"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 18:33:45+00:00",
      "link": "https://arxiv.org/pdf/2602.10067v3",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10056v1",
      "title": "WildCat: Near-Linear Attention in Theory and Practice",
      "abstract": "We introduce WildCat, a high-accuracy, low-cost approach to compressing the attention mechanism in neural networks. While attention is a staple of modern network architectures, it is also notoriously expensive to deploy due to resource requirements that scale quadratically with the input sequence length $n$. WildCat avoids these quadratic costs by only attending over a small weighted coreset. Crucially, we select the coreset using a fast but spectrally-accurate subsampling algorithm -- randomly pivoted Cholesky -- and weight the elements optimally to minimise reconstruction error. Remarkably, given bounded inputs, WildCat approximates exact attention with super-polynomial $O(n^{-\\sqrt{\\log(\\log(n))}})$ error decay while running in near-linear $O(n^{1+o(1)})$ time. In contrast, prior practical approximations either lack error guarantees or require quadratic runtime to guarantee such high fidelity. We couple this advance with a GPU-optimized PyTorch implementation and a suite of benchmark experiments demonstrating the benefits of WildCat for image generation, image classification, and language model KV cache compression.",
      "authors": [
        "Tobias Schröder",
        "Lester Mackey"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-10 18:22:32+00:00",
      "link": "https://arxiv.org/pdf/2602.10056v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10048v1",
      "title": "Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization",
      "abstract": "Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \\textbf{F}ine-grained \\textbf{G}roup policy \\textbf{O}ptimization (\\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two major limitations of the GRPO: inefficient data utilization and entropy collapse. We evaluate FGO on multiple reasoning LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Experimental results show that FGO achieves efficient CoT compression without degrading performance, and simultaneously resolves the key limitations of GRPO.",
      "authors": [
        "Xinchen Han",
        "Hossam Afifi",
        "Michel Marot",
        "Xilu Wang",
        "Lu Yin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-10 18:15:58+00:00",
      "link": "https://arxiv.org/pdf/2602.10048v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10037v1",
      "title": "Effectiveness of Binary Autoencoders for QUBO-Based Optimization Problems",
      "abstract": "In black-box combinatorial optimization, objective evaluations are often expensive, so high quality solutions must be found under a limited budget. Factorization machine with quantum annealing (FMQA) builds a quadratic surrogate model from evaluated samples and optimizes it on an Ising machine. However, FMQA requires binary decision variables, and for nonbinary structures such as integer permutations, the choice of binary encoding strongly affects search efficiency. If the encoding fails to reflect the original neighborhood structure, small Hamming moves may not correspond to meaningful modifications in the original solution space, and constrained problems can yield many infeasible candidates that waste evaluations. Recent work combines FMQA with a binary autoencoder (bAE) that learns a compact binary latent code from feasible solutions, yet the mechanism behind its performance gains is unclear. Using a small traveling salesman problem as an interpretable testbed, we show that the bAE reconstructs feasible tours accurately and, compared with manually designed encodings at similar compression, better aligns tour distances with latent Hamming distances, yields smoother neighborhoods under small bit flips, and produces fewer local optima. These geometric properties explain why bAE+FMQA improves the approximation ratio faster while maintaining feasibility throughout optimization, and they provide guidance for designing latent representations for black-box optimization.",
      "authors": [
        "Tetsuro Abe",
        "Masashi Yamashita",
        "Shu Tanaka"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.stat-mech",
        "quant-ph"
      ],
      "published": "2026-02-10 17:59:29+00:00",
      "link": "https://arxiv.org/pdf/2602.10037v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10031v1",
      "title": "Position: Message-passing and spectral GNNs are two sides of the same coin",
      "abstract": "Graph neural networks (GNNs) are commonly divided into message-passing neural networks (MPNNs) and spectral graph neural networks, reflecting two largely separate research traditions in machine learning and signal processing. This paper argues that this divide is mostly artificial, hindering progress in the field. We propose a viewpoint in which both MPNNs and spectral GNNs are understood as different parametrizations of permutation-equivariant operators acting on graph signals. From this perspective, many popular architectures are equivalent in expressive power, while genuine gaps arise only in specific regimes. We further argue that MPNNs and spectral GNNs offer complementary strengths. That is, MPNNs provide a natural language for discrete structure and expressivity analysis using tools from logic and graph isomorphism research, while the spectral perspective provides principled tools for understanding smoothing, bottlenecks, stability, and community structure. Overall, we posit that progress in graph learning will be accelerated by clearly understanding the key similarities and differences between these two types of GNNs, and by working towards unifying these perspectives within a common theoretical and conceptual framework rather than treating them as competing paradigms.",
      "authors": [
        "Antonis Vasileiou",
        "Juan Cervino",
        "Pascal Frossard",
        "Charilaos I. Kanatsoulis",
        "Christopher Morris",
        "Michael T. Schaub",
        "Pierre Vandergheynst",
        "Zhiyang Wang",
        "Guy Wolf",
        "Ron Levie"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 17:53:40+00:00",
      "link": "https://arxiv.org/pdf/2602.10031v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10024v1",
      "title": "Overview of the TREC 2025 RAGTIME Track",
      "abstract": "The principal goal of the RAG TREC Instrument for Multilingual Evaluation (RAGTIME) track at TREC is to study report generation from multilingual source documents. The track has created a document collection containing Arabic, Chinese, English, and Russian news stories. RAGTIME includes three task types: Multilingual Report Generation, English Report Generation, and Multilingual Information Retrieval (MLIR). A total of 125 runs were submitted by 13 participating teams (and as baselines by the track coordinators) for three tasks. This overview describes these three tasks and presents the available results.",
      "authors": [
        "Dawn Lawrie",
        "Sean MacAvaney",
        "James Mayfield",
        "Luca Soldaini",
        "Eugene Yang",
        "Andrew Yates"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "published": "2026-02-10 17:47:20+00:00",
      "link": "https://arxiv.org/pdf/2602.10024v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10023v1",
      "title": "MEVER: Multi-Modal and Explainable Claim Verification with Graph-based Evidence Retrieval",
      "abstract": "Verifying the truthfulness of claims usually requires joint multi-modal reasoning over both textual and visual evidence, such as analyzing both textual caption and chart image for claim verification. In addition, to make the reasoning process transparent, a textual explanation is necessary to justify the verification result. However, most claim verification works mainly focus on the reasoning over textual evidence only or ignore the explainability, resulting in inaccurate and unconvincing verification. To address this problem, we propose a novel model that jointly achieves evidence retrieval, multi-modal claim verification, and explanation generation. For evidence retrieval, we construct a two-layer multi-modal graph for claims and evidence, where we design image-to-text and text-to-image reasoning for multi-modal retrieval. For claim verification, we propose token- and evidence-level fusion to integrate claim and evidence embeddings for multi-modal verification. For explanation generation, we introduce multi-modal Fusion-in-Decoder for explainability. Finally, since almost all the datasets are in general domain, we create a scientific dataset, AIChartClaim, in AI domain to complement claim verification community. Experiments show the strength of our model.",
      "authors": [
        "Delvin Ce Zhang",
        "Suhan Cui",
        "Zhelin Chu",
        "Xianren Zhang",
        "Dongwon Lee"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 17:44:57+00:00",
      "link": "https://arxiv.org/pdf/2602.10023v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10021v1",
      "title": "Decoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference",
      "abstract": "The integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of catastrophic forgetting. In this paper, we propose DRIFT, a novel dual-model architecture designed to explicitly decouple knowledge extraction from the reasoning process. Unlike static prompt compression, DRIFT employs a lightweight knowledge model to dynamically compress document chunks into implicit fact tokens conditioned on the query. These dense representations are projected into the reasoning model's embedding space, replacing raw, redundant text while maintaining inference accuracy. Extensive experiments show that DRIFT significantly improves performance on long-context tasks, outperforming strong baselines among comparably sized models. Our approach provides a scalable and efficient paradigm for extending the effective context window and reasoning capabilities of LLMs. Our code is available at https://github.com/Lancelot-Xie/DRIFT.",
      "authors": [
        "Wenxuan Xie",
        "Yujia Wang",
        "Xin Tan",
        "Chaochao Lu",
        "Xia Hu",
        "Xuhong Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-10 17:42:31+00:00",
      "link": "https://arxiv.org/pdf/2602.10021v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10017v1",
      "title": "SCORE: Specificity, Context Utilization, Robustness, and Relevance for Reference-Free LLM Evaluation",
      "abstract": "Large language models (LLMs) are increasingly used to support question answering and decision-making in high-stakes, domain-specific settings such as natural hazard response and infrastructure planning, where effective answers must convey fine-grained, decision-critical details. However, existing evaluation frameworks for retrieval-augmented generation (RAG) and open-ended question answering primarily rely on surface-level similarity, factual consistency, or semantic relevance, and often fail to assess whether responses provide the specific information required for domain-sensitive decisions. To address this gap, we propose a multi-dimensional, reference-free evaluation framework that assesses LLM outputs along four complementary dimensions: specificity, robustness to paraphrasing and semantic perturbations, answer relevance, and context utilization. We introduce a curated dataset of 1,412 domain-specific question-answer pairs spanning 40 professional roles and seven natural hazard types to support systematic evaluation. We further conduct human evaluation to assess inter-annotator agreement and alignment between model outputs and human judgments, which highlights the inherent subjectivity of open-ended, domain-specific evaluation. Our results show that no single metric sufficiently captures answer quality in isolation and demonstrate the need for structured, multi-metric evaluation frameworks when deploying LLMs in high-stakes applications.",
      "authors": [
        "Homaira Huda Shomee",
        "Rochana Chaturvedi",
        "Yangxinyu Xie",
        "Tanwi Mallick"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 17:39:17+00:00",
      "link": "https://arxiv.org/pdf/2602.10017v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10015v2",
      "title": "RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments",
      "abstract": "Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.",
      "authors": [
        "Dharmendra Sharma",
        "Archit Sharma",
        "John Rebeiro",
        "Vaibhav Kesharwani",
        "Peeyush Thakur",
        "Narendra Kumar Dhar",
        "Laxmidhar Behera"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2026-02-10 17:37:35+00:00",
      "link": "https://arxiv.org/pdf/2602.10015v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10014v1",
      "title": "A Task-Centric Theory for Iterative Self-Improvement with Easy-to-Hard Curricula",
      "abstract": "Iterative self-improvement fine-tunes an autoregressive large language model (LLM) on reward-verified outputs generated by the LLM itself. In contrast to the empirical success of self-improvement, the theoretical foundation of this generative, iterative procedure in a practical, finite-sample setting remains limited. We make progress toward this goal by modeling each round of self-improvement as maximum-likelihood fine-tuning on a reward-filtered distribution and deriving finite-sample guarantees for the expected reward. Our analysis reveals an explicit feedback loop where better models accept more data per iteration, supporting sustained self-improvement while explaining eventual saturation of such improvement. Adopting a task-centric view by considering reasoning tasks with multiple difficulty levels, we further prove quantifiable conditions on model initialization, task difficulty, and sample budget where easy-to-hard curricula provably achieve better guarantees than training on fixed mixtures of tasks. Our analyses are validated via Monte-Carlo simulations and controlled experiments on graph-based reasoning tasks.",
      "authors": [
        "Chenruo Liu",
        "Yijun Dong",
        "Yiqiu Shen",
        "Qi Lei"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-10 17:36:41+00:00",
      "link": "https://arxiv.org/pdf/2602.10014v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10009v1",
      "title": "Discovering High Level Patterns from Simulation Traces",
      "abstract": "Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics. The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data. In this paper, we propose a natural language guided method to discover coarse-grained patterns (e.g., 'rigid-body collision', 'stable support', etc.) from detailed simulation logs. Specifically, we synthesize programs that operate on simulation logs and map them to a series of high level activated patterns. We show, through two physics benchmarks, that this annotated representation of the simulation log is more amenable to natural language reasoning about physical systems. We demonstrate how this method enables LMs to generate effective reward programs from goals specified in natural language, which may be used within the context of planning or supervised learning.",
      "authors": [
        "Sean Memery",
        "Kartic Subr"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "published": "2026-02-10 17:31:39+00:00",
      "link": "https://arxiv.org/pdf/2602.10009v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10006v1",
      "title": "Answer First, Reason Later: Aligning Search Relevance via Mode-Balanced Reinforcement Learning",
      "abstract": "Building a search relevance model that achieves both low latency and high performance is a long-standing challenge in the search industry. To satisfy the millisecond-level response requirements of online systems while retaining the interpretable reasoning traces of Large Language Models (LLMs), we propose a novel \\textbf{Answer-First, Reason Later (AFRL)} paradigm. This paradigm requires the model to output the definitive relevance score in the very first token, followed by a structured logical explanation. Inspired by the success of reasoning models, we adopt a \"Supervised Fine-Tuning (SFT) + Reinforcement Learning (RL)\" pipeline to achieve AFRL. However, directly applying existing RL training often leads to \\textbf{mode collapse} in the search relevance task, where the model forgets complex long-tail rules in pursuit of high rewards. From an information theory perspective: RL inherently minimizes the \\textbf{Reverse KL divergence}, which tends to seek probability peaks (mode-seeking) and is prone to \"reward hacking.\" On the other hand, SFT minimizes the \\textbf{Forward KL divergence}, forcing the model to cover the data distribution (mode-covering) and effectively anchoring expert rules. Based on this insight, we propose a \\textbf{Mode-Balanced Optimization} strategy, incorporating an SFT auxiliary loss into Stepwise-GRPO training to balance these two properties. Furthermore, we construct an automated instruction evolution system and a multi-stage curriculum to ensure expert-level data quality. Extensive experiments demonstrate that our 32B teacher model achieves state-of-the-art performance. Moreover, the AFRL architecture enables efficient knowledge distillation, successfully transferring expert-level logic to a 0.6B model, thereby reconciling reasoning depth with deployment latency.",
      "authors": [
        "Shijie Zhang",
        "Xiang Guo",
        "Rujun Guo",
        "Shaoyu Liu",
        "Xiaozhao Wang",
        "Guanjun Jiang",
        "Kevin Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 17:28:12+00:00",
      "link": "https://arxiv.org/pdf/2602.10006v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10004v1",
      "title": "ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference",
      "abstract": "Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated <stop> signals, and (iii) <stop>-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.",
      "authors": [
        "Junda Wang",
        "Zhichao Yang",
        "Dongxu Zhang",
        "Sanjit Singh Batra",
        "Robert E. Tillman"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-10 17:27:26+00:00",
      "link": "https://arxiv.org/pdf/2602.10004v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10003v1",
      "title": "ViSpeechFormer: A Phonemic Approach for Vietnamese Automatic Speech Recognition",
      "abstract": "Vietnamese has a phonetic orthography, where each grapheme corresponds to at most one phoneme and vice versa. Exploiting this high grapheme-phoneme transparency, we propose ViSpeechFormer (\\textbf{Vi}etnamese \\textbf{Speech} Trans\\textbf{Former}), a phoneme-based approach for Vietnamese Automatic Speech Recognition (ASR). To the best of our knowledge, this is the first Vietnamese ASR framework that explicitly models phonemic representations. Experiments on two publicly available Vietnamese ASR datasets show that ViSpeechFormer achieves strong performance, generalizes better to out-of-vocabulary words, and is less affected by training bias. This phoneme-based paradigm is also promising for other languages with phonetic orthographies. The code will be released upon acceptance of this paper.",
      "authors": [
        "Khoa Anh Nguyen",
        "Long Minh Hoang",
        "Nghia Hieu Nguyen",
        "Luan Thanh Nguyen",
        "Ngan Luu-Thuy Nguyen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 17:26:55+00:00",
      "link": "https://arxiv.org/pdf/2602.10003v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09988v1",
      "title": "Empirical Stability Analysis of Kolmogorov-Arnold Networks in Hard-Constrained Recurrent Physics-Informed Discovery",
      "abstract": "We investigate the integration of Kolmogorov-Arnold Networks (KANs) into hard-constrained recurrent physics-informed architectures (HRPINN) to evaluate the fidelity of learned residual manifolds in oscillatory systems. Motivated by the Kolmogorov-Arnold representation theorem and preliminary gray-box results, we hypothesized that KANs would enable efficient recovery of unknown terms compared to MLPs. Through initial sensitivity analysis on configuration sensitivity, parameter scale, and training paradigm, we found that while small KANs are competitive on univariate polynomial residuals (Duffing), they exhibit severe hyperparameter fragility, instability in deeper configurations, and consistent failure on multiplicative terms (Van der Pol), generally outperformed by standard MLPs. These empirical challenges highlight limitations of the additive inductive bias in the original KAN formulation for state coupling and provide preliminary empirical evidence of inductive bias limitations for future hybrid modeling.",
      "authors": [
        "Enzo Nicolas Spotorno",
        "Josafat Leal Filho",
        "Antonio Augusto Medeiros Frohlich"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.comp-ph"
      ],
      "published": "2026-02-10 17:13:51+00:00",
      "link": "https://arxiv.org/pdf/2602.09988v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09985v1",
      "title": "Online Monitoring Framework for Automotive Time Series Data using JEPA Embeddings",
      "abstract": "As autonomous vehicles are rolled out, measures must be taken to ensure their safe operation. In order to supervise a system that is already in operation, monitoring frameworks are frequently employed. These run continuously online in the background, supervising the system status and recording anomalies. This work proposes an online monitoring framework to detect anomalies in object state representations. Thereby, a key challenge is creating a framework for anomaly detection without anomaly labels, which are usually unavailable for unknown anomalies. To address this issue, this work applies a self-supervised embedding method to translate object data into a latent representation space. For this, a JEPA-based self-supervised prediction task is constructed, allowing training without anomaly labels and the creation of rich object embeddings. The resulting expressive JEPA embeddings serve as input for established anomaly detection methods, in order to identify anomalies within object state representations. This framework is particularly useful for applications in real-world environments, where new or unknown anomalies may occur during operation for which there are no labels available. Experiments performed on the publicly available, real-world nuScenes dataset illustrate the framework's capabilities.",
      "authors": [
        "Alexander Fertig",
        "Karthikeyan Chandra Sekaran",
        "Lakshman Balasubramanian",
        "Michael Botsch"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-10 17:10:29+00:00",
      "link": "https://arxiv.org/pdf/2602.09985v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09983v1",
      "title": "Coupled Inference in Diffusion Models for Semantic Decomposition",
      "abstract": "Many visual scenes can be described as compositions of latent factors. Effective recognition, reasoning, and editing often require not only forming such compositional representations, but also solving the decomposition problem. One popular choice for constructing these representations is through the binding operation. Resonator networks, which can be understood as coupled Hopfield networks, were proposed as a way to perform decomposition on such bound representations. Recent works have shown notable similarities between Hopfield networks and diffusion models. Motivated by these observations, we introduce a framework for semantic decomposition using coupled inference in diffusion models. Our method frames semantic decomposition as an inverse problem and couples the diffusion processes using a reconstruction-driven guidance term that encourages the composition of factor estimates to match the bound vector. We also introduce a novel iterative sampling scheme that improves the performance of our model. Finally, we show that attention-based resonator networks are a special case of our framework. Empirically, we demonstrate that our coupled inference framework outperforms resonator networks across a range of synthetic semantic decomposition tasks.",
      "authors": [
        "Calvin Yeung",
        "Ali Zakeri",
        "Zhuowen Zou",
        "Mohsen Imani"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-10 17:10:05+00:00",
      "link": "https://arxiv.org/pdf/2602.09983v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09980v1",
      "title": "Supervised Metric Regularization Through Alternating Optimization for Multi-Regime Physics-Informed Neural Networks",
      "abstract": "Standard Physics-Informed Neural Networks (PINNs) often face challenges when modeling parameterized dynamical systems with sharp regime transitions, such as bifurcations. In these scenarios, the continuous mapping from parameters to solutions can result in spectral bias or \"mode collapse\", where the network averages distinct physical behaviors. We propose a Topology-Aware PINN (TAPINN) that aims to mitigate this challenge by structuring the latent space via Supervised Metric Regularization. Unlike standard parametric PINNs that map physical parameters directly to solutions, our method conditions the solver on a latent state optimized to reflect the metric-based separation between regimes, showing ~49% lower physics residual (0.082 vs. 0.160). We train this architecture using a phase-based Alternating Optimization (AO) schedule to manage gradient conflicts between the metric and physics objectives. Preliminary experiments on the Duffing Oscillator demonstrate that while standard baselines suffer from spectral bias and high-capacity Hypernetworks overfit (memorizing data while violating physics), our approach achieves stable convergence with 2.18x lower gradient variance than a multi-output Sobolev Error baseline, and 5x fewer parameters than a hypernetwork-based alternative.",
      "authors": [
        "Enzo Nicolas Spotorno",
        "Josafat Ribeiro Leal",
        "Antonio Augusto Frohlich"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.comp-ph"
      ],
      "published": "2026-02-10 17:06:57+00:00",
      "link": "https://arxiv.org/pdf/2602.09980v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09961v1",
      "title": "ViMultiChoice: Toward a Method That Gives Explanation for Multiple-Choice Reading Comprehension in Vietnamese",
      "abstract": "Multiple-choice Reading Comprehension (MCRC) models aim to select the correct answer from a set of candidate options for a given question. However, they typically lack the ability to explain the reasoning behind their choices. In this paper, we introduce a novel Vietnamese dataset designed to train and evaluate MCRC models with explanation generation capabilities. Furthermore, we propose ViMultiChoice, a new method specifically designed for modeling Vietnamese reading comprehension that jointly predicts the correct answer and generates a corresponding explanation. Experimental results demonstrate that ViMultiChoice outperforms existing MCRC baselines, achieving state-of-the-art (SotA) performance on both the ViMMRC 2.0 benchmark and the newly introduced dataset. Additionally, we show that jointly training option decision and explanation generation leads to significant improvements in multiple-choice accuracy.",
      "authors": [
        "Trung Tien Cao",
        "Lam Minh Thai",
        "Nghia Hieu Nguyen",
        "Duc-Vu Nguyen",
        "Ngan Luu-Thuy Nguyen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 16:48:07+00:00",
      "link": "https://arxiv.org/pdf/2602.09961v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09959v1",
      "title": "Statistical-Computational Trade-offs in Learning Multi-Index Models via Harmonic Analysis",
      "abstract": "We study the problem of learning multi-index models (MIMs), where the label depends on the input $\\boldsymbol{x} \\in \\mathbb{R}^d$ only through an unknown $\\mathsf{s}$-dimensional projection $\\boldsymbol{W}_*^\\mathsf{T} \\boldsymbol{x} \\in \\mathbb{R}^\\mathsf{s}$. Exploiting the equivariance of this problem under the orthogonal group $\\mathcal{O}_d$, we obtain a sharp harmonic-analytic characterization of the learning complexity for MIMs with spherically symmetric inputs -- which refines and generalizes previous Gaussian-specific analyses. Specifically, we derive statistical and computational complexity lower bounds within the Statistical Query (SQ) and Low-Degree Polynomial (LDP) frameworks. These bounds decompose naturally across spherical harmonic subspaces. Guided by this decomposition, we construct a family of spectral algorithms based on harmonic tensor unfolding that sequentially recover the latent directions and (nearly) achieve these SQ and LDP lower bounds. Depending on the choice of harmonic degree sequence, these estimators can realize a broad range of trade-offs between sample and runtime complexity. From a technical standpoint, our results build on the semisimple decomposition of the $\\mathcal{O}_d$-action on $L^2 (\\mathbb{S}^{d-1})$ and the intertwining isomorphism between spherical harmonics and traceless symmetric tensors.",
      "authors": [
        "Hugo Latourelle-Vigeant",
        "Theodor Misiakiewicz"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-10 16:46:32+00:00",
      "link": "https://arxiv.org/pdf/2602.09959v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09953v1",
      "title": "ATTNPO: Attention-Guided Process Supervision for Efficient Reasoning",
      "abstract": "Large reasoning models trained with reinforcement learning and verifiable rewards (RLVR) achieve strong performance on complex reasoning tasks, yet often overthink, generating redundant reasoning without performance gains. Existing trajectory-level length penalties often fail to effectively shorten reasoning length and degrade accuracy, as they uniformly treat all reasoning steps and lack fine-grained signals to distinguish redundancy from necessity. Meanwhile, process-supervised methods are typically resource-intensive and suffer from inaccurate credit assignment. To address these issues, we propose ATTNPO, a low-overhead process-supervised RL framework that leverages the model's intrinsic attention signals for step-level credit assignment. We first identify a set of special attention heads that naturally focus on essential steps while suppressing redundant ones. By leveraging the attention scores of these heads, We then employ two sub-strategies to mitigate overthinking by discouraging redundant steps while preserving accuracy by reducing penalties on essential steps. Experimental results show that ATTNPO substantially reduces reasoning length while significantly improving performance across 9 benchmarks.",
      "authors": [
        "Shuaiyi Nie",
        "Siyu Ding",
        "Wenyuan Zhang",
        "Linhao Yu",
        "Tianmeng Yang",
        "Yao Chen",
        "Tingwen Liu",
        "Weichong Yin",
        "Yu Sun",
        "Hua Wu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 16:40:22+00:00",
      "link": "https://arxiv.org/pdf/2602.09953v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09947v1",
      "title": "Trustworthy Agentic AI Requires Deterministic Architectural Boundaries",
      "abstract": "Current agentic AI architectures are fundamentally incompatible with the security and epistemological requirements of high-stakes scientific workflows. The problem is not inadequate alignment or insufficient guardrails, it is architectural: autoregressive language models process all tokens uniformly, making deterministic command--data separation unattainable through training alone. We argue that deterministic, architectural enforcement, not probabilistic learned behavior, is a necessary condition for trustworthy AI-assisted science. We introduce the Trinity Defense Architecture, which enforces security through three mechanisms: action governance via a finite action calculus with reference-monitor enforcement, information-flow control via mandatory access labels preventing cross-scope leakage, and privilege separation isolating perception from execution. We show that without unforgeable provenance and deterministic mediation, the ``Lethal Trifecta'' (untrusted inputs, privileged data access, external action capability) turns authorization security into an exploit-discovery problem: training-based defenses may reduce empirical attack rates but cannot provide deterministic guarantees. The ML community must recognize that alignment is insufficient for authorization security, and that architectural mediation is required before agentic AI can be safely deployed in consequential scientific domains.",
      "authors": [
        "Manish Bhattarai",
        "Minh Vu"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR"
      ],
      "published": "2026-02-10 16:33:40+00:00",
      "link": "https://arxiv.org/pdf/2602.09947v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09945v1",
      "title": "Closing Reasoning Gaps in Clinical Agents with Differential Reasoning Learning",
      "abstract": "Clinical decision support requires not only correct answers but also clinically valid reasoning. We propose Differential Reasoning Learning (DRL), a framework that improves clinical agents by learning from reasoning discrepancies. From reference reasoning rationales (e.g., physician-authored clinical rationale, clinical guidelines, or outputs from more capable models) and the agent's free-form chain-of-thought (CoT), DRL extracts reasoning graphs as directed acyclic graphs (DAGs) and performs a clinically weighted graph edit distance (GED)-based discrepancy analysis. An LLM-as-a-judge aligns semantically equivalent nodes and diagnoses discrepancies between graphs. These graph-level discrepancy diagnostics are converted into natural-language instructions and stored in a Differential Reasoning Knowledge Base (DR-KB). At inference, we retrieve top-$k$ instructions via Retrieval-Augmented Generation (RAG) to augment the agent prompt and patch likely logic gaps. Evaluation on open medical question answering (QA) benchmarks and a Return Visit Admissions (RVA) prediction task from internal clinical data demonstrates gains over baselines, improving both final-answer accuracy and reasoning fidelity. Ablation studies confirm gains from infusing reference reasoning rationales and the top-$k$ retrieval strategy. Clinicians' review of the output provides further assurance of the approach. Together, results suggest that DRL supports more reliable clinical decision-making in complex reasoning scenarios and offers a practical mechanism for deployment under limited token budgets.",
      "authors": [
        "Jinsong Liu",
        "Yuhang Jiang",
        "Ramayya Krishnan",
        "Rema Padman",
        "Yiye Zhang",
        "Jiang Bian"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-10 16:29:32+00:00",
      "link": "https://arxiv.org/pdf/2602.09945v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09934v1",
      "title": "VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization",
      "abstract": "Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.",
      "authors": [
        "Yikun Liu",
        "Yuan Liu",
        "Shangzhe Di",
        "Haicheng Wang",
        "Zhongyin Zhao",
        "Le Tian",
        "Xiao Zhou",
        "Jie Zhou",
        "Jiangchao Yao",
        "Yanfeng Wang",
        "Weidi Xie"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-10 16:08:19+00:00",
      "link": "https://arxiv.org/pdf/2602.09934v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09930v1",
      "title": "JMigBench: A Benchmark for Evaluating LLMs on Source Code Migration (Java 8 to Java 11)",
      "abstract": "We build a benchmark to evaluate large language models (LLMs) for source code migration tasks, specifically upgrading functions from Java 8 to Java 11. We first collected a dataset of function pairs from open-source repositories, but limitations in data quality led us to construct a refined dataset covering eight categories of deprecated APIs. Using this dataset, the Mistral Codestral model was evaluated with CodeBLEU and keyword-based metrics to measure lexical and semantic similarity as well as migration correctness. Results show that the evaluated model (Mistral Codestral) can handle trivial one-to-one API substitutions with moderate success, achieving identical migrations in 11.11% of the cases, but it struggles with more complex migrations such as CORBA or JAX-WS. These findings suggest Mistral Codestral can partially reduce developer effort by automating repetitive migration tasks but cannot yet replace humans within the scope of the JMigBench benchmark. The benchmark and analysis provide a foundation for future work on expanding datasets, refining prompting strategies, and improving migration performance across different LLMs.",
      "authors": [
        "Nishil Amin",
        "Zhiwei Fei",
        "Xiang Li",
        "Justyna Petke",
        "He Ye"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-10 16:04:00+00:00",
      "link": "https://arxiv.org/pdf/2602.09930v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09924v1",
      "title": "LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations",
      "abstract": "Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty",
      "authors": [
        "William Lugoloobi",
        "Thomas Foster",
        "William Bankes",
        "Chris Russell"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-10 15:57:00+00:00",
      "link": "https://arxiv.org/pdf/2602.09924v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09914v1",
      "title": "AmharicIR+Instr: A Two-Dataset Resource for Neural Retrieval and Instruction Tuning",
      "abstract": "Neural retrieval and GPT-style generative models rely on large, high-quality supervised data, which is still scarce for low-resource languages such as Amharic. We release an Amharic data resource consisting of two datasets that supports research on (i) neural retrieval-ranking and (ii) instruction-following text generation. The retrieval-ranking dataset contains 1,091 manually verified query-positive-negative document triplets drawn from diverse Amharic sources and constructed to support contrastive training and benchmarking of neural retrievers (e.g., DPR, ColBERT-style late interaction and SPLADE-style sparse neural retrieval). Triplets are created through a combination of expert-curated queries, web-derived queries, and LLM-assisted generation, with positive/negative documents selected from the web or synthesized by LLMs and then validated by native speakers. The instruction prompt-response dataset comprises 6,285 Amharic prompt-response pairs spanning multiple domains and instruction types, generated with several LLMs and refined through manual review and correction for grammaticality, relevance, fluency, and factual plausibility. We release both datasets with standardized splits and formats (CSV,JSON,JSONL) to enable reproducible work on Amharic retrieval, ranking, and generative modelling. These datasets also come with a methodology that can be generalized to other low-resource languages.",
      "authors": [
        "Tilahun Yeshambel",
        "Moncef Garouani",
        "Josiane Mothe"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-02-10 15:45:20+00:00",
      "link": "https://arxiv.org/pdf/2602.09914v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09907v1",
      "title": "Self-Regulated Reading with AI Support: An Eight-Week Study with Students",
      "abstract": "College students increasingly use AI chatbots to support academic reading, yet we lack granular understanding of how these interactions shape their reading experience and cognitive engagement. We conducted an eight-week longitudinal study with 15 undergraduates who used AI to support assigned readings in a course. We collected 838 prompts across 239 reading sessions and developed a coding schema categorizing prompts into four cognitive themes: Decoding, Comprehension, Reasoning, and Metacognition. Comprehension prompts dominated (59.6%), with Reasoning (29.8%), Metacognition (8.5%), and Decoding (2.1%) less frequent. Most sessions (72%) contained exactly three prompts, the required minimum of the reading assignment. Within sessions, students showed natural cognitive progression from comprehension toward reasoning, but this progression was truncated. Across eight weeks, students' engagement patterns remained stable, with substantial individual differences persisting throughout. Qualitative analysis revealed an intention-behavior gap: students recognized that effective prompting required effort but rarely applied this knowledge, with efficiency emerging as the primary driver. Students also strategically triaged their engagement based on interest and academic pressures, exhibiting a novel pattern of reading through AI rather than with it: using AI-generated summaries as primary material to filter which sections merited deeper attention. We discuss design implications for AI reading systems that scaffold sustained cognitive engagement.",
      "authors": [
        "Yue Fu",
        "Joel Wester",
        "Niels Van Berkel",
        "Alexis Hiniker"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2026-02-10 15:41:15+00:00",
      "link": "https://arxiv.org/pdf/2602.09907v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09901v1",
      "title": "QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search",
      "abstract": "Query Processing (QP) bridges user intent and content supply in large-scale Social Network Service (SNS) search engines. Traditional QP systems rely on pipelines of isolated discriminative models (e.g., BERT), suffering from limited semantic understanding and high maintenance overhead. While Large Language Models (LLMs) offer a potential solution, existing approaches often optimize sub-tasks in isolation, neglecting intrinsic semantic synergy and necessitating independent iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the gap between open-domain corpora and informal SNS linguistic patterns, while struggling to adhere to rigorous business definitions. We present QP-OneModel, a Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. We reformulate heterogeneous sub-tasks into a unified sequence generation paradigm, adopting a progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning. Furthermore, QP-OneModel generates intent descriptions as a novel high-fidelity semantic signal, effectively augmenting downstream tasks such as query rewriting and ranking. Offline evaluations show QP-OneModel achieves a 7.35% overall gain over discriminative baselines, with significant F1 boosts in NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization, surpassing a 32B model by 7.60% accuracy on unseen tasks. Fully deployed at Xiaohongshu, online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%.",
      "authors": [
        "Jianzhao Huang",
        "Xiaorui Huang",
        "Fei Zhao",
        "Yunpeng Liu",
        "Hui Zhang",
        "Fangcheng Shi",
        "Congfeng Li",
        "Zechen Sun",
        "Yi Wu",
        "Yao Hu",
        "Yunhan Bai",
        "Shaosheng Cao"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "published": "2026-02-10 15:38:17+00:00",
      "link": "https://arxiv.org/pdf/2602.09901v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09870v1",
      "title": "Steer2Edit: From Activation Steering to Component-Level Editing",
      "abstract": "Steering methods influence Large Language Model behavior by identifying semantic directions in hidden representations, but are typically realized through inference-time activation interventions that apply a fixed, global modification to the model's internal states. While effective, such interventions often induce unfavorable attribute-utility trade-offs under strong control, as they ignore the fact that many behaviors are governed by a small and heterogeneous subset of model components. We propose Steer2Edit, a theoretically grounded, training-free framework that transforms steering vectors from inference-time control signals into diagnostic signals for component-level rank-1 weight editing. Instead of uniformly injecting a steering direction during generation, Steer2Edit selectively redistributes behavioral influence across individual attention heads and MLP neurons, yielding interpretable edits that preserve the standard forward pass and remain compatible with optimized parallel inference. Across safety alignment, hallucination mitigation, and reasoning efficiency, Steer2Edit consistently achieves more favorable attribute-utility trade-offs: at matched downstream performance, it improves safety by up to 17.2%, increases truthfulness by 9.8%, and reduces reasoning length by 12.2% on average. Overall, Steer2Edit provides a principled bridge between representation steering and weight editing by translating steering signals into interpretable, training-free parameter updates.",
      "authors": [
        "Chung-En Sun",
        "Ge Yan",
        "Zimo Wang",
        "Tsui-Wei Weng"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 15:15:15+00:00",
      "link": "https://arxiv.org/pdf/2602.09870v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09869v1",
      "title": "Statistical benchmarking of transformer models in low signal-to-noise time-series forecasting",
      "abstract": "We study the performance of transformer architectures for multivariate time-series forecasting in low-data regimes consisting of only a few years of daily observations. Using synthetically generated processes with known temporal and cross-sectional dependency structures and varying signal-to-noise ratios, we conduct bootstrapped experiments that enable direct evaluation via out-of-sample correlations with the optimal ground-truth predictor. We show that two-way attention transformers, which alternate between temporal and cross-sectional self-attention, can outperform standard baselines-Lasso, boosting methods, and fully connected multilayer perceptrons-across a wide range of settings, including low signal-to-noise regimes. We further introduce a dynamic sparsification procedure for attention matrices applied during training, and demonstrate that it becomes significantly effective in noisy environments, where the correlation between the target variable and the optimal predictor is on the order of a few percent. Analysis of the learned attention patterns reveals interpretable structure and suggests connections to sparsity-inducing regularization in classical regression, providing insight into why these models generalize effectively under noise.",
      "authors": [
        "Cyril Garcia",
        "Guillaume Remy"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 15:13:57+00:00",
      "link": "https://arxiv.org/pdf/2602.09869v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09864v1",
      "title": "Differentiable Tripartite Modularity for Clustering Heterogeneous Graphs",
      "abstract": "Clustering heterogeneous relational data remains a central challenge in graph learning, particularly when interactions involve more than two types of entities. While differentiable modularity objectives such as DMoN have enabled end-to-end community detection on homogeneous and bipartite graphs, extending these approaches to higher-order relational structures remains non-trivial.   In this work, we introduce a differentiable formulation of tripartite modularity for graphs composed of three node types connected through mediated interactions. Community structure is defined in terms of weighted co-paths across the tripartite graph, together with an exact factorized computation that avoids the explicit construction of dense third-order tensors. A structural normalization at pivot nodes is introduced to control extreme degree heterogeneity and ensure stable optimization.   The resulting objective can be optimized jointly with a graph neural network in an end-to-end manner, while retaining linear complexity in the number of edges. We validate the proposed framework on large-scale urban cadastral data, where it exhibits robust convergence behavior and produces spatially coherent partitions. These results highlight differentiable tripartite modularity as a generic methodological building block for unsupervised clustering of heterogeneous graphs.",
      "authors": [
        "Benoît Hurpeau"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.SI"
      ],
      "published": "2026-02-10 15:06:53+00:00",
      "link": "https://arxiv.org/pdf/2602.09864v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09851v1",
      "title": "CoFEH: LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization",
      "abstract": "Feature Engineering (FE) is pivotal in automated machine learning (AutoML) but remains a bottleneck for traditional methods, which treat it as a black-box search, operating within rigid, predefined search spaces and lacking domain awareness. While Large Language Models (LLMs) offer a promising alternative by leveraging semantic reasoning to generate unbounded operators, existing methods fail to construct free-form FE pipelines, remaining confined to isolated subtasks such as feature generation. Most importantly, they are rarely optimized jointly with hyperparameter optimization (HPO) of the ML model, leading to greedy \"FE-then-HPO\" workflows that cannot capture strong FE-HPO interactions. In this paper, we present CoFEH, a collaborative framework that interleaves LLM-based FE and Bayesian HPO for robust end-to-end AutoML. CoFEH uses an LLM-driven FE optimizer powered by Tree of Thought (ToT) to explore flexible FE pipelines, a Bayesian optimization (BO) module to solve HPO, and a dynamic optimizer selector that realizes interleaved optimization by adaptively scheduling FE and HPO steps. Crucially, we introduce a mutual conditioning mechanism that shares context between LLM and BO, enabling mutually informed decisions. Experiments show that CoFEH not only outperforms traditional and LLM-based FE baselines, but also achieves superior end-to-end performance under joint optimization.",
      "authors": [
        "Beicheng Xu",
        "Keyao Ding",
        "Wei Liu",
        "Yupeng Lu",
        "Bin Cui"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 14:54:17+00:00",
      "link": "https://arxiv.org/pdf/2602.09851v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09850v1",
      "title": "Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection",
      "abstract": "Industrial anomaly detection demands precise reasoning over fine-grained defect patterns. However, existing multimodal large language models (MLLMs), pretrained on general-domain data, often struggle to capture category-specific anomalies, thereby limiting both detection accuracy and interpretability. To address these limitations, we propose Reason-IAD, a knowledge-guided dynamic latent reasoning framework for explainable industrial anomaly detection. Reason-IAD comprises two core components. First, a retrieval-augmented knowledge module incorporates category-specific textual descriptions into the model input, enabling context-aware reasoning over domain-specific defects. Second, an entropy-driven latent reasoning mechanism conducts iterative exploration within a compact latent space using optimizable latent think tokens, guided by an entropy-based reward that encourages confident and stable predictions. Furthermore, a dynamic visual injection strategy selectively incorporates the most informative image patches into the latent sequence, directing the reasoning process toward regions critical for anomaly detection. Extensive experimental results demonstrate that Reason-IAD consistently outperforms state-of-the-art methods. The code will be publicly available at https://github.com/chenpeng052/Reason-IAD.",
      "authors": [
        "Peng Chen",
        "Chao Huang",
        "Yunkang Cao",
        "Chengliang Liu",
        "Wenqiang Wang",
        "Mingbo Yang",
        "Li Shen",
        "Wenqi Ren",
        "Xiaochun Cao"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-10 14:54:17+00:00",
      "link": "https://arxiv.org/pdf/2602.09850v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09842v1",
      "title": "Step-Size Stability in Stochastic Optimization: A Theoretical Perspective",
      "abstract": "We present a theoretical analysis of stochastic optimization methods in terms of their sensitivity with respect to the step size. We identify a key quantity that, for each method, describes how the performance degrades as the step size becomes too large. For convex problems, we show that this quantity directly impacts the suboptimality bound of the method. Most importantly, our analysis provides direct theoretical evidence that adaptive step-size methods, such as SPS or NGN, are more robust than SGD. This allows us to quantify the advantage of these adaptive methods beyond empirical evaluation. Finally, we show through experiments that our theoretical bound qualitatively mirrors the actual performance as a function of the step size, even for nonconvex problems.",
      "authors": [
        "Fabian Schaipp",
        "Robert M. Gower",
        "Adrien Taylor"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "published": "2026-02-10 14:46:14+00:00",
      "link": "https://arxiv.org/pdf/2602.09842v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09839v1",
      "title": "ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge",
      "abstract": "Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.",
      "authors": [
        "Yijie Lin",
        "Guofeng Ding",
        "Haochen Zhou",
        "Haobin Li",
        "Mouxing Yang",
        "Xi Peng"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-10 14:45:02+00:00",
      "link": "https://arxiv.org/pdf/2602.09839v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09838v1",
      "title": "How Do People Quantify Naturally: Evidence from Mandarin Picture Description",
      "abstract": "Quantification is a fundamental component of everyday language use, yet little is known about how speakers decide whether and how to quantify in naturalistic production. We investigate quantification in Mandarin Chinese using a picture-based elicited description task in which speakers freely described scenes containing multiple objects, without explicit instructions to count or quantify. Across both spoken and written modalities, we examine three aspects of quantification: whether speakers choose to quantify at all, how precise their quantification is, and which quantificational strategies they adopt. Results show that object numerosity, animacy, and production modality systematically shape quantificational behaviour. In particular, increasing numerosity reduces both the likelihood and the precision of quantification, while animate referents and modality selectively modulate strategy choice. This study demonstrates how quantification can be examined under unconstrained production conditions and provides a naturalistic dataset for further analyses of quantity expression in language production.",
      "authors": [
        "Yayun Zhang",
        "Guanyi Chen",
        "Fahime Same",
        "Saad Mahamood",
        "Tingting He"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 14:45:00+00:00",
      "link": "https://arxiv.org/pdf/2602.09838v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09832v1",
      "title": "LLM Reasoning Predicts When Models Are Right: Evidence from Coding Classroom Discourse",
      "abstract": "Large Language Models (LLMs) are increasingly deployed to automatically label and analyze educational dialogue at scale, yet current pipelines lack reliable ways to detect when models are wrong. We investigate whether reasoning generated by LLMs can be used to predict the correctness of a model's own predictions. We analyze 30,300 teacher utterances from classroom dialogue, each labeled by multiple state-of-the-art LLMs with an instructional move construct and an accompanying reasoning. Using human-verified ground-truth labels, we frame the task as predicting whether a model's assigned label for a given utterance is correct. We encode LLM reasoning using Term Frequency-Inverse Document Frequency (TF-IDF) and evaluate five supervised classifiers. A Random Forest classifier achieves an F1 score of 0.83 (Recall = 0.854), successfully identifying most incorrect predictions and outperforming baselines. Training specialist detectors for specific instructional move constructs further improves performance on difficult constructs, indicating that error detection benefits from construct-specific linguistic cues. Using the Linguistic Inquiry and Word Count (LIWC) framework, we examine four linguistic markers of correctness: Causation, Differentiation, Tentativeness, and Insight. Correct predictions exhibit grounded causal language (e.g., because, therefore), while incorrect reasoning is substantially more likely to rely on epistemic hedging (e.g., might, could) and performative metacognition (e.g., think, realize). Syntactic complexity does not distinguish correct from incorrect reasoning, and longer reasoning is not more reliable. These findings demonstrate that reasoning-based error detection offers a practical and scalable approach to quality control in automated educational dialogue analysis.",
      "authors": [
        "Bakhtawar Ahtisham",
        "Kirk Vanacore",
        "Zhuqian Zhou",
        "Jinsook Lee",
        "Rene F. Kizilcec"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 14:38:13+00:00",
      "link": "https://arxiv.org/pdf/2602.09832v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09826v1",
      "title": "From FusHa to Folk: Exploring Cross-Lingual Transfer in Arabic Language Models",
      "abstract": "Arabic Language Models (LMs) are pretrained predominately on Modern Standard Arabic (MSA) and are expected to transfer to its dialects. While MSA as the standard written variety is commonly used in formal settings, people speak and write online in various dialects that are spread across the Arab region. This poses limitations for Arabic LMs, since its dialects vary in their similarity to MSA. In this work we study cross-lingual transfer of Arabic models using probing on 3 Natural Language Processing (NLP) Tasks, and representational similarity. Our results indicate that transfer is possible but disproportionate across dialects, which we find to be partially explained by their geographic proximity. Furthermore, we find evidence for negative interference in models trained to support all Arabic dialects. This questions their degree of similarity, and raises concerns for cross-lingual transfer in Arabic models.",
      "authors": [
        "Abdulmuizz Khalak",
        "Abderrahmane Issam",
        "Gerasimos Spanakis"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 14:34:04+00:00",
      "link": "https://arxiv.org/pdf/2602.09826v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09821v2",
      "title": "Text summarization via global structure awareness",
      "abstract": "Text summarization is a fundamental task in natural language processing (NLP), and the information explosion has made long-document processing increasingly demanding, making summarization essential. Existing research mainly focuses on model improvements and sentence-level pruning, but often overlooks global structure, leading to disrupted coherence and weakened downstream performance. Some studies employ large language models (LLMs), which achieve higher accuracy but incur substantial resource and time costs. To address these issues, we introduce GloSA-sum, the first summarization approach that achieves global structure awareness via topological data analysis (TDA). GloSA-sum summarizes text efficiently while preserving semantic cores and logical dependencies. Specifically, we construct a semantic-weighted graph from sentence embeddings, where persistent homology identifies core semantics and logical structures, preserved in a ``protection pool'' as the backbone for summarization. We design a topology-guided iterative strategy, where lightweight proxy metrics approximate sentence importance to avoid repeated high-cost computations, thus preserving structural integrity while improving efficiency. To further enhance long-text processing, we propose a hierarchical strategy that integrates segment-level and global summarization. Experiments on multiple datasets demonstrate that GloSA-sum reduces redundancy while preserving semantic and logical integrity, striking a balance between accuracy and efficiency, and further benefits LLM downstream tasks by shortening contexts while retaining essential reasoning chains.",
      "authors": [
        "Jiaquan Zhang",
        "Chaoning Zhang",
        "Shuxu Chen",
        "Yibei Liu",
        "Chenghao Li",
        "Qigan Sun",
        "Shuai Yuan",
        "Fachrina Dewi Puspitasari",
        "Dongshen Han",
        "Guoqing Wang",
        "Sung-Ho Bae",
        "Yang Yang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-10 14:29:54+00:00",
      "link": "https://arxiv.org/pdf/2602.09821v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09817v1",
      "title": "AnalyticsGPT: An LLM Workflow for Scientometric Question Answering",
      "abstract": "This paper introduces AnalyticsGPT, an intuitive and efficient large language model (LLM)-powered workflow for scientometric question answering. This underrepresented downstream task addresses the subcategory of meta-scientific questions concerning the \"science of science.\" When compared to traditional scientific question answering based on papers, the task poses unique challenges in the planning phase. Namely, the need for named-entity recognition of academic entities within questions and multi-faceted data retrieval involving scientometric indices, e.g. impact factors. Beyond their exceptional capacity for treating traditional natural language processing tasks, LLMs have shown great potential in more complex applications, such as task decomposition and planning and reasoning. In this paper, we explore the application of LLMs to scientometric question answering, and describe an end-to-end system implementing a sequential workflow with retrieval-augmented generation and agentic concepts. We also address the secondary task of effectively synthesizing the data into presentable and well-structured high-level analyses. As a database for retrieval-augmented generation, we leverage a proprietary research performance assessment platform. For evaluation, we consult experienced subject matter experts and leverage LLMs-as-judges. In doing so, we provide valuable insights on the efficacy of LLMs towards a niche downstream task. Our (skeleton) code and prompts are available at: https://github.com/lyvykhang/llm-agents-scientometric-qa/tree/acl.",
      "authors": [
        "Khang Ly",
        "Georgios Cheirmpos",
        "Adrian Raudaschl",
        "Christopher James",
        "Seyed Amin Tabatabaei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.DL"
      ],
      "published": "2026-02-10 14:23:55+00:00",
      "link": "https://arxiv.org/pdf/2602.09817v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09813v1",
      "title": "Efficient Unsupervised Environment Design through Hierarchical Policy Representation Learning",
      "abstract": "Unsupervised Environment Design (UED) has emerged as a promising approach to developing general-purpose agents through automated curriculum generation. Popular UED methods focus on Open-Endedness, where teacher algorithms rely on stochastic processes for infinite generation of useful environments. This assumption becomes impractical in resource-constrained scenarios where teacher-student interaction opportunities are limited. To address this challenge, we introduce a hierarchical Markov Decision Process (MDP) framework for environment design. Our framework features a teacher agent that leverages student policy representations derived from discovered evaluation environments, enabling it to generate training environments based on the student's capabilities. To improve efficiency, we incorporate a generative model that augments the teacher's training dataset with synthetic data, reducing the need for teacher-student interactions. In experiments across several domains, we show that our method outperforms baseline approaches while requiring fewer teacher-student interactions in a single episode. The results suggest the applicability of our approach in settings where training opportunities are limited.",
      "authors": [
        "Dexun Li",
        "Sidney Tio",
        "Pradeep Varakantham"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-10 14:19:40+00:00",
      "link": "https://arxiv.org/pdf/2602.09813v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09809v1",
      "title": "SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing",
      "abstract": "Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.",
      "authors": [
        "Tong Zhang",
        "Honglin Lin",
        "Zhou Liu",
        "Chong Chen",
        "Wentao Zhang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-10 14:15:35+00:00",
      "link": "https://arxiv.org/pdf/2602.09809v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09805v1",
      "title": "Decomposing Reasoning Efficiency in Large Language Models",
      "abstract": "Large language models trained for reasoning trade off inference tokens against accuracy, yet standard evaluations report only final accuracy, obscuring where tokens are spent or wasted. We introduce a trace-optional framework that decomposes token efficiency into interpretable factors: completion under a fixed token budget (avoiding truncation), conditional correctness given completion, and verbosity (token usage). When benchmark metadata provides per-instance workload proxies, we further factor verbosity into two components: mean verbalization overhead (tokens per work unit) and a coupling coefficient capturing how overhead scales with task workload. When reasoning traces are available, we add deterministic trace-quality measures (grounding, repetition, prompt copying) to separate degenerate looping from verbose-but-engaged reasoning, avoiding human labeling and LLM judges. Evaluating 25 models on CogniLoad, we find that accuracy and token-efficiency rankings diverge (Spearman $ρ=0.63$), efficiency gaps are often driven by conditional correctness, and verbalization overhead varies by about 9 times (only weakly related to model scale). Our decomposition reveals distinct bottleneck profiles that suggest different efficiency interventions.",
      "authors": [
        "Daniel Kaiser",
        "Arnoldo Frigessi",
        "Ali Ramezani-Kebrya",
        "Benjamin Ricaud"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-10 14:09:18+00:00",
      "link": "https://arxiv.org/pdf/2602.09805v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09801v1",
      "title": "Tiny Moves: Game-based Hypothesis Refinement",
      "abstract": "Most machine learning approaches to scientific discovery frame hypotheses as end-to-end predictions, obscuring the incremental structure of scientific reasoning. We propose The Hypothesis Game, a symbolic formalism for hypothesis refinement in which LLM agents operate on a shared hypothesis state using a fixed grammar of reasoning moves. The framework is motivated by the observation that scientific progress often proceeds through small, localized revisions, grounded in domain context, rather than extensive rewrites. We instantiate a minimal game with LLM agents and evaluate it on pathway-level mechanistic refinement tasks. In the primary setting of corruption recovery, where hypotheses contain controlled errors, the game-based approach consistently removes more errors and achieves higher precision than strong prompting baselines, while preserving valid structure through incremental edits. In a secondary reconstruction setting from partial cues, it performs comparably to the strongest baseline, indicating that explicit move-based refinement remains competitive even when ground-truth recovery is difficult. These findings support game-based reasoning as a principled route to more controllable, interpretable, and transferable hypothesis refinement systems for scientific discovery.",
      "authors": [
        "Agnieszka Dobrowolska",
        "Rogier Hintzen",
        "Martin Balla",
        "Karl Gemayel",
        "Sabine Reichert",
        "Thomas Charman",
        "Jen Ning Lim",
        "Lindsay Edwards",
        "Anna Gogleva"
      ],
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA"
      ],
      "published": "2026-02-10 14:04:29+00:00",
      "link": "https://arxiv.org/pdf/2602.09801v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10171v1",
      "title": "EvoCodeBench: A Human-Performance Benchmark for Self-Evolving LLM-Driven Coding Systems",
      "abstract": "As large language models (LLMs) continue to advance in programming tasks, LLM-driven coding systems have evolved from one-shot code generation into complex systems capable of iterative improvement during inference. However, existing code benchmarks primarily emphasize static correctness and implicitly assume fixed model capability during inference. As a result, they do not capture inference-time self-evolution, such as whether accuracy and efficiency improve as an agent iteratively refines its solutions. They also provide limited accounting of resource costs and rarely calibrate model performance against that of human programmers. Moreover, many benchmarks are dominated by high-resource languages, leaving cross-language robustness and long-tail language stability underexplored. Therefore, we present EvoCodeBench, a benchmark for evaluating self-evolving LLM-driven coding systems across programming languages with direct comparison to human performance. EvoCodeBench tracks performance dynamics, measuring solution correctness alongside efficiency metrics such as solving time, memory consumption, and improvement algorithmic design over repeated problem-solving attempts. To ground evaluation in a human-centered reference frame, we directly compare model performance with that of human programmers on the same tasks, enabling relative performance assessment within the human ability distribution. Furthermore, EvoCodeBench supports multiple programming languages, enabling systematic cross-language and long-tail stability analyses under a unified protocol. Our results demonstrate that self-evolving systems exhibit measurable gains in efficiency over time, and that human-relative and multi-language analyses provide insights unavailable through accuracy alone. EvoCodeBench establishes a foundation for evaluating coding intelligence in evolving LLM-driven systems.",
      "authors": [
        "Wentao Zhang",
        "Jianfeng Wang",
        "Liheng Liang",
        "Yilei Zhao",
        "HaiBin Wen",
        "Zhe Zhao"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published": "2026-02-10 14:04:22+00:00",
      "link": "https://arxiv.org/pdf/2602.10171v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09794v1",
      "title": "GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis",
      "abstract": "Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.",
      "authors": [
        "Jiaquan Zhang",
        "Chaoning Zhang",
        "Shuxu Chen",
        "Xudong Wang",
        "Zhenzhen Huang",
        "Pengcheng Zheng",
        "Shuai Yuan",
        "Sheng Zheng",
        "Qigan Sun",
        "Jie Zou",
        "Lik-Hang Lee",
        "Yang Yang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-10 14:00:30+00:00",
      "link": "https://arxiv.org/pdf/2602.09794v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09791v1",
      "title": "Toeplitz Based Spectral Methods for Data-driven Dynamical Systems",
      "abstract": "We introduce a Toeplitz-based framework for data-driven spectral estimation of linear evolution operators in dynamical systems. Focusing on transfer and Koopman operators from equilibrium trajectories without access to the underlying equations of motion, our method applies Toeplitz filters to the infinitesimal generator to extract eigenvalues, eigenfunctions, and spectral measures. Structural prior knowledge, such as self-adjointness or skew-symmetry, can be incorporated by design. The approach is statistically consistent and computationally efficient, leveraging both primal and dual algorithms commonly used in statistical learning. Numerical experiments on deterministic and chaotic systems demonstrate that the framework can recover spectral properties beyond the reach of standard data-driven methods.",
      "authors": [
        "Vladimir R. Kostic",
        "Karim Lounici",
        "Massimiliano Pontil"
      ],
      "primary_category": "math.DS",
      "categories": [
        "math.DS",
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-02-10 13:53:21+00:00",
      "link": "https://arxiv.org/pdf/2602.09791v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09785v1",
      "title": "Where Are We At with Automatic Speech Recognition for the Bambara Language?",
      "abstract": "This paper introduces the first standardized benchmark for evaluating Automatic Speech Recognition (ASR) in the Bambara language, utilizing one hour of professionally recorded Malian constitutional text. Designed as a controlled reference set under near-optimal acoustic and linguistic conditions, the benchmark was used to evaluate 37 models, ranging from Bambara-trained systems to large-scale commercial models. Our findings reveal that current ASR performance remains significantly below deployment standards in a narrow formal domain; the top-performing system in terms of Word Error Rate (WER) achieved 46.76\\% and the best Character Error Rate (CER) of 13.00\\% was set by another model, while several prominent multilingual models exceeded 100\\% WER. These results suggest that multilingual pre-training and model scaling alone are insufficient for underrepresented languages. Furthermore, because this dataset represents a best-case scenario of the most simplified and formal form of spoken Bambara, these figures are yet to be tested against practical, real-world settings. We provide the benchmark and an accompanying public leaderboard to facilitate transparent evaluation and future research in Bambara speech technology.",
      "authors": [
        "Seydou Diallo",
        "Yacouba Diarra",
        "Mamadou K. Keita",
        "Panga Azazia Kamaté",
        "Adam Bouno Kampo",
        "Aboubacar Ouattara"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 13:44:51+00:00",
      "link": "https://arxiv.org/pdf/2602.09785v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09783v1",
      "title": "Why Linear Interpretability Works: Invariant Subspaces as a Result of Architectural Constraints",
      "abstract": "Linear probes and sparse autoencoders consistently recover meaningful structure from transformer representations -- yet why should such simple methods succeed in deep, nonlinear systems? We show this is not merely an empirical regularity but a consequence of architectural necessity: transformers communicate information through linear interfaces (attention OV circuits, unembedding matrices), and any semantic feature decoded through such an interface must occupy a context-invariant linear subspace. We formalize this as the \\emph{Invariant Subspace Necessity} theorem and derive the \\emph{Self-Reference Property}: tokens directly provide the geometric direction for their associated features, enabling zero-shot identification of semantic structure without labeled data or learned probes. Empirical validation in eight classification tasks and four model families confirms the alignment between class tokens and semantically related instances. Our framework provides \\textbf{a principled architectural explanation} for why linear interpretability methods work, unifying linear probes and sparse autoencoders.",
      "authors": [
        "Andres Saurez",
        "Yousung Lee",
        "Dongsoo Har"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-10 13:42:55+00:00",
      "link": "https://arxiv.org/pdf/2602.09783v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09782v1",
      "title": "Flexible Entropy Control in RLVR with Gradient-Preserving Perspective",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a critical method for enhancing the reasoning capabilities of Large Language Models (LLMs). However, continuous training often leads to policy entropy collapse, characterized by a rapid decay in entropy that results in premature overconfidence, reduced output diversity, and vanishing gradient norms that inhibit learning. Gradient-Preserving Clipping is a primary factor influencing these dynamics, but existing mitigation strategies are largely static and lack a framework connecting clipping mechanisms to precise entropy control. This paper proposes reshaping entropy control in RL from the perspective of Gradient-Preserving Clipping. We first theoretically and empirically verify the contributions of specific importance sampling ratio regions to entropy growth and reduction. Leveraging these findings, we introduce a novel regulation mechanism using dynamic clipping threshold to precisely manage entropy. Furthermore, we design and evaluate dynamic entropy control strategies, including increase-then-decrease, decrease-increase-decrease, and oscillatory decay. Experimental results demonstrate that these strategies effectively mitigate entropy collapse, and achieve superior performance across multiple benchmarks.",
      "authors": [
        "Kun Chen",
        "Peng Shi",
        "Fanfan Liu",
        "Haibo Qiu",
        "Zhixiong Zeng",
        "Siqi Yang",
        "Wenji Mao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-10 13:42:12+00:00",
      "link": "https://arxiv.org/pdf/2602.09782v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09774v1",
      "title": "QRS: A Rule-Synthesizing Neuro-Symbolic Triad for Autonomous Vulnerability Discovery",
      "abstract": "Static Application Security Testing (SAST) tools are integral to modern DevSecOps pipelines, yet tools like CodeQL, Semgrep, and SonarQube remain fundamentally constrained: they require expert-crafted queries, generate excessive false positives, and detect only predefined vulnerability patterns. Recent work has explored augmenting SAST with Large Language Models (LLMs), but these approaches typically use LLMs to triage existing tool outputs rather than to reason about vulnerability semantics directly. We introduce QRS (Query, Review, Sanitize), a neuro-symbolic framework that inverts this paradigm. Rather than filtering results from static rules, QRS employs three autonomous agents that generate CodeQL queries from a structured schema definition and few-shot examples, then validate findings through semantic reasoning and automated exploit synthesis. This architecture enables QRS to discover vulnerability classes beyond predefined patterns while substantially reducing false positives. We evaluate QRS on full Python packages rather than isolated snippets. In 20 historical CVEs in popular PyPI libraries, QRS achieves 90.6% detection accuracy. Applied to the 100 most-downloaded PyPI packages, QRS identified 39 medium-to-high-severity vulnerabilities, 5 of which were assigned new CVEs, 5 received documentation updates, while the remaining 29 were independently discovered by concurrent researchers, validating both the severity and discoverability of these findings. QRS accomplishes this with low time overhead and manageable token costs, demonstrating that LLM-driven query synthesis and code review can complement manually curated rule sets and uncover vulnerability patterns that evade existing industry tools.",
      "authors": [
        "George Tsigkourakos",
        "Constantinos Patsakis"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR"
      ],
      "published": "2026-02-10 13:35:24+00:00",
      "link": "https://arxiv.org/pdf/2602.09774v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09772v1",
      "title": "Design and Evaluation of an Assisted Programming Interface for Behavior Trees in Robotics",
      "abstract": "The possibility to create reactive robot programs faster without the need for extensively trained programmers is becoming increasingly important. So far, it has not been explored how various techniques for creating Behavior Tree (BT) program representations could be combined with complete graphical user interfaces (GUIs) to allow a human user to validate and edit trees suggested by automated methods. In this paper, we introduce BEhavior TRee GUI (BETR-GUI) for creating BTs with the help of an AI assistant that combines methods using large language models, planning, genetic programming, and Bayesian optimization with a drag-and-drop editor. A user study with 60 participants shows that by combining different assistive methods, BETR-GUI enables users to perform better at solving the robot programming tasks. The results also show that humans using the full variant of BETR-GUI perform better than the AI assistant running on its own.",
      "authors": [
        "Jonathan Styrud",
        "Matteo Iovino",
        "Rebecca Stower",
        "Mart Kartašev",
        "Mikael Norrlöf",
        "Mårten Björkman",
        "Christian Smith"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-10 13:34:00+00:00",
      "link": "https://arxiv.org/pdf/2602.09772v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09764v1",
      "title": "Self-Supervised Learning as Discrete Communication",
      "abstract": "Most self-supervised learning (SSL) methods learn continuous visual representations by aligning different views of the same input, offering limited control over how information is structured across representation dimensions. In this work, we frame visual self-supervised learning as a discrete communication process between a teacher and a student network, where semantic information is transmitted through a fixed-capacity binary channel. Rather than aligning continuous features, the student predicts multi-label binary messages produced by the teacher. Discrete agreement is enforced through an element-wise binary cross-entropy objective, while a coding-rate regularization term encourages effective utilization of the constrained channel, promoting structured representations. We further show that periodically reinitializing the projection head strengthens this effect by encouraging embeddings that remain predictive across multiple discrete encodings. Extensive experiments demonstrate consistent improvements over continuous agreement baselines on image classification, retrieval, and dense visual prediction tasks, as well as under domain shift through self-supervised adaptation. Beyond backbone representations, we analyze the learned binary codes and show that they form a compact and informative discrete language, capturing semantic factors reusable across classes.",
      "authors": [
        "Kawtar Zaher",
        "Ilyass Moummad",
        "Olivier Buisson",
        "Alexis Joly"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2026-02-10 13:24:06+00:00",
      "link": "https://arxiv.org/pdf/2602.09764v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09761v1",
      "title": "Grounding LTL Tasks in Sub-Symbolic RL Environments for Zero-Shot Generalization",
      "abstract": "In this work we address the problem of training a Reinforcement Learning agent to follow multiple temporally-extended instructions expressed in Linear Temporal Logic in sub-symbolic environments. Previous multi-task work has mostly relied on knowledge of the mapping between raw observations and symbols appearing in the formulae. We drop this unrealistic assumption by jointly training a multi-task policy and a symbol grounder with the same experience. The symbol grounder is trained only from raw observations and sparse rewards via Neural Reward Machines in a semi-supervised fashion. Experiments on vision-based environments show that our method achieves performance comparable to using the true symbol grounding and significantly outperforms state-of-the-art methods for sub-symbolic environments.",
      "authors": [
        "Matteo Pannacci",
        "Andrea Fanti",
        "Elena Umili",
        "Roberto Capobianco"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-10 13:20:29+00:00",
      "link": "https://arxiv.org/pdf/2602.09761v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09760v1",
      "title": "Improving Interpretability of Lexical Semantic Change with Neurobiological Features",
      "abstract": "Lexical Semantic Change (LSC) is the phenomenon in which the meaning of a word change over time. Most studies on LSC focus on improving the performance of estimating the degree of LSC, however, it is often difficult to interpret how the meaning of a word change. Enhancing the interpretability of LSC is a significant challenge as it could lead to novel insights in this field. To tackle this challenge, we propose a method to map the semantic space of contextualized embeddings of words obtained by a pre-trained language model to a neurobiological feature space. In the neurobiological feature space, each dimension corresponds to a primitive feature of words, and its value represents the intensity of that feature. This enables humans to interpret LSC systematically. When employed for the estimation of the degree of LSC, our method demonstrates superior performance in comparison to the majority of the previous methods. In addition, given the high interpretability of the proposed method, several analyses on LSC are carried out. The results demonstrate that our method not only discovers interesting types of LSC that have been overlooked in previous studies but also effectively searches for words with specific types of LSC.",
      "authors": [
        "Kohei Oda",
        "Hiroya Takamura",
        "Kiyoaki Shirai",
        "Natthawut Kertkeidkachorn"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 13:20:08+00:00",
      "link": "https://arxiv.org/pdf/2602.09760v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09757v1",
      "title": "Towards Poisoning Robustness Certification for Natural Language Generation",
      "abstract": "Understanding the reliability of natural language generation is critical for deploying foundation models in security-sensitive domains. While certified poisoning defenses provide provable robustness bounds for classification tasks, they are fundamentally ill-equipped for autoregressive generation: they cannot handle sequential predictions or the exponentially large output space of language models. To establish a framework for certified natural language generation, we formalize two security properties: stability (robustness to any change in generation) and validity (robustness to targeted, harmful changes in generation). We introduce Targeted Partition Aggregation (TPA), the first algorithm to certify validity/targeted attacks by computing the minimum poisoning budget needed to induce a specific harmful class, token, or phrase. Further, we extend TPA to provide tighter guarantees for multi-turn generations using mixed integer linear programming (MILP). Empirically, we demonstrate TPA's effectiveness across diverse settings including: certifying validity of agent tool-calling when adversaries modify up to 0.5% of the dataset and certifying 8-token stability horizons in preference-based alignment. Though inference-time latency remains an open challenge, our contributions enable certified deployment of language models in security-critical applications.",
      "authors": [
        "Mihnea Ghitu",
        "Matthew Wicker"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 13:09:44+00:00",
      "link": "https://arxiv.org/pdf/2602.09757v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09748v1",
      "title": "Linear Model Extraction via Factual and Counterfactual Queries",
      "abstract": "In model extraction attacks, the goal is to reveal the parameters of a black-box machine learning model by querying the model for a selected set of data points. Due to an increasing demand for explanations, this may involve counterfactual queries besides the typically considered factual queries. In this work, we consider linear models and three types of queries: factual, counterfactual, and robust counterfactual. First, for an arbitrary set of queries, we derive novel mathematical formulations for the classification regions for which the decision of the unknown model is known, without recovering any of the model parameters. Second, we derive bounds on the number of queries needed to extract the model's parameters for (robust) counterfactual queries under arbitrary norm-based distances. We show that the full model can be recovered using just a single counterfactual query when differentiable distance measures are employed. In contrast, when using polyhedral distances for instance, the number of required queries grows linearly with the dimension of the data space. For robust counterfactuals, the latter number of queries doubles. Consequently, the applied distance function and robustness of counterfactuals have a significant impact on the model's security.",
      "authors": [
        "Daan Otto",
        "Jannis Kurtz",
        "Dick den Hertog",
        "Ilker Birbil"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "published": "2026-02-10 12:57:53+00:00",
      "link": "https://arxiv.org/pdf/2602.09748v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09746v1",
      "title": "Sparse Axonal and Dendritic Delays Enable Competitive SNNs for Keyword Classification",
      "abstract": "Training transmission delays in spiking neural networks (SNNs) has been shown to substantially improve their performance on complex temporal tasks. In this work, we show that learning either axonal or dendritic delays enables deep feedforward SNNs composed of leaky integrate-and-fire (LIF) neurons to reach accuracy comparable to existing synaptic delay learning approaches, while significantly reducing memory and computational overhead. SNN models with either axonal or dendritic delays achieve up to $95.58\\%$ on the Google Speech Command (GSC) and $80.97\\%$ on the Spiking Speech Command (SSC) datasets, matching or exceeding prior methods based on synaptic delays or more complex neuron models. By adjusting the delay parameters, we obtain improved performance for synaptic delay learning baselines, strengthening the comparison. We find that axonal delays offer the most favorable trade-off, combining lower buffering requirements with slightly higher accuracy than dendritic delays. We further show that the performance of axonal and dendritic delay models is largely preserved under strong delay sparsity, with as few as $20\\%$ of delays remaining active, further reducing buffering requirements. Overall, our results indicate that learnable axonal and dendritic delays provide a resource-efficient and effective mechanism for temporal representation in SNNs. Code will be made available publicly upon acceptance. Code is available at https://github.com/YounesBouhadjar/AxDenSynDelaySNN",
      "authors": [
        "Younes Bouhadjar",
        "Emre Neftci"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-10 12:57:02+00:00",
      "link": "https://arxiv.org/pdf/2602.09746v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09724v1",
      "title": "Targum -- A Multilingual New Testament Translation Corpus",
      "abstract": "Many European languages possess rich biblical translation histories, yet existing corpora - in prioritizing linguistic breadth - often fail to capture this depth. To address this gap, we introduce a multilingual corpus of 657 New Testament translations, of which 352 are unique, with unprecedented depth in five languages: English (208 unique versions from 396 total), French (41 from 78), Italian (18 from 33), Polish (30 from 48), and Spanish (55 from 102). Aggregated from 12 online biblical libraries and one preexisting corpus, each translation is manually annotated with metadata that maps the text to a standardized identifier for the work, its specific edition, and its year of revision. This canonicalization empowers researchers to define \"uniqueness\" for their own needs: they can perform micro-level analyses on translation families, such as the KJV lineage, or conduct macro-level studies by deduplicating closely related texts. By providing the first resource designed for such flexible, multilevel analysis, our corpus establishes a new benchmark for the quantitative study of translation history.",
      "authors": [
        "Maciej Rapacz",
        "Aleksander Smywiński-Pohl"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 12:27:57+00:00",
      "link": "https://arxiv.org/pdf/2602.09724v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09723v1",
      "title": "AI-Assisted Scientific Assessment: A Case Study on Climate Change",
      "abstract": "The emerging paradigm of AI co-scientists focuses on tasks characterized by repeatable verification, where agents explore search spaces in 'guess and check' loops. This paradigm does not extend to problems where repeated evaluation is impossible and ground truth is established by the consensus synthesis of theory and existing evidence. We evaluate a Gemini-based AI environment designed to support collaborative scientific assessment, integrated into a standard scientific workflow. In collaboration with a diverse group of 13 scientists working in the field of climate science, we tested the system on a complex topic: the stability of the Atlantic Meridional Overturning Circulation (AMOC). Our results show that AI can accelerate the scientific workflow. The group produced a comprehensive synthesis of 79 papers through 104 revision cycles in just over 46 person-hours. AI contribution was significant: most AI-generated content was retained in the report. AI also helped maintain logical consistency and presentation quality. However, expert additions were crucial to ensure its acceptability: less than half of the report was produced by AI. Furthermore, substantial oversight was required to expand and elevate the content to rigorous scientific standards.",
      "authors": [
        "Christian Buck",
        "Levke Caesar",
        "Michelle Chen Huebscher",
        "Massimiliano Ciaramita",
        "Erich M. Fischer",
        "Zeke Hausfather",
        "Özge Kart Tokmak",
        "Reto Knutti",
        "Markus Leippold",
        "Joseph Ludescher",
        "Katharine J. Mach",
        "Sofia Palazzo Corner",
        "Kasra Rafiezadeh Shahi",
        "Johan Rockström",
        "Joeri Rogelj",
        "Boris Sakschewski"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 12:26:58+00:00",
      "link": "https://arxiv.org/pdf/2602.09723v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09720v1",
      "title": "Continual Learning for non-stationary regression via Memory-Efficient Replay",
      "abstract": "Data streams are rarely static in dynamic environments like Industry 4.0. Instead, they constantly change, making traditional offline models outdated unless they can quickly adjust to the new data. This need can be adequately addressed by continual learning (CL), which allows systems to gradually acquire knowledge without incurring the prohibitive costs of retraining them from scratch. Most research on continual learning focuses on classification problems, while very few studies address regression tasks. We propose the first prototype-based generative replay framework designed for online task-free continual regression. Our approach defines an adaptive output-space discretization model, enabling prototype-based generative replay for continual regression without storing raw data. Evidence obtained from several benchmark datasets shows that our framework reduces forgetting and provides more stable performance than other state-of-the-art solutions.",
      "authors": [
        "Pablo García-Santaclara",
        "Bruno Fernández-Castro",
        "RebecaP. Díaz-Redondo",
        "Martín Alonso-Gamarra"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-10 12:22:59+00:00",
      "link": "https://arxiv.org/pdf/2602.09720v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09719v1",
      "title": "Unsupervised Layer-Wise Dynamic Test Time Adaptation for LLMs",
      "abstract": "Test-time adaptation (TTA) for large language models (LLMs) updates model parameters at inference time using signals available at deployment. This paper focuses on a common yet under-explored regime: unsupervised, sample-specific TTA, where the model adapts independently for each prompt using only the prompt itself, without gold answers or external supervision. Although appealing, naive unsupervised TTA with a fixed, handcrafted learning rate can be unstable: updates may overfit to prompt-specific statistics, drift from the desired answer distribution, and ultimately degrade generation quality. This failure mode is not surprising, as in this case TTA must adapt to a single prompt within only a few gradient steps, unlike standard training that averages updates over large datasets and long optimization horizons. Therefore, we propose layer-wise dynamic test-time adaptation, a framework which explicitly modulates TTA strength as a function of prompt representation, LLM structure and adaptation step. In our setting, TTA updates only LoRA parameters, and a lightweight hypernetwork predicts per-layer, per-step learning-rate multipliers, enabling fine-grained control. Experiments across various datasets and LLMs consistently show that our method substantially strengthens TTA by learning effective scaling patterns over adaptation steps and transformer layer projections, improving stability while delivering better performance.",
      "authors": [
        "Longhuan Xu",
        "Cunjian Chen",
        "Feng Yin"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 12:22:14+00:00",
      "link": "https://arxiv.org/pdf/2602.09719v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09717v1",
      "title": "From Lightweight CNNs to SpikeNets: Benchmarking Accuracy-Energy Tradeoffs with Pruned Spiking SqueezeNet",
      "abstract": "Spiking Neural Networks (SNNs) are increasingly studied as energy-efficient alternatives to Convolutional Neural Networks (CNNs), particularly for edge intelligence. However, prior work has largely emphasized large-scale models, leaving the design and evaluation of lightweight CNN-to-SNN pipelines underexplored. In this paper, we present the first systematic benchmark of lightweight SNNs obtained by converting compact CNN architectures into spiking networks, where activations are modeled with Leaky-Integrate-and-Fire (LIF) neurons and trained using surrogate gradient descent under a unified setup. We construct spiking variants of ShuffleNet, SqueezeNet, MnasNet, and MixNet, and evaluate them on CIFAR-10, CIFAR-100, and TinyImageNet, measuring accuracy, F1-score, parameter count, computational complexity, and energy consumption. Our results show that SNNs can achieve up to 15.7x higher energy efficiency than their CNN counterparts while retaining competitive accuracy. Among these, the SNN variant of SqueezeNet consistently outperforms other lightweight SNNs. To further optimize this model, we apply a structured pruning strategy that removes entire redundant modules, yielding a pruned architecture, SNN-SqueezeNet-P. This pruned model improves CIFAR-10 accuracy by 6% and reduces parameters by 19% compared to the original SNN-SqueezeNet. Crucially, it narrows the gap with CNN-SqueezeNet, achieving nearly the same accuracy (only 1% lower) but with an 88.1% reduction in energy consumption due to sparse spike-driven computations. Together, these findings establish lightweight SNNs as practical, low-power alternatives for edge deployment, highlighting a viable path toward deploying high-performance, low-power intelligence on the edge.",
      "authors": [
        "Radib Bin Kabir",
        "Tawsif Tashwar Dipto",
        "Mehedi Ahamed",
        "Sabbir Ahmed",
        "Md Hasanul Kabir"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "published": "2026-02-10 12:20:11+00:00",
      "link": "https://arxiv.org/pdf/2602.09717v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09712v1",
      "title": "TraceMem: Weaving Narrative Memory Schemata from User Conversational Traces",
      "abstract": "Sustaining long-term interactions remains a bottleneck for Large Language Models (LLMs), as their limited context windows struggle to manage dialogue histories that extend over time. Existing memory systems often treat interactions as disjointed snippets, failing to capture the underlying narrative coherence of the dialogue stream. We propose TraceMem, a cognitively-inspired framework that weaves structured, narrative memory schemata from user conversational traces through a three-stage pipeline: (1) Short-term Memory Processing, which employs a deductive topic segmentation approach to demarcate episode boundaries and extract semantic representation; (2) Synaptic Memory Consolidation, a process that summarizes episodes into episodic memories before distilling them alongside semantics into user-specific traces; and (3) Systems Memory Consolidation, which utilizes two-stage hierarchical clustering to organize these traces into coherent, time-evolving narrative threads under unifying themes. These threads are encapsulated into structured user memory cards, forming narrative memory schemata. For memory utilization, we provide an agentic search mechanism to enhance reasoning process. Evaluation on the LoCoMo benchmark shows that TraceMem achieves state-of-the-art performance with a brain-inspired architecture. Analysis shows that by constructing coherent narratives, it surpasses baselines in multi-hop and temporal reasoning, underscoring its essential role in deep narrative comprehension. Additionally, we provide an open discussion on memory systems, offering our perspectives and future outlook on the field. Our code implementation is available at: https://github.com/YimingShu-teay/TraceMem",
      "authors": [
        "Yiming Shu",
        "Pei Liu",
        "Tiange Zhang",
        "Ruiyang Gao",
        "Jun Ma",
        "Chen Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 12:14:58+00:00",
      "link": "https://arxiv.org/pdf/2602.09712v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09708v1",
      "title": "Physics-informed diffusion models in spectral space",
      "abstract": "We propose a methodology that combines generative latent diffusion models with physics-informed machine learning to generate solutions of parametric partial differential equations (PDEs) conditioned on partial observations, which includes, in particular, forward and inverse PDE problems. We learn the joint distribution of PDE parameters and solutions via a diffusion process in a latent space of scaled spectral representations, where Gaussian noise corresponds to functions with controlled regularity. This spectral formulation enables significant dimensionality reduction compared to grid-based diffusion models and ensures that the induced process in function space remains within a class of functions for which the PDE operators are well defined. Building on diffusion posterior sampling, we enforce physics-informed constraints and measurement conditions during inference, applying Adam-based updates at each diffusion step. We evaluate the proposed approach on Poisson, Helmholtz, and incompressible Navier--Stokes equations, demonstrating improved accuracy and computational efficiency compared with existing diffusion-based PDE solvers, which are state of the art for sparse observations. Code is available at https://github.com/deeplearningmethods/PISD.",
      "authors": [
        "Davide Gallon",
        "Philippe von Wurstemberger",
        "Patrick Cheridito",
        "Arnulf Jentzen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "math.NA"
      ],
      "published": "2026-02-10 12:11:07+00:00",
      "link": "https://arxiv.org/pdf/2602.09708v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09703v1",
      "title": "Maastricht University at AMIYA: Adapting LLMs for Dialectal Arabic using Fine-tuning and MBR Decoding",
      "abstract": "Large Language Models (LLMs) are becoming increasingly multilingual, supporting hundreds of languages, especially high resource ones. Unfortunately, Dialect variations are still underrepresented due to limited data and linguistic variation. In this work, we adapt a pre-trained LLM to improve dialectal performance. Specifically, we use Low Rank Adaptation (LoRA) fine-tuning on monolingual and English Dialect parallel data, adapter merging and dialect-aware MBR decoding to improve dialectal fidelity generation and translation. Experiments on Syrian, Moroccan, and Saudi Arabic show that merging and MBR improve dialectal fidelity while preserving semantic accuracy. This combination provides a compact and effective framework for robust dialectal Arabic generation.",
      "authors": [
        "Abdulhai Alali",
        "Abderrahmane Issam"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-10 12:02:34+00:00",
      "link": "https://arxiv.org/pdf/2602.09703v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09691v1",
      "title": "Life Cycle-Aware Evaluation of Knowledge Distillation for Machine Translation: Environmental Impact and Translation Quality Trade-offs",
      "abstract": "Knowledge distillation (KD) is a tool to compress a larger system (teacher) into a smaller one (student). In machine translation, studies typically report only the translation quality of the student and omit the computational complexity of performing KD, making it difficult to select among the many available KD choices under compute-induced constraints. In this study, we evaluate representative KD methods by considering both translation quality and computational cost. We express computational cost as a carbon footprint using the machine learning life cycle assessment (MLCA) tool. This assessment accounts for runtime operational emissions and amortized hardware production costs throughout the KD model life cycle (teacher training, distillation, and inference). We find that (i) distillation overhead dominates the total footprint at small deployment volumes, (ii) inference dominates at scale, making KD beneficial only beyond a task-dependent usage threshold, and (iii) word-level distillation typically offers more favorable footprint-quality trade-offs than sequence-level distillation. Our protocol provides reproducible guidance for selecting KD methods under explicit quality and compute-induced constraints.",
      "authors": [
        "Joseph Attieh",
        "Timothee Mickus",
        "Anne-Laure Ligozat",
        "Aurélie Névéol",
        "Jörg Tiedemann"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-10 11:46:32+00:00",
      "link": "https://arxiv.org/pdf/2602.09691v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09690v1",
      "title": "Contextual and Seasonal LSTMs for Time Series Anomaly Detection",
      "abstract": "Univariate time series (UTS), where each timestamp records a single variable, serve as crucial indicators in web systems and cloud servers. Anomaly detection in UTS plays an essential role in both data mining and system reliability management. However, existing reconstruction-based and prediction-based methods struggle to capture certain subtle anomalies, particularly small point anomalies and slowly rising anomalies. To address these challenges, we propose a novel prediction-based framework named Contextual and Seasonal LSTMs (CS-LSTMs). CS-LSTMs are built upon a noise decomposition strategy and jointly leverage contextual dependencies and seasonal patterns, thereby strengthening the detection of subtle anomalies. By integrating both time-domain and frequency-domain representations, CS-LSTMs achieve more accurate modeling of periodic trends and anomaly localization. Extensive evaluations on public benchmark datasets demonstrate that CS-LSTMs consistently outperform state-of-the-art methods, highlighting their effectiveness and practical value in robust time series anomaly detection.",
      "authors": [
        "Lingpei Zhang",
        "Qingming Li",
        "Yong Yang",
        "Jiahao Chen",
        "Rui Zeng",
        "Chenyang Lyu",
        "Shouling Ji"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 11:46:15+00:00",
      "link": "https://arxiv.org/pdf/2602.09690v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09681v1",
      "title": "Resilient Class-Incremental Learning: on the Interplay of Drifting, Unlabelled and Imbalanced Data Streams",
      "abstract": "In today's connected world, the generation of massive streaming data across diverse domains has become commonplace. In the presence of concept drift, class imbalance, label scarcity, and new class emergence, they jointly degrade representation stability, bias learning toward outdated distributions, and reduce the resilience and reliability of detection in dynamic environments. This paper proposes SCIL (Streaming Class-Incremental Learning) to address these challenges. The SCIL framework integrates an autoencoder (AE) with a multi-layer perceptron for multi-class prediction, uses a dual-loss strategy (classification and reconstruction) for prediction and new class detection, employs corrected pseudo-labels for online training, manages classes with queues, and applies oversampling to handle imbalance. The rationale behind the method's structure is elucidated through ablation studies and a comprehensive experimental evaluation is performed using both real-world and synthetic datasets that feature class imbalance, incremental classes, and concept drifts. Our results demonstrate that SCIL outperforms strong baselines and state-of-the-art methods. Based on our commitment to Open Science, we make our code and datasets available to the community.",
      "authors": [
        "Jin Li",
        "Kleanthis Malialis",
        "Marios Polycarpou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-10 11:37:39+00:00",
      "link": "https://arxiv.org/pdf/2602.09681v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13315v1",
      "title": "IDPruner: Harmonizing Importance and Diversity in Visual Token Pruning for MLLMs",
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities, yet they encounter significant computational bottlenecks due to the massive volume of visual tokens. Consequently, visual token pruning, which substantially reduces the token count, has emerged as a critical technique for accelerating MLLM inference. Existing approaches focus on token importance, diversity, or an intuitive combination of both, without a principled framework for their optimal integration. To address this issue, we first conduct a systematic analysis to characterize the trade-off between token importance and semantic diversity. Guided by this analysis, we propose the \\textbf{I}mportance and \\textbf{D}iversity Pruner (\\textbf{IDPruner}), which leverages the Maximal Marginal Relevance (MMR) algorithm to achieve a Pareto-optimal balance between these two objectives. Crucially, our method operates without requiring attention maps, ensuring full compatibility with FlashAttention and efficient deployment via one-shot pruning. We conduct extensive experiments across various model architectures and multimodal benchmarks, demonstrating that IDPruner achieves state-of-the-art performance and superior generalization across diverse architectures and tasks. Notably, on Qwen2.5-VL-7B-Instruct, IDPruner retains 95.18\\% of baseline performance when pruning 75\\% of the tokens, and still maintains 86.40\\% even under an extreme 90\\% pruning ratio. Our code is available at https://github.com/Tencent/AngelSlim.",
      "authors": [
        "Yifan Tan",
        "Yifu Sun",
        "Shirui Huang",
        "Hong Liu",
        "Guanghua Yu",
        "Jianchen Zhu",
        "Yangdong Deng"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-10 11:20:24+00:00",
      "link": "https://arxiv.org/pdf/2602.13315v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09653v2",
      "title": "ClinAlign: Scaling Healthcare Alignment from Clinician Preference",
      "abstract": "Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B-A3B model trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.",
      "authors": [
        "Shiwei Lyu",
        "Xidong Wang",
        "Lei Liu",
        "Hao Zhu",
        "Chaohe Zhang",
        "Jian Wang",
        "Jinjie Gu",
        "Benyou Wang",
        "Yue Shen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-10 11:02:57+00:00",
      "link": "https://arxiv.org/pdf/2602.09653v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09651v1",
      "title": "The Entropic Signature of Class Speciation in Diffusion Models",
      "abstract": "Diffusion models do not recover semantic structure uniformly over time. Instead, samples transition from semantic ambiguity to class commitment within a narrow regime. Recent theoretical work attributes this transition to dynamical instabilities along class-separating directions, but practical methods to detect and exploit these windows in trained models are still limited. We show that tracking the class-conditional entropy of a latent semantic variable given the noisy state provides a reliable signature of these transition regimes. By restricting the entropy to semantic partitions, the entropy can furthermore resolve semantic decisions at different levels of abstraction. We analyze this behavior in high-dimensional Gaussian mixture models and show that the entropy rate concentrates on the same logarithmic time scale as the speciation symmetry-breaking instability previously identified in variance-preserving diffusion. We validate our method on EDM2-XS and Stable Diffusion 1.5, where class-conditional entropy consistently isolates the noise regimes critical for semantic structure formation. Finally, we use our framework to quantify how guidance redistributes semantic information over time. Together, these results connect information-theoretic and statistical physics perspectives on diffusion and provide a principled basis for time-localized control.",
      "authors": [
        "Florian Handke",
        "Dejan Stančević",
        "Felix Koulischer",
        "Thomas Demeester",
        "Luca Ambrogioni"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-10 10:56:46+00:00",
      "link": "https://arxiv.org/pdf/2602.09651v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09642v1",
      "title": "MATA: Multi-Agent Framework for Reliable and Flexible Table Question Answering",
      "abstract": "Recent advances in Large Language Models (LLMs) have significantly improved table understanding tasks such as Table Question Answering (TableQA), yet challenges remain in ensuring reliability, scalability, and efficiency, especially in resource-constrained or privacy-sensitive environments. In this paper, we introduce MATA, a multi-agent TableQA framework that leverages multiple complementary reasoning paths and a set of tools built with small language models. MATA generates candidate answers through diverse reasoning styles for a given table and question, then refines or selects the optimal answer with the help of these tools. Furthermore, it incorporates an algorithm designed to minimize expensive LLM agent calls, enhancing overall efficiency. MATA maintains strong performance with small, open-source models and adapts easily across various LLM types. Extensive experiments on two benchmarks of varying difficulty with ten different LLMs demonstrate that MATA achieves state-of-the-art accuracy and highly efficient reasoning while avoiding excessive LLM inference. Our results highlight that careful orchestration of multiple reasoning pathways yields scalable and reliable TableQA. The code is available at https://github.com/AIDAS-Lab/MATA.",
      "authors": [
        "Sieun Hyeon",
        "Jusang Oh",
        "Sunghwan Steve Cho",
        "Jaeyoung Do"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-10 10:43:02+00:00",
      "link": "https://arxiv.org/pdf/2602.09642v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09624v1",
      "title": "MILE-RefHumEval: A Reference-Free, Multi-Independent LLM Framework for Human-Aligned Evaluation",
      "abstract": "We introduce MILE-RefHumEval, a reference-free framework for evaluating Large Language Models (LLMs) without ground-truth annotations or evaluator coordination. It leverages an ensemble of independently prompted evaluators guided by a human-aligned schema, supporting both discrete and continuous scoring judgement. With task-specific prompts from best candidate selection, summarization and image captioning to dialogue, MILE-RefHumEval provides flexible, interpretable, and scalable assessments. Experiments show it aligns closely with human judgments, outperforms prior methods, and reduces computational overhead, offering an efficient, robust, and human-aligned solution for real-world LLM evaluation.",
      "authors": [
        "Nalin Srun",
        "Parisa Rastin",
        "Guénaël Cabanes",
        "Lydia Boudjeloud Assala"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 10:10:41+00:00",
      "link": "https://arxiv.org/pdf/2602.09624v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09621v2",
      "title": "AlignTune: Modular Toolkit for Post-Training Alignment of Large Language Models",
      "abstract": "Post-training alignment is central to deploying large language models (LLMs), yet practical workflows remain split across backend-specific tools and ad-hoc glue code, making experiments hard to reproduce. We identify backend interference, reward fragmentation, and irreproducible pipelines as key obstacles in alignment research. We introduce AlignTune, a modular toolkit exposing a unified interface for supervised fine-tuning (SFT) and RLHF-style optimization with interchangeable TRL and Unsloth backends. AlignTune standardizes configuration, provides an extensible reward layer (rule-based and learned), and integrates evaluation over standard benchmarks and custom tasks. By isolating backend-specific logic behind a single factory boundary, AlignTune enables controlled comparisons and reproducible alignment experiments.",
      "authors": [
        "R E Zera Marveen Lyngkhoi",
        "Chirag Chawla",
        "Pratinav Seth",
        "Utsav Avaiya",
        "Soham Bhattacharjee",
        "Mykola Khandoga",
        "Rui Yuan",
        "Vinay Kumar Sankarapu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-10 10:08:51+00:00",
      "link": "https://arxiv.org/pdf/2602.09621v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09620v1",
      "title": "FLINGO -- Instilling ASP Expressiveness into Linear Integer Constraints",
      "abstract": "Constraint Answer Set Programming (CASP) is a hybrid paradigm that enriches Answer Set Programming (ASP) with numerical constraint processing, something required in many real-world applications. The usual specification of constraints in most CASP solvers is closer to the numerical back-end expressiveness and semantics, rather than to standard specification in ASP. In the latter, numerical attributes are represented with predicates and this allows declaring default values, leaving the attribute undefined, making non-deterministic assignments with choice rules or using aggregated values. In CASP, most (if not all) of these features are lost once we switch to a constraint-based representation of those same attributes. In this paper, we present the FLINGO language (and tool) that incorporates the aforementioned expressiveness inside the numerical constraints and we illustrate its use with several examples. Based on previous work that established its semantic foundations, we also present a translation from the newly introduced FLINGO syntax to regular CASP programs following the CLINGCON input format.",
      "authors": [
        "Jorge Fandinno",
        "Pedro Cabalar",
        "Philipp Wanko",
        "Torsten Schaub"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "published": "2026-02-10 10:08:05+00:00",
      "link": "https://arxiv.org/pdf/2602.09620v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09616v1",
      "title": "With Argus Eyes: Assessing Retrieval Gaps via Uncertainty Scoring to Detect and Remedy Retrieval Blind Spots",
      "abstract": "Reliable retrieval-augmented generation (RAG) systems depend fundamentally on the retriever's ability to find relevant information. We show that neural retrievers used in RAG systems have blind spots, which we define as the failure to retrieve entities that are relevant to the query, but have low similarity to the query embedding. We investigate the training-induced biases that cause such blind spot entities to be mapped to inaccessible parts of the embedding space, resulting in low retrievability. Using a large-scale dataset constructed from Wikidata relations and first paragraphs of Wikipedia, and our proposed Retrieval Probability Score (RPS), we show that blind spot risk in standard retrievers (e.g., CONTRIEVER, REASONIR) can be predicted pre-index from entity embedding geometry, avoiding expensive retrieval evaluations. To address these blind spots, we introduce ARGUS, a pipeline that enables the retrievability of high-risk (low-RPS) entities through targeted document augmentation from a knowledge base (KB), first paragraphs of Wikipedia, in our case. Extensive experiments on BRIGHT, IMPLIRET, and RAR-B show that ARGUS achieves consistent improvements across all evaluated retrievers (averaging +3.4 nDCG@5 and +4.5 nDCG@10 absolute points), with substantially larger gains in challenging subsets. These results establish that preemptively remedying blind spots is critical for building robust and trustworthy RAG systems (Code and Data).",
      "authors": [
        "Zeinab Sadat Taghavi",
        "Ali Modarressi",
        "Hinrich Schutze",
        "Andreas Marfurt"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-10 10:04:55+00:00",
      "link": "https://arxiv.org/pdf/2602.09616v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09613v1",
      "title": "Tracking Finite-Time Lyapunov Exponents to Robustify Neural ODEs",
      "abstract": "We investigate finite-time Lyapunov exponents (FTLEs), a measure for exponential separation of input perturbations, of deep neural networks within the framework of continuous-depth neural ODEs. We demonstrate that FTLEs are powerful organizers for input-output dynamics, allowing for better interpretability and the comparison of distinct model architectures. We establish a direct connection between Lyapunov exponents and adversarial vulnerability, and propose a novel training algorithm that improves robustness by FTLE regularization. The key idea is to suppress exponents far from zero in the early stage of the input dynamics. This approach enhances robustness and reduces computational cost compared to full-interval regularization, as it avoids a full ``double'' backpropagation.",
      "authors": [
        "Tobias Wöhrer",
        "Christian Kuehn"
      ],
      "primary_category": "math.DS",
      "categories": [
        "math.DS",
        "cs.LG"
      ],
      "published": "2026-02-10 10:04:08+00:00",
      "link": "https://arxiv.org/pdf/2602.09613v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09598v1",
      "title": "Learning from the Irrecoverable: Error-Localized Policy Optimization for Tool-Integrated LLM Reasoning",
      "abstract": "Tool-integrated reasoning (TIR) enables LLM agents to solve tasks through planning, tool use, and iterative revision, but outcome-only reinforcement learning in this setting suffers from sparse, delayed rewards and weak step-level credit assignment. In long-horizon TIR trajectories, an early irrecoverable mistake can determine success or failure, making it crucial to localize the first irrecoverable step and leverage it for fine-grained credit assignment. We propose Error-Localized Policy Optimization (ELPO), which localizes the first irrecoverable step via binary-search rollout trees under a fixed rollout budget, converts the resulting tree into stable learning signals through hierarchical advantage attribution, and applies error-localized adaptive clipping to strengthen corrective updates on the critical step and its suffix. Across TIR benchmarks in math, science QA, and code execution, ELPO consistently outperforms strong Agentic RL baselines under comparable sampling budgets, with additional gains in Pass@K and Major@K scaling, rollout ranking quality, and tool-call efficiency. Our code will be publicly released soon.",
      "authors": [
        "Qiao Liang",
        "Yuke Zhu",
        "Chao Ge",
        "Lei Yang",
        "Ying Shen",
        "Bo Zheng",
        "Sheng Guo"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 09:50:24+00:00",
      "link": "https://arxiv.org/pdf/2602.09598v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09593v1",
      "title": "Why the Counterintuitive Phenomenon of Likelihood Rarely Appears in Tabular Anomaly Detection with Deep Generative Models?",
      "abstract": "Deep generative models with tractable and analytically computable likelihoods, exemplified by normalizing flows, offer an effective basis for anomaly detection through likelihood-based scoring. We demonstrate that, unlike in the image domain where deep generative models frequently assign higher likelihoods to anomalous data, such counterintuitive behavior occurs far less often in tabular settings. We first introduce a domain-agnostic formulation that enables consistent detection and evaluation of the counterintuitive phenomenon, addressing the absence of precise definition. Through extensive experiments on 47 tabular datasets and 10 CV/NLP embedding datasets in ADBench, benchmarked against 13 baseline models, we demonstrate that the phenomenon, as defined, is consistently rare in general tabular data. We further investigate this phenomenon from both theoretical and empirical perspectives, focusing on the roles of data dimensionality and difference in feature correlation. Our results suggest that likelihood-only detection with normalizing flows offers a practical and reliable approach for anomaly detection in tabular domains.",
      "authors": [
        "Donghwan Kim",
        "Junghun Phee",
        "Hyunsoo Yoon"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-10 09:47:34+00:00",
      "link": "https://arxiv.org/pdf/2602.09593v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09590v1",
      "title": "Context-Aware Counterfactual Data Augmentation for Gender Bias Mitigation in Language Models",
      "abstract": "A challenge in mitigating social bias in fine-tuned language models (LMs) is the potential reduction in language modeling capability, which can harm downstream performance. Counterfactual data augmentation (CDA), a widely used method for fine-tuning, highlights this issue by generating synthetic data that may align poorly with real-world distributions or creating overly simplistic counterfactuals that ignore the social context of altered sensitive attributes (e.g., gender) in the pretraining corpus. To address these limitations, we propose a simple yet effective context-augmented CDA method, Context-CDA, which uses large LMs to enhance the diversity and contextual relevance of the debiasing corpus. By minimizing discrepancies between the debiasing corpus and pretraining data through augmented context, this approach ensures better alignment, enhancing language modeling capability. We then employ uncertainty-based filtering to exclude generated counterfactuals considered low-quality by the target smaller LMs (i.e., LMs to be debiased), further improving the fine-tuning corpus quality. Experimental results on gender bias benchmarks demonstrate that Context-CDA effectively mitigates bias without sacrificing language modeling performance while offering insights into social biases by analyzing distribution shifts in next-token generation probabilities.",
      "authors": [
        "Shweta Parihar",
        "Liu Guangliang",
        "Natalie Parde",
        "Lu Cheng"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-10 09:45:21+00:00",
      "link": "https://arxiv.org/pdf/2602.09590v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09587v1",
      "title": "MieDB-100k: A Comprehensive Dataset for Medical Image Editing",
      "abstract": "The scarcity of high-quality data remains a primary bottleneck in adapting multimodal generative models for medical image editing. Existing medical image editing datasets often suffer from limited diversity, neglect of medical image understanding and inability to balance quality with scalability. To address these gaps, we propose MieDB-100k, a large-scale, high-quality and diverse dataset for text-guided medical image editing. It categorizes editing tasks into perspectives of Perception, Modification and Transformation, considering both understanding and generation abilities. We construct MieDB-100k via a data curation pipeline leveraging both modality-specific expert models and rule-based data synthetic methods, followed by rigorous manual inspection to ensure clinical fidelity. Extensive experiments demonstrate that model trained with MieDB-100k consistently outperform both open-source and proprietary models while exhibiting strong generalization ability. We anticipate that this dataset will serve as a cornerstone for future advancements in specialized medical image editing.",
      "authors": [
        "Yongfan Lai",
        "Wen Qian",
        "Bo Liu",
        "Hongyan Li",
        "Hao Luo",
        "Fan Wang",
        "Bohan Zhuang",
        "Shenda Hong"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-10 09:37:05+00:00",
      "link": "https://arxiv.org/pdf/2602.09587v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09581v1",
      "title": "Mitigating the Likelihood Paradox in Flow-based OOD Detection via Entropy Manipulation",
      "abstract": "Deep generative models that can tractably compute input likelihoods, including normalizing flows, often assign unexpectedly high likelihoods to out-of-distribution (OOD) inputs. We mitigate this likelihood paradox by manipulating input entropy based on semantic similarity, applying stronger perturbations to inputs that are less similar to an in-distribution memory bank. We provide a theoretical analysis showing that entropy control increases the expected log-likelihood gap between in-distribution and OOD samples in favor of the in-distribution, and we explain why the procedure works without any additional training of the density model. We then evaluate our method against likelihood-based OOD detectors on standard benchmarks and find consistent AUROC improvements over baselines, supporting our explanation.",
      "authors": [
        "Donghwan Kim",
        "Hyunsoo Yoon"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-10 09:31:03+00:00",
      "link": "https://arxiv.org/pdf/2602.09581v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09574v1",
      "title": "Aligning Tree-Search Policies with Fixed Token Budgets in Test-Time Scaling of LLMs",
      "abstract": "Tree-search decoding is an effective form of test-time scaling for large language models (LLMs), but real-world deployment imposes a fixed per-query token budget that varies across settings. Existing tree-search policies are largely budget-agnostic, treating the budget as a termination condition, which can lead to late-stage over-branching or premature termination. We propose {Budget-Guided MCTS} (BG-MCTS), a tree-search decoding algorithm that aligns its search policy with the remaining token budget: it starts with broad exploration, then prioritizes refinement and answer completion as the budget depletes while reducing late-stage branching from shallow nodes. BG-MCTS consistently outperforms budget-agnostic tree-search baselines across different budgets on MATH500 and AIME24/25 with open-weight LLMs.",
      "authors": [
        "Sora Miyamoto",
        "Daisuke Oba",
        "Naoaki Okazaki"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-10 09:23:26+00:00",
      "link": "https://arxiv.org/pdf/2602.09574v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09572v2",
      "title": "Predictive Query Language: A Domain-Specific Language for Predictive Modeling on Relational Databases",
      "abstract": "The purpose of predictive modeling on relational data is to predict future or missing values in a relational database, for example, future purchases of a user, risk of readmission of the patient, or the likelihood that a financial transaction is fraudulent. Typically powered by machine learning methods, predictive models are used in recommendations, financial fraud detection, supply chain optimization, and other systems, providing billions of predictions every day. However, training a machine learning model requires manual work to extract the required training examples - prediction entities and target labels - from the database, which is slow, laborious, and prone to mistakes. Here, we present the Predictive Query Language (PQL), an SQL-inspired declarative language for defining predictive tasks on relational databases. PQL allows specifying a predictive task in a single declarative query, enabling the automatic computation of training labels for a large variety of machine learning tasks, such as regression, classification, time-series forecasting, and recommender systems. PQL is already successfully integrated and used in a collection of use cases as part of a predictive AI platform. The versatility of the language can be demonstrated through its many ongoing use cases, including financial fraud, item recommendations, and workload prediction. We demonstrate its versatile design through two implementations; one for small-scale, low-latency use and one that can handle large-scale databases.",
      "authors": [
        "Vid Kocijan",
        "Jinu Sunil",
        "Jan Eric Lenssen",
        "Viman Deb",
        "Xinwei Xe",
        "Federico Reyes Gomez",
        "Matthias Fey",
        "Jure Leskovec"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-10 09:22:17+00:00",
      "link": "https://arxiv.org/pdf/2602.09572v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09570v1",
      "title": "LEMUR: A Corpus for Robust Fine-Tuning of Multilingual Law Embedding Models for Retrieval",
      "abstract": "Large language models (LLMs) are increasingly used to access legal information. Yet, their deployment in multilingual legal settings is constrained by unreliable retrieval and the lack of domain-adapted, open-embedding models. In particular, existing multilingual legal corpora are not designed for semantic retrieval, and PDF-based legislative sources introduce substantial noise due to imperfect text extraction. To address these challenges, we introduce LEMUR, a large-scale multilingual corpus of EU environmental legislation constructed from 24,953 official EUR-Lex PDF documents covering 25 languages. We quantify the fidelity of PDF-to-text conversion by measuring lexical consistency against authoritative HTML versions using the Lexical Content Score (LCS). Building on LEMUR, we fine-tune three state-of-the-art multilingual embedding models using contrastive objectives in both monolingual and bilingual settings, reflecting realistic legal-retrieval scenarios. Experiments across low- and high-resource languages demonstrate that legal-domain fine-tuning consistently improves Top-k retrieval accuracy relative to strong baselines, with particularly pronounced gains for low-resource languages. Cross-lingual evaluations show that these improvements transfer to unseen languages, indicating that fine-tuning primarily enhances language-independent, content-level legal representations rather than language-specific cues. We publish code\\footnote{\\href{https://github.com/nargesbh/eur_lex}{GitHub Repository}} and data\\footnote{\\href{https://huggingface.co/datasets/G4KMU/LEMUR}{Hugging Face Dataset}}.",
      "authors": [
        "Narges Baba Ahmadi",
        "Jan Strich",
        "Martin Semmann",
        "Chris Biemann"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2026-02-10 09:20:24+00:00",
      "link": "https://arxiv.org/pdf/2602.09570v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09569v1",
      "title": "Training deep physical neural networks with local physical information bottleneck",
      "abstract": "Deep learning has revolutionized modern society but faces growing energy and latency constraints. Deep physical neural networks (PNNs) are interconnected computing systems that directly exploit analog dynamics for energy-efficient, ultrafast AI execution. Realizing this potential, however, requires universal training methods tailored to physical intricacies. Here, we present the Physical Information Bottleneck (PIB), a general and efficient framework that integrates information theory and local learning, enabling deep PNNs to learn under arbitrary physical dynamics. By allocating matrix-based information bottlenecks to each unit, we demonstrate supervised, unsupervised, and reinforcement learning across electronic memristive chips and optical computing platforms. PIB also adapts to severe hardware faults and allows for parallel training via geographically distributed resources. Bypassing auxiliary digital models and contrastive measurements, PIB recasts PNN training as an intrinsic, scalable information-theoretic process compatible with diverse physical substrates.",
      "authors": [
        "Hao Wang",
        "Ziao Wang",
        "Xiangpeng Liang",
        "Han Zhao",
        "Jianqi Hu",
        "Junjie Jiang",
        "Xing Fu",
        "Jianshi Tang",
        "Huaqiang Wu",
        "Sylvain Gigan",
        "Qiang Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "physics.app-ph"
      ],
      "published": "2026-02-10 09:20:12+00:00",
      "link": "https://arxiv.org/pdf/2602.09569v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09566v1",
      "title": "ECG-IMN: Interpretable Mesomorphic Neural Networks for 12-Lead Electrocardiogram Interpretation",
      "abstract": "Deep learning has achieved expert-level performance in automated electrocardiogram (ECG) diagnosis, yet the \"black-box\" nature of these models hinders their clinical deployment. Trust in medical AI requires not just high accuracy but also transparency regarding the specific physiological features driving predictions. Existing explainability methods for ECGs typically rely on post-hoc approximations (e.g., Grad-CAM and SHAP), which can be unstable, computationally expensive, and unfaithful to the model's actual decision-making process. In this work, we propose the ECG-IMN, an Interpretable Mesomorphic Neural Network tailored for high-resolution 12-lead ECG classification. Unlike standard classifiers, the ECG-IMN functions as a hypernetwork: a deep convolutional backbone generates the parameters of a strictly linear model specific to each input sample. This architecture enforces intrinsic interpretability, as the decision logic is mathematically transparent and the generated weights (W) serve as exact, high-resolution feature attribution maps. We introduce a transition decoder that effectively maps latent features to sample-wise weights, enabling precise localization of pathological evidence (e.g., ST-elevation, T-wave inversion) in both time and lead dimensions. We evaluate our approach on the PTB-XL dataset for classification tasks, demonstrating that the ECG-IMN achieves competitive predictive performance (AUROC comparable to black-box baselines) while providing faithful, instance-specific explanations. By explicitly decoupling parameter generation from prediction execution, our framework bridges the gap between deep learning capability and clinical trustworthiness, offering a principled path toward \"white-box\" cardiac diagnostics.",
      "authors": [
        "Vajira Thambawita",
        "Jonas L. Isaksen",
        "Jørgen K. Kanters",
        "Hugo L. Hammer",
        "Pål Halvorsen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ME"
      ],
      "published": "2026-02-10 09:17:29+00:00",
      "link": "https://arxiv.org/pdf/2602.09566v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09555v2",
      "title": "Advancing Block Diffusion Language Models for Test-Time Scaling",
      "abstract": "Recent advances in block diffusion language models have demonstrated competitive performance and strong scalability on reasoning tasks. However, existing BDLMs have limited exploration under the test-time scaling setting and face more severe decoding challenges in long Chain-of-Thought reasoning, particularly in balancing the decoding speed and effectiveness. In this work, we propose a unified framework for test-time scaling in BDLMs that introduces adaptivity in both decoding and block-wise generation. At the decoding level, we propose Bounded Adaptive Confidence Decoding (BACD), a difficulty-aware sampling strategy that dynamically adjusts denoising based on model confidence, accelerating inference while controlling error accumulation. Beyond step-wise adaptivity, we introduce Think Coarse, Critic Fine (TCCF), a test-time scaling paradigm that allocates large block sizes to exploratory reasoning and smaller block sizes to refinement, achieving an effective efficiency-effectiveness balance. To enable efficient and effective decoding with a large block size, we adopt Progressive Block Size Extension, which mitigates performance degradation when scaling block sizes. Extensive experiments show that applying BACD and TCCF to TDAR-8B yields significant improvements over strong baselines such as TraDo-8B (2.26x speedup, +11.2 points on AIME24). These results mark an important step toward unlocking the potential of BDLMs for test-time scaling in complex reasoning tasks.",
      "authors": [
        "Yi Lu",
        "Deyang Kong",
        "Jianing Wang",
        "Linsen Guo",
        "Xue Wang",
        "Qi Guo",
        "Tao Gui",
        "Xuanjing Huang",
        "Wei Ye",
        "Shikun Zhang",
        "Wei Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 09:05:07+00:00",
      "link": "https://arxiv.org/pdf/2602.09555v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09552v1",
      "title": "Comprehensive Comparison of RAG Methods Across Multi-Domain Conversational QA",
      "abstract": "Conversational question answering increasingly relies on retrieval-augmented generation (RAG) to ground large language models (LLMs) in external knowledge. Yet, most existing studies evaluate RAG methods in isolation and primarily focus on single-turn settings. This paper addresses the lack of a systematic comparison of RAG methods for multi-turn conversational QA, where dialogue history, coreference, and shifting user intent substantially complicate retrieval. We present a comprehensive empirical study of vanilla and advanced RAG methods across eight diverse conversational QA datasets spanning multiple domains. Using a unified experimental setup, we evaluate retrieval quality and answer generation using generator and retrieval metrics, and analyze how performance evolves across conversation turns. Our results show that robust yet straightforward methods, such as reranking, hybrid BM25, and HyDE, consistently outperform vanilla RAG. In contrast, several advanced techniques fail to yield gains and can even degrade performance below the No-RAG baseline. We further demonstrate that dataset characteristics and dialogue length strongly influence retrieval effectiveness, explaining why no single RAG strategy dominates across settings. Overall, our findings indicate that effective conversational RAG depends less on method complexity than on alignment between the retrieval strategy and the dataset structure. We publish the code used.\\footnote{\\href{https://github.com/Klejda-A/exp-rag.git}{GitHub Repository}}",
      "authors": [
        "Klejda Alushi",
        "Jan Strich",
        "Chris Biemann",
        "Martin Semmann"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2026-02-10 08:59:23+00:00",
      "link": "https://arxiv.org/pdf/2602.09552v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09548v1",
      "title": "ReSIM: Re-ranking Binary Similarity Embeddings to Improve Function Search Performance",
      "abstract": "Binary Function Similarity (BFS), the problem of determining whether two binary functions originate from the same source code, has been extensively studied in recent research across security, software engineering, and machine learning communities. This interest arises from its central role in developing vulnerability detection systems, copyright infringement analysis, and malware phylogeny tools. Nearly all binary function similarity systems embed assembly functions into real-valued vectors, where similar functions map to points that lie close to each other in the metric space. These embeddings enable function search: a query function is embedded and compared against a database of candidate embeddings to retrieve the most similar matches.   Despite their effectiveness, such systems rely on bi-encoder architectures that embed functions independently, limiting their ability to capture cross-function relationships and similarities. To address this limitation, we introduce ReSIM, a novel and enhanced function search system that complements embedding-based search with a neural re-ranker. Unlike traditional embedding models, our reranking module jointly processes query-candidate pairs to compute ranking scores based on their mutual representation, allowing for more accurate similarity assessment. By re-ranking the top results from embedding-based retrieval, ReSIM leverages fine-grained relation information that bi-encoders cannot capture.   We evaluate ReSIM across seven embedding models on two benchmark datasets, demonstrating consistent improvements in search effectiveness, with average gains of 21.7% in terms of nDCG and 27.8% in terms of Recall.",
      "authors": [
        "Gianluca Capozzi",
        "Anna Paola Giancaspro",
        "Fabio Petroni",
        "Leonardo Querzoni",
        "Giuseppe Antonio Di Luna"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR"
      ],
      "published": "2026-02-10 08:57:49+00:00",
      "link": "https://arxiv.org/pdf/2602.09548v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09538v1",
      "title": "UniARM: Towards a Unified Autoregressive Reward Model for Multi-Objective Test-Time Alignment",
      "abstract": "Multi-objective alignment aims to align LLM responses with multiple human preference objectives. Among existing methods, guiding the generation of frozen LLMs through autoregressive reward models (ARMs) to accomplish multi-objective test-time alignment is a low-cost solution. However, these methods typically rely on independent parameters for each preference objective, either by training ARMs independently across preference dimensions, which neglects interactions among preference features, or by training a single ARM with separate feature extraction modules for each preference, which can cause feature entanglement. Both strategies can result in misalignment between generated outputs and user preferences. To address this limitation, we propose Preference-Modulated \\& Shared Low-Rank Adaptation (MoSLoRA) for ARM training, which first extracts shared features via a preference-agnostic module and then applies affine transformations to shared features via a preference modulation module conditioned on mixed preference vectors. This design mitigates feature entanglement and enables precise control over preference trade-offs during inference. Building on this, we introduce the Unified Autoregressive Reward Model (UniARM), a novel framework for multi-objective test-time alignment. UniARM jointly models all preference dimensions in a single parameter space, eliminating the need for independent parameters for each preference objective. es on larger-scale LLMs, enhancing its practical usability.",
      "authors": [
        "Hongyan Xie",
        "Yikun Ban",
        "Ruiyu Fang",
        "Zixuan Huang",
        "Deqing Wang",
        "Jianxin Li",
        "Yitong Yao",
        "Chao Wang",
        "Shuangyong Song"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 08:49:28+00:00",
      "link": "https://arxiv.org/pdf/2602.09538v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09530v1",
      "title": "Learning to Discover Iterative Spectral Algorithms",
      "abstract": "We introduce AutoSpec, a neural network framework for discovering iterative spectral algorithms for large-scale numerical linear algebra and numerical optimization. Our self-supervised models adapt to input operators using coarse spectral information (e.g., eigenvalue estimates and residual norms), and they predict recurrence coefficients for computing or applying a matrix polynomial tailored to a downstream task. The effectiveness of AutoSpec relies on three ingredients: an architecture whose inference pass implements short, executable numerical linear algebra recurrences; efficient training on small synthetic problems with transfer to large-scale real-world operators; and task-defined objectives that enforce the desired approximation or preconditioning behavior across the range of spectral profiles represented in the training set. We apply AutoSpec to discovering algorithms for representative numerical linear algebra tasks: accelerating matrix-function approximation; accelerating sparse linear solvers; and spectral filtering/preconditioning for eigenvalue computations. On real-world matrices, the learned procedures deliver orders-of-magnitude improvements in accuracy and/or reductions in iteration count, relative to basic baselines. We also find clear connections to classical theory: the induced polynomials often exhibit near-equiripple, near-minimax behavior characteristic of Chebyshev polynomials.",
      "authors": [
        "Zihang Liu",
        "Oleg Balabanov",
        "Yaoqing Yang",
        "Michael W. Mahoney"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "published": "2026-02-10 08:39:59+00:00",
      "link": "https://arxiv.org/pdf/2602.09530v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09520v1",
      "title": "Rashomon Sets and Model Multiplicity in Federated Learning",
      "abstract": "The Rashomon set captures the collection of models that achieve near-identical empirical performance yet may differ substantially in their decision boundaries. Understanding the differences among these models, i.e., their multiplicity, is recognized as a crucial step toward model transparency, fairness, and robustness, as it reveals decision boundaries instabilities that standard metrics obscure. However, the existing definitions of Rashomon set and multiplicity metrics assume centralized learning and do not extend naturally to decentralized, multi-party settings like Federated Learning (FL). In FL, multiple clients collaboratively train models under a central server's coordination without sharing raw data, which preserves privacy but introduces challenges from heterogeneous client data distribution and communication constraints. In this setting, the choice of a single best model may homogenize predictive behavior across diverse clients, amplify biases, or undermine fairness guarantees. In this work, we provide the first formalization of Rashomon sets in FL.First, we adapt the Rashomon set definition to FL, distinguishing among three perspectives: (I) a global Rashomon set defined over aggregated statistics across all clients, (II) a t-agreement Rashomon set representing the intersection of local Rashomon sets across a fraction t of clients, and (III) individual Rashomon sets specific to each client's local distribution.Second, we show how standard multiplicity metrics can be estimated under FL's privacy constraints. Finally, we introduce a multiplicity-aware FL pipeline and conduct an empirical study on standard FL benchmark datasets. Our results demonstrate that all three proposed federated Rashomon set definitions offer valuable insights, enabling clients to deploy models that better align with their local data, fairness considerations, and practical requirements.",
      "authors": [
        "Xenia Heilmann",
        "Luca Corbucci",
        "Mattia Cerrato"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published": "2026-02-10 08:25:35+00:00",
      "link": "https://arxiv.org/pdf/2602.09520v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09517v1",
      "title": "Knowledge Integration Decay in Search-Augmented Reasoning of Large Language Models",
      "abstract": "Modern Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks by employing search-augmented reasoning to incorporate external knowledge into long chains of thought. However, we identify a critical yet underexplored bottleneck in this paradigm, termed Knowledge Integration Decay (KID). Specifically, we observe that as the length of reasoning generated before search grows, models increasingly fail to integrate retrieved evidence into subsequent reasoning steps, limiting performance even when relevant information is available. To address this, we propose Self-Anchored Knowledge Encoding (SAKE), a training-free inference-time strategy designed to stabilize knowledge utilization. By anchoring retrieved knowledge at both the beginning and end of the reasoning process, SAKE prevents it from being overshadowed by prior context, thereby preserving its semantic integrity. Extensive experiments on multi-hop QA and complex reasoning benchmarks demonstrate that SAKE significantly mitigates KID and improves performance, offering a lightweight yet effective solution for knowledge integration in agentic LLMs.",
      "authors": [
        "Sangwon Yu",
        "Ik-hwan Kim",
        "Donghun Kang",
        "Bongkyu Hwang",
        "Junhwa Choi",
        "Suk-hoon Jung",
        "Seungki Hong",
        "Taehee Lee",
        "Sungroh Yoon"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 08:20:26+00:00",
      "link": "https://arxiv.org/pdf/2602.09517v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09516v1",
      "title": "The CLEF-2026 CheckThat! Lab: Advancing Multilingual Fact-Checking",
      "abstract": "The CheckThat! lab aims to advance the development of innovative technologies combating disinformation and manipulation efforts in online communication across a multitude of languages and platforms. While in early editions the focus has been on core tasks of the verification pipeline (check-worthiness, evidence retrieval, and verification), in the past three editions, the lab added additional tasks linked to the verification process. In this year's edition, the verification pipeline is at the center again with the following tasks: Task 1 on source retrieval for scientific web claims (a follow-up of the 2025 edition), Task 2 on fact-checking numerical and temporal claims, which adds a reasoning component to the 2025 edition, and Task 3, which expands the verification pipeline with generation of full-fact-checking articles. These tasks represent challenging classification and retrieval problems as well as generation challenges at the document and span level, including multilingual settings.",
      "authors": [
        "Julia Maria Struß",
        "Sebastian Schellhammer",
        "Stefan Dietze",
        "Venktesh V",
        "Vinay Setty",
        "Tanmoy Chakraborty",
        "Preslav Nakov",
        "Avishek Anand",
        "Primakov Chungkham",
        "Salim Hafid",
        "Dhruv Sahnan",
        "Konstantin Todorov"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 08:20:18+00:00",
      "link": "https://arxiv.org/pdf/2602.09516v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09509v2",
      "title": "Beyond Student: An Asymmetric Network for Neural Network Inheritance",
      "abstract": "Knowledge Distillation (KD) has emerged as a powerful technique for model compression, enabling lightweight student networks to benefit from the performance of redundant teacher networks. However, the inherent capacity gap often limits the performance of student networks. Inspired by the expressiveness of pretrained teacher networks, a compelling research question arises: is there a type of network that can not only inherit the teacher's structure but also maximize the inheritance of its knowledge? Furthermore, how does the performance of such an inheriting network compare to that of student networks, all benefiting from the same teacher network? To further explore this question, we propose InherNet, a neural network inheritance method that performs asymmetric low-rank decomposition on the teacher's weights and reconstructs a lightweight yet expressive network without significant architectural disruption. By leveraging Singular Value Decomposition (SVD) for initialization to ensure the inheritance of principal knowledge, InherNet effectively balances depth, width, and compression efficiency. Experimental results across unimodal and multimodal tasks demonstrate that InherNet achieves higher performance compared to student networks of similar parameter sizes. Our findings reveal a promising direction for future research in efficient model compression beyond traditional distillation.",
      "authors": [
        "Yiyun Zhou",
        "Jingwei Shi",
        "Mingjing Xu",
        "Zhonghua Jiang",
        "Jingyuan Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 08:09:10+00:00",
      "link": "https://arxiv.org/pdf/2602.09509v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09501v1",
      "title": "Where-to-Unmask: Ground-Truth-Guided Unmasking Order Learning for Masked Diffusion Language Models",
      "abstract": "Masked Diffusion Language Models (MDLMs) generate text by iteratively filling masked tokens, requiring two coupled decisions at each step: which positions to unmask (where-to-unmask) and which tokens to place (what-to-unmask). While standard MDLM training directly optimizes token prediction (what-to-unmask), inference-time unmasking orders (where-to-unmask) are typically determined by heuristic confidence measures or trained through reinforcement learning with costly on-policy rollouts. To address this, we introduce Gt-Margin, a position-wise score derived from ground-truth tokens, defined as the probability margin between the correct token and its strongest alternative. Gt-Margin yields an oracle unmasking order that prioritizes easier positions first under each partially masked state. We demonstrate that leveraging this oracle unmasking order significantly enhances final generation quality, particularly on logical reasoning benchmarks. Building on this insight, we train a supervised unmasking planner via learning-to-rank to imitate the oracle ordering from masked contexts. The resulting planner integrates into standard MDLM sampling to select where-to-unmask, improving reasoning accuracy without modifying the token prediction model.",
      "authors": [
        "Hikaru Asano",
        "Tadashi Kozuno",
        "Kuniaki Saito",
        "Yukino Baba"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 07:56:46+00:00",
      "link": "https://arxiv.org/pdf/2602.09501v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09499v1",
      "title": "Computationally Efficient Replicable Learning of Parities",
      "abstract": "We study the computational relationship between replicability (Impagliazzo et al. [STOC `22], Ghazi et al. [NeurIPS `21]) and other stability notions. Specifically, we focus on replicable PAC learning and its connections to differential privacy (Dwork et al. [TCC 2006]) and to the statistical query (SQ) model (Kearns [JACM `98]). Statistically, it was known that differentially private learning and replicable learning are equivalent and strictly more powerful than SQ-learning. Yet, computationally, all previously known efficient (i.e., polynomial-time) replicable learning algorithms were confined to SQ-learnable tasks or restricted distributions, in contrast to differentially private learning.   Our main contribution is the first computationally efficient replicable algorithm for realizable learning of parities over arbitrary distributions, a task that is known to be hard in the SQ-model, but possible under differential privacy. This result provides the first evidence that efficient replicable learning over general distributions strictly extends efficient SQ-learning, and is closer in power to efficient differentially private learning, despite computational separations between replicability and privacy. Our main building block is a new, efficient, and replicable algorithm that, given a set of vectors, outputs a subspace of their linear span that covers most of them.",
      "authors": [
        "Moshe Noivirt",
        "Jessica Sorrell",
        "Eliad Tsfadia"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published": "2026-02-10 07:53:46+00:00",
      "link": "https://arxiv.org/pdf/2602.09499v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11204v1",
      "title": "Zero-Sacrifice Persistent-Robustness Adversarial Defense for Pre-Trained Encoders",
      "abstract": "The widespread use of publicly available pre-trained encoders from self-supervised learning (SSL) has exposed a critical vulnerability: their susceptibility to downstream-agnostic adversarial examples (DAEs), which are crafted without knowledge of the downstream tasks but capable of misleading downstream models. While several defense methods have been explored recently, they rely primarily on task-specific adversarial fine-tuning, which inevitably limits generalizability and causes catastrophic forgetting and deteriorates benign performance. Different with previous works, we propose a more rigorous defense goal that requires only a single tuning for diverse downstream tasks to defend against DAEs and preserve benign performance. To achieve this defense goal, we introduce Zero-Sacrifice Persistent-Robustness Adversarial Defense (ZePAD), which is inspired by the inherent sensitivity of neural networks to data characteristics. Specifically, ZePAD is a dual-branch structure, which consists of a Multi-Pattern Adversarial Enhancement Branch (MPAE-Branch) that uses two adversarially fine-tuned encoders to strengthen adversarial resistance. The Benign Memory Preservation Branch (BMP-Branch) is trained on local data to ensure adversarial robustness does not compromise benign performance. Surprisingly, we find that ZePAD can directly detect DAEs by evaluating branch confidence, without introducing any adversarial exsample identification task during training. Notably, by enriching feature diversity, our method enables a single adversarial fine-tuning to defend against DAEs across downstream tasks, thereby achieving persistent robustness. Extensive experiments on 11 SSL methods and 6 datasets validate its effectiveness. In certain cases, it achieves a 29.20% improvement in benign performance and a 73.86% gain in adversarial robustness, highlighting its zero-sacrifice property.",
      "authors": [
        "Zhuxin Lei",
        "Ziyuan Yang",
        "Yi Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-10 07:41:34+00:00",
      "link": "https://arxiv.org/pdf/2602.11204v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09489v1",
      "title": "Computing Conditional Shapley Values Using Tabular Foundation Models",
      "abstract": "Shapley values have become a cornerstone of explainable AI, but they are computationally expensive to use, especially when features are dependent. Evaluating them requires approximating a large number of conditional expectations, either via Monte Carlo integration or regression. Until recently it has not been possible to fully exploit deep learning for the regression approach, because retraining for each conditional expectation takes too long. Tabular foundation models such as TabPFN overcome this computational hurdle by leveraging in-context learning, so each conditional expectation can be approximated without any re-training. In this paper, we compute Shapley values with multiple variants of TabPFN and compare their performance with state-of-the-art methods on both simulated and real datasets. In most cases, TabPFN yields the best performance; where it does not, it is only marginally worse than the best method, at a fraction of the runtime. We discuss further improvements and how tabular foundation models can be better adapted specifically for conditional Shapley value estimation.",
      "authors": [
        "Lars Henry Berge Olsen",
        "Dennis Christensen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-10 07:36:41+00:00",
      "link": "https://arxiv.org/pdf/2602.09489v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09487v1",
      "title": "Adaptive recurrent flow map operator learning for reaction diffusion dynamics",
      "abstract": "Reaction-diffusion (RD) equations underpin pattern formation across chemistry, biology, and physics, yet learning stable operators that forecast their long-term dynamics from data remains challenging. Neural-operator surrogates provide resolution-robust prediction, but autoregressive rollouts can drift due to the accumulation of error, and out-of-distribution (OOD) initial conditions often degrade accuracy. Physics-based numerical residual objectives can regularize operator learning, although they introduce additional assumptions, sensitivity to discretization and loss design, and higher training cost. Here we develop a purely data-driven operator learner with adaptive recurrent training (DDOL-ART) using a robust recurrent strategy with lightweight validation milestones that early-exit unproductive rollout segments and redirect optimization. Trained only on a single in-distribution toroidal Gaussian family over short horizons, DDOL-ART learns one-step operators that remain stable under long rollouts and generalize zero-shot to strong morphology shifts across FitzHugh-Nagumo (FN), Gray-Scott (GS), and Lambda-Omega (LO) systems. Across these benchmarks, DDOL-ART delivers a strong accuracy and cost trade-off. It is several-fold faster than a physics-based numerical-loss operator learner (NLOL) under matched settings, and it remains competitive on both in-distribution stability and OOD robustness. Training-dynamics diagnostics show that adaptivity strengthens the correlation between validation error and OOD test error performance, acting as a feedback controller that limits optimization drift. Our results indicate that feedback-controlled recurrent training of DDOL-ART generates robust flow-map surrogates without PDE residuals, while simultaneously maintaining competitiveness with NLOL at significantly reduced training costs.",
      "authors": [
        "Huseyin Tunc"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-02-10 07:33:13+00:00",
      "link": "https://arxiv.org/pdf/2602.09487v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09469v1",
      "title": "NOWJ @BioCreative IX ToxHabits: An Ensemble Deep Learning Approach for Detecting Substance Use and Contextual Information in Clinical Texts",
      "abstract": "Extracting drug use information from unstructured Electronic Health Records remains a major challenge in clinical Natural Language Processing. While Large Language Models demonstrate advancements, their use in clinical NLP is limited by concerns over trust, control, and efficiency. To address this, we present NOWJ submission to the ToxHabits Shared Task at BioCreative IX. This task targets the detection of toxic substance use and contextual attributes in Spanish clinical texts, a domain-specific, low-resource setting. We propose a multi-output ensemble system tackling both Subtask 1 - ToxNER and Subtask 2 - ToxUse. Our system integrates BETO with a CRF layer for sequence labeling, employs diverse training strategies, and uses sentence filtering to boost precision. Our top run achieved 0.94 F1 and 0.97 precision for Trigger Detection, and 0.91 F1 for Argument Detection.",
      "authors": [
        "Huu-Huy-Hoang Tran",
        "Gia-Bao Duong",
        "Quoc-Viet-Anh Tran",
        "Thi-Hai-Yen Vuong",
        "Hoang-Quynh Le"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-10 07:04:44+00:00",
      "link": "https://arxiv.org/pdf/2602.09469v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09464v1",
      "title": "AlgoVeri: An Aligned Benchmark for Verified Code Generation on Classical Algorithms",
      "abstract": "Vericoding refers to the generation of formally verified code from rigorous specifications. Recent AI models show promise in vericoding, but a unified methodology for cross-paradigm evaluation is lacking. Existing benchmarks test only individual languages/tools (e.g., Dafny, Verus, and Lean) and each covers very different tasks, so the performance numbers are not directly comparable. We address this gap with AlgoVeri, a benchmark that evaluates vericoding of $77$ classical algorithms in Dafny, Verus, and Lean. By enforcing identical functional contracts, AlgoVeri reveals critical capability gaps in verification systems. While frontier models achieve tractable success in Dafny ($40.3$% for Gemini-3 Flash), where high-level abstractions and SMT automation simplify the workflow, performance collapses under the systems-level memory constraints of Verus ($24.7$%) and the explicit proof construction required by Lean (7.8%). Beyond aggregate metrics, we uncover a sharp divergence in test-time compute dynamics: Gemini-3 effectively utilizes iterative repair to boost performance (e.g., tripling pass rates in Dafny), whereas GPT-OSS saturates early. Finally, our error analysis shows that language design affects the refinement trajectory: while Dafny allows models to focus on logical correctness, Verus and Lean trap models in persistent syntactic and semantic barriers. All data and evaluation code can be found at https://github.com/haoyuzhao123/algoveri.",
      "authors": [
        "Haoyu Zhao",
        "Ziran Yang",
        "Jiawei Li",
        "Deyuan He",
        "Zenan Li",
        "Chi Jin",
        "Venugopal V. Veeravalli",
        "Aarti Gupta",
        "Sanjeev Arora"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-10 06:58:26+00:00",
      "link": "https://arxiv.org/pdf/2602.09464v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09457v1",
      "title": "From Average Sensitivity to Small-Loss Regret Bounds under Random-Order Model",
      "abstract": "We study online learning in the random-order model, where the multiset of loss functions is chosen adversarially but revealed in a uniformly random order. Building on the batch-to-online conversion by Dong and Yoshida (2023), we show that if an offline algorithm admits a $(1+\\varepsilon)$-approximation guarantee and the effect of $\\varepsilon$ on its average sensitivity is characterized by a function $\\varphi(\\varepsilon)$, then an adaptive choice of $\\varepsilon$ yields a small-loss regret bound of $\\tilde O(\\varphi^{\\star}(\\mathrm{OPT}_T))$, where $\\varphi^{\\star}$ is the concave conjugate of $\\varphi$, $\\mathrm{OPT}_T$ is the offline optimum over $T$ rounds, and $\\tilde O$ hides polylogarithmic factors in $T$. Our method requires no regularity assumptions on loss functions, such as smoothness, and can be viewed as a generalization of the AdaGrad-style tuning applied to the approximation parameter $\\varepsilon$. Our result recovers and strengthens the $(1+\\varepsilon)$-approximate regret bounds of Dong and Yoshida (2023) and yields small-loss regret bounds for online $k$-means clustering, low-rank approximation, and regression. We further apply our framework to online submodular function minimization using $(1\\pm\\varepsilon)$-cut sparsifiers of submodular hypergraphs, obtaining a small-loss regret bound of $\\tilde O(n^{3/4}(1 + \\mathrm{OPT}_T^{3/4}))$, where $n$ is the ground-set size. Our approach sheds light on the power of sparsification and related techniques in establishing small-loss regret bounds in the random-order model.",
      "authors": [
        "Shinsaku Sakaue",
        "Yuichi Yoshida"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.DS",
        "cs.LG"
      ],
      "published": "2026-02-10 06:46:01+00:00",
      "link": "https://arxiv.org/pdf/2602.09457v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09448v1",
      "title": "The Wisdom of Many Queries: Complexity-Diversity Principle for Dense Retriever Training",
      "abstract": "Prior work reports conflicting results on query diversity in synthetic data generation for dense retrieval. We identify this conflict and design Q-D metrics to quantify diversity's impact, making the problem measurable. Through experiments on 4 benchmark types (31 datasets), we find query diversity especially benefits multi-hop retrieval. Deep analysis on multi-hop data reveals that diversity benefit correlates strongly with query complexity ($r$$\\geq$0.95, $p$$<$0.05 in 12/14 conditions), measured by content words (CW). We formalize this as the Complexity-Diversity Principle (CDP): query complexity determines optimal diversity. CDP provides actionable thresholds (CW$>$10: use diversity; CW$<$7: avoid it). Guided by CDP, we propose zero-shot multi-query synthesis for multi-hop tasks, achieving state-of-the-art performance.",
      "authors": [
        "Xincan Feng",
        "Noriki Nishida",
        "Yusuke Sakai",
        "Yuji Matsumoto"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "published": "2026-02-10 06:33:10+00:00",
      "link": "https://arxiv.org/pdf/2602.09448v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09444v1",
      "title": "Conceptual Cultural Index: A Metric for Cultural Specificity via Relative Generality",
      "abstract": "Large language models (LLMs) are increasingly deployed in multicultural settings; however, systematic evaluation of cultural specificity at the sentence level remains underexplored. We propose the Conceptual Cultural Index (CCI), which estimates cultural specificity at the sentence level. CCI is defined as the difference between the generality estimate within the target culture and the average generality estimate across other cultures. This formulation enables users to operationally control the scope of culture via comparison settings and provides interpretability, since the score derives from the underlying generality estimates. We validate CCI on 400 sentences (200 culture-specific and 200 general), and the resulting score distribution exhibits the anticipated pattern: higher for culture-specific sentences and lower for general ones. For binary separability, CCI outperforms direct LLM scoring, yielding more than a 10-point improvement in AUC for models specialized to the target culture. Our code is available at https://github.com/IyatomiLab/CCI .",
      "authors": [
        "Takumi Ohashi",
        "Hitoshi Iyatomi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-10 06:29:33+00:00",
      "link": "https://arxiv.org/pdf/2602.09444v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09443v1",
      "title": "P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads",
      "abstract": "The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.",
      "authors": [
        "Yun Luo",
        "Futing Wang",
        "Qianjia Cheng",
        "Fangchen Yu",
        "Haodi Lei",
        "Jianhao Yan",
        "Chenxi Li",
        "Jiacheng Chen",
        "Yufeng Zhao",
        "Haiyuan Wan",
        "Yuchen Zhang",
        "Shenghe Zheng",
        "Junchi Yao",
        "Qingyang Zhang",
        "Haonan He",
        "Wenxuan Zeng",
        "Li Sheng",
        "Chengxing Xie",
        "Yuxin Zuo",
        "Yizhuo Li",
        "Yulun Wu",
        "Rui Huang",
        "Dongzhan Zhou",
        "Kai Chen",
        "Yu Qiao",
        "Lei Bai",
        "Yu Cheng",
        "Ning Ding",
        "Bowen Zhou",
        "Peng Ye",
        "Ganqu Cui"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-10 06:28:08+00:00",
      "link": "https://arxiv.org/pdf/2602.09443v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09430v1",
      "title": "Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments",
      "abstract": "Robotic laboratories play a critical role in autonomous scientific discovery by enabling scalable, continuous experimental execution. Recent vision-language-action (VLA) models offer a promising foundation for robotic laboratories. However, scientific experiments typically involve long-horizon tasks composed of multiple atomic tasks, posing a fundamental challenge to existing VLA models. While VLA models fine-tuned for scientific tasks can reliably execute atomic experimental actions seen during training, they often fail to perform composite tasks formed by reordering and composing these known atomic actions. This limitation arises from a distributional mismatch between training-time atomic tasks and inference-time composite tasks, which prevents VLA models from executing necessary transitional operations between atomic tasks. To address this challenge, we propose an Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments. It introduces an LLM-based agentic inference mechanism that intervenes when executing sequential manipulation tasks. By performing explicit transition inference and generating transitional robotic action code, the proposed plugin guides VLA models through missing transitional steps, enabling reliable execution of composite scientific workflows without any additional training. This inference-only intervention makes our method computationally efficient, data-efficient, and well-suited for open-ended and long-horizon robotic laboratory tasks. We build 3D assets of scientific instruments and common scientific operating scenes within an existing simulation environment. In these scenes, we have verified that our method increases the average success rate per atomic task by 42\\% during inference. Furthermore, we show that our method can be easily transferred from the simulation to real scientific laboratories.",
      "authors": [
        "Yiwen Pang",
        "Bo Zhou",
        "Changjin Li",
        "Xuanhao Wang",
        "Shengxiang Xu",
        "Deng-Bao Wang",
        "Min-Ling Zhang",
        "Shimin Di"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2026-02-10 05:50:19+00:00",
      "link": "https://arxiv.org/pdf/2602.09430v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09402v1",
      "title": "Learning with Multiple Correct Answers -- A Trichotomy of Regret Bounds under Different Feedback Models",
      "abstract": "We study an online learning problem with multiple correct answers, where each instance admits a set of valid labels, and in each round the learner must output a valid label for the queried example. This setting is motivated by language generation tasks, in which a prompt may admit many acceptable completions, but not every completion is acceptable. We study this problem under three feedback models. For each model, we characterize the optimal mistake bound in the realizable setting using an appropriate combinatorial dimension. We then establish a trichotomy of regret bounds across the three models in the agnostic setting. Our results also imply sample complexity bounds for the batch setup that depend on the respective combinatorial dimensions.",
      "authors": [
        "Alireza F. Pour",
        "Farnam Mansouri",
        "Shai Ben-David"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 04:17:02+00:00",
      "link": "https://arxiv.org/pdf/2602.09402v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09395v1",
      "title": "Sparse Layer Sharpness-Aware Minimization for Efficient Fine-Tuning",
      "abstract": "Sharpness-aware minimization (SAM) seeks the minima with a flat loss landscape to improve the generalization performance in machine learning tasks, including fine-tuning. However, its extra parameter perturbation step doubles the computation cost, which becomes the bottleneck of SAM in the practical implementation. In this work, we propose an approach SL-SAM to break this bottleneck by introducing the sparse technique to layers. Our key innovation is to frame the dynamic selection of layers for both the gradient ascent (perturbation) and descent (update) steps as a multi-armed bandit problem. At the beginning of each iteration, SL-SAM samples a part of the layers of the model according to the gradient norm to participate in the backpropagation of the following parameter perturbation and update steps, thereby reducing the computation complexity. We then provide the analysis to guarantee the convergence of SL-SAM. In the experiments of fine-tuning models in several tasks, SL-SAM achieves the performances comparable to the state-of-the-art baselines, including a \\#1 rank on LLM fine-tuning. Meanwhile, SL-SAM significantly reduces the ratio of active parameters in backpropagation compared to vanilla SAM (SL-SAM activates 47\\%, 22\\% and 21\\% parameters on the vision, moderate and large language model respectively while vanilla SAM always activates 100\\%), verifying the efficiency of our proposed algorithm.",
      "authors": [
        "Yifei Cheng",
        "Xianglin Yang",
        "Guoxia Wang",
        "Chao Huang",
        "Fei Ma",
        "Dianhai Yu",
        "Xiaochun Cao",
        "Li Shen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 04:05:43+00:00",
      "link": "https://arxiv.org/pdf/2602.09395v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09394v2",
      "title": "The Critical Horizon: Inspection Design Principles for Multi-Stage Operations and Deep Reasoning",
      "abstract": "Manufacturing lines, service journeys, supply chains, and AI reasoning chains share a common challenge: attributing a terminal outcome to the intermediate stage that caused it. We establish an information-theoretic barrier to this credit assignment problem: the signal connecting early steps to final outcomes decays exponentially with depth, creating a critical horizon beyond which reliable learning from endpoint data alone requires exponentially many samples. We prove four results. First, a Signal Decay Bound: sample complexity for attributing outcomes to early stages grows exponentially in the number of intervening steps. Second, Width Limits: parallel rollouts provide only logarithmic relief, with correlation capping the effective number of independent samples. Third, an Objective Mismatch: additive reward aggregation optimizes the wrong quantity when sequential validity requires all steps to be correct. Fourth, Optimal Inspection Design: uniform checkpoint spacing is minimax-optimal under homogeneous signal attenuation, while a greedy algorithm yields optimal non-uniform schedules under heterogeneous attenuation. Together, these results provide a common analytical foundation for inspection design in operations and supervision design in AI.",
      "authors": [
        "Seyed Morteza Emadi"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.IT",
        "cs.LG"
      ],
      "published": "2026-02-10 04:02:29+00:00",
      "link": "https://arxiv.org/pdf/2602.09394v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09388v1",
      "title": "Effective vocabulary expanding of multilingual language models for extremely low-resource languages",
      "abstract": "Multilingual pre-trained language models(mPLMs) offer significant benefits for many low-resource languages. To further expand the range of languages these models can support, many works focus on continued pre-training of these models. However, few works address how to extend mPLMs to low-resource languages that were previously unsupported. To tackle this issue, we expand the model's vocabulary using a target language corpus. We then screen out a subset from the model's original vocabulary, which is biased towards representing the source language(e.g. English), and utilize bilingual dictionaries to initialize the representations of the expanded vocabulary. Subsequently, we continue to pre-train the mPLMs using the target language corpus, based on the representations of these expanded vocabulary. Experimental results show that our proposed method outperforms the baseline, which uses randomly initialized expanded vocabulary for continued pre-training, in POS tagging and NER tasks, achieving improvements by 0.54% and 2.60%, respectively. Furthermore, our method demonstrates high robustness in selecting the training corpora, and the models' performance on the source language does not degrade after continued pre-training.",
      "authors": [
        "Jianyu Zheng"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 03:57:07+00:00",
      "link": "https://arxiv.org/pdf/2602.09388v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09379v2",
      "title": "LingxiDiagBench: A Multi-Agent Framework for Benchmarking LLMs in Chinese Psychiatric Consultation and Diagnosis",
      "abstract": "Mental disorders are highly prevalent worldwide, but the shortage of psychiatrists and the inherent subjectivity of interview-based diagnosis create substantial barriers to timely and consistent mental-health assessment. Progress in AI-assisted psychiatric diagnosis is constrained by the absence of benchmarks that simultaneously provide realistic patient simulation, clinician-verified diagnostic labels, and support for dynamic multi-turn consultation. We present LingxiDiagBench, a large-scale multi-agent benchmark that evaluates LLMs on both static diagnostic inference and dynamic multi-turn psychiatric consultation in Chinese. At its core is LingxiDiag-16K, a dataset of 16,000 EMR-aligned synthetic consultation dialogues designed to reproduce real clinical demographic and diagnostic distributions across 12 ICD-10 psychiatric categories. Through extensive experiments across state-of-the-art LLMs, we establish key findings: (1) although LLMs achieve high accuracy on binary depression--anxiety classification (up to 92.3%), performance deteriorates substantially for depression--anxiety comorbidity recognition (43.0%) and 12-way differential diagnosis (28.5%); (2) dynamic consultation often underperforms static evaluation, indicating that ineffective information-gathering strategies significantly impair downstream diagnostic reasoning; (3) consultation quality assessed by LLM-as-a-Judge shows only moderate correlation with diagnostic accuracy, suggesting that well-structured questioning alone does not ensure correct diagnostic decisions. We release LingxiDiag-16K and the full evaluation framework to support reproducible research at https://github.com/Lingxi-mental-health/LingxiDiagBench.",
      "authors": [
        "Shihao Xu",
        "Tiancheng Zhou",
        "Jiatong Ma",
        "Yanli Ding",
        "Yiming Yan",
        "Ming Xiao",
        "Guoyi Li",
        "Haiyang Geng",
        "Yunyun Han",
        "Jianhua Chen",
        "Yafeng Deng"
      ],
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.CL"
      ],
      "published": "2026-02-10 03:46:05+00:00",
      "link": "https://arxiv.org/pdf/2602.09379v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09375v1",
      "title": "Latent Poincaré Shaping for Agentic Reinforcement Learning",
      "abstract": "We propose LaPha, a method for training AlphaZero-like LLM agents in a Poincaré latent space. Under LaPha, the search process can be visualized as a tree rooted at the prompt and growing outward from the origin toward the boundary of the Poincaré ball, where negative curvature provides exponentially increasing capacity with radius. Using hyperbolic geodesic distance to rule-verified correctness, we define a node potential and assign dense process rewards by potential differences. We further attach a lightweight value head on the same shared latent space, enabling self-guided test-time scaling with almost no additional overhead. On MATH-500, LaPha improves Qwen2.5-Math-1.5B from 66.0% to 88.2%. With value-head-guided search, LaPha-1.5B reaches 56.7% accuracy on AIME'24, and LaPha-7B further achieves 60.0% on AIME'24 and 53.3% on AIME'25.",
      "authors": [
        "Hanchen Xia",
        "Baoyou Chen",
        "Zelin Zang",
        "Yutang Ge",
        "Guojiang Zhao",
        "Siyu Zhu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 03:35:52+00:00",
      "link": "https://arxiv.org/pdf/2602.09375v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09374v1",
      "title": "Surrogate-Guided Quantum Discovery in Black-Box Landscapes with Latent-Quadratic Interaction Embedding Transformers",
      "abstract": "Discovering configurations that are both high-utility and structurally diverse under expensive black-box evaluation and strict query budgets remains a central challenge in data-driven discovery. Many classical optimizers concentrate on dominant modes, while quality-diversity methods require large evaluation budgets to populate high-dimensional archives. Quantum Approximate Optimization Algorithm (QAOA) provides distributional sampling but requires an explicit problem Hamiltonian, which is unavailable in black-box settings. Practical quantum circuits favor quadratic Hamiltonians since higher-order interaction terms are costly to realize. Learned quadratic surrogates such as Factorization Machines (FM) have been used as proxies, but are limited to pairwise structure. We extend this surrogate-to-Hamiltonian approach by modelling higher-order variable dependencies via self-attention and projects them into a valid Positive Semi-Definite quadratic form compatible with QAOA. This enables diversity-oriented quantum sampling from learned energy landscapes while capturing interaction structure beyond pairwise terms. We evaluate on risk discovery for enterprise document processing systems against diverse classical optimizers. Quantum-guided samplers achieve competitive utility while consistently improving structural diversity and exclusive discovery. FM surrogates provide stronger early coverage, whereas ours yields higher-fidelity surrogate landscapes and better extreme-case discovery. Our method recovers roughly twice as many structurally tail-risk outliers as most classical baselines and identify an exclusive non-overlapping fraction of high-utility configurations not found by competing methods, highlighting that an effective mechanism for learning higher-order interaction structure and projecting it into quadratic surrogate Hamiltonians for quantum-assisted black-box discovery.",
      "authors": [
        "Saisubramaniam Gopalakrishnan",
        "Dagnachew Birru"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "published": "2026-02-10 03:32:23+00:00",
      "link": "https://arxiv.org/pdf/2602.09374v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09373v1",
      "title": "AfriNLLB: Efficient Translation Models for African Languages",
      "abstract": "In this work, we present AfriNLLB, a series of lightweight models for efficient translation from and into African languages. AfriNLLB supports 15 language pairs (30 translation directions), including Swahili, Hausa, Yoruba, Amharic, Somali, Zulu, Lingala, Afrikaans, Wolof, and Egyptian Arabic, as well as other African Union official languages such as Arabic (MSA), French, Portuguese, and Spanish. Our training data covers bidirectional translation between English and 13 languages, and between French and two languages (Lingala and Wolof).   AfriNLLB models are based on NLLB-200 600M, which we compress using iterative layer pruning and quantization. We fine-tune the pruned models on parallel corpora we curated for African languages, employing knowledge distillation from a larger teacher model. Our work aims at enabling efficient deployment of translation models for African languages in resource-constrained settings.   Our evaluation results demonstrate that AfriNLLB models achieve performance comparable to the baseline while being significantly faster. We release two versions of the AfriNLLB models, a Transformers version that allows further fine-tuning and a CTranslate2 version for efficient inference. Moreover, we release all the training data that we used for fine-tuning the baseline and pruned models to facilitate further research.",
      "authors": [
        "Yasmin Moslem",
        "Aman Kassahun Wassie",
        "Amanuel Gizachew Abebe"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 03:23:59+00:00",
      "link": "https://arxiv.org/pdf/2602.09373v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09366v1",
      "title": "Unsupervised Cross-Lingual Part-of-Speech Tagging with Monolingual Corpora Only",
      "abstract": "Due to the scarcity of part-of-speech annotated data, existing studies on low-resource languages typically adopt unsupervised approaches for POS tagging. Among these, POS tag projection with word alignment method transfers POS tags from a high-resource source language to a low-resource target language based on parallel corpora, making it particularly suitable for low-resource language settings. However, this approach relies heavily on parallel corpora, which are often unavailable for many low-resource languages. To overcome this limitation, we propose a fully unsupervised cross-lingual part-of-speech(POS) tagging framework that relies solely on monolingual corpora by leveraging unsupervised neural machine translation(UNMT) system. This UNMT system first translates sentences from a high-resource language into a low-resource one, thereby constructing pseudo-parallel sentence pairs. Then, we train a POS tagger for the target language following the standard projection procedure based on word alignments. Moreover, we propose a multi-source projection technique to calibrate the projected POS tags on the target side, enhancing to train a more effective POS tagger. We evaluate our framework on 28 language pairs, covering four source languages (English, German, Spanish and French) and seven target languages (Afrikaans, Basque, Finnis, Indonesian, Lithuanian, Portuguese and Turkish). Experimental results show that our method can achieve performance comparable to the baseline cross-lingual POS tagger with parallel sentence pairs, and even exceeds it for certain target languages. Furthermore, our proposed multi-source projection technique further boosts performance, yielding an average improvement of 1.3% over previous methods.",
      "authors": [
        "Jianyu Zheng"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 03:13:52+00:00",
      "link": "https://arxiv.org/pdf/2602.09366v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09346v1",
      "title": "Digital Linguistic Bias in Spanish: Evidence from Lexical Variation in LLMs",
      "abstract": "This study examines the extent to which Large Language Models (LLMs) capture geographic lexical variation in Spanish, a language that exhibits substantial regional variation. Treating LLMs as virtual informants, we probe their dialectal knowledge using two survey-style question formats: Yes-No questions and multiple-choice questions. To this end, we exploited a large-scale, expert-curated database of Spanish lexical variation. Our evaluation covers more than 900 lexical items across 21 Spanish-speaking countries and is conducted at both the country and dialectal area levels. Across both evaluation formats, the results reveal systematic differences in how LLMs represent Spanish language varieties. Lexical variation associated with Spain, Equatorial Guinea, Mexico & Central America, and the La Plata River is recognized more accurately by the models, while the Chilean variety proves particularly difficult for the models to distinguish. Importantly, differences in the volume of country-level digital resources do not account for these performance patterns, suggesting that factors beyond data quantity shape dialectal representation in LLMs. By providing a fine-grained, large-scale evaluation of geographic lexical variation, this work advances empirical understanding of dialectal knowledge in LLMs and contributes new evidence to discussions of Digital Linguistic Bias in Spanish.",
      "authors": [
        "Yoshifumi Kawasaki"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 02:42:22+00:00",
      "link": "https://arxiv.org/pdf/2602.09346v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09340v1",
      "title": "Measuring Dataset Diversity from a Geometric Perspective",
      "abstract": "Diversity can be broadly defined as the presence of meaningful variation across elements, which can be viewed from multiple perspectives, including statistical variation and geometric structural richness in the dataset. Existing diversity metrics, such as feature-space dispersion and metric-space magnitude, primarily capture distributional variation or entropy, while largely neglecting the geometric structure of datasets. To address this gap, we introduce a framework based on topological data analysis (TDA) and persistence landscapes (PLs) to extract and quantify geometric features from data. This approach provides a theoretically grounded means of measuring diversity beyond entropy, capturing the rich geometric and structural properties of datasets. Through extensive experiments across diverse modalities, we demonstrate that our proposed PLs-based diversity metric (PLDiv) is powerful, reliable, and interpretable, directly linking data diversity to its underlying geometry and offering a foundational tool for dataset construction, augmentation, and evaluation.",
      "authors": [
        "Yang Ba",
        "Mohammad Sadeq Abolhasani",
        "Michelle V Mancenido",
        "Rong Pan"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-10 02:17:16+00:00",
      "link": "https://arxiv.org/pdf/2602.09340v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09337v1",
      "title": "Kyrtos: A methodology for automatic deep analysis of graphic charts with curves in technical documents",
      "abstract": "Deep Understanding of Technical Documents (DUTD) has become a very attractive field with great potential due to large amounts of accumulated documents and the valuable knowledge contained in them. In addition, the holistic understanding of technical documents depends on the accurate analysis of its particular modalities, such as graphics, tables, diagrams, text, etc. and their associations. In this paper, we introduce the Kyrtos methodology for the automatic recognition and analysis of charts with curves in graphics images of technical documents. The recognition processing part adopts a clustering based approach to recognize middle-points that delimit the line-segments that construct the illustrated curves. The analysis processing part parses the extracted line-segments of curves to capture behavioral features such as direction, trend and etc. These associations assist the conversion of recognized segments' relations into attributed graphs, for the preservation of the curves' structural characteristics. The graph relations are also are expressed into natural language (NL) text sentences, enriching the document's text and facilitating their conversion into Stochastic Petri-net (SPN) graphs, which depict the internal functionality represented in the chart image. Extensive evaluation results demonstrate the accuracy of Kyrtos' recognition and analysis methods by measuring the structural similarity between input chart curves and the approximations generated by Kyrtos for charts with multiple functions.",
      "authors": [
        "Michail S. Alexiou",
        "Nikolaos G. Bourbakis"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-10 02:09:57+00:00",
      "link": "https://arxiv.org/pdf/2602.09337v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09331v1",
      "title": "Beyond Uniform Credit: Causal Credit Assignment for Policy Optimization",
      "abstract": "Policy gradient methods for language model reasoning, such as GRPO and DAPO, assign uniform credit to all generated tokens - the filler phrase \"Let me think\" receives the same gradient update as the critical calculation \"23 + 45 = 68.\" We propose counterfactual importance weighting: mask reasoning spans, measure the drop in answer probability, and upweight tokens accordingly during policy gradient updates. Our method requires no auxiliary models or external annotation, instead importance is estimated directly from the policy model's own probability shifts. Experiments on GSM8K across three models spanning the Qwen and Llama families demonstrate consistent improvements over uniform baselines and faster convergence to equivalent accuracy. Inverting the importance signal hurts performance, confirming we capture genuine causal structure rather than noise. Analysis shows the method correctly prioritizes calculation steps over scaffolding text. We view these findings as establishing counterfactual importance weighting as a foundation for further research rather than a complete solution.",
      "authors": [
        "Mykola Khandoga",
        "Rui Yuan",
        "Vinay Kumar Sankarapu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-10 01:57:02+00:00",
      "link": "https://arxiv.org/pdf/2602.09331v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09317v1",
      "title": "SnareNet: Flexible Repair Layers for Neural Networks with Hard Constraints",
      "abstract": "Neural networks are increasingly used as surrogate solvers and control policies, but unconstrained predictions can violate physical, operational, or safety requirements. We propose SnareNet, a feasibility-controlled architecture for learning mappings whose outputs must satisfy input-dependent nonlinear constraints. SnareNet appends a differentiable repair layer that navigates in the constraint map's range space, steering iterates toward feasibility and producing a repaired output that satisfies constraints to a user-specified tolerance. To stabilize end-to-end training, we introduce adaptive relaxation, which designs a relaxed feasible set that snares the neural network at initialization and shrinks it into the feasible set, enabling early exploration and strict feasibility later in training. On optimization-learning and trajectory planning benchmarks, SnareNet consistently attains improved objective quality while satisfying constraints more reliably than prior work.",
      "authors": [
        "Ya-Chi Chu",
        "Alkiviades Boukas",
        "Madeleine Udell"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-02-10 01:24:32+00:00",
      "link": "https://arxiv.org/pdf/2602.09317v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09316v2",
      "title": "Effective MoE-based LLM Compression by Exploiting Heterogeneous Inter-Group Experts Routing Frequency and Information Density",
      "abstract": "Mixture-of-Experts (MoE) based Large Language Models (LLMs) have achieved superior performance, yet the massive memory overhead caused by storing multiple expert networks severely hinders their practical deployment. Singular Value Decomposition (SVD)-based compression has emerged as a promising post-training technique; however, most existing methods apply uniform rank allocation or rely solely on static weight properties. This overlooks the substantial heterogeneity in expert utilization observed in MoE models, where frequent routing patterns and intrinsic information density vary significantly across experts. In this work, we propose RFID-MoE, an effective framework for MoE compression by exploiting heterogeneous Routing Frequency and Information Density. We first introduce a fused metric that combines expert activation frequency with effective rank to measure expert importance, adaptively allocating higher ranks to critical expert groups under a fixed budget. Moreover, instead of discarding compression residuals, we reconstruct them via a parameter-efficient sparse projection mechanism to recover lost information with minimal parameter overhead. Extensive experiments on representative MoE LLMs (e.g., Qwen3, DeepSeekMoE) across multiple compression ratios demonstrate that RFID-MoE consistently outperforms state-of-the-art methods like MoBE and D2-MoE. Notably, RFID-MoE achieves a perplexity of 16.92 on PTB with the Qwen3-30B model at a 60% compression ratio, reducing perplexity by over 8.0 compared to baselines, and improves zero-shot accuracy on HellaSwag by approximately 8%.",
      "authors": [
        "Zhendong Mi",
        "Yixiao Chen",
        "Pu Zhao",
        "Xiaodong Yu",
        "Hao Wang",
        "Yanzhi Wang",
        "Shaoyi Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 01:24:28+00:00",
      "link": "https://arxiv.org/pdf/2602.09316v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13308v2",
      "title": "Learning to Select Like Humans: Explainable Active Learning for Medical Imaging",
      "abstract": "Medical image analysis requires substantial labeled data for model training, yet expert annotation is expensive and time-consuming. Active learning (AL) addresses this challenge by strategically selecting the most informative samples for the annotation purpose, but traditional methods solely rely on predictive uncertainty while ignoring whether models learn from clinically meaningful features a critical requirement for clinical deployment. We propose an explainability-guided active learning framework that integrates spatial attention alignment into a sample acquisition process. Our approach advocates for a dual-criterion selection strategy combining: (i) classification uncertainty to identify informative examples, and (ii) attention misalignment with radiologist-defined regions-of-interest (ROIs) to target samples where the model focuses on incorrect features. By measuring misalignment between Grad-CAM attention maps and expert annotations using Dice similarity, our acquisition function judiciously identifies samples that enhance both predictive performance and spatial interpretability. We evaluate the framework using three expert-annotated medical imaging datasets, namely, BraTS (MRI brain tumors), VinDr-CXR (chest X-rays), and SIIM-COVID-19 (chest X-rays). Using only 570 strategically selected samples, our explainability-guided approach consistently outperforms random sampling across all the datasets, achieving 77.22% accuracy on BraTS, 52.37% on VinDr-CXR, and 52.66% on SIIM-COVID. Grad-CAM visualizations confirm that the models trained by our dual-criterion selection focus on diagnostically relevant regions, demonstrating that incorporating explanation guidance into sample acquisition yields superior data efficiency while maintaining clinical interpretability.",
      "authors": [
        "Ifrat Ikhtear Uddin",
        "Longwei Wang",
        "Xiao Qin",
        "Yang Zhou",
        "KC Santosh"
      ],
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-10 01:20:37+00:00",
      "link": "https://arxiv.org/pdf/2602.13308v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09314v1",
      "title": "Clarifying Shampoo: Adapting Spectral Descent to Stochasticity and the Parameter Trajectory",
      "abstract": "Optimizers leveraging the matrix structure in neural networks, such as Shampoo and Muon, are more data-efficient than element-wise algorithms like Adam and Signum. While in specific settings, Shampoo and Muon reduce to spectral descent analogous to how Adam and Signum reduce to sign descent, their general relationship and relative data efficiency under controlled settings remain unclear. Through extensive experiments on language models, we demonstrate that Shampoo achieves higher token efficiency than Muon, mirroring Adam's advantage over Signum. We show that Shampoo's update applied to weight matrices can be decomposed into an adapted Muon update. Consistent with this, Shampoo's benefits can be exclusively attributed to its application to weight matrices, challenging interpretations agnostic to parameter shapes. This admits a new perspective that also avoids shortcomings of related interpretations based on variance adaptation and whitening: rather than enforcing semi-orthogonality as in spectral descent, Shampoo's updates are time-averaged semi-orthogonal in expectation.",
      "authors": [
        "Runa Eschenhagen",
        "Anna Cai",
        "Tsung-Hsien Lee",
        "Hao-Jun Michael Shi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-02-10 01:19:40+00:00",
      "link": "https://arxiv.org/pdf/2602.09314v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09312v1",
      "title": "Don't Shoot The Breeze: Topic Continuity Model Using Nonlinear Naive Bayes With Attention",
      "abstract": "Utilizing Large Language Models (LLM) as chatbots in diverse business scenarios often presents the challenge of maintaining topic continuity. Abrupt shifts in topics can lead to poor user experiences and inefficient utilization of computational resources. In this paper, we present a topic continuity model aimed at assessing whether a response aligns with the initial conversation topic. Our model is built upon the expansion of the corresponding natural language understanding (NLU) model into quantifiable terms using a Naive Bayes approach. Subsequently, we have introduced an attention mechanism and logarithmic nonlinearity to enhance its capability to capture topic continuity. This approach allows us to convert the NLU model into an interpretable analytical formula. In contrast to many NLU models constrained by token limits, our proposed model can seamlessly handle conversations of any length with linear time complexity. Furthermore, the attention mechanism significantly improves the model's ability to identify topic continuity in complex conversations. According to our experiments, our model consistently outperforms traditional methods, particularly in handling lengthy and intricate conversations. This unique capability offers us an opportunity to ensure the responsible and interpretable use of LLMs.",
      "authors": [
        "Shu-Ting Pi",
        "Pradeep Bagavan",
        "Yejia Li",
        "Disha",
        "Qun Liu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-10 01:05:31+00:00",
      "link": "https://arxiv.org/pdf/2602.09312v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09306v1",
      "title": "Empowering Contrastive Federated Sequential Recommendation with LLMs",
      "abstract": "Federated sequential recommendation (FedSeqRec) aims to perform next-item prediction while keeping user data decentralised, yet model quality is frequently constrained by fragmented, noisy, and homogeneous interaction logs stored on individual devices. Many existing approaches attempt to compensate through manual data augmentation or additional server-side constraints, but these strategies either introduce limited semantic diversity or increase system overhead. To overcome these challenges, we propose \\textbf{LUMOS}, a parameter-isolated FedSeqRec architecture that integrates large language models (LLMs) as \\emph{local semantic generators}. Instead of sharing gradients or auxiliary parameters, LUMOS privately invokes an on-device LLM to construct three complementary sequence variants from each user history: (i) \\emph{future-oriented} trajectories that infer plausible behavioural continuations, (ii) \\emph{semantically equivalent rephrasings} that retain user intent while diversifying interaction patterns, and (iii) \\emph{preference-inconsistent counterfactuals} that serve as informative negatives. These synthesized sequences are jointly encoded within the federated backbone through a tri-view contrastive optimisation scheme, enabling richer representation learning without exposing sensitive information. Experimental results across three public benchmarks show that LUMOS achieves consistent gains over competitive centralised and federated baselines on HR@20 and NDCG@20. In addition, the use of semantically grounded positive signals and counterfactual negatives improves robustness under noisy and adversarial environments, even without dedicated server-side protection modules. Overall, this work demonstrates the potential of LLM-driven semantic generation as a new paradigm for advancing privacy-preserving federated recommendation.",
      "authors": [
        "Thi Minh Chau Nguyen",
        "Minh Hieu Nguyen",
        "Duc Anh Nguyen",
        "Xuan Huong Tran",
        "Thanh Trung Huynh",
        "Quoc Viet Hung Nguyen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "published": "2026-02-10 00:47:43+00:00",
      "link": "https://arxiv.org/pdf/2602.09306v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09304v1",
      "title": "Statistical Roughness-Informed Machine Unlearning",
      "abstract": "Machine unlearning aims to remove the influence of a designated forget set from a trained model while preserving utility on the retained data. In modern deep networks, approximate unlearning frequently fails under large or adversarial deletions due to pronounced layer-wise heterogeneity: some layers exhibit stable, well-regularized representations while others are brittle, undertrained, or overfit, so naive update allocation can trigger catastrophic forgetting or unstable dynamics. We propose Statistical-Roughness Adaptive Gradient Unlearning (SRAGU), a mechanism-first unlearning algorithm that reallocates unlearning updates using layer-wise statistical roughness operationalized via heavy-tailed spectral diagnostics of layer weight matrices. Starting from an Adaptive Gradient Unlearning (AGU) sensitivity signal computed on the forget set, SRAGU estimates a WeightWatcher-style heavy-tailed exponent for each layer, maps it to a bounded spectral stability weight, and uses this stability signal to spectrally reweight the AGU sensitivities before applying the same minibatch update form. This concentrates unlearning motion in spectrally stable layers while damping updates in unstable or overfit layers, improving stability under hard deletions. We evaluate unlearning via behavioral alignment to a gold retrained reference model trained from scratch on the retained data, using empirical prediction-divergence and KL-to-gold proxies on a forget-focused query set; we additionally report membership inference auditing as a complementary leakage signal, treating forget-set points as should-be-forgotten members during evaluation.",
      "authors": [
        "Mohammad Partohaghighi",
        "Roummel Marcia",
        "Bruce J. West",
        "YangQuan Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 00:40:36+00:00",
      "link": "https://arxiv.org/pdf/2602.09304v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09303v1",
      "title": "Stabilizing Physics-Informed Consistency Models via Structure-Preserving Training",
      "abstract": "We propose a physics-informed consistency modeling framework for solving partial differential equations (PDEs) via fast, few-step generative inference. We identify a key stability challenge in physics-constrained consistency training, where PDE residuals can drive the model toward trivial or degenerate solutions, degrading the learned data distribution. To address this, we introduce a structure-preserving two-stage training strategy that decouples distribution learning from physics enforcement by freezing the coefficient decoder during physics-informed fine-tuning. We further propose a two-step residual objective that enforces physical consistency on refined, structurally valid generative trajectories rather than noisy single-step predictions. The resulting framework enables stable, high-fidelity inference for both unconditional generation and forward problems. We demonstrate that forward solutions can be obtained via a projection-based zero-shot inpainting procedure, achieving consistent accuracy of diffusion baselines with orders of magnitude reduction in computational cost.",
      "authors": [
        "Che-Chia Chang",
        "Chen-Yang Dai",
        "Te-Sheng Lin",
        "Ming-Chih Lai",
        "Chieh-Hsin Lai"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-02-10 00:40:19+00:00",
      "link": "https://arxiv.org/pdf/2602.09303v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09302v1",
      "title": "A Theory for Probabilistic Polynomial-Time Reasoning",
      "abstract": "In this work, we propose a new bounded arithmetic theory, denoted $APX_1$, designed to formalize a broad class of probabilistic arguments commonly used in theoretical computer science. Under plausible assumptions, $APX_1$ is strictly weaker than previously proposed frameworks, such as the theory $APC_1$ introduced in the seminal work of Jerabek (2007). From a computational standpoint, $APX_1$ is closely tied to approximate counting and to the central question in derandomization, the prBPP versus prP problem, whereas $APC_1$ is linked to the dual weak pigeonhole principle and to the existence of Boolean functions with exponential circuit complexity.   A key motivation for introducing $APX_1$ is that its weaker axioms expose finer proof-theoretic structure, making it a natural setting for several lines of research, including unprovability of complexity conjectures and reverse mathematics of randomized lower bounds. In particular, the framework we develop for $APX_1$ enables the formulation of precise questions concerning the provability of prBPP=prP in deterministic feasible mathematics. Since the (un)provability of P versus NP in bounded arithmetic has long served as a central theme in the field, we expect this line of investigation to be of particular interest.   Our technical contributions include developing a comprehensive foundation for probabilistic reasoning from weaker axioms, formalizing non-trivial results from theoretical computer science in $APX_1$, and establishing a tailored witnessing theorem for its provably total TFNP problems. As a byproduct of our analysis of the minimal proof-theoretic strength required to formalize statements arising in theoretical computer science, we resolve an open problem regarding the provability of $AC^0$ lower bounds in $PV_1$, which was considered in earlier works by Razborov (1995), Krajicek (1995), and Muller and Pich (2020).",
      "authors": [
        "Lijie Chen",
        "Jiatu Li",
        "Igor C. Oliveira",
        "Ryan Williams"
      ],
      "primary_category": "cs.CC",
      "categories": [
        "cs.CC",
        "cs.LO",
        "math.LO"
      ],
      "published": "2026-02-10 00:39:49+00:00",
      "link": "https://arxiv.org/pdf/2602.09302v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09297v1",
      "title": "The Laplacian Mechanism Improves Transformers by Reshaping Token Geometry",
      "abstract": "Transformers leverage attention, the residual connection, and layer normalization to control the variance of token representations. We propose to modify attention into a Laplacian mechanism that gives the model more direct control over token variance. We conjecture that this helps transformers achieve the ideal token geometry. To investigate our conjecture, we first show that incorporating the Laplacian mechanism into transformers induces consistent improvements across benchmarks in computer vision and language. Next, we study how the Laplacian mechanism impacts the geometry of token representations using various tools: 1) principal component analysis, 2) cosine similarity metric, 3) analysis of variance, and 4) Neural Collapse metrics. Our investigation shows that the Laplacian mechanism reshapes token embeddings toward a geometry of maximal separability: tokens collapse according to their classes, and the class means exhibit Neural Collapse.",
      "authors": [
        "Yuchong Zhang",
        "Vardan Papyan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 00:27:45+00:00",
      "link": "https://arxiv.org/pdf/2602.09297v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09288v1",
      "title": "Measuring Privacy Risks and Tradeoffs in Financial Synthetic Data Generation",
      "abstract": "We explore the privacy-utility tradeoff of synthetic data generation schemes on tabular financial datasets, a domain characterized by high regulatory risk and severe class imbalance. We consider representative tabular data generators, including autoencoders, generative adversarial networks, diffusion, and copula synthesizers. To address the challenges of the financial domain, we provide novel privacy-preserving implementations of GAN and autoencoder synthesizers. We evaluate whether and how well the generators simultaneously achieve data quality, downstream utility, and privacy, with comparison across balanced and imbalanced input datasets. Our results offer insight into the distinct challenges of generating synthetic data from datasets that exhibit severe class imbalance and mixed-type attributes.",
      "authors": [
        "Michael Zuo",
        "Inwon Kang",
        "Stacy Patterson",
        "Oshani Seneviratne"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 00:14:19+00:00",
      "link": "https://arxiv.org/pdf/2602.09288v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09278v1",
      "title": "The effect of whitening on explanation performance",
      "abstract": "Explainable Artificial Intelligence (XAI) aims to provide transparent insights into machine learning models, yet the reliability of many feature attribution methods remains a critical challenge. Prior research (Haufe et al., 2014; Wilming et al., 2022, 2023) has demonstrated that these methods often erroneously assign significant importance to non-informative variables, such as suppressor variables, leading to fundamental misinterpretations. Since statistical suppression is induced by feature dependencies, this study investigates whether data whitening, a common preprocessing technique for decorrelation, can mitigate such errors. Using the established XAI-TRIS benchmark (Clark et al., 2024b), which offers synthetic ground-truth data and quantitative measures of explanation correctness, we empirically evaluate 16 popular feature attribution methods applied in combination with 5 distinct whitening transforms. Additionally, we analyze a minimal linear two-dimensional classification problem (Wilming et al., 2023) to theoretically assess whether whitening can remove the impact of suppressor features from Bayes-optimal models. Our results indicate that, while specific whitening techniques can improve explanation performance, the degree of improvement varies substantially across XAI methods and model architectures. These findings highlight the complex relationship between data non-linearities, preprocessing quality, and attribution fidelity, underscoring the vital role of pre-processing techniques in enhancing model interpretability.",
      "authors": [
        "Benedict Clark",
        "Stoyan Karastoyanov",
        "Rick Wilming",
        "Stefan Haufe"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 23:41:57+00:00",
      "link": "https://arxiv.org/pdf/2602.09278v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09276v1",
      "title": "Effective Reasoning Chains Reduce Intrinsic Dimensionality",
      "abstract": "Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing a consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as a quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach a given accuracy threshold on a given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies, we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe a strong inverse correlation between the intrinsic dimensionality of a reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering a new quantitative metric for analyzing reasoning processes.",
      "authors": [
        "Archiki Prasad",
        "Mandar Joshi",
        "Kenton Lee",
        "Mohit Bansal",
        "Peter Shaw"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-09 23:32:12+00:00",
      "link": "https://arxiv.org/pdf/2602.09276v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09258v1",
      "title": "Generalizing GNNs with Tokenized Mixture of Experts",
      "abstract": "Deployed graph neural networks (GNNs) are frozen at deployment yet must fit clean data, generalize under distribution shifts, and remain stable to perturbations. We show that static inference induces a fundamental tradeoff: improving stability requires reducing reliance on shift-sensitive features, leaving an irreducible worst-case generalization floor. Instance-conditional routing can break this ceiling, but is fragile because shifts can mislead routing and perturbations can make routing fluctuate. We capture these effects via two decompositions separating coverage vs selection, and base sensitivity vs fluctuation amplification. Based on these insights, we propose STEM-GNN, a pretrain-then-finetune framework with a mixture-of-experts encoder for diverse computation paths, a vector-quantized token interface to stabilize encoder-to-head signals, and a Lipschitz-regularized head to bound output amplification. Across nine node, link, and graph benchmarks, STEM-GNN achieves a stronger three-way balance, improving robustness to degree/homophily shifts and to feature/edge corruptions while remaining competitive on clean graphs.",
      "authors": [
        "Xiaoguang Guo",
        "Zehong Wang",
        "Jiazheng Li",
        "Shawn Spitzel",
        "Qi Yang",
        "Kaize Ding",
        "Jundong Li",
        "Chuxu Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 22:48:30+00:00",
      "link": "https://arxiv.org/pdf/2602.09258v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09246v1",
      "title": "Marco IA593: Modelo de Gobernanza, Ética y Estrategia para la Integración de la Inteligencia Artificial en la Educación Superior del Ecuador",
      "abstract": "The integration of Artificial Intelligence (AI) into Higher Education Institutions (HEIs) in Ecuador is not a technological option but a strategic imperative to prevent institutional obsolescence and academic irrelevance in Latin America. This paper presents the IA593 Framework, a governance, ethics, and operational model designed for the Universidad Nacional de Loja (UNL) and scalable as a reference for the Ecuadorian higher education system. The current context reveals a critical urgency: the Latin American Artificial Intelligence Index 2025 classifies Ecuador as a late awakening adopter, exposing severe structural gaps, including R and D investment of only 0.44 percent of GDP and a marginal contribution to global AI scientific output. Although a National Strategy for the Promotion of AI exists and calls for multisectoral governance, universities still lack internal regulations governing the use of Generative AI, placing academic integrity and data privacy at risk. The IA593 Framework addresses this challenge through five interconnected pillars aligned with the FATE principles of Fairness, Accountability, Transparency, and Ethics and UNESCO recommendations on AI ethics: Transversal Governance, Teaching and Training, Research, Outreach, and Management. This framework enables HEIs to move from passive technology consumption toward a sovereign and critical adoption of AI, ensuring compliance with national academic regulations and positioning UNL as a key actor in reducing the digital divide and brain drain in Ecuador.",
      "authors": [
        "Luis Chamba-Eras",
        "Oscar Miguel Cumbicus Pineda",
        "Edison Leonardo Coronel Romero",
        "Jessica Katherine Gaona Alvarado",
        "Luis Rodrigo Barba Guamán"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY"
      ],
      "published": "2026-02-09 22:18:33+00:00",
      "link": "https://arxiv.org/pdf/2602.09246v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09242v1",
      "title": "Open Mathematical Tasks as a Didactic Response to Generative Artificial Intelligence in Post-AI Contexts",
      "abstract": "The widespread availability of generative artificial intelligence tools poses new challenges for school mathematics education, particularly regarding the formative role of traditional mathematical tasks. In post-AI educational contexts, many activities can be solved automatically, without engaging students in interpretation, decision-making, or mathematical validation processes.   This study analyzes a secondary school classroom experience in which open mathematical tasks are implemented as a didactic response to this scenario, aiming to sustain students' mathematical activity. Adopting a qualitative and descriptive-interpretative approach, the study examines the forms of mathematical work that emerge during task resolution, mediated by the didactic regulation device COMPAS.   The analysis is structured around four analytical axes: open task design in post-AI contexts, students' mathematical agency, human-AI complementarity, and modeling and validation practices. The findings suggest that, under explicit didactic regulation, students retain epistemic control over mathematical activity, even in the presence of generative artificial intelligence.",
      "authors": [
        "Felix De la Cruz Serrano"
      ],
      "primary_category": "math.HO",
      "categories": [
        "math.HO",
        "cs.CY"
      ],
      "published": "2026-02-09 22:16:04+00:00",
      "link": "https://arxiv.org/pdf/2602.09242v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09240v1",
      "title": "Optimal Estimation in Orthogonally Invariant Generalized Linear Models: Spectral Initialization and Approximate Message Passing",
      "abstract": "We consider the problem of parameter estimation from a generalized linear model with a random design matrix that is orthogonally invariant in law. Such a model allows the design have an arbitrary distribution of singular values and only assumes that its singular vectors are generic. It is a vast generalization of the i.i.d. Gaussian design typically considered in the theoretical literature, and is motivated by the fact that real data often have a complex correlation structure so that methods relying on i.i.d. assumptions can be highly suboptimal. Building on the paradigm of spectrally-initialized iterative optimization, this paper proposes optimal spectral estimators and combines them with an approximate message passing (AMP) algorithm, establishing rigorous performance guarantees for these two algorithmic steps. Both the spectral initialization and the subsequent AMP meet existing conjectures on the fundamental limits to estimation -- the former on the optimal sample complexity for efficient weak recovery, and the latter on the optimal errors. Numerical experiments suggest the effectiveness of our methods and accuracy of our theory beyond orthogonally invariant data.",
      "authors": [
        "Yihan Zhang",
        "Hong Chang Ji",
        "Ramji Venkataramanan",
        "Marco Mondelli"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "cs.IT",
        "cs.LG",
        "math.PR",
        "stat.ML"
      ],
      "published": "2026-02-09 22:08:09+00:00",
      "link": "https://arxiv.org/pdf/2602.09240v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09238v3",
      "title": "Feature salience - not task-informativeness - drives machine learning model explanations",
      "abstract": "Explainable AI (XAI) promises to provide insight into machine learning models' decision processes, where one goal is to identify failures such as shortcut learning. This promise relies on the field's assumption that input features marked as important by an XAI must contain information about the target variable. However, it is unclear whether informativeness is indeed the main driver of importance attribution in practice, or if other data properties such as statistical suppression, novelty at test-time, or high feature salience substantially contribute. To clarify this, we trained deep learning models on three variants of a binary image classification task, in which translucent watermarks are either absent, act as class-dependent confounds, or represent class-independent noise. Results for five popular attribution methods show substantially elevated relative importance in watermarked areas (RIW) for all models regardless of the training setting ($R^2 \\geq .45$). By contrast, whether the presence of watermarks is class-dependent or not only has a marginal effect on RIW ($R^2 \\leq .03$), despite a clear impact impact on model performance and generalisation ability. XAI methods show similar behaviour to model-agnostic edge detection filters and attribute substantially less importance to watermarks when bright image intensities are encoded by smaller instead of larger feature values. These results indicate that importance attribution is most strongly driven by the salience of image structures at test time rather than statistical associations learned by machine learning models. Previous studies demonstrating successful XAI application should be reevaluated with respect to a possibly spurious concurrency of feature salience and informativeness, and workflows using feature attribution methods as building blocks should be scrutinised.",
      "authors": [
        "Benedict Clark",
        "Marta Oliveira",
        "Rick Wilming",
        "Stefan Haufe"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 22:07:59+00:00",
      "link": "https://arxiv.org/pdf/2602.09238v3",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09196v1",
      "title": "Fair Feature Importance Scores via Feature Occlusion and Permutation",
      "abstract": "As machine learning models increasingly impact society, their opaque nature poses challenges to trust and accountability, particularly in fairness contexts. Understanding how individual features influence model outcomes is crucial for building interpretable and equitable models. While feature importance metrics for accuracy are well-established, methods for assessing feature contributions to fairness remain underexplored. We propose two model-agnostic approaches to measure fair feature importance. First, we propose to compare model fairness before and after permuting feature values. This simple intervention-based approach decouples a feature and model predictions to measure its contribution to training. Second, we evaluate the fairness of models trained with and without a given feature. This occlusion-based score enjoys dramatic computational simplification via minipatch learning. Our empirical results reflect the simplicity and effectiveness of our proposed metrics for multiple predictive tasks. Both methods offer simple, scalable, and interpretable solutions to quantify the influence of features on fairness, providing new tools for responsible machine learning development.",
      "authors": [
        "Camille Little",
        "Madeline Navarro",
        "Santiago Segarra",
        "Genevera Allen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-09 21:02:52+00:00",
      "link": "https://arxiv.org/pdf/2602.09196v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09194v1",
      "title": "ML-DCN: Masked Low-Rank Deep Crossing Network Towards Scalable Ads Click-through Rate Prediction at Pinterest",
      "abstract": "Deep learning recommendation systems rely on feature interaction modules to model complex user-item relationships across sparse categorical and dense features. In large-scale ad ranking, increasing model capacity is a promising path to improving both predictive performance and business outcomes, yet production serving budgets impose strict constraints on latency and FLOPs. This creates a central tension: we want interaction modules that both scale effectively with additional compute and remain compute-efficient at serving time. In this work, we study how to scale feature interaction modules under a fixed serving budget. We find that naively scaling DCNv2 and MaskNet, despite their widespread adoption in industry, yields rapidly diminishing offline gains in the Pinterest ads ranking system. To overcome aforementioned limitations, we propose ML-DCN, an interaction module that integrates an instance-conditioned mask into a low-rank crossing layer, enabling per-example selection and amplification of salient interaction directions while maintaining efficient computation. This novel architecture combines the strengths of DCNv2 and MaskNet, scales efficiently with increased compute, and achieves state-of-the-art performance. Experiments on a large internal Pinterest ads dataset show that ML-DCN achieves higher AUC than DCNv2, MaskNet, and recent scaling-oriented alternatives at matched FLOPs, and it scales more favorably overall as compute increases, exhibiting a stronger AUC-FLOPs trade-off. Finally, online A/B tests demonstrate statistically significant improvements in key ads metrics (including CTR and click-quality measures) and ML-DCN has been deployed in the production system with neutral serving cost.",
      "authors": [
        "Jiacheng Li",
        "Yixiong Meng",
        "Yi wu",
        "Yun Zhao",
        "Sharare Zehtabian",
        "Jiayin Jin",
        "Degao Peng",
        "Jinfeng Zhuang",
        "Qifei Shen",
        "Kungang Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 20:59:19+00:00",
      "link": "https://arxiv.org/pdf/2602.09194v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09190v1",
      "title": "Gradient Residual Connections",
      "abstract": "Existing work has linked properties of a function's gradient to the difficulty of function approximation. Motivated by these insights, we study how gradient information can be leveraged to improve neural network's ability to approximate high-frequency functions, and we propose a gradient-based residual connection as a complement to the standard identity skip connection used in residual networks. We provide simple theoretical intuition for why gradient information can help distinguish inputs and improve the approximation of functions with rapidly varying behaviour. On a synthetic regression task with a high-frequency sinusoidal ground truth, we show that conventional residual connections struggle to capture high-frequency patterns. In contrast, our gradient residual substantially improves approximation quality. We then introduce a convex combination of the standard and gradient residuals, allowing the network to flexibly control how strongly it relies on gradient information. After validating the design choices of our proposed method through an ablation study, we further validate our approach's utility on the single-image super-resolution task, where the underlying function may be high-frequency. Finally, on standard tasks such as image classification and segmentation, our method achieves performance comparable to standard residual networks, suggesting its broad utility.",
      "authors": [
        "Yangchen Pan",
        "Qizhen Ying",
        "Philip Torr",
        "Bo Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-09 20:53:33+00:00",
      "link": "https://arxiv.org/pdf/2602.09190v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09182v1",
      "title": "One RNG to Rule Them All: How Randomness Becomes an Attack Vector in Machine Learning",
      "abstract": "Machine learning relies on randomness as a fundamental component in various steps such as data sampling, data augmentation, weight initialization, and optimization. Most machine learning frameworks use pseudorandom number generators as the source of randomness. However, variations in design choices and implementations across different frameworks, software dependencies, and hardware backends along with the lack of statistical validation can lead to previously unexplored attack vectors on machine learning systems. Such attacks on randomness sources can be extremely covert, and have a history of exploitation in real-world systems. In this work, we examine the role of randomness in the machine learning development pipeline from an adversarial point of view, and analyze the implementations of PRNGs in major machine learning frameworks. We present RNGGuard to help machine learning engineers secure their systems with low effort. RNGGuard statically analyzes a target library's source code and identifies instances of random functions and modules that use them. At runtime, RNGGuard enforces secure execution of random functions by replacing insecure function calls with RNGGuard's implementations that meet security specifications. Our evaluations show that RNGGuard presents a practical approach to close existing gaps in securing randomness sources in machine learning systems.",
      "authors": [
        "Kotekar Annapoorna Prabhu",
        "Andrew Gan",
        "Zahra Ghodsi"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published": "2026-02-09 20:41:05+00:00",
      "link": "https://arxiv.org/pdf/2602.09182v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09181v1",
      "title": "Weighted Wasserstein Barycenter of Gaussian Processes for exotic Bayesian Optimization tasks",
      "abstract": "Exploiting the analogy between Gaussian Distributions and Gaussian Processes' posterior, we present how the weighted Wasserstein Barycenter of Gaussian Processes (W2BGP) can be used to unify, under a common framework, different exotic Bayesian Optimization (BO) tasks. Specifically, collaborative/federated BO, (synchronous) batch BO, and multi-fidelity BO are considered in this paper. Our empirical analysis proves that each one of these tasks requires just an appropriate weighting schema for the W2BGP, while the entire framework remains untouched. Moreover, we demonstrate that the most well-known BO acquisition functions can be easily re-interpreted under the proposed framework and also enable a more computationally efficient way to deal with the computation of the Wasserstein Barycenter, compared with state-of-the-art methods from the Machine Learning literature. Finally, research perspectives branching from the proposed approach are presented.",
      "authors": [
        "Antonio Candelieri",
        "Francesco Archetti"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 20:40:33+00:00",
      "link": "https://arxiv.org/pdf/2602.09181v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09170v1",
      "title": "Quantifying Epistemic Uncertainty in Diffusion Models",
      "abstract": "To ensure high quality outputs, it is important to quantify the epistemic uncertainty of diffusion models. Existing methods are often unreliable because they mix epistemic and aleatoric uncertainty. We introduce a method based on Fisher information that explicitly isolates epistemic variance, producing more reliable plausibility scores for generated data. To make this approach scalable, we propose FLARE (Fisher-Laplace Randomized Estimator), which approximates the Fisher information using a uniformly random subset of model parameters. Empirically, FLARE improves uncertainty estimation in synthetic time-series generation tasks, achieving more accurate and reliable filtering than other methods. Theoretically, we bound the convergence rate of our randomized approximation and provide analytic and empirical evidence that last-layer Laplace approximations are insufficient for this task.",
      "authors": [
        "Aditi Gupta",
        "Raphael A. Meyer",
        "Yotam Yaniv",
        "Elynn Chen",
        "N. Benjamin Erichson"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-09 20:22:33+00:00",
      "link": "https://arxiv.org/pdf/2602.09170v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09169v1",
      "title": "Train Less, Infer Faster: Efficient Model Finetuning and Compression via Structured Sparsity",
      "abstract": "Fully finetuning foundation language models (LMs) with billions of parameters is often impractical due to high computational costs, memory requirements, and the risk of overfitting. Although methods like low-rank adapters help address these challenges by adding small trainable modules to the frozen LM, they also increase memory usage and do not reduce inference latency. We uncover an intriguing phenomenon: sparsifying specific model rows and columns enables efficient task adaptation without requiring weight tuning. We propose a scheme for effective finetuning via sparsification using training stochastic gates, which requires minimal trainable parameters, reduces inference time, and removes 20--40\\% of model parameters without significant accuracy loss. Empirical results show it outperforms recent finetuning baselines in efficiency and performance. Additionally, we provide theoretical guarantees for the convergence of this stochastic gating process, and show that our method admits a simpler and better-conditioned optimization landscape compared to LoRA. Our results highlight sparsity as a compelling mechanism for task-specific adaptation in LMs.",
      "authors": [
        "Jonathan Svirsky",
        "Yehonathan Refael",
        "Ofir Lindenbaum"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 20:20:29+00:00",
      "link": "https://arxiv.org/pdf/2602.09169v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09163v1",
      "title": "FlyAOC: Evaluating Agentic Ontology Curation of Drosophila Scientific Knowledge Bases",
      "abstract": "Scientific knowledge bases accelerate discovery by curating findings from primary literature into structured, queryable formats for both human researchers and emerging AI systems. Maintaining these resources requires expert curators to search relevant papers, reconcile evidence across documents, and produce ontology-grounded annotations - a workflow that existing benchmarks, focused on isolated subtasks like named entity recognition or relation extraction, do not capture. We present FlyBench to evaluate AI agents on end-to-end agentic ontology curation from scientific literature. Given only a gene symbol, agents must search and read from a corpus of 16,898 full-text papers to produce structured annotations: Gene Ontology terms describing function, expression patterns, and historical synonyms linking decades of nomenclature. The benchmark includes 7,397 expert-curated annotations across 100 genes drawn from FlyBase, the Drosophila (fruit fly) knowledge base. We evaluate four baseline agent architectures: memorization, fixed pipeline, single-agent, and multi-agent. We find that architectural choices significantly impact performance, with multi-agent designs outperforming simpler alternatives, yet scaling backbone models yields diminishing returns. All baselines leave substantial room for improvement. Our analysis surfaces several findings to guide future development; for example, agents primarily use retrieval to confirm parametric knowledge rather than discover new information. We hope FlyBench will drive progress on retrieval-augmented scientific reasoning, a capability with broad applications across scientific domains.",
      "authors": [
        "Xingjian Zhang",
        "Sophia Moylan",
        "Ziyang Xiong",
        "Qiaozhu Mei",
        "Yichen Luo",
        "Jiaqi W. Ma"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-02-09 20:12:38+00:00",
      "link": "https://arxiv.org/pdf/2602.09163v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09162v1",
      "title": "Boltzmann Reinforcement Learning for Noise resilience in Analog Ising Machines",
      "abstract": "Analog Ising machines (AIMs) have emerged as a promising paradigm for combinatorial optimization, utilizing physical dynamics to solve Ising problems with high energy efficiency. However, the performance of traditional optimization and sampling algorithms on these platforms is often limited by inherent measurement noise. We introduce BRAIN (Boltzmann Reinforcement for Analog Ising Networks), a distribution learning framework that utilizes variational reinforcement learning to approximate the Boltzmann distribution. By shifting from state-by-state sampling to aggregating information across multiple noisy measurements, BRAIN is resilient to Gaussian noise characteristic of AIMs. We evaluate BRAIN across diverse combinatorial topologies, including the Curie-Weiss and 2D nearest-neighbor Ising systems. We find that under realistic 3\\% Gaussian measurement noise, BRAIN maintains 98\\% ground state fidelity, whereas Markov Chain Monte Carlo (MCMC) methods degrade to 51\\% fidelity. Furthermore, BRAIN reaches the MCMC-equivalent solution up to 192x faster under these conditions. BRAIN exhibits $\\mathcal{O}(N^{1.55})$ scaling up to 65,536 spins and maintains robustness against severe measurement uncertainty up to 40\\%. Beyond ground state optimization, BRAIN accurately captures thermodynamic phase transitions and metastable states, providing a scalable and noise-resilient method for utilizing analog computing architectures in complex optimizations.",
      "authors": [
        "Aditya Choudhary",
        "Saaketh Desai",
        "Prasad Iyer"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-02-09 20:07:42+00:00",
      "link": "https://arxiv.org/pdf/2602.09162v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09161v1",
      "title": "Minimum Distance Summaries for Robust Neural Posterior Estimation",
      "abstract": "Simulation-based inference (SBI) enables amortized Bayesian inference by first training a neural posterior estimator (NPE) on prior-simulator pairs, typically through low-dimensional summary statistics, which can then be cheaply reused for fast inference by querying it on new test observations. Because NPE is estimated under the training data distribution, it is susceptible to misspecification when observations deviate from the training distribution. Many robust SBI approaches address this by modifying NPE training or introducing error models, coupling robustness to the inference network and compromising amortization and modularity. We introduce minimum-distance summaries, a plug-in robust NPE method that adapts queried test-time summaries independently of the pretrained NPE. Leveraging the maximum mean discrepancy (MMD) as a distance between observed data and a summary-conditional predictive distribution, the adapted summary inherits strong robustness properties from the MMD. We demonstrate that the algorithm can be implemented efficiently with random Fourier feature approximations, yielding a lightweight, model-free test-time adaptation procedure. We provide theoretical guarantees for the robustness of our algorithm and empirically evaluate it on a range of synthetic and real-world tasks, demonstrating substantial robustness gains with minimal additional overhead.",
      "authors": [
        "Sherman Khoo",
        "Dennis Prangle",
        "Song Liu",
        "Mark Beaumont"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-09 20:06:15+00:00",
      "link": "https://arxiv.org/pdf/2602.09161v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09159v1",
      "title": "CoMMa: Contribution-Aware Medical Multi-Agents From A Game-Theoretic Perspective",
      "abstract": "Recent multi-agent frameworks have broadened the ability to tackle oncology decision support tasks that require reasoning over dynamic, heterogeneous patient data. We propose Contribution-Aware Medical Multi-Agents (CoMMa), a decentralized LLM-agent framework in which specialists operate on partitioned evidence and coordinate through a game-theoretic objective for robust decision-making. In contrast to most agent architectures relying on stochastic narrative-based reasoning, CoMMa utilizes deterministic embedding projections to approximate contribution-aware credit assignment. This yields explicit evidence attribution by estimating each agent's marginal utility, producing interpretable and mathematically grounded decision pathways with improved stability. Evaluated on diverse oncology benchmarks, including a real-world multidisciplinary tumor board dataset, CoMMa achieves higher accuracy and more stable performance than data-centralized and role-based multi-agents baselines.",
      "authors": [
        "Yichen Wu",
        "Yujin Oh",
        "Sangjoon Park",
        "Kailong Fan",
        "Dania Daye",
        "Hana Farzaneh",
        "Xiang Li",
        "Raul Uppot",
        "Quanzheng Li"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "published": "2026-02-09 20:04:58+00:00",
      "link": "https://arxiv.org/pdf/2602.09159v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09154v1",
      "title": "A Hybrid Deterministic Framework for Named Entity Extraction in Broadcast News Video",
      "abstract": "The growing volume of video-based news content has heightened the need for transparent and reliable methods to extract on-screen information. Yet the variability of graphical layouts, typographic conventions, and platform-specific design patterns renders manual indexing impractical. This work presents a comprehensive framework for automatically detecting and extracting personal names from broadcast and social-media-native news videos. It introduces a curated and balanced corpus of annotated frames capturing the diversity of contemporary news graphics and proposes an interpretable, modular extraction pipeline designed to operate under deterministic and auditable conditions.   The pipeline is evaluated against a contrasting class of generative multimodal methods, revealing a clear trade-off between deterministic auditability and stochastic inference. The underlying detector achieves 95.8% mAP@0.5, demonstrating operationally robust performance for graphical element localisation. While generative systems achieve marginally higher raw accuracy (F1: 84.18% vs 77.08%), they lack the transparent data lineage required for journalistic and analytical contexts. The proposed pipeline delivers balanced precision (79.9%) and recall (74.4%), avoids hallucination, and provides full traceability across each processing stage. Complementary user findings indicate that 59% of respondents report difficulty reading on-screen names in fast-paced broadcasts, underscoring the practical relevance of the task. The results establish a methodologically rigorous and interpretable baseline for hybrid multimodal information extraction in modern news media.",
      "authors": [
        "Andrea Filiberto Lucas",
        "Dylan Seychell"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "published": "2026-02-09 19:58:50+00:00",
      "link": "https://arxiv.org/pdf/2602.09154v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09147v1",
      "title": "Overview of PAN 2026: Voight-Kampff Generative AI Detection, Text Watermarking, Multi-Author Writing Style Analysis, Generative Plagiarism Detection, and Reasoning Trajectory Detection",
      "abstract": "The goal of the PAN workshop is to advance computational stylometry and text forensics via objective and reproducible evaluation. In 2026, we run the following five tasks: (1) Voight-Kampff Generative AI Detection, particularly in mixed and obfuscated authorship scenarios, (2) Text Watermarking, a new task that aims to find new and benchmark the robustness of existing text watermarking schemes, (3) Multi-author Writing Style Analysis, a continued task that aims to find positions of authorship change, (4) Generative Plagiarism Detection, a continued task that targets source retrieval and text alignment between generated text and source documents, and (5) Reasoning Trajectory Detection, a new task that deals with source detection and safety detection of LLM-generated or human-written reasoning trajectories. As in previous years, PAN invites software submissions as easy-to-reproduce Docker containers for most of the tasks. Since PAN 2012, more than 1,100 submissions have been made this way via the TIRA experimentation platform.",
      "authors": [
        "Janek Bevendorff",
        "Maik Fröbe",
        "André Greiner-Petter",
        "Andreas Jakoby",
        "Maximilian Mayerl",
        "Preslav Nakov",
        "Henry Plutz",
        "Martin Potthast",
        "Benno Stein",
        "Minh Ngoc Ta",
        "Yuxia Wang",
        "Eva Zangerle"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-09 19:48:16+00:00",
      "link": "https://arxiv.org/pdf/2602.09147v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09132v1",
      "title": "SciDataCopilot: An Agentic Data Preparation Framework for AGI-driven Scientific Discovery",
      "abstract": "The current landscape of AI for Science (AI4S) is predominantly anchored in large-scale textual corpora, where generative AI systems excel at hypothesis generation, literature search, and multi-modal reasoning. However, a critical bottleneck for accelerating closed-loop scientific discovery remains the utilization of raw experimental data. Characterized by extreme heterogeneity, high specificity, and deep domain expertise requirements, raw data possess neither direct semantic alignment with linguistic representations nor structural homogeneity suitable for a unified embedding space. The disconnect prevents the emerging class of Artificial General Intelligence for Science (AGI4S) from effectively interfacing with the physical reality of experimentation. In this work, we extend the text-centric AI-Ready concept to Scientific AI-Ready data paradigm, explicitly formalizing how scientific data is specified, structured, and composed within a computational workflow. To operationalize this idea, we propose SciDataCopilot, an autonomous agentic framework designed to handle data ingestion, scientific intent parsing, and multi-modal integration in a end-to-end manner. By positioning data readiness as a core operational primitive, the framework provides a principled foundation for reusable, transferable systems, enabling the transition toward experiment-driven scientific general intelligence. Extensive evaluations across three heterogeneous scientific domains show that SciDataCopilot improves efficiency, scalability, and consistency over manual pipelines, with up to 30$\\times$ speedup in data preparation.",
      "authors": [
        "Jiyong Rao",
        "Yicheng Qiu",
        "Jiahui Zhang",
        "Juntao Deng",
        "Shangquan Sun",
        "Fenghua Ling",
        "Hao Chen",
        "Nanqing Dong",
        "Zhangyang Gao",
        "Siqi Sun",
        "Yuqiang Li",
        "Dongzhan Zhou",
        "Guangyu Wang",
        "Lijun Wu",
        "Conghui He",
        "Xuhong Wang",
        "Jing Shao",
        "Xiang Liu",
        "Yu Zhu",
        "Mianxin Liu",
        "Qihao Zheng",
        "Yinghui Zhang",
        "Jiamin Wu",
        "Xiaosong Wang",
        "Shixiang Tang",
        "Wenlong Zhang",
        "Bo Zhang",
        "Wanli Ouyang",
        "Runkai Zhao",
        "Chunfeng Song",
        "Lei Bai",
        "Chi Zhang"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.ET",
        "cs.MA"
      ],
      "published": "2026-02-09 19:23:32+00:00",
      "link": "https://arxiv.org/pdf/2602.09132v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09130v2",
      "title": "UniComp: A Unified Evaluation of Large Language Model Compression via Pruning, Quantization and Distillation",
      "abstract": "Model compression is increasingly essential for deploying large language models (LLMs), yet existing evaluations are limited in method coverage and focus primarily on knowledge-centric benchmarks. Thus, we introduce UniComp, a unified evaluation framework for comparing pruning, quantization, and knowledge distillation. UniComp evaluates compressed models along three dimensions: performance, reliability, and efficiency, using a diverse set of capability- and safety-oriented benchmarks together with a hardware-aware efficiency analysis. Through extensive evaluation of six compression techniques on modern LLMs across more than 40 datasets, we find that (i) compression exhibits a consistent knowledge bias, where knowledge-intensive tasks are relatively preserved while reasoning, multilingual, and instruction-following capabilities degrade substantially; (ii) quantization provides the best overall trade-off between retained performance and efficiency, whereas distillation yields strong runtime acceleration gains at high computational cost; and (iii) task-specific calibration can significantly improve the reasoning ability of pruned models by up to 50%.",
      "authors": [
        "Jonathan von Rad",
        "Yong Cao",
        "Andreas Geiger"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 19:20:56+00:00",
      "link": "https://arxiv.org/pdf/2602.09130v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09128v1",
      "title": "Counterfactual Maps: What They Are and How to Find Them",
      "abstract": "Counterfactual explanations are a central tool in interpretable machine learning, yet computing them exactly for complex models remains challenging. For tree ensembles, predictions are piecewise constant over a large collection of axis-aligned hyperrectangles, implying that an optimal counterfactual for a point corresponds to its projection onto the nearest rectangle with an alternative label under a chosen metric. Existing methods largely overlook this geometric structure, relying either on heuristics with no optimality guarantees or on mixed-integer programming formulations that do not scale to interactive use.   In this work, we revisit counterfactual generation through the lens of nearest-region search and introduce counterfactual maps, a global representation of recourse for tree ensembles. Leveraging the fact that any tree ensemble can be compressed into an equivalent partition of labeled hyperrectangles, we cast counterfactual search as the problem of identifying the generalized Voronoi cell associated with the nearest rectangle of an alternative label. This leads to an exact, amortized algorithm based on volumetric k-dimensional (KD) trees, which performs branch-and-bound nearest-region queries with explicit optimality certificates and sublinear average query time after a one-time preprocessing phase.   Our experimental analyses on several real datasets drawn from high-stakes application domains show that this approach delivers globally optimal counterfactual explanations with millisecond-level latency, achieving query times that are orders of magnitude faster than existing exact, cold-start optimization methods.",
      "authors": [
        "Awa Khouna",
        "Julien Ferry",
        "Thibaut Vidal"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 19:20:16+00:00",
      "link": "https://arxiv.org/pdf/2602.09128v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09127v2",
      "title": "Epistemic Throughput: Fundamental Limits of Attention-Constrained Inference",
      "abstract": "Recent generative and tool-using AI systems can surface a large volume of candidates at low marginal cost, yet only a small fraction can be checked carefully. This creates a decoder-side bottleneck: downstream decision-makers must form reliable posteriors from many public records under scarce attention. We formalize this regime via Attention-Constrained Inference (ACI), in which a cheap screening stage processes $K$ records and an expensive verification stage can follow up on at most $B$ of them. Under Bayes log-loss, we study the maximum achievable reduction in posterior uncertainty per window, which we call \\emph{epistemic throughput}. Our main result is a ``JaKoB'' scaling law showing that epistemic throughput has a baseline term that grows linearly with verification and prevalence, and an additional \\emph{information-leverage} term that scales as $\\sqrt{JKB}$, where $J$ summarizes screening quality. Thus, expanding cheap screening can nonlinearly amplify scarce verification, even when informative records are rare. We further show that this scaling is tight in a weak-screening limit, and that in the sparse-verification regime ($B \\ll K$), substantial leverage requires heavy-tailed score distributions; for light-tailed scores the amplification is only logarithmic.",
      "authors": [
        "Lei You"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "published": "2026-02-09 19:18:21+00:00",
      "link": "https://arxiv.org/pdf/2602.09127v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09116v2",
      "title": "Importance inversion transfer identifies shared principles for cross-domain learning",
      "abstract": "The capacity to transfer knowledge across scientific domains relies on shared organizational principles. However, existing transfer-learning methodologies often fail to bridge radically heterogeneous systems, particularly under severe data scarcity or stochastic noise. This study formalizes Explainable Cross-Domain Transfer Learning (X-CDTL), a framework unifying network science and explainable artificial intelligence to identify structural invariants that generalize across biological, linguistic, molecular, and social networks. By introducing the Importance Inversion Transfer (IIT) mechanism, the framework prioritizes domain-invariant structural anchors over idiosyncratic, highly discriminative features. In anomaly detection tasks, models guided by these principles achieve significant performance gains - exhibiting a 56% relative improvement in decision stability under extreme noise - over traditional baselines. These results provide evidence for a shared organizational signature across heterogeneous domains, establishing a principled paradigm for cross-disciplinary knowledge propagation. By shifting from opaque latent representations to explicit structural laws, this work advances machine learning as a robust engine for scientific discovery.",
      "authors": [
        "Daniele Caligiore"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "physics.soc-ph",
        "q-bio.QM"
      ],
      "published": "2026-02-09 19:06:52+00:00",
      "link": "https://arxiv.org/pdf/2602.09116v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09112v1",
      "title": "A Small-Scale System for Autoregressive Program Synthesis Enabling Controlled Experimentation",
      "abstract": "What research can be pursued with small models trained to complete true programs? Typically, researchers study program synthesis via large language models (LLMs) which introduce issues such as knowing what is in or out of distribution, understanding fine-tuning effects, understanding the effects of tokenization, and higher demand on compute and storage to carry out experiments. We present a system called Cadmus which includes an integer virtual machine (VM), a dataset composed of true programs of diverse tasks, and an autoregressive transformer model that is trained for under \\$200 of compute cost. The system can be used to study program completion, out-of-distribution representations, inductive reasoning, and instruction following in a setting where researchers have effective and affordable fine-grained control of the training distribution and the ability to inspect and instrument models. Smaller models working on complex reasoning tasks enable instrumentation and investigations that may be prohibitively expensive on larger models. To demonstrate that these tasks are complex enough to be of interest, we show that these Cadmus models outperform GPT-5 (by achieving 100\\% accuracy while GPT-5 has 95\\% accuracy) even on a simple task of completing correct, integer arithmetic programs in our domain-specific language (DSL) while providing transparency into the dataset's relationship to the problem. We also show that GPT-5 brings unknown priors into its reasoning process when solving the same tasks, demonstrating a confounding factor that prevents the use of large-scale LLMs for some investigations where the training set relationship to the task needs to be fully understood.",
      "authors": [
        "Russ Webb",
        "Jason Ramapuram"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-09 19:03:04+00:00",
      "link": "https://arxiv.org/pdf/2602.09112v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09009v1",
      "title": "ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling",
      "abstract": "Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.",
      "authors": [
        "Yilang Zhang",
        "Bingcong Li",
        "Niao He",
        "Georgios B. Giannakis"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-09 18:54:18+00:00",
      "link": "https://arxiv.org/pdf/2602.09009v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09008v1",
      "title": "ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification",
      "abstract": "Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.",
      "authors": [
        "Sijia Peng",
        "Yun Xiong",
        "Xi Chen",
        "Yi Xie",
        "Guanzhi Li",
        "Yanwei Yu",
        "Yangyong Zhu",
        "Zhiqiang Shen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 18:53:08+00:00",
      "link": "https://arxiv.org/pdf/2602.09008v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09866v1",
      "title": "SinFoS: A Parallel Dataset for Translating Sinhala Figures of Speech",
      "abstract": "Figures of Speech (FoS) consist of multi-word phrases that are deeply intertwined with culture. While Neural Machine Translation (NMT) performs relatively well with the figurative expressions of high-resource languages, it often faces challenges when dealing with low-resource languages like Sinhala due to limited available data. To address this limitation, we introduce a corpus of 2,344 Sinhala figures of speech with cultural and cross-lingual annotations. We examine this dataset to classify the cultural origins of the figures of speech and to identify their cross-lingual equivalents. Additionally, we have developed a binary classifier to differentiate between two types of FOS in the dataset, achieving an accuracy rate of approximately 92%. We also evaluate the performance of existing LLMs on this dataset. Our findings reveal significant shortcomings in the current capabilities of LLMs, as these models often struggle to accurately convey idiomatic meanings. By making this dataset publicly available, we offer a crucial benchmark for future research in low-resource NLP and culturally aware machine translation.",
      "authors": [
        "Johan Sofalas",
        "Dilushri Pavithra",
        "Nevidu Jayatilleke",
        "Ruvan Weerasinghe"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-09 18:48:06+00:00",
      "link": "https://arxiv.org/pdf/2602.09866v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09003v1",
      "title": "Data Science and Technology Towards AGI Part I: Tiered Data Management",
      "abstract": "The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.",
      "authors": [
        "Yudong Wang",
        "Zixuan Fu",
        "Hengyu Zhao",
        "Chen Zhao",
        "Chuyue Zhou",
        "Xinle Lin",
        "Hongya Lyu",
        "Shuaikang Xue",
        "Yi Yi",
        "Yingjiao Wang",
        "Zhi Zheng",
        "Yuzhou Zhang",
        "Jie Zhou",
        "Chaojun Xiao",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-09 18:47:51+00:00",
      "link": "https://arxiv.org/pdf/2602.09003v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09001v1",
      "title": "DirMoE: Dirichlet-routed Mixture of Experts",
      "abstract": "Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.",
      "authors": [
        "Amirhossein Vahidi",
        "Hesam Asadollahzadeh",
        "Navid Akhavan Attar",
        "Marie Moullet",
        "Kevin Ly",
        "Xingyi Yang",
        "Mohammad Lotfollahi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 18:45:43+00:00",
      "link": "https://arxiv.org/pdf/2602.09001v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09000v1",
      "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
      "abstract": "Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.",
      "authors": [
        "Ali Hatamizadeh",
        "Shrimai Prabhumoye",
        "Igor Gitman",
        "Ximing Lu",
        "Seungju Han",
        "Wei Ping",
        "Yejin Choi",
        "Jan Kautz"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-09 18:45:11+00:00",
      "link": "https://arxiv.org/pdf/2602.09000v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10149v1",
      "title": "Exploring Semantic Labeling Strategies for Third-Party Cybersecurity Risk Assessment Questionnaires",
      "abstract": "Third-Party Risk Assessment (TPRA) is a core cybersecurity practice for evaluating suppliers against standards such as ISO/IEC 27001 and NIST. TPRA questionnaires are typically drawn from large repositories of security and compliance questions, yet tailoring assessments to organizational needs remains a largely manual process. Existing retrieval approaches rely on keyword or surface-level similarity, which often fails to capture implicit assessment scope and control semantics.   This paper explores strategies for organizing and retrieving TPRA cybersecurity questions using semantic labels that describe both control domains and assessment scope. We compare direct question-level labeling with a Large Language Model (LLM) against a hybrid semi-supervised semantic labeling (SSSL) pipeline that clusters questions in embedding space, labels a small representative subset using an LLM, and propagates labels to remaining questions using k-Nearest Neighbors; we also compare downstream retrieval based on direct question similarity versus retrieval in the label space. We find that semantic labels can improve retrieval alignment when labels are discriminative and consistent, and that SSSL can generalize labels from a small labeled subset to large repositories while substantially reducing LLM usage and cost.",
      "authors": [
        "Ali Nour Eldin",
        "Mohamed Sellami",
        "Walid Gaaloul"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published": "2026-02-09 18:36:50+00:00",
      "link": "https://arxiv.org/pdf/2602.10149v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.08990v1",
      "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
      "abstract": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
      "authors": [
        "Shiyang Feng",
        "Runmin Ma",
        "Xiangchao Yan",
        "Yue Fan",
        "Yusong Hu",
        "Songtao Huang",
        "Shuaiyu Zhang",
        "Zongsheng Cao",
        "Tianshuo Peng",
        "Jiakang Yuan",
        "Zijie Guo",
        "Zhijie Zhong",
        "Shangheng Du",
        "Weida Wang",
        "Jinxin Shi",
        "Yuhao Zhou",
        "Xiaohan He",
        "Zhiyin Yu",
        "Fangchen Yu",
        "Qihao Zheng",
        "Jiamin Wu",
        "Mianxin Liu",
        "Chi Zhang",
        "Shaowei Hou",
        "Shuya Li",
        "Yankai Jiang",
        "Wenjie Lou",
        "Lilong Wang",
        "Zifu Wang",
        "Jiong Wang",
        "Wanghan Xu",
        "Yue Deng",
        "Dongrui Liu",
        "Yiheng Wang",
        "Wenlong Zhang",
        "Fenghua Ling",
        "Shufei Zhang",
        "Xiaosong Wang",
        "Shuangjia Zheng",
        "Xun Huang",
        "Siqi Sun",
        "Shuyue Hu",
        "Peng Ye",
        "Chunfeng Song",
        "Bin Wang",
        "Conghui He",
        "Yihao Liu",
        "Xin Li",
        "Qibin Hou",
        "Tao Chen",
        "Xiangyu Yue",
        "Bin Wang",
        "Liang He",
        "Dahua Lin",
        "Bowen Zhou",
        "Bo Zhang",
        "Lei Bai"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-09 18:36:06+00:00",
      "link": "https://arxiv.org/pdf/2602.08990v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.08984v1",
      "title": "Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models",
      "abstract": "We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.",
      "authors": [
        "Yuliang Liu",
        "Yunchong Song",
        "Yixuan Wang",
        "Kewen Ge",
        "Alex Lamb",
        "Qipeng Guo",
        "Kai Chen",
        "Bowen Zhou",
        "Zhouhan Lin"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-09 18:33:31+00:00",
      "link": "https://arxiv.org/pdf/2602.08984v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.08983v1",
      "title": "StretchTime: Adaptive Time Series Forecasting via Symplectic Attention",
      "abstract": "Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit \"time-warped\" dynamics where the effective flow of time decouples from the sampling index. In this work, we first formalize this misalignment and prove that rotary position embedding (RoPE) is mathematically incapable of representing non-affine temporal warping. To address this, we propose Symplectic Positional Embeddings (SyPE), a learnable encoding framework derived from Hamiltonian mechanics. SyPE strictly generalizes RoPE by extending the rotation group $\\mathrm{SO}(2)$ to the symplectic group $\\mathrm{Sp}(2,\\mathbb{R})$, modulated by a novel input-dependent adaptive warp module. By allowing the attention mechanism to adaptively dilate or contract temporal coordinates end-to-end, our approach captures locally varying periodicities without requiring pre-defined warping functions. We implement this mechanism in StretchTime, a multivariate forecasting architecture that achieves state-of-the-art performance on standard benchmarks, demonstrating superior robustness on datasets exhibiting non-stationary temporal dynamics.",
      "authors": [
        "Yubin Kim",
        "Viresh Pati",
        "Jevon Twitty",
        "Vinh Pham",
        "Shihao Yang",
        "Jiecheng Lu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-09 18:29:25+00:00",
      "link": "https://arxiv.org/pdf/2602.08983v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.08980v1",
      "title": "When do neural ordinary differential equations generalize on complex networks?",
      "abstract": "Neural ordinary differential equations (neural ODEs) can effectively learn dynamical systems from time series data, but their behavior on graph-structured data remains poorly understood, especially when applied to graphs with different size or structure than encountered during training. We study neural ODEs ($\\mathtt{nODE}$s) with vector fields following the Barabási-Barzel form, trained on synthetic data from five common dynamical systems on graphs. Using the $\\mathbb{S}^1$-model to generate graphs with realistic and tunable structure, we find that degree heterogeneity and the type of dynamical system are the primary factors in determining $\\mathtt{nODE}$s' ability to generalize across graph sizes and properties. This extends to $\\mathtt{nODE}$s' ability to capture fixed points and maintain performance amid missing data. Average clustering plays a secondary role in determining $\\mathtt{nODE}$ performance. Our findings highlight $\\mathtt{nODE}$s as a powerful approach to understanding complex systems but underscore challenges emerging from degree heterogeneity and clustering in realistic graphs.",
      "authors": [
        "Moritz Laber",
        "Tina Eliassi-Rad",
        "Brennan Klein"
      ],
      "primary_category": "physics.soc-ph",
      "categories": [
        "physics.soc-ph",
        "cs.LG",
        "cs.SI",
        "stat.ML"
      ],
      "published": "2026-02-09 18:28:41+00:00",
      "link": "https://arxiv.org/pdf/2602.08980v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.08979v1",
      "title": "Beyond Transcripts: A Renewed Perspective on Audio Chaptering",
      "abstract": "Audio chaptering, the task of automatically segmenting long-form audio into coherent sections, is increasingly important for navigating podcasts, lectures, and videos. Despite its relevance, research remains limited and text-based, leaving key questions unresolved about leveraging audio information, handling ASR errors, and transcript-free evaluation. We address these gaps through three contributions: (1) a systematic comparison between text-based models with acoustic features, a novel audio-only architecture (AudioSeg) operating on learned audio representations, and multimodal LLMs; (2) empirical analysis of factors affecting performance, including transcript quality, acoustic features, duration, and speaker composition; and (3) formalized evaluation protocols contrasting transcript-dependent text-space protocols with transcript-invariant time-space protocols. Our experiments on YTSeg reveal that AudioSeg substantially outperforms text-based approaches, pauses provide the largest acoustic gains, and MLLMs remain limited by context length and weak instruction following, yet MLLMs are promising on shorter audio.",
      "authors": [
        "Fabian Retkowski",
        "Maike Züfle",
        "Thai Binh Nguyen",
        "Jan Niehues",
        "Alexander Waibel"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "published": "2026-02-09 18:28:10+00:00",
      "link": "https://arxiv.org/pdf/2602.08979v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.08976v1",
      "title": "Distributionally Robust Optimization via Generative Ambiguity Modeling",
      "abstract": "This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.",
      "authors": [
        "Jiaqi Wen",
        "Jianyi Yang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 18:20:29+00:00",
      "link": "https://arxiv.org/pdf/2602.08976v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09080v1",
      "title": "Looping Back to Move Forward: Recursive Transformers for Efficient and Flexible Large Multimodal Models",
      "abstract": "Large Multimodal Models (LMMs) have achieved remarkable success in vision-language tasks, yet their vast parameter counts are often underutilized during both training and inference. In this work, we embrace the idea of looping back to move forward: reusing model parameters through recursive refinement to extract stronger multimodal representations without increasing model size. We propose RecursiveVLM, a recursive Transformer architecture tailored for LMMs. Two key innovations enable effective looping: (i) a Recursive Connector that aligns features across recursion steps by fusing intermediate-layer hidden states and applying modality-specific projections, respecting the distinct statistical structures of vision and language tokens; (ii) a Monotonic Recursion Loss that supervises every step and guarantees performance improves monotonically with recursion depth. This design transforms recursion into an on-demand refinement mechanism: delivering strong results with few loops on resource-constrained devices and progressively improving outputs when more computation resources are available. Experiments show consistent gains of +3% over standard Transformers and +7% over vanilla recursive baselines, demonstrating that strategic looping is a powerful path toward efficient, deployment-adaptive LMMs.",
      "authors": [
        "Ruihan Xu",
        "Yuting Gao",
        "Lan Wang",
        "Jianing Li",
        "Weihao Chen",
        "Qingpei Guo",
        "Ming Yang",
        "Shiliang Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-09 17:58:23+00:00",
      "link": "https://arxiv.org/pdf/2602.09080v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16711v1",
      "title": "TeCoNeRV: Leveraging Temporal Coherence for Compressible Neural Representations for Videos",
      "abstract": "Implicit Neural Representations (INRs) have recently demonstrated impressive performance for video compression. However, since a separate INR must be overfit for each video, scaling to high-resolution videos while maintaining encoding efficiency remains a significant challenge. Hypernetwork-based approaches predict INR weights (hyponetworks) for unseen videos at high speeds, but with low quality, large compressed size, and prohibitive memory needs at higher resolutions. We address these fundamental limitations through three key contributions: (1) an approach that decomposes the weight prediction task spatially and temporally, by breaking short video segments into patch tubelets, to reduce the pretraining memory overhead by 20$\\times$; (2) a residual-based storage scheme that captures only differences between consecutive segment representations, significantly reducing bitstream size; and (3) a temporal coherence regularization framework that encourages changes in the weight space to be correlated with video content. Our proposed method, TeCoNeRV, achieves substantial improvements of 2.47dB and 5.35dB PSNR over the baseline at 480p and 720p on UVG, with 36% lower bitrates and 1.5-3$\\times$ faster encoding speeds. With our low memory usage, we are the first hypernetwork approach to demonstrate results at 480p, 720p and 1080p on UVG, HEVC and MCL-JCV. Our project page is available at https://namithap10.github.io/teconerv/ .",
      "authors": [
        "Namitha Padmanabhan",
        "Matthew Gwilliam",
        "Abhinav Shrivastava"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-18 18:59:55+00:00",
      "link": "https://arxiv.org/pdf/2602.16711v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16709v1",
      "title": "Knowledge-Embedded Latent Projection for Robust Representation Learning",
      "abstract": "Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.",
      "authors": [
        "Weijing Tang",
        "Ming Yuan",
        "Zongqi Xia",
        "Tianxi Cai"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.ME"
      ],
      "published": "2026-02-18 18:58:16+00:00",
      "link": "https://arxiv.org/pdf/2602.16709v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16707v1",
      "title": "E-Graphs as a Persistent Compiler Abstraction",
      "abstract": "Recent algorithmic advances have made equality saturation an appealing approach to program optimization because it avoids the phase-ordering problem. Existing work uses external equality saturation libraries, or custom implementations that are deeply tied to the specific application. However, these works only apply equality saturation at a single level of abstraction, or discard the discovered equalities when code is transformed by other compiler passes. We propose an alternative approach that represents an e-graph natively in the compiler's intermediate representation, facilitating the application of constructive compiler passes that maintain the e-graph state throughout the compilation flow. We build on a Python-based MLIR framework, xDSL, and introduce a new MLIR dialect, eqsat, that represents e-graphs in MLIR code. We show that this representation expands the scope of equality saturation in the compiler, allowing us to interleave pattern rewriting with other compiler transformations. The eqsat dialect provides a unified abstraction for compilers to utilize equality saturation across various levels of intermediate representations concurrently within the same MLIR flow.",
      "authors": [
        "Jules Merckx",
        "Alexandre Lopoukhine",
        "Samuel Coward",
        "Jianyi Cheng",
        "Bjorn De Sutter",
        "Tobias Grosser"
      ],
      "primary_category": "cs.PL",
      "categories": [
        "cs.PL"
      ],
      "published": "2026-02-18 18:56:52+00:00",
      "link": "https://arxiv.org/pdf/2602.16707v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16704v1",
      "title": "Reinforced Fast Weights with Next-Sequence Prediction",
      "abstract": "Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.",
      "authors": [
        "Hee Seung Hwang",
        "Xindi Wu",
        "Sanghyuk Chun",
        "Olga Russakovsky"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-18 18:53:18+00:00",
      "link": "https://arxiv.org/pdf/2602.16704v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16703v1",
      "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
      "abstract": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.",
      "authors": [
        "Shen Zhou Hong",
        "Alex Kleinman",
        "Alyssa Mathiowetz",
        "Adam Howes",
        "Julian Cohen",
        "Suveer Ganta",
        "Alex Letizia",
        "Dora Liao",
        "Deepika Pahari",
        "Xavier Roberts-Gaal",
        "Luca Righetti",
        "Joe Torres"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "published": "2026-02-18 18:51:28+00:00",
      "link": "https://arxiv.org/pdf/2602.16703v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16702v1",
      "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
      "abstract": "Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \\emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.",
      "authors": [
        "Mingjia Shi",
        "Yinhan He",
        "Yaochen Zhu",
        "Jundong Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-18 18:49:56+00:00",
      "link": "https://arxiv.org/pdf/2602.16702v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16684v1",
      "title": "Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition",
      "abstract": "Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.",
      "authors": [
        "Bo Pan",
        "Peter Zhiping Zhang",
        "Hao-Wei Pang",
        "Alex Zhu",
        "Xiang Yu",
        "Liying Zhang",
        "Liang Zhao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 18:27:21+00:00",
      "link": "https://arxiv.org/pdf/2602.16684v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16673v1",
      "title": "Neighborhood Stability as a Measure of Nearest Neighbor Searchability",
      "abstract": "Clustering-based Approximate Nearest Neighbor Search (ANNS) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query. Despite its popularity, there are virtually no analytical tools to determine the suitability of clustering-based ANNS for a given dataset -- what we call \"searchability.\" To address that gap, we present two measures for flat clusterings of high-dimensional points in Euclidean space. First is Clustering-Neighborhood Stability Measure (clustering-NSM), an internal measure of clustering quality -- a function of a clustering of a dataset -- that we show to be predictive of ANNS accuracy. The second, Point-Neighborhood Stability Measure (point-NSM), is a measure of clusterability -- a function of the dataset itself -- that is predictive of clustering-NSM. The two together allow us to determine whether a dataset is searchable by clustering-based ANNS given only the data points. Importantly, both are functions of nearest neighbor relationships between points, not distances, making them applicable to various distance functions including inner product.",
      "authors": [
        "Thomas Vecchiato",
        "Sebastian Bruch"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "published": "2026-02-18 18:09:47+00:00",
      "link": "https://arxiv.org/pdf/2602.16673v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16664v1",
      "title": "Unpaired Image-to-Image Translation via a Self-Supervised Semantic Bridge",
      "abstract": "Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations. Adversarial approaches require target-domain adversarial loss during training, which can limit generalization to unseen data, while diffusion-inversion methods often produce low-fidelity translations due to imperfect inversion into noise-latent representations. In this work, we propose the Self-Supervised Semantic Bridge (SSB), a versatile framework that integrates external semantic priors into diffusion bridge models to enable spatially faithful translation without cross-domain supervision. Our key idea is to leverage self-supervised visual encoders to learn representations that are invariant to appearance changes but capture geometric structure, forming a shared latent space that conditions the diffusion bridges. Extensive experiments show that SSB outperforms strong prior methods for challenging medical image synthesis in both in-domain and out-of-domain settings, and extends easily to high-quality text-guided editing.",
      "authors": [
        "Jiaming Liu",
        "Felix Petersen",
        "Yunhe Gao",
        "Yabin Zhang",
        "Hyojin Kim",
        "Akshay S. Chaudhari",
        "Yu Sun",
        "Stefano Ermon",
        "Sergios Gatidis"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-18 18:05:00+00:00",
      "link": "https://arxiv.org/pdf/2602.16664v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16660v1",
      "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
      "abstract": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.",
      "authors": [
        "Yuyan Bu",
        "Xiaohao Liu",
        "ZhaoXing Ren",
        "Yaodong Yang",
        "Juntao Dai"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-18 18:01:23+00:00",
      "link": "https://arxiv.org/pdf/2602.16660v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16650v1",
      "title": "Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System",
      "abstract": "Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. Retrieval-augmented generation (RAG) offers a promising way to overcome this limitation by combining large language models (LLMs) with external retrieval, but its effectiveness depends strongly on how domain knowledge is represented. In this work, we develop two retrieval pipelines: a dense semantic vector-based approach (VectorRAG) and a graph-based approach (GraphRAG). Using over 1,000 polyhydroxyalkanoate (PHA) papers, we construct context-preserving paragraph embeddings and a canonicalized structured knowledge graph supporting entity disambiguation and multi-hop reasoning. We evaluate these pipelines through standard retrieval metrics, comparisons with general state-of-the-art systems such as GPT and Gemini, and qualitative validation by a domain chemist. The results show that GraphRAG achieves higher precision and interpretability, while VectorRAG provides broader recall, highlighting complementary trade-offs. Expert validation further confirms that the tailored pipelines, particularly GraphRAG, produce well-grounded, citation-reliable responses with strong domain relevance. By grounding every statement in evidence, these systems enable researchers to navigate the literature, compare findings across studies, and uncover patterns that are difficult to extract manually. More broadly, this work establishes a practical framework for building materials science assistants using curated corpora and retrieval design, reducing reliance on proprietary models while enabling trustworthy literature analysis at scale.",
      "authors": [
        "Sonakshi Gupta",
        "Akhlak Mahmood",
        "Wei Xiong",
        "Rampi Ramprasad"
      ],
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "published": "2026-02-18 17:46:09+00:00",
      "link": "https://arxiv.org/pdf/2602.16650v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16640v1",
      "title": "Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval",
      "abstract": "The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a \"resource divide.\" State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes \"lexical density\" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.",
      "authors": [
        "Subrit Dikshit"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-18 17:29:43+00:00",
      "link": "https://arxiv.org/pdf/2602.16640v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16612v1",
      "title": "Causal and Compositional Abstraction",
      "abstract": "Abstracting from a low level to a more explanatory high level of description, and ideally while preserving causal structure, is fundamental to scientific practice, to causal inference problems, and to robust, efficient and interpretable AI. We present a general account of abstractions between low and high level models as natural transformations, focusing on the case of causal models. This provides a new formalisation of causal abstraction, unifying several notions in the literature, including constructive causal abstraction, Q-$τ$ consistency, abstractions based on interchange interventions, and `distributed' causal abstractions. Our approach is formalised in terms of category theory, and uses the general notion of a compositional model with a given set of queries and semantics in a monoidal, cd- or Markov category; causal models and their queries such as interventions being special cases. We identify two basic notions of abstraction: downward abstractions mapping queries from high to low level; and upward abstractions, mapping concrete queries such as Do-interventions from low to high. Although usually presented as the latter, we show how common causal abstractions may, more fundamentally, be understood in terms of the former. Our approach also leads us to consider a new stronger notion of `component-level' abstraction, applying to the individual components of a model. In particular, this yields a novel, strengthened form of constructive causal abstraction at the mechanism-level, for which we prove characterisation results. Finally, we show that abstraction can be generalised to further compositional models, including those with a quantum semantics implemented by quantum circuits, and we take first steps in exploring abstractions between quantum compositional circuit models and high-level classical causal models as a means to explainable quantum AI.",
      "authors": [
        "Robin Lorenz",
        "Sean Tull"
      ],
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO",
        "cs.AI",
        "math.CT",
        "quant-ph"
      ],
      "published": "2026-02-18 17:06:09+00:00",
      "link": "https://arxiv.org/pdf/2602.16612v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16610v1",
      "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
      "abstract": "Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.",
      "authors": [
        "Mengjie Qian",
        "Guangzhi Sun",
        "Mark J. F. Gales",
        "Kate M. Knill"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-18 17:04:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16610v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16608v1",
      "title": "Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models",
      "abstract": "Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attention patterns without unification, and lack context-awareness of inter-token dependencies and structural components. They also fail to capture how relevance evolves across layers and how structural components shape decision-making. To address these limitations, we proposed the \\textbf{Context-Aware Layer-wise Integrated Gradients (CA-LIG) Framework}, a unified hierarchical attribution framework that computes layer-wise Integrated Gradients within each Transformer block and fuses these token-level attributions with class-specific attention gradients. This integration yields signed, context-sensitive attribution maps that capture supportive and opposing evidence while tracing the hierarchical flow of relevance through the Transformer layers. We evaluate the CA-LIG Framework across diverse tasks, domains, and transformer model families, including sentiment analysis and long and multi-class document classification with BERT, hate speech detection in a low-resource language setting with XLM-R and AfroLM, and image classification with Masked Autoencoder vision Transformer model. Across all tasks and architectures, CA-LIG provides more faithful attributions, shows stronger sensitivity to contextual dependencies, and produces clearer, more semantically coherent visualizations than established explainability methods. These results indicate that CA-LIG provides a more comprehensive, context-aware, and reliable explanation of Transformer decision-making, advancing both the practical interpretability and conceptual understanding of deep neural models.",
      "authors": [
        "Melkamu Abay Mersha",
        "Jugal Kalita"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-18 17:03:10+00:00",
      "link": "https://arxiv.org/pdf/2602.16608v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16607v1",
      "title": "CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes",
      "abstract": "Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.",
      "authors": [
        "Miguel Marques",
        "Ana Luísa Fernandes",
        "Ana Filipa Pacheco",
        "Rute Rebouças",
        "Inês Cantante",
        "José Isidro",
        "Luís Filipe Cunha",
        "Alípio Jorge",
        "Nuno Guimarães",
        "Sérgio Nunes",
        "António Leal",
        "Purificação Silvano",
        "Ricardo Campos"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-18 17:03:07+00:00",
      "link": "https://arxiv.org/pdf/2602.16607v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16601v1",
      "title": "Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study",
      "abstract": "Machine learning models are increasingly trained or fine-tuned on synthetic data. Recursively training on such data has been observed to significantly degrade performance in a wide range of tasks, often characterized by a progressive drift away from the target distribution. In this work, we theoretically analyze this phenomenon in the setting of score-based diffusion models. For a realistic pipeline where each training round uses a combination of synthetic data and fresh samples from the target distribution, we obtain upper and lower bounds on the accumulated divergence between the generated and target distributions. This allows us to characterize different regimes of drift, depending on the score estimation error and the proportion of fresh data used in each generation. We also provide empirical results on synthetic data and images to illustrate the theory.",
      "authors": [
        "Nail B. Khelifa",
        "Richard E. Turner",
        "Ramji Venkataramanan"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-18 16:56:36+00:00",
      "link": "https://arxiv.org/pdf/2602.16601v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16585v1",
      "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
      "abstract": "Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a single formal system where data structure, computational dependencies, and integrity constraints are all queryable, enforceable, and machine-readable. Four technical innovations extend this foundation: object-augmented schemas integrating relational metadata with scalable object storage, semantic matching using attribute lineage to prevent erroneous joins, an extensible type system for domain-specific formats, and distributed job coordination designed for composability with external orchestration. By unifying data structure, data, and computational transformations, DataJoint creates a substrate for SciOps where agents can participate in scientific workflows without risking data corruption.",
      "authors": [
        "Dimitri Yatsenko",
        "Thinh T. Nguyen"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "published": "2026-02-18 16:35:47+00:00",
      "link": "https://arxiv.org/pdf/2602.16585v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16571v1",
      "title": "Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset",
      "abstract": "Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the \"numeric ambiguity\" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling.",
      "authors": [
        "Zhuqian Zhou",
        "Kirk Vanacore",
        "Bakhtawar Ahtisham",
        "Jinsook Lee",
        "Doug Pietrzak",
        "Daryl Hedley",
        "Jorge Dias",
        "Chris Shaw",
        "Ruth Schäfer",
        "René F. Kizilcec"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-18 16:12:46+00:00",
      "link": "https://arxiv.org/pdf/2602.16571v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16570v1",
      "title": "Steering diffusion models with quadratic rewards: a fine-grained analysis",
      "abstract": "Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved.   In this paper, we consider the task of sampling from a reward-tilted diffusion model -- that is, sampling from $p^{\\star}(x) \\propto p(x) \\exp(r(x))$ -- given a reward function $r$ and pre-trained diffusion oracle for $p$. We provide a fine-grained analysis of the computational tractability of this task for quadratic rewards $r(x) = x^\\top A x + b^\\top x$. We show that linear-reward tilts are always efficiently sampleable -- a simple result that seems to have gone unnoticed in the literature. We use this as a building block, along with a conceptually new ingredient -- the Hubbard-Stratonovich transform -- to provide an efficient algorithm for sampling from low-rank positive-definite quadratic tilts, i.e. $r(x) = x^\\top A x$ where $A$ is positive-definite and of rank $O(1)$. For negative-definite tilts, i.e. $r(x) = - x^\\top A x$ where $A$ is positive-definite, we prove that the problem is intractable even if $A$ is of rank 1 (albeit with exponentially-large entries).",
      "authors": [
        "Ankur Moitra",
        "Andrej Risteski",
        "Dhruv Rohatgi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DS"
      ],
      "published": "2026-02-18 16:11:17+00:00",
      "link": "https://arxiv.org/pdf/2602.16570v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16568v1",
      "title": "Separating Oblivious and Adaptive Models of Variable Selection",
      "abstract": "Sparse recovery is among the most well-studied problems in learning theory and high-dimensional statistics. In this work, we investigate the statistical and computational landscapes of sparse recovery with $\\ell_\\infty$ error guarantees. This variant of the problem is motivated by \\emph{variable selection} tasks, where the goal is to estimate the support of a $k$-sparse signal in $\\mathbb{R}^d$. Our main contribution is a provable separation between the \\emph{oblivious} (``for each'') and \\emph{adaptive} (``for all'') models of $\\ell_\\infty$ sparse recovery. We show that under an oblivious model, the optimal $\\ell_\\infty$ error is attainable in near-linear time with $\\approx k\\log d$ samples, whereas in an adaptive model, $\\gtrsim k^2$ samples are necessary for any algorithm to achieve this bound. This establishes a surprising contrast with the standard $\\ell_2$ setting, where $\\approx k \\log d$ samples suffice even for adaptive sparse recovery. We conclude with a preliminary examination of a \\emph{partially-adaptive} model, where we show nontrivial variable selection guarantees are possible with $\\approx k\\log d$ measurements.",
      "authors": [
        "Ziyun Chen",
        "Jerry Li",
        "Kevin Tian",
        "Yusong Zhu"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "cs.DS",
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "published": "2026-02-18 16:10:35+00:00",
      "link": "https://arxiv.org/pdf/2602.16568v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16554v1",
      "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
      "abstract": "We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for semantic review. We evaluate MerLean on three theoretical quantum computing papers producing 2,050 Lean declarations from 114 statements in total. MerLean achieves end-to-end formalization on all three papers, reducing the verification burden to only the newly introduced definitions and axioms. Our results demonstrate that agentic autoformalization can scale to frontier research, offering both a practical tool for machine-verified peer review and a scalable engine for mining high-quality synthetic data to train future reasoning models. Our approach can also be generalized to any other rigorous research in mathematics and theoretical physics.",
      "authors": [
        "Yuanjie Ren",
        "Jinzheng Li",
        "Yidi Qi"
      ],
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.ET",
        "quant-ph"
      ],
      "published": "2026-02-18 15:54:32+00:00",
      "link": "https://arxiv.org/pdf/2602.16554v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16551v1",
      "title": "Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation",
      "abstract": "The preservation of cultural heritage is increasingly transitioning towards data-driven predictive maintenance and \"Digital Twin\" construction. However, the mechanical constitutive models required for high-fidelity simulations remain fragmented across decades of unstructured scientific literature, creating a \"Data Silo\" that hinders conservation engineering. To address this, we present an automated, two-stage agentic framework leveraging Large Language Models (LLMs) to extract mechanical constitutive equations, calibrated parameters, and metadata from PDF documents. The workflow employs a resource-efficient \"Gatekeeper\" agent for relevance filtering and a high-capability \"Analyst\" agent for fine-grained extraction, featuring a novel Context-Aware Symbolic Grounding mechanism to resolve mathematical ambiguities. Applied to a corpus of over 2,000 research papers, the system successfully isolated 113 core documents and constructed a structured database containing 185 constitutive model instances and over 450 calibrated parameters. The extraction precision reached 80.4\\%, establishing a highly efficient \"Human-in-the-loop\" workflow that reduces manual data curation time by approximately 90\\%. We demonstrate the system's utility through a web-based Knowledge Retrieval Platform, which enables rapid parameter discovery for computational modeling. This work transforms scattered literature into a queryable digital asset, laying the data foundation for the \"Digital Material Twin\" of built heritage.",
      "authors": [
        "Rui Hu",
        "Yue Wu",
        "Tianhao Su",
        "Yin Wang",
        "Shunbo Hu",
        "Jizhong Huang"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-18 15:53:15+00:00",
      "link": "https://arxiv.org/pdf/2602.16551v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16537v1",
      "title": "Optimal training-conditional regret for online conformal prediction",
      "abstract": "We study online conformal prediction for non-stationary data streams subject to unknown distribution drift. While most prior work studied this problem under adversarial settings and/or assessed performance in terms of gaps of time-averaged marginal coverage, we instead evaluate performance through training-conditional cumulative regret. We specifically focus on independently generated data with two types of distribution shift: abrupt change points and smooth drift.   When non-conformity score functions are pretrained on an independent dataset, we propose a split-conformal style algorithm that leverages drift detection to adaptively update calibration sets, which provably achieves minimax-optimal regret. When non-conformity scores are instead trained online, we develop a full-conformal style algorithm that again incorporates drift detection to handle non-stationarity; this approach relies on stability - rather than permutation symmetry - of the model-fitting algorithm, which is often better suited to online learning under evolving environments. We establish non-asymptotic regret guarantees for our online full conformal algorithm, which match the minimax lower bound under appropriate restrictions on the prediction sets. Numerical experiments corroborate our theoretical findings.",
      "authors": [
        "Jiadong Liang",
        "Zhimei Ren",
        "Yuxin Chen"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "cs.IT",
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-18 15:31:15+00:00",
      "link": "https://arxiv.org/pdf/2602.16537v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16531v1",
      "title": "Transfer Learning of Linear Regression with Multiple Pretrained Models: Benefiting from More Pretrained Models via Overparameterization Debiasing",
      "abstract": "We study transfer learning for a linear regression task using several least-squares pretrained models that can be overparameterized.   We formulate the target learning task as optimization that minimizes squared errors on the target dataset with penalty on the distance of the learned model from the pretrained models. We analytically formulate the test error of the learned target model and provide the corresponding empirical evaluations.   Our results elucidate when using more pretrained models can improve transfer learning. Specifically, if the pretrained models are overparameterized, using sufficiently many of them is important for beneficial transfer learning. However, the learning may be compromised by overparameterization bias of pretrained models, i.e., the minimum $\\ell_2$-norm solution's restriction to a small subspace spanned by the training examples in the high-dimensional parameter space. We propose a simple debiasing via multiplicative correction factor that can reduce the overparameterization bias and leverage more pretrained models to learn a target predictor.",
      "authors": [
        "Daniel Boharon",
        "Yehuda Dar"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 15:19:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16531v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16530v1",
      "title": "FEKAN: Feature-Enriched Kolmogorov-Arnold Networks",
      "abstract": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a compelling alternative to multilayer perceptrons, offering enhanced interpretability via functional decomposition. However, existing KAN architectures, including spline-, wavelet-, radial-basis variants, etc., suffer from high computational cost and slow convergence, limiting scalability and practical applicability. Here, we introduce Feature-Enriched Kolmogorov-Arnold Networks (FEKAN), a simple yet effective extension that preserves all the advantages of KAN while improving computational efficiency and predictive accuracy through feature enrichment, without increasing the number of trainable parameters. By incorporating these additional features, FEKAN accelerates convergence, increases representation capacity, and substantially mitigates the computational overhead characteristic of state-of-the-art KAN architectures. We investigate FEKAN across a comprehensive set of benchmarks, including function-approximation tasks, physics-informed formulations for diverse partial differential equations (PDEs), and neural operator settings that map between input and output function spaces. For function approximation, we systematically compare FEKAN against a broad family of KAN variants, FastKAN, WavKAN, ReLUKAN, HRKAN, ChebyshevKAN, RBFKAN, and the original SplineKAN. Across all tasks, FEKAN demonstrates substantially faster convergence and consistently higher approximation accuracy than the underlying baseline architectures. We also establish the theoretical foundations for FEKAN, showing its superior representation capacity compared to KAN, which contributes to improved accuracy and efficiency.",
      "authors": [
        "Sidharth S. Menon",
        "Ameya D. Jagtap"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math-ph"
      ],
      "published": "2026-02-18 15:17:55+00:00",
      "link": "https://arxiv.org/pdf/2602.16530v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16516v1",
      "title": "Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification",
      "abstract": "This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention.",
      "authors": [
        "Taja Kuzman Pungeršek",
        "Peter Rupnik",
        "Daniela Širinić",
        "Nikola Ljubešić"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-18 15:04:30+00:00",
      "link": "https://arxiv.org/pdf/2602.16516v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16512v1",
      "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs",
      "abstract": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.",
      "authors": [
        "Felix Fricke",
        "Simon Malberg",
        "Georg Groh"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-18 14:58:25+00:00",
      "link": "https://arxiv.org/pdf/2602.16512v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16505v1",
      "title": "Functional Decomposition and Shapley Interactions for Interpreting Survival Models",
      "abstract": "Hazard and survival functions are natural, interpretable targets in time-to-event prediction, but their inherent non-additivity fundamentally limits standard additive explanation methods. We introduce Survival Functional Decomposition (SurvFD), a principled approach for analyzing feature interactions in machine learning survival models. By decomposing higher-order effects into time-dependent and time-independent components, SurvFD offers a previously unrecognized perspective on survival explanations, explicitly characterizing when and why additive explanations fail. Building on this theoretical decomposition, we propose SurvSHAP-IQ, which extends Shapley interactions to time-indexed functions, providing a practical estimator for higher-order, time-dependent interactions. Together, SurvFD and SurvSHAP-IQ establish an interaction- and time-aware interpretability approach for survival modeling, with broad applicability across time-to-event prediction tasks.",
      "authors": [
        "Sophie Hanna Langbein",
        "Hubert Baniecki",
        "Fabian Fumagalli",
        "Niklas Koenen",
        "Marvin N. Wright",
        "Julia Herbinger"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-18 14:47:20+00:00",
      "link": "https://arxiv.org/pdf/2602.16505v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16503v1",
      "title": "Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects",
      "abstract": "Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \\emph{Conditionally Additive Local Models} (CALMs), a new model class, that balances the interpretability of GAMs with the accuracy of GA$^2$Ms. CALMs allow multiple univariate shape functions per feature, each active in different regions of the input space. These regions are defined independently for each feature as simple logical conditions (thresholds) on the features it interacts with. As a result, effects remain locally additive while varying across subregions to capture interactions. We further propose a principled distillation-based training pipeline that identifies homogeneous regions with limited interactions and fits interpretable shape functions via region-aware backfitting. Experiments on diverse classification and regression tasks show that CALMs consistently outperform GAMs and achieve accuracy comparable with GA$^2$Ms. Overall, CALMs offer a compelling trade-off between predictive accuracy and interpretability.",
      "authors": [
        "Vasilis Gkolemis",
        "Loukas Kavouras",
        "Dimitrios Kyriakopoulos",
        "Konstantinos Tsopelas",
        "Dimitrios Rontogiannis",
        "Giuseppe Casalicchio",
        "Theodore Dalamagas",
        "Christos Diou"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-18 14:45:33+00:00",
      "link": "https://arxiv.org/pdf/2602.16503v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16500v1",
      "title": "Optimizing Soft Prompt Tuning via Structural Evolution",
      "abstract": "Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.",
      "authors": [
        "Zhenzhen Huang",
        "Chaoning Zhang",
        "Haoyu Bian",
        "Songbo Zhang",
        "Chi-lok Andy Tai",
        "Jiaquan Zhang",
        "Caiyan Qin",
        "Jingjing Qu",
        "Yalan Ye",
        "Yang Yang",
        "Heng Tao Shen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-18 14:43:20+00:00",
      "link": "https://arxiv.org/pdf/2602.16500v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16498v1",
      "title": "Fast and Scalable Analytical Diffusion",
      "abstract": "Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\\bf 71 \\times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling.",
      "authors": [
        "Xinyi Shang",
        "Peng Sun",
        "Jingyu Lin",
        "Zhiqiang Shen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-18 14:41:09+00:00",
      "link": "https://arxiv.org/pdf/2602.16498v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16490v1",
      "title": "From Growing to Looping: A Unified View of Iterative Computation in LLMs",
      "abstract": "Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.",
      "authors": [
        "Ferdinand Kapl",
        "Emmanouil Angelis",
        "Kaitlin Maile",
        "Johannes von Oswald",
        "Stefan Bauer"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-18 14:25:16+00:00",
      "link": "https://arxiv.org/pdf/2602.16490v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16488v1",
      "title": "Learning to Learn from Language Feedback with Social Meta-Learning",
      "abstract": "Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.",
      "authors": [
        "Jonathan Cook",
        "Diego Antognini",
        "Martin Klissarov",
        "Claudiu Musat",
        "Edward Grefenstette"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-18 14:22:13+00:00",
      "link": "https://arxiv.org/pdf/2602.16488v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16481v1",
      "title": "Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach",
      "abstract": "Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.",
      "authors": [
        "Zihao Li",
        "Fabrizio Russo"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-18 14:15:21+00:00",
      "link": "https://arxiv.org/pdf/2602.16481v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16473v1",
      "title": "Synthesis and Verification of Transformer Programs",
      "abstract": "C-RASP is a simple programming language that was recently shown to capture concepts expressible by transformers. In this paper, we develop new algorithmic techniques for automatically verifying C-RASPs. To this end, we establish a connection to the verification of synchronous dataflow programs in Lustre, which enables us to exploit state-of-the-art model checkers utilizing highly optimized SMT-solvers. Our second contribution addresses learning a C-RASP program in the first place. To this end, we provide a new algorithm for learning a C-RASP from examples using local search. We demonstrate efficacy of our implementation for benchmarks of C-RASPs in the literature, in particular in connection to the following applications: (1) transformer program optimization, and (2) constrained learning of transformer programs (based on a partial specification).",
      "authors": [
        "Hongjian Jiang",
        "Matthew Hague",
        "Philipp Rümmer",
        "Anthony Widjaja Lin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.FL",
        "cs.LO"
      ],
      "published": "2026-02-18 14:04:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16473v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16469v1",
      "title": "Training Models on Dialects of Translationese Shows How Lexical Diversity and Source-Target Syntactic Similarity Shape Learning",
      "abstract": "Machine-translated data is widely used in multilingual NLP, particularly when native text is scarce. However, translated text differs systematically from native text. This phenomenon is known as translationese, and it reflects both traces of the source language and characteristic properties of translation itself. In this paper, we study how training on machine-translated data affects small English language models, focusing on how translationese from different source languages shapes linguistic acceptability judgments and language modelling for different domains. We train models on English text translated from 24 typologically and resource-diverse source languages, enabling a systematic analysis of how source language and corpus properties influence what models learn. Our results show that the source language has a clear impact on model behavior: general perplexity is more driven by the lexical diversity of the translated corpus, while grammatical performance is strongly correlated to typological similarity to English, given enough data.",
      "authors": [
        "Jenny Kunz"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-18 13:59:08+00:00",
      "link": "https://arxiv.org/pdf/2602.16469v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16468v1",
      "title": "HPMixer: Hierarchical Patching for Multivariate Time Series Forecasting",
      "abstract": "In long-term multivariate time series forecasting, effectively capturing both periodic patterns and residual dynamics is essential. To address this within standard deep learning benchmark settings, we propose the Hierarchical Patching Mixer (HPMixer), which models periodicity and residuals in a decoupled yet complementary manner. The periodic component utilizes a learnable cycle module [7] enhanced with a nonlinear channel-wise MLP for greater expressiveness. The residual component is processed through a Learnable Stationary Wavelet Transform (LSWT) to extract stable, shift-invariant frequency-domain representations. Subsequently, a channel-mixing encoder models explicit inter-channel dependencies, while a two-level non-overlapping hierarchical patching mechanism captures coarse- and fine-scale residual variations. By integrating decoupled periodicity modeling with structured, multi-scale residual learning, HPMixer provides an effective framework. Extensive experiments on standard multivariate benchmarks demonstrate that HPMixer achieves competitive or state-of-the-art performance compared to recent baselines.",
      "authors": [
        "Jung Min Choi",
        "Vijaya Krishna Yalavarthi",
        "Lars Schmidt-Thieme"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 13:59:04+00:00",
      "link": "https://arxiv.org/pdf/2602.16468v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16467v1",
      "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
      "abstract": "The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.",
      "authors": [
        "Saurabh Bharti",
        "Gaurav Azad",
        "Abhinaw Jagtap",
        "Nachiket Tapas"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-18 13:55:57+00:00",
      "link": "https://arxiv.org/pdf/2602.16467v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16465v1",
      "title": "The Complexity Landscape of Two-Stage Robust Selection Problems with Budgeted Uncertainty",
      "abstract": "A standard type of uncertainty set in robust optimization is budgeted uncertainty, where an interval of possible values for each parameter is given and the total deviation from their lower bounds is bounded. In the two-stage setting, discrete and continuous budgeted uncertainty have to be distinguished. The complexity of such problems is largely unexplored, in particular if the underlying nominal optimization problem is simple, such as for selection problems. In this paper, we give a comprehensive answer to long-standing open complexity questions for three types of selection problems and three types of budgeted uncertainty sets. In particular, we demonstrate that the two-stage selection problem with continuous budgeted uncertainty is NP-hard, while the corresponding two-stage representative selection problem is solvable in polynomial time. Our hardness result implies that also the two-stage assignment problem with continuous budgeted uncertainty is NP-hard.",
      "authors": [
        "Marc Goerigk",
        "Dorothee Henke",
        "Lasse Wulf"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.DS"
      ],
      "published": "2026-02-18 13:55:03+00:00",
      "link": "https://arxiv.org/pdf/2602.16465v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16456v1",
      "title": "Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC",
      "abstract": "Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank updates on top of frozen weights, dramatically reducing trainable parameters and memory. In this work, we address the gap between training with full steps with low-rank projections (SVDLoRA) and LoRA fine-tuning. We propose LoRSum, a memory-efficient subroutine that closes this gap for gradient descent by casting LoRA optimization as a proximal sub-problem and solving it efficiently with alternating least squares updates, which we prove to be an implicit block power method. We recover several recently proposed preconditioning methods for LoRA as special cases, and show that LoRSum can also be used for updating a low-rank momentum. In order to address full steps with preconditioned gradient descent, we propose a scaled variant of LoRSum that uses structured metrics such as K-FAC and Shampoo, and we show that storing the diagonal of these metrics still allows them to perform well while remaining memory-efficient. Experiments on a synthetic task, CIFAR-100, and language-model fine-tuning on GLUE, SQuAD v2, and WikiText-103, show that our method can match or improve LoRA baselines given modest compute overhead, while avoiding full-matrix SVD projections and retaining LoRA-style parameter efficiency.",
      "authors": [
        "Abdulla Jasem Almansoori",
        "Maria Ivanova",
        "Andrey Veprikov",
        "Aleksandr Beznosikov",
        "Samuel Horváth",
        "Martin Takáč"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 13:41:41+00:00",
      "link": "https://arxiv.org/pdf/2602.16456v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16449v1",
      "title": "GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation",
      "abstract": "Generative model evaluation commonly relies on high-dimensional embedding spaces to compute distances between samples. We show that dataset representations in these spaces are affected by the hubness phenomenon, which distorts nearest neighbor relationships and biases distance-based metrics. Building on the classical Iterative Contextual Dissimilarity Measure (ICDM), we introduce Generative ICDM (GICDM), a method to correct neighborhood estimation for both real and generated data. We introduce a multi-scale extension to improve empirical behavior. Extensive experiments on synthetic and real benchmarks demonstrate that GICDM resolves hubness-induced failures, restores reliable metric behavior, and improves alignment with human judgment.",
      "authors": [
        "Nicolas Salvy",
        "Hugues Talbot",
        "Bertrand Thirion"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2026-02-18 13:33:54+00:00",
      "link": "https://arxiv.org/pdf/2602.16449v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16442v1",
      "title": "Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA",
      "abstract": "As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA implementation of event-graph neural networks for audio processing. We utilise an artificial cochlea that converts time-series signals into sparse event data, reducing memory and computation costs. Our architecture was implemented on a SoC FPGA and evaluated on two open-source datasets. For classification task, our baseline floating-point model achieves 92.7% accuracy on SHD dataset - only 2.4% below the state of the art - while requiring over 10x and 67x fewer parameters. On SSC, our models achieve 66.9-71.0% accuracy. Compared to FPGA-based spiking neural networks, our quantised model reaches 92.3% accuracy, outperforming them by up to 19.3% while reducing resource usage and latency. For SSC, we report the first hardware-accelerated evaluation. We further demonstrate the first end-to-end FPGA implementation of event-audio keyword spotting, combining graph convolutional layers with recurrent sequence modelling. The system achieves up to 95% word-end detection accuracy, with only 10.53 microsecond latency and 1.18 W power consumption, establishing a strong benchmark for energy-efficient event-driven KWS.",
      "authors": [
        "Kamil Jeziorek",
        "Piotr Wzorek",
        "Krzysztof Blachut",
        "Hiroshi Nakano",
        "Manon Dampfhoffer",
        "Thomas Mesquida",
        "Hiroaki Nishi",
        "Thomas Dalgaty",
        "Tomasz Kryjak"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "published": "2026-02-18 13:26:22+00:00",
      "link": "https://arxiv.org/pdf/2602.16442v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16436v1",
      "title": "Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent",
      "abstract": "Releasing data once and for all under noninteractive Local Differential Privacy (LDP) enables complete data reusability, but the resulting noise may create bias in subsequent analyses. In this work, we leverage the Weierstrass transform to characterize this bias in binary classification. We prove that inverting this transform leads to a bias-correction method to compute unbiased estimates of nonlinear functions on examples released under LDP. We then build a novel stochastic gradient descent algorithm called Inverse Weierstrass Private SGD (IWP-SGD). It converges to the true population risk minimizer at a rate of $\\mathcal{O}(1/n)$, with $n$ the number of examples. We empirically validate IWP-SGD on binary classification tasks using synthetic and real-world datasets.",
      "authors": [
        "Jean Dufraiche",
        "Paul Mangold",
        "Michaël Perrot",
        "Marc Tommasi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CR",
        "stat.ML"
      ],
      "published": "2026-02-18 13:13:43+00:00",
      "link": "https://arxiv.org/pdf/2602.16436v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16435v1",
      "title": "Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning",
      "abstract": "Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.",
      "authors": [
        "Arun Vignesh Malarkkan",
        "Wangyang Ying",
        "Yanjie Fu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "published": "2026-02-18 13:12:11+00:00",
      "link": "https://arxiv.org/pdf/2602.16435v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16432v1",
      "title": "Bibby AI -- AI Latex Editor writing assistant for researchers vs Overleaf Alternative vs OpenAI Prism. (Bibby AI Latex Editor)",
      "abstract": "Large language models are increasingly integrated into academic writing workflows; however, the most widely used \\LaTeX\\ editors remain AI-peripheral -- offering compilation and collaboration, but no native intelligence. This separation forces researchers to leave their editing environment for AI assistance, fragmenting document context and interrupting writing flow. We present Bibby AI (trybibby.com), a native, AI-first \\LaTeX\\ editor that unifies the complete research writing lifecycle within a single interface. Bibby embeds an AI writing assistant, smart citation search, AI table and equation generation, an AI paper reviewer, abstract generator, literature review drafting, a deep research assistant, and real-time \\LaTeX\\ error detection and auto-fix -- all natively, without plugins or copy-paste workflows. We introduce LaTeXBench-500, a benchmark of 500 real-world compilation errors across six categories. Bibby achieves 91.4\\% detection accuracy and 83.7\\% one-click fix accuracy, outperforming Overleaf's native diagnostics (61.2\\%) and OpenAI Prism (78.3 / 64.1\\%) by large margins. Bibby demonstrates that a privacy-preserving, research-first AI editor can meaningfully accelerate every stage of academic manuscript preparation. We found that Bibby AI is a far superior alternative to overleaf latex and better than OpenAI Prism functionalities and AI.",
      "authors": [
        "Nilesh jain",
        "Rohit Yadav",
        "Andrej Karpathy"
      ],
      "primary_category": "cs.ET",
      "categories": [
        "cs.ET"
      ],
      "published": "2026-02-18 13:10:35+00:00",
      "link": "https://arxiv.org/pdf/2602.16432v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16430v1",
      "title": "Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems",
      "abstract": "Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context.",
      "authors": [
        "Ali Faraz",
        "Raja Kolla",
        "Ashish Kulkarni",
        "Shubham Agarwal"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-18 13:03:05+00:00",
      "link": "https://arxiv.org/pdf/2602.16430v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16429v1",
      "title": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers",
      "abstract": "Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.",
      "authors": [
        "Ido Levy",
        "Eilam Shapira",
        "Yinon Goldshtein",
        "Avi Yaeli",
        "Nir Mashkif",
        "Segev Shlomov"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-18 13:01:17+00:00",
      "link": "https://arxiv.org/pdf/2602.16429v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16427v1",
      "title": "Formalized Run-Time Analysis of Active Learning -- Coalgebraically in Agda",
      "abstract": "The objective of automata learning is to reconstruct the implementation of a hidden automaton, to which only a teacher has access. The learner can ask certain kinds of queries to the teacher to gain more knowledge about the hidden automaton. The run-time of such a learning algorithm is then measured in the number of queries it takes until the hidden automaton is successfully reconstructed, which is usually parametric in the number of states of that hidden automaton. How can we prove such a run-time complexity of learning algorithms in a proof assistant if we do not have the hidden automaton and the number of states available?   In the present paper, we solve this by considering learning algorithms themselves as generalized automata, more specifically as coalgebras. We introduce formal and yet compact definitions of what a learner and a teacher is, which make it easy to prove upper and lower bounds of different kinds of learning games in the proof assistant Agda.   As a running example, we discuss the common number guessing game where a teacher thinks of a natural number and answers guesses by the learner with `correct', `too high', or `too low'. To demonstrate our framework, we formally prove in Agda that both the lower and upper bound on number of guesses by the learner is $\\mathcal{O}(\\log n)$, where $n$ is the teacher's secret number.",
      "authors": [
        "Thorsten Wißmann"
      ],
      "primary_category": "cs.FL",
      "categories": [
        "cs.FL"
      ],
      "published": "2026-02-18 12:58:51+00:00",
      "link": "https://arxiv.org/pdf/2602.16427v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16422v1",
      "title": "Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model",
      "abstract": "Generating diagnostic text from histopathology whole slide images (WSIs) is challenging due to the gigapixel scale of the input and the requirement for precise, domain specific language. We propose a hierarchical vision language framework that combines a frozen pathology foundation model with a Transformer decoder for report generation. To make WSI processing tractable, we perform multi resolution pyramidal patch selection (downsampling factors 2^3 to 2^6) and remove background and artifacts using Laplacian variance and HSV based criteria. Patch features are extracted with the UNI Vision Transformer and projected to a 6 layer Transformer decoder that generates diagnostic text via cross attention. To better represent biomedical terminology, we tokenize the output using BioGPT. Finally, we add a retrieval based verification step that compares generated reports with a reference corpus using Sentence BERT embeddings; if a high similarity match is found, the generated report is replaced with the retrieved ground truth reference to improve reliability.",
      "authors": [
        "Ahmet Halici",
        "Ece Tugba Cebeci",
        "Musa Balci",
        "Mustafa Cini",
        "Serkan Sokmen"
      ],
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-18 12:55:20+00:00",
      "link": "https://arxiv.org/pdf/2602.16422v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16400v1",
      "title": "Easy Data Unlearning Bench",
      "abstract": "Evaluating machine unlearning methods remains technically challenging, with recent benchmarks requiring complex setups and significant engineering overhead. We introduce a unified and extensible benchmarking suite that simplifies the evaluation of unlearning algorithms using the KLoM (KL divergence of Margins) metric. Our framework provides precomputed model ensembles, oracle outputs, and streamlined infrastructure for running evaluations out of the box. By standardizing setup and metrics, it enables reproducible, scalable, and fair comparison across unlearning methods. We aim for this benchmark to serve as a practical foundation for accelerating research and promoting best practices in machine unlearning. Our code and data are publicly available.",
      "authors": [
        "Roy Rinberg",
        "Pol Puigdemont",
        "Martin Pawelczyk",
        "Volkan Cevher"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 12:20:32+00:00",
      "link": "https://arxiv.org/pdf/2602.16400v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16340v1",
      "title": "The Implicit Bias of Adam and Muon on Smooth Homogeneous Neural Networks",
      "abstract": "We study the implicit bias of momentum-based optimizers on homogeneous models. We first extend existing results on the implicit bias of steepest descent in homogeneous models to normalized steepest descent with an optional learning rate schedule. We then show that for smooth homogeneous models, momentum steepest descent algorithms like Muon (spectral norm), MomentumGD ($\\ell_2$ norm), and Signum ($\\ell_\\infty$ norm) are approximate steepest descent trajectories under a decaying learning rate schedule, proving that these algorithms too have a bias towards KKT points of the corresponding margin maximization problem. We extend the analysis to Adam (without the stability constant), which maximizes the $\\ell_\\infty$ margin, and to Muon-Signum and Muon-Adam, which maximize a hybrid norm. Our experiments corroborate the theory and show that the identity of the margin maximized depends on the choice of optimizer. Overall, our results extend earlier lines of work on steepest descent in homogeneous models and momentum-based optimizers in linear models.",
      "authors": [
        "Eitan Gronich",
        "Gal Vardi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-18 10:25:07+00:00",
      "link": "https://arxiv.org/pdf/2602.16340v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16337v1",
      "title": "Subtractive Modulative Network with Learnable Periodic Activations",
      "abstract": "We propose the Subtractive Modulative Network (SMN), a novel, parameter-efficient Implicit Neural Representation (INR) architecture inspired by classical subtractive synthesis. The SMN is designed as a principled signal processing pipeline, featuring a learnable periodic activation layer (Oscillator) that generates a multi-frequency basis, and a series of modulative mask modules (Filters) that actively generate high-order harmonics. We provide both theoretical analysis and empirical validation for our design. Our SMN achieves a PSNR of $40+$ dB on two image datasets, comparing favorably against state-of-the-art methods in terms of both reconstruction accuracy and parameter efficiency. Furthermore, consistent advantage is observed on the challenging 3D NeRF novel view synthesis task. Supplementary materials are available at https://inrainbws.github.io/smn/.",
      "authors": [
        "Tiou Wang",
        "Zhuoqian Yang",
        "Markus Flierl",
        "Mathieu Salzmann",
        "Sabine Süsstrunk"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-18 10:20:50+00:00",
      "link": "https://arxiv.org/pdf/2602.16337v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16336v1",
      "title": "HAWX: A Hardware-Aware FrameWork for Fast and Scalable ApproXimation of DNNs",
      "abstract": "This work presents HAWX, a hardware-aware scalable exploration framework that employs multi-level sensitivity scoring at different DNN abstraction levels (operator, filter, layer, and model) to guide selective integration of heterogeneous AxC blocks. Supported by predictive models for accuracy, power, and area, HAWX accelerates the evaluation of candidate configurations, achieving over 23* speedup in a layer-level search with two candidate approximate blocks and more than (3*106)* speedup at the filter-level search only for LeNet-5, while maintaining accuracy comparable to exhaustive search. Experiments across state-of-the-art DNN benchmarks such as VGG-11, ResNet-18, and EfficientNetLite demonstrate that the efficiency benefits of HAWX scale exponentially with network size. The HAWX hardware-aware search algorithm supports both spatial and temporal accelerator architectures, leveraging either off-the-shelf approximate components or customized designs.",
      "authors": [
        "Samira Nazari",
        "Mohammad Saeed Almasi",
        "Mahdi Taheri",
        "Ali Azarpeyvand",
        "Ali Mokhtari",
        "Ali Mahani",
        "Christian Herglotz"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-18 10:16:57+00:00",
      "link": "https://arxiv.org/pdf/2602.16336v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16335v1",
      "title": "Inductive Satisfiability Certification for Universal Quantifiers and Uninterpreted Function Symbols",
      "abstract": "The combination of uninterpreted function symbols and universal quantification occurs in many applications of automated reasoning, for example, due to their ability to reason about arrays. Yet the satisfiability of such formulas is, in general, undecidable. In practice, SMT solvers are often successful in the unsatisfiable case, using heuristics. However, in the satisfiable case, they rely on explicit model construction, which fails for formulas whose smallest model is not small enough. We introduce an alternative approach that certifies satisfiability using induction arguments, and apply it to the case of linear integer arithmetic. The resulting algorithm is able to prove satisfiability of formulas that are out of reach for current SMT solvers.",
      "authors": [
        "Stefan Ratschan",
        "Anggha Nugraha",
        "Mikoláš Janota",
        "Marek Dančo"
      ],
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO"
      ],
      "published": "2026-02-18 10:16:43+00:00",
      "link": "https://arxiv.org/pdf/2602.16335v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16316v1",
      "title": "A Graph Meta-Network for Learning on Kolmogorov-Arnold Networks",
      "abstract": "Weight-space models learn directly from the parameters of neural networks, enabling tasks such as predicting their accuracy on new datasets. Naive methods -- like applying MLPs to flattened parameters -- perform poorly, making the design of better weight-space architectures a central challenge. While prior work leveraged permutation symmetries in standard networks to guide such designs, no analogous analysis or tailored architecture yet exists for Kolmogorov-Arnold Networks (KANs). In this work, we show that KANs share the same permutation symmetries as MLPs, and propose the KAN-graph, a graph representation of their computation. Building on this, we develop WS-KAN, the first weight-space architecture that learns on KANs, which naturally accounts for their symmetry. We analyze WS-KAN's expressive power, showing it can replicate an input KAN's forward pass - a standard approach for assessing expressiveness in weight-space architectures. We construct a comprehensive ``zoo'' of trained KANs spanning diverse tasks, which we use as benchmarks to empirically evaluate WS-KAN. Across all tasks, WS-KAN consistently outperforms structure-agnostic baselines, often by a substantial margin. Our code is available at https://github.com/BarSGuy/KAN-Graph-Metanetwork.",
      "authors": [
        "Guy Bar-Shalom",
        "Ami Tavory",
        "Itay Evron",
        "Maya Bechler-Speicher",
        "Ido Guy",
        "Haggai Maron"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-18 09:53:53+00:00",
      "link": "https://arxiv.org/pdf/2602.16316v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16311v1",
      "title": "When to Identify Is to Control: On the Controllability of Combinatorial Optimization Problems",
      "abstract": "Consider a finite ground set $E$, a set of feasible solutions $X \\subseteq \\mathbb{R}^{E}$, and a class of objective functions $\\mathcal{C}$ defined on $X$. We are interested in subsets $S$ of $E$ that control $X$ in the sense that we can induce any given solution $x \\in X$ as an optimum for any given objective function $c \\in \\mathcal{C}$ by adding linear terms to $c$ on the coordinates corresponding to $S$. This problem has many applications, e.g., when $X$ corresponds to the set of all traffic flows, the ability to control implies that one is able to induce all target flows by imposing tolls on the edges in $S$.   Our first result shows the equivalence between controllability and identifiability. If $X$ is convex, or if $X$ consists of binary vectors, then $S$ controls $X$ if and only if the restriction of $x$ to $S$ uniquely determines $x$ among all solutions in $X$. In the convex case, we further prove that the family of controlling sets forms a matroid. This structural insight yields an efficient algorithm for computing minimum-weight controlling sets from a description of the affine hull of $X$.   While the equivalence extends to matroid base families, the picture changes sharply for other discrete domains. We show that when $X$ is equal to the set of $s$-$t$-paths in a directed graph, deciding whether an identifying set of a given cardinality exists is $Σ\\mathsf{_2^P}$-complete. The problem remains $\\mathsf{NP}$-hard even on acyclic graphs. For acyclic instances, however, we obtain an approximation guarantee by proving a tight bound on the gap between the smallest identifying sets for $X$ and its convex hull, where the latter corresponds to the $s$-$t$-flow polyhedron.",
      "authors": [
        "Max Klimm",
        "Jannik Matuschke"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS"
      ],
      "published": "2026-02-18 09:46:47+00:00",
      "link": "https://arxiv.org/pdf/2602.16311v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16307v1",
      "title": "Generative AI Usage of University Students: Navigating Between Education and Business",
      "abstract": "This study investigates generative artificial intelligence (GenAI) usage of university students who study alongside their professional career. Previous literature has paid little attention to part-time students and the intersectional use of GenAI between education and business. This study examines with a grounded theory approach the characteristics of GenAI usage of part-time students. Eleven students from a distance learning university were interviewed. Three causal and four intervening conditions, as well as strategies were identified, to influence the use of GenAI. The study highlights both the potential and challenges of GenAI usage in education and business. While GenAI can significantly enhance productivity and learning outcomes, concerns about ethical implications, reliability, and the risk of academic misconduct persist. The developed grounded model offers a comprehensive understanding of GenAI usage among students, providing valuable insights for educators, policymakers, and developers of GenAI tools seeking to bridge the gap between education and business.",
      "authors": [
        "Fabian Walke",
        "Veronika Föller"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "published": "2026-02-18 09:37:56+00:00",
      "link": "https://arxiv.org/pdf/2602.16307v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16306v1",
      "title": "Dynamic and Streaming Algorithms for Union Volume Estimation",
      "abstract": "The union volume estimation problem asks to $(1\\pm\\varepsilon)$-approximate the volume of the union of $n$ given objects $X_1,\\ldots,X_n \\subset \\mathbb{R}^d$. In their seminal work in 1989, Karp, Luby, and Madras solved this problem in time $O(n/\\varepsilon^2)$ in an oracle model where each object $X_i$ can be accessed via three types of queries: obtain the volume of $X_i$, sample a random point from $X_i$, and test whether $X_i$ contains a given point $x$. This running time was recently shown to be optimal [Bringmann, Larsen, Nusser, Rotenberg, and Wang, SoCG'25]. In another line of work, Meel, Vinodchandran, and Chakraborty [PODS'21] designed algorithms that read the objects in one pass using polylogarithmic time per object and polylogarithmic space; this can be phrased as a dynamic algorithm supporting insertions of objects for union volume estimation in the oracle model.   In this paper, we study algorithms for union volume estimation in the oracle model that support both insertions and deletions of objects. We obtain the following results:   - an algorithm supporting insertions and deletions in polylogarithmic update and query time and linear space (this is the first such dynamic algorithm, even for 2D triangles);   - an algorithm supporting insertions and suffix queries (which generalizes the sliding window setting) in polylogarithmic update and query time and space;   - an algorithm supporting insertions and deletions of convex bodies of constant dimension in polylogarithmic update and query time and space.",
      "authors": [
        "Sujoy Bhore",
        "Karl Bringmann",
        "Timothy M. Chan",
        "Yanheng Wang"
      ],
      "primary_category": "cs.CG",
      "categories": [
        "cs.CG",
        "cs.DS"
      ],
      "published": "2026-02-18 09:37:20+00:00",
      "link": "https://arxiv.org/pdf/2602.16306v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16299v1",
      "title": "MICE: Minimal Interaction Cross-Encoders for efficient Re-ranking",
      "abstract": "Cross-encoders deliver state-of-the-art ranking effectiveness in information retrieval, but have a high inference cost. This prevents them from being used as first-stage rankers, but also incurs a cost when re-ranking documents. Prior work has addressed this bottleneck from two largely separate directions: accelerating cross-encoder inference by sparsifying the attention process or improving first-stage retrieval effectiveness using more complex models, e.g. late-interaction ones. In this work, we propose to bridge these two approaches, based on an in-depth understanding of the internal mechanisms of cross-encoders. Starting from cross-encoders, we show that it is possible to derive a new late-interaction-like architecture by carefully removing detrimental or unnecessary interactions. We name this architecture MICE (Minimal Interaction Cross-Encoders). We extensively evaluate MICE across both in-domain (ID) and out-of-domain (OOD) datasets. MICE decreases fourfold the inference latency compared to standard cross-encoders, matching late-interaction models like ColBERT while retaining most of cross-encoder ID effectiveness and demonstrating superior generalization abilities in OOD.",
      "authors": [
        "Mathias Vast",
        "Victor Morand",
        "Basile van Cooten",
        "Laure Soulier",
        "Josiane Mothe",
        "Benjamin Piwowarski"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-18 09:30:29+00:00",
      "link": "https://arxiv.org/pdf/2602.16299v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16298v1",
      "title": "MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models",
      "abstract": "Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark 3 common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task.",
      "authors": [
        "Martin Hyben",
        "Sebastian Kula",
        "Jan Cegin",
        "Jakub Simko",
        "Ivan Srba",
        "Robert Moro"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-18 09:28:53+00:00",
      "link": "https://arxiv.org/pdf/2602.16298v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16290v1",
      "title": "Aladdin-FTI @ AMIYA Three Wishes for Arabic NLP: Fidelity, Diglossia, and Multidialectal Generation",
      "abstract": "Arabic dialects have long been under-represented in Natural Language Processing (NLP) research due to their non-standardization and high variability, which pose challenges for computational modeling. Recent advances in the field, such as Large Language Models (LLMs), offer promising avenues to address this gap by enabling Arabic to be modeled as a pluricentric language rather than a monolithic system. This paper presents Aladdin-FTI, our submission to the AMIYA shared task. The proposed system is designed to both generate and translate dialectal Arabic (DA). Specifically, the model supports text generation in Moroccan, Egyptian, Palestinian, Syrian, and Saudi dialects, as well as bidirectional translation between these dialects, Modern Standard Arabic (MSA), and English. The code and trained model are publicly available.",
      "authors": [
        "Jonathan Mutal",
        "Perla Al Almaoui",
        "Simon Hengchen",
        "Pierrette Bouillon"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-18 09:15:20+00:00",
      "link": "https://arxiv.org/pdf/2602.16290v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16284v1",
      "title": "Fast KV Compaction via Attention Matching",
      "abstract": "Scaling language models to long contexts is often bottlenecked by the size of the key-value (KV) cache. In deployed settings, long contexts are typically managed through compaction in token space via summarization. However, summarization can be highly lossy, substantially harming downstream performance. Recent work on Cartridges has shown that it is possible to train highly compact KV caches in latent space that closely match full-context performance, but at the cost of slow and expensive end-to-end optimization. This work describes an approach for fast context compaction in latent space through Attention Matching, which constructs compact keys and values to reproduce attention outputs and preserve attention mass at a per-KV-head level. We show that this formulation naturally decomposes into simple subproblems, some of which admit efficient closed-form solutions. Within this framework, we develop a family of methods that significantly push the Pareto frontier of compaction time versus quality, achieving up to 50x compaction in seconds on some datasets with little quality loss.",
      "authors": [
        "Adam Zweiger",
        "Xinghong Fu",
        "Han Guo",
        "Yoon Kim"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 09:06:53+00:00",
      "link": "https://arxiv.org/pdf/2602.16284v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16274v1",
      "title": "Regret and Sample Complexity of Online Q-Learning via Concentration of Stochastic Approximation with Time-Inhomogeneous Markov Chains",
      "abstract": "We present the first high-probability regret bound for classical online Q-learning in infinite-horizon discounted Markov decision processes, without relying on optimism or bonus terms. We first analyze Boltzmann Q-learning with decaying temperature and show that its regret depends critically on the suboptimality gap of the MDP: for sufficiently large gaps, the regret is sublinear, while for small gaps it deteriorates and can approach linear growth. To address this limitation, we study a Smoothed $ε_n$-Greedy exploration scheme that combines $ε_n$-greedy and Boltzmann exploration, for which we prove a gap-robust regret bound of near-$\\tilde{O}(N^{9/10})$. To analyze these algorithms, we develop a high-probability concentration bound for contractive Markovian stochastic approximation with iterate- and time-dependent transition dynamics. This bound may be of independent interest as the contraction factor in our bound is governed by the mixing time and is allowed to converge to one asymptotically.",
      "authors": [
        "Rahul Singh",
        "Siddharth Chandak",
        "Eric Moulines",
        "Vivek S. Borkar",
        "Nicholas Bambos"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-18 08:47:07+00:00",
      "link": "https://arxiv.org/pdf/2602.16274v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16264v1",
      "title": "Prediction of Major Solar Flares Using Interpretable Class-dependent Reward Framework with Active Region Magnetograms and Domain Knowledge",
      "abstract": "In this work, we develop, for the first time, a supervised classification framework with class-dependent rewards (CDR) to predict $\\geq$MM flares within 24 hr. We construct multiple datasets, covering knowledge-informed features and line-of sight (LOS) magnetograms. We also apply three deep learning models (CNN, CNN-BiLSTM, and Transformer) and three CDR counterparts (CDR-CNN, CDR-CNN-BiLSTM, and CDR-Transformer). First, we analyze the importance of LOS magnetic field parameters with the Transformer, then compare its performance using LOS-only, vector-only, and combined magnetic field parameters. Second, we compare flare prediction performance based on CDR models versus deep learning counterparts. Third, we perform sensitivity analysis on reward engineering for CDR models. Fourth, we use the SHAP method for model interpretability. Finally, we conduct performance comparison between our models and NASA/CCMC. The main findings are: (1)Among LOS feature combinations, R_VALUE and AREA_ACR consistently yield the best results. (2)Transformer achieves better performance with combined LOS and vector magnetic field data than with either alone. (3)Models using knowledge-informed features outperform those using magnetograms. (4)While CNN and CNN-BiLSTM outperform their CDR counterparts on magnetograms, CDR-Transformer is slightly superior to its deep learning counterpart when using knowledge-informed features. Among all models, CDR-Transformer achieves the best performance. (5)The predictive performance of the CDR models is not overly sensitive to the reward choices.(6)Through SHAP analysis, the CDR model tends to regard TOTUSJH as more important, while the Transformer tends to prioritize R_VALUE more.(7)Under identical prediction time and active region (AR) number, the CDR-Transformer shows superior predictive capabilities compared to NASA/CCMC.",
      "authors": [
        "Zixian Wu",
        "Xuebao Li",
        "Yanfang Zheng",
        "Rui Wang",
        "Shunhuang Zhang",
        "Jinfang Wei",
        "Yongshang Lv",
        "Liang Dong",
        "Zamri Zainal Abidin",
        "Noraisyah Mohamed Shah",
        "Hongwei Ye",
        "Pengchao Yan",
        "Xuefeng Li",
        "Xiaojia Ji",
        "Xusheng Huang",
        "Xiaotian Wang",
        "Honglei Jin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "astro-ph.SR"
      ],
      "published": "2026-02-18 08:30:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16264v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16240v1",
      "title": "Submodular Maximization under Supermodular Constraint: Greedy Guarantees",
      "abstract": "Motivated by a wide range of applications in data mining and machine learning, we consider the problem of maximizing a submodular function subject to supermodular cost constraints. In contrast to the well-understood setting of cardinality and matroid constraints, where greedy algorithms admit strong guarantees, the supermodular constraint regime remains poorly understood -- guarantees for greedy methods and other efficient algorithmic paradigms are largely open. We study this family of fundamental optimization problems under an upper-bound constraint on a supermodular cost function with curvature parameter $γ$. Our notion of supermodular curvature is less restrictive than prior definitions, substantially expanding the class of admissible cost functions. We show that our greedy algorithm that iteratively includes elements maximizing the ratio of the objective and constraint functions, achieves a $\\left(1 - e^{-(1-γ)}\\right)$-approximation before stopping. We prove that this approximation is indeed tight for this algorithm. Further, if the objective function has a submodular curvature $c$, then we show that the bound further improves to $\\left(1 - (1- (1-c)(1-γ))^{1/(1-c)}\\right)$, which can be further improved by continuing to violate the constraint. Finally, we show that the Greedy-Ratio-Marginal in conjunction with binary search leads to a bicriteria approximation for the dual problem -- minimizing a supermodular function under a lower bound constraint on a submodular function. We conduct a number of experiments on a simulation of LLM agents debating over multiple rounds -- the task is to select a subset of agents to maximize correctly answered questions. Our algorithm outperforms all other greedy heuristics, and on smaller problems, it achieves the same performance as the optimal set found by exhaustive search.",
      "authors": [
        "Ajitesh Srivastava",
        "Shanghua Teng"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS",
        "cs.CC"
      ],
      "published": "2026-02-18 07:33:51+00:00",
      "link": "https://arxiv.org/pdf/2602.16240v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16236v1",
      "title": "Online Prediction of Stochastic Sequences with High Probability Regret Bounds",
      "abstract": "We revisit the classical problem of universal prediction of stochastic sequences with a finite time horizon $T$ known to the learner. The question we investigate is whether it is possible to derive vanishing regret bounds that hold with high probability, complementing existing bounds from the literature that hold in expectation. We propose such high-probability bounds which have a very similar form as the prior expectation bounds. For the case of universal prediction of a stochastic process over a countable alphabet, our bound states a convergence rate of $\\mathcal{O}(T^{-1/2} δ^{-1/2})$ with probability as least $1-δ$ compared to prior known in-expectation bounds of the order $\\mathcal{O}(T^{-1/2})$. We also propose an impossibility result which proves that it is not possible to improve the exponent of $δ$ in a bound of the same form without making additional assumptions.",
      "authors": [
        "Matthias Frey",
        "Jonathan H. Manton",
        "Jingge Zhu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "published": "2026-02-18 07:26:37+00:00",
      "link": "https://arxiv.org/pdf/2602.16236v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16233v1",
      "title": "DistributedEstimator: Distributed Training of Quantum Neural Networks via Circuit Cutting",
      "abstract": "Circuit cutting decomposes a large quantum circuit into a collection of smaller subcircuits. The outputs of these subcircuits are then classically reconstructed to recover the original expectation values. While prior work characterises cutting overhead largely in terms of subcircuit counts and sampling complexity, its end-to-end impact on iterative, estimator-driven training pipelines remains insufficiently measured from a systems perspective. In this paper, we propose a cut-aware estimator execution pipeline that treats circuit cutting as a staged distributed workload and instruments each estimator query into partitioning, subexperiment generation, parallel execution, and classical reconstruction phases. Using logged runtime traces and learning outcomes on two binary classification workloads (Iris and MNIST), we quantify cutting overheads, scaling limits, and sensitivity to injected stragglers, and we evaluate whether accuracy and robustness are preserved under matched training budgets. Our measurements show that cutting introduces substantial end-to-end overheads that grow with the number of cuts, and that reconstruction constitutes a dominant fraction of per-query time, bounding achievable speed-up under increased parallelism. Despite these systems costs, test accuracy and robustness are preserved in the measured regimes, with configuration-dependent improvements observed in some cut settings. These results indicate that practical scaling of circuit cutting for learning workloads hinges on reducing and overlapping reconstruction and on scheduling policies that account for barrier-dominated critical paths.",
      "authors": [
        "Prabhjot Singh",
        "Adel N. Toosi",
        "Rajkumar Buyya"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.LG",
        "quant-ph"
      ],
      "published": "2026-02-18 07:17:29+00:00",
      "link": "https://arxiv.org/pdf/2602.16233v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16231v1",
      "title": "DataCube: A Video Retrieval Platform via Natural Language Semantic Profiling",
      "abstract": "Large-scale video repositories are increasingly available for modern video understanding and generation tasks. However, transforming raw videos into high-quality, task-specific datasets remains costly and inefficient. We present DataCube, an intelligent platform for automatic video processing, multi-dimensional profiling, and query-driven retrieval. DataCube constructs structured semantic representations of video clips and supports hybrid retrieval with neural re-ranking and deep semantic matching. Through an interactive web interface, users can efficiently construct customized video subsets from massive repositories for training, analysis, and evaluation, and build searchable systems over their own private video collections. The system is publicly accessible at https://datacube.baai.ac.cn/. Demo Video: https://baai-data-cube.ks3-cn-beijing.ksyuncs.com/custom/Adobe%20Express%20-%202%E6%9C%8818%E6%97%A5%20%281%29%281%29%20%281%29.mp4",
      "authors": [
        "Yiming Ju",
        "Hanyu Zhao",
        "Quanyue Ma",
        "Donglin Hao",
        "Chengwei Wu",
        "Ming Li",
        "Songjing Wang",
        "Tengfei Pan"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-18 07:12:01+00:00",
      "link": "https://arxiv.org/pdf/2602.16231v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16220v1",
      "title": "SEMixer: Semantics Enhanced MLP-Mixer for Multiscale Mixing and Long-term Time Series Forecasting",
      "abstract": "Modeling multiscale patterns is crucial for long-term time series forecasting (TSF). However, redundancy and noise in time series, together with semantic gaps between non-adjacent scales, make the efficient alignment and integration of multi-scale temporal dependencies challenging. To address this, we propose SEMixer, a lightweight multiscale model designed for long-term TSF. SEMixer features two key components: a Random Attention Mechanism (RAM) and a Multiscale Progressive Mixing Chain (MPMC). RAM captures diverse time-patch interactions during training and aggregates them via dropout ensemble at inference, enhancing patch-level semantics and enabling MLP-Mixer to better model multi-scale dependencies. MPMC further stacks RAM and MLP-Mixer in a memory-efficient manner, achieving more effective temporal mixing. It addresses semantic gaps across scales and facilitates better multiscale modeling and forecasting performance. We not only validate the effectiveness of SEMixer on 10 public datasets, but also on the \\textit{2025 CCF AlOps Challenge} based on 21GB real wireless network data, where SEMixer achieves third place. The code is available at the link https://github.com/Meteor-Stars/SEMixer.",
      "authors": [
        "Xu Zhang",
        "Qitong Wang",
        "Peng Wang",
        "Wei Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 06:53:32+00:00",
      "link": "https://arxiv.org/pdf/2602.16220v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16218v1",
      "title": "Bayesian Quadrature: Gaussian Processes for Integration",
      "abstract": "Bayesian quadrature is a probabilistic, model-based approach to numerical integration, the estimation of intractable integrals, or expectations. Although Bayesian quadrature was popularised already in the 1980s, no systematic and comprehensive treatment has been published. The purpose of this survey is to fill this gap. We review the mathematical foundations of Bayesian quadrature from different points of view; present a systematic taxonomy for classifying different Bayesian quadrature methods along the three axes of modelling, inference, and sampling; collect general theoretical guarantees; and provide a controlled numerical study that explores and illustrates the effect of different choices along the axes of the taxonomy. We also provide a realistic assessment of practical challenges and limitations to application of Bayesian quadrature methods and include an up-to-date and nearly exhaustive bibliography that covers not only machine learning and statistics literature but all areas of mathematics and engineering in which Bayesian quadrature or equivalent methods have seen use.",
      "authors": [
        "Maren Mahsereci",
        "Toni Karvonen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.NA",
        "stat.ML"
      ],
      "published": "2026-02-18 06:43:18+00:00",
      "link": "https://arxiv.org/pdf/2602.16218v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16209v1",
      "title": "Geometric Neural Operators via Lie Group-Constrained Latent Dynamics",
      "abstract": "Neural operators offer an effective framework for learning solutions of partial differential equations for many physical systems in a resolution-invariant and data-driven manner. Existing neural operators, however, often suffer from instability in multi-layer iteration and long-horizon rollout, which stems from the unconstrained Euclidean latent space updates that violate the geometric and conservation laws. To address this challenge, we propose to constrain manifolds with low-rank Lie algebra parameterization that performs group action updates on the latent representation. Our method, termed Manifold Constraining based on Lie group (MCL), acts as an efficient \\emph{plug-and-play} module that enforces geometric inductive bias to existing neural operators. Extensive experiments on various partial differential equations, such as 1-D Burgers and 2-D Navier-Stokes, over a wide range of parameters and steps demonstrate that our method effectively lowers the relative prediction error by 30-50\\% at the cost of 2.26\\% of parameter increase. The results show that our approach provides a scalable solution for improving long-term prediction fidelity by addressing the principled geometric constraints absent in the neural operator updates.",
      "authors": [
        "Jiaquan Zhang",
        "Fachrina Dewi Puspitasari",
        "Songbo Zhang",
        "Yibei Liu",
        "Kuien Liu",
        "Caiyan Qin",
        "Fan Mo",
        "Peng Wang",
        "Yang Yang",
        "Chaoning Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-18 06:17:47+00:00",
      "link": "https://arxiv.org/pdf/2602.16209v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16204v1",
      "title": "Linked Data Classification using Neurochaos Learning",
      "abstract": "Neurochaos Learning (NL) has shown promise in recent times over traditional deep learning due to its two key features: ability to learn from small sized training samples, and low compute requirements. In prior work, NL has been implemented and extensively tested on separable and time series data, and demonstrated its superior performance on both classification and regression tasks. In this paper, we investigate the next step in NL, viz., applying NL to linked data, in particular, data that is represented in the form of knowledge graphs. We integrate linked data into NL by implementing node aggregation on knowledge graphs, and then feeding the aggregated node features to the simplest NL architecture: ChaosNet. We demonstrate the results of our implementation on homophilic graph datasets as well as heterophilic graph datasets of verying heterophily. We show better efficacy of our approach on homophilic graphs than on heterophilic graphs. While doing so, we also present our analysis of the results, as well as suggestions for future work.",
      "authors": [
        "Pooja Honna",
        "Ayush Patravali",
        "Nithin Nagaraj",
        "Nanjangud C. Narendra"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 05:55:59+00:00",
      "link": "https://arxiv.org/pdf/2602.16204v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16201v1",
      "title": "Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications",
      "abstract": "Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives.   We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.",
      "authors": [
        "Sanket Badhe",
        "Deep Shah",
        "Nehal Kathrotia"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2026-02-18 05:49:45+00:00",
      "link": "https://arxiv.org/pdf/2602.16201v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16200v1",
      "title": "The Validity of Coreference-based Evaluations of Natural Language Understanding",
      "abstract": "In this thesis, I refine our understanding as to what conclusions we can reach from coreference-based evaluations by expanding existing evaluation practices and considering the extent to which evaluation results are either converging or conflicting. First, I analyze standard coreference evaluations and show that their design often leads to non-generalizable conclusions due to issues of measurement validity - including contestedness (multiple, competing definitions of coreference) and convergent validity (evaluation results that rank models differently across benchmarks). Second, I propose and implement a novel evaluation focused on testing systems' ability to infer the relative plausibility of events, a key aspect of resolving coreference. Through this extended evaluation, I find that contemporary language models demonstrate strong performance on standard benchmarks - improving over earlier baseline systems within certain domains and types of coreference - but remain sensitive to the evaluation conditions: they often fail to generalize in ways one would expect a human to be capable of when evaluation contexts are slightly modified. Taken together, these findings clarify both the strengths, such as improved accuracy over baselines on widely used evaluations, and the limitations of the current NLP paradigm, including weaknesses in measurement validity, and suggest directions for future work in developing better evaluation methods and more genuinely generalizable systems.",
      "authors": [
        "Ian Porada"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-18 05:49:28+00:00",
      "link": "https://arxiv.org/pdf/2602.16200v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16193v1",
      "title": "Rethinking Input Domains in Physics-Informed Neural Networks via Geometric Compactification Mappings",
      "abstract": "Several complex physical systems are governed by multi-scale partial differential equations (PDEs) that exhibit both smooth low-frequency components and localized high-frequency structures. Existing physics-informed neural network (PINN) methods typically train with fixed coordinate system inputs, where geometric misalignment with these structures induces gradient stiffness and ill-conditioning that hinder convergence. To address this issue, we introduce a mapping paradigm that reshapes the input coordinates through differentiable geometric compactification mappings and couples the geometric structure of PDEs with the spectral properties of residual operators. Based on this paradigm, we propose Geometric Compactification (GC)-PINN, a framework that introduces three mapping strategies for periodic boundaries, far-field scale expansion, and localized singular structures in the input domain without modifying the underlying PINN architecture. Extensive empirical evaluation demonstrates that this approach yields more uniform residual distributions and higher solution accuracy on representative 1D and 2D PDEs, while improving training stability and convergence speed.",
      "authors": [
        "Zhenzhen Huang",
        "Haoyu Bian",
        "Jiaquan Zhang",
        "Yibei Liu",
        "Kuien Liu",
        "Caiyan Qin",
        "Guoqing Wang",
        "Yang Yang",
        "Chaoning Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-18 05:27:53+00:00",
      "link": "https://arxiv.org/pdf/2602.16193v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16189v1",
      "title": "Beyond Learning: A Training-Free Alternative to Model Adaptation",
      "abstract": "Despite the continuous research and evolution of language models, they sometimes underperform previous versions. Existing approaches to overcome these challenges are resource-intensive, highlighting the need for alternatives that enable immediate action. We assume that each language model has a local module inside that is suitable for a specific function. First, this work identifies a set of modules showing consistent and local activation changes under an inference workload through activation-based analysis. Subsequently, we transplant an internal module that is properly activated for a specific task into the target model, leading to immediate and measurable functional changes without additional training or fine-tuning. To experimentally demonstrate the effectiveness of the transplant technique, we quantify the relationship between transplant strength and performance improvement under different conditions for two language models. In the cross-generation setting, we find that transplanting activation-selected modules can substantially improve the underperforming model, reaching up to twice the target baseline and achieving gap-based recovery above 100%. Moreover, in transplant experiments between a base model and its instruction-tuned counterpart, transplantation improves the underperforming model toward the stronger baseline, yielding up to about 2.33 times the target baseline with gap-based recovery reaching up to 100% in the best case. These results show that meaningful capacity transfer can be realized through the implantation of highly localized modules implied by language models. Overall, this work provides empirical evidence for task-localized modularity in language models and presents a new research area: model transplantation.",
      "authors": [
        "Namkyung Yoon",
        "Kyeonghyun Yoo",
        "Wooyong Jung",
        "Sanghong Kim",
        "Hwangnam Kim"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-18 05:17:44+00:00",
      "link": "https://arxiv.org/pdf/2602.16189v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16188v1",
      "title": "Deep TPC: Temporal-Prior Conditioning for Time Series Forecasting",
      "abstract": "LLM-for-time series (TS) methods typically treat time shallowly, injecting positional or prompt-based cues once at the input of a largely frozen decoder, which limits temporal reasoning as this information degrades through the layers. We introduce Temporal-Prior Conditioning (TPC), which elevates time to a first-class modality that conditions the model at multiple depths. TPC attaches a small set of learnable time series tokens to the patch stream; at selected layers these tokens cross-attend to temporal embeddings derived from compact, human-readable temporal descriptors encoded by the same frozen LLM, then feed temporal context back via self-attention. This disentangles time series signal and temporal information while maintaining a low parameter budget. We show that by training only the cross-attention modules and explicitly disentangling time series signal and temporal information, TPC consistently outperforms both full fine-tuning and shallow conditioning strategies, achieving state-of-the-art performance in long-term forecasting across diverse datasets. Code available at: https://github.com/fil-mp/Deep_tpc",
      "authors": [
        "Filippos Bellos",
        "NaveenJohn Premkumar",
        "Yannis Avrithis",
        "Nam H. Nguyen",
        "Jason J. Corso"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 05:16:29+00:00",
      "link": "https://arxiv.org/pdf/2602.16188v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16177v1",
      "title": "Conjugate Learning Theory: Uncovering the Mechanisms of Trainability and Generalization in Deep Neural Networks",
      "abstract": "In this work, we propose a notion of practical learnability grounded in finite sample settings, and develop a conjugate learning theoretical framework based on convex conjugate duality to characterize this learnability property. Building on this foundation, we demonstrate that training deep neural networks (DNNs) with mini-batch stochastic gradient descent (SGD) achieves global optima of empirical risk by jointly controlling the extreme eigenvalues of a structure matrix and the gradient energy, and we establish a corresponding convergence theorem. We further elucidate the impact of batch size and model architecture (including depth, parameter count, sparsity, skip connections, and other characteristics) on non-convex optimization. Additionally, we derive a model-agnostic lower bound for the achievable empirical risk, theoretically demonstrating that data determines the fundamental limit of trainability. On the generalization front, we derive deterministic and probabilistic bounds on generalization error based on generalized conditional entropy measures. The former explicitly delineates the range of generalization error, while the latter characterizes the distribution of generalization error relative to the deterministic bounds under independent and identically distributed (i.i.d.) sampling conditions. Furthermore, these bounds explicitly quantify the influence of three key factors: (i) information loss induced by irreversibility in the model, (ii) the maximum attainable loss value, and (iii) the generalized conditional entropy of features with respect to labels. Moreover, they offer a unified theoretical lens for understanding the roles of regularization, irreversible transformations, and network depth in shaping the generalization behavior of deep neural networks. Extensive experiments validate all theoretical predictions, confirming the framework's correctness and consistency.",
      "authors": [
        "Binchuan Qi"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-18 04:26:55+00:00",
      "link": "https://arxiv.org/pdf/2602.16177v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16167v1",
      "title": "Muon with Spectral Guidance: Efficient Optimization for Scientific Machine Learning",
      "abstract": "Physics-informed neural networks and neural operators often suffer from severe optimization difficulties caused by ill-conditioned gradients, multi-scale spectral behavior, and stiffness induced by physical constraints. Recently, the Muon optimizer has shown promise by performing orthogonalized updates in the singular-vector basis of the gradient, thereby improving geometric conditioning. However, its unit-singular-value updates may lead to overly aggressive steps and lack explicit stability guarantees when applied to physics-informed learning. In this work, we propose SpecMuon, a spectral-aware optimizer that integrates Muon's orthogonalized geometry with a mode-wise relaxed scalar auxiliary variable (RSAV) mechanism. By decomposing matrix-valued gradients into singular modes and applying RSAV updates individually along dominant spectral directions, SpecMuon adaptively regulates step sizes according to the global loss energy while preserving Muon's scale-balancing properties. This formulation interprets optimization as a multi-mode gradient flow and enables principled control of stiff spectral components. We establish rigorous theoretical properties of SpecMuon, including a modified energy dissipation law, positivity and boundedness of auxiliary variables, and global convergence with a linear rate under the Polyak-Lojasiewicz condition. Numerical experiments on physics-informed neural networks, DeepONets, and fractional PINN-DeepONets demonstrate that SpecMuon achieves faster convergence and improved stability compared with Adam, AdamW, and the original Muon optimizer on benchmark problems such as the one-dimensional Burgers equation and fractional partial differential equations.",
      "authors": [
        "Binghang Lu",
        "Jiahao Zhang",
        "Guang Lin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 03:56:20+00:00",
      "link": "https://arxiv.org/pdf/2602.16167v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16155v1",
      "title": "Differentially Private Non-convex Distributionally Robust Optimization",
      "abstract": "Real-world deployments routinely face distribution shifts, group imbalances, and adversarial perturbations, under which the traditional Empirical Risk Minimization (ERM) framework can degrade severely.   Distributionally Robust Optimization (DRO) addresses this issue by optimizing the worst-case expected loss over an uncertainty set of distributions, offering a principled approach to robustness.   Meanwhile, as training data in DRO always involves sensitive information, safeguarding it against leakage under Differential Privacy (DP) is essential.   In contrast to classical DP-ERM, DP-DRO has received much less attention due to its minimax optimization structure with uncertainty constraint.   To bridge the gap, we provide a comprehensive study of DP-(finite-sum)-DRO with $ψ$-divergence and non-convex loss.   First, we study DRO with general $ψ$-divergence by reformulating it as a minimization problem, and develop a novel $(\\varepsilon, δ)$-DP optimization method, called DP Double-Spider, tailored to this structure.   Under mild assumptions, we show that it achieves a utility bound of $\\mathcal{O}(\\frac{1}{\\sqrt{n}}+ (\\frac{\\sqrt{d \\log (1/δ)}}{n \\varepsilon})^{2/3})$ in terms of the gradient norm, where $n$ denotes the data size and $d$ denotes the model dimension.   We further improve the utility rate for specific divergences.   In particular, for DP-DRO with KL-divergence, by transforming the problem into a compositional finite-sum optimization problem, we develop a DP Recursive-Spider method and show that it achieves a utility bound of $\\mathcal{O}((\\frac{\\sqrt{d \\log(1/δ)}}{n\\varepsilon})^{2/3} )$, matching the best-known result for non-convex DP-ERM.   Experimentally, we demonstrate that our proposed methods outperform existing approaches for DP minimax optimization.",
      "authors": [
        "Difei Xu",
        "Meng Ding",
        "Zebin Ma",
        "Huanyi Xie",
        "Youming Tao",
        "Aicha Slaitane",
        "Di Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 03:00:30+00:00",
      "link": "https://arxiv.org/pdf/2602.16155v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16151v1",
      "title": "Queer NLP: A Critical Survey on Literature Gaps, Biases and Trends",
      "abstract": "Natural language processing (NLP) technologies are rapidly reshaping how language is created, processed, and analyzed by humans. With current and potential applications in hiring, law, healthcare, and other areas that impact people's lives, understanding and mitigating harms towards marginalized groups is critical. In this survey, we examine NLP research papers that explicitly address the relationship between LGBTQIA+ communities and NLP technologies. We systematically review all such papers published in the ACL Anthology, to answer the following research questions: (1) What are current research trends? (2) What gaps exist in terms of topics and methods? (3) What areas are open for future work? We find that while the number of papers on queer NLP has grown within the last few years, most papers take a reactive rather than a proactive approach, pointing out bias more often than mitigating it, and focusing on shortcomings of existing systems rather than creating new solutions. Our survey uncovers many opportunities for future work, especially regarding stakeholder involvement, intersectionality, interdisciplinarity, and languages other than English. We also offer an outlook from a queer studies perspective, highlighting understudied topics and gaps in the harms addressed in NLP papers. Beyond being a roadmap of what has been done, this survey is a call to action for work towards more just and inclusive NLP technologies.",
      "authors": [
        "Sabine Weber",
        "Angelina Wang",
        "Ankush Gupta",
        "Arjun Subramonian",
        "Dennis Ulmer",
        "Eshaan Tanwar",
        "Geetanjali Aich",
        "Hannah Devinney",
        "Jacob Hobbs",
        "Jennifer Mickel",
        "Joshua Tint",
        "Mae Sosto",
        "Ray Groshan",
        "Simone Astarita",
        "Vagrant Gautam",
        "Verena Blaschke",
        "William Agnew",
        "Wilson Y Lee",
        "Yanan Long"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY"
      ],
      "published": "2026-02-18 02:54:28+00:00",
      "link": "https://arxiv.org/pdf/2602.16151v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16144v1",
      "title": "Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis",
      "abstract": "As multimodal systems increasingly process sensitive personal data, the ability to selectively revoke specific data modalities has become a critical requirement for privacy compliance and user autonomy. We present Missing-by-Design (MBD), a unified framework for revocable multimodal sentiment analysis that combines structured representation learning with a certifiable parameter-modification pipeline. Revocability is critical in privacy-sensitive applications where users or regulators may request removal of modality-specific information. MBD learns property-aware embeddings and employs generator-based reconstruction to recover missing channels while preserving task-relevant signals. For deletion requests, the framework applies saliency-driven candidate selection and a calibrated Gaussian update to produce a machine-verifiable Modality Deletion Certificate. Experiments on benchmark datasets show that MBD achieves strong predictive performance under incomplete inputs and delivers a practical privacy-utility trade-off, positioning surgical unlearning as an efficient alternative to full retraining.",
      "authors": [
        "Rong Fu",
        "Wenxin Zhang",
        "Ziming Wang",
        "Chunlei Meng",
        "Jiaxuan Lu",
        "Jiekai Wu",
        "Kangan Qian",
        "Hao Zhang",
        "Simon Fong"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-18 02:29:33+00:00",
      "link": "https://arxiv.org/pdf/2602.16144v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16138v1",
      "title": "IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models",
      "abstract": "We introduce IRIS (Intent Resolution via Inference-time Saccades), a novel training-free approach that uses eye-tracking data in real-time to resolve ambiguity in open-ended VQA. Through a comprehensive user study with 500 unique image-question pairs, we demonstrate that fixations closest to the time participants start verbally asking their questions are the most informative for disambiguation in Large VLMs, more than doubling the accuracy of responses on ambiguous questions (from 35.2% to 77.2%) while maintaining performance on unambiguous queries. We evaluate our approach across state-of-the-art VLMs, showing consistent improvements when gaze data is incorporated in ambiguous image-question pairs, regardless of architectural differences. We release a new benchmark dataset to use eye movement data for disambiguated VQA, a novel real-time interactive protocol, and an evaluation suite.",
      "authors": [
        "Parsa Madinei",
        "Srijita Karmakar",
        "Russell Cohen Hoffing",
        "Felix Gervitz",
        "Miguel P. Eckstein"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-18 02:06:24+00:00",
      "link": "https://arxiv.org/pdf/2602.16138v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16136v1",
      "title": "Retrieval Collapses When AI Pollutes the Web",
      "abstract": "The rapid proliferation of AI-generated content on the Web presents a structural risk to information retrieval, as search engines and Retrieval-Augmented Generation (RAG) systems increasingly consume evidence produced by the Large Language Models (LLMs). We characterize this ecosystem-level failure mode as Retrieval Collapse, a two-stage process where (1) AI-generated content dominates search results, eroding source diversity, and (2) low-quality or adversarial content infiltrates the retrieval pipeline. We analyzed this dynamic through controlled experiments involving both high-quality SEO-style content and adversarially crafted content. In the SEO scenario, a 67\\% pool contamination led to over 80\\% exposure contamination, creating a homogenized yet deceptively healthy state where answer accuracy remains stable despite the reliance on synthetic sources. Conversely, under adversarial contamination, baselines like BM25 exposed $\\sim$19\\% of harmful content, whereas LLM-based rankers demonstrated stronger suppression capabilities. These findings highlight the risk of retrieval pipelines quietly shifting toward synthetic evidence and the need for retrieval-aware strategies to prevent a self-reinforcing cycle of quality decline in Web-grounded systems.",
      "authors": [
        "Hongyeon Yu",
        "Dongchan Kim",
        "Young-Bum Kim"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2026-02-18 02:01:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16136v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16131v1",
      "title": "Empirical Cumulative Distribution Function Clustering for LLM-based Agent System Analysis",
      "abstract": "Large language models (LLMs) are increasingly used as agents to solve complex tasks such as question answering (QA), scientific debate, and software development. A standard evaluation procedure aggregates multiple responses from LLM agents into a single final answer, often via majority voting, and compares it against reference answers. However, this process can obscure the quality and distributional characteristics of the original responses. In this paper, we propose a novel evaluation framework based on the empirical cumulative distribution function (ECDF) of cosine similarities between generated responses and reference answers. This enables a more nuanced assessment of response quality beyond exact match metrics. To analyze the response distributions across different agent configurations, we further introduce a clustering method for ECDFs using their distances and the $k$-medoids algorithm. Our experiments on a QA dataset demonstrate that ECDFs can distinguish between agent settings with similar final accuracies but different quality distributions. The clustering analysis also reveals interpretable group structures in the responses, offering insights into the impact of temperature, persona, and question topics.",
      "authors": [
        "Chihiro Watanabe",
        "Jingyu Sun"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-18 01:49:35+00:00",
      "link": "https://arxiv.org/pdf/2602.16131v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16125v1",
      "title": "On the Power of Source Screening for Learning Shared Feature Extractors",
      "abstract": "Learning with shared representation is widely recognized as an effective way to separate commonalities from heterogeneity across various heterogeneous sources. Most existing work includes all related data sources via simultaneously training a common feature extractor and source-specific heads. It is well understood that data sources with low relevance or poor quality may hinder representation learning. In this paper, we further dive into the question of which data sources should be learned jointly by focusing on the traditionally deemed ``good'' collection of sources, in which individual sources have similar relevance and qualities with respect to the true underlying common structure. Towards tractability, we focus on the linear setting where sources share a low-dimensional subspace. We find that source screening can play a central role in statistically optimal subspace estimation. We show that, for a broad class of problem instances, training on a carefully selected subset of sources suffices to achieve minimax optimality, even when a substantial portion of data is discarded. We formalize the notion of an informative subpopulation, develop algorithms and practical heuristics for identifying such subsets, and validate their effectiveness through both theoretical analysis and empirical evaluations on synthetic and real-world datasets.",
      "authors": [
        "Leo",
        "Wang",
        "Connor Mclaughlin",
        "Lili Su"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-18 01:32:10+00:00",
      "link": "https://arxiv.org/pdf/2602.16125v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16124v1",
      "title": "Rethinking ANN-based Retrieval: Multifaceted Learnable Index for Large-scale Recommendation System",
      "abstract": "Approximate nearest neighbor (ANN) search is widely used in the retrieval stage of large-scale recommendation systems. In this stage, candidate items are indexed using their learned embedding vectors, and ANN search is executed for each user (or item) query to retrieve a set of relevant items. However, ANN-based retrieval has two key limitations. First, item embeddings and their indices are typically learned in separate stages: indexing is often performed offline after embeddings are trained, which can yield suboptimal retrieval quality-especially for newly created items. Second, although ANN offers sublinear query time, it must still be run for every request, incurring substantial computation cost at industry scale. In this paper, we propose MultiFaceted Learnable Index (MFLI), a scalable, real-time retrieval paradigm that learns multifaceted item embeddings and indices within a unified framework and eliminates ANN search at serving time. Specifically, we construct a multifaceted hierarchical codebook via residual quantization of item embeddings and co-train the codebook with the embeddings. We further introduce an efficient multifaceted indexing structure and mechanisms that support real-time updates. At serving time, the learned hierarchical indices are used directly to identify relevant items, avoiding ANN search altogether. Extensive experiments on real-world data with billions of users show that MFLI improves recall on engagement tasks by up to 11.8\\%, cold-content delivery by up to 57.29\\%, and semantic relevance by 13.5\\% compared with prior state-of-the-art methods. We also deploy MFLI in the system and report online experimental results demonstrating improved engagement, less popularity bias, and higher serving efficiency.",
      "authors": [
        "Jiang Zhang",
        "Yubo Wang",
        "Wei Chang",
        "Lu Han",
        "Xingying Cheng",
        "Feng Zhang",
        "Min Li",
        "Songhao Jiang",
        "Wei Zheng",
        "Harry Tran",
        "Zhen Wang",
        "Lei Chen",
        "Yueming Wang",
        "Benyu Zhang",
        "Xiangjun Fan",
        "Bi Xue",
        "Qifan Wang"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-18 01:31:29+00:00",
      "link": "https://arxiv.org/pdf/2602.16124v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16123v1",
      "title": "\"You Can Actually Do Something\": Shifts in High School Computer Science Teachers' Conceptions of AI/ML Systems and Algorithmic Justice",
      "abstract": "The recent proliferation of artificial intelligence and machine learning (AI/ML) systems highlights the need for all people to develop effective competencies to interact with and examine AI/ML systems. We study shifts in five experienced high school CS teachers' understanding of AI/ML systems after one year of participatory design, where they co-developed lessons on AI auditing, a systematic method to query AI/ML systems. Drawing on individual and group interviews, we found that teachers' perspectives became more situated, grounding their understanding in everyday contexts; more critical, reflecting growing awareness of harms; and more agentic, highlighting possibilities for action. Further, across all three perspectives, teachers consistently framed algorithmic justice through their role as educators, situating their concerns within their school communities. In the discussion, we consider the ways teachers' perspectives shifted, how AI auditing can shape these shifts, and the implications of these findings on AI literacy for both teachers and students.",
      "authors": [
        "Daniel J. Noh",
        "Deborah A. Fields",
        "Yasmin B. Kafai",
        "Danaé Metaxa"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-18 01:27:35+00:00",
      "link": "https://arxiv.org/pdf/2602.16123v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16113v1",
      "title": "Evolutionary Context Search for Automated Skill Acquisition",
      "abstract": "Large Language Models cannot reliably acquire new knowledge post-deployment -- even when relevant text resources exist, models fail to transform them into actionable knowledge without retraining. Retrieval-Augmented Generation attempts to bridge this gap by surfacing relevant documents at inference time, yet similarity-based retrieval often fails to identify context that actually improves task performance. We introduce Evolutionary Context Search (ECS), an evolutionary method that searches context combinations using accuracy on a small development set, requiring only inference calls without weight updates. ECS moves beyond semantic similarity to discover non-obvious context pairings that significantly boost performance. Our empirical results show that ECS improves BackendBench by 27\\% and $τ$-bench airline by 7\\%. The evolved contexts are model-agnostic, as those evolved with Gemini-3-Flash transfer effectively to Claude Sonnet and DeepSeek. This suggests that ECS opens a path toward automated context discovery for skill acquisition -- an efficient alternative to manual prompt engineering or costly fine-tuning.",
      "authors": [
        "Qi Sun",
        "Stefan Nielsen",
        "Rio Yokota",
        "Yujin Tang"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "published": "2026-02-18 00:47:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16113v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16093v1",
      "title": "Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities",
      "abstract": "Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously learn new knowledge from an adaptation document corpora and mitigate the forgetting of earlier learned capabilities. To address this, we introduce Distillation via Split Contexts (DiSC), a simple context-distillation based approach for continual knowledge adaptation. \\methodname~derives student and teacher distributions by conditioning on distinct segments of the training example and minimizes the KL divergence between the shared tokens. This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains. Compared to prior finetuning and distillation methods for continual adaptation, DiSC consistently reports the best trade-off between learning new knowledge and mitigating forgetting of previously learned skills like instruction-following, reasoning, and factual knowledge.",
      "authors": [
        "Shankar Padmanabhan",
        "Mustafa Omer Gul",
        "Tanya Goyal"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-17 23:49:47+00:00",
      "link": "https://arxiv.org/pdf/2602.16093v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16092v1",
      "title": "Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff",
      "abstract": "Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation.",
      "authors": [
        "Patrick Pynadath",
        "Ruqi Zhang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-17 23:39:39+00:00",
      "link": "https://arxiv.org/pdf/2602.16092v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16091v1",
      "title": "Can Causality Cure Confusion Caused By Correlation (in Software Analytics)?",
      "abstract": "Background: Symbolic models, particularly decision trees, are widely used in software engineering for explainable analytics in defect prediction, configuration tuning, and software quality assessment. Most of these models rely on correlational split criteria, such as variance reduction or information gain, which identify statistical associations but cannot imply causation between X and Y. Recent empirical studies in software engineering show that both correlational models and causal discovery algorithms suffer from pronounced instability. This instability arises from two complementary issues: 1-Correlation-based methods conflate association with causation. 2-Causal discovery algorithms rely on heuristic approximations to cope with the NP-hard nature of structure learning, causing their inferred graphs to vary widely under minor input perturbations. Together, these issues undermine trust, reproducibility, and the reliability of explanations in real-world SE tasks. Objective: This study investigates whether incorporating causality-aware split criteria into symbolic models can improve their stability and robustness, and whether such gains come at the cost of predictive or optimization performance. We additionally examine how the stability of human expert judgments compares to that of automated models. Method: Using 120+ multi-objective optimization tasks from the MOOT repository of multi-objective optimization tasks, we evaluate stability through a preregistered bootstrap-ensemble protocol that measures variance with win-score assignments. We compare the stability of human causal assessments with correlation-based decision trees (EZR). We would also compare the causality-aware trees, which leverage conditional-entropy split criteria and confounder filtering. Stability and performance differences are analyzed using statistical methods (variance, Gini Impurity, KS test, Cliff's delta)",
      "authors": [
        "Amirali Rayegan",
        "Tim Menzies"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-17 23:35:50+00:00",
      "link": "https://arxiv.org/pdf/2602.16091v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16086v1",
      "title": "LGQ: Learning Discretization Geometry for Scalable and Stable Image Tokenization",
      "abstract": "Discrete image tokenization is a key bottleneck for scalable visual generation: a tokenizer must remain compact for efficient latent-space priors while preserving semantic structure and using discrete capacity effectively. Existing quantizers face a trade-off: vector-quantized tokenizers learn flexible geometries but often suffer from biased straight-through optimization, codebook under-utilization, and representation collapse at large vocabularies. Structured scalar or implicit tokenizers ensure stable, near-complete utilization by design, yet rely on fixed discretization geometries that may allocate capacity inefficiently under heterogeneous latent statistics.   We introduce Learnable Geometric Quantization (LGQ), a discrete image tokenizer that learns discretization geometry end-to-end. LGQ replaces hard nearest-neighbor lookup with temperature-controlled soft assignments, enabling fully differentiable training while recovering hard assignments at inference. The assignments correspond to posterior responsibilities of an isotropic Gaussian mixture and minimize a variational free-energy objective, provably converging to nearest-neighbor quantization in the low-temperature limit. LGQ combines a token-level peakedness regularizer with a global usage regularizer to encourage confident yet balanced code utilization without imposing rigid grids.   Under a controlled VQGAN-style backbone on ImageNet across multiple vocabulary sizes, LGQ achieves stable optimization and balanced utilization. At 16K codebook size, LGQ improves rFID by 11.88% over FSQ while using 49.96% fewer active codes, and improves rFID by 6.06% over SimVQ with 49.45% lower effective representation rate, achieving comparable fidelity with substantially fewer active entries. Our GitHub repository is available at: https://github.com/KurbanIntelligenceLab/LGQ",
      "authors": [
        "Idil Bilge Altun",
        "Mert Onur Cakiroglu",
        "Elham Buxton",
        "Mehmet Dalkilic",
        "Hasan Kurban"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-02-17 23:20:26+00:00",
      "link": "https://arxiv.org/pdf/2602.16086v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16075v1",
      "title": "DARTH-PUM: A Hybrid Processing-Using-Memory Architecture",
      "abstract": "Analog processing-using-memory (PUM; a.k.a. in-memory computing) makes use of electrical interactions inside memory arrays to perform bulk matrix-vector multiplication (MVM) operations. However, many popular matrix-based kernels need to execute non-MVM operations, which analog PUM cannot directly perform. To retain its energy efficiency, analog PUM architectures augment memory arrays with CMOS-based domain-specific fixed-function hardware to provide complete kernel functionality, but the difficulty of integrating such specialized CMOS logic with memory arrays has largely limited analog PUM to being an accelerator for machine learning inference, or for closely related kernels. An opportunity exists to harness analog PUM for general-purpose computation: recent works have shown that memory arrays can also perform Boolean PUM operations, albeit with very different supporting hardware and electrical signals than analog PUM.   We propose DARTH-PUM, a general-purpose hybrid PUM architecture that tackles key hardware and software challenges to integrating analog PUM and digital PUM. We propose optimized peripheral circuitry, coordinating hardware to manage and interface between both types of PUM, an easy-to-use programming interface, and low-cost support for flexible data widths. These design elements allow us to build a practical PUM architecture that can execute kernels fully in memory, and can scale easily to cater to domains ranging from embedded applications to large-scale data-driven computing. We show how three popular applications (AES encryption, convolutional neural networks, large-language models) can map to and benefit from DARTH-PUM, with speedups of 59.4x, 14.8x, and 40.8x over an analog+CPU baseline.",
      "authors": [
        "Ryan Wong",
        "Ben Feinberg",
        "Saugata Ghose"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR",
        "cs.CR",
        "cs.ET"
      ],
      "published": "2026-02-17 22:57:55+00:00",
      "link": "https://arxiv.org/pdf/2602.16075v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16070v1",
      "title": "Access in the Shadow of Ableism: An Autoethnography of a Blind Student's Higher Education Experience in China",
      "abstract": "The HCI research community has witnessed a growing body of research on accessibility and disability driven by efforts to improve access. Yet, the concept of access reveals its limitations when examined within broader ableist structures. Drawing on an autoethnographic method, this study shares the co-first author Zhang's experiences at two higher-education institutions in China, including a specialized program exclusively for blind and low-vision students and a mainstream university where he was the first blind student admitted. Our analysis revealed tensions around access in both institutions: they either marginalized blind students within society at large or imposed pressures to conform to sighted norms. Both institutions were further constrained by systemic issues, including limited accessible resources, pervasive ableist cultures, and the lack of formalized policies. In response to these tensions, we conceptualize access as a contradictory construct and argue for understanding accessibility as an ongoing, exploratory practice within ableist structures.",
      "authors": [
        "Weijun Zhang",
        "Xinru Tang"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "published": "2026-02-17 22:52:41+00:00",
      "link": "https://arxiv.org/pdf/2602.16070v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16065v1",
      "title": "Can Generative Artificial Intelligence Survive Data Contamination? Theoretical Guarantees under Contaminated Recursive Training",
      "abstract": "Generative Artificial Intelligence (AI), such as large language models (LLMs), has become a transformative force across science, industry, and society. As these systems grow in popularity, web data becomes increasingly interwoven with this AI-generated material and it is increasingly difficult to separate them from naturally generated content. As generative models are updated regularly, later models will inevitably be trained on mixtures of human-generated data and AI-generated data from earlier versions, creating a recursive training process with data contamination. Existing theoretical work has examined only highly simplified settings, where both the real data and the generative model are discrete or Gaussian, where it has been shown that such recursive training leads to model collapse. However, real data distributions are far more complex, and modern generative models are far more flexible than Gaussian and linear mechanisms. To fill this gap, we study recursive training in a general framework with minimal assumptions on the real data distribution and allow the underlying generative model to be a general universal approximator. In this framework, we show that contaminated recursive training still converges, with a convergence rate equal to the minimum of the baseline model's convergence rate and the fraction of real data used in each iteration. To the best of our knowledge, this is the first (positive) theoretical result on recursive training without distributional assumptions on the data. We further extend the analysis to settings where sampling bias is present in data collection and support all theoretical results with empirical studies.",
      "authors": [
        "Kevin Wang",
        "Hongqian Niu",
        "Didong Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML"
      ],
      "published": "2026-02-17 22:38:18+00:00",
      "link": "https://arxiv.org/pdf/2602.16065v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16061v1",
      "title": "Partial Identification under Missing Data Using Weak Shadow Variables from Pretrained Models",
      "abstract": "Estimating population quantities such as mean outcomes from user feedback is fundamental to platform evaluation and social science, yet feedback is often missing not at random (MNAR): users with stronger opinions are more likely to respond, so standard estimators are biased and the estimand is not identified without additional assumptions. Existing approaches typically rely on strong parametric assumptions or bespoke auxiliary variables that may be unavailable in practice. In this paper, we develop a partial identification framework in which sharp bounds on the estimand are obtained by solving a pair of linear programs whose constraints encode the observed data structure. This formulation naturally incorporates outcome predictions from pretrained models, including large language models (LLMs), as additional linear constraints that tighten the feasible set. We call these predictions weak shadow variables: they satisfy a conditional independence assumption with respect to missingness but need not meet the completeness conditions required by classical shadow-variable methods. When predictions are sufficiently informative, the bounds collapse to a point, recovering standard identification as a special case. In finite samples, to provide valid coverage of the identified set, we propose a set-expansion estimator that achieves slower-than-$\\sqrt{n}$ convergence rate in the set-identified regime and the standard $\\sqrt{n}$ rate under point identification. In simulations and semi-synthetic experiments on customer-service dialogues, we find that LLM predictions are often ill-conditioned for classical shadow-variable methods yet remain highly effective in our framework. They shrink identification intervals by 75--83\\% while maintaining valid coverage under realistic MNAR mechanisms.",
      "authors": [
        "Hongyu Chen",
        "David Simchi-Levi",
        "Ruoxuan Xiong"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "econ.EM",
        "stat.ME"
      ],
      "published": "2026-02-17 22:18:27+00:00",
      "link": "https://arxiv.org/pdf/2602.16061v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16050v1",
      "title": "Evidence-Grounded Subspecialty Reasoning: Evaluating a Curated Clinical Intelligence Layer on the 2025 Endocrinology Board-Style Examination",
      "abstract": "Background: Large language models have demonstrated strong performance on general medical examinations, but subspecialty clinical reasoning remains challenging due to rapidly evolving guidelines and nuanced evidence hierarchies. Methods: We evaluated January Mirror, an evidence-grounded clinical reasoning system, against frontier LLMs (GPT-5, GPT-5.2, Gemini-3-Pro) on a 120-question endocrinology board-style examination. Mirror integrates a curated endocrinology and cardiometabolic evidence corpus with a structured reasoning architecture to generate evidence-linked outputs. Mirror operated under a closed-evidence constraint without external retrieval. Comparator LLMs had real-time web access to guidelines and primary literature. Results: Mirror achieved 87.5% accuracy (105/120; 95% CI: 80.4-92.3%), exceeding a human reference of 62.3% and frontier LLMs including GPT-5.2 (74.6%), GPT-5 (74.0%), and Gemini-3-Pro (69.8%). On the 30 most difficult questions (human accuracy less than 50%), Mirror achieved 76.7% accuracy. Top-2 accuracy was 92.5% for Mirror versus 85.25% for GPT-5.2. Conclusions: Mirror provided evidence traceability: 74.2% of outputs cited at least one guideline-tier source, with 100% citation accuracy on manual verification. Curated evidence with explicit provenance can outperform unconstrained web retrieval for subspecialty clinical reasoning and supports auditability for clinical deployment.",
      "authors": [
        "Amir Hosseinian",
        "MohammadReza Zare Shahneh",
        "Umer Mansoor",
        "Gilbert Szeto",
        "Kirill Karlin",
        "Nima Aghaeepour"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-17 21:58:17+00:00",
      "link": "https://arxiv.org/pdf/2602.16050v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16039v1",
      "title": "How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment",
      "abstract": "The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.",
      "authors": [
        "Hang Li",
        "Kaiqi Yang",
        "Xianxuan Long",
        "Fedor Filippov",
        "Yucheng Chu",
        "Yasemin Copur-Gencturk",
        "Peng He",
        "Cory Miller",
        "Namsoo Shin",
        "Joseph Krajcik",
        "Hui Liu",
        "Jiliang Tang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-17 21:46:52+00:00",
      "link": "https://arxiv.org/pdf/2602.16039v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16038v1",
      "title": "Heuristic Search as Language-Guided Program Optimization",
      "abstract": "Large Language Models (LLMs) have advanced Automated Heuristic Design (AHD) in combinatorial optimization (CO) in the past few years. However, existing discovery pipelines often require extensive manual trial-and-error or reliance on domain expertise to adapt to new or complex problems. This stems from tightly coupled internal mechanisms that limit systematic improvement of the LLM-driven design process. To address this challenge, we propose a structured framework for LLM-driven AHD that explicitly decomposes the heuristic discovery process into modular stages: a forward pass for evaluation, a backward pass for analytical feedback, and an update step for program refinement. This separation provides a clear abstraction for iterative refinement and enables principled improvements of individual components. We validate our framework across four diverse real-world CO domains, where it consistently outperforms baselines, achieving up to $0.17$ improvement in QYI on unseen test sets. Finally, we show that several popular AHD methods are restricted instantiations of our framework. By integrating them in our structured pipeline, we can upgrade the components modularly and significantly improve their performance.",
      "authors": [
        "Mingxin Yu",
        "Ruixiao Yang",
        "Chuchu Fan"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "published": "2026-02-17 21:45:42+00:00",
      "link": "https://arxiv.org/pdf/2602.16038v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16037v1",
      "title": "Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection",
      "abstract": "Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.",
      "authors": [
        "Cameron Cagan",
        "Pedram Fard",
        "Jiazi Tian",
        "Jingya Cheng",
        "Shawn N. Murphy",
        "Hossein Estiri"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "published": "2026-02-17 21:45:20+00:00",
      "link": "https://arxiv.org/pdf/2602.16037v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16033v1",
      "title": "Transforming GenAI Policy to Prompting Instruction: An RCT of Scalable Prompting Interventions in a CS1 Course",
      "abstract": "Despite universal GenAI adoption, students cannot distinguish task performance from actual learning and lack skills to leverage AI for learning, leading to worse exam performance when AI use remains unreflective. Yet few interventions teaching students to prompt AI as a tutor rather than solution provider have been validated at scale through randomized controlled trials (RCTs). To bridge this gap, we conducted a semester-long RCT (N=979) with four ICAP framework-based instructional conditions varying in engagement intensity with a pre-test, immediate and delayed post-test and surveys. Mixed methods analysis results showed: (1) All conditions significantly improved prompting skills, with gains increasing progressively from Condition 1 to Condition 4, validating ICAP's cognitive engagement hierarchy; (2) for students with similar pre-test scores, higher learning gain in immediate post-test predict higher final exam score, though no direct between-group differences emerged; (3) Our interventions are suitable and scalable solutions for diverse educational contexts, resources and learners. Together, this study makes empirical and theoretical contributions: (1) theoretically, we provided one of the first large-scale RCTs examining how cognitive engagement shapes learning in prompting literacy and clarifying the relationship between learning-oriented prompting skills and broader academic performance; (2) empirically, we offered timely design guidance for transforming GenAI classroom policies into scalable, actionable prompting literacy instruction to advance learning in the era of Generative AI.",
      "authors": [
        "Ruiwei Xiao",
        "Runlong Ye",
        "Xinying Hou",
        "Jessica Wen",
        "Harsh Kumar",
        "Michael Liut",
        "John Stamper"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2026-02-17 21:40:12+00:00",
      "link": "https://arxiv.org/pdf/2602.16033v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16024v1",
      "title": "Bit-Width-Aware Design Environment for Few-Shot Learning on Edge AI Hardware",
      "abstract": "In this study, we propose an implementation methodology of real-time few-shot learning on tiny FPGA SoCs such as the PYNQ-Z1 board with arbitrary fixed-point bit-widths. Tensil-based conventional design environments limited hardware implementations to fixed-point bit-widths of 16 or 32 bits. To address this, we adopt the FINN framework, enabling implementations with arbitrary bit-widths. Several customizations and minor adjustments are made, including: 1.Optimization of Transpose nodes to resolve data format mismatches, 2.Addition of handling for converting the final reduce mean operation to Global Average Pooling (GAP). These adjustments allow us to reduce the bit-width while maintaining the same accuracy as the conventional realization, and achieve approximately twice the throughput in evaluations using CIFAR-10 dataset.",
      "authors": [
        "R. Kanda",
        "H. L. Blevec",
        "N. Onizawa",
        "M. Leonardon",
        "V. Gripon",
        "T. Hanyu"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR"
      ],
      "published": "2026-02-17 21:23:33+00:00",
      "link": "https://arxiv.org/pdf/2602.16024v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16023v1",
      "title": "A Curious Class of Adpositional Multiword Expressions in Korean",
      "abstract": "Multiword expressions (MWEs) have been widely studied in cross-lingual annotation frameworks such as PARSEME. However, Korean MWEs remain underrepresented in these efforts. In particular, Korean multiword adpositions lack systematic analysis, annotated resources, and integration into existing multilingual frameworks. In this paper, we study a class of Korean functional multiword expressions: postpositional verb-based constructions (PVCs). Using data from Korean Wikipedia, we survey and analyze several PVC expressions and contrast them with non-MWEs and light verb constructions (LVCs) with similar structure. Building on this analysis, we propose annotation guidelines designed to support future work in Korean multiword adpositions and facilitate alignment with cross-lingual frameworks.",
      "authors": [
        "Junghyun Min",
        "Na-Rae Han",
        "Jena D. Hwang",
        "Nathan Schneider"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 21:23:16+00:00",
      "link": "https://arxiv.org/pdf/2602.16023v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16020v1",
      "title": "MolCrystalFlow: Molecular Crystal Structure Prediction via Flow Matching",
      "abstract": "Molecular crystal structure prediction represents a grand challenge in computational chemistry due to large sizes of constituent molecules and complex intra- and intermolecular interactions. While generative modeling has revolutionized structure discovery for molecules, inorganic solids, and metal-organic frameworks, extending such approaches to fully periodic molecular crystals is still elusive. Here, we present MolCrystalFlow, a flow-based generative model for molecular crystal structure prediction. The framework disentangles intramolecular complexity from intermolecular packing by embedding molecules as rigid bodies and jointly learning the lattice matrix, molecular orientations, and centroid positions. Centroids and orientations are represented on their native Riemannian manifolds, allowing geodesic flow construction and graph neural network operations that respects geometric symmetries. We benchmark our model against state-of-the-art generative models for large-size periodic crystals and rule-based structure generation methods on two open-source molecular crystal datasets. We demonstrate an integration of MolCrystalFlow model with universal machine learning potential to accelerate molecular crystal structure prediction, paving the way for data-driven generative discovery of molecular crystals.",
      "authors": [
        "Cheng Zeng",
        "Harry W. Sullivan",
        "Thomas Egg",
        "Maya M. Martirossyan",
        "Philipp Höllmer",
        "Jirui Jin",
        "Richard G. Hennig",
        "Adrian Roitberg",
        "Stefano Martiniani",
        "Ellad B. Tadmor",
        "Mingjie Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci"
      ],
      "published": "2026-02-17 21:22:08+00:00",
      "link": "https://arxiv.org/pdf/2602.16020v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16019v1",
      "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval",
      "abstract": "Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations as Gaussian embeddings through a probabilistic contrastive objective that explicitly captures uncertainty and many-to-many correspondences between radiographs and clinical narratives. A variational information bottleneck mitigates overconfident predictions, while MedProbCLIP employs multi-view radiograph encoding and multi-section report encoding during training to provide fine-grained supervision for clinically aligned correspondence, yet requires only a single radiograph and a single report at inference. Evaluated on the MIMIC-CXR dataset, MedProbCLIP outperforms deterministic and probabilistic baselines, including CLIP, CXR-CLIP, and PCME++, in both retrieval and zero-shot classification. Beyond accuracy, MedProbCLIP demonstrates superior calibration, risk-coverage behavior, selective retrieval reliability, and robustness to clinically relevant corruptions, underscoring the value of probabilistic vision-language modeling for improving the trustworthiness and safety of radiology image-text retrieval systems.",
      "authors": [
        "Ahmad Elallaf",
        "Yu Zhang",
        "Yuktha Priya Masupalli",
        "Jeong Yang",
        "Young Lee",
        "Zechun Cao",
        "Gongbo Liang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-17 21:20:32+00:00",
      "link": "https://arxiv.org/pdf/2602.16019v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16015v1",
      "title": "Geometry-Aware Uncertainty Quantification via Conformal Prediction on Manifolds",
      "abstract": "Conformal prediction provides distribution-free coverage guaranties for regression; yet existing methods assume Euclidean output spaces and produce prediction regions that are poorly calibrated when responses lie on Riemannian manifolds. We propose \\emph{adaptive geodesic conformal prediction}, a framework that replaces Euclidean residuals with geodesic nonconformity scores and normalizes them by a cross-validated difficulty estimator to handle heteroscedastic noise. The resulting prediction regions, geodesic caps on the sphere, have position-independent area and adapt their size to local prediction difficulty, yielding substantially more uniform conditional coverage than non-adaptive alternatives. In a synthetic sphere experiment with strong heteroscedasticity and a real-world geomagnetic field forecasting task derived from IGRF-14 satellite data, the adaptive method markedly reduces conditional coverage variability and raises worst-case coverage much closer to the nominal level, while coordinate-based baselines waste a large fraction of coverage area due to chart distortion.",
      "authors": [
        "Marzieh Amiri Shahbazi",
        "Ali Baheri"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 21:12:47+00:00",
      "link": "https://arxiv.org/pdf/2602.16015v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16012v1",
      "title": "Towards Efficient Constraint Handling in Neural Solvers for Routing Problems",
      "abstract": "Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.",
      "authors": [
        "Jieyi Bi",
        "Zhiguang Cao",
        "Jianan Zhou",
        "Wen Song",
        "Yaoxin Wu",
        "Jie Zhang",
        "Yining Ma",
        "Cathy Wu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-02-17 21:06:23+00:00",
      "link": "https://arxiv.org/pdf/2602.16012v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16006v1",
      "title": "BTReport: A Framework for Brain Tumor Radiology Report Generation with Clinically Relevant Features",
      "abstract": "Recent advances in radiology report generation (RRG) have been driven by large paired image-text datasets; however, progress in neuro-oncology has been limited due to a lack of open paired image-report datasets. Here, we introduce BTReport, an open-source framework for brain tumor RRG that constructs natural language radiology reports using deterministically extracted imaging features. Unlike existing approaches that rely on large general-purpose or fine-tuned vision-language models for both image interpretation and report composition, BTReport performs deterministic feature extraction for image analysis and uses large language models only for syntactic structuring and narrative formatting. By separating RRG into a deterministic feature extraction step and a report generation step, the generated reports are completely interpretable and less prone to hallucinations. We show that the features used for report generation are predictive of key clinical outcomes, including survival and IDH mutation status, and reports generated by BTReport are more closely aligned with reference clinical reports than existing baselines for RRG. Finally, we introduce BTReport-BraTS, a companion dataset that augments BraTS imaging with synthetically generated radiology reports produced with BTReport. Code for this project can be found at  https://github.com/KurtLabUW/BTReport.",
      "authors": [
        "Juampablo E. Heras Rivera",
        "Dickson T. Chen",
        "Tianyi Ren",
        "Daniel K. Low",
        "Asma Ben Abacha",
        "Alberto Santamaria-Pang",
        "Mehmet Kurt"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-17 20:55:00+00:00",
      "link": "https://arxiv.org/pdf/2602.16006v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16005v1",
      "title": "ODYN: An All-Shifted Non-Interior-Point Method for Quadratic Programming in Robotics and AI",
      "abstract": "We introduce ODYN, a novel all-shifted primal-dual non-interior-point quadratic programming (QP) solver designed to efficiently handle challenging dense and sparse QPs. ODYN combines all-shifted nonlinear complementarity problem (NCP) functions with proximal method of multipliers to robustly address ill-conditioned and degenerate problems, without requiring linear independence of the constraints. It exhibits strong warm-start performance and is well suited to both general-purpose optimization, and robotics and AI applications, including model-based control, estimation, and kernel-based learning methods. We provide an open-source implementation and benchmark ODYN on the Maros-Mészáros test set, demonstrating state-of-the-art convergence performance in small-to-high-scale problems. The results highlight ODYN's superior warm-starting capabilities, which are critical in sequential and real-time settings common in robotics and AI. These advantages are further demonstrated by deploying ODYN as the backend of an SQP-based predictive control framework (OdynSQP), as the implicitly differentiable optimization layer for deep learning (ODYNLayer), and the optimizer of a contact-dynamics simulation (ODYNSim).",
      "authors": [
        "Jose Rojas",
        "Aristotelis Papatheodorou",
        "Sergi Martinez",
        "Ioannis Havoutis",
        "Carlos Mastalli"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2026-02-17 20:52:32+00:00",
      "link": "https://arxiv.org/pdf/2602.16005v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15997v1",
      "title": "Anatomy of Capability Emergence: Scale-Invariant Representation Collapse and Top-Down Reorganization in Neural Networks",
      "abstract": "Capability emergence during neural network training remains mechanistically opaque. We track five geometric measures across five model scales (405K-85M parameters), 120+ emergence events in eight algorithmic tasks, and three Pythia language models (160M-2.8B). We find: (1) training begins with a universal representation collapse to task-specific floors that are scale-invariant across a 210X parameter range (e.g., modular arithmetic collapses to RANKME ~ 2.0 regardless of model size); (2) collapse propagates top-down through layers (32/32 task X model consistency), contradicting bottom-up feature-building intuition; (3) a geometric hierarchy in which representation geometry leads emergence (75-100% precursor rate for hard tasks), while the local learning coefficient is synchronous (0/24 precursor) and Hessian measures lag. We also delineate prediction limits: geometric measures encode coarse task difficulty but not fine-grained timing (within-class concordance 27%; when task ordering reverses across scales, prediction fails at 26%). On Pythia, global geometric patterns replicate but per-task precursor signals do not -- the precursor relationship requires task-training alignment that naturalistic pre-training does not provide. Our contribution is the geometric anatomy of emergence and its boundary conditions, not a prediction tool.",
      "authors": [
        "Jayadev Billa"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-17 20:39:02+00:00",
      "link": "https://arxiv.org/pdf/2602.15997v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15984v1",
      "title": "Verifier-Constrained Flow Expansion for Discovery Beyond the Data",
      "abstract": "Flow and diffusion models are typically pre-trained on limited available data (e.g., molecular samples), covering only a fraction of the valid design space (e.g., the full molecular space). As a consequence, they tend to generate samples from only a narrow portion of the feasible domain. This is a fundamental limitation for scientific discovery applications, where one typically aims to sample valid designs beyond the available data distribution. To this end, we address the challenge of leveraging access to a verifier (e.g., an atomic bonds checker), to adapt a pre-trained flow model so that its induced density expands beyond regions of high data availability, while preserving samples validity. We introduce formal notions of strong and weak verifiers and propose algorithmic frameworks for global and local flow expansion via probability-space optimization. Then, we present Flow Expander (FE), a scalable mirror descent scheme that provably tackles both problems by verifier-constrained entropy maximization over the flow process noised state space. Next, we provide a thorough theoretical analysis of the proposed method, and state convergence guarantees under both idealized and general assumptions. Ultimately, we empirically evaluate our method on both illustrative, yet visually interpretable settings, and on a molecular design task showcasing the ability of FE to expand a pre-trained flow model increasing conformer diversity while preserving validity.",
      "authors": [
        "Riccardo De Santi",
        "Kimon Protopapas",
        "Ya-Ping Hsieh",
        "Andreas Krause"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 20:20:38+00:00",
      "link": "https://arxiv.org/pdf/2602.15984v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15983v1",
      "title": "ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization",
      "abstract": "Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.",
      "authors": [
        "Junbo Jacob Lian",
        "Yujun Sun",
        "Huiling Chen",
        "Chaoyu Zhang",
        "Chung-Piaw Teo"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-02-17 20:20:33+00:00",
      "link": "https://arxiv.org/pdf/2602.15983v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15972v1",
      "title": "Fast Online Learning with Gaussian Prior-Driven Hierarchical Unimodal Thompson Sampling",
      "abstract": "We study a type of Multi-Armed Bandit (MAB) problems in which arms with a Gaussian reward feedback are clustered. Such an arm setting finds applications in many real-world problems, for example, mmWave communications and portfolio management with risky assets, as a result of the universality of the Gaussian distribution. Based on the Thompson Sampling algorithm with Gaussian prior (TSG) algorithm for the selection of the optimal arm, we propose our Thompson Sampling with Clustered arms under Gaussian prior (TSCG) specific to the 2-level hierarchical structure. We prove that by utilizing the 2-level structure, we can achieve a lower regret bound than we do with ordinary TSG. In addition, when the reward is Unimodal, we can reach an even lower bound on the regret by our Unimodal Thompson Sampling algorithm with Clustered Arms under Gaussian prior (UTSCG). Each of our proposed algorithms are accompanied by theoretical evaluation of the upper regret bound, and our numerical experiments confirm the advantage of our proposed algorithms.",
      "authors": [
        "Tianchi Zhao",
        "He Liu",
        "Hongyin Shi",
        "Jinliang Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-17 19:46:00+00:00",
      "link": "https://arxiv.org/pdf/2602.15972v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15971v1",
      "title": "B-DENSE: Branching For Dense Ensemble Network Learning",
      "abstract": "Inspired by non-equilibrium thermodynamics, diffusion models have achieved state-of-the-art performance in generative modeling. However, their iterative sampling nature results in high inference latency. While recent distillation techniques accelerate sampling, they discard intermediate trajectory steps. This sparse supervision leads to a loss of structural information and introduces significant discretization errors. To mitigate this, we propose B-DENSE, a novel framework that leverages multi-branch trajectory alignment. We modify the student architecture to output $K$-fold expanded channels, where each subset corresponds to a specific branch representing a discrete intermediate step in the teacher's trajectory. By training these branches to simultaneously map to the entire sequence of the teacher's target timesteps, we enforce dense intermediate trajectory alignment. Consequently, the student model learns to navigate the solution space from the earliest stages of training, demonstrating superior image generation quality compared to baseline distillation frameworks.",
      "authors": [
        "Cherish Puniani",
        "Tushar Kumar",
        "Arnav Bendre",
        "Gaurav Kumar",
        "Shree Singhi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "published": "2026-02-17 19:40:58+00:00",
      "link": "https://arxiv.org/pdf/2602.15971v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15965v1",
      "title": "FLoPS: Semantics, Operations, and Properties of P3109 Floating-Point Representations in Lean",
      "abstract": "The upcoming IEEE-P3109 standard for low-precision floating-point arithmetic can become the foundation of future machine learning hardware and software. Unlike the fixed types of IEEE-754, P3109 introduces a parametric framework defined by bitwidth, precision, signedness, and domain. This flexibility results in a vast combinatorial space of formats -- some with as little as one bit of precision -- alongside novel features such as stochastic rounding and saturation arithmetic. These deviations create a unique verification gap that this paper intends to address. This paper presents FLoPS, Formalization in Lean of the P3109 Standard, which is a comprehensive formal model of P3109 in Lean. Our work serves as a rigorous, machine-checked specification that facilitates deep analysis of the standard. We demonstrate the model's utility by verifying foundational properties and analyzing key algorithms within the P3109 context. Specifically, we reveal that FastTwoSum exhibits a novel property of computing exact \"overflow error\" under saturation using any rounding mode, whereas previously established properties of the ExtractScalar algorithm fail for formats with one bit of precision. This work provides a verified foundation for reasoning about P3109 and enables formal verification of future numerical software. Our Lean development is open source and publicly available.",
      "authors": [
        "Tung-Che Chang",
        "Sehyeok Park",
        "Jay P Lim",
        "Santosh Nagarakatte"
      ],
      "primary_category": "cs.MS",
      "categories": [
        "cs.MS"
      ],
      "published": "2026-02-17 19:30:24+00:00",
      "link": "https://arxiv.org/pdf/2602.15965v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15964v1",
      "title": "Computing Approximate Pareto Frontiers for Submodular Utility and Cost Tradeoffs",
      "abstract": "In many data-mining applications, including recommender systems, influence maximization, and team formation, the goal is to pick a subset of elements (e.g., items, nodes in a network, experts to perform a task) to maximize a monotone submodular utility function while simultaneously minimizing a cost function. Classical formulations model this tradeoff via cardinality or knapsack constraints, or by combining utility and cost into a single weighted objective. However, such approaches require committing to a specific tradeoff in advance and return only a single solution, offering limited insight into the space of viable utility-cost tradeoffs. In this paper, we depart from the single-solution paradigm and examine the problem of computing representative sets of high-quality solutions that expose different tradeoffs between submodular utility and cost. For this, we introduce $(α_1,α_2)$-approximate Pareto frontiers that provably approximate the achievable tradeoffs between submodular utility and cost. Specifically, we formalize the Pareto-$\\langle f,c \\rangle$ problem and develop efficient algorithms for multiple instantiations arising from different combinations of submodular utility $f$ and cost functions $c$. Our results offer a principled and practical framework for understanding and exploiting utility-cost tradeoffs in submodular optimization. Experiments on datasets from diverse application domains demonstrate that our algorithms efficiently compute approximate Pareto frontiers in practice.",
      "authors": [
        "Karan Vombatkere",
        "Evimaria Terzi"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS",
        "cs.DM",
        "cs.SI"
      ],
      "published": "2026-02-17 19:28:55+00:00",
      "link": "https://arxiv.org/pdf/2602.15964v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15958v1",
      "title": "DocSplit: A Comprehensive Benchmark Dataset and Evaluation Approach for Document Packet Recognition and Splitting",
      "abstract": "Document understanding in real-world applications often requires processing heterogeneous, multi-page document packets containing multiple documents stitched together. Despite recent advances in visual document understanding, the fundamental task of document packet splitting, which involves separating a document packet into individual units, remains largely unaddressed. We present the first comprehensive benchmark dataset, DocSplit, along with novel evaluation metrics for assessing the document packet splitting capabilities of large language models. DocSplit comprises five datasets of varying complexity, covering diverse document types, layouts, and multimodal settings. We formalize the DocSplit task, which requires models to identify document boundaries, classify document types, and maintain correct page ordering within a document packet. The benchmark addresses real-world challenges, including out-of-order pages, interleaved documents, and documents lacking clear demarcations. We conduct extensive experiments evaluating multimodal LLMs on our datasets, revealing significant performance gaps in current models' ability to handle complex document splitting tasks. The DocSplit benchmark datasets and proposed novel evaluation metrics provide a systematic framework for advancing document understanding capabilities essential for legal, financial, healthcare, and other document-intensive domains. We release the datasets to facilitate future research in document packet processing.",
      "authors": [
        "Md Mofijul Islam",
        "Md Sirajus Salekin",
        "Nivedha Balakrishnan",
        "Vincil C. Bishop",
        "Niharika Jain",
        "Spencer Romo",
        "Bob Strahan",
        "Boyi Xie",
        "Diego A. Socolinsky"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-17 19:17:55+00:00",
      "link": "https://arxiv.org/pdf/2602.15958v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15951v1",
      "title": "MadEvolve: Evolutionary Optimization of Cosmological Algorithms with Large Language Models",
      "abstract": "We develop a general framework to discover scientific algorithms and apply it to three problems in computational cosmology. Our code, MadEvolve, is similar to Google's AlphaEvolve, but places a stronger emphasis on free parameters and their optimization. Our code starts with a baseline human algorithm implementation, and then optimizes its performance metrics by making iterative changes to its code. As a further convenient feature, MadEvolve automatically generates a report that compares the input algorithm with the evolved algorithm, describes the algorithmic innovations and lists the free parameters and their function. Our code supports both auto-differentiable, gradient-based parameter optimization and gradient-free optimization methods. We apply MadEvolve to the reconstruction of cosmological initial conditions, 21cm foreground contamination reconstruction and effective baryonic physics in N-body simulations. In all cases, we find substantial improvements over the base algorithm. We make MadEvolve and our three tasks publicly available at madevolve.org.",
      "authors": [
        "Tianyi Li",
        "Shihui Zang",
        "Moritz Münchmeyer"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO",
        "cs.LG"
      ],
      "published": "2026-02-17 19:06:52+00:00",
      "link": "https://arxiv.org/pdf/2602.15951v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15820v1",
      "title": "Stabilizing Test-Time Adaptation of High-Dimensional Simulation Surrogates via D-Optimal Statistics",
      "abstract": "Machine learning surrogates are increasingly used in engineering to accelerate costly simulations, yet distribution shifts between training and deployment often cause severe performance degradation (e.g., unseen geometries or configurations). Test-Time Adaptation (TTA) can mitigate such shifts, but existing methods are largely developed for lower-dimensional classification with structured outputs and visually aligned input-output relationships, making them unstable for the high-dimensional, unstructured and regression problems common in simulation. We address this challenge by proposing a TTA framework based on storing maximally informative (D-optimal) statistics, which jointly enables stable adaptation and principled parameter selection at test time. When applied to pretrained simulation surrogates, our method yields up to 7% out-of-distribution improvements at negligible computational cost. To the best of our knowledge, this is the first systematic demonstration of effective TTA for high-dimensional simulation regression and generative design optimization, validated on the SIMSHIFT and EngiBench benchmarks.",
      "authors": [
        "Anna Zimmel",
        "Paul Setinek",
        "Gianluca Galletti",
        "Johannes Brandstetter",
        "Werner Zellinger"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 18:55:18+00:00",
      "link": "https://arxiv.org/pdf/2602.15820v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15814v1",
      "title": "Avey-B",
      "abstract": "Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.",
      "authors": [
        "Devang Acharya",
        "Mohammad Hammoud"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-17 18:50:40+00:00",
      "link": "https://arxiv.org/pdf/2602.15814v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15813v1",
      "title": "FAST-EQA: Efficient Embodied Question Answering with Global and Local Region Relevancy",
      "abstract": "Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability. A central challenge is to confine physical search to question-relevant subspaces while maintaining a compact, actionable memory of observations. Furthermore, for real-world deployment, fast inference time during exploration is crucial. We introduce FAST-EQA, a question-conditioned framework that (i) identifies likely visual targets, (ii) scores global regions of interest to guide navigation, and (iii) employs Chain-of-Thought (CoT) reasoning over visual memory to answer confidently. FAST-EQA maintains a bounded scene memory that stores a fixed-capacity set of region-target hypotheses and updates them online, enabling robust handling of both single and multi-target questions without unbounded growth. To expand coverage efficiently, a global exploration policy treats narrow openings and doors as high-value frontiers, complementing local target seeking with minimal computation. Together, these components focus the agent's attention, improve scene coverage, and improve answer reliability while running substantially faster than prior approaches. On HMEQA and EXPRESS-Bench, FAST-EQA achieves state-of-the-art performance, while performing competitively on OpenEQA and MT-HM3D.",
      "authors": [
        "Haochen Zhang",
        "Nirav Savaliya",
        "Faizan Siddiqui",
        "Enna Sachdeva"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-02-17 18:49:43+00:00",
      "link": "https://arxiv.org/pdf/2602.15813v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15781v1",
      "title": "Neural Scaling Laws for Boosted Jet Tagging",
      "abstract": "The success of Large Language Models (LLMs) has established that scaling compute, through joint increases in model capacity and dataset size, is the primary driver of performance in modern machine learning. While machine learning has long been an integral component of High Energy Physics (HEP) data analysis workflows, the compute used to train state-of-the-art HEP models remains orders of magnitude below that of industry foundation models. With scaling laws only beginning to be studied in the field, we investigate neural scaling laws for boosted jet classification using the public JetClass dataset. We derive compute optimal scaling laws and identify an effective performance limit that can be consistently approached through increased compute. We study how data repetition, common in HEP where simulation is expensive, modifies the scaling yielding a quantifiable effective dataset size gain. We then study how the scaling coefficients and asymptotic performance limits vary with the choice of input features and particle multiplicity, demonstrating that increased compute reliably drives performance toward an asymptotic limit, and that more expressive, lower-level features can raise the performance limit and improve results at fixed dataset size.",
      "authors": [
        "Matthias Vigl",
        "Nicole Hartman",
        "Michael Kagan",
        "Lukas Heinrich"
      ],
      "primary_category": "hep-ex",
      "categories": [
        "hep-ex",
        "cs.LG",
        "hep-ph",
        "physics.data-an"
      ],
      "published": "2026-02-17 18:13:01+00:00",
      "link": "https://arxiv.org/pdf/2602.15781v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15778v1",
      "title": "*-PLUIE: Personalisable metric with Llm Used for Improved Evaluation",
      "abstract": "Evaluating the quality of automatically generated text often relies on LLM-as-a-judge (LLM-judge) methods. While effective, these approaches are computationally expensive and require post-processing. To address these limitations, we build upon ParaPLUIE, a perplexity-based LLM-judge metric that estimates confidence over ``Yes/No'' answers without generating text. We introduce *-PLUIE, task specific prompting variants of ParaPLUIE and evaluate their alignment with human judgement. Our experiments show that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost.",
      "authors": [
        "Quentin Lemesle",
        "Léane Jourdan",
        "Daisy Munson",
        "Pierre Alain",
        "Jonathan Chevelu",
        "Arnaud Delhay",
        "Damien Lolive"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 18:10:00+00:00",
      "link": "https://arxiv.org/pdf/2602.15778v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15925v1",
      "title": "Robust Stochastic Gradient Posterior Sampling with Lattice Based Discretisation",
      "abstract": "Stochastic-gradient MCMC methods enable scalable Bayesian posterior sampling but often suffer from sensitivity to minibatch size and gradient noise. To address this, we propose Stochastic Gradient Lattice Random Walk (SGLRW), an extension of the Lattice Random Walk discretization. Unlike conventional Stochastic Gradient Langevin Dynamics (SGLD), SGLRW introduces stochastic noise only through the off-diagonal elements of the update covariance; this yields greater robustness to minibatch size while retaining asymptotic correctness. Furthermore, as comparison we analyze a natural analogue of SGLD utilizing gradient clipping. Experimental validation on Bayesian regression and classification demonstrates that SGLRW remains stable in regimes where SGLD fails, including in the presence of heavy-tailed gradient noise, and matches or improves predictive performance.",
      "authors": [
        "Zier Mensch",
        "Lars Holdijk",
        "Samuel Duffield",
        "Maxwell Aifer",
        "Patrick J. Coles",
        "Max Welling",
        "Miranda C. N. Cheng"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-17 18:09:49+00:00",
      "link": "https://arxiv.org/pdf/2602.15925v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15769v1",
      "title": "ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution",
      "abstract": "Multimodal Large Language Models (mLLMs) are often used to answer questions in structured data such as tables in Markdown, JSON, and images. While these models can often give correct answers, users also need to know where those answers come from. In this work, we study structured data attribution/citation, which is the ability of the models to point to the specific rows and columns that support an answer. We evaluate several mLLMs across different table formats and prompting strategies. Our results show a clear gap between question answering and evidence attribution. Although question answering accuracy remains moderate, attribution accuracy is much lower, near random for JSON inputs, across all models. We also find that models are more reliable at citing rows than columns, and struggle more with textual formats than images. Finally, we observe notable differences across model families. Overall, our findings show that current mLLMs are unreliable at providing fine-grained, trustworthy attribution for structured data, which limits their usage in applications requiring transparency and traceability.",
      "authors": [
        "Yahia Alqurnawi",
        "Preetom Biswas",
        "Anmol Rao",
        "Tejas Anvekar",
        "Chitta Baral",
        "Vivek Gupta"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 18:01:35+00:00",
      "link": "https://arxiv.org/pdf/2602.15769v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15756v1",
      "title": "A Note on Non-Composability of Layerwise Approximate Verification for Neural Inference",
      "abstract": "A natural and informal approach to verifiable (or zero-knowledge) ML inference over floating-point data is: ``prove that each layer was computed correctly up to tolerance $δ$; therefore the final output is a reasonable inference result''. This short note gives a simple counterexample showing that this inference is false in general: for any neural network, we can construct a functionally equivalent network for which adversarially chosen approximation-magnitude errors in individual layer computations suffice to steer the final output arbitrarily (within a prescribed bounded range).",
      "authors": [
        "Or Zamir"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published": "2026-02-17 17:41:59+00:00",
      "link": "https://arxiv.org/pdf/2602.15756v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15755v1",
      "title": "RaCo: Ranking and Covariance for Practical Learned Keypoints",
      "abstract": "This paper introduces RaCo, a lightweight neural network designed to learn robust and versatile keypoints suitable for a variety of 3D computer vision tasks. The model integrates three key components: the repeatable keypoint detector, a differentiable ranker to maximize matches with a limited number of keypoints, and a covariance estimator to quantify spatial uncertainty in metric scale. Trained on perspective image crops only, RaCo operates without the need for covisible image pairs. It achieves strong rotational robustness through extensive data augmentation, even without the use of computationally expensive equivariant network architectures. The method is evaluated on several challenging datasets, where it demonstrates state-of-the-art performance in keypoint repeatability and two-view matching, particularly under large in-plane rotations. Ultimately, RaCo provides an effective and simple strategy to independently estimate keypoint ranking and metric covariance without additional labels, detecting interpretable and repeatable interest points. The code is available at https://github.com/cvg/RaCo.",
      "authors": [
        "Abhiram Shenoi",
        "Philipp Lindenberger",
        "Paul-Edouard Sarlin",
        "Marc Pollefeys"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2026-02-17 17:39:52+00:00",
      "link": "https://arxiv.org/pdf/2602.15755v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15753v1",
      "title": "Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac",
      "abstract": "Low-resource languages pose persistent challenges for Natural Language Processing tasks such as lemmatization and part-of-speech (POS) tagging. This paper investigates the capacity of recent large language models (LLMs), including GPT-4 variants and open-weight Mistral models, to address these tasks in few-shot and zero-shot settings for four historically and linguistically diverse under-resourced languages: Ancient Greek, Classical Armenian, Old Georgian, and Syriac. Using a novel benchmark comprising aligned training and out-of-domain test corpora, we evaluate the performance of foundation models across lemmatization and POS-tagging, and compare them with PIE, a task-specific RNN baseline. Our results demonstrate that LLMs, even without fine-tuning, achieve competitive or superior performance in POS-tagging and lemmatization across most languages in few-shot settings. Significant challenges persist for languages characterized by complex morphology and non-Latin scripts, but we demonstrate that LLMs are a credible and relevant option for initiating linguistic annotation tasks in the absence of data, serving as an effective aid for annotation.",
      "authors": [
        "Chahan Vidal-Gorène",
        "Bastien Kindt",
        "Florian Cafiero"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 17:34:32+00:00",
      "link": "https://arxiv.org/pdf/2602.15753v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15751v1",
      "title": "Enabling Low-Latency Machine learning on Radiation-Hard FPGAs with hls4ml",
      "abstract": "This paper presents the first demonstration of a viable, ultra-fast, radiation-hard machine learning (ML) application on FPGAs, which could be used in future high-energy physics experiments. We present a three-fold contribution, with the PicoCal calorimeter, planned for the LHCb Upgrade II experiment, used as a test case. First, we develop a lightweight autoencoder to compress a 32-sample timing readout, representative of that of the PicoCal, into a two-dimensional latent space. Second, we introduce a systematic, hardware-aware quantization strategy and show that the model can be reduced to 10-bit weights with minimal performance loss. Third, as a barrier to the adoption of on-detector ML is the lack of support for radiation-hard FPGAs in the High-Energy Physics community's standard ML synthesis tool, hls4ml, we develop a new backend for this library. This new back-end enables the automatic translation of ML models into High-Level Synthesis (HLS) projects for the Microchip PolarFire family of FPGAs, one of the few commercially available and radiation hard FPGAs. We present the synthesis of the autoencoder on a target PolarFire FPGA, which indicates that a latency of 25 ns can be achieved. We show that the resources utilized are low enough that the model can be placed within the inherently protected logic of the FPGA. Our extension to hls4ml is a significant contribution, paving the way for broader adoption of ML on FPGAs in high-radiation environments.",
      "authors": [
        "Katya Govorkova",
        "Julian Garcia Pardinas",
        "Vladimir Loncar",
        "Victoria Nguyen",
        "Sebastian Schmitt",
        "Marco Pizzichemi",
        "Loris Martinazzoli",
        "Eluned Anne Smith"
      ],
      "primary_category": "hep-ex",
      "categories": [
        "hep-ex",
        "cs.LG"
      ],
      "published": "2026-02-17 17:30:28+00:00",
      "link": "https://arxiv.org/pdf/2602.15751v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15738v1",
      "title": "Beyond Labels: Information-Efficient Human-in-the-Loop Learning using Ranking and Selection Queries",
      "abstract": "Integrating human expertise into machine learning systems often reduces the role of experts to labeling oracles, a paradigm that limits the amount of information exchanged and fails to capture the nuances of human judgment. We address this challenge by developing a human-in-the-loop framework to learn binary classifiers with rich query types, consisting of item ranking and exemplar selection. We first introduce probabilistic human response models for these rich queries motivated by the relationship experimentally observed between the perceived implicit score of an item and its distance to the unknown classifier. Using these models, we then design active learning algorithms that leverage the rich queries to increase the information gained per interaction. We provide theoretical bounds on sample complexity and develop a tractable and computationally efficient variational approximation. Through experiments with simulated annotators derived from crowdsourced word-sentiment and image-aesthetic datasets, we demonstrate significant reductions on sample complexity. We further extend active learning strategies to select queries that maximize information rate, explicitly balancing informational value against annotation cost. This algorithm in the word sentiment classification task reduces learning time by more than 57\\% compared to traditional label-only active learning.",
      "authors": [
        "Belén Martín-Urcelay",
        "Yoonsang Lee",
        "Matthieu R. Bloch",
        "Christopher J. Rozell"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.LG"
      ],
      "published": "2026-02-17 17:14:15+00:00",
      "link": "https://arxiv.org/pdf/2602.15738v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15736v1",
      "title": "SVD Incidence Centrality: A Unified Spectral Framework for Node and Edge Analysis in Directed Networks and Hypergraphs",
      "abstract": "Identifying influential nodes and edges in directed networks remains a fundamental challenge across domains from social influence to biological regulation. Most existing centrality measures face a critical limitation: they either discard directional information through symmetrization or produce sparse, implementation-dependent rankings that obscure structural importance. We introduce a unified spectral framework for centrality analysis in directed networks grounded in the Singular value decomposition of the incidence matrix. The proposed approach derives both vertex and edge centralities via the pseudoinverse of Hodge Laplacians, yielding dense and well-resolved rankings that overcome the sparsity limitations commonly observed in betweenness centrality for directed graphs. Unlike traditional measures that require graph symmetrization, our framework naturally preserves directional information, enabling principled hub/authority analysis while maintaining mathematical consistency through spectral graph theory. The method extends naturally to hypergraphs through the same mathematical foundation. Experimental validation on real-world networks demonstrates the framework's effectiveness across diverse domains where traditional centrality measures encounter limitations due to implementation dependencies and sparse outputs.",
      "authors": [
        "Jorge Luiz Franco",
        "Thomas Peron",
        "Alcebiades Dal Col",
        "Fabiano Petronetto",
        "Filipe Alves Neto Verri",
        "Eric K. Tokuda",
        "Luiz Gustavo Nonato"
      ],
      "primary_category": "cs.SI",
      "categories": [
        "cs.SI",
        "physics.soc-ph"
      ],
      "published": "2026-02-17 17:11:00+00:00",
      "link": "https://arxiv.org/pdf/2602.15736v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15730v1",
      "title": "Causal Effect Estimation with Latent Textual Treatments",
      "abstract": "Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our work first performs hypothesis generation and steering via sparse autoencoders (SAEs), followed by robust causal estimation. Our pipeline addresses both computational and statistical challenges in text-as-treatment experiments. We demonstrate that naive estimation of causal effects suffers from significant bias as text inherently conflates treatment and covariate information. We describe the estimation bias induced in this setting and propose a solution based on covariate residualization. Our empirical results show that our pipeline effectively induces variation in target features and mitigates estimation error, providing a robust foundation for causal effect estimation in text-as-treatment settings.",
      "authors": [
        "Omri Feldman",
        "Amar Venugopal",
        "Jann Spiess",
        "Amir Feder"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "econ.EM"
      ],
      "published": "2026-02-17 17:06:12+00:00",
      "link": "https://arxiv.org/pdf/2602.15730v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15725v1",
      "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models",
      "abstract": "Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.",
      "authors": [
        "Sarim Chaudhry"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-17 17:01:42+00:00",
      "link": "https://arxiv.org/pdf/2602.15725v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15716v1",
      "title": "Rethinking Metrics for Lexical Semantic Change Detection",
      "abstract": "Lexical semantic change detection (LSCD) increasingly relies on contextualised language model embeddings, yet most approaches still quantify change using a small set of semantic change metrics, primarily Average Pairwise Distance (APD) and cosine distance over word prototypes (PRT). We introduce Average Minimum Distance (AMD) and Symmetric Average Minimum Distance (SAMD), new measures that quantify semantic change via local correspondence between word usages across time periods. Across multiple languages, encoder models, and representation spaces, we show that AMD often provides more robust performance, particularly under dimensionality reduction and with non-specialised encoders, while SAMD excels with specialised encoders. We suggest that LSCD may benefit from considering alternative semantic change metrics beyond APD and PRT, with AMD offering a robust option for contextualised embedding-based analysis.",
      "authors": [
        "Roksana Goworek",
        "Haim Dubossarsky"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 16:47:40+00:00",
      "link": "https://arxiv.org/pdf/2602.15716v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15712v1",
      "title": "Criteria-first, semantics-later: reproducible structure discovery in image-based sciences",
      "abstract": "Across the natural and life sciences, images have become a primary measurement modality, yet the dominant analytic paradigm remains semantics-first. Structure is recovered by predicting or enforcing domain-specific labels. This paradigm fails systematically under the conditions that make image-based science most valuable, including open-ended scientific discovery, cross-sensor and cross-site comparability, and long-term monitoring in which domain ontologies and associated label sets drift culturally, institutionally, and ecologically. A deductive inversion is proposed in the form of criteria-first and semantics-later. A unified framework for criteria-first structure discovery is introduced. It separates criterion-defined, semantics-free structure extraction from downstream semantic mapping into domain ontologies or vocabularies and provides a domain-general scaffold for reproducible analysis across image-based sciences. Reproducible science requires that the first analytic layer perform criterion-driven, semantics-free structure discovery, yielding stable partitions, structural fields, or hierarchies defined by explicit optimality criteria rather than local domain ontologies. Semantics is not discarded; it is relocated downstream as an explicit mapping from the discovered structural product to a domain ontology or vocabulary, enabling plural interpretations and explicit crosswalks without rewriting upstream extraction. Grounded in cybernetics, observation-as-distinction, and information theory's separation of information from meaning, the argument is supported by cross-domain evidence showing that criteria-first components recur whenever labels do not scale. Finally, consequences are outlined for validation beyond class accuracy and for treating structural products as FAIR, AI-ready digital objects for long-term monitoring and digital twins.",
      "authors": [
        "Jan Bumberger"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-17 16:45:49+00:00",
      "link": "https://arxiv.org/pdf/2602.15712v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15681v1",
      "title": "A universal LLM Framework for General Query Refinements",
      "abstract": "Numerous studies have explored the SQL query refinement problem, where the objective is to minimally modify an input query so that it satisfies a specified set of constraints. However, these works typically target restricted classes of queries or constraints. We present OmniTune, a general framework for refining arbitrary SQL queries using LLM-based optimization by prompting (OPRO). OmniTune employs a two-step OPRO scheme that explores promising refinement subspaces and samples candidates within them, supported by concise history and skyline summaries for effective feedback.   Experiments on a comprehensive benchmark demonstrate that OmniTune handles both previously studied refinement tasks and more complex scenarios beyond the scope of existing solutions.",
      "authors": [
        "Eldar Hacohen",
        "Yuval Moskovitch",
        "Amit Somech"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-17 16:04:03+00:00",
      "link": "https://arxiv.org/pdf/2602.15681v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15676v1",
      "title": "Relative Geometry of Neural Forecasters: Linking Accuracy and Alignment in Learned Latent Geometry",
      "abstract": "Neural networks can accurately forecast complex dynamical systems, yet how they internally represent underlying latent geometry remains poorly understood. We study neural forecasters through the lens of representational alignment, introducing anchor-based, geometry-agnostic relative embeddings that remove rotational and scaling ambiguities in latent spaces. Applying this framework across seven canonical dynamical systems - ranging from periodic to chaotic - we reveal reproducible family-level structure: multilayer perceptrons align with other MLPs, recurrent networks with RNNs, while transformers and echo-state networks achieve strong forecasts despite weaker alignment. Alignment generally correlates with forecasting accuracy, yet high accuracy can coexist with low alignment. Relative geometry thus provides a simple, reproducible foundation for comparing how model families internalize and represent dynamical structure.",
      "authors": [
        "Deniz Kucukahmetler",
        "Maximilian Jean Hemmann",
        "Julian Mosig von Aehrenfeld",
        "Maximilian Amthor",
        "Christian Deubel",
        "Nico Scherf",
        "Diaaeldin Taha"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-17 16:00:08+00:00",
      "link": "https://arxiv.org/pdf/2602.15676v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15675v1",
      "title": "LLM-to-Speech: A Synthetic Data Pipeline for Training Dialectal Text-to-Speech Models",
      "abstract": "Despite the advances in neural text to speech (TTS), many Arabic dialectal varieties remain marginally addressed, with most resources concentrated on Modern Spoken Arabic (MSA) and Gulf dialects, leaving Egyptian Arabic -- the most widely understood Arabic dialect -- severely under-resourced. We address this gap by introducing NileTTS: 38 hours of transcribed speech from two speakers across diverse domains including medical, sales, and general conversations. We construct this dataset using a novel synthetic pipeline: large language models (LLM) generate Egyptian Arabic content, which is then converted to natural speech using audio synthesis tools, followed by automatic transcription and speaker diarization with manual quality verification. We fine-tune XTTS v2, a state-of-the-art multilingual TTS model, on our dataset and evaluate against the baseline model trained on other Arabic dialects. Our contributions include: (1) the first publicly available Egyptian Arabic TTS dataset, (2) a reproducible synthetic data generation pipeline for dialectal TTS, and (3) an open-source fine-tuned model. All resources are released to advance Egyptian Arabic speech synthesis research.",
      "authors": [
        "Ahmed Khaled Khamis",
        "Hesham Ali"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 15:58:27+00:00",
      "link": "https://arxiv.org/pdf/2602.15675v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15669v1",
      "title": "PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra",
      "abstract": "Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.",
      "authors": [
        "Xiachong Feng",
        "Liang Zhao",
        "Weihong Zhong",
        "Yichong Huang",
        "Yuxuan Gu",
        "Lingpeng Kong",
        "Xiaocheng Feng",
        "Bing Qin"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-17 15:47:58+00:00",
      "link": "https://arxiv.org/pdf/2602.15669v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15659v1",
      "title": "Can Recommender Systems Teach Themselves? A Recursive Self-Improving Framework with Fidelity Control",
      "abstract": "The scarcity of high-quality training data presents a fundamental bottleneck to scaling machine learning models. This challenge is particularly acute in recommendation systems, where extreme sparsity in user interactions leads to rugged optimization landscapes and poor generalization. We propose the Recursive Self-Improving Recommendation (RSIR) framework, a paradigm in which a model bootstraps its own performance without reliance on external data or teacher models. RSIR operates in a closed loop: the current model generates plausible user interaction sequences, a fidelity-based quality control mechanism filters them for consistency with user's approximate preference manifold, and a successor model is augmented on the enriched dataset. Our theoretical analysis shows that RSIR acts as a data-driven implicit regularizer, smoothing the optimization landscape and guiding models toward more robust solutions. Empirically, RSIR yields consistent, cumulative gains across multiple benchmarks and architectures. Notably, even smaller models benefit, and weak models can generate effective training curricula for stronger ones. These results demonstrate that recursive self-improvement is a general, model-agnostic approach to overcoming data sparsity, suggesting a scalable path forward for recommender systems and beyond. Our anonymized code is available at https://anonymous.4open.science/r/RSIR-7C5B .",
      "authors": [
        "Luankang Zhang",
        "Hao Wang",
        "Zhongzhou Liu",
        "Mingjia Yin",
        "Yonghao Huang",
        "Jiaqi Li",
        "Wei Guo",
        "Yong Liu",
        "Huifeng Guo",
        "Defu Lian",
        "Enhong Chen"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR"
      ],
      "published": "2026-02-17 15:31:32+00:00",
      "link": "https://arxiv.org/pdf/2602.15659v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15650v1",
      "title": "Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation",
      "abstract": "Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.",
      "authors": [
        "Marco Salmè",
        "Federico Siciliano",
        "Fabrizio Silvestri",
        "Paolo Soda",
        "Rosa Sicilia",
        "Valerio Guarrasi"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-17 15:18:07+00:00",
      "link": "https://arxiv.org/pdf/2602.15650v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15649v1",
      "title": "Continuous-Time Piecewise-Linear Recurrent Neural Networks",
      "abstract": "In dynamical systems reconstruction (DSR) we aim to recover the dynamical system (DS) underlying observed time series. Specifically, we aim to learn a generative surrogate model which approximates the underlying, data-generating DS, and recreates its long-term properties (`climate statistics'). In scientific and medical areas, in particular, these models need to be mechanistically tractable -- through their mathematical analysis we would like to obtain insight into the recovered system's workings. Piecewise-linear (PL), ReLU-based RNNs (PLRNNs) have a strong track-record in this regard, representing SOTA DSR models while allowing mathematical insight by virtue of their PL design. However, all current PLRNN variants are discrete-time maps. This is in disaccord with the assumed continuous-time nature of most physical and biological processes, and makes it hard to accommodate data arriving at irregular temporal intervals. Neural ODEs are one solution, but they do not reach the DSR performance of PLRNNs and often lack their tractability. Here we develop theory for continuous-time PLRNNs (cPLRNNs): We present a novel algorithm for training and simulating such models, bypassing numerical integration by efficiently exploiting their PL structure. We further demonstrate how important topological objects like equilibria or limit cycles can be determined semi-analytically in trained models. We compare cPLRNNs to both their discrete-time cousins as well as Neural ODEs on DSR benchmarks, including systems with discontinuities which come with hard thresholds.",
      "authors": [
        "Alena Brändle",
        "Lukas Eisenmann",
        "Florian Götz",
        "Daniel Durstewitz"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 15:16:12+00:00",
      "link": "https://arxiv.org/pdf/2602.15649v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15638v1",
      "title": "Who Is Doing the Thinking? AI as a Dynamic Cognitive Partner: A Learner-Informed Framework",
      "abstract": "Artificial intelligence is increasingly embedded in education, yet there remains a need to explain how students conceptualize AI's role in their thinking and learning. This study proposes a framework positioning AI as a dynamic cognitive partner whose function shifts across learning situations. Using qualitative analysis of written responses from 133 secondary students in Hong Kong following completion of an AI literacy course, we identified nine interrelated dimensions through which learners described AI as partnering with their cognition: conceptual scaffolding for difficult ideas; feedback and error detection; idea stimulation; cognitive organization; adaptive tutoring support; metacognitive monitoring support; task and cognitive load regulation; learning continuity beyond classroom boundaries; and explanation reframing through representational flexibility during moments of being stuck or overwhelmed. Across these dimensions, students distinguished between productive support that extends understanding and unproductive reliance that replaces cognitive effort, indicating situational awareness of when AI should and should not be used. Grounded in sociocultural theory, distributed cognition, self-regulated learning, and cognitive load perspectives, the framework clarifies how AI becomes integrated into learners' cognitive activity while illuminating the boundary between cognitive extension and substitution.",
      "authors": [
        "C. K. Y Chan"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY"
      ],
      "published": "2026-02-17 15:07:06+00:00",
      "link": "https://arxiv.org/pdf/2602.15638v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15634v1",
      "title": "Beyond ReLU: Bifurcation, Oversmoothing, and Topological Priors",
      "abstract": "Graph Neural Networks (GNNs) learn node representations through iterative network-based message-passing. While powerful, deep GNNs suffer from oversmoothing, where node features converge to a homogeneous, non-informative state. We re-frame this problem of representational collapse from a \\emph{bifurcation theory} perspective, characterizing oversmoothing as convergence to a stable ``homogeneous fixed point.'' Our central contribution is the theoretical discovery that this undesired stability can be broken by replacing standard monotone activations (e.g., ReLU) with a class of functions. Using Lyapunov-Schmidt reduction, we analytically prove that this substitution induces a bifurcation that destabilizes the homogeneous state and creates a new pair of stable, non-homogeneous \\emph{patterns} that provably resist oversmoothing. Our theory predicts a precise, nontrivial scaling law for the amplitude of these emergent patterns, which we quantitatively validate in experiments. Finally, we demonstrate the practical utility of our theory by deriving a closed-form, bifurcation-aware initialization and showing its utility in real benchmark experiments.",
      "authors": [
        "Erkan Turan",
        "Gaspard Abel",
        "Maysam Behmanesh",
        "Emery Pierson",
        "Maks Ovsjanikov"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 15:03:28+00:00",
      "link": "https://arxiv.org/pdf/2602.15634v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15632v1",
      "title": "Neural-POD: A Plug-and-Play Neural Operator Framework for Infinite-Dimensional Functional Nonlinear Proper Orthogonal Decomposition",
      "abstract": "The rapid development of AI for Science is often hindered by the \"discretization\", where learned representations remain restricted to the specific grids or resolutions used during training. We propose the Neural Proper Orthogonal Decomposition (Neural-POD), a plug-and-play neural operator framework that constructs nonlinear, orthogonal basis functions in infinite-dimensional space using neural networks. Unlike the classical Proper Orthogonal Decomposition (POD), which is limited to linear subspace approximations obtained through singular value decomposition (SVD), Neural-POD formulates basis construction as a sequence of residual minimization problems solved through neural network training. Each basis function is obtained by learning to represent the remaining structure in the data, following a process analogous to Gram--Schmidt orthogonalization. This neural formulation introduces several key advantages over classical POD: it enables optimization in arbitrary norms (e.g., $L^2$, $L^1$), learns mappings between infinite-dimensional function spaces that is resolution-invariant, generalizes effectively to unseen parameter regimes, and inherently captures nonlinear structures in complex spatiotemporal systems. The resulting basis functions are interpretable, reusable, and enabling integration into both reduced order modeling (ROM) and operator learning frameworks such as deep operator learning (DeepONet). We demonstrate the robustness of Neural-POD with different complex spatiotemporal systems, including the Burgers' and Navier-Stokes equations. We further show that Neural-POD serves as a high performance, plug-and-play bridge between classical Galerkin projection and operator learning that enables consistent integration with both projection-based reduced order models and DeepONet frameworks.",
      "authors": [
        "Changhong Mou",
        "Binghang Lu",
        "Guang Lin"
      ],
      "primary_category": "physics.comp-ph",
      "categories": [
        "physics.comp-ph",
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-02-17 15:01:40+00:00",
      "link": "https://arxiv.org/pdf/2602.15632v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15613v1",
      "title": "Algorithmic differentiation for domain specific languages in C++ with expression templates",
      "abstract": "The application of operator overloading algorithmic differentiation (AD) to computer programs in order to compute the derivative is quite common. But, the replacement of the underlying computational floating point type with the specialized type of an AD tool has two problems. First, the memory structure of the program is changed and floating-point data is interleaved with identifiers from AD. This prevents the compiler from performing optimizations such as SIMD optimizations. Second, the AD tool does not see any domain-specific operations, e.,g. linear algebra operations, that the program uses. This prevents the AD tool from using specialized algorithms in such places. We propose a new AD tool that is tailored to such situations. The memory structure of the primal data is retained by associating an identifier with each entity, e.,g. matrix, and not with each floating point value, e.,g. element of the matrix. Operations on such entities can then be annotated and a generator is used to create the AD overloads. We demonstrate that this approach provides performance comparable to that of other specializations. In addition, the run-time factor is below the theoretical 4.5 of reverse AD for programs that are written purely with linear algebra entities and operations.",
      "authors": [
        "Max Sagebaum",
        "Nicolas R. Gauger"
      ],
      "primary_category": "cs.MS",
      "categories": [
        "cs.MS"
      ],
      "published": "2026-02-17 14:42:07+00:00",
      "link": "https://arxiv.org/pdf/2602.15613v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15603v1",
      "title": "Symbolic recovery of PDEs from measurement data",
      "abstract": "Models based on partial differential equations (PDEs) are powerful for describing a wide range of complex relationships in the natural sciences. Accurately identifying the PDE model, which represents the underlying physical law, is essential for a proper understanding of the problem. This reconstruction typically relies on indirect and noisy measurements of the system's state and, without specifically tailored methods, rarely yields symbolic expressions, thereby hindering interpretability. In this work, we address this issue by considering existing neural network architectures based on rational functions for the symbolic representation of physical laws. These networks leverage the approximation power of rational functions while also benefiting from their flexibility in representing arithmetic operations. Our main contribution is an identifiability result, showing that, in the limit of noiseless, complete measurements, such symbolic networks can uniquely reconstruct the simplest physical law within the PDE model. Specifically, reconstructed laws remain expressible within the symbolic network architecture, with regularization-minimizing parameterizations promoting interpretability and sparsity in case of $L^1$-regularization. In addition, we provide regularity results for symbolic networks. Empirical validation using the ParFam architecture supports these theoretical findings, providing evidence for the practical reconstructibility of physical laws.",
      "authors": [
        "Erion Morina",
        "Philipp Scholl",
        "Martin Holler"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.SC",
        "math.OC"
      ],
      "published": "2026-02-17 14:20:36+00:00",
      "link": "https://arxiv.org/pdf/2602.15603v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15602v1",
      "title": "Certified Per-Instance Unlearning Using Individual Sensitivity Bounds",
      "abstract": "Certified machine unlearning can be achieved via noise injection leading to differential privacy guarantees, where noise is calibrated to worst-case sensitivity. Such conservative calibration often results in performance degradation, limiting practical applicability. In this work, we investigate an alternative approach based on adaptive per-instance noise calibration tailored to the individual contribution of each data point to the learned solution. This raises the following challenge: how can one establish formal unlearning guarantees when the mechanism depends on the specific point to be removed? To define individual data point sensitivities in noisy gradient dynamics, we consider the use of per-instance differential privacy. For ridge regression trained via Langevin dynamics, we derive high-probability per-instance sensitivity bounds, yielding certified unlearning with substantially less noise injection. We corroborate our theoretical findings through experiments in linear settings and provide further empirical evidence on the relevance of the approach in deep learning settings.",
      "authors": [
        "Hanna Benarroch",
        "Jamal Atif",
        "Olivier Cappé"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-17 14:18:47+00:00",
      "link": "https://arxiv.org/pdf/2602.15602v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15595v1",
      "title": "Multi-Objective Coverage via Constraint Active Search",
      "abstract": "In this paper, we formulate the new multi-objective coverage (MOC) problem where our goal is to identify a small set of representative samples whose predicted outcomes broadly cover the feasible multi-objective space. This problem is of great importance in many critical real-world applications, e.g., drug discovery and materials design, as this representative set can be evaluated much faster than the whole feasible set, thus significantly accelerating the scientific discovery process. Existing works cannot be directly applied as they either focus on sample space coverage or multi-objective optimization that targets the Pareto front. However, chemically diverse samples often yield identical objective profiles, and safety constraints are usually defined on the objectives. To solve this MOC problem, we propose a novel search algorithm, MOC-CAS, which employs an upper confidence bound-based acquisition function to select optimistic samples guided by Gaussian process posterior predictions. For enabling efficient optimization, we develop a smoothed relaxation of the hard feasibility test and derive an approximate optimizer. Compared to the competitive baselines, we show that our MOC-CAS empirically achieves superior performances across large-scale protein-target datasets for SARS-CoV-2 and cancer, each assessed on five objectives derived from SMILES-based features.",
      "authors": [
        "Zakaria Shams Siam",
        "Xuefeng Liu",
        "Chong Liu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 14:07:16+00:00",
      "link": "https://arxiv.org/pdf/2602.15595v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15593v1",
      "title": "A unified theory of feature learning in RNNs and DNNs",
      "abstract": "Recurrent and deep neural networks (RNNs/DNNs) are cornerstone architectures in machine learning. Remarkably, RNNs differ from DNNs only by weight sharing, as can be shown through unrolling in time. How does this structural similarity fit with the distinct functional properties these networks exhibit? To address this question, we here develop a unified mean-field theory for RNNs and DNNs in terms of representational kernels, describing fully trained networks in the feature learning ($μ$P) regime. This theory casts training as Bayesian inference over sequences and patterns, directly revealing the functional implications induced by the RNNs' weight sharing. In DNN-typical tasks, we identify a phase transition when the learning signal overcomes the noise due to randomness in the weights: below this threshold, RNNs and DNNs behave identically; above it, only RNNs develop correlated representations across timesteps. For sequential tasks, the RNNs' weight sharing furthermore induces an inductive bias that aids generalization by interpolating unsupervised time steps. Overall, our theory offers a way to connect architectural structure to functional biases.",
      "authors": [
        "Jan P. Bauer",
        "Kirsten Fischer",
        "Moritz Helias",
        "Agostina Palmigiano"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn"
      ],
      "published": "2026-02-17 14:06:34+00:00",
      "link": "https://arxiv.org/pdf/2602.15593v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15592v1",
      "title": "Uni-Flow: a unified autoregressive-diffusion model for complex multiscale flows",
      "abstract": "Spatiotemporal flows govern diverse phenomena across physics, biology, and engineering, yet modelling their multiscale dynamics remains a central challenge. Despite major advances in physics-informed machine learning, existing approaches struggle to simultaneously maintain long-term temporal evolution and resolve fine-scale structure across chaotic, turbulent, and physiological regimes. Here, we introduce Uni-Flow, a unified autoregressive-diffusion framework that explicitly separates temporal evolution from spatial refinement for modelling complex dynamical systems. The autoregressive component learns low-resolution latent dynamics that preserve large-scale structure and ensure stable long-horizon rollouts, while the diffusion component reconstructs high-resolution physical fields, recovering fine-scale features in a small number of denoising steps. We validate Uni-Flow across canonical benchmarks, including two-dimensional Kolmogorov flow, three-dimensional turbulent channel inflow generation with a quantum-informed autoregressive prior, and patient-specific simulations of aortic coarctation derived from high-fidelity lattice Boltzmann hemodynamic solvers. In the cardiovascular setting, Uni-Flow enables task-level faster than real-time inference of pulsatile hemodynamics, reconstructing high-resolution pressure fields over physiologically relevant time horizons in seconds rather than hours. By transforming high-fidelity hemodynamic simulation from an offline, HPC-bound process into a deployable surrogate, Uni-Flow establishes a pathway to faster-than-real-time modelling of complex multiscale flows, with broad implications for scientific machine learning in flow physics.",
      "authors": [
        "Xiao Xue",
        "Tianyue Yang",
        "Mingyang Gao",
        "Leyu Pan",
        "Maida Wang",
        "Kewei Zhu",
        "Shuo Wang",
        "Jiuling Li",
        "Marco F. P. ten Eikelder",
        "Peter V. Coveney"
      ],
      "primary_category": "physics.flu-dyn",
      "categories": [
        "physics.flu-dyn",
        "cs.LG",
        "physics.comp-ph"
      ],
      "published": "2026-02-17 14:04:37+00:00",
      "link": "https://arxiv.org/pdf/2602.15592v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15586v1",
      "title": "Uniform error bounds for quantized dynamical models",
      "abstract": "This paper provides statistical guarantees on the accuracy of dynamical models learned from dependent data sequences. Specifically, we develop uniform error bounds that apply to quantized models and imperfect optimization algorithms commonly used in practical contexts for system identification, and in particular hybrid system identification. Two families of bounds are obtained: slow-rate bounds via a block decomposition and fast-rate, variance-adaptive, bounds via a novel spaced-point strategy. The bounds scale with the number of bits required to encode the model and thus translate hardware constraints into interpretable statistical complexities.",
      "authors": [
        "Abdelkader Metakalard",
        "Fabien Lauer",
        "Kevin Colin",
        "Marion Gilson"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-17 13:56:04+00:00",
      "link": "https://arxiv.org/pdf/2602.15586v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15585v1",
      "title": "Optimal detection of planted stars via a random energy model",
      "abstract": "We study the problem of detecting a planted star in the Erd{ő}s--R{é}nyi random graph $G(n,m)$, formulated as a hypothesis test. We determine the scaling window for critical detection in $m$ in terms of the star size, and characterize the asymptotic total variation distance between the null and alternative hypotheses in this window. In the course of the proofs we show a condensation phase transition in the likelihood ratio that closely resembles that of the random energy model from spin glass theory.",
      "authors": [
        "Ijay Narang",
        "Will Perkins",
        "Timothy L. H. Wee"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "cs.IT",
        "math.PR"
      ],
      "published": "2026-02-17 13:55:43+00:00",
      "link": "https://arxiv.org/pdf/2602.15585v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15580v1",
      "title": "How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning",
      "abstract": "When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \\emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \\emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\\% of the final prediction, and cross-modal synergy remains below 2\\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.",
      "authors": [
        "Hongxuan Wu",
        "Yukun Zhang",
        "Xueqing Zhou"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-17 13:49:49+00:00",
      "link": "https://arxiv.org/pdf/2602.15580v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15571v1",
      "title": "Accelerated Predictive Coding Networks via Direct Kolen-Pollack Feedback Alignment",
      "abstract": "Predictive coding (PC) is a biologically inspired algorithm for training neural networks that relies only on local updates, allowing parallel learning across layers. However, practical implementations face two key limitations: error signals must still propagate from the output to early layers through multiple inference-phase steps, and feedback decays exponentially during this process, leading to vanishing updates in early layers. We propose direct Kolen-Pollack predictive coding (DKP-PC), which simultaneously addresses both feedback delay and exponential decay, yielding a more efficient and scalable variant of PC while preserving update locality. Leveraging direct feedback alignment and direct Kolen-Pollack algorithms, DKP-PC introduces learnable feedback connections from the output layer to all hidden layers, establishing a direct pathway for error transmission. This yields an algorithm that reduces the theoretical error propagation time complexity from O(L), with L being the network depth, to O(1), removing depth-dependent delay in error signals. Moreover, empirical results demonstrate that DKP-PC achieves performance at least comparable to, and often exceeding, that of standard PC, while offering improved latency and computational performance, supporting its potential for custom hardware-efficient implementations.",
      "authors": [
        "Davide Casnici",
        "Martin Lefebvre",
        "Justin Dauwels",
        "Charlotte Frenkel"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 13:29:14+00:00",
      "link": "https://arxiv.org/pdf/2602.15571v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15564v1",
      "title": "Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL",
      "abstract": "Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs' reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at https://github.com/Satissss/SquRL",
      "authors": [
        "Yihan Wang",
        "Peiyu Liu",
        "Runyu Chen",
        "Wei Xu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-17 13:24:56+00:00",
      "link": "https://arxiv.org/pdf/2602.15564v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15563v1",
      "title": "1-Bit Wonder: Improving QAT Performance in the Low-Bit Regime through K-Means Quantization",
      "abstract": "Quantization-aware training (QAT) is an effective method to drastically reduce the memory footprint of LLMs while keeping performance degradation at an acceptable level. However, the optimal choice of quantization format and bit-width presents a challenge in practice. The full design space of quantization is not fully explored in the context of QAT, and the precise trade-off between quantization and downstream performance is poorly understood, as comparisons often rely solely on perplexity-based evaluations. In this work, we address these shortcomings with an empirical study of QAT in the low-bit regime. We show that k-means based weight quantization outperforms integer formats and can be implemented efficiently on standard hardware. Furthermore, we find that, under a fixed inference memory budget, the best performance on generative downstream tasks is achieved with $1$-bit quantized weights.",
      "authors": [
        "Sohir Maskey",
        "Constantin Eichenberg",
        "Johannes Messner",
        "Douglas Orr"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 13:23:26+00:00",
      "link": "https://arxiv.org/pdf/2602.15563v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15552v1",
      "title": "Latent Regularization in Generative Test Input Generation",
      "abstract": "This study investigates the impact of regularization of latent spaces through truncation on the quality of generated test inputs for deep learning classifiers. We evaluate this effect using style-based GANs, a state-of-the-art generative approach, and assess quality along three dimensions: validity, diversity, and fault detection. We evaluate our approach on the boundary testing of deep learning image classifiers across three datasets, MNIST, Fashion MNIST, and CIFAR-10. We compare two truncation strategies: latent code mixing with binary search optimization and random latent truncation for generative exploration. Our experiments show that the latent code-mixing approach yields a higher fault detection rate than random truncation, while also improving both diversity and validity.",
      "authors": [
        "Giorgi Merabishvili",
        "Oliver Weißl",
        "Andrea Stocco"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.LG"
      ],
      "published": "2026-02-17 12:57:17+00:00",
      "link": "https://arxiv.org/pdf/2602.15552v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15547v1",
      "title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation",
      "abstract": "Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.",
      "authors": [
        "Mohammad Kalim Akram",
        "Saba Sturua",
        "Nastia Havriushenko",
        "Quentin Herreros",
        "Michael Günther",
        "Maximilian Werk",
        "Han Xiao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 12:50:50+00:00",
      "link": "https://arxiv.org/pdf/2602.15547v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15546v1",
      "title": "CEPAE: Conditional Entropy-Penalized Autoencoders for Time Series Counterfactuals",
      "abstract": "The ability to accurately perform counterfactual inference on time series is crucial for decision-making in fields like finance, healthcare, and marketing, as it allows us to understand the impact of events or treatments on outcomes over time. In this paper, we introduce a new counterfactual inference approach tailored to time series data impacted by market events, which is motivated by an industrial application. Utilizing the abduction-action-prediction procedure and the Structural Causal Model framework, we first adapt methods based on variational autoencoders and adversarial autoencoders, both previously used in counterfactual literature although not in time series settings. Then, we present the Conditional Entropy-Penalized Autoencoder (CEPAE), a novel autoencoder-based approach for counterfactual inference, which employs an entropy penalization loss over the latent space to encourage disentangled data representations. We validate our approach both theoretically and experimentally on synthetic, semi-synthetic, and real-world datasets, showing that CEPAE generally outperforms the other approaches in the evaluated metrics.",
      "authors": [
        "Tomàs Garriga",
        "Gerard Sanz",
        "Eduard Serrahima de Cambra",
        "Axel Brando"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 12:49:44+00:00",
      "link": "https://arxiv.org/pdf/2602.15546v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15540v1",
      "title": "Perspectives - Interactive Document Clustering in the Discourse Analysis Tool Suite",
      "abstract": "This paper introduces Perspectives, an interactive extension of the Discourse Analysis Tool Suite designed to empower Digital Humanities (DH) scholars to explore and organize large, unstructured document collections. Perspectives implements a flexible, aspect-focused document clustering pipeline with human-in-the-loop refinement capabilities. We showcase how this process can be initially steered by defining analytical lenses through document rewriting prompts and instruction-based embeddings, and further aligned with user intent through tools for refining clusters and mechanisms for fine-tuning the embedding model. The demonstration highlights a typical workflow, illustrating how DH researchers can leverage Perspectives's interactive document map to uncover topics, sentiments, or other relevant categories, thereby gaining insights and preparing their data for subsequent in-depth analysis.",
      "authors": [
        "Tim Fischer",
        "Chris Biemann"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 12:44:05+00:00",
      "link": "https://arxiv.org/pdf/2602.15540v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15538v1",
      "title": "Functional Central Limit Theorem for Stochastic Gradient Descent",
      "abstract": "We study the asymptotic shape of the trajectory of the stochastic gradient descent algorithm applied to a convex objective function. Under mild regularity assumptions, we prove a functional central limit theorem for the properly rescaled trajectory. Our result characterizes the long-term fluctuations of the algorithm around the minimizer by providing a diffusion limit for the trajectory. In contrast with classical central limit theorems for the last iterate or Polyak-Ruppert averages, this functional result captures the temporal structure of the fluctuations and applies to non-smooth settings such as robust location estimation, including the geometric median.",
      "authors": [
        "Kessang Flamand",
        "Victor-Emmanuel Brunel"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-02-17 12:42:19+00:00",
      "link": "https://arxiv.org/pdf/2602.15538v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15921v1",
      "title": "Latent Objective Induction and Diversity-Constrained Selection: Algorithms for Multi-Locale Retrieval Pipelines",
      "abstract": "We present three algorithms with formal correctness guarantees and complexity bounds for the problem of selecting a diverse, multi-locale set of sources from ranked search results. First, we formulate weighted locale allocation as a constrained integer partition problem and give an $O(n \\log n)$ algorithm that simultaneously satisfies minimum-representation, budget-exhaustion, and proportionality-bound constraints; we prove all three hold with a tight deviation bound of $< 1$. Second, we define a cascaded country-code inference function as a deterministic priority chain over heterogeneous signals (TLD structure, model-inferred metadata, language fallback) and prove it satisfies both determinism and graceful degradation. Third, we introduce a $κ$-domain diversity constraint for source selection and give an $O(|K| \\cdot R)$ algorithm that maintains the invariant via hash-map lookup, eliminating the aggregator monopolization pathology present in URL-level deduplication. We further formalize Latent Objective Induction (LOI), an environment-shaping operator over prompt spaces that steers downstream model behavior without restricting the feasible output set, and prove its convergence under mild assumptions. Applied to a multi-locale retrieval pipeline, these algorithms yield 62% improvement in first-party source ratio and 89% reduction in same-domain duplication across 120 multilingual queries.",
      "authors": [
        "Faruk Alpay",
        "Levent Sarioglu"
      ],
      "primary_category": "cs.DS",
      "categories": [
        "cs.DS",
        "cs.IR"
      ],
      "published": "2026-02-17 12:25:22+00:00",
      "link": "https://arxiv.org/pdf/2602.15921v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15531v1",
      "title": "GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway",
      "abstract": "This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.",
      "authors": [
        "Javier Irigoyen",
        "Roberto Daza",
        "Aythami Morales",
        "Julian Fierrez",
        "Francisco Jurado",
        "Alvaro Ortigosa",
        "Ruben Tolosana"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "published": "2026-02-17 12:11:49+00:00",
      "link": "https://arxiv.org/pdf/2602.15531v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15521v1",
      "title": "ExpertWeaver: Unlocking the Inherent MoE in Dense LLMs with GLU Activation Patterns",
      "abstract": "Mixture-of-Experts (MoE) effectively scales model capacity while preserving computational efficiency through sparse expert activation. However, training high-quality MoEs from scratch is prohibitively expensive. A promising alternative is to convert pretrained dense models into sparse MoEs. Existing dense-to-MoE methods fall into two categories: \\textbf{dynamic structural pruning} that converts dense models into MoE architectures with moderate sparsity to balance performance and inference efficiency, and \\textbf{downcycling} approaches that use pretrained dense models to initialize highly sparse MoE architectures. However, existing methods break the intrinsic activation patterns within dense models, leading to suboptimal expert construction. In this work, we argue that the Gated Linear Unit (GLU) mechanism provides a natural blueprint for dense-to-MoE conversion. We show that the fine-grained neural-wise activation patterns of GLU reveal a coarse-grained structure, uncovering an inherent MoE architecture composed of consistently activated universal neurons and dynamically activated specialized neurons. Leveraging this discovery, we introduce ExpertWeaver, a training-free framework that partitions neurons according to their activation patterns and constructs shared experts and specialized routed experts with layer-adaptive configurations. Our experiments demonstrate that ExpertWeaver significantly outperforms existing methods, both as a training-free dynamic structural pruning technique and as a downcycling strategy for superior MoE initialization.",
      "authors": [
        "Ziyu Zhao",
        "Tong Zhu",
        "Zhi Zhang",
        "Tiantian Fan",
        "Jinluan Yang",
        "Kun Kuang",
        "Zhongyu Wei",
        "Fei Wu",
        "Yu Cheng"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2026-02-17 11:50:58+00:00",
      "link": "https://arxiv.org/pdf/2602.15521v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15514v1",
      "title": "DependencyAI: Detecting AI Generated Text through Dependency Parsing",
      "abstract": "As large language models (LLMs) become increasingly prevalent, reliable methods for detecting AI-generated text are critical for mitigating potential risks. We introduce DependencyAI, a simple and interpretable approach for detecting AI-generated text using only the labels of linguistic dependency relations. Our method achieves competitive performance across monolingual, multi-generator, and multilingual settings. To increase interpretability, we analyze feature importance to reveal syntactic structures that distinguish AI-generated from human-written text. We also observe a systematic overprediction of certain models on unseen domains, suggesting that generator-specific writing styles may affect cross-domain generalization. Overall, our results demonstrate that dependency relations alone provide a robust signal for AI-generated text detection, establishing DependencyAI as a strong linguistically grounded, interpretable, and non-neural network baseline.",
      "authors": [
        "Sara Ahmed",
        "Tracy Hammond"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 11:42:28+00:00",
      "link": "https://arxiv.org/pdf/2602.15514v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15511v1",
      "title": "Generating Theorems by Generating Proof Structures",
      "abstract": "We address generating theorems from a given set of axioms, without proof goal, aiming at value from a mathematical point of view or as lemmas for automated proving. As benchmark, we convert a fragment of the Metamath database set.mm. Our techniques are centered on proof terms and condensed detachment, which ties in with approaches to automated first-order proving by proof structure enumeration, and links to Metamath as well as to formulas-as-types. Our methods for generating theorems are based on partitioning the set of proof terms into inductively characterized levels. We study two ideas for improvement: Lemma synthesis by DAG compression of proof term sets and incorporating combinators into proof term construction.",
      "authors": [
        "Christoph Wernhard"
      ],
      "primary_category": "cs.LO",
      "categories": [
        "cs.LO"
      ],
      "published": "2026-02-17 11:39:10+00:00",
      "link": "https://arxiv.org/pdf/2602.15511v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15510v1",
      "title": "On the Geometric Coherence of Global Aggregation in Federated GNN",
      "abstract": "Federated Learning (FL) enables distributed training across multiple clients without centralized data sharing, while Graph Neural Networks (GNNs) model relational data through message passing. In federated GNN settings, client graphs often exhibit heterogeneous structural and propagation characteristics. When standard aggregation mechanisms are applied to such heterogeneous updates, the global model may converge numerically while exhibiting degraded relational behavior.Our work identifies a geometric failure mode of global aggregation in Cross- Domain Federated GNNs. Although GNN parameters are numerically represented as vectors, they encode relational transformations that govern the direction, strength, and sensitivity of information flow across graph neighborhoods. Aggregating updates originating from incompatible propagation regimes can therefore introduce destructive interference in this transformation space.This leads to loss of coherence in global message passing. Importantly, this degradation is not necessarily reflected in conventional metrics such as loss or accuracy.To address this issue, we propose GGRS (Global Geometric Reference Structure), a server-side framework that regulates client updates prior to aggregation based on geometric admissibility criteria. GGRS preserves directional consistency of relational transformations as well as maintains diversity of admissible propagation subspaces. It also stabilizes sensitivity to neighborhood interactions, without accessing client data or graph topology. Experiments on heterogeneous GNN-native, Amazon Co-purchase datasets demonstrate that GGRS preserves global message-passing coherence across training rounds by highlighting the necessity of geometry-aware regulation in federated graph learning.",
      "authors": [
        "Chethana Prasad Kabgere",
        "Shylaja SS"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.NI"
      ],
      "published": "2026-02-17 11:34:04+00:00",
      "link": "https://arxiv.org/pdf/2602.15510v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15506v1",
      "title": "LuxMT Technical Report",
      "abstract": "We introduce LuxMT, a machine translation system based on Gemma 3 27B and fine-tuned for translation from Luxembourgish (LB) into French (FR) and English (EN). To assess translation performance, we construct a novel benchmark covering LB-FR, LB-EN, and LB-FR using human-translated data from Luci, a tourist magazine about Luxembourg. Training data stems from LuxAlign, a parallel corpus of multilingual Luxembourgish news articles, and LB parliamentary transcripts augmented with Google Translate. We filter the data using LuxEmbedder, LB sentence embeddings, to remove low-equivalence segment-pairs. Overall, LuxMT's results suggest strong improvements over the Gemma 3 baseline, even for translating LB to German (DE), despite the training data not containing any DE. We also explore LuxEmbedder's potential to be used as a quality estimation metric and find strong correlations with other reference-based metrics. However, we call for further research to fully assess the metric's utility and advise using it with caution.",
      "authors": [
        "Nils Rehlinger"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 11:26:32+00:00",
      "link": "https://arxiv.org/pdf/2602.15506v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15504v1",
      "title": "Towards Expectation Detection in Language: A Case Study on Treatment Expectations in Reddit",
      "abstract": "Patients' expectations towards their treatment have a substantial effect on the treatments' success. While primarily studied in clinical settings, online patient platforms like medical subreddits may hold complementary insights: treatment expectations that patients feel unnecessary or uncomfortable to share elsewhere. Despite this, no studies examine what type of expectations users discuss online and how they express them. Presumably this is because expectations have not been studied in natural language processing (NLP) before. Therefore, we introduce the task of Expectation Detection, arguing that expectations are relevant for many applications, including opinion mining and product design. Subsequently, we present a case study for the medical domain, where expectations are particularly crucial to extract. We contribute RedHOTExpect, a corpus of Reddit posts (4.5K posts) to study expectations in this context. We use a large language model (LLM) to silver-label the data and validate its quality manually (label accuracy ~78%). Based on this, we analyze which linguistic patterns characterize expectations and explore what patients expect and why. We find that optimism and proactive framing are more pronounced in posts about physical or treatment-related illnesses compared to mental-health contexts, and that in our dataset, patients mostly discuss benefits rather than negative outcomes. The RedHOTExpect corpus can be obtained from https://www.ims.uni-stuttgart.de/data/RedHOTExpect",
      "authors": [
        "Aswathy Velutharambath",
        "Amelie Wührl"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 11:21:40+00:00",
      "link": "https://arxiv.org/pdf/2602.15504v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15503v1",
      "title": "Approximation Theory for Lipschitz Continuous Transformers",
      "abstract": "Stability and robustness are critical for deploying Transformers in safety-sensitive settings. A principled way to enforce such behavior is to constrain the model's Lipschitz constant. However, approximation-theoretic guarantees for architectures that explicitly preserve Lipschitz continuity have yet to be established. In this work, we bridge this gap by introducing a class of gradient-descent-type in-context Transformers that are Lipschitz-continuous by construction. We realize both MLP and attention blocks as explicit Euler steps of negative gradient flows, ensuring inherent stability without sacrificing expressivity. We prove a universal approximation theorem for this class within a Lipschitz-constrained function space. Crucially, our analysis adopts a measure-theoretic formalism, interpreting Transformers as operators on probability measures, to yield approximation guarantees independent of token count. These results provide a rigorous theoretical foundation for the design of robust, Lipschitz continuous Transformer architectures.",
      "authors": [
        "Takashi Furuya",
        "Davide Murari",
        "Carola-Bibiane Schönlieb"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-17 11:16:52+00:00",
      "link": "https://arxiv.org/pdf/2602.15503v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15488v1",
      "title": "Efficient Approximate Nearest Neighbor Search under Multi-Attribute Range Filter",
      "abstract": "Nearest neighbor search on high-dimensional vectors is fundamental in modern AI and database systems. In many real-world applications, queries involve constraints on multiple numeric attributes, giving rise to range-filtering approximate nearest neighbor search (RFANNS). While there exist RFANNS indexes for single-attribute range predicates, extending them to the multi-attribute setting is nontrivial and often ineffective. In this paper, we propose KHI, an index for multi-attribute RFANNS that combines an attribute-space partitioning tree with HNSW graphs attached to tree nodes. A skew-aware splitting rule bounds the tree height by $O(\\log n)$, and queries are answered by routing through the tree and running greedy search on the HNSW graphs. Experiments on four real-world datasets show that KHI consistently achieves high query throughput while maintaining high recall. Compared with the state-of-the-art RFANNS baseline, KHI improves QPS by $2.46\\times$ on average and up to $16.22\\times$ on the hard dataset, with larger gains for smaller selectivity, larger $k$, and higher predicate cardinality.",
      "authors": [
        "Yuanhang Yu",
        "Dawei Cheng",
        "Ying Zhang",
        "Lu Qin",
        "Wenjie Zhang",
        "Xuemin Lin"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-17 10:52:57+00:00",
      "link": "https://arxiv.org/pdf/2602.15488v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15484v1",
      "title": "Bottleneck Transformer-Based Approach for Improved Automatic STOI Score Prediction",
      "abstract": "In this study, we have presented a novel approach to predict the Short-Time Objective Intelligibility (STOI) metric using a bottleneck transformer architecture. Traditional methods for calculating STOI typically requires clean reference speech, which limits their applicability in the real world. To address this, numerous deep learning-based nonintrusive speech assessment models have garnered significant interest. Many studies have achieved commendable performance, but there is room for further improvement.   We propose the use of bottleneck transformer, incorporating convolution blocks for learning frame-level features and a multi-head self-attention (MHSA) layer to aggregate the information. These components enable the transformer to focus on the key aspects of the input data. Our model has shown higher correlation and lower mean squared error for both seen and unseen scenarios compared to the state-of-the-art model using self-supervised learning (SSL) and spectral features as inputs.",
      "authors": [
        "Amartyaveer",
        "Murali Kadambi",
        "Chandra Mohan Sharma",
        "Anupam Mondal",
        "Prasanta Kumar Ghosh"
      ],
      "primary_category": "eess.AS",
      "categories": [
        "eess.AS",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2026-02-17 10:46:54+00:00",
      "link": "https://arxiv.org/pdf/2602.15484v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15481v1",
      "title": "LLM-as-Judge on a Budget",
      "abstract": "LLM-as-a-judge has emerged as a cornerstone technique for evaluating large language models by leveraging LLM reasoning to score prompt-response pairs. Since LLM judgments are stochastic, practitioners commonly query each pair multiple times to estimate mean scores accurately. This raises a critical challenge: given a fixed computational budget $B$, how to optimally allocate queries across $K$ prompt-response pairs to minimize estimation error? % We present a principled variance-adaptive approach leveraging multi-armed bandit theory and concentration inequalities. Our method dynamically allocates queries based on estimated score variances, concentrating resources where uncertainty is highest. Further, our algorithm is shown to achieve a worst-case score-estimation error of $\\tilde{O}\\left(\\sqrt{\\frac{\\sum_{i=1}^K σ_i^2}{B}}\\right)$, $σ_i^2$ being the unknown score variance for pair $i \\in [K]$ with near-optimal budget allocation. % Experiments on \\emph{Summarize-From-Feedback} and \\emph{HelpSteer2} demonstrate that our method significantly outperforms uniform allocation, reducing worst-case estimation error while maintaining identical budgets. Our work establishes a theoretical foundation for efficient LLM evaluation with practical implications for AI safety, model alignment, and automated assessment at scale.",
      "authors": [
        "Aadirupa Saha",
        "Aniket Wagde",
        "Branislav Kveton"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 10:35:41+00:00",
      "link": "https://arxiv.org/pdf/2602.15481v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15478v1",
      "title": "Evaluating Federated Learning for Cross-Country Mood Inference from Smartphone Sensing Data",
      "abstract": "Mood instability is a key behavioral indicator of mental health, yet traditional assessments rely on infrequent and retrospective reports that fail to capture its continuous nature. Smartphone-based mobile sensing enables passive, in-the-wild mood inference from everyday behaviors; however, deploying such systems at scale remains challenging due to privacy constraints, uneven sensing availability, and substantial variability in behavioral patterns.   In this work, we study mood inference using smartphone sensing data in a cross-country federated learning setting, where each country participates as an independent client while retaining local data. We introduce FedFAP, a feature-aware personalized federated framework designed to accommodate heterogeneous sensing modalities across regions. Evaluations across geographically and culturally diverse populations show that FedFAP achieves an AUROC of 0.744, outperforming both centralized approaches and existing personalized federated baselines. Beyond inference, our results offer design insights for mood-aware systems, demonstrating how population-aware personalization and privacy-preserving learning can enable scalable and mood-aware mobile sensing technologies.",
      "authors": [
        "Sharmad Kalpande",
        "Saurabh Shirke",
        "Haroon R. Lone"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 10:34:03+00:00",
      "link": "https://arxiv.org/pdf/2602.15478v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15476v1",
      "title": "How to Detect Information Voids Using Longitudinal Data from Social Media and Web Searches",
      "abstract": "The model of the attention economy, where content producers compete for the attention of users, relies on two key forces: information supply and demand. This study leverages the feedback loop between these forces to develop a method for detecting and quantifying information voids, i.e., periods in which little or no reliable information is available on a given topic. Using a case study on COVID-19 vaccines rollout in six European countries, and drawing on data from multiple platforms including Facebook, Google, Twitter, Wikipedia, and online news outlets, we examine how information voids emerge, persist and correlate with a decline in the proportion of high-quality information circulating online. By conceptualising information voids as a specific regime of information spreading, we also quantify their counterpart, information overabundance, which constitute a central component of the current definition of infodemic. We show that information voids are associated with a higher prevalence of misinformation, thus representing problematic hotspots in which individuals are more likely to be misled by low-quality online content. Overall, our findings provide empirical support for the inclusion of information voids in mechanistic explanations of misinformation emergence.",
      "authors": [
        "Irene Scalco",
        "Francesco Gesualdo",
        "Roy Cerqueti",
        "Matteo Cinelli"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "physics.soc-ph"
      ],
      "published": "2026-02-17 10:30:05+00:00",
      "link": "https://arxiv.org/pdf/2602.15476v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15473v1",
      "title": "POP: Prior-fitted Optimizer Policies",
      "abstract": "Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their performance relies on carefully tuned learning rates, momentum, and gradient accumulation. To address these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes conditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimization functions of various complexity, where it consistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimization, and a recent meta-learned competitor under matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning.",
      "authors": [
        "Jan Kobiolka",
        "Christian Frey",
        "Gresa Shala",
        "Arlind Kadra",
        "Erind Bedalli",
        "Josif Grabocka"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 10:27:07+00:00",
      "link": "https://arxiv.org/pdf/2602.15473v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15472v1",
      "title": "Fluids You Can Trust: Property-Preserving Operator Learning for Incompressible Flows",
      "abstract": "We present a novel property-preserving kernel-based operator learning method for incompressible flows governed by the incompressible Navier-Stokes equations. Traditional numerical solvers incur significant computational costs to respect incompressibility. Operator learning offers efficient surrogate models, but current neural operators fail to exactly enforce physical properties such as incompressibility, periodicity, and turbulence. Our method maps input functions to expansion coefficients of output functions in a property-preserving kernel basis, ensuring that predicted velocity fields analytically and simultaneously preserve the aforementioned physical properties. We evaluate the method on challenging 2D and 3D, laminar and turbulent, incompressible flow problems. Our method achieves up to six orders of magnitude lower relative $\\ell_2$ errors upon generalization and trains up to five orders of magnitude faster compared to neural operators. Moreover, while our method enforces incompressibility analytically, neural operators exhibit very large deviations. Our results show that our method provides an accurate and efficient surrogate for incompressible flows.",
      "authors": [
        "Ramansh Sharma",
        "Matthew Lowery",
        "Houman Owhadi",
        "Varun Shankar"
      ],
      "primary_category": "physics.flu-dyn",
      "categories": [
        "physics.flu-dyn",
        "cs.LG"
      ],
      "published": "2026-02-17 10:20:46+00:00",
      "link": "https://arxiv.org/pdf/2602.15472v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15457v1",
      "title": "Benchmarking IoT Time-Series AD with Event-Level Augmentations",
      "abstract": "Anomaly detection (AD) for safety-critical IoT time series should be judged at the event level: reliability and earliness under realistic perturbations. Yet many studies still emphasize point-level results on curated base datasets, limiting value for model selection in practice. We introduce an evaluation protocol with unified event-level augmentations that simulate real-world issues: calibrated sensor dropout, linear and log drift, additive noise, and window shifts. We also perform sensor-level probing via mask-as-missing zeroing with per-channel influence estimation to support root-cause analysis. We evaluate 14 representative models on five public anomaly datasets (SWaT, WADI, SMD, SKAB, TEP) and two industrial datasets (steam turbine, nuclear turbogenerator) using unified splits and event aggregation. There is no universal winner: graph-structured models transfer best under dropout and long events (e.g., on SWaT under additive noise F1 drops 0.804->0.677 for a graph autoencoder, 0.759->0.680 for a graph-attention variant, and 0.762->0.756 for a hybrid graph attention model); density/flow models work well on clean stationary plants but can be fragile to monotone drift; spectral CNNs lead when periodicity is strong; reconstruction autoencoders become competitive after basic sensor vetting; predictive/hybrid dynamics help when faults break temporal dependencies but remain window-sensitive. The protocol also informs design choices: on SWaT under log drift, replacing normalizing flows with Gaussian density reduces high-stress F1 from ~0.75 to ~0.57, and fixing a learned DAG gives a small clean-set gain (~0.5-1.0 points) but increases drift sensitivity by ~8x.",
      "authors": [
        "Dmitry Zhevnenko",
        "Ilya Makarov",
        "Aleksandr Kovalenko",
        "Fedor Meshchaninov",
        "Anton Kozhukhov",
        "Vladislav Travnikov",
        "Makar Ippolitov",
        "Kirill Yashunin",
        "Iurii Katser"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 09:45:44+00:00",
      "link": "https://arxiv.org/pdf/2602.15457v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15920v1",
      "title": "Including Node Textual Metadata in Laplacian-constrained Gaussian Graphical Models",
      "abstract": "This paper addresses graph learning in Gaussian Graphical Models (GGMs). In this context, data matrices often come with auxiliary metadata (e.g., textual descriptions associated with each node) that is usually ignored in traditional graph estimation processes. To fill this gap, we propose a graph learning approach based on Laplacian-constrained GGMs that jointly leverages the node signals and such metadata. The resulting formulation yields an optimization problem, for which we develop an efficient majorization-minimization (MM) algorithm with closed-form updates at each iteration. Experimental results on a real-world financial dataset demonstrate that the proposed method significantly improves graph clustering performance compared to state-of-the-art approaches that use either signals or metadata alone, thus illustrating the interest of fusing both sources of information.",
      "authors": [
        "Jianhua Wang",
        "Killian Cressant",
        "Pedro Braconnot Velloso",
        "Arnaud Breloy"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2026-02-17 09:43:47+00:00",
      "link": "https://arxiv.org/pdf/2602.15920v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15423v1",
      "title": "GaiaFlow: Semantic-Guided Diffusion Tuning for Carbon-Frugal Search",
      "abstract": "As the burgeoning power requirements of sophisticated neural architectures escalate, the information retrieval community has recognized ecological sustainability as a pivotal priority that necessitates a fundamental paradigm shift in model design. While contemporary neural rankers have attained unprecedented accuracy, the substantial environmental externalities associated with their computational intensity often remain overlooked in large-scale deployments. We present GaiaFlow, an innovative framework engineered to facilitate carbon-frugal search by operationalizing semantic-guided diffusion tuning. Our methodology orchestrates the convergence of retrieval-guided Langevin dynamics and a hardware-independent performance modeling strategy to optimize the trade-off between search precision and environmental preservation. By incorporating adaptive early exit protocols and precision-aware quantized inference, the proposed architecture significantly mitigates operational carbon footprints while maintaining robust retrieval quality across heterogeneous computing infrastructures. Extensive experimental evaluations demonstrate that GaiaFlow achieves a superior equilibrium between effectiveness and energy efficiency, offering a scalable and sustainable pathway for next-generation neural search systems.",
      "authors": [
        "Rong Fu",
        "Wenxin Zhang",
        "Jia Yee Tan",
        "Chunlei Meng",
        "Shuo Yin",
        "Xiaowen Ma",
        "Wangyu Wu",
        "Muge Qi",
        "Guangzhen Yao",
        "Zhaolu Kang",
        "Zeli Su",
        "Simon Fong"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "published": "2026-02-17 08:35:11+00:00",
      "link": "https://arxiv.org/pdf/2602.15423v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15413v1",
      "title": "StatCounter: A Longitudinal Study of a Portable Scholarly Metric Display",
      "abstract": "This study explores a handheld, battery-operated e-ink device displaying Google Scholar citation statistics. The StatCounter places academic metrics into the flow of daily life rather than a desktop context. The work draws on a first-person, longitudinal auto-ethnographic inquiry examining how constant access to scholarly metrics influences motivation, attention, reflection, and emotional responses across work and non-work settings. The ambient proximity and pervasive availability of scholarly metrics invites frequent micro-checks, short reflective pauses, but also introduces moments of second-guessing when numbers drop or stagnate. Carrying the device prompts new narratives about academic identity, including a sense of companionship during travel and periods away from the office. Over time, the presence of the device turns metrics from an occasional reference into an ambient background of scholarly life. The study contributes insight into how situated, embodied access to academic metrics reshapes their meaning, and frames opportunities for designing tools that engage with scholarly evaluation in reflective ways.",
      "authors": [
        "Jonas Oppenlaender"
      ],
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.DL"
      ],
      "published": "2026-02-17 08:13:55+00:00",
      "link": "https://arxiv.org/pdf/2602.15413v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15405v1",
      "title": "Joint Enhancement and Classification using Coupled Diffusion Models of Signals and Logits",
      "abstract": "Robust classification in noisy environments remains a fundamental challenge in machine learning. Standard approaches typically treat signal enhancement and classification as separate, sequential stages: first enhancing the signal and then applying a classifier. This approach fails to leverage the semantic information in the classifier's output during denoising. In this work, we propose a general, domain-agnostic framework that integrates two interacting diffusion models: one operating on the input signal and the other on the classifier's output logits, without requiring any retraining or fine-tuning of the classifier. This coupled formulation enables mutual guidance, where the enhancing signal refines the class estimation and, conversely, the evolving class logits guide the signal reconstruction towards discriminative regions of the manifold. We introduce three strategies to effectively model the joint distribution of the input and the logit. We evaluated our joint enhancement method for image classification and automatic speech recognition. The proposed framework surpasses traditional sequential enhancement baselines, delivering robust and flexible improvements in classification accuracy under diverse noise conditions.",
      "authors": [
        "Gilad Nurko",
        "Roi Benita",
        "Yehoshua Dissen",
        "Tomohiro Nakatani",
        "Marc Delcroix",
        "Shoko Araki",
        "Joseph Keshet"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 07:25:41+00:00",
      "link": "https://arxiv.org/pdf/2602.15405v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15391v1",
      "title": "Improving LLM Reliability through Hybrid Abstention and Adaptive Detection",
      "abstract": "Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.",
      "authors": [
        "Ankit Sharma",
        "Nachiket Tapas",
        "Jyotiprakash Patra"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-17 07:00:09+00:00",
      "link": "https://arxiv.org/pdf/2602.15391v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15380v1",
      "title": "Fractional-Order Federated Learning",
      "abstract": "Federated learning (FL) allows remote clients to train a global model collaboratively while protecting client privacy. Despite its privacy-preserving benefits, FL has significant drawbacks, including slow convergence, high communication cost, and non-independent-and-identically-distributed (non-IID) data. In this work, we present a novel FedAvg variation called Fractional-Order Federated Averaging (FOFedAvg), which incorporates Fractional-Order Stochastic Gradient Descent (FOSGD) to capture long-range relationships and deeper historical information. By introducing memory-aware fractional-order updates, FOFedAvg improves communication efficiency and accelerates convergence while mitigating instability caused by heterogeneous, non-IID client data. We compare FOFedAvg against a broad set of established federated optimization algorithms on benchmark datasets including MNIST, FEMNIST, CIFAR-10, CIFAR-100, EMNIST, the Cleveland heart disease dataset, Sent140, PneumoniaMNIST, and Edge-IIoTset. Across a range of non-IID partitioning schemes, FOFedAvg is competitive with, and often outperforms, these baselines in terms of test performance and convergence speed. On the theoretical side, we prove that FOFedAvg converges to a stationary point under standard smoothness and bounded-variance assumptions for fractional order $0<α\\le 1$. Together, these results show that fractional-order, memory-aware updates can substantially improve the robustness and effectiveness of federated learning, offering a practical path toward distributed training on heterogeneous data.",
      "authors": [
        "Mohammad Partohaghighi",
        "Roummel Marcia",
        "YangQuan Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 06:25:23+00:00",
      "link": "https://arxiv.org/pdf/2602.15380v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15379v1",
      "title": "FlashMem: Supporting Modern DNN Workloads on Mobile with GPU Memory Hierarchy Optimizations",
      "abstract": "The increasing size and complexity of modern deep neural networks (DNNs) pose significant challenges for on-device inference on mobile GPUs, with limited memory and computational resources. Existing DNN acceleration frameworks primarily deploy a weight preloading strategy, where all model parameters are loaded into memory before execution on mobile GPUs. We posit that this approach is not adequate for modern DNN workloads that comprise very large model(s) and possibly execution of several distinct models in succession. In this work, we introduce FlashMem, a memory streaming framework designed to efficiently execute large-scale modern DNNs and multi-DNN workloads while minimizing memory consumption and reducing inference latency. Instead of fully preloading weights, FlashMem statically determines model loading schedules and dynamically streams them on demand, leveraging 2.5D texture memory to minimize data transformations and improve execution efficiency. Experimental results on 11 models demonstrate that FlashMem achieves 2.0x to 8.4x memory reduction and 1.7x to 75.0x speedup compared to existing frameworks, enabling efficient execution of large-scale models and multi-DNN support on resource-constrained mobile GPUs.",
      "authors": [
        "Zhihao Shu",
        "Md Musfiqur Rahman Sanim",
        "Hangyu Zheng",
        "Kunxiong Zhu",
        "Miao Yin",
        "Gagan Agrawal",
        "Wei Niu"
      ],
      "primary_category": "cs.DC",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "published": "2026-02-17 06:23:40+00:00",
      "link": "https://arxiv.org/pdf/2602.15379v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15378v1",
      "title": "Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language",
      "abstract": "Can large language models converse in languages virtually absent from their training data? We investigate this question through a case study on Tulu, a Dravidian language with over 2 million speakers but minimal digital presence. Rather than fine-tuning an LLM, we examine whether structured prompts alone can elicit basic conversational ability under controlled prompting. We systematically tackle various challenges posed by absence of training data for Tulu by combining explicit grammar documentation, negative constraints to suppress high-probability tokens from related languages, romanization standardization, and quality-controlled synthetic data generation via self-play. Evaluated on a manually curated held-out set across three LLMs (Gemini 2.0 Flash, GPT-4o, Llama 3.1 70B) and validated by native speakers, our approach reduces vocabulary contamination from 80% to 5% while achieving 85% grammatical accuracy. Cross-model analysis reveals that negative constraints provide consistent improvements (12--18 percentage points), while grammar documentation effects vary by model architecture (8--22 points).",
      "authors": [
        "Prathamesh Devadiga",
        "Paras Chopra"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 06:20:09+00:00",
      "link": "https://arxiv.org/pdf/2602.15378v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15373v2",
      "title": "Far Out: Evaluating Language Models on Slang in Australian and Indian English",
      "abstract": "Language models exhibit systematic performance gaps when processing text in non-standard language varieties, yet their ability to comprehend variety-specific slang remains underexplored for several languages. We present a comprehensive evaluation of slang awareness in Indian English (en-IN) and Australian English (en-AU) across seven state-of-the-art language models. We construct two complementary datasets: WEB, containing 377 web-sourced usage examples from Urban Dictionary, and GEN, featuring 1,492 synthetically generated usages of these slang terms, across diverse scenarios. We assess language models on three tasks: target word prediction (TWP), guided target word prediction (TWP$^*$) and target word selection (TWS). Our results reveal four key findings: (1) Higher average model performance TWS versus TWP and TWP$^*$, with average accuracy score increasing from 0.03 to 0.49 respectively (2) Stronger average model performance on WEB versus GEN datasets, with average similarity score increasing by 0.03 and 0.05 across TWP and TWP$^*$ tasks respectively (3) en-IN tasks outperform en-AU when averaged across all models and datasets, with TWS demonstrating the largest disparity, increasing average accuracy from 0.44 to 0.54. These findings underscore fundamental asymmetries between generative and discriminative competencies for variety-specific language, particularly in the context of slang expressions despite being in a technologically rich language such as English.",
      "authors": [
        "Deniz Kaya Dilsiz",
        "Dipankar Srirag",
        "Aditya Joshi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-17 05:59:20+00:00",
      "link": "https://arxiv.org/pdf/2602.15373v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15360v1",
      "title": "Crane: An Accurate and Scalable Neural Sketch for Graph Stream Summarization",
      "abstract": "Graph streams are rapidly evolving sequences of edges that convey continuously changing relationships among entities, playing a crucial role in domains such as networking, finance, and cybersecurity. Their massive scale and high dynamism make obtaining accurate statistics challenging with limited memory constraints. Traditional methods summarize graph streams through hand-crafted sketches, while recent studies have begun to replace these sketches with neural counterparts to improve adaptability and accuracy. However, this shift faces a major challenge: under limited memory, dominant frequent items tend to overshadow rare ones, hindering the neural network's ability to recover accurate statistics. To address this, we propose Crane, a hierarchical neural sketch architecture for graph stream summarization. Crane uses a hierarchical carry mechanism that automatically elevates frequent items to higher memory layers, reducing interference between frequent and infrequent items within the same layer. To better accommodate real-world deployment, Crane further adopts an adaptive memory expansion strategy that dynamically adds new layers once the occupancy of the top layer exceeds a threshold, enabling scalability across diverse data magnitudes. Extensive experiments on various datasets ranging from 20K to 60M edges demonstrate that Crane reduces estimation error by roughly 10x compared to state-of-the-art methods.",
      "authors": [
        "Boyan Wang",
        "Zhuochen Fan",
        "Dayu Wang",
        "Fangcheng Fu",
        "Zeyu Luan",
        "Lei Zou",
        "Qing Li",
        "Tong Yang"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-17 04:59:10+00:00",
      "link": "https://arxiv.org/pdf/2602.15360v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15353v1",
      "title": "NeuroSymActive: Differentiable Neural-Symbolic Reasoning with Active Exploration for Knowledge Graph Question Answering",
      "abstract": "Large pretrained language models and neural reasoning systems have advanced many natural language tasks, yet they remain challenged by knowledge-intensive queries that require precise, structured multi-hop inference. Knowledge graphs provide a compact symbolic substrate for factual grounding, but integrating graph structure with neural models is nontrivial: naively embedding graph facts into prompts leads to inefficiency and fragility, while purely symbolic or search-heavy approaches can be costly in retrievals and lack gradient-based refinement. We introduce NeuroSymActive, a modular framework that combines a differentiable neural-symbolic reasoning layer with an active, value-guided exploration controller for Knowledge Graph Question Answering. The method couples soft-unification style symbolic modules with a neural path evaluator and a Monte-Carlo style exploration policy that prioritizes high-value path expansions. Empirical results on standard KGQA benchmarks show that NeuroSymActive attains strong answer accuracy while reducing the number of expensive graph lookups and model calls compared to common retrieval-augmented baselines.",
      "authors": [
        "Rong Fu",
        "Yang Li",
        "Zeyu Zhang",
        "Jiekai Wu",
        "Yaohua Liu",
        "Shuaishuai Cao",
        "Yangchen Zeng",
        "Yuhang Zhang",
        "Xiaojing Du",
        "Chuang Zhao",
        "Kangning Cui",
        "Simon Fong"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-17 04:47:29+00:00",
      "link": "https://arxiv.org/pdf/2602.15353v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15915v1",
      "title": "MaS-VQA: A Mask-and-Select Framework for Knowledge-Based Visual Question Answering",
      "abstract": "Knowledge-based Visual Question Answering (KB-VQA) requires models to answer questions by integrating visual information with external knowledge. However, retrieved knowledge is often noisy, partially irrelevant, or misaligned with the visual content, while internal model knowledge is difficult to control and interpret. Naive aggregation of these sources limits reasoning effectiveness and reduces answer accuracy. To address this, we propose MaS-VQA, a selection-driven framework that tightly couples explicit knowledge filtering with implicit knowledge reasoning. MaS-VQA first retrieves candidate passages and applies a Mask-and-Select mechanism to jointly prune irrelevant image regions and weakly relevant knowledge fragments, producing compact, high-signal multimodal knowledge . This filtered knowledge then guides the activation of internal knowledge in a constrained semantic space, enabling complementary co-modeling of explicit and implicit knowledge for robust answer prediction. Experiments on Encyclopedic-VQA and InfoSeek demonstrate consistent performance gains across multiple MLLM backbones, and ablations verify that the selection mechanism effectively reduces noise and enhances knowledge utilization.",
      "authors": [
        "Xianwei Mao",
        "Kai Ye",
        "Sheng Zhou",
        "Nan Zhang",
        "Haikuan Huang",
        "Bin Li",
        "Jiajun Bu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-02-17 04:45:05+00:00",
      "link": "https://arxiv.org/pdf/2602.15915v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15338v1",
      "title": "Discovering Implicit Large Language Model Alignment Objectives",
      "abstract": "Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of \"unknown unknowns\", or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework's robustness. Experiments with popular open-source reward models show that the framework consistently captures > 90% of reward behavior, a finding further corroborated by human evaluation. Additionally, a case study on alignment with an open-source reward model reveals that Obj-Disco can successfully identify latent misaligned incentives that emerge alongside intended behaviors. Our work provides a crucial tool for uncovering the implicit objectives in LLM alignment, paving the way for more transparent and safer AI development.",
      "authors": [
        "Edward Chen",
        "Sanmi Koyejo",
        "Carlos Guestrin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-02-17 03:58:55+00:00",
      "link": "https://arxiv.org/pdf/2602.15338v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15337v1",
      "title": "FedPSA: Modeling Behavioral Staleness in Asynchronous Federated Learning",
      "abstract": "Asynchronous Federated Learning (AFL) has emerged as a significant research area in recent years. By not waiting for slower clients and executing the training process concurrently, it achieves faster training speed compared to traditional federated learning. However, due to the staleness introduced by the asynchronous process, its performance may degrade in some scenarios. Existing methods often use the round difference between the current model and the global model as the sole measure of staleness, which is coarse-grained and lacks observation of the model itself, thereby limiting the performance ceiling of asynchronous methods. In this paper, we propose FedPSA (Parameter Sensitivity-based Asynchronous Federated Learning), a more fine-grained AFL framework that leverages parameter sensitivity to measure model obsolescence and establishes a dynamic momentum queue to assess the current training phase in real time, thereby adjusting the tolerance for outdated information dynamically. Extensive experiments on multiple datasets and comparisons with various methods demonstrate the superior performance of FedPSA, achieving up to 6.37\\% improvement over baseline methods and 1.93\\% over the current state-of-the-art method.",
      "authors": [
        "Chaoyi Lu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-17 03:57:07+00:00",
      "link": "https://arxiv.org/pdf/2602.15337v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15336v1",
      "title": "Human-AI Interaction: Evaluating LLM Reasoning on Digital Logic Circuit included Graph Problems, in terms of creativity in design and analysis",
      "abstract": "Large Language Models (LLMs) are increasingly used by undergraduate students as on-demand tutors, yet their reliability on circuit- and diagram-based digital logic problems remains unclear. We present a human- AI study evaluating three widely used LLMs (GPT, Gemini, and Claude) on 10 undergraduate-level digital logic questions spanning non-standard counters, JK-based state transitions, timing diagrams, frequency division, and finite-state machines. Twenty-four students performed pairwise model comparisons, providing per-question judgments on (i) preferred model, (ii) perceived correctness, (iii) consistency, (iv) verbosity, and (v) confidence, along with global ratings of overall model quality, satisfaction across multiple dimensions (e.g., accuracy and clarity), and perceived mental effort required to verify answers. To benchmark technical validity, we applied an independent judge-based evaluation against official solutions for all ten questions, using strict correctness criteria. Results reveal a consistent gap between perceived helpfulness and formal correctness: for the most sequentially demanding problems (Q1- Q7), none of the evaluated LLMs matched the official answers, despite producing confident, well-structured explanations that students often rated favorably. Error analysis indicates that models frequently default to canonical textbook templates (e.g., standard ripple counters) and struggle to translate circuit structure into exact state evolution and timing behavior. These findings suggest that, without verification scaffolds, LLMs may be unreliable for core digital logic topics and can inadvertently reinforce misconceptions in undergraduate instruction.",
      "authors": [
        "Yogeswar Reddy Thota",
        "Setareh Rafatirad",
        "Homayoun Houman",
        "Tooraj Nikoubin"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR"
      ],
      "published": "2026-02-17 03:47:57+00:00",
      "link": "https://arxiv.org/pdf/2602.15336v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15332v1",
      "title": "Directional Reasoning Trajectory Change (DRTC): Identifying Critical Trace Segments in Reasoning Models",
      "abstract": "Understanding how language models carry out long-horizon reasoning remains an open challenge. Existing interpretability methods often highlight tokens or spans correlated with an answer, but they rarely reveal where the model makes consequential reasoning turns, which earlier context causally triggers those turns, or whether the highlighted text actually steers the reasoning process. We introduce Directional Reasoning Trajectory Change (DRTC), a process-causal framework for interpreting long-form reasoning from a single on-policy rollout. DRTC detects pivot decision points using uncertainty and distribution-shift signals, then applies receiver-side interventions that preserve the realized rollout without resampling the continuation while blocking information flow from selected earlier chunks only at a pivot. It measures whether each intervention redirects the direction of the model's log-probability trajectory relative to the realized rollout direction, producing a signed per-chunk attribution score. We also compute turning-angle curvature changes on raw logits as a complementary diagnostic and introduce curvature signatures to summarize shared intervention-response geometry. Empirically, directional influence is sharply concentrated across four reasoning models (per-example |DRTC| shares yield Gini 0.50 to 0.58 and top-5 percent mass 0.23 to 0.28), and learned pivots induce stronger intervention magnitudes than matched random spans. In a scaling study on 500 MATH problems with R1-Distill-Qwen-1.5B, learned spans outperform matched random spans (median delta = 0.409, 355 of 500 positive; sign test p = 2.3e-21). Overall, DRTC provides a causally grounded, trajectory-level view of how specific context elements steer reasoning under on-policy dynamics.",
      "authors": [
        "Waldemar Chang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-17 03:38:16+00:00",
      "link": "https://arxiv.org/pdf/2602.15332v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15330v1",
      "title": "A Scalable Curiosity-Driven Game-Theoretic Framework for Long-Tail Multi-Label Learning in Data Mining",
      "abstract": "The long-tail distribution, where a few head labels dominate while rare tail labels abound, poses a persistent challenge for large-scale Multi-Label Classification (MLC) in real-world data mining applications. Existing resampling and reweighting strategies often disrupt inter-label dependencies or require brittle hyperparameter tuning, especially as the label space expands to tens of thousands of labels. To address this issue, we propose Curiosity-Driven Game-Theoretic Multi-Label Learning (CD-GTMLL), a scalable cooperative framework that recasts long-tail MLC as a multi-player game - each sub-predictor (\"player\") specializes in a partition of the label space, collaborating to maximize global accuracy while pursuing intrinsic curiosity rewards based on tail label rarity and inter-player disagreement. This mechanism adaptively injects learning signals into under-represented tail labels without manual balancing or tuning. We further provide a theoretical analysis showing that our CD-GTMLL converges to a tail-aware equilibrium and formally links the optimization dynamics to improvements in the Rare-F1 metric. Extensive experiments across 7 benchmarks, including extreme multi-label classification datasets with 30,000+ labels, demonstrate that CD-GTMLL consistently surpasses state-of-the-art methods, with gains up to +1.6% P@3 on Wiki10-31K. Ablation studies further confirm the contributions of both game-theoretic cooperation and curiosity-driven exploration to robust tail performance. By integrating game theory with curiosity mechanisms, CD-GTMLL not only enhances model efficiency in resource-constrained environments but also paves the way for more adaptive learning in imbalanced data scenarios across industries like e-commerce and healthcare.",
      "authors": [
        "Jing Yang",
        "Keze Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-17 03:33:23+00:00",
      "link": "https://arxiv.org/pdf/2602.15330v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15327v1",
      "title": "Prescriptive Scaling Reveals the Evolution of Language Model Capabilities",
      "abstract": "For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.",
      "authors": [
        "Hanlin Zhang",
        "Jikai Jin",
        "Vasilis Syrgkanis",
        "Sham Kakade"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "published": "2026-02-17 03:13:51+00:00",
      "link": "https://arxiv.org/pdf/2602.15327v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15322v1",
      "title": "On Surprising Effectiveness of Masking Updates in Adaptive Optimizers",
      "abstract": "Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively.",
      "authors": [
        "Taejong Joo",
        "Wenhan Xia",
        "Cheolmin Kim",
        "Ming Zhang",
        "Eugene Ie"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-17 02:57:12+00:00",
      "link": "https://arxiv.org/pdf/2602.15322v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15313v1",
      "title": "Mnemis: Dual-Route Retrieval on Hierarchical Graphs for Long-Term LLM Memory",
      "abstract": "AI Memory, specifically how models organizes and retrieves historical messages, becomes increasingly valuable to Large Language Models (LLMs), yet existing methods (RAG and Graph-RAG) primarily retrieve memory through similarity-based mechanisms. While efficient, such System-1-style retrieval struggles with scenarios that require global reasoning or comprehensive coverage of all relevant information. In this work, We propose Mnemis, a novel memory framework that integrates System-1 similarity search with a complementary System-2 mechanism, termed Global Selection. Mnemis organizes memory into a base graph for similarity retrieval and a hierarchical graph that enables top-down, deliberate traversal over semantic hierarchies. By combining the complementary strength from both retrieval routes, Mnemis retrieves memory items that are both semantically and structurally relevant. Mnemis achieves state-of-the-art performance across all compared methods on long-term memory benchmarks, scoring 93.9 on LoCoMo and 91.6 on LongMemEval-S using GPT-4.1-mini.",
      "authors": [
        "Zihao Tang",
        "Xin Yu",
        "Ziyu Xiao",
        "Zengxuan Wen",
        "Zelin Li",
        "Jiaxi Zhou",
        "Hualei Wang",
        "Haohua Wang",
        "Haizhen Huang",
        "Weiwei Deng",
        "Feng Sun",
        "Qi Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-17 02:44:03+00:00",
      "link": "https://arxiv.org/pdf/2602.15313v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15312v1",
      "title": "Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement",
      "abstract": "Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.",
      "authors": [
        "Stephan Ludwig",
        "Peter J. Danaher",
        "Xiaohao Yang",
        "Yu-Ting Lin",
        "Ehsan Abedin",
        "Dhruv Grewal",
        "Lan Du"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "econ.EM"
      ],
      "published": "2026-02-17 02:33:51+00:00",
      "link": "https://arxiv.org/pdf/2602.15312v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15306v1",
      "title": "Sparse Additive Model Pruning for Order-Based Causal Structure Learning",
      "abstract": "Causal structure learning, also known as causal discovery, aims to estimate causal relationships between variables as a form of a causal directed acyclic graph (DAG) from observational data. One of the major frameworks is the order-based approach that first estimates a topological order of the underlying DAG and then prunes spurious edges from the fully-connected DAG induced by the estimated topological order. Previous studies often focus on the former ordering step because it can dramatically reduce the search space of DAGs. In practice, the latter pruning step is equally crucial for ensuring both computational efficiency and estimation accuracy. Most existing methods employ a pruning technique based on generalized additive models and hypothesis testing, commonly known as CAM-pruning. However, this approach can be a computational bottleneck as it requires repeatedly fitting additive models for all variables. Furthermore, it may harm estimation quality due to multiple testing. To address these issues, we introduce a new pruning method based on sparse additive models, which enables direct pruning of redundant edges without relying on hypothesis testing. We propose an efficient algorithm for learning sparse additive models by combining the randomized tree embedding technique with group-wise sparse regression. Experimental results on both synthetic and real datasets demonstrated that our method is significantly faster than existing pruning methods while maintaining comparable or superior accuracy.",
      "authors": [
        "Kentaro Kanamori",
        "Hirofumi Suzuki",
        "Takuya Takagi"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-17 02:06:42+00:00",
      "link": "https://arxiv.org/pdf/2602.15306v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15304v1",
      "title": "Hybrid Federated and Split Learning for Privacy Preserving Clinical Prediction and Treatment Optimization",
      "abstract": "Collaborative clinical decision support is often constrained by governance and privacy rules that prevent pooling patient-level records across institutions. We present a hybrid privacy-preserving framework that combines Federated Learning (FL) and Split Learning (SL) to support decision-oriented healthcare modeling without raw-data sharing. The approach keeps feature-extraction trunks on clients while hosting prediction heads on a coordinating server, enabling shared representation learning and exposing an explicit collaboration boundary where privacy controls can be applied. Rather than assuming distributed training is inherently private, we audit leakage empirically using membership inference on cut-layer representations and study lightweight defenses based on activation clipping and additive Gaussian noise. We evaluate across three public clinical datasets under non-IID client partitions using a unified pipeline and assess performance jointly along four deployment-relevant axes: factual predictive utility, uplift-based ranking under capacity constraints, audited privacy leakage, and communication overhead. Results show that hybrid FL-SL variants achieve competitive predictive performance and decision-facing prioritization behavior relative to standalone FL or SL, while providing a tunable privacy-utility trade-off that can reduce audited leakage without requiring raw-data sharing. Overall, the work positions hybrid FL-SL as a practical design space for privacy-preserving healthcare decision support where utility, leakage risk, and deployment cost must be balanced explicitly.",
      "authors": [
        "Farzana Akter",
        "Rakib Hossain",
        "Deb Kanna Roy Toushi",
        "Mahmood Menon Khan",
        "Sultana Amin",
        "Lisan Al Amin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-17 01:57:27+00:00",
      "link": "https://arxiv.org/pdf/2602.15304v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15303v1",
      "title": "On the Entropy of General Mixture Distributions",
      "abstract": "Mixture distributions are a workhorse model for multimodal data in information theory, signal processing, and machine learning. Yet even when each component density is simple, the differential entropy of the mixture is notoriously hard to compute because the mixture couples a logarithm with a sum. This paper develops a deterministic, closed-form toolkit for bounding and accurately approximating mixture entropy directly from component parameters. Our starting point is an information-theoretic channel viewpoint: the latent mixture label plays the role of an input, and the observation is the output. This viewpoint separates mixture entropy into an average within-component uncertainty plus an overlap term that quantifies how much the observation reveals about the hidden label. We then bound and approximate this overlap term using pairwise overlap integrals between component densities, yielding explicit expressions whenever these overlaps admit a closed form. A simple, family-dependent offset corrects the systematic bias of the Jensen overlap bound and is calibrated to be exact in the two limiting regimes of complete overlap and near-perfect separation. A final clipping step guarantees that the estimate always respects universal information-theoretic bounds. Closed-form specializations are provided for Gaussian, factorized Laplacian, uniform, and hybrid mixtures, and numerical experiments validate the resulting bounds and approximations across separation, dimension, number of components, and correlated covariances.",
      "authors": [
        "Namyoon Lee"
      ],
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT",
        "math.ST"
      ],
      "published": "2026-02-17 01:55:01+00:00",
      "link": "https://arxiv.org/pdf/2602.15303v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15283v1",
      "title": "Complex-Valued Unitary Representations as Classification Heads for Improved Uncertainty Quantification in Deep Neural Networks",
      "abstract": "Modern deep neural networks achieve high predictive accuracy but remain poorly calibrated: their confidence scores do not reliably reflect the true probability of correctness. We propose a quantum-inspired classification head architecture that projects backbone features into a complex-valued Hilbert space and evolves them under a learned unitary transformation parameterised via the Cayley map. Through a controlled hybrid experimental design - training a single shared backbone and comparing lightweight interchangeable heads - we isolate the effect of complex-valued unitary representations on calibration. Our ablation study on CIFAR-10 reveals that the unitary magnitude head (complex features evolved under a Cayley unitary, read out via magnitude and softmax) achieves an Expected Calibration Error (ECE) of 0.0146, representing a 2.4x improvement over a standard softmax head (0.0355) and a 3.5x improvement over temperature scaling (0.0510). Surprisingly, replacing the softmax readout with a Born rule measurement layer - the quantum-mechanically motivated approach - degrades calibration to an ECE of 0.0819. On the CIFAR-10H human-uncertainty benchmark, the wave function head achieves the lowest KL-divergence (0.336) to human soft labels among all compared methods, indicating that complex-valued representations better capture the structure of human perceptual ambiguity. We provide theoretical analysis connecting norm-preserving unitary dynamics to calibration through feature-space geometry, report negative results on out-of-distribution detection and sentiment analysis to delineate the method's scope, and discuss practical implications for safety-critical applications. Code is publicly available.",
      "authors": [
        "Akbar Anbar Jafari",
        "Cagri Ozcinar",
        "Gholamreza Anbarjafari"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-17 00:45:27+00:00",
      "link": "https://arxiv.org/pdf/2602.15283v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15277v1",
      "title": "Accelerating Large-Scale Dataset Distillation via Exploration-Exploitation Optimization",
      "abstract": "Dataset distillation compresses the original data into compact synthetic datasets, reducing training time and storage while retaining model performance, enabling deployment under limited resources. Although recent decoupling-based distillation methods enable dataset distillation at large-scale, they continue to face an efficiency gap: optimization-based decoupling methods achieve higher accuracy but demand intensive computation, whereas optimization-free decoupling methods are efficient but sacrifice accuracy. To overcome this trade-off, we propose Exploration-Exploitation Distillation (E^2D), a simple, practical method that minimizes redundant computation through an efficient pipeline that begins with full-image initialization to preserve semantic integrity and feature diversity. It then uses a two-phase optimization strategy: an exploration phase that performs uniform updates and identifies high-loss regions, and an exploitation phase that focuses updates on these regions to accelerate convergence. We evaluate E^2D on large-scale benchmarks, surpassing the state-of-the-art on ImageNet-1K while being 18x faster, and on ImageNet-21K, our method substantially improves accuracy while remaining 4.3x faster. These results demonstrate that targeted, redundancy-reducing updates, rather than brute-force optimization, bridge the gap between accuracy and efficiency in large-scale dataset distillation. Code is available at https://github.com/ncsu-dk-lab.",
      "authors": [
        "Muhammad J. Alahmadi",
        "Peng Gao",
        "Feiyi Wang",
        "Dongkuan",
        "Xu"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-17 00:27:58+00:00",
      "link": "https://arxiv.org/pdf/2602.15277v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15273v1",
      "title": "FrameRef: A Framing Dataset and Simulation Testbed for Modeling Bounded Rational Information Health",
      "abstract": "Information ecosystems increasingly shape how people internalize exposure to adverse digital experiences, raising concerns about the long-term consequences for information health. In modern search and recommendation systems, ranking and personalization policies play a central role in shaping such exposure and its long-term effects on users. To study these effects in a controlled setting, we present FrameRef, a large-scale dataset of 1,073,740 systematically reframed claims across five framing dimensions: authoritative, consensus, emotional, prestige, and sensationalist, and propose a simulation-based framework for modeling sequential information exposure and reinforcement dynamics characteristic of ranking and recommendation systems. Within this framework, we construct framing-sensitive agent personas by fine-tuning language models with framing-conditioned loss attenuation, inducing targeted biases while preserving overall task competence. Using Monte Carlo trajectory sampling, we show that small, systematic shifts in acceptance and confidence can compound over time, producing substantial divergence in cumulative information health trajectories. Human evaluation further confirms that FrameRef's generated framings measurably affect human judgment. Together, our dataset and framework provide a foundation for systematic information health research through simulation, complementing and informing responsible human-centered research. We release FrameRef, code, documentation, human evaluation data, and persona adapter models at https://github.com/infosenselab/frameref.",
      "authors": [
        "Victor De Lima",
        "Jiqun Liu",
        "Grace Hui Yang"
      ],
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.CL"
      ],
      "published": "2026-02-17 00:09:44+00:00",
      "link": "https://arxiv.org/pdf/2602.15273v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15260v1",
      "title": "Fast and Effective On-policy Distillation from Reasoning Prefixes",
      "abstract": "On-policy distillation (OPD), which samples trajectories from the student model and supervises them with a teacher at the token level, avoids relying solely on verifiable terminal rewards and can yield better generalization than off-policy distillation. However, OPD requires expensive on-the-fly sampling of the student policy during training, which substantially increases training cost, especially for long responses. Our initial analysis shows that, during OPD, training signals are often concentrated in the prefix of each output, and that even a short teacher-generated prefix can significantly help the student produce the correct answer. Motivated by these observations, we propose a simple yet effective modification of OPD: we apply the distillation objective only to prefixes of student-generated outputs and terminate each sampling early during distillation. Experiments on a suite of AI-for-Math and out-of-domain benchmarks show that on-policy prefix distillation matches the performance of full OPD while reducing training FLOP by 2x-47x.",
      "authors": [
        "Dongxu Zhang",
        "Zhichao Yang",
        "Sepehr Janghorbani",
        "Jun Han",
        "Andrew Ressler",
        "Qian Qian",
        "Gregory D. Lyng",
        "Sanjit Singh Batra",
        "Robert E. Tillman"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16 23:28:54+00:00",
      "link": "https://arxiv.org/pdf/2602.15260v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15257v1",
      "title": "How to Train Your Long-Context Visual Document Model",
      "abstract": "We present the first comprehensive, large-scale study of training long-context vision language models up to 344K context, targeting long-document visual question answering with measured transfer to long-context text. While several such strong are open-weight, namely Qwen3 VL and GLM 4.5/6V, their training recipes and data pipelines are not reproducible. We systematically study continued pretraining, supervised finetuning, and preference optimization for 24B and 32B parameter models, backed by extensive LC evaluations and ablations to bridge this gap, and achieve state-of-the-art performance on MMLongBenchDoc for both parameter scales. In addition to this, our key findings include: (i) training on context lengths that match evaluation context lengths outperforms training on longer contexts, (ii) training and evaluating with page indices provides a simple, high-impact boost to long-document performance, (iii) our synthetic data pipelines enable self-improvement via continued pretraining and supervised finetuning, and (iv) we extend the known text-to-visual long context transfer to the reverse, showing that visual long context training transfers to long-context text performance. We also release MMLBD-C, a manually corrected version of MMLongBenchDoc to reduce erroneous and low quality examples in the benchmark.",
      "authors": [
        "Austin Veselka"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-16 23:26:51+00:00",
      "link": "https://arxiv.org/pdf/2602.15257v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15241v1",
      "title": "GenAI for Systems: Recurring Challenges and Design Principles from Software to Silicon",
      "abstract": "Generative AI is reshaping how computing systems are designed, optimized, and built, yet research remains fragmented across software, architecture, and chip design communities. This paper takes a cross-stack perspective, examining how generative models are being applied from code generation and distributed runtimes through hardware design space exploration to RTL synthesis, physical layout, and verification. Rather than reviewing each layer in isolation, we analyze how the same structural difficulties and effective responses recur across the stack. Our central finding is one of convergence. Despite the diversity of domains and tools, the field keeps encountering five recurring challenges (the feedback loop crisis, the tacit knowledge problem, trust and validation, co-design across boundaries, and the shift from determinism to dynamism) and keeps arriving at five design principles that independently emerge as effective responses (embracing hybrid approaches, designing for continuous feedback, separating concerns by role, matching methods to problem structure, and building on decades of systems knowledge). We organize these into a challenge--principle map that serves as a diagnostic and design aid, showing which principles have proven effective for which challenges across layers. Through concrete cross-stack examples, we show how systems navigate this map as they mature, and argue that the field needs shared engineering methodology, including common vocabularies, cross-layer benchmarks, and systematic design practices, so that progress compounds across communities rather than being rediscovered in each one. Our analysis covers more than 275 papers spanning eleven application areas across three layers of the computing stack, and distills open research questions that become visible only from a cross-layer vantage point.",
      "authors": [
        "Arya Tschand",
        "Chenyu Wang",
        "Zishen Wan",
        "Andrew Cheng",
        "Ioana Cristescu",
        "Kevin He",
        "Howard Huang",
        "Alexander Ingare",
        "Akseli Kangaslahti",
        "Sara Kangaslahti",
        "Theo Lebryk",
        "Hongjin Lin",
        "Jeffrey Jian Ma",
        "Alexandru Meterez",
        "Clara Mohri",
        "Depen Morwani",
        "Sunny Qin",
        "Roy Rinberg",
        "Paula Rodriguez-Diaz",
        "Alyssa Mia Taliotis",
        "Pernille Undrum Fathi",
        "Rosie Zhao",
        "Todd Zhou",
        "Vijay Janapa Reddi"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published": "2026-02-16 22:45:33+00:00",
      "link": "https://arxiv.org/pdf/2602.15241v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15239v1",
      "title": "Size Transferability of Graph Transformers with Convolutional Positional Encodings",
      "abstract": "Transformers have achieved remarkable success across domains, motivating the rise of Graph Transformers (GTs) as attention-based architectures for graph-structured data. A key design choice in GTs is the use of Graph Neural Network (GNN)-based positional encodings to incorporate structural information. In this work, we study GTs through the lens of manifold limit models for graph sequences and establish a theoretical connection between GTs with GNN positional encodings and Manifold Neural Networks (MNNs). Building on transferability results for GNNs under manifold convergence, we show that GTs inherit transferability guarantees from their positional encodings. In particular, GTs trained on small graphs provably generalize to larger graphs under mild assumptions. We complement our theory with extensive experiments on standard graph benchmarks, demonstrating that GTs exhibit scalable behavior on par with GNNs. To further show the efficiency in a real-world scenario, we implement GTs for shortest path distance estimation over terrains to better illustrate the efficiency of the transferable GTs. Our results provide new insights into the understanding of GTs and suggest practical directions for efficient training of GTs in large-scale settings.",
      "authors": [
        "Javier Porras-Valenzuela",
        "Zhiyang Wang",
        "Alejandro Ribeiro"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16 22:38:56+00:00",
      "link": "https://arxiv.org/pdf/2602.15239v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15238v2",
      "title": "Closing the Distribution Gap in Adversarial Training for LLMs",
      "abstract": "Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.",
      "authors": [
        "Chengzhi Hu",
        "Jonas Dornbusch",
        "David Lüdke",
        "Stephan Günnemann",
        "Leo Schwinn"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "published": "2026-02-16 22:34:52+00:00",
      "link": "https://arxiv.org/pdf/2602.15238v2",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15229v1",
      "title": "tensorFM: Low-Rank Approximations of Cross-Order Feature Interactions",
      "abstract": "We address prediction problems on tabular categorical data, where each instance is defined by multiple categorical attributes, each taking values from a finite set. These attributes are often referred to as fields, and their categorical values as features. Such problems frequently arise in practical applications, including click-through rate prediction and social sciences. We introduce and analyze {tensorFM}, a new model that efficiently captures high-order interactions between attributes via a low-rank tensor approximation representing the strength of these interactions. Our model generalizes field-weighted factorization machines. Empirically, tensorFM demonstrates competitive performance with state-of-the-art methods. Additionally, its low latency makes it well-suited for time-sensitive applications, such as online advertising.",
      "authors": [
        "Alessio Mazzetto",
        "Mohammad Mahdi Khalili",
        "Laura Fee Nern",
        "Michael Viderman",
        "Alex Shtoff",
        "Krzysztof Dembczyński"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "published": "2026-02-16 22:21:48+00:00",
      "link": "https://arxiv.org/pdf/2602.15229v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15228v1",
      "title": "An Empirical Study on the Effects of System Prompts in Instruction-Tuned Models for Code Generation",
      "abstract": "Instruction-tuned Language Models (ILMs) have become essential components of modern AI systems, demonstrating exceptional versatility across natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs -- commonly referred to as Code Language Models (CLMs) -- translate human intent into executable programs. While progress has been driven by advances in scaling and training methodologies, one critical aspect remains underexplored: the impact of system prompts on both general-purpose ILMs and specialized CLMs for code generation. We systematically evaluate how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect code assistant. Our experimental setting spans 360 configurations across four models, five system prompts, three prompting strategies, two languages, and two temperature settings. We find that (1) increasing system-prompt constraint specificity does not monotonically improve correctness -- prompt effectiveness is configuration-dependent and can help or hinder based on alignment with task requirements and decoding context; (2) for larger code-specialized models, few-shot examples can degrade performance relative to zero-shot generation, contrary to conventional wisdom; and (3) programming language matters, with Java exhibiting significantly greater sensitivity to system prompt variations than Python, suggesting language-specific prompt engineering strategies may be necessary.",
      "authors": [
        "Zaiyu Cheng",
        "Antonio Mastropaolo"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE"
      ],
      "published": "2026-02-16 22:11:21+00:00",
      "link": "https://arxiv.org/pdf/2602.15228v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15210v1",
      "title": "ÜberWeb: Insights from Multilingual Curation for a 20-Trillion-Token Dataset",
      "abstract": "Multilinguality is a core capability for modern foundation models, yet training high-quality multilingual models remains challenging due to uneven data availability across languages. A further challenge is the performance interference that can arise from joint multilingual training, commonly referred to as the \"curse of multilinguality\". We study multilingual data curation across thirteen languages and find that many reported regressions are not inherent to multilingual scaling but instead stem from correctable deficiencies in data quality and composition rather than fundamental capacity limits. In controlled bilingual experiments, improving data quality for any single language benefits others: curating English improves non-English performance in 12 of 13 languages, while curating non-English yields reciprocal improvements in English. Bespoke per-language curation produces substantially larger within-language improvements. Extending these findings to large-scale general-purpose training mixtures, we show that curated multilingual allocations comprising under 8% of total tokens remain remarkably effective. We operationalize this approach within an effort that produced a 20T-token pretraining corpus derived entirely from public sources. Models with 3B and 8B parameters trained on a 1T-token random subset achieve competitive multilingual accuracy with 4-10x fewer training FLOPs than strong public baselines, establishing a new Pareto frontier in multilingual performance versus compute. Moreover, these benefits extend to frontier model scale: the 20T-token corpus served as part of the pretraining dataset for Trinity Large (400B/A13B), which exhibits strong multilingual performance relative to its training FLOPs. These results show that targeted, per-language data curation mitigates multilingual interference and enables compute-efficient multilingual scaling.",
      "authors": [
        "DatologyAI",
        ":",
        "Aldo Gael Carranza",
        "Kaleigh Mentzer",
        "Ricardo Pio Monti",
        "Alex Fang",
        "Alvin Deng",
        "Amro Abbas",
        "Anshuman Suri",
        "Brett Larsen",
        "Cody Blakeney",
        "Darren Teh",
        "David Schwab",
        "Diego Kiner",
        "Fan Pan",
        "Haakon Mongstad",
        "Jack Urbanek",
        "Jason Lee",
        "Jason Telanoff",
        "Josh Wills",
        "Luke Merrick",
        "Parth Doshi",
        "Paul Burstein",
        "Pratyush Maini",
        "Spandan Das",
        "Tony Jiang",
        "Vineeth Dorna",
        "Zhengping Wang",
        "Bogdan Gaza",
        "Ari Morcos",
        "Matthew Leavitt"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16 21:40:03+00:00",
      "link": "https://arxiv.org/pdf/2602.15210v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15206v1",
      "title": "MAVRL: Learning Reward Functions from Multiple Feedback Types with Amortized Variational Inference",
      "abstract": "Reward learning typically relies on a single feedback type or combines multiple feedback types using manually weighted loss terms. Currently, it remains unclear how to jointly learn reward functions from heterogeneous feedback types such as demonstrations, comparisons, ratings, and stops that provide qualitatively different signals. We address this challenge by formulating reward learning from multiple feedback types as Bayesian inference over a shared latent reward function, where each feedback type contributes information through an explicit likelihood. We introduce a scalable amortized variational inference approach that learns a shared reward encoder and feedback-specific likelihood decoders and is trained by optimizing a single evidence lower bound. Our approach avoids reducing feedback to a common intermediate representation and eliminates the need for manual loss balancing. Across discrete and continuous-control benchmarks, we show that jointly inferred reward posteriors outperform single-type baselines, exploit complementary information across feedback types, and yield policies that are more robust to environment perturbations. The inferred reward uncertainty further provides interpretable signals for analyzing model confidence and consistency across feedback types.",
      "authors": [
        "Raphaël Baur",
        "Yannick Metz",
        "Maria Gkoulta",
        "Mennatallah El-Assady",
        "Giorgia Ramponi",
        "Thomas Kleine Buening"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16 21:36:28+00:00",
      "link": "https://arxiv.org/pdf/2602.15206v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15190v1",
      "title": "AIC CTU@AVerImaTeC: dual-retriever RAG for image-text fact checking",
      "abstract": "In this paper, we present our 3rd place system in the AVerImaTeC shared task, which combines our last year's retrieval-augmented generation (RAG) pipeline with a reverse image search (RIS) module. Despite its simplicity, our system delivers competitive performance with a single multimodal LLM call per fact-check at just $0.013 on average using GPT5.1 via OpenAI Batch API. Our system is also easy to reproduce and tweak, consisting of only three decoupled modules - a textual retrieval module based on similarity search, an image retrieval module based on API-accessed RIS, and a generation module using GPT5.1 - which is why we suggest it as an accesible starting point for further experimentation. We publish its code and prompts, as well as our vector stores and insights into the scheme's running costs and directions for further improvement.",
      "authors": [
        "Herbert Ullrich",
        "Jan Drchal"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 21:00:29+00:00",
      "link": "https://arxiv.org/pdf/2602.15190v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15189v1",
      "title": "ScrapeGraphAI-100k: A Large-Scale Dataset for LLM-Based Web Information Extraction",
      "abstract": "The use of large language models for web information extraction is becoming increasingly fundamental to modern web information retrieval pipelines. However, existing datasets tend to be small, synthetic or text-only, failing to capture the structural context of the web. We introduce ScrapeGraphAI-100k, a large-scale dataset comprising real-world LLM extraction events, collected via opt-in ScrapeGraphAI telemetry during Q2 and Q3 of 2025. Starting from 9M events, we deduplicate and balance by schema to produce 93,695 examples spanning diverse domains and languages. Each instance includes Markdown content, a prompt, a JSON schema, the LLM response, and complexity/validation metadata. We characterize the datasets structural diversity and its failure modes as schema complexity increases. We also provide a fine-tuning experiment showing that a small language model (1.7B) trained on a subset narrows the gap to larger baselines (30B), underscoring the datasets utility for efficient extraction. ScrapeGraphAI-100k enables fine-tuning small models, benchmarking structured extraction, and studying schema induction for web IR indexing, and is publicly available on HuggingFace.",
      "authors": [
        "William Brach",
        "Francesco Zuppichini",
        "Marco Vinciguerra",
        "Lorenzo Padoan"
      ],
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-16 20:56:59+00:00",
      "link": "https://arxiv.org/pdf/2602.15189v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15184v1",
      "title": "Learning Data-Efficient and Generalizable Neural Operators via Fundamental Physics Knowledge",
      "abstract": "Recent advances in scientific machine learning (SciML) have enabled neural operators (NOs) to serve as powerful surrogates for modeling the dynamic evolution of physical systems governed by partial differential equations (PDEs). While existing approaches focus primarily on learning simulations from the target PDE, they often overlook more fundamental physical principles underlying these equations. Inspired by how numerical solvers are compatible with simulations of different settings of PDEs, we propose a multiphysics training framework that jointly learns from both the original PDEs and their simplified basic forms. Our framework enhances data efficiency, reduces predictive errors, and improves out-of-distribution (OOD) generalization, particularly in scenarios involving shifts of physical parameters and synthetic-to-real transfer. Our method is architecture-agnostic and demonstrates consistent improvements in normalized root mean square error (nRMSE) across a wide range of 1D/2D/3D PDE problems. Through extensive experiments, we show that explicit incorporation of fundamental physics knowledge significantly strengthens the generalization ability of neural operators. We will release models and codes at https://sites.google.com/view/sciml-fundemental-pde.",
      "authors": [
        "Siying Ma",
        "Mehrdad M. Zadeh",
        "Mauricio Soroco",
        "Wuyang Chen",
        "Jiguo Cao",
        "Vijay Ganesh"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-02-16 20:45:10+00:00",
      "link": "https://arxiv.org/pdf/2602.15184v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15169v1",
      "title": "Learning the S-matrix from data: Rediscovering gravity from gauge theory via symbolic regression",
      "abstract": "We demonstrate that modern machine-learning methods can autonomously reconstruct several flagship analytic structures in scattering amplitudes directly from numerical on-shell data. In particular, we show that the Kawai--Lewellen--Tye (KLT) relations can be rediscovered using symbolic regression applied to colour-ordered Yang--Mills amplitudes with Mandelstam invariants as input features. Using standard feature-selection techniques, specifically column-pivoted QR factorisation, we simultaneously recover the Kleiss--Kuijf and Bern--Carrasco--Johansson (BCJ) relations, identifying a minimal basis of partial amplitudes without any group-theoretic input. We obtain the tree-level KLT relations with high numerical accuracy up to five external legs, using only minimal theoretical priors, and we comment on the obstacles to generalising the method to higher multiplicity. Our results establish symbolic regression as a practical tool for exploring the analytic structure of the scattering-amplitude landscape, and suggests a general data-driven strategy for uncovering hidden relations in general theories. For comparison, we benchmark this general approach with a recently introduced neural-network based method.",
      "authors": [
        "Nathan Moynihan"
      ],
      "primary_category": "hep-th",
      "categories": [
        "hep-th",
        "cs.LG",
        "hep-ph"
      ],
      "published": "2026-02-16 20:15:50+00:00",
      "link": "https://arxiv.org/pdf/2602.15169v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15166v1",
      "title": "Fast and Fusiest: An Optimal Fusion-Aware Mapper for Accelerator Modeling and Evaluation",
      "abstract": "The latency and energy of tensor algebra accelerators depend on how data movement and operations are scheduled (i.e., mapped) onto accelerators, so determining the potential of an accelerator architecture requires both a performance model and a mapper to search for the optimal mapping. A key optimization that the mapper must explore is fusion, meaning holding data on-chip between computation steps, which has been shown to reduce energy and latency by reducing DRAM accesses. However, prior mappers cannot find optimal mappings with fusion (i.e., fused mappings) in a feasible runtime because the number of fused mappings to search increases exponentially with the number of workload computation steps.   In this paper, we introduce the Fast and Fusiest Mapper (FFM), the first mapper to quickly find optimal mappings in a comprehensive fused mapspace for tensor algebra workloads. FFM shrinks the search space by pruning subsets of mappings (i.e., partial mappings) that are shown to never be a part of optimal mappings, quickly eliminating all suboptimal mappings with those partial mappings as subsets. Then FFM joins partial mappings to construct optimal fused mappings. We evaluate FFM and show that, although the mapspace size grows exponentially with the number of computation steps, FFM's runtime scales approximately linearly. FFM is orders of magnitude faster ($>1000\\times$) than prior state-of-the-art approaches at finding optimal mappings for Transformers.",
      "authors": [
        "Tanner Andrulis",
        "Michael Gilbert",
        "Vivienne Sze",
        "Joel S. Emer"
      ],
      "primary_category": "cs.AR",
      "categories": [
        "cs.AR"
      ],
      "published": "2026-02-16 20:08:51+00:00",
      "link": "https://arxiv.org/pdf/2602.15166v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15164v1",
      "title": "Synthesizing Trajectory Queries from Examples",
      "abstract": "Data scientists often need to write programs to process predictions of machine learning models, such as object detections and trajectories in video data. However, writing such queries can be challenging due to the fuzzy nature of real-world data; in particular, they often include real-valued parameters that must be tuned by hand. We propose a novel framework called Quivr that synthesizes trajectory queries matching a given set of examples. To efficiently synthesize parameters, we introduce a novel technique for pruning the parameter space and a novel quantitative semantics that makes this more efficient. We evaluate Quivr on a benchmark of 17 tasks, including several from prior work, and show both that it can synthesize accurate queries for each task and that our optimizations substantially reduce synthesis time.",
      "authors": [
        "Stephen Mell",
        "Favyen Bastani",
        "Steve Zdancewic",
        "Osbert Bastani"
      ],
      "primary_category": "cs.PL",
      "categories": [
        "cs.PL"
      ],
      "published": "2026-02-16 20:07:58+00:00",
      "link": "https://arxiv.org/pdf/2602.15164v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15156v1",
      "title": "Panini: Continual Learning in Token Space via Structured Memory",
      "abstract": "Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.",
      "authors": [
        "Shreyas Rajesh",
        "Pavan Holur",
        "Mehmet Yigit Turali",
        "Chenda Duan",
        "Vwani Roychowdhury"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-16 19:58:03+00:00",
      "link": "https://arxiv.org/pdf/2602.15156v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15155v1",
      "title": "Refine Now, Query Fast: A Decoupled Refinement Paradigm for Implicit Neural Fields",
      "abstract": "Implicit Neural Representations (INRs) have emerged as promising surrogates for large 3D scientific simulations due to their ability to continuously model spatial and conditional fields, yet they face a critical fidelity-speed dilemma: deep MLPs suffer from high inference cost, while efficient embedding-based models lack sufficient expressiveness. To resolve this, we propose the Decoupled Representation Refinement (DRR) architectural paradigm. DRR leverages a deep refiner network, alongside non-parametric transformations, in a one-time offline process to encode rich representations into a compact and efficient embedding structure. This approach decouples slow neural networks with high representational capacity from the fast inference path. We introduce DRR-Net, a simple network that validates this paradigm, and a novel data augmentation strategy, Variational Pairs (VP) for improving INRs under complex tasks like high-dimensional surrogate modeling. Experiments on several ensemble simulation datasets demonstrate that our approach achieves state-of-the-art fidelity, while being up to 27$\\times$ faster at inference than high-fidelity baselines and remaining competitive with the fastest models. The DRR paradigm offers an effective strategy for building powerful and practical neural field surrogates and \\rev{INRs in broader applications}, with a minimal compromise between speed and quality.",
      "authors": [
        "Tianyu Xiong",
        "Skylar Wurster",
        "Han-Wei Shen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CE",
        "cs.CV",
        "cs.GR"
      ],
      "published": "2026-02-16 19:55:16+00:00",
      "link": "https://arxiv.org/pdf/2602.15155v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15143v1",
      "title": "Protecting Language Models Against Unauthorized Distillation through Trace Rewriting",
      "abstract": "Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \\emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \\emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.",
      "authors": [
        "Xinhang Ma",
        "William Yeoh",
        "Ning Zhang",
        "Yevgeniy Vorobeychik"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-16 19:40:07+00:00",
      "link": "https://arxiv.org/pdf/2602.15143v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15139v1",
      "title": "CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding",
      "abstract": "Accurate QA over classical Islamic texts remains challenging due to domain specific semantics, long context dependencies, and concept sensitive reasoning. Therefore, a new CGRA DeBERTa, a concept guided residual domain augmentation transformer framework, is proposed that enhances theological QA over Hadith corpora. The CGRA DeBERTa builds on a customized DeBERTa transformer backbone with lightweight LoRA based adaptations and a residual concept aware gating mechanism. The customized DeBERTa embedding block learns global and positional context, while Concept Guided Residual Blocks incorporate theological priors from a curated Islamic Concept Dictionary of 12 core terms. Moreover, the Concept Gating Mechanism selectively amplifies semantically critical tokens via importance weighted attention, applying differential scaling from 1.04 to 3.00. This design preserves contextual integrity, strengthens domain-specific semantic representations, and enables accurate, efficient span extraction while maintaining computational efficiency. This paper reports the results of training CGRA using a specially constructed dataset of 42591 QA pairs from the text of Sahih alBukhari and Sahih Muslim. While BERT achieved an EM score of 75.87 and DeBERTa one of 89.77, our model scored 97.85 and thus surpassed them by 8.08 on an absolute scale, all while adding approximately 8 inference overhead due to parameter efficient gating. The qualitative evaluation noted better extraction and discrimination and theological precision. This study presents Hadith QA systems that are efficient, interpretable, and accurate and that scale provide educational materials with necessary theological nuance.",
      "authors": [
        "Tahir Hussain",
        "Saddam Hussain Khan"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2026-02-16 19:36:32+00:00",
      "link": "https://arxiv.org/pdf/2602.15139v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15136v1",
      "title": "Universal priors: solving empirical Bayes via Bayesian inference and pretraining",
      "abstract": "We theoretically justify the recent empirical finding of [Teh et al., 2025] that a transformer pretrained on synthetically generated data achieves strong performance on empirical Bayes (EB) problems. We take an indirect approach to this question: rather than analyzing the model architecture or training dynamics, we ask why a pretrained Bayes estimator, trained under a prespecified training distribution, can adapt to arbitrary test distributions. Focusing on Poisson EB problems, we identify the existence of universal priors such that training under these priors yields a near-optimal regret bound of $\\widetilde{O}(\\frac{1}{n})$ uniformly over all test distributions. Our analysis leverages the classical phenomenon of posterior contraction in Bayesian statistics, showing that the pretrained transformer adapts to unknown test distributions precisely through posterior contraction. This perspective also explains the phenomenon of length generalization, in which the test sequence length exceeds the training length, as the model performs Bayesian inference using a generalized posterior.",
      "authors": [
        "Nick Cannella",
        "Anzo Teh",
        "Yanjun Han",
        "Yury Polyanskiy"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2026-02-16 19:29:27+00:00",
      "link": "https://arxiv.org/pdf/2602.15136v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15128v1",
      "title": "PolyNODE: Variable-dimension Neural ODEs on M-polyfolds",
      "abstract": "Neural ordinary differential equations (NODEs) are geometric deep learning models based on dynamical systems and flows generated by vector fields on manifolds. Despite numerous successful applications, particularly within the flow matching paradigm, all existing NODE models are fundamentally constrained to fixed-dimensional dynamics by the intrinsic nature of the manifold's dimension. In this paper, we extend NODEs to M-polyfolds (spaces that can simultaneously accommodate varying dimensions and a notion of differentiability) and introduce PolyNODEs, the first variable-dimensional flow-based model in geometric deep learning. As an example application, we construct explicit M-polyfolds featuring dimensional bottlenecks and PolyNODE autoencoders based on parametrised vector fields that traverse these bottlenecks. We demonstrate experimentally that our PolyNODE models can be trained to solve reconstruction tasks in these spaces, and that latent representations of the input can be extracted and used to solve downstream classification tasks. The code used in our experiments is publicly available at https://github.com/turbotage/PolyNODE .",
      "authors": [
        "Per Åhag",
        "Alexander Friedrich",
        "Fredrik Ohlsson",
        "Viktor Vigren Näslund"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16 19:11:06+00:00",
      "link": "https://arxiv.org/pdf/2602.15128v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15022v1",
      "title": "Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation",
      "abstract": "Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \\times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation.",
      "authors": [
        "Cai Zhou",
        "Zijie Chen",
        "Zian Li",
        "Jike Wang",
        "Kaiyi Jiang",
        "Pan Li",
        "Rose Yu",
        "Muhan Zhang",
        "Stephen Bates",
        "Tommi Jaakkola"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.GR",
        "q-bio.BM"
      ],
      "published": "2026-02-16 18:58:55+00:00",
      "link": "https://arxiv.org/pdf/2602.15022v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15021v1",
      "title": "Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI",
      "abstract": "Cross-survey generalization is a critical challenge in stellar spectral analysis, particularly in cases such as transferring from low- to moderate-resolution surveys. We investigate this problem using pre-trained models, focusing on simple neural networks such as multilayer perceptrons (MLPs), with a case study transferring from LAMOST low-resolution spectra (LRS) to DESI medium-resolution spectra (MRS). Specifically, we pre-train MLPs on either LRS or their embeddings and fine-tune them for application to DESI stellar spectra. We compare MLPs trained directly on spectra with those trained on embeddings derived from transformer-based models (self-supervised foundation models pre-trained for multiple downstream tasks). We also evaluate different fine-tuning strategies, including residual-head adapters, LoRA, and full fine-tuning. We find that MLPs pre-trained on LAMOST LRS achieve strong performance, even without fine-tuning, and that modest fine-tuning with DESI spectra further improves the results. For iron abundance, embeddings from a transformer-based model yield advantages in the metal-rich ([Fe/H] > -1.0) regime, but underperform in the metal-poor regime compared to MLPs trained directly on LRS. We also show that the optimal fine-tuning strategy depends on the specific stellar parameter under consideration. These results highlight that simple pre-trained MLPs can provide competitive cross-survey generalization, while the role of spectral foundation models for cross-survey stellar parameter estimation requires further exploration.",
      "authors": [
        "Xiaosheng Zhao",
        "Yuan-Sen Ting",
        "Rosemary F. G. Wyse",
        "Alexander S. Szalay",
        "Yang Huang",
        "László Dobos",
        "Tamás Budavári",
        "Viska Wei"
      ],
      "primary_category": "astro-ph.SR",
      "categories": [
        "astro-ph.SR",
        "astro-ph.GA",
        "cs.LG"
      ],
      "published": "2026-02-16 18:58:47+00:00",
      "link": "https://arxiv.org/pdf/2602.15021v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15019v2",
      "title": "Hunt Globally: Wide Search AI Agents for Drug Asset Scouting in Investing, Business Development, and Competitive Intelligence",
      "abstract": "Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests that over 85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total. A growing share of scholarly output is also non-U.S. Industry estimates put China at 30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface \"under-the-radar\" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high recall discovery across heterogeneous, multilingual sources without hallucination. We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real-deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. On this benchmark, our Bioptic Agent achieves 79.7% F1 score, outperforming Claude Opus 4.6 (56.2%), Gemini 3 Pro + Deep Research (50.6%), OpenAI GPT-5.2 Pro (46.6%), Perplexity Deep Research (44.2%), and Exa Websets (26.9%). Performance improves steeply with additional compute, supporting the view that more compute yields better results.",
      "authors": [
        "Alisa Vinogradova",
        "Vlad Vinogradov",
        "Luba Greenwood",
        "Ilya Yasny",
        "Dmitry Kobyzev",
        "Shoman Kasbekar",
        "Kong Nguyen",
        "Dmitrii Radkevich",
        "Roman Doronin",
        "Andrey Doronichev"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published": "2026-02-16 18:57:49+00:00",
      "link": "https://arxiv.org/pdf/2602.15019v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15013v1",
      "title": "Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation",
      "abstract": "This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to synthesize such parallel datasets from monolingual corpora. This approach creates 'neutralized' text devoid of stylistic attributes, essentially creating a shared input style at training-time and inference-time. Experimental results demonstrate consistent superiority of this method over zero-shot prompting and fewshot ICL techniques measured by BLEU scores and style accuracy scores across four investigated domains. Furthermore, the integration of retrieval-augmented generation (RAG) for terminology and name knowledge enhances robustness and stylistic consistency.",
      "authors": [
        "Ruoxi Liu",
        "Philipp Koehn"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 18:52:43+00:00",
      "link": "https://arxiv.org/pdf/2602.15013v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15008v1",
      "title": "Efficient Sampling with Discrete Diffusion Models: Sharp and Adaptive Guarantees",
      "abstract": "Diffusion models over discrete spaces have recently shown striking empirical success, yet their theoretical foundations remain incomplete. In this paper, we study the sampling efficiency of score-based discrete diffusion models under a continuous-time Markov chain (CTMC) formulation, with a focus on $τ$-leaping-based samplers. We establish sharp convergence guarantees for attaining $\\varepsilon$ accuracy in Kullback-Leibler (KL) divergence for both uniform and masking noising processes. For uniform discrete diffusion, we show that the $τ$-leaping algorithm achieves an iteration complexity of order $\\tilde O(d/\\varepsilon)$, with $d$ the ambient dimension of the target distribution, eliminating linear dependence on the vocabulary size $S$ and improving existing bounds by a factor of $d$; moreover, we establish a matching algorithmic lower bound showing that linear dependence on the ambient dimension is unavoidable in general. For masking discrete diffusion, we introduce a modified $τ$-leaping sampler whose convergence rate is governed by an intrinsic information-theoretic quantity, termed the effective total correlation, which is bounded by $d \\log S$ but can be sublinear or even constant for structured data. As a consequence, the sampler provably adapts to low-dimensional structure without prior knowledge or algorithmic modification, yielding sublinear convergence rates for various practical examples (such as hidden Markov models, image data, and random graphs). Our analysis requires no boundedness or smoothness assumptions on the score estimator beyond control of the score entropy loss.",
      "authors": [
        "Daniil Dmitriev",
        "Zhihan Huang",
        "Yuting Wei"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IT",
        "math.ST",
        "stat.ML"
      ],
      "published": "2026-02-16 18:48:17+00:00",
      "link": "https://arxiv.org/pdf/2602.15008v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15005v1",
      "title": "Learning User Interests via Reasoning and Distillation for Cross-Domain News Recommendation",
      "abstract": "News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user's underlying information needs from heterogeneous signals that often extend beyond direct news consumption. A key challenge lies in moving beyond surface-level behaviors to capture deeper, reusable user interests while maintaining scalability in large-scale production systems. In this paper, we present a reinforcement learning framework that trains large language models to generate high-quality lists of interest-driven news search queries from cross-domain user signals. We formulate query-list generation as a policy optimization problem and employ GRPO with multiple reward signals. We systematically study two compute dimensions: inference-time sampling and model capacity, and empirically observe consistent improvements with increased compute that exhibit scaling-like behavior. Finally, we perform on-policy distillation to transfer the learned policy from a large, compute-intensive teacher to a compact student model suitable for scalable deployment. Extensive offline experiments, ablation studies and large-scale online A/B tests in a production news recommendation system demonstrate consistent gains in both interest modeling quality and downstream recommendation performance.",
      "authors": [
        "Mengdan Zhu",
        "Yufan Zhao",
        "Tao Di",
        "Yulan Yan",
        "Liang Zhao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2026-02-16 18:45:40+00:00",
      "link": "https://arxiv.org/pdf/2602.15005v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15004v1",
      "title": "PDE foundation models are skillful AI weather emulators for the Martian atmosphere",
      "abstract": "We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the Martian atmosphere. We base our work on the Poseidon PDE foundation model for two-dimensional systems. We develop a method to extend Poseidon from two to three dimensions while keeping the pretraining information. Moreover, we investigate the performance of the model in the presence of sparse initial conditions. Our results make use of four Martian years (approx.~34 GB) of training data and a median compute budget of 13 GPU hours. We find that the combination of pretraining and model extension yields a performance increase of 34.4\\% on a held-out year. This shows that PDEs-FMs can not only approximate solutions to (other) PDEs but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.",
      "authors": [
        "Johannes Schmude",
        "Sujit Roy",
        "Liping Wang",
        "Theodore van Kessel",
        "Levente Klein",
        "Marcus Freitag",
        "Eloisa Bentivegna",
        "Robert Manson-Sawko",
        "Bjorn Lutjens",
        "Manil Maskey",
        "Campbell Watson",
        "Rahul Ramachandran",
        "Juan Bernabe-Moreno"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "physics.ao-ph"
      ],
      "published": "2026-02-16 18:44:46+00:00",
      "link": "https://arxiv.org/pdf/2602.15004v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14997v1",
      "title": "Spectral Convolution on Orbifolds for Geometric Deep Learning",
      "abstract": "Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.",
      "authors": [
        "Tim Mangliers",
        "Bernhard Mössner",
        "Benjamin Himpel"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-16 18:28:38+00:00",
      "link": "https://arxiv.org/pdf/2602.14997v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14983v1",
      "title": "Orthogonalized Multimodal Contrastive Learning with Asymmetric Masking for Structured Representations",
      "abstract": "Multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their interaction. While self-supervised multimodal contrastive learning has achieved remarkable progress, most existing methods predominantly capture redundant cross-modal signals, often neglecting modality-specific (unique) and interaction-driven (synergistic) information. Recent extensions broaden this perspective, yet they either fail to explicitly model synergistic interactions or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage. We introduce \\textbf{COrAL}, a principled framework that explicitly and simultaneously preserves redundant, unique, and synergistic information within multimodal representations. COrAL employs a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features, ensuring a clean separation of information components. To promote synergy modeling, we introduce asymmetric masking with complementary view-specific patterns, compelling the model to infer cross-modal dependencies rather than rely solely on redundant cues. Extensive experiments on synthetic benchmarks and diverse MultiBench datasets demonstrate that COrAL consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs. These results indicate that explicitly modeling the full spectrum of multimodal information yields more stable, reliable, and comprehensive embeddings.",
      "authors": [
        "Carolin Cissee",
        "Raneen Younis",
        "Zahra Ahmadi"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16 18:06:53+00:00",
      "link": "https://arxiv.org/pdf/2602.14983v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14972v1",
      "title": "Use What You Know: Causal Foundation Models with Partial Graphs",
      "abstract": "Estimating causal quantities traditionally relies on bespoke estimators tailored to specific assumptions. Recently proposed Causal Foundation Models (CFMs) promise a more unified approach by amortising causal discovery and inference in a single step. However, in their current state, they do not allow for the incorporation of any domain knowledge, which can lead to suboptimal predictions. We bridge this gap by introducing methods to condition CFMs on causal information, such as the causal graph or more readily available ancestral information. When access to complete causal graph information is too strict a requirement, our approach also effectively leverages partial causal information. We systematically evaluate conditioning strategies and find that injecting learnable biases into the attention mechanism is the most effective method to utilise full and partial causal information. Our experiments show that this conditioning allows a general-purpose CFM to match the performance of specialised models trained on specific causal structures. Overall, our approach addresses a central hurdle on the path towards all-in-one causal foundation models: the capability to answer causal queries in a data-driven manner while effectively leveraging any amount of domain expertise.",
      "authors": [
        "Arik Reuter",
        "Anish Dhir",
        "Cristiana Diaconu",
        "Jake Robertson",
        "Ole Ossen",
        "Frank Hutter",
        "Adrian Weller",
        "Mark van der Wilk",
        "Bernhard Schölkopf"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-16 17:56:37+00:00",
      "link": "https://arxiv.org/pdf/2602.14972v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14970v1",
      "title": "Counterfactual Fairness Evaluation of LLM-Based Contact Center Agent Quality Assurance System",
      "abstract": "Large Language Models (LLMs) are increasingly deployed in contact-center Quality Assurance (QA) to automate agent performance evaluation and coaching feedback. While LLMs offer unprecedented scalability and speed, their reliance on web-scale training data raises concerns regarding demographic and behavioral biases that may distort workforce assessment. We present a counterfactual fairness evaluation of LLM-based QA systems across 13 dimensions spanning three categories: Identity, Context, and Behavioral Style. Fairness is quantified using the Counterfactual Flip Rate (CFR), the frequency of binary judgment reversals, and the Mean Absolute Score Difference (MASD), the average shift in coaching or confidence scores across counterfactual pairs. Evaluating 18 LLMs on 3,000 real-world contact center transcripts, we find systematic disparities, with CFR ranging from 5.4% to 13.0% and consistent MASD shifts across confidence, positive, and improvement scores. Larger, more strongly aligned models show lower unfairness, though fairness does not track accuracy. Contextual priming of historical performance induces the most severe degradations (CFR up to 16.4%), while implicit linguistic identity cues remain a persistent bias source. Finally, we analyze the efficacy of fairness-aware prompting, finding that explicit instructions yield only modest improvements in evaluative consistency. Our findings underscore the need for standardized fairness auditing pipelines prior to deploying LLMs in high-stakes workforce evaluation.",
      "authors": [
        "Kawin Mayilvaghanan",
        "Siddhant Gupta",
        "Ayush Kumar"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-16 17:56:18+00:00",
      "link": "https://arxiv.org/pdf/2602.14970v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14949v1",
      "title": "Max-Min Bilinear Completely Positive Programs: A Semidefinite Relaxation with Tightness Guarantees",
      "abstract": "Max-min bilinear optimization models, where one agent maximizes and an adversary minimizes a common bilinear objective, serve as canonical saddle-point formulations in optimization theory. They capture, among others, two-player zero-sum games, robust and distributionally robust optimization, and adversarial machine learning. This study focuses on the subclass whose variables lie in the completely positive (CP) cone, capturing a broad family of mixed-binary quadratic max-min problems through the modelling power of completely positive programming. We show that such problems admit an equivalent single-stage linear reformulation over the COP-CP cone, defined as the Cartesian product of the copositive (COP) and CP cones. Because testing membership in COP cones is co-NP-complete, the resulting COP-CP program inherits NP-hardness. To address this challenge, we develop a hierarchy of semidefinite relaxations based on moment and sum-of-squares representations of the COP and CP cones, and flat truncation conditions are applied to certify the tightness. We show that the tightness of the hierarchy is guaranteed under mild conditions. The framework extends existing CP/COP approaches for distributionally robust optimization and polynomial games. We apply the framework to the cyclic Colonel Blotto game, an extension of Borel's classic allocation contest. Across multiple instances, the semidefinite relaxation meets the flat-truncation conditions and solves the exact mixed-strategy equilibrium.",
      "authors": [
        "Sarah Yini Gao",
        "Xindong Tang",
        "Yancheng Yuan"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-16 17:29:39+00:00",
      "link": "https://arxiv.org/pdf/2602.14949v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14786v1",
      "title": "New Randomized Global Generalized Minimum Residual (RGl-GMRES) method",
      "abstract": "In this paper, we develop a new Randomized Global Generalized Minimum Residual (RGlGMRES) algorithm for efficiently computing solutions to large scale linear systems with multiple right hand sides.The proposed method builds on a recently developed randomized global Gram Schmidt process, in which sketched Frobenius inner products are employed to approximate the exact Frobenius inner products of high-dimensional matrices. We give some new convergence results of the randomized global GMRES method for multiple linear systems. In the case where the coefficient matrix A is diagonalizable, we derive new upper bounds for the randomized Frobenius norm of the residual. In this paper, we study how to introduce matrix sketching in this algorithm. It allows us to reduce the dimension of the problem in one of the main steps of the algorithm. To validate the effectiveness and practicality of this approach, we conduct several numerical experiments, which demonstrate that our RGl-GMRES method is competitive with the GlGMRES method for solving large scale problems with multiple right-hand sides.",
      "authors": [
        "Achraf Badahmane",
        "Xian-Ming GU"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-02-16 14:36:32+00:00",
      "link": "https://arxiv.org/pdf/2602.14786v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14692v1",
      "title": "Weak Poincaré inequalities for Deterministic-scan Metropolis-within-Gibbs samplers",
      "abstract": "Using the framework of weak Poincaré inequalities, we analyze the convergence properties of deterministic-scan Metropolis-within-Gibbs samplers, an important class of Markov chain Monte Carlo algorithms. Our analysis applies to nonreversible Markov chains and yields explicit (subgeometric) convergence bounds through novel comparison techniques based on Dirichlet forms. We show that the joint chain inherits the convergence behavior of the marginal chain and conversely. In addition, we establish several fundamental results for weak Poincaré inequalities for discrete-time Markov chains, such as a tensorization property for independent chains. We apply our theoretical results through applications to algorithms for Bayesian inference for a hierarchical regression model and a diffusion model under discretely-observed data.",
      "authors": [
        "Mengxi Gao",
        "Gareth O. Roberts",
        "Andi Q. Wang"
      ],
      "primary_category": "stat.CO",
      "categories": [
        "stat.CO",
        "math.PR"
      ],
      "published": "2026-02-16 12:27:31+00:00",
      "link": "https://arxiv.org/pdf/2602.14692v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14683v1",
      "title": "Joint Majorization-Minimization for Nonnegative CP and Tucker Decompositions under $β$-Divergences: Unfolding-Free Updates",
      "abstract": "We study majorization-minimization methods for nonnegative tensor decompositions under the $β$-divergence family, focusing on nonnegative CP and Tucker models. Our aim is to avoid explicit mode unfoldings and large auxiliary matrices by deriving separable surrogates whose multiplicative updates can be implemented using only tensor contractions (einsum-style operations). We present both classical block-MM updates in contraction-only form and a joint majorization strategy, inspired by joint MM for matrix $β$-NMF, that reuses cached reference quantities across inexpensive inner updates. We prove tightness of the proposed majorizers, establish monotonic decrease of the objective, and show convergence of the sequence of objective values; we also discuss how BSUM theory applies to the block-MM scheme for analyzing limit points. Finally, experiments on synthetic tensors and the Uber spatiotemporal count tensor demonstrate substantial speedups over unfolding-based baselines and a recent einsum-factorization framework.",
      "authors": [
        "Valentin Leplat"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "math.NA"
      ],
      "published": "2026-02-16 12:16:01+00:00",
      "link": "https://arxiv.org/pdf/2602.14683v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14618v1",
      "title": "Finitary coding and Gaussian concentration for random fields",
      "abstract": "We study Gaussian concentration inequalities for random fields obtained as finitary codings of i.i.d.\\ processes, establishing a natural bridge between finitary codings and concentration inequalities. A finitary coding formalizes how a random field with nontrivial dependence, such as the Ising model, can be generated from an i.i.d.\\ source by a shift-equivariant map that determines each output spin from a finite, configuration-dependent window of the input. Gaussian concentration yields uniform sub-Gaussian fluctuation bounds for all local observables.   Our main abstract result shows that Gaussian concentration is preserved under finitary codings of i.i.d.\\ fields provided the coding volume has finite second moment. The proof relies on a refinement of the classical worst-case bounded-differences inequality of Talagrand and Marton, allowing us to exploit configuration-dependent bounds. Under an additional structural condition that we call the short-range factorization property, satisfied in particular by codings constructed via coupling-from-the-past algorithms, finite first moment of the coding volume already suffices. These moment conditions are sharp.   Applications include Gibbs measures and Markov random fields on $\\mathbb Z^d$, probabilistic cellular automata, and one-dimensional stochastic processes. For classical lattice models such as the Ising, Potts, and random-cluster models, we obtain sharp criteria for Gaussian concentration coinciding with uniqueness regimes. For countable-state Markov chains, we derive equivalent characterizations, including geometric ergodicity, exponential return-time tails, and the existence of finitary i.i.d.\\ codings with exponential tails.",
      "authors": [
        "J. -R. Chazottes",
        "S. Gallo",
        "D. Takahashi"
      ],
      "primary_category": "math.PR",
      "categories": [
        "math.PR",
        "math-ph",
        "math.DS"
      ],
      "published": "2026-02-16 10:24:13+00:00",
      "link": "https://arxiv.org/pdf/2602.14618v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15079v1",
      "title": "Fundamental questions on robustness and accuracy for classical and quantum learning algorithms",
      "abstract": "This chapter introduces and investigates some fundamental questions on the relationship between accuracy and robustness in both classical and quantum classification algorithms under noisy and adversarial conditions. We introduce and clarify various definitions of robustness and accuracy, including corrupted-instance robustness accuracy and prediction-change robustness, distinguishing them from conventional accuracy and robustness measures. Through theoretical analysis and toy models, we establish conditions under which trade-offs between accuracy and robustness accuracy arise and identify scenarios where such trade-offs can be avoided. The framework developed highlights the nuanced interplay between model bias, noise characteristics, and perturbation types, including relevant and irrelevant perturbations. We explore the implications of some of these results for incompatible noise, adversarial quantum perturbations, the no free lunch theorem, and suggest future methods to examine these problems from the lens of dynamical systems.",
      "authors": [
        "Nana Liu"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "math-ph"
      ],
      "published": "2026-02-16 08:12:57+00:00",
      "link": "https://arxiv.org/pdf/2602.15079v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14537v1",
      "title": "On the Existence of Koopman Linear Embeddings for Controlled Nonlinear Systems",
      "abstract": "Koopman linear representations have become a popular tool for control design of nonlinear systems, yet it remains unclear when such representations are exact. In this paper, we establish sufficient and necessary conditions under which a controlled nonlinear system admits an exact finite-dimensional Koopman linear representation, which we term Koopman linear embedding. We show that such a system must be transformable into a special control-affine preserved (CAP) structure, which enforces affine dependence of the state on the control input and isolates all nonlinearities into an autonomous subsystem. We further prove that this autonomous subsystem must itself admit a finite-dimensional Koopman linear model with a sufficiently-rich Koopman invariant subspace. Finally, we introduce a symbolic procedure to determine whether a given controlled nonlinear system admits the CAP structure, thereby elucidating whether Koopman approximation errors arise from intrinsic system dynamics or from the choice of lifting functions.",
      "authors": [
        "Xu Shang",
        "Masih Haseli",
        "Jorge Cortés",
        "Yang Zheng"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "eess.SY"
      ],
      "published": "2026-02-16 07:54:19+00:00",
      "link": "https://arxiv.org/pdf/2602.14537v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14480v1",
      "title": "Graph-Guided Fused Regularization for Single- and Multi-Task Regression on Spatiotemporal Data",
      "abstract": "Spatiotemporal matrix-valued data arise frequently in modern applications, yet performing effective regression analysis remains challenging due to complex, dimension-specific dependencies. In this work, we propose a regularized framework for spatiotemporal matrix regression that characterizes temporal and spatial dependencies through tailored penalties. Specifically, the model incorporates a fused penalty to capture smooth temporal evolution and a graph-guided penalty to promote spatial similarity. The framework also extends to the multi-task setting, enabling joint estimation across related tasks. We provide a comprehensive analysis of the framework from both theoretical and computational perspectives. Theoretically, we establish the statistical consistency of the proposed estimators. Computationally, we develop an efficient solver based on the Halpern Peaceman-Rachford method for the resulting composite convex optimization problem. The proposed algorithm achieves a fast global non-ergodic $\\mathcal{O}(1/k)$ convergence rate with low per-iteration complexity. Extensive numerical experiments demonstrate that our method significantly outperforms state-of-the-art approaches in terms of predictive accuracy and estimation error, while also exhibiting superior computational efficiency and scalability.",
      "authors": [
        "Meixia Lin",
        "Ziyang Zeng",
        "Yangjing Zhang"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-16 05:45:40+00:00",
      "link": "https://arxiv.org/pdf/2602.14480v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14185v1",
      "title": "Smoothing Meets Perturbation: Unified and Tight Analysis for Nonconvex-Concave Minimax Optimization",
      "abstract": "In this paper, we investigate smooth nonconvex-concave minimax optimization problems and analyze two widely used acceleration mechanisms -- perturbation and smoothing. Perturbation augments the dual objective with a small quadratic regularization term, whereas smoothing employs an auxiliary primal sequence to approximate a proximal-point update of the value function. While both techniques are known to improve convergence guarantees, their respective roles and relative strengths remain unclear. We develop a unified analytical framework that disentangles and quantifies the respective roles of smoothing and perturbation. With this analytical framework, we design new first-order methods that improve the state-of-the-art iteration complexity bounds for both single-loop and double-loop schemes, for achieving both approximate game stationary (GS) and optimization stationary (OS) points. We also establish matching lower bounds based on carefully constructed hard instances, showing that the resulting complexity bounds are tight. Taken together, these results reveal a fundamental difference between approximate GS and OS in terms of their intrinsic complexity behavior and the following understanding: smoothing and perturbation play fundamentally different yet complementary roles in achieving approximate GS. Their combination creates a synergistic effect that yields strictly faster convergence speed than either mechanism alone, whereas perturbation by itself is insufficient for OS.",
      "authors": [
        "Jiajin Li",
        "Mahesh Nagarajan",
        "Siyu Pan",
        "Nanxi Zhang"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-15 15:22:21+00:00",
      "link": "https://arxiv.org/pdf/2602.14185v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14058v1",
      "title": "A Homogeneous Second-Order Descent Ascent Algorithm for Nonconvex-Strongly Concave Minimax Problems",
      "abstract": "This paper introduces a novel Homogeneous Second-order Descent Ascent (HSDA) algorithm for nonconvex-strongly concave minimax optimization problems. At each iteration, HSDA uniquely computes a search direction by solving a homogenized eigenvalue subproblem built from the gradient and Hessian of the objective function. This formulation guarantees a descent direction with sufficient negative curvature even in near-positive-semidefinite Hessian regimes--a key feature that enhances escape from saddle points. We prove that HSDA finds an $\\mathcal{O}(\\varepsilon,\\sqrt{\\varepsilon})$-second-order stationary point within $\\tilde{\\mathcal{O}}(\\varepsilon^{-3/2})$ iterations, matching the optimal $\\varepsilon$-order iteration complexity among second-order methods for this problem class. To address large-scale applications, we further design an inexact variant (IHSDA) that preserves the single-loop structure while solving the subproblem approximately via a Lanczos procedure. With high probability, IHSDA achieves the same $\\tilde{\\mathcal{O}}(\\varepsilon^{-3/2})$ iteration complexity and attains an $\\mathcal{O}(\\varepsilon, \\sqrt{\\varepsilon})$-second-order stationary point, with the total Hessian-vector product cost bounded by $\\tilde{\\mathcal{O}}(\\varepsilon^{-7/4})$. Experiments on synthetic minimax problems and adversarial training tasks confirm the practical effectiveness and robustness of the proposed algorithms.",
      "authors": [
        "Jia-Hao Chen",
        "Zi Xu",
        "Hui-Ling Zhang"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-15 09:08:48+00:00",
      "link": "https://arxiv.org/pdf/2602.14058v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15068v1",
      "title": "A Unified Benchmark of Physics-Informed Neural Networks and Kolmogorov-Arnold Networks for Ordinary and Partial Differential Equations",
      "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful mesh-free framework for solving ordinary and partial differential equations by embedding the governing physical laws directly into the loss function. However, their classical formulation relies on multilayer perceptrons (MLPs), whose fixed activation functions and global approximation biases limit performance in problems with oscillatory behavior, multiscale dynamics, or sharp gradients. In parallel, Kolmogorov-Arnold Networks (KANs) have been introduced as a functionally adaptive architecture based on learnable univariate transformations along each edge, providing richer local approximations and improved expressivity. This work presents a systematic and controlled comparison between standard MLP-based PINNs and their KAN-based counterparts, Physics-Informed Kolmogorov-Arnold Networks (PIKANs), using identical physics-informed formulations and matched parameter budgets to isolate the architectural effect. Both models are evaluated across a representative collection of ODEs and PDEs, including cases with known analytical solutions that allow direct assessment of gradient reconstruction accuracy. The results show that PIKANs consistently achieve more accurate solutions, converge in fewer iterations, and yield superior gradient estimates, highlighting their advantage for physics-informed learning. These findings underline the potential of KAN-based architectures as a next-generation approach for scientific machine learning and provide rigorous evidence to guide model selection in differential equation solving.",
      "authors": [
        "Salvador K. Dzimah",
        "Sonia Rubio Herranz",
        "Fernando Carlos Lopez Hernandez",
        "Antonio López Montes"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-02-14 11:58:40+00:00",
      "link": "https://arxiv.org/pdf/2602.15068v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13729v1",
      "title": "Semi-supervised linear regression with missing covariates",
      "abstract": "Missing values in datasets are common in applied statistics. For regression problems, theoretical work thus far has largely considered the issue of missing covariates as distinct from missing responses. However, in practice, many datasets have both forms of missingness. Motivated by this gap, we study linear regression with a labelled dataset containing missing covariates, potentially alongside an unlabelled dataset. We consider both structured (blockwise-missing) and unstructured missingness patterns, along with sparse and non-sparse regression parameters. For the non-sparse case, we provide an estimator based on imputing the missing data combined with a reweighting step. For the high-dimensional sparse case, we use a modified version of the Dantzig selector. We provide non-asymptotic upper bounds on the risk of both procedures. These are matched by several new minimax lower bounds, demonstrating the rate optimality of our estimators. Notably, even when the linear model is well-specified, our results characterise substantial differences in the minimax rates when unlabelled data is present relative to the fully supervised setting. Particular consequences of our sparse and non-sparse results include the first matching upper and lower bounds on the minimax rate for the supervised setting when either unstructured or structured missingness is present. Our theory is coupled with extensive simulations and a semi-synthetic application to the California housing dataset.",
      "authors": [
        "Benedict M. Risebrow",
        "Thomas B. Berrett"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "stat.ME"
      ],
      "published": "2026-02-14 11:41:48+00:00",
      "link": "https://arxiv.org/pdf/2602.13729v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13654v1",
      "title": "From time series to dissipativity of linear systems with dynamic supply rates",
      "abstract": "This paper studies the problem of verifying dissipativity of linear time-invariant (LTI) systems using input-output data. We leverage behavioral systems theory to express dissipativity in terms of quadratic difference forms (QDFs), allowing the study of general dynamic quadratic supply rates. We work under the assumptions that the data-generating system is controllable, and an upper bound is given on its lag. As our main results, we provide sufficient conditions for the data to be informative for dissipativity. We also show that for a specific class of static supply rates, these conditions are both necessary and sufficient. For the latter supply rates, it turns out that certification of dissipativity is only possible from data that enable unique system identification. As auxiliary results, we highlight some properties of QDFs, such as upper bounds on the degree of storage functions.",
      "authors": [
        "Henk J. van Waarde",
        "Jeremy Coulson",
        "Alberto Padoan"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "math.DS"
      ],
      "published": "2026-02-14 07:45:19+00:00",
      "link": "https://arxiv.org/pdf/2602.13654v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13620v1",
      "title": "An adaptive framework for first-order gradient methods",
      "abstract": "Gradient methods are widely used in optimization problems. In practice, while the smoothness parameter can be estimated utilizing techniques such as backtracking, estimating the strong convexity parameter remains a challenge; moreover, even with the optimal parameter choice, convergence can be slow. In this work, we propose a framework for dynamically adapting the step size and momentum parameters in first-order gradient methods for the optimization problem, without prior knowledge of the strong convexity parameter. The main idea is to use the geometric average of the ratios of successive residual norms as an empirical estimate of the upper bound on the convergence rate, which in turn allows us to adaptively update the algorithm parameters. The resulting algorithms are simple to implement, yet efficient in practice, requiring only a few additional computations on existing information. The proposed adaptive gradient methods are shown to converge at least as fast as gradient descent for quadratic optimization problems. Numerical experiments on both quadratic and nonlinear problems validate the effectiveness of the proposed adaptive algorithms. The results show that the adaptive algorithms are comparable to their counterparts using optimal parameters, and in some cases, they capture local information and exhibit improved performance.",
      "authors": [
        "Xiaozhe Hu",
        "Sara Pollock",
        "Zhongqin Xue",
        "Yunrong Zhu"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "math.NA"
      ],
      "published": "2026-02-14 06:13:31+00:00",
      "link": "https://arxiv.org/pdf/2602.13620v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13169v1",
      "title": "Operator Learning for Families of Finite-State Mean-Field Games",
      "abstract": "Finite-state mean-field games (MFGs) arise as limits of large interacting particle systems and are governed by an MFG system, a coupled forward-backward differential equation consisting of a forward Kolmogorov-Fokker-Planck (KFP) equation describing the population distribution and a backward Hamilton-Jacobi-Bellman (HJB) equation defining the value function. Solving MFG systems efficiently is challenging, with the structure of each system depending on an initial distribution of players and the terminal cost of the game. We propose an operator learning framework that solves parametric families of MFGs, enabling generalization without retraining for new initial distributions and terminal costs. We provide theoretical guarantees on the approximation error, parametric complexity, and generalization performance of our method, based on a novel regularity result for an appropriately defined flow map corresponding to an MFG system. We demonstrate empirically that our framework achieves accurate approximation for two representative instances of MFGs: a cybersecurity example and a high-dimensional quadratic model commonly used as a benchmark for numerical methods for MFGs.",
      "authors": [
        "William Hofgard",
        "Asaf Cohen",
        "Mathieu Laurière"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "stat.ML"
      ],
      "published": "2026-02-13 18:28:34+00:00",
      "link": "https://arxiv.org/pdf/2602.13169v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13152v1",
      "title": "Detecting Parameter Instabilities in Functional Concurrent Linear Regression",
      "abstract": "We develop methodology to detect structural breaks in the slope function of a concurrent functional linear regression model for functional time series in $C[0,1]$. Our test is based on a CUSUM process of regressor-weighted OLS residual functions. To accommodate both global and local changes, we propose $L^2$- and sup-norm versions, with the sup-norm particularly sensitive to spike-like changes. Under Hölder regularity and weak dependence conditions, we establish a functional strong invariance principle, derive the asymptotic null distribution, and show that the resulting tests are consistent against a broad class of alternatives with breaks in the slope function. Simulation studies illustrate finite-sample size and power. We apply the method to sports data obtained via body-worn sensors from running athletes, focusing on hip and knee joint-angle trajectories recorded during a fatiguing run. As fatigue accumulates, runners adapt their movement patterns, and sufficiently pronounced adjustments are expected to appear as a change point in the regression relationship. In this manner, we illustrate how the proposed tests support interpretable inference for biomechanical functional time series.",
      "authors": [
        "Rupsa Basu",
        "Sven Otto"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "published": "2026-02-13 18:05:03+00:00",
      "link": "https://arxiv.org/pdf/2602.13152v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13063v1",
      "title": "Reinterpreting EMML as Mirror Descent for Constrained Maximum Likelihood Estimation",
      "abstract": "The Expectation--Maximization Maximum Likelihood (EMML) algorithm belongs to the Expectation--Maximization family and is widely used for image reconstruction problems under Poisson noise.In this paper, we reinterpret EMML as a mirror descent method applied to a reparametrized objective function. This perspective allows us to incorporate convex constraints into the algorithm through appropriately chosen Bregman projections, while preserving the multiplicative structure of the EMML updates to ensure computational efficiency. We then establish the convergence of the resulting algorithm toward a solution of the constrained maximum-likelihood problem. Numerical experiments on hyperspectral unmixing problems demonstrate that the constrained EMML converges in fewer iterations than the classical EMML.",
      "authors": [
        "Antonin Clerc",
        "Ségolène Martin",
        "Nicolas Papadakis",
        "Gabriele Steidl"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-13 16:17:53+00:00",
      "link": "https://arxiv.org/pdf/2602.13063v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13005v1",
      "title": "Optimizing Initial Feature-Mapping Variables from Given Designs via Tracking",
      "abstract": "A feature-mapping framework for inverse reconstruction of density-based topology optimization results is proposed. Unlike SIMP, whose voxelized outputs are hard to interpret or reuse, the method represents designs with high-level geometric primitives mapped to a fixed analysis grid. Capsule-shaped bars (endpoints plus radius) are used, with closed-form signed distances and smooth transition functions providing derivatives up to second order. Differentiable pseudo-densities are aggregated with smooth operators, enabling gradient-based optimization with exact Hessians. Robustness is improved through asymmetric transition functions that propagate sensitivities into void regions, a reward-only objective for initialization, and geometric safeguards against degenerate configurations. Reconstruction is performed in stages (exploration, bridging, convergence) with optional refinement that can add, remove, or merge features based on residuals and geometric criteria. Experiments on canonical SIMP benchmarks, including five-bar and cantilever layouts, show high-fidelity reconstructions using a moderate number of features. p-norm and softmax aggregation yield sharp results; pruning removes redundant features and additive refinement restores coverage. Exact Hessians accelerate convergence and improve robustness compared to quasi-Newton updates, providing a bridge from voxel-based outputs to explicit parametric models.",
      "authors": [
        "Patrick Jung"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-13 15:12:20+00:00",
      "link": "https://arxiv.org/pdf/2602.13005v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12872v1",
      "title": "Neural Evolutionary Kernel Method: A Knowledge-Guided Framework for Solving Evolutionary PDEs",
      "abstract": "Numerical solution of partial differential equations (PDEs) plays a vital role in various fields of science and engineering. In recent years, deep neural networks (DNNs) have emerged as a powerful tool for solving PDEs, leveraging their approximation capabilities to handle complex domains and high-dimensional problems. Among these, operator learning has gained increasing attention by learning mappings between function spaces using DNNs. This paper proposes a novel approach, termed the Neural Evolutionary Kernel Method (NEKM), for solving a class of time-dependent partial differential equations (PDEs) via deep neural network (DNN)-based kernel representations. By integrating boundary integral techniques with operator learning, prior mathematical information of time-dependent partial differential equations (PDEs) is embedded into the design of neural network architectures for predicting their solutions, enhancing both computational efficiency and solution accuracy. Numerical experiments on the heat, wave, and Schrödinger equations demonstrate that the Neural Evolutionary Kernel Method (NEKM) achieves high accuracy and favorable computational efficiency. Furthermore, the operator learning framework inherently supports the simultaneous prediction of solutions to multiple PDEs with different coefficients, rendering its capability for solving random PDEs.",
      "authors": [
        "Shuo Ling",
        "Wenjun Ying",
        "Zhen Zhang"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-02-13 12:22:58+00:00",
      "link": "https://arxiv.org/pdf/2602.12872v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12867v1",
      "title": "Parametric Biobjective Linear Programming",
      "abstract": "We consider parametric linear programming problems with multiple objective functions depending linearly on some parameter. Both parametric (single-objective) linear programming and (non-parametric) multi-objective linear programming are well-researched topics. However, literature on the combination of both, parametric linear programming with multiple objectives, is scarce. This research gap encourages our work in this field. Our main focus is on biobjective linear programs with a single parameter. We establish a connection of this problem to non-parametric multi-objective problems. Using the so-called weight set decomposition, we are able to explain the behavior of parametric biobjective linear programs when the parameter value is variated. We investigate two special cases of parametric biobjective linear programs: In the first, there is only one parametric objective and, in the second, the parametric dependency is the same for both objectives. We prove that there is a one-to-one correspondence between the solution of the parametric program and the solution of the triobjective program using the weighted sum scalarization. We provide structural insights to the solution of the parametric biobjective linear program with respect to extreme weights of the weight set of the triobjective linear program and develop solution strategies for the parametric program.",
      "authors": [
        "Kezang Yuden",
        "Levin Nemesch",
        "Stefan Ruzika"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-13 12:17:13+00:00",
      "link": "https://arxiv.org/pdf/2602.12867v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12821v1",
      "title": "Explicit data-dependent characterizations of the subdifferential of convex pointwise suprema and optimality conditions",
      "abstract": "We establish explicit data-dependent and symmetric characterizations of the subdifferential of the supremum of convex functions, formulated directly in terms of the underlying data functions. In our approach, both active and non-active functions contribute equally through their subdifferentials, thereby avoiding the need for additional geometric constructions, such as the domain of the supremum, that arise in previous developments. Applications to infinite convex optimization yield sharp Karush-Kuhn-Tucker and Fritz-John optimality conditions, expressed exclusively in terms of the objective and constraint functions and clearly distinguishing the roles of (almost) active and non-active constraints.",
      "authors": [
        "Stephanie Caro",
        "Abderrahim Hantoute"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-13 11:08:16+00:00",
      "link": "https://arxiv.org/pdf/2602.12821v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12604v1",
      "title": "Differentially Private Two-Stage Empirical Risk Minimization and Applications to Individualized Treatment Rule",
      "abstract": "Differential Privacy (DP) provides a rigorous framework for deriving privacy-preserving estimators by injecting calibrated noise to mask individual contributions while preserving population-level insights. Its central challenge lies in the privacy-utility trade-off: calibrating noise levels to ensure robust protection without compromising statistical performance. Standard DP methods struggle with a particular class of two-stage problems prevalent in individualized treatment rules (ITRs) and causal inference. In these settings, data-dependent weights are first computed to satisfy distributional constraints, such as covariate balance, before the final parameter of interest is estimated. Current DP approaches often privatize stages independently, which either degrades weight efficacy-leading to biased and inconsistent estimates-or introduces excessive noise to account for worst-case scenarios.   To address these challenges, we propose the Differentially Private Two-Stage Empirical Risk Minimization (DP-2ERM), a framework that injects a carefully calibrated noise only into the second stage while maintaining privacy for the entire pipeline and preserving the integrity of the first stage weights. Our theoretical contributions include deterministic bounds on weight perturbations across various widely used weighting methods, and probabilistic bounds on sensitivity for the final estimator. Simulations and real-world applications in ITR demonstrate that DP-2ERM significantly enhances utility over existing methods while providing rigorous privacy guarantees.",
      "authors": [
        "Joowon Lee",
        "Guanhua Chen"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "stat.ML"
      ],
      "published": "2026-02-13 04:22:46+00:00",
      "link": "https://arxiv.org/pdf/2602.12604v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12589v1",
      "title": "Berry-Esseen Bounds and Moderate Deviations for Catoni-Type Robust Estimation",
      "abstract": "A powerful robust mean estimator introduced by Catoni (2012) allows for mean estimation of heavy-tailed data while achieving the performance characteristics of classical mean estimator for sub-Gaussian data. While Catoni's framework has been widely extended across statistics, stochastic algorithms, and machine learning, fundamental asymptotic questions regarding the Central Limit Theorem and rare event deviations remain largely unaddressed. In this paper, we investigate Catoni-type robust estimators in two contexts: (i) mean estimation for heavy-tailed data, and (ii) linear regression with heavy-tailed innovations. For the first model, we establish the Berry--Esseen bound and moderate deviation principles, addressing both known and unknown variance settings. For the second model, we demonstrate that the associated estimator is consistent and satisfies a multi-dimensional Berry-Esseen bound.",
      "authors": [
        "Zhijun Cai",
        "Xiang Li",
        "Lihu Xu"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST"
      ],
      "published": "2026-02-13 04:02:59+00:00",
      "link": "https://arxiv.org/pdf/2602.12589v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12483v1",
      "title": "Quantile Randomized Kaczmarz Algorithm with Whitelist Trust Mechanism",
      "abstract": "Randomized Kaczmarz (RK) is a simple and fast solver for consistent overdetermined systems, but it is known to be fragile under noise. We study overdetermined $m\\times n$ linear systems with a sparse set of corrupted equations, $ {\\bf A}{\\bf x}^\\star = {\\bf b}, $where only $\\tilde{\\bf b} = {\\bf b} + \\boldsymbol{\\varepsilon}$ is observed with $\\|\\boldsymbol{\\varepsilon}\\|_0 \\le βm$. The recently introduced QuantileRK (QRK) algorithm addresses this issue by testing residuals against a quantile threshold, but computing a per-iteration quantile across many rows is costly. In this work we (i) reanalyze QRK and show that its convergence rate improves monotonically as the corruption fraction $β$ decreases; (ii) propose a simple online detector that flags and removes unreliable rows, which reduces the effective $β$ and speeds up convergence; and (iii) make the method practical by estimating quantiles from a small random subsample of rows, preserving robustness while lowering the per-iteration cost. Simulations on imaging and synthetic data demonstrate the efficiency of the proposed method.",
      "authors": [
        "Sofiia Shvaiko",
        "Longxiu Huang",
        "Elizaveta Rebrova"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA",
        "stat.ME"
      ],
      "published": "2026-02-12 23:46:51+00:00",
      "link": "https://arxiv.org/pdf/2602.12483v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12365v1",
      "title": "A versatile FEM framework with native GPU scalability via globally-applied AD",
      "abstract": "Energy-based finite-element formulations provide a unified framework for describing complex physical systems in computational mechanics. In these energy-based methods, the governing equations can be obtained directly by considering the derivatives of a single global energy functional. While Automatic Differentiation (AD) can be used to automate the generation of these derivatives, current frameworks face a clear trade-off based primarily on the scale upon which the AD method is applied. Globally applied AD offers high expressivity but cannot currently be scaled to large problems. Locally applied AD scales well through traditional assembly methods, but the variety of physics and couplings that the framework can easily represent is more limited than the global approach. Here, we introduce an energy-centric framework tatva (https://github.com/smec-ethz/tatva) that defines the physics of a problem as a single global functional and applies AD globally to generate residual and tangent operators. By leveraging Jacobian-vector products for matrix-free solvers and coloring-based sparse differentiation for materializing sparse tangent stiffness matrices when needed, our flexible design scales linearly with the problem size on GPUs. We demonstrate that our framework can handle large problems (with millions of degrees of freedom) without memory exhaustion. Additionally, it offers a unified, fully differentiable methodology that can address a wide range of problems, including multi-point constraints, mixed-dimensional coupling, and the incorporation of neural networks, while maintaining high performance and scalability on modern GPU architectures.",
      "authors": [
        "Mohit Pundir",
        "Flavio Lorez",
        "David S. Kammer"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA",
        "cond-mat.soft"
      ],
      "published": "2026-02-12 19:51:03+00:00",
      "link": "https://arxiv.org/pdf/2602.12365v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12334v1",
      "title": "Reconstruction of finite Quasi-Probability and Probability from Principles: The Role of Syntactic Locality",
      "abstract": "Quasi-probabilities appear across diverse areas of physics, but their conceptual foundations remain unclear: they are often treated merely as computational tools, and operations like conditioning and Bayes' theorem become ambiguous. We address both issues by developing a principled framework that derives quasi-probabilities and their conditional calculus from structural consistency requirements on how statements are valued across different universes of discourse, understood as finite Boolean algebras of statements.We begin with a universal valuation that assigns definite (possibly complex) values to all statements. The central concept is Syntactic Locality: every universe can be embedded within a larger ambient one, and the universal valuation must behave coherently under such embeddings and restrictions. From a set of structural principles, we prove a representation theorem showing that every admissible valuation can be re-expressed as a finitely additive measure on mutually exclusive statements, mirroring the usual probability sum rule. We call such additive representatives pre-probabilities. This representation is unique up to an additive regraduation freedom. When this freedom can be fixed canonically, pre-probabilities reduce to finite quasi-probabilities, thereby elevating quasi-probability theory from a computational device to a uniquely determined additive representation of universal valuations. Classical finite probabilities arise as the subclass of quasi-probabilities stable under relativisation, i.e., closed under restriction to sub-universes. Finally, the same framework enables us to define a coherent theory of conditionals, yielding a well-defined generalized Bayes' theorem applicable to both pre-probabilities and quasi-probabilities. We conclude by discussing additional regularity conditions, including the role of rational versus irrational probabilities in this setting.",
      "authors": [
        "Jacopo Surace"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "math.LO",
        "math.PR",
        "math.ST"
      ],
      "published": "2026-02-12 19:00:08+00:00",
      "link": "https://arxiv.org/pdf/2602.12334v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12234v1",
      "title": "Batch-based Bayesian Optimal Experimental Design in Linear Inverse Problems",
      "abstract": "Experimental design is central to science and engineering. A ubiquitous challenge is how to maximize the value of information obtained from expensive or constrained experimental settings. Bayesian optimal experimental design (OED) provides a principled framework for addressing such questions. In this paper, we study experimental design problems such as the optimization of sensor locations over a continuous domain in the context of linear Bayesian inverse problems. We focus in particular on batch design, that is, the simultaneous optimization of multiple design variables, which leads to a notoriously difficult non-convex optimization problem. We tackle this challenge using a promising strategy recently proposed in the frequentist setting, which relaxes A-optimal design to the space of finite positive measures. Our main contribution is the rigorous identification of the Bayesian inference problem corresponding to this relaxed A-optimal OED formulation. Moreover, building on recent work, we develop a Wasserstein gradient-flow -based optimization algorithm for the expected utility and introduce novel regularization schemes that guarantee convergence to an empirical measure. These theoretical results are supported by numerical experiments demonstrating both convergence and the effectiveness of the proposed regularization strategy.",
      "authors": [
        "Sofia Mäkinen",
        "Andrew B. Duncan",
        "Tapio Helin"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "math.OC"
      ],
      "published": "2026-02-12 18:11:38+00:00",
      "link": "https://arxiv.org/pdf/2602.12234v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12114v1",
      "title": "Matrix bordering structure of the Faddeev-Jackiw algorithm: Schur complement regularization and symbolic automation",
      "abstract": "We show that the iterative Faddeev-Jackiw (FJ) reduction for singular Lagrangian systems constitutes a geometrically constrained instance of the Matrix Bordering Technique (MBT). For a first-order Lagrangian with singular pre-symplectic form, each iteration of the Barcelos-Neto-Wotzasek algorithm produces an extended symplectic matrix of canonical bordered form, \\begin{eqnarray}   f^{(m)} = \\left( \\begin{matrix}   f^{(0)} & B \\\\ -B^{\\mathsf{T}} & 0 \\end{matrix} \\right) \\end{eqnarray} where the bordering block $B$ is determined by the gradients of the consistency constraints. We prove that the nondegeneracy of the extended matrix is governed by the corresponding Schur complement, which is algebraically isomorphic to the Poisson bracket matrix of constraints. As a consequence, the Faddeev-Jackiw algorithm terminates if and only if the constraint algebra is nondegenerate, i.e., when the constraints form a second-class system. This algebraic characterization provides a rigorous foundation for automating the Faddeev-Jackiw procedure symbolically. We present a fully symbolic implementation in the Wolfram Language, and validate the approach on representative mechanical systems with nontrivial constraint structure. The resulting rule-based engine preserves parametric dependencies throughout the reduction, enabling reliable analysis of degeneracy, structural stability (when no bifurcations occur), and possible bifurcation scenarios as critical parameters are varied.",
      "authors": [
        "E. Chan-López",
        "A. Martín-Ruiz",
        "Jaime Manuel Cabrera",
        "Jorge Mauricio Paulin Fuentes"
      ],
      "primary_category": "math-ph",
      "categories": [
        "math-ph"
      ],
      "published": "2026-02-12 16:04:59+00:00",
      "link": "https://arxiv.org/pdf/2602.12114v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11947v1",
      "title": "Mixed-Integer Programming for Change-point Detection",
      "abstract": "We present a new mixed-integer programming (MIP) approach for offline multiple change-point detection by casting the problem as a globally optimal piecewise linear (PWL) fitting problem. Our main contribution is a family of strengthened MIP formulations whose linear programming (LP) relaxations admit integral projections onto the segment assignment variables, which encode the segment membership of each data point. This property yields provably tighter relaxations than existing formulations for offline multiple change-point detection. We further extend the framework to two settings of active research interest: (i) multidimensional PWL models with shared change-points, and (ii) sparse change-point detection, where only a subset of dimensions undergo structural change. Extensive computational experiments on benchmark real-world datasets demonstrate that the proposed formulations achieve reductions in solution times under both $\\ell_1$ and $\\ell_2$ loss functions in comparison to the state-of-the-art.",
      "authors": [
        "Apoorva Narula",
        "Santanu S. Dey",
        "Yao Xie"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "stat.ML"
      ],
      "published": "2026-02-12 13:43:56+00:00",
      "link": "https://arxiv.org/pdf/2602.11947v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11926v1",
      "title": "Optimal Quantization for Nonuniform Densities on Spherical Curves",
      "abstract": "We present an analysis of optimal quantization of probability measures with nonuniform densities on spherical curves. We begin by deriving the centroid condition, followed by a high-resolution asymptotic analysis to establish the point-density formula. We further quantify the asymptotic error formula for the nonuniform densities. We apply these theorems to the von Mises distributions and characterize the optimal condition. We also provide applications using the high-resolution asymptotic and its corresponding error formula. Our results can be used in geometric probability theory and quantization theory of spherical curves.",
      "authors": [
        "Silpi Saha",
        "Sangita Jha",
        "Mrinal Kanti Roychowdhury"
      ],
      "primary_category": "math.PR",
      "categories": [
        "math.PR",
        "math.OC"
      ],
      "published": "2026-02-12 13:24:51+00:00",
      "link": "https://arxiv.org/pdf/2602.11926v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11859v1",
      "title": "Tree Capacity and Splitting Isometries for Subinvariant Kernels",
      "abstract": "Starting from a subinvariant positive definite kernel under a branching pullback, we attach to the resulting kernel tower a canonical electrical network on the word tree whose edge weights are the diagonal increments. This converts diagonal growth into effective resistance and capacity, giving explicit criteria and quantitative bounds, together with a matching upper bound under a mild level regularity condition. When the diagonal tower has finite limit at a point, we prove convergence of the full kernels and obtain an invariant completion with a minimality property. We also describe the associated RKHS splitting and a boundary martingale construction leading to weighted invariant majorants.",
      "authors": [
        "James Tian"
      ],
      "primary_category": "math.PR",
      "categories": [
        "math.PR",
        "math.FA"
      ],
      "published": "2026-02-12 12:06:00+00:00",
      "link": "https://arxiv.org/pdf/2602.11859v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11849v1",
      "title": "Data-driven discovery of chemical reaction networks",
      "abstract": "We propose a unified framework that allows for the full mechanistic reconstruction of chemical reaction networks (CRNs) from concentration data. The framework utilizes an integral formulation of the differential equations governing the chemical reactions, followed by an automatic procedure to recover admissible mass-action mechanisms from the equations. We provide theoretical justification for the use of integral formulations using analytical and numerical error bounds. The integral formulation is demonstrated to offer superior robustness to noise and improved accuracy in both rate-law and graph recovery when compared to other commonly used formulations. Together, our developments advance the goal of fully automated, data-driven chemical mechanism discovery.",
      "authors": [
        "Abraham Reyes-Velazquez",
        "Stefan Güttel",
        "Igor Larrosa",
        "Jonas Latz"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-02-12 11:41:42+00:00",
      "link": "https://arxiv.org/pdf/2602.11849v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11827v1",
      "title": "The partial gossip problem revisited",
      "abstract": "We present correct proof of G. Chung, Y.-J. Tsay result on partial gossip problem.",
      "authors": [
        "Konstantin Kokhas",
        "Olga Bursian"
      ],
      "primary_category": "math.CO",
      "categories": [
        "math.CO"
      ],
      "published": "2026-02-12 11:11:46+00:00",
      "link": "https://arxiv.org/pdf/2602.11827v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11770v1",
      "title": "An objective-function-free algorithm for general smooth constrained optimization",
      "abstract": "A new algorithm for smooth constrained optimization is proposed that never computes the value of the problem's objective function and that handles both equality and inequality constraints. The algorithm uses an adaptive switching strategy between a normal step aiming at reducing constraint's infeasibility and a tangential step improving dual optimality, the latter being inspired by the AdaGrad-norm method. Its worst-case iteration complexity is analyzed, showing that the norm of the gradients generated converges to zero like O(1/\\sqrt{k+1}) for problems with full-rank Jacobians. Numerical experiments show that the algorithm's performance is remarkably insensitive to noise in the objective function's gradient.",
      "authors": [
        "S. Bellavia",
        "S. Gratton",
        "B. Morini",
        "Ph. L. Toint"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-12 09:50:32+00:00",
      "link": "https://arxiv.org/pdf/2602.11770v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11747v1",
      "title": "High-Probability Minimax Adaptive Estimation in Besov Spaces via Online-to-Batch",
      "abstract": "We study nonparametric regression over Besov spaces from noisy observations under sub-exponential noise, aiming to achieve minimax-optimal guarantees on the integrated squared error that hold with high probability and adapt to the unknown noise level. To this end, we propose a wavelet-based online learning algorithm that dynamically adjusts to the observed gradient noise by adaptively clipping it at an appropriate level, eliminating the need to tune parameters such as the noise variance or gradient bounds. As a by-product of our analysis, we derive high-probability adaptive regret bounds that scale with the $\\ell_1$-norm of the competitor. Finally, in the batch statistical setting, we obtain adaptive and minimax-optimal estimation rates for Besov spaces via a refined online-to-batch conversion. This approach carefully exploits the structure of the squared loss in combination with self-normalized concentration inequalities.",
      "authors": [
        "Paul Liautaud",
        "Pierre Gaillard",
        "Olivier Wintenberger"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "stat.ML"
      ],
      "published": "2026-02-12 09:24:08+00:00",
      "link": "https://arxiv.org/pdf/2602.11747v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11594v2",
      "title": "Composite Optimization using Local Models and Global Approximations",
      "abstract": "This work presents a unified framework that combines global approximations with locally built models to handle challenging nonconvex and nonsmooth composite optimization problems, including cases involving extended real-valued functions. We show that near-stationary points of the approximating problems converge to stationary points of the original problem under suitable conditions. Building on this, we develop practical algorithms that use tractable convex master programs derived from local models of the approximating problems. The resulting double-loop structure improves global approximations while adapting local models, providing a flexible and implementable approach for a wide class of composite optimization problems. It also lays the groundwork for new algorithmic developments in this domain.",
      "authors": [
        "Welington de Oliveira",
        "Johannes O. Royset"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-12 05:22:30+00:00",
      "link": "https://arxiv.org/pdf/2602.11594v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11566v1",
      "title": "On a group of invariances in a class of functions",
      "abstract": "A class of parametric functions formed by alternating compositions of multivariate polynomials and rectification style monomial maps is studied (the layer-wise exponents are treated as fixed hyperparameters and are not optimized). For this family, nontrivial parametric invariances are identified and characterized, i.e., distinct parameter settings that induce identical input-output maps. A constructive description of the invariance structure is provided, enabling sparse function representations, parameter obfuscation, and potential dimensionality reduction for optimization.",
      "authors": [
        "Shravan Mohan"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-12 04:40:48+00:00",
      "link": "https://arxiv.org/pdf/2602.11566v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11515v1",
      "title": "Algorithms and Differential Game Representations for Exploring Nonconvex Pareto Fronts in High Dimensions",
      "abstract": "We develop a new Hamiton-Jacobi (HJ) and differential game approach for exploring the Pareto front of (constrained) multi-objective optimization (MOO) problems. Given a preference function, we embed the scalarized MOO problem into the value function of a parameterized zero-sum game, whose upper value solves a first-order HJ equation that admits a Hopf-Lax representation formula. For each parameter value, this representation yields an inner minimizer that can be interpreted as an approximate solution to a shifted scalarization of the original MOO problem. Under mild assumptions, the resulting family of solutions maps to a dense subset of the weak Pareto front. Finally, we propose a primal-dual algorithm based on this approach for solving the corresponding optimality system. Numerical experiments show that our algorithm mitigates the curse of dimensionality (scaling polynomially with the dimension of the decision and objective spaces) and is able to expose continuous curves along nonconvex Pareto fronts in 100D in just $\\sim$100 seconds.",
      "authors": [
        "Shanqing Liu",
        "Paula Chen",
        "Youngkyu Lee",
        "Jerome Darbon"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-12 03:16:48+00:00",
      "link": "https://arxiv.org/pdf/2602.11515v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11511v1",
      "title": "Representation Learning with Blockwise Missingness and Signal Heterogeneity",
      "abstract": "Unified representation learning for multi-source data integration faces two important challenges: blockwise missingness and blockwise signal heterogeneity. The former arises from sources observing different, yet potentially overlapping, feature sets, while the latter involves varying signal strengths across subject groups and feature sets. While existing methods perform well with fully observed data or uniform signal strength, their performance degenerates when these two challenges coincide, which is common in practice. To address this, we propose Anchor Projected Principal Component Analysis (APPCA), a general framework for representation learning with structured blockwise missingness that is robust to signal heterogeneity. APPCA first recovers robust group-specific column spaces using all observed feature sets, and then aligns them by projecting shared \"anchor\" features onto these subspaces before performing PCA. This projection step induces a significant denoising effect. We establish estimation error bounds for embedding reconstruction through a fine-grained perturbation analysis. In particular, using a novel spectral slicing technique, our bound eliminates the standard dependency on the signal strength of subject embeddings, relying instead solely on the signal strength of integrated feature sets. We validate the proposed method through extensive simulation studies and an application to multimodal single-cell sequencing data.",
      "authors": [
        "Ziqi Liu",
        "Ye Tian",
        "Weijing Tang"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "published": "2026-02-12 03:12:21+00:00",
      "link": "https://arxiv.org/pdf/2602.11511v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11290v2",
      "title": "Entropic vector quantile regression: Duality and Gaussian case",
      "abstract": "Vector quantile regression (VQR) is an optimal transport (OT) problem subject to a mean-independence constraint that extends classical linear quantile regression to vector response variables. Motivated by computational considerations, prior work has considered entropic relaxation of VQR, but its fundamental structural and approximation properties are still much less understood than entropic OT. The goal of this paper is to address some of these gaps. First, we study duality theory for entropic VQR and establish strong duality and dual attainment for marginals with possibly unbounded supports. In addition, when all marginals are compactly supported, we show that dual potentials are real analytic. Second, building on our duality theory, when all marginals are Gaussian, we show that entropic VQR has a closed-form optimal solution, which is again Gaussian, and establish the precise approximation rate toward unregularized VQR.",
      "authors": [
        "Kengo Kato",
        "Boyu Wang"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "math.OC"
      ],
      "published": "2026-02-11 19:08:25+00:00",
      "link": "https://arxiv.org/pdf/2602.11290v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11289v1",
      "title": "Wasserstein-enabled characterization of designs and myopic decisions in Bayesian Optimization",
      "abstract": "Impractical assumptions, an inherently myopic nature, and the crucial role of the initial design, all together contribute to making theoretical convergence proofs of little value in real-life Bayesian Optimization applications. In this paper, we propose a novel characterization of the design depending on its distributional properties, separately measured with respect to the coverage of the search space and the concentration around the best observed function value. These measures are based on the Wasserstein distance and enable a model-free evaluation of the information value of the design before deciding the next query. Then, embracing the myopic nature of Bayesian Optimization, we take an empirical approach to analyze the relation between the proposed characterization of the design and the quality of the next query. Ultimately, we provide important and useful insights that might inspire the definition of a new generation of acquisition functions in Bayesian Optimization.",
      "authors": [
        "Antonio Candelieri",
        "Francesco Archetti"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-11 19:08:25+00:00",
      "link": "https://arxiv.org/pdf/2602.11289v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11080v1",
      "title": "Constrained Fiducial Inference for Gaussian Models",
      "abstract": "We propose a new fiducial Markov Chain Monte Carlo (MCMC) method for fitting parametric Gaussian models. We utilize the Cayley transform to decompose the parametric covariance matrix, which in turn allows us to formulate a general data generating algorithm for Gaussian data. Leveraging constrained generalized fiducial inference, we are able to create the basis of an MCMC algorithm, which can be specified to parametric models with minimal effort. The appeal of this novel approach is the wide class of models which it permits, ease of implementation and the posterior-like fiducial distribution without the need for a prior. We provide background information for the derivation of the relevant fiducial quantities, and a proof that the proposed MCMC algorithm targets the correct fiducial distribution. We need not assume independence nor identical distribution of the data, which makes the method attractive for application to time series and spatial data. Well-performing simulation results of the MA(1) and Matérn models are presented.",
      "authors": [
        "Hank Flury",
        "Jan Hannig",
        "Richard Smith"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "published": "2026-02-11 17:45:55+00:00",
      "link": "https://arxiv.org/pdf/2602.11080v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10936v1",
      "title": "Trajectory-based data-driven predictive control and the state-space predictor",
      "abstract": "We define trajectory predictive control (TPC) as a family of output-feedback indirect data-driven predictive control (DDPC) methods that represent the output trajectory of a discrete-time system as a linear function of the recent input/output history and the planned input trajectory. This paper shows that for different choices of the trajectory predictor, TPC encompasses a wide variety of DDPC methods, including subspace predictive control (SPC), closed-loop SPC, $γ$-DDPC, causal-$γ$-DDPC, transient predictive control, and others. This paper introduces a trajectory predictor that corresponds to a linear state-space model with the recent input/output history as the state. With this state-space predictor, TPC is a special case of linear model predictive control and therefore inherits its mature theory. In numerical experiments, TPC performance approaches the limit of oracle $H_2$-optimal control with perfect knowledge of the underlying system model. For TPC with small training datasets, the state-space predictor outperforms other predictors because it has fewer parameters.",
      "authors": [
        "Levi D. Reyes Premer",
        "Arash J. Khabbazi",
        "Kevin J. Kircher"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "math.OC"
      ],
      "published": "2026-02-11 15:16:35+00:00",
      "link": "https://arxiv.org/pdf/2602.10936v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10838v1",
      "title": "Mirror descent actor-critic methods for entropy regularised MDPs in general spaces: stability and convergence",
      "abstract": "We provide theoretical guarantees for convergence of discrete-time policy mirror descent with inexact advantage functions updated using temporal difference (TD) learning for entropy regularised MDPs in Polish state and action spaces. We rigorously derive sufficient conditions under which the single-loop actor-critic scheme is stable and convergent. To weaken these conditions, we introduce a variant that performs multiple TD steps per policy update and derive an explicit lower bound on the number of TD steps required to ensure stability. Finally, we establish sub-linear convergence when the number of TD steps grows logarithmically with the number of policy updates, and linear convergence when it grows linearly under a concentrability assumption.",
      "authors": [
        "Denis Zorba",
        "David Šiška",
        "Lukasz Szpruch"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-11 13:24:07+00:00",
      "link": "https://arxiv.org/pdf/2602.10838v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10786v1",
      "title": "Why summation by parts is not enough",
      "abstract": "We investigate the construction and performance of summation-by-parts (SBP) operators, which offer a powerful framework for the systematic development of structure-preserving numerical discretizations of partial differential equations. Previous approaches for the construction of SBP operators have usually relied on either local methods or sparse differentiation matrices, as commonly used in finite difference schemes. However, these methods often impose implicit requirements that are not part of the formal SBP definition. We demonstrate that adherence to the SBP definition alone does not guarantee the desired accuracy, and we identify conditions for SBP operators to achieve both accuracy and stability. Specifically, we analyze the error minimization for an augmented basis, discuss the role of sparsity, and examine the importance of nullspace consistency in the construction of SBP operators. Furthermore, we show how these design criteria can be integrated into a recently proposed optimization-based construction procedure for function space SBP (FSBP) operators on arbitrary grids. Our findings are supported by numerical experiments that illustrate the improved accuracy for the numerical solution using the proposed SBP operators.",
      "authors": [
        "Jan Glaubitz",
        "Armin Iske",
        "Joshua Lampert",
        "Philipp Öffner"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-02-11 12:20:34+00:00",
      "link": "https://arxiv.org/pdf/2602.10786v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10697v1",
      "title": "Fast and Large-Scale Unbalanced Optimal Transport via its Semi-Dual and Adaptive Gradient Methods",
      "abstract": "Unbalanced Optimal Transport (UOT) has emerged as a robust relaxation of standard Optimal Transport, particularly effective for handling outliers and mass variations. However, scalable algorithms for UOT, specifically those based on Gradient Descent (SGD), remain largely underexplored. In this work, we address this gap by analyzing the semi-dual formulation of Entropic UOT and demonstrating its suitability for adaptive gradient methods. While the semi-dual is a standard tool for large-scale balanced OT, its geometry in the unbalanced setting appears ill-conditioned under standard analysis. Specifically, worst-case bounds on the marginal penalties using $χ^2$ divergence suggest a condition number scaling with $n/\\varepsilon$, implying poor scalability. In contrast, we show that the local condition number actually scales as $\\mathcal{O}(1/\\varepsilon)$, effectively removing the ill-conditioned dependence on $n$. Exploiting this property, we prove that SGD methods adapt to this local curvature, achieving a convergence rate of $\\mathcal{O}(n/\\varepsilon T)$ in the stochastic and online regimes, making it suitable for large-scale and semi-discrete applications. Finally, for the full batch discrete setting, we derive a nearly tight upper bound on local smoothness depending solely on the gradient. Using it to adapt step sizes, we propose a modified Adaptive Nesterov Accelerated Gradient (ANAG) method on the semi-dual functional and prove that it achieves a local complexity of $\\mathcal{O}(n^2\\sqrt{1/\\varepsilon}\\ln(1/δ))$.",
      "authors": [
        "Ferdinand Genans"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "stat.ML"
      ],
      "published": "2026-02-11 09:57:30+00:00",
      "link": "https://arxiv.org/pdf/2602.10697v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10580v1",
      "title": "Almost Sure Convergence of Nonlinear Stochastic Approximation: An Interplay of Noise and Step Size",
      "abstract": "We study the almost sure convergence of the Stochastic Approximation algorithm to the fixed point $x^\\star$ of a nonlinear operator under a negative drift condition and a general noise sequence with finite $p$-th moment for some $p > 1$. Classical almost sure convergence results of Stochastic Approximation are mostly analyzed for the square-integrable noise setting, and it is shown that any non-summable but square-summable step size sequence is sufficient to obtain almost sure convergence. However, such a limitation prevents wider algorithmic application. In particular, many applications in Machine Learning and Operations Research admit heavy-tailed noise with infinite variance, rendering such guarantees inapplicable. On the other hand, when a stronger condition on the noise is available, such guarantees on the step size would be too conservative, as practitioners would like to pick a larger step size for a more preferable convergence behavior. To this end, we show that any non-summable but $p$-th power summable step size sequence is sufficient to guarantee almost sure convergence, covering the gap in the literature.   Our guarantees are obtained using a universal Lyapunov drift argument. For the regime $p \\in (1, 2)$, we show that using the Lyapunov function $\\norm{x-x^\\star}^p$ and applying a Taylor-like bound suffice. For $p > 2$, such an approach is no longer applicable, and therefore, we introduce a novel iterate projection technique to control the nonlinear terms produced by high-moment bounds and multiplicative noise. We believe our proof techniques and their implications could be of independent interest and pave the way for finite-time analysis of Stochastic Approximation under a general noise condition.",
      "authors": [
        "Quang Dinh Thien Nguyen",
        "Duc Anh Nguyen",
        "Hoang Huy Nguyen",
        "Siva Theja Maguluri"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "math.PR"
      ],
      "published": "2026-02-11 07:09:41+00:00",
      "link": "https://arxiv.org/pdf/2602.10580v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10566v1",
      "title": "Finite-sample confidence regions for spectral clustering and graph centrality",
      "abstract": "Let a graph be observed through a finite random sampling mechanism. Spectral methods are routinely applied to such graphs, yet their outputs are treated as deterministic objects. This paper develops finite-sample inference for spectral graph procedures.   The primary result constructs explicit confidence regions for latent eigenspaces of graph operators under an explicit sampling model. These regions propagate to confidence regions for spectral clustering assignments and for smooth graph centrality functionals. All bounds are nonasymptotic and depend explicitly on the sample size, noise level, and spectral gap.   The analysis isolates a failure of common practice: asymptotic perturbation arguments are often invoked without a finite-sample spectral gap, leading to invalid uncertainty claims. Under verifiable gap and concentration conditions, the present framework yields coverage guarantees and certified stability regions. Several corollaries address fairness-constrained post-processing and topological summaries derived from spectral embeddings.",
      "authors": [
        "Chandrasekhar Gokavarapu",
        "Sekhar Babu Gosala",
        "Vamis Pasalapudi",
        "Tarakarama Kapakayala"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "math.CO",
        "math.RA",
        "math.SP"
      ],
      "published": "2026-02-11 06:35:08+00:00",
      "link": "https://arxiv.org/pdf/2602.10566v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10470v1",
      "title": "Revisiting Superlinear Convergence of Proximal Newton-Like Methods to Degenerate Solutions",
      "abstract": "We describe inexact proximal Newton-like methods for solving degenerate regularized optimization problems and for the broader problem of   finding a zero of a generalized equation that is the sum of a continuous map and a maximal monotone operator. Superlinear convergence for both the distance to the solution set and a certain measure of first-order optimality can be achieved under a Hölderian error bound condition, including for problems in which the continuous map is nonmonotone, with Jacobian singular at the solution and not Lipschitz. Superlinear convergence is attainable even when the Jacobian is merely uniformly continuous, relaxing the standard Lipschitz assumption to its theoretical limit. For convex regularized optimization problems, we introduce a novel globalization strategy that ensures strict objective decrease and avoids the Maratos effect, attaining local $Q$-superlinear convergence without prior knowledge of problem parameters. Unit step size acceptance in our line search strategy does not rely on continuity or even existence of the Hessian of the smooth term in the objective, making the framework compatible with other potential candidates for superlinearly convergent updates.",
      "authors": [
        "Ching-pei Lee",
        "Stephen J. Wright"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-11 03:17:21+00:00",
      "link": "https://arxiv.org/pdf/2602.10470v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10464v1",
      "title": "Do More Predictions Improve Statistical Inference? Filtered Prediction-Powered Inference",
      "abstract": "Recent advances in artificial intelligence have enabled the generation of large-scale, low-cost predictions with increasingly high fidelity. As a result, the primary challenge in statistical inference has shifted from data scarcity to data reliability. Prediction-powered inference methods seek to exploit such predictions to improve efficiency when labeled data are limited. However, existing approaches implicitly adopt a use-all philosophy, under which incorporating more predictions is presumed to improve inference. When prediction quality is heterogeneous, this assumption can fail, and indiscriminate use of unlabeled data may dilute informative signals and degrade inferential accuracy. In this paper, we propose Filtered Prediction-Powered Inference (FPPI), a framework that selectively incorporates predictions by identifying a data-adaptive filtered region in which predictions are informative for inference. We show that this region can be consistently estimated under a margin condition, achieving fast rates of convergence. By restricting the prediction-powered correction to the estimated filtered region, FPPI adaptively mitigates the impact of biased or noisy predictions. We establish that FPPI attains strictly improved asymptotic efficiency compared with existing prediction-powered inference methods. Numerical studies and a real-data application to large language model evaluation demonstrate that FPPI substantially reduces reliance on expensive labels by selectively leveraging reliable predictions, yielding accurate inference even in the presence of heterogeneous prediction quality.",
      "authors": [
        "Shirong Xu",
        "Will Wei Sun"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "stat.ML"
      ],
      "published": "2026-02-11 03:02:24+00:00",
      "link": "https://arxiv.org/pdf/2602.10464v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10436v1",
      "title": "Active set identification and rapid convergence for degenerate primal-dual problems",
      "abstract": "Primal-dual methods for solving convex optimization problems with functional constraints often exhibit a distinct two-stage behavior. Initially, they converge towards a solution at a sublinear rate. Then, after a certain point, the method identifies the set of active constraints and the convergence enters a faster local linear regime. Theory characterizing this phenomenon spans over three decades. However, most existing work only guarantees eventual identification of the active set and relies heavily on nondegeneracy conditions, such as strict complementarity, which often fail to hold in practice. We characterize mild conditions on the problem geometry and the algorithm under which this phenomenon provably occurs. Our guarantees are entirely nonasymptotic and, importantly, do not rely on strict complementarity. Our framework encompasses several widely-used algorithms, including the proximal point method, the primal-dual hybrid gradient method, the alternating direction method of multipliers, and the extragradient method.",
      "authors": [
        "Mateo Díaz",
        "Pedro Izquierdo Lehmann",
        "Haihao Lu",
        "Jinwen Yang"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-11 02:26:53+00:00",
      "link": "https://arxiv.org/pdf/2602.10436v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10424v1",
      "title": "A Numerical Analysis of Sketched Linear Squares Problems and Stopping Criteria for Iterative Solvers",
      "abstract": "Randomized subspace embedding methods have had a great impact on the solution of a linear least squares (LS) problem by reducing its row dimension, leading to a randomized or sketched LS (sLS) problem, and use the solution of the sLS problem as an approximate solution of the LS problem. This work makes a numerical analysis on the sLS problem, establishes its numerous theoretical properties, and show their crucial roles on the most effective and efficient use of iterative solvers. We first establish a compact bound on the norm of the residual difference between the solutions of the LS and sLS problems, which is the first key result towards understanding the rationale of the sLS problem. Then from the perspective of backward errors, we prove that the solution of the sLS problem is the one of a certain perturbed LS problem with minimal backward error, and quantify how the embedded quality affects the residuals, solution errors, and the relative residual norms of normal equations of the LS and sLS problems. These theoretical results enable us to propose new novel and reliable general-purpose stopping criteria for iterative solvers for the sLS problem, which dynamically monitor stabilization patterns of iterative solvers for the LS problem itself and terminate them at the earliest iteration. Numerical experiments justify the theoretical bounds and demonstrate that the new stopping criteria work reliably and result in a tremendous reduction in computational cost without sacrificing attainable accuracy.",
      "authors": [
        "Zhongxiao Jia",
        "Xinyuan Wan"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-02-11 02:09:47+00:00",
      "link": "https://arxiv.org/pdf/2602.10424v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10374v1",
      "title": "Relationships between full-space and subspace quadratic interpolation models and simplex derivatives",
      "abstract": "Quadratic interpolation models and simplex derivatives are fundamental tools in numerical optimization, particularly in derivative-free optimization. When constructed in suitably chosen affine subspaces, these tools have been shown to be especially effective for high-dimensional derivative-free optimization problems, where full-space model construction is often impractical. In this paper, we analyze the relationships between full-space and subspace formulations of these tools. In particular, we derive explicit conversion formulas between full-space and subspace models, including minimum-norm models, minimum Frobenius norm models, least Frobenius norm updating models, as well as models constructed via generalized simplex gradients and Hessians. We show that the full-space and subspace models coincide on the affine subspace and, in general, along directions in the orthogonal complement. Overall, our results provide a theoretical framework for understanding subspace approximation techniques and offer insight into the design and analysis of derivative-free optimization methods.",
      "authors": [
        "Yiwen Chen"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "math.NA"
      ],
      "published": "2026-02-10 23:48:22+00:00",
      "link": "https://arxiv.org/pdf/2602.10374v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10274v1",
      "title": "Asymptotic equivalence for nonparametric additive regression",
      "abstract": "We prove asymptotic equivalence of nonparametric additive regression and an appropriate Gaussian white noise experiment in which a multidimensional shifted Wiener process is observed, whose dimension equals the number of additive components. The shift depends on the additive components of the regression function and solely the one- and two-dimensional marginal distributions of the covariates via an explicitly specified bounded but non-compact linear operator~$Γ$. The number of additive components $d$ is allowed to increase moderately with respect to the sample size. In the special case of pairwise independent components of the covariates, the white noise model decomposes into $d$ independent univariate processes. Moreover, we study approximation in some semiparametric setting where $Γ$ splits into a multiplication operator and an asymptotically negligible Hilbert-Schmidt operator.",
      "authors": [
        "Moritz Jirak",
        "Alexander Meister",
        "Angelika Rohde"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST"
      ],
      "published": "2026-02-10 20:35:34+00:00",
      "link": "https://arxiv.org/pdf/2602.10274v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10247v1",
      "title": "Discretization-free Bayesian inverse problems in distribution spaces",
      "abstract": "The Bayesian approach to inverse problems provides a practical way to solve ill-posed problems by augmenting the observation model with prior information. Due to the measure-theoretic underpinnings, the approach has raised theoretical interest, leading to a rather comprehensive description in infinite-dimensional function spaces. The goal of this article is to bridge the infinite-dimensional theory for linear inverse problems in distribution spaces and associated computational inverse problems without resorting to a discrete approximation of the forward model. We will shown that under certain assumptions, discretization of the unknown of interest is not necessary for the numerical treatment of the problem, the only approximations required being numerical quadratures that are independent of any discrete representation of the unknown. An analysis of the connection between the proposed approach and discretization-based ones is also provided.",
      "authors": [
        "Daniela Calvetti",
        "Erkki Somersalo"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-02-10 19:41:27+00:00",
      "link": "https://arxiv.org/pdf/2602.10247v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10103v1",
      "title": "Minimax properties of gamma kernel density estimators under $L^p$ loss and $β$-Hölder smoothness of the target",
      "abstract": "This paper considers the asymptotic behavior in $β$-Hölder spaces, and under $L^p$ loss, of the gamma kernel density estimator introduced by Chen [Ann. Inst. Statist. Math. 52 (2000), 471-480] for the analysis of nonnegative data, when the target's support is assumed to be upper bounded. It is shown that this estimator can achieve the minimax rate asymptotically for a suitable choice of bandwidth whenever $(p,β)\\in [1,3)\\times(0,2]$ or $(p,β)\\in [3,4)\\times ((p-3)/(p-2),2]$. It is also shown that this estimator cannot be minimax when either $p\\in [4,\\infty)$ or $β\\in (2,\\infty)$.",
      "authors": [
        "Frédéric Ouimet"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "math.PR"
      ],
      "published": "2026-02-10 18:58:29+00:00",
      "link": "https://arxiv.org/pdf/2602.10103v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10022v1",
      "title": "Acceleration for Polyak-Łojasiewicz Functions with a Gradient Aiming Condition",
      "abstract": "It is known that when minimizing smooth Polyak-Łojasiewicz (PL) functions, momentum algorithms cannot significantly improve the convergence bound of gradient descent, contrasting with the acceleration phenomenon occurring in the strongly convex case. To bridge this gap, the literature has proposed strongly quasar-convex functions as an intermediate non-convex class, for which accelerated bounds have been suggested to persist. We show that this is not true in general: the additional structure of strong quasar-convexity does not suffice to guaranty better worst-case bounds for momentum compared to gradient descent. As an alternative, we study PL functions under an aiming condition that measures how well the descent direction points toward a minimizer. This perspective clarifies the geometric ingredient enabling provable acceleration by momentum when minimizing PL functions.",
      "authors": [
        "Julien Hermant"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-10 17:42:38+00:00",
      "link": "https://arxiv.org/pdf/2602.10022v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10018v1",
      "title": "Online Selective Conformal Prediction with Asymmetric Rules: A Permutation Test Approach",
      "abstract": "Selective conformal prediction aims to construct prediction sets with valid coverage for a test unit conditional on it being selected by a data-driven mechanism. While existing methods in the offline setting handle any selection mechanism that is permutation invariant to the labeled data, their extension to the online setting -- where data arrives sequentially and later decisions depend on earlier ones -- is challenged by the fact that the selection mechanism is naturally asymmetric. As such, existing methods only address a limited collection of selection mechanisms.   In this paper, we propose PErmutation-based Mondrian Conformal Inference (PEMI), a general permutation-based framework for selective conformal prediction with arbitrary asymmetric selection rules. Motivated by full and Mondrian conformal prediction, PEMI identifies all permutations of the observed data (or a Monte-Carlo subset thereof) that lead to the same selection event, and calibrates a prediction set using conformity scores over this selection-preserving reference set. Under standard exchangeability conditions, our prediction sets achieve finite-sample exact selection-conditional coverage for any asymmetric selection mechanism and any prediction model. PEMI naturally incorporates additional offline labeled data, extends to selection mechanisms with multiple test samples, and achieves FCR control with fine-grained selection taxonomies. We further work out several efficient instantiations for commonly-used online selection rules, including covariate-based rules, conformal p/e-values-based procedures, and selection based on earlier outcomes. Finally, we demonstrate the efficacy of our methods across various selection rules on a real drug discovery dataset and investigate their performance via simulations.",
      "authors": [
        "Mingyi Zheng",
        "Ying Jin"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.ML"
      ],
      "published": "2026-02-10 17:39:36+00:00",
      "link": "https://arxiv.org/pdf/2602.10018v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09996v2",
      "title": "Learning to Choose Branching Rules for Nonconvex MINLPs",
      "abstract": "Outer-approximation-based branch-and-bound is a common algorithmic framework for solving MINLPs (mixed-integer nonlinear programs) to global optimality, with branching variable selection critically influencing overall performance. In modern global MINLP solvers, it is unclear whether branching on fractional integer variables should be prioritized over spatial branching on variables, potentially continuous, that show constraint violations, with different solvers following different defaults. We address this question using a data-driven approach. Based on a test set of hundreds of heterogeneous public and industrial MINLP instances, we train linear and random forest regression models to predict the relative speedup of the FICO(R) Xpress Global solver when using a branching rule that always prioritizes variables with violated integralities versus a mixed rule, allowing for early spatial branches.   We introduce a practical evaluation methodology that measures the effect of the learned model directly in terms of the shifted geometric mean runtime. Using only four features derived from strong branching and the nonlinear structure, our linear regression model achieves an 8-9% reduction in geometric-mean solving time for the Xpress solver, with over 10% improvement on hard instances. We also analyze a random regression forest model. Experiments across solver versions show that a model trained on Xpress 9.6 still yields significant improvements on Xpress 9.8 without retraining.   Our results demonstrate how regression models can successfully guide the branching-rule selection and improve the performance of a state-of-the-art commercial MINLP solver.",
      "authors": [
        "Timo Berthold",
        "Fritz Geis"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-10 17:20:38+00:00",
      "link": "https://arxiv.org/pdf/2602.09996v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.11205v1",
      "title": "Multi-scale Vandermonde test kernels for spectral trace formulas",
      "abstract": "We construct a family of test kernels for use in spectral trace formulas on locally symmetric spaces. The key innovation is the factorization $h_T = g_T \\star \\widetilde{g}_T$, which simultaneously achieves: (i) automatic positive semi-definiteness of the spectral multiplier $m_{h_T}(π) = |m_{g_T}(π)|^2 \\ge 0$; (ii) $J$-fold moment annihilation via a multi-scale Vandermonde construction, yielding super-polynomial decay of all error terms; (iii) uniform spectral parameter bounds (Master-Bound) $\\mathfrak{E}_{\\mathrm{tot}}(T) \\ll T^{d+1-δ}$ with $δ> 0$ depending only on the symmetry order $k$ and the annihilation depth $J \\asymp \\sqrt{(\\log T)/k}$, representing a power saving over the main term $\\asymp T^{d+1}$. The cost is a controlled polynomial growth $T^{c_0^2/2+o(1)}$ in the Vandermonde coefficients (with exponent strictly less than 1), which is dominated by the super-polynomial decay of the off-diagonal terms. The construction is axiomatized over two analytic hypotheses -- a Weyl law and Bessel/Airy asymptotics -- making it applicable beyond the classical $\\mathrm{GL}(2)$ setting.",
      "authors": [
        "Stefan Horvath"
      ],
      "primary_category": "math.NT",
      "categories": [
        "math.NT"
      ],
      "published": "2026-02-10 17:09:30+00:00",
      "link": "https://arxiv.org/pdf/2602.11205v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09956v1",
      "title": "Elliptic Multiple Polylogarithms with Arbitrary Arguments in \\textsc{GiNaC}",
      "abstract": "We present an algorithm for the numerical evaluation of elliptic multiple polylogarithms for arbitrary arguments and to arbitrary precision. The cornerstone of our approach is a procedure to obtain a convergent $q$-series representation of elliptic multiple polylogarithms. Its coefficients are expressed in terms of ordinary multiple polylogarithms, which can be evaluated efficiently using existing libraries. In a series of preparation steps the elliptic polylogarithms are mapped into a region where the $q$-series converges rapidly. We also present an implementation of our algorithm into the \\texttt{GiNaC} framework. This release constitutes the first public package capable of evaluating elliptic multiple polylogarithms to high precision and for arbitrary values of the arguments.",
      "authors": [
        "Claude Duhr",
        "Florian Lorkowski",
        "Robin Marzucca",
        "Sofia Mauc",
        "Stefan Weinzierl"
      ],
      "primary_category": "hep-ph",
      "categories": [
        "hep-ph",
        "hep-th",
        "math-ph",
        "math.NA"
      ],
      "published": "2026-02-10 16:43:08+00:00",
      "link": "https://arxiv.org/pdf/2602.09956v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09762v1",
      "title": "Asymptotic analysis of the Gaussian kernel matrix for partially noisy data in high dimensions",
      "abstract": "The Gaussian kernel is one of the most important kernels, applicable to many research fields, including scientific computing and data science. In this paper, we present asymptotic analysis of the Gaussian kernel matrix in high dimension under a statistical model of noisy data. The main result is a nice combination of Karoui's asymptotic analysis with procedures of constrained low rank matrix approximations. More specifically, Karouli clarified an important asymptotic structure of the Gaussian kernel matrix, leading to strong consistency of the eigenvectors, though the eigenvalues are inconsistent. This paper focuses on the above results and presents a consistent estimator with the use of the smallest eigenvalue, whenever the target kernel matrix tends to low rank in the asymptotic regime. Importantly, asymptotic analysis is given under a statistical model representing partial noise. Although a naive estimator is inconsistent, applying an optimization method for low rank approximations with constraints, we overcome the difficulty caused by the inconsistency, resulting in a new estimator with strong consistency in rank deficient cases.",
      "authors": [
        "Kensuke Aishima"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "math.NA"
      ],
      "published": "2026-02-10 13:20:32+00:00",
      "link": "https://arxiv.org/pdf/2602.09762v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09619v1",
      "title": "Discrete-time, discrete-state multistate Markov models from the perspective of algebraic statistics",
      "abstract": "We study discrete-time, discrete-state multistate Markov models from the perspective of algebraic statistics. These models are widely studied in event history analysis, and are characterized by the state space, the initial distribution and the transition probabilities. A finite path under the multistate Markov model is a particular set of states occupied at finite time instances $\\{1, \\dots, n\\}$. The main goal of this paper is to establish a bridge between event history analysis and algebraic statistics. The joint probabilities of finite paths in these models have a natural monomial parametrization in terms of the initial distribution and the transition probabilities. We study the polynomial relations among joint path probabilities. When the statistical constraints on the parameters are disregarded, nonhomogeneous multistate Markov models of arbitrary order can be viewed as slices of decomposable hierarchical models. This yields a complete description of their vanishing ideals as toric ideals generated by explicit families of binomials. Moreover, the variety of this vanishing ideal equals the nonhomogeneous multistate Markov model on the probability simplex. In contrast, homogeneous multistate Markov models exhibit different algebraic behavior, as time homogeneity imposes additional polynomial relations, leading to vanishing ideals that are strictly larger than in the nonhomogeneous case. We also derive families of binomial relations that vanish on homogeneous multistate Markov models. We investigate maximum likelihood estimation from statistical and algebraic perspectives. For nonhomogeneous models, classical and algebraic formulas agree; in the homogeneous case, the algebraic approach is more complex. Lastly, we provide data applications where we demonstrate the statistical theory to obtain the maximum likelihood estimates of the parameters under specific multistate Markov models.",
      "authors": [
        "Dario Gasbarra",
        "Kaie Kubjas",
        "Sangita Kulathinal",
        "Nataliia Kushnerchuk",
        "Fatemeh Mohammadi",
        "Etienne Sebag"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "math.AG"
      ],
      "published": "2026-02-10 10:07:43+00:00",
      "link": "https://arxiv.org/pdf/2602.09619v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09539v1",
      "title": "Tensor CUR Decomposition under the Linear-Map-Based Tensor-Tensor Multiplication",
      "abstract": "The factorization of three-dimensional data continues to gain attention due to its relevance in representing and compressing large-scale datasets. The linear-map-based tensor-tensor multiplication is a matrix-mimetic operation that extends the notion of matrix multiplication to higher order tensors, and which is a generalization of the T-product. Under this framework, we introduce the tensor CUR decomposition, show its performance in video foreground-background separation for different linear maps and compare it to a robust matrix CUR decomposition, another tensor approximation and the slice-based singular value decomposition (SS-SVD). We also provide a theoretical analysis of our tensor CUR decomposition, extending classical matrix results to establish exactness conditions and perturbation bounds.",
      "authors": [
        "Susana Lopez-Moreno",
        "June-Ho Lee",
        "Taehyeong Kim"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA"
      ],
      "published": "2026-02-10 08:50:24+00:00",
      "link": "https://arxiv.org/pdf/2602.09539v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09285v1",
      "title": "Submodularity of the expected information gain in infinite-dimensional linear inverse problems",
      "abstract": "We consider infinite-dimensional linear Gaussian Bayesian inverse problems with uncorrelated sensor data, and focus on the problem of finding sensor placements that maximize the expected information gain (EIG). This study is motivated by optimal sensor placement for linear inverse problems constrained by partial differential equations (PDEs). We consider measurement models where each sensor collects a single-snapshot measurement. This covers sensor placement for inverse problems governed by linear steady PDEs or evolution equations with final-in-time observations. It is well-known that in the finite-dimensional (discretized) formulations of such inverse problems, EIG is a monotone submodular function. This also entails a theoretical guarantee for greedy sensor placement in the discretized setting. We extend the result on submodularity of the EIG to the infinite-dimensional setting, proving that the approximation guarantee of greedy sensor placement remains valid in the infinite-dimensional limit. We also discuss computational considerations and present strategies that exploit problem structure and submodularity to yield an efficient implementation of the greedy procedure.",
      "authors": [
        "Alen Alexanderian",
        "Steven Maio"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-10 00:04:37+00:00",
      "link": "https://arxiv.org/pdf/2602.09285v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09219v1",
      "title": "Goodness-of-fit testing for nonlinear inverse problems with random observations",
      "abstract": "This work is concerned with nonparametric goodness-of-fit testing in the context of nonlinear inverse problems with random observations. Bayesian posterior distributions based upon a Gaussian process prior distribution are proven to contract at a certain rate uniformly over a set of true parameters. The corresponding posterior mean is shown to converge uniformly at the posterior contraction rate in the sense of satisfying a concentration inequality. Distinguishability for bounded alternatives separated from a composite null hypothesis at the posterior contraction rate is established using infimum plug-in tests based on the posterior mean and also on maximum a posteriori estimators. The results are applied to a class of inverse problems governed by ordinary differential equation initial value problems that is widely used in pharmacokinetics. For this class, uniform posterior contraction rates are proven and then used to establish distinguishability.",
      "authors": [
        "Remo Kretschmann",
        "Han Cheng Lie"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST"
      ],
      "published": "2026-02-09 21:42:23+00:00",
      "link": "https://arxiv.org/pdf/2602.09219v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16606v1",
      "title": "On Sharpened Convergence Rate of Generalized Sliced Inverse Regression for Nonlinear Sufficient Dimension Reduction",
      "abstract": "Generalized Sliced Inverse Regression (GSIR) is one of the most important methods for nonlinear sufficient dimension reduction. As shown in Li and Song (2017), it enjoys a convergence rate that is independent of the dimension of the predictor, thus avoiding the curse of dimensionality. In this paper we establish an improved convergence rate of GSIR under additional mild eigenvalue decay rate and smoothness conditions. Our convergence rate can be made arbitrarily close to $n^{-1/3}$ under appropriate decay rate and smoothness parameters. As a comparison, the rate of Li and Song (2017) is $n^{-1/4}$ under the best conditions. This improvement is significant because, for example, in a semiparametric estimation problem involving an infinite-dimensional nuisance parameter, the convergence rate of the estimator of the nuisance parameter is often required to be faster than $n^{-1/4}$ to guarantee desired semiparametric properties such as asymptotic efficiency. This can be achieved by the improved convergence rate, but not by the original rate. The sharpened convergence rate can also be established for GSIR in more general settings, such as functional sufficient dimension reduction.",
      "authors": [
        "Chak Fung Choi",
        "Yin Tang",
        "Bing Li"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "stat.ME"
      ],
      "published": "2026-02-18 17:01:53+00:00",
      "link": "https://arxiv.org/pdf/2602.16606v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16259v1",
      "title": "HAL-MLE Log-Splines Density Estimation (Part I: Univariate)",
      "abstract": "We study nonparametric maximum likelihood estimation of probability densities under a total variation (TV) type penalty, sectional variation norm (also named as Hardy-Krause variation). TV regularization has a long history in regression and density estimation, including results on $L^2$ and KL divergence convergence rates. Here, we revisit this task using the Highly Adaptive Lasso (HAL) framework. We formulate a HAL-based maximum likelihood estimator (HAL-MLE) using the log-spline link function from \\citet{kooperberg1992logspline}, and show that in the univariate setting the bounded sectional variation norm assumption underlying HAL coincides with the classical bounded TV assumption. This equivalence directly connects HAL-MLE to existing TV-penalized approaches such as local adaptive splines \\citep{mammen1997locally}. We establish three new theoretical results: (i) the univariate HAL-MLE is asymptotically linear, (ii) it admits pointwise asymptotic normality, and (iii) it achieves uniform convergence at rate $n^{-(k+1)/(2k+3)}$ up to logarithmic factors for the smoothness order $k \\geq 1$. These results extend existing results from \\citet{van2017uniform}, which previously guaranteed only uniform consistency without rates when $k=0$. We will include the uniform convergence for general dimension $d$ in the follow-up work of this paper. The intention of this paper is to provide a unified framework for the TV-penalized density estimation methods, and to connect the HAL-MLE to the existing TV-penalized methods in the univariate case, despite that the general HAL-MLE is defined for multivariate cases.",
      "authors": [
        "Yilong Hou",
        "Zhengpu Zhao",
        "Yi Li",
        "Mark van der Laan"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "stat.CO",
        "stat.ME"
      ],
      "published": "2026-02-18 08:16:05+00:00",
      "link": "https://arxiv.org/pdf/2602.16259v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15822v1",
      "title": "Finite Free Information Inequalities",
      "abstract": "We develop finite free information theory for real-rooted polynomials, establishing finite free analogues of entropy and Fisher information monotonicity, as well as the Stam and entropy power inequalities. These results resolve conjectures by Shlyakhtenko and Gribinski and recover inequalities in free probability in the large-degree limit. Equivalently, our results may be interpreted as potential-theoretic inequalities for the zeros of real-rooted polynomials under differential operators which preserve real-rootedness. Our proofs leverage a new connection between score vectors and Jacobians of root maps, combined with convexity results for hyperbolic polynomials.",
      "authors": [
        "Jorge Garza-Vargas",
        "Nikhil Srivastava",
        "Zachary Stier"
      ],
      "primary_category": "math.PR",
      "categories": [
        "math.PR",
        "math.CA",
        "math.CO",
        "math.OA"
      ],
      "published": "2026-02-17 18:57:07+00:00",
      "link": "https://arxiv.org/pdf/2602.15822v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15559v1",
      "title": "Fixed-Horizon Self-Normalized Inference for Adaptive Experiments via Martingale AIPW/DML with Logged Propensities",
      "abstract": "Adaptive randomized experiments update treatment probabilities as data accrue, but still require an end-of-study interval for the average treatment effect (ATE) at a prespecified horizon. Under adaptive assignment, propensities can keep changing, so the predictable quadratic variation of AIPW/DML score increments may remain random. When no deterministic variance limit exists, Wald statistics normalized by a single long-run variance target can be conditionally miscalibrated given the realized variance regime. We assume no interference, sequential randomization, i.i.d. arrivals, and executed overlap on a prespecified scored set, and we require two auditable pipeline conditions: the platform logs the executed randomization probability for each unit, and the nuisance regressions used to score unit $t$ are constructed predictably from past data only. These conditions make the centered AIPW/DML scores an exact martingale difference sequence. Using self-normalized martingale limit theory, we show that the Studentized statistic, with variance estimated by realized quadratic variation, is asymptotically N(0,1) at the prespecified horizon, even without variance stabilization. Simulations validate the theory and highlight when standard fixed-variance Wald reporting fails.",
      "authors": [
        "Gabriel Saco"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST",
        "stat.ML"
      ],
      "published": "2026-02-17 13:12:31+00:00",
      "link": "https://arxiv.org/pdf/2602.15559v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15390v1",
      "title": "Space-filling lattice designs for computer experiments",
      "abstract": "This paper investigates the construction of space-filling designs for computer experiments. The space-filling property is characterized by the covering and separation radii of a design, which are integrated through the unified criterion of quasi-uniformity. We focus on a special class of designs, known as quasi-Monte Carlo (QMC) lattice point sets, and propose two construction algorithms. The first algorithm generates rank-1 lattice point sets as an approximation of quasi-uniform Kronecker sequences, where the generating vector is determined explicitly. As a byproduct of our analysis, we prove that this explicit point set achieves an isotropic discrepancy of $O(N^{-1/d})$. The second algorithm utilizes Korobov lattice point sets, employing the Lenstra--Lenstra--Lovász (LLL) basis reduction algorithm to identify the generating vector that ensures quasi-uniformity. Numerical experiments are provided to validate our theoretical claims regarding quasi-uniformity. Furthermore, we conduct empirical comparisons between various QMC point sets in the context of Gaussian process regression, showcasing the efficacy of the proposed designs for computer experiments.",
      "authors": [
        "Naoki Sakai",
        "Takashi Goda"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "math.NA",
        "math.NT"
      ],
      "published": "2026-02-17 06:59:39+00:00",
      "link": "https://arxiv.org/pdf/2602.15390v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15365v1",
      "title": "Data Informativeness in Linear Optimization under Uncertainty",
      "abstract": "We study the problem of determining what data is required to solve a decision-making task when only partial information about the state of the world is available. Focusing on linear programs, we introduce a decision-focused notion of data informativeness that formalizes when a data set is sufficient to recover the optimal decision. Our notion abstracts away the notion of estimators (how data is used): it depends solely on the structure of the optimization task and the uncertainty. Our main result provides a geometric characterization of data sufficiency: a data set is sufficient if and only if, together with prior knowledge, it captures all cost directions that can change the optimal solution, given the task structure and the uncertainty set. Building on our characterization, we develop a tractable algorithm to determine minimal sufficient data sets under general data collection constraints. Taken together, our work introduces a principled framework for task-aware data collection. We demonstrate the approach in two applications: selecting where to conduct field experiments to inform infrastructure design and choosing which candidates to interview in order to make an optimal hiring decision. Our results illustrate that small, carefully selected data sets often suffice to determine the optimal decisions.",
      "authors": [
        "Omar Bennouna",
        "Amine Bennouna",
        "Saurabh Amin",
        "Asuman Ozdaglar"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-17 05:20:20+00:00",
      "link": "https://arxiv.org/pdf/2602.15365v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15328v1",
      "title": "Non-Stationary Covariance Functions for Spatial Data on Linear Networks",
      "abstract": "We introduce a novel class of non-stationary covariance functions for random fields on linear networks that allows both the variance and the correlation range of the random field to vary spatially. The proposed covariance functions are useful to model random fields with a spatial dependence that is locally isotropic with respect to the resistance metric, a distance that reflects the topology of the network. We assess the statistical and computational performance of a weighted local likelihood estimator for the proposed models using synthetic data generated on the street network of the University of Chicago neighborhood.",
      "authors": [
        "Alfredo Alegría"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST"
      ],
      "published": "2026-02-17 03:14:45+00:00",
      "link": "https://arxiv.org/pdf/2602.15328v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15297v1",
      "title": "Bayes Risk for Goodness of Fit Tests",
      "abstract": "We develop a unified framework for goodness-of-fit (GOF) testing through the lens of Bayes risk. Classical GOF procedures are commonly calibrated either at fixed significance level (CLT scale) or through exponential error exponents (LDP scale). We establish that Bayes-risk optimal calibration operates on the moderate-deviation (MDP) scale, producing canonical $\\sqrt{\\log n}$ inflation of rejection thresholds and polynomially decaying Type I error.   Our main contributions are: (i) we formalise the Rubin--Sethuraman program for KS-type statistics as a risk-calibration theorem with explicit regularity conditions on priors and empirical-process functionals; (ii) we develop the precise connection between Bayes-risk expansions and Sanov information asymptotics, showing how $\\log n$-order truncations arise naturally when risk, rather than pure exponents, is the evaluation criterion; (iii) we provide detailed applications to location testing under Laplace families, shape testing via Bayes factors, and connections to Fisher information geometry. The organizing principle throughout is that sample size enters Bayes-optimal GOF cutoffs through the MDP scale, unifying KS-based and Sanov-based perspectives under a single risk criterion.",
      "authors": [
        "Nicholas G. Polson",
        "Vadim Sokolov",
        "Daniel Zantedeschi"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST"
      ],
      "published": "2026-02-17 01:44:16+00:00",
      "link": "https://arxiv.org/pdf/2602.15297v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15266v1",
      "title": "A golden-ratio partition of information and the balance between prediction and surprise: a neuro-cognitive route to antifragility",
      "abstract": "Adaptive systems must strike a balance between prediction and surprise to thrive in uncertain environments. We propose an information-theoretic balance function, $ f(p) = -(1 - p)\\ln(1 - p) + \\ln p $, which quantifies the net informational gain from contrasting explained variance $p$ with unexplained novelty $(1 - p)$. This function is strictly concave on $(0,1)$ and reaches its unique maximum at $ p^* \\approx 0.882$, revealing a regime where confidence is high but the residual uncertainty carries a disproportionate potential for surprise.   Independently of this maximum, imposing a self-similarity condition between known, unknown and total information, $p : (1-p) = 1 : p$, leads to the golden-ratio reciprocal $p = 1/\\varphi \\approx 0.618$, where $ \\varphi$ is the golden ratio. We interpret this value not as the maximizer of $f$, but as a structurally privileged \\emph{partition} in which known and unknown are proportionally nested across scales.   Embedding this dual structure into a Compute-Inference-Model-Action (CIMA) loop yields a dynamic process that maintains the system near a critical regime where prediction and surprise coexist. At this edge, neuronal dynamics exhibit power-law structure and maximal dynamic range, while the system's response to perturbations becomes convex at the level of its payoff function-fulfilling the formal definition of antifragility. We suggest that the golden-ratio partition is not merely a mathematical artifact, but a candidate design principle linking prediction, surprise, criticality, and antifragile adaptation across scales and domains, while the maximum of $f$ identifies the point of greatest informational vulnerability to being wrong.",
      "authors": [
        "Pablo Padilla",
        "Oliver López-Corona",
        "Elvia Ramírez-Carrillo",
        "Ariadne Hernández Sánchez"
      ],
      "primary_category": "math.DS",
      "categories": [
        "math.DS",
        "q-bio.NC"
      ],
      "published": "2026-02-16 23:51:36+00:00",
      "link": "https://arxiv.org/pdf/2602.15266v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15094v1",
      "title": "On propagation of chaos for the Fisher-Rao gradient flow in entropic mean-field optimization",
      "abstract": "We consider a class of optimization problems on the space of probability measures motivated by the mean-field approach to studying neural networks. Such problems can be solved by constructing continuous-time gradient flows that converge to the minimizer of the energy function under consideration, and then implementing discrete-time algorithms that approximate the flow. In this work, we focus on the Fisher-Rao gradient flow and we construct an interacting particle system that approximates the flow as its mean-field limit. We discuss the connection between the energy function, the gradient flow and the particle system and explain different approaches to smoothing out the energy function with an appropriate kernel in a way that allows for the particle system to be well-defined. We provide a rigorous proof of the existence and uniqueness of thus obtained kernelized flows, as well as a propagation of chaos result that provides a theoretical justification for using the corresponding kernelized particle systems as approximation algorithms in entropic mean-field optimization.",
      "authors": [
        "Petra Lazić",
        "Linshan Liu",
        "Mateusz B. Majka"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "math.PR",
        "stat.ML"
      ],
      "published": "2026-02-16 18:34:19+00:00",
      "link": "https://arxiv.org/pdf/2602.15094v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15000v1",
      "title": "ALiA: Adaptive Linearized ADMM",
      "abstract": "We propose ALiA, a novel adaptive variant of the alternating direction method of multipliers (ADMM). Specifically, ALiA is a variant of function-linearized proximal ADMM (FLiP ADMM), which generalizes the classical ADMM by leveraging the differentiable structure of the objective function, making it highly versatile. Notably, ALiA features an adaptive stepsize selection scheme that eliminates the need for backtracking linesearch. Motivated by recent advances in adaptive gradient and proximal methods, we establish point convergence of ALiA for convex and differentiable objectives. Furthermore, by introducing negligible computational overhead, we develop an alternative stepsize selection scheme for ALiA that improves the convergence speed both theoretically and empirically. Extensive numerical experiments on practical datasets confirm the accelerated performance of ALiA compared to standard FLiP ADMM. Additionally, we demonstrate that ALiA either outperforms or matches the practical performance of existing adaptive methods across problem classes where it is applicable.",
      "authors": [
        "Uijeong Jang",
        "Kaizhao Sun",
        "Wotao Yin",
        "Ernest K Ryu"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-16 18:28:56+00:00",
      "link": "https://arxiv.org/pdf/2602.15000v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14969v1",
      "title": "Topological trivialization in non-convex empirical risk minimization",
      "abstract": "Given data $\\{({\\boldsymbol x}_i,y_i): i\\le n\\}$, with ${\\boldsymbol x}_i$ standard $d$-dimensional Gaussian feature vectors, and $y_i\\in{\\mathbb R}$ response variables, we study the general problem of learning a model parametrized by ${\\boldsymbol θ}\\in{\\mathbb R}^d$, by minimizing a loss function that depends on ${\\boldsymbol θ}$ via the one-dimensional projections ${\\boldsymbol θ}^{\\sf T}{\\boldsymbol x}_i$. While previous work mostly dealt with convex losses, our approach assumes general (non-convex) losses hence covering classical, yet poorly understood examples such as the perceptron and non-convex robust regression. We use the Kac-Rice formula to control the asymptotics of the expected number of local minima of the empirical risk, under the proportional asymptotics $n,d\\to\\infty$, $n/d\\toα>1$. Specifically, we prove a finite dimensional variational formula for the exponential growth rate of the expected number of local minima. Further we provide sufficient conditions under which the exponential growth rate vanishes and all empirical risk minimizers have the same asymptotic properties (in fact, we expect the minimizer to be unique in these circumstances). We refer to this phenomenon as `rate trivialization.' If the population risk has a unique minimizer, our sufficient condition for rate trivialization is typically verified when the samples/parameters ratio $α$ is larger than a suitable constant $α_{\\star}$. Previous general results of this type required $n\\ge Cd \\log d$. We illustrate our results in the case of non-convex robust regression. Based on heuristic arguments and numerical simulations, we present a conjecture for the exact location of the trivialization phase transition $α_{\\text{tr}}$.",
      "authors": [
        "Andrea Montanari",
        "Basil Saeed"
      ],
      "primary_category": "math.ST",
      "categories": [
        "math.ST"
      ],
      "published": "2026-02-16 17:55:50+00:00",
      "link": "https://arxiv.org/pdf/2602.14969v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.14630v1",
      "title": "Bayesian Cosmic Void Finding with Graph Flows",
      "abstract": "Cosmic voids contain higher-order cosmological information and are of interest for astroparticle physics. Finding genuine matter underdensities in sparse galaxy surveys is, however, an underconstrained problem. Traditional void finding algorithms produce deterministic void catalogs, neglecting the probabilistic nature of the problem. We present a method to sample from the stochastic mapping from galaxy catalogs to arbitrary void definitions. Our algorithm uses a deep graph neural network to evolve \"test particles\" according to a flow-matching objective. We demonstrate the method in a simplified example setting but outline steps to generalize it towards practically usable void finders. Trained on a deterministic teacher, the model performs well but has considerable stochasticity which we interpret as regularization. Cosmological information in the predicted void catalogs outperforms the teacher. On the one hand, our method can cheaply emulate existing void finders with apparently useful regularization. More importantly, it also allows us to find the Bayes-optimal mapping between observed galaxies and any void definition. This includes definitions operating at the level of simulated matter density and velocity fields.",
      "authors": [
        "Leander Thiele"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO",
        "stat.ML"
      ],
      "published": "2026-02-16 10:37:53+00:00",
      "link": "https://arxiv.org/pdf/2602.14630v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14520v1",
      "title": "Accelerating Posterior Inference from Pulsar Light Curves via Learned Latent Representations and Local Simulator-Guided Optimization",
      "abstract": "Posterior inference from pulsar observations in the form of light curves is commonly performed using Markov chain Monte Carlo methods, which are accurate but computationally expensive. We introduce a framework that accelerates posterior inference while maintaining accuracy by combining learned latent representations with local simulator-guided optimization. A masked U-Net is first pretrained to reconstruct complete light curves from partial observations and to produce informative latent embeddings. Given a query light curve, we identify similar simulated light curves from the simulation bank by measuring similarity in the learned embedding space produced by pretrained U-Net encoder, yielding an initial empirical approximation to the posterior over parameters. This initialization is then refined using a local optimization procedure using hill-climbing updates, guided by a forward simulator, progressively shifting the empirical posterior toward higher-likelihood parameter regions. Experiments on the observed light curve of PSR J0030+0451, captured by NASA's Neutron Star Interior Composition Explorer (NICER), show that our method closely matches posterior estimates obtained using traditional MCMC methods while achieving 120 times reduction in inference time (from 24 hours to 12 minutes), demonstrating the effectiveness of learned representations and simulator-guided optimization for accelerated posterior inference.",
      "authors": [
        "Farhana Taiyebah",
        "Abu Bucker Siddik",
        "Indronil Bhattacharjee",
        "Diane Oyen",
        "Soumi De",
        "Greg Olmschenk",
        "Constantinos Kalapotharakos"
      ],
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "astro-ph.HE"
      ],
      "published": "2026-02-16 07:12:23+00:00",
      "link": "https://arxiv.org/pdf/2602.14520v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14349v1",
      "title": "Same Prompt, Different Outcomes: Evaluating the Reproducibility of Data Analysis by LLMs",
      "abstract": "We systematically evaluate the reproducibility of data analysis conducted by Large Language Models (LLMs). We evaluate two prompting strategies, six models, and four temperature settings, with ten independent executions per configuration, yielding 480 total attempts. We assess the completion, concordance, validity, and consistency of each attempt and find considerable variation in the analytical results even for consistent configurations. This suggests, as with human data analysis, the data analysis conducted by LLMs can vary, even given the same task, data, and settings. Our results mean that if an LLM is being used to conduct data analysis, then it should be run multiple times independently and the distribution of results considered.",
      "authors": [
        "Jiaxin Cui",
        "Rohan Alexander"
      ],
      "primary_category": "stat.AP",
      "categories": [
        "stat.AP"
      ],
      "published": "2026-02-15 23:39:37+00:00",
      "link": "https://arxiv.org/pdf/2602.14349v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13538v1",
      "title": "Empirical Bayes data integreation for multi-response regression",
      "abstract": "Motivated by applications in tissue-wide association studies (TWAS), we develop a flexible and theoretically grounded empirical Bayes approach for integrating %vector-valued outcomes data obtained from different sources. We propose a linear shrinkage estimator that effectively shrinks singular values of a data matrix. This problem is closely connected to estimating covariance matrices under a specific loss, for which we develop asymptotically optimal estimators. The basic linear shrinkage estimator is then extended to a local linear shrinkage estimator, offering greater flexibility. Crucially, the proposed method works under sparse/dense or low-rank/non low-rank parameter settings unlike well-known sparse or reduced rank estimators in the literature. Furthermore, the empirical Bayes approach offers greater scalability in computation compared to intensive full Bayes procedures. The method is evaluated through an extensive set of numerical experiments, and applied to a real TWAS data obtained from the Genotype-Tissue Expression (GTEx) project.",
      "authors": [
        "Antik Chakraborty",
        "Fei Xue"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-14 00:40:33+00:00",
      "link": "https://arxiv.org/pdf/2602.13538v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13450v1",
      "title": "Inference From Random Restarts",
      "abstract": "Algorithms for computing equilibria, optima, and fixed points in nonconvex problems often depend sensitively on practitioner-chosen initial conditions. When uniqueness of a solution is of interest, a common heuristic is to run such algorithms from many randomly selected initial conditions and to interpret repeated convergence to the same output as evidence of a unique solution or a dominant basin of attraction. Despite its widespread use, this practice lacks a formal inferential foundation.   We provide a simple probabilistic framework for interpreting such numerical evidence. First, we give sufficient conditions under which an algorithm's terminal output is a measurable function of its initial condition, allowing probabilistic reasoning over outcomes. Second, we provide sufficient conditions ensuring that an algorithm admits only finitely many possible terminal outcomes. While these conditions may be difficult to verify on a case-by-case basis, we give simple sufficient conditions for broad classes of problems under which almost all instances admit only finitely many outcomes (in the sense of prevalence). Standard algorithms such as gradient descent and damped fixed-point iteration applied to sufficiently smooth functions satisfy these conditions.   Within this framework, repeated solver runs correspond to independent samples from the induced distribution over outcomes. We adopt a Bayesian approach to infer basin sizes and the probability of solution uniqueness from repeated identical outputs, and we establish convergence rates for the resulting posterior beliefs. Finally, we apply our framework to settings in the existing industrial organization literature, where random-restart heuristics are used. Our results formalize and qualify these arguments, clarifying when repeated convergence provides meaningful evidence for uniqueness and when it does not.",
      "authors": [
        "Moeen Nehzati",
        "Diego Cussen"
      ],
      "primary_category": "econ.EM",
      "categories": [
        "econ.EM",
        "stat.AP",
        "stat.ME"
      ],
      "published": "2026-02-13 20:51:58+00:00",
      "link": "https://arxiv.org/pdf/2602.13450v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13442v1",
      "title": "Measuring Neural Network Complexity via Effective Degrees of Freedom",
      "abstract": "Quantifying the complexity of feed-forward neural networks (FFNNs) remains challenging due to their nonlinear, hierarchical structure and numerous parameters. We apply generalized degrees of freedom (GDF) to measure model complexity in FFNNs with binary outcomes, adapting the algorithm for discrete responses. We compare GDF with both the effective number of parameters derived via log-likelihood cross-validation and the null degrees of freedom of Landsittel et al. Through simulation studies and a real data analysis, we demonstrate that GDF provides a robust assessment of model complexity for neural network models, as it depends only on the sensitivity of fitted values to perturbations in the observed responses rather than on assumptions about the likelihood. In contrast, cross-validation-based estimates of model complexity and the null degrees of freedom rely on the correctness of the assumed likelihood and may exhibit substantial variability. We find that GDF, cross-validation-based measures, and null degrees of freedom yield similar assessments of model complexity only when the fitted model adequately represents the data-generating mechanism. These findings highlight GDF as a stable and broadly applicable measure of model complexity for neural networks in statistical modeling.",
      "authors": [
        "Jia Zhou",
        "Douglas Landsittel"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "published": "2026-02-13 20:37:12+00:00",
      "link": "https://arxiv.org/pdf/2602.13442v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13184v1",
      "title": "Profiling systematic uncertainties in Simulation-Based Inference with Factorizable Normalizing Flows",
      "abstract": "Unbinned likelihood fits aim at maximizing the information one can extract from experimental data, yet their application in realistic statistical analyses is often hindered by the computational cost of profiling systematic uncertainties. Additionally, current machine learning-based inference methods are typically limited to estimating scalar parameters in a multidimensional space rather than full differential distributions. We propose a general framework for Simulation-Based Inference (SBI) that efficiently profiles nuisance parameters while measuring multivariate Distributions of Interest (DoI), defined as learnable invertible transformations of the feature space. We introduce Factorizable Normalizing Flows to model systematic variations as parametric deformations of a nominal density, preserving tractability without combinatorial explosion. Crucially, we develop an amortized training strategy that learns the conditional dependence of the DoI on nuisance parameters in a single optimization process, bypassing the need for repetitive training during the likelihood scan. This allows for the simultaneous extraction of the underlying distribution and the robust profiling of nuisances. The method is validated on a synthetic dataset emulating a high-energy physics measurement with multiple systematic sources, demonstrating its potential for unbinned, functional measurements in complex analyses.",
      "authors": [
        "Davide Valsecchi",
        "Mauro Donegà",
        "Rainer Wallny"
      ],
      "primary_category": "hep-ph",
      "categories": [
        "hep-ph",
        "physics.data-an",
        "stat.ML"
      ],
      "published": "2026-02-13 18:48:12+00:00",
      "link": "https://arxiv.org/pdf/2602.13184v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12490v1",
      "title": "Transformer-based CoVaR: Systemic Risk in Textual Information",
      "abstract": "Conditional Value-at-Risk (CoVaR) quantifies systemic financial risk by measuring the loss quantile of one asset, conditional on another asset experiencing distress. We develop a Transformer-based methodology that integrates financial news articles directly with market data to improve CoVaR estimates. Unlike approaches that use predefined sentiment scores, our method incorporates raw text embeddings generated by a large language model (LLM). We prove explicit error bounds for our Transformer CoVaR estimator, showing that accurate CoVaR learning is possible even with small datasets. Using U.S. market returns and Reuters news items from 2006--2013, our out-of-sample results show that textual information impacts the CoVaR forecasts. With better predictive performance, we identify a pronounced negative dip during market stress periods across several equity assets when comparing the Transformer-based CoVaR to both the CoVaR without text and the CoVaR using traditional sentiment measures. Our results show that textual data can be used to effectively model systemic risk without requiring prohibitively large data sets.",
      "authors": [
        "Junyu Chen",
        "Tom Boot",
        "Lingwei Kong",
        "Weining Wang"
      ],
      "primary_category": "econ.EM",
      "categories": [
        "econ.EM",
        "q-fin.RM",
        "stat.ML"
      ],
      "published": "2026-02-13 00:10:30+00:00",
      "link": "https://arxiv.org/pdf/2602.12490v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12435v1",
      "title": "Scalable Changepoint Detection for Large Spatiotemporal Data on the Sphere",
      "abstract": "We propose a novel Bayesian framework for changepoint detection in large-scale spherical spatiotemporal data, with broad applicability in environmental and climate sciences. Our approach models changepoints as spatially dependent categorical variables using a multinomial probit model (MPM) with a latent Gaussian process, effectively capturing complex spatial correlation structures on the sphere. To handle the high dimensionality inherent in global datasets, we leverage stochastic partial differential equations (SPDE) and spherical harmonic transformations for efficient representation and scalable inference, drastically reducing computational burden while maintaining high accuracy. Through extensive simulation studies, we demonstrate the efficiency and robustness of the proposed method for changepoint estimation, as well as the significant computational gains achieved through the combined use of the MPM and truncated spectral representations of latent processes. Finally, we apply our method to global aerosol optical depth data, successfully identifying changepoints associated with a major atmospheric event.",
      "authors": [
        "Samantha Shi-Jun",
        "Bo Li"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "stat.CO"
      ],
      "published": "2026-02-12 21:48:27+00:00",
      "link": "https://arxiv.org/pdf/2602.12435v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11333v1",
      "title": "Cross-Fitting-Free Debiased Machine Learning with Multiway Dependence",
      "abstract": "This paper develops an asymptotic theory for two-step debiased machine learning (DML) estimators in generalised method of moments (GMM) models with general multiway clustered dependence, without relying on cross-fitting. While cross-fitting is commonly employed, it can be statistically inefficient and computationally burdensome when first-stage learners are complex and the effective sample size is governed by the number of independent clusters. We show that valid inference can be achieved without sample splitting by combining Neyman-orthogonal moment conditions with a localisation-based empirical process approach, allowing for an arbitrary number of clustering dimensions. The resulting DML-GMM estimators are shown to be asymptotically linear and asymptotically normal under multiway clustered dependence. A central technical contribution of the paper is the derivation of novel global and local maximal inequalities for general classes of functions of sums of separately exchangeable arrays, which underpin our theoretical arguments and are of independent interest.",
      "authors": [
        "Kaicheng Chen",
        "Harold D. Chiang"
      ],
      "primary_category": "econ.EM",
      "categories": [
        "econ.EM",
        "stat.ML"
      ],
      "published": "2026-02-11 20:09:23+00:00",
      "link": "https://arxiv.org/pdf/2602.11333v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10749v2",
      "title": "The Dataset of Daily Air Quality for the Years 2013-2023 in Italy",
      "abstract": "Air quality and climate are major issues in Italian society and lie at the intersection of many research fields, including public health and policy planning. There is an increasing need for readily available, easily accessible, ready-to-use and well-documented datasets on air quality and climate. In this paper, we present the GRINS AQCLIM dataset, created under the GRINS project framework covering the Italian domain for an extensive time period. It includes daily statistics (e.g., minimum, quartiles, mean, median and maximum) for a collection of air pollutant concentrations and climate variables at the locations of the 700+ available monitoring stations. Input data are retrieved from the European Environmental Agency and Copernicus Programme and were subjected to multiple processing steps to ensure their reliability and quality. These steps include automatic procedures for fixing raw files, manual inspection of stations information, the detection and removal of anomalies, and the temporal harmonisation on a daily basis. Datasets are hosted on Zenodo under open-access principles.",
      "authors": [
        "Alessandro Fusta Moro",
        "Alessandro Fassò",
        "Jacopo Rodeschini"
      ],
      "primary_category": "stat.AP",
      "categories": [
        "stat.AP"
      ],
      "published": "2026-02-11 11:25:06+00:00",
      "link": "https://arxiv.org/pdf/2602.10749v2",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.10332v1",
      "title": "Generalized Prediction-Powered Inference, with Application to Binary Classifier Evaluation",
      "abstract": "In the partially-observed outcome setting, a recent set of proposals known as \"prediction-powered inference\" (PPI) involve (i) applying a pre-trained machine learning model to predict the response, and then (ii) using these predictions to obtain an estimator of the parameter of interest with asymptotic variance no greater than that which would be obtained using only the labeled observations. While existing PPI proposals consider estimators arising from M-estimation, in this paper we generalize PPI to any regular asymptotically linear estimator. Furthermore, by situating PPI within the context of an existing rich literature on missing data and semi-parametric efficiency theory, we show that while PPI does not achieve the semi-parametric efficiency lower bound outside of very restrictive and unrealistic scenarios, it can be viewed as a computationally-simple alternative to proposals in that literature. We exploit connections to that literature to propose modified PPI estimators that can handle three distinct forms of covariate distribution shift. Finally, we illustrate these developments by constructing PPI estimators of true positive rate, false positive rate, and area under the curve via numerical studies.",
      "authors": [
        "Runjia Zou",
        "Daniela Witten",
        "Brian Williamson"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "published": "2026-02-10 22:11:26+00:00",
      "link": "https://arxiv.org/pdf/2602.10332v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.10026v1",
      "title": "Degrees-of-Freedom Approximations for Conditional-Mean Inference in Random-Lot Stability Analysis",
      "abstract": "Linear mixed models are widely used for pharmaceutical stability trending when sufficient lots are available. Expiry support is typically based on whether lot-specific conditional-mean confidence limits remain within specification through a proposed expiry. These limits depend on the denominator degrees-of-freedom (DDF) method used for $t$-based inference. We document an operationally important boundary-proximal phenomenon: when a fitted random-effect variance component is close to zero, Satterthwaite DDF for conditional-mean predictions can collapse, inflating $t$ critical values and producing unnecessarily wide and sometimes nonmonotone pointwise confidence limits on scheduled time grids. In contrast, containment DDF yields stable degrees of freedom and avoids sharp discontinuities as variance components approach the boundary. Using a worked example and simulation studies, we show that DDF choice can materially change pass/fail conclusions even when observed data comfortably meet specifications. Containment-based inference with the full random-effects model provides a single modeling framework that avoids the discontinuities introduced by data-dependent model reduction at arbitrary cutoffs. When containment is unavailable, a 10\\% variance-contribution reduction workflow mitigates extreme Satterthwaite behavior by simplifying the random-effects structure only when fitted contributions at the proposed expiry are negligible. An AICc step-down is also evaluated but is best treated as a sensitivity analysis, as it can be liberal when the margin between the mean trend and the specification limit at the proposed expiry is small.",
      "authors": [
        "Andrew T. Karl",
        "Heath Rushing",
        "Richard K. Burdick",
        "Jeff Hofer"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-10 17:49:15+00:00",
      "link": "https://arxiv.org/pdf/2602.10026v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09542v1",
      "title": "High Dimensional Mean Test for Shrinking Random Variables with Applications to Backtesting",
      "abstract": "We propose a high dimensional mean test framework for shrinking random variables, where the underlying random variables shrink to zero as the sample size increases. By pooling observations across overlapping subsets of dimensions, we estimate subsets means and test whether the maximum absolute mean deviates from zero. This approach overcomes cancellations that occur in simple averaging and remains valid even when marginal asymptotic normality fails. We establish theoretical properties of the test statistic and develop a multiplier bootstrap procedure to approximate its distribution. The method provides a flexible and powerful tool for the validation and comparative backtesting of value-at-risk. Simulations show superior performance in high-dimensional settings, and a real-data application demonstrates its practical effectiveness in backtesting.",
      "authors": [
        "Liujun Chen",
        "Chen Zhou"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-10 08:54:41+00:00",
      "link": "https://arxiv.org/pdf/2602.09542v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09351v1",
      "title": "Supervised Learning of Functional Outcomes with Predictors at Different Scales: A Functional Gaussian Process Approach",
      "abstract": "The analysis of complex computer simulations, often involving functional data, presents unique statistical challenges. Conventional regression methods, such as function-on-function regression, typically associate functional outcomes with both scalar and functional predictors on a per-realization basis. However, simulation studies often demand a more nuanced approach to disentangle nonlinear relationships of functional outcome with predictors observed at multiple scales: domain-specific functional predictors that are fixed across simulation runs, and realization-specific global predictors that vary between runs. In this article, we develop a novel supervised learning framework tailored to this setting. We propose an additive nonlinear regression model that flexibly captures the influence of both predictor types. The effects of functional predictors are modeled through spatially-varying coefficients governed by a Gaussian process prior. Crucially, to capture the impact of global predictors on the functional outcome, we introduce a functional Gaussian process (fGP) prior. This new prior jointly models the entire collection of unknown, spatially-indexed nonlinear functions that encode the effects of the global predictors over the entire domain, explicitly accounting for their spatial dependence. This integrated architecture enables simultaneous learning from both predictor types, provides a principled strategies to quantify their respective contributions in predicting the functional outcome, and delivers rigorous uncertainty estimates for both model parameters and predictions. The utility and robustness of our approach are demonstrated through multiple synthetic datasets and a real-world application involving outputs from the Sea, Lake, and Overland Surges from Hurricanes (SLOSH) model.",
      "authors": [
        "R. Jacob Andros",
        "Rajarshi Guhaniyogi",
        "Devin Francom",
        "Donatella Pasqualini"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-10 02:56:03+00:00",
      "link": "https://arxiv.org/pdf/2602.09351v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09247v1",
      "title": "Motivating REML via Prediction-Error Covariances in EM Updates for Linear Mixed Models",
      "abstract": "We present a computational motivation for restricted maximum likelihood (REML) estimation in linear mixed models using an expectation--maximization (EM) algorithm. At each iteration, maximum likelihood (ML) and REML solve the same mixed-model equations for the best linear unbiased estimator (BLUE) of the fixed effects and the best linear unbiased predictor (BLUP) of the random effects. They differ only in the trace adjustments used in the variance-component updates: ML uses conditional covariances of the random effects given the data, whereas REML uses prediction-error covariances from Henderson's C-matrix, reflecting uncertainty from estimating the fixed effects. Short R code makes this switch explicit, exposes the key matrices for classroom inspection, and reproduces lme4 ML and REML fits.",
      "authors": [
        "Andrew T. Karl"
      ],
      "primary_category": "stat.CO",
      "categories": [
        "stat.CO"
      ],
      "published": "2026-02-09 22:24:48+00:00",
      "link": "https://arxiv.org/pdf/2602.09247v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09167v1",
      "title": "Mean regression for (0,1) responses via beta scale mixtures",
      "abstract": "To achieve a greater general flexibility for modeling heavy-tailed bounded responses, a beta scale mixture model is proposed. Each member of the family is obtained by multiplying the scale parameter of the conditional beta distribution by a mixing random variable taking values on all or part of the positive real line and whose distribution depends on a single parameter governing the tail behavior of the resulting compound distribution. These family members allow for a wider range of values for skewness and kurtosis. To validate the effectiveness of the proposed model, we conduct experiments on both simulated data and real datasets. The results indicate that the beta scale mixture model demonstrates superior performance relative to the classical beta regression model and alternative competing methods for modeling responses on the bounded unit domain.",
      "authors": [
        "Arno Otto",
        "Andriëtte Bekker",
        "Johan Ferreira",
        "Lebogang Rathebe"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-09 20:18:37+00:00",
      "link": "https://arxiv.org/pdf/2602.09167v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16497v1",
      "title": "Factor-Adjusted Multiple Testing for High-Dimensional Individual Mediation Effects",
      "abstract": "Identifying individual mediators is a central goal of high-dimensional mediation analysis, yet pervasive dependence among mediators can invalidate standard debiased inference and lead to substantial false discovery rate (FDR) inflation. We propose a Factor-Adjusted Debiased Mediation Testing (FADMT) framework that enables large-scale inference for individual mediation effects with FDR control under complex dependence structures. Our approach posits an approximate factor structure on the unobserved errors of the mediator model, extracts common latent factors, and constructs decorrelated pseudo-mediators for the subsequent inferential procedure. We establish the asymptotic normality of the debiased estimator and develop a multiple testing procedure with theoretical FDR control under mild high-dimensional conditions. By adjusting for latent factor induced dependence, FADMT also improves robustness to spurious associations driven by shared latent variation in observational studies. Extensive simulations demonstrate the superior finite-sample performance across a wide range of correlation structures. Applications to TCGA-BRCA multi-omics data and to China's stock connect study further illustrate the practical utility of the proposed method.",
      "authors": [
        "Chen Shi",
        "Zhao Chen",
        "Christina Dan Wang"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-18 14:40:42+00:00",
      "link": "https://arxiv.org/pdf/2602.16497v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16376v1",
      "title": "Two-way Clustering Robust Variance Estimator in Quantile Regression Models",
      "abstract": "We study inference for linear quantile regression with two-way clustered data. Using a separately exchangeable array framework and a projection decomposition of the quantile score, we characterize regime-dependent convergence rates and establish a self-normalized Gaussian approximation. We propose a two-way cluster-robust sandwich variance estimator with a kernel-based density ``bread'' and a projection-matched ``meat'', and prove consistency and validity of inference in Gaussian regimes. We also show an impossibility result for uniform inference in a non-Gaussian interaction regime.",
      "authors": [
        "Ulrich Hounyo",
        "Jiahao Lin"
      ],
      "primary_category": "econ.EM",
      "categories": [
        "econ.EM",
        "stat.AP"
      ],
      "published": "2026-02-18 11:35:18+00:00",
      "link": "https://arxiv.org/pdf/2602.16376v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16328v1",
      "title": "A general framework for modeling Gaussian process with qualitative and quantitative factors",
      "abstract": "Computer experiments involving both qualitative and quantitative (QQ) factors have attracted increasing attention. Gaussian process (GP) models have proven effective in this context by choosing specialized covariance functions for QQ factors. In this work, we extend the latent variable-based GP approach, which maps qualitative factors into a continuous latent space, by establishing a general framework to apply standard kernel functions to continuous latent variables. This approach provides a novel perspective for interpreting some existing GP models for QQ factors and introduces new covariance structures in some situations. The ordinal structure can be incorporated naturally and seamlessly in this framework. Furthermore, the Bayesian information criterion and leave-one-out cross-validation are employed for model selection and model averaging. The performance of the proposed method is comprehensively studied on several examples.",
      "authors": [
        "Linsui Deng",
        "C. F. Jeff Wu"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-18 10:08:03+00:00",
      "link": "https://arxiv.org/pdf/2602.16328v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16146v1",
      "title": "Uncertainty-Aware Neural Multivariate Geostatistics",
      "abstract": "We propose Deep Neural Coregionalization, a scalable framework for uncertainty-aware multivariate geostatistics. DNC models multivariate spatial effects through spatially varying latent factors and loadings, assigning deep Gaussian process (DGP) priors to both the factors and the entries of the loading matrix. This joint construction learns shared latent spatial structure together with response-specific, location-dependent mixing weights, enabling flexible nonlinear and space-dependent associations within and across variables. A key contribution is a variational formulation that makes the DGP to deep neural network (DNN) correspondence explicit: maximizing the DGP evidence lower bound (ELBO) is equivalent to training DNNs with weight decay and Monte Carlo (MC) dropout. This yields fast mini-batch stochastic optimization without Markov Chain Monte Carlo (MCMC), while providing principled uncertainty quantification through MC-dropout forward passes as approximate posterior draws, producing calibrated credible surfaces for prediction and spatial effect estimation. Across simulations, DNC is competitive with existing spatial factor models, particularly under strong nonstationarity and complex cross-dependence, while delivering substantial computational gains. In a multivariate environmental case study, DNC captures spatially varying cross-variable interactions, produces interpretable maps of multivariate outcomes, and scales uncertainty quantification to large datasets with orders-of-magnitude reductions in runtime.",
      "authors": [
        "Yeseul Jeon",
        "Aaron Scheffler",
        "Rajarshi Guhaniyogi"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-18 02:38:22+00:00",
      "link": "https://arxiv.org/pdf/2602.16146v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16099v1",
      "title": "Quantifying and Attributing Submodel Uncertainty in Stochastic Simulation Models and Digital Twins",
      "abstract": "Stochastic simulation is widely used to study complex systems composed of various interconnected subprocesses, such as input processes, routing and control logic, optimization routines, and data-driven decision modules. In practice, these subprocesses may be inherently unknown or too computationally intensive to directly embed in the simulation model. Replacing these elements with estimated or learned approximations introduces a form of epistemic uncertainty that we refer to as submodel uncertainty. This paper investigates how submodel uncertainty affects the estimation of system performance metrics. We develop a framework for quantifying submodel uncertainty in stochastic simulation models and extend the framework to digital-twin settings, where simulation experiments are repeatedly conducted with the model initialized from observed system states. Building on approaches from input uncertainty analysis, we leverage bootstrapping and Bayesian model averaging to construct quantile-based confidence or credible intervals for key performance indicators. We propose a tree-based method that decomposes total output variability and attributes uncertainty to individual submodels in the form of importance scores. The proposed framework is model-agnostic and accommodates both parametric and nonparametric submodels under frequentist and Bayesian modeling paradigms. A synthetic numerical experiment and a more realistic digital-twin simulation of a contact center illustrate the importance of understanding how and how much individual submodels contribute to overall uncertainty.",
      "authors": [
        "Mohammadmahdi Ghasemloo",
        "David J. Eckman",
        "Yaxian Li"
      ],
      "primary_category": "stat.CO",
      "categories": [
        "stat.CO",
        "stat.ME",
        "stat.ML"
      ],
      "published": "2026-02-18 00:06:39+00:00",
      "link": "https://arxiv.org/pdf/2602.16099v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16041v1",
      "title": "Predictive Subsampling for Scalable Inference in Networks",
      "abstract": "Network datasets appear across a wide range of scientific fields, including biology, physics, and the social sciences. To enable data-driven discoveries from these networks, statistical inference techniques like estimation and hypothesis testing are crucial. However, the size of modern networks often exceeds the storage and computational capacities of existing methods, making timely, statistically rigorous inference difficult. In this work, we introduce a subsampling-based approach aimed at reducing the computational burden associated with estimation and two-sample hypothesis testing. Our strategy involves selecting a small random subset of nodes from the network, conducting inference on the resulting subgraph, and then using interpolation based on the observed connections between the subsample and the rest of the nodes to estimate the entire graph. We develop the methodology under the generalized random dot product graph framework, which affords broad applicability and permits rigorous analysis. Within this setting, we establish consistency guarantees and corroborate the practical effectiveness of the approach through comprehensive simulation studies.",
      "authors": [
        "Arpan Kumar",
        "Minh Tang",
        "Srijan Sengupta"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-17 21:48:57+00:00",
      "link": "https://arxiv.org/pdf/2602.16041v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15731v1",
      "title": "Generalised Exponential Kernels for Nonparametric Density Estimation",
      "abstract": "This paper introduces a novel kernel density estimator (KDE) based on the generalised exponential (GE) distribution, designed specifically for positive continuous data. The proposed GE KDE offers a mathematically tractable form that avoids the use of special functions, for instance, distinguishing it from the widely used gamma KDE, which relies on the gamma function. Despite its simpler form, the GE KDE maintains similar flexibility and shape characteristics, aligning with distributions such as the gamma, which are known for their effectiveness in modelling positive data. We derive the asymptotic bias and variance of the proposed kernel density estimator, and formally demonstrate the order of magnitude of the remaining terms in these expressions. We also propose a second GE KDE, for which we are able to show that it achieves the optimal mean integrated squared error, something that is difficult to establish for the former. Through numerical experiments involving simulated and real data sets, we show that GE KDEs can be an important alternative and competitive to existing KDEs.",
      "authors": [
        "Laura M. Craig",
        "Wagner Barreto-Souza"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME"
      ],
      "published": "2026-02-17 17:08:57+00:00",
      "link": "https://arxiv.org/pdf/2602.15731v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15916v1",
      "title": "Nonparametric Identification and Inference for Counterfactual Distributions with Confounding",
      "abstract": "We propose nonparametric identification and semiparametric estimation of joint potential outcome distributions in the presence of confounding. First, in settings with observed confounding, we derive tighter, covariate-informed bounds on the joint distribution by leveraging conditional copulas. To overcome the non-differentiability of bounding min/max operators, we establish the asymptotic properties for both a direct estimator with polynomial margin condition and a smooth approximation with log-sum-exp operator, facilitating valid inference for individual-level effects under the canonical rank-preserving assumption. Second, we tackle the challenge of unmeasured confounding by introducing a causal representation learning framework. By utilizing instrumental variables, we prove the nonparametric identifiability of the latent confounding subspace under injectivity and completeness conditions. We develop a ``triple machine learning\" estimator that employs cross-fitting scheme to sequentially handle the learned representation, nuisance parameters, and target functional. We characterize the asymptotic distribution with variance inflation induced by representation learning error, and provide conditions for semiparametric efficiency. We also propose a practical VAE-based algorithm for confounding representation learning. Simulations and real-world analysis validate the effectiveness of proposed methods. By bridging classical semiparametric theory with modern representation learning, this work provides a robust statistical foundation for distributional and counterfactual inference in complex causal systems.",
      "authors": [
        "Jianle Sun",
        "Kun Zhang"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "published": "2026-02-17 05:00:13+00:00",
      "link": "https://arxiv.org/pdf/2602.15916v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15150v1",
      "title": "bayesics: Core Statistical Methods via Bayesian Inference in R",
      "abstract": "Bayesian statistics is an integral part of contemporary applied science. bayesics provides a single framework, unified in syntax and output, for performing the most commonly used statistical procedures, ranging from one- and two-sample inference to general mediation analysis. bayesics leans hard away from the requirement that users be familiar with sampling algorithms by using closed-form solutions whenever possible, and automatically selecting the number of posterior samples required for accurate inference when such solutions are not possible. bayesics} focuses on providing key inferential quantities: point estimates, credible intervals, probability of direction, region of practical equivalance (ROPE), and, when applicable, Bayes factors. While algorithmic assessment is not required in bayesics, model assessment is still critical; towards that, bayesics provides diagnostic plots for parametric inference, including Bayesian p-values. Finally, bayesics provides extensions to models implemented in alternative R packages and, in the case of mediation analysis, correction to existing implementations.",
      "authors": [
        "Daniel K. Sewell",
        "Alan T. Arakkal"
      ],
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "stat.CO"
      ],
      "published": "2026-02-16 19:47:50+00:00",
      "link": "https://arxiv.org/pdf/2602.15150v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13503v1",
      "title": "Hermes: Large DEL Datasets Train Generalizable Protein-Ligand Binding Prediction Models",
      "abstract": "The quality and consistency of training data remain critical bottlenecks for protein-ligand binding prediction. Public affinity datasets, aggregated from thousands of labs and assay formats, introduce biases that limit model generalization and complicate evaluation. DNA-encoded chemical libraries (DELs) offer a potential solution: unified experimental protocols generating massive binding datasets across diverse chemical and protein target space. We present Hermes, a lightweight transformer trained exclusively on DEL data from screens against hundreds of protein targets, representing one of the largest and most protein-diverse DEL training sets applied to protein-ligand interaction (PLI) modeling to date. Despite never seeing traditional affinity measurements during training, Hermes generalizes to held-out targets, novel chemical scaffolds, and external benchmarks derived from public binding data and high-throughput screens. Our results demonstrate that DEL data alone captures transferable protein-ligand interaction representations, while Hermes' minimal architecture enables inference speeds suitable for large-scale virtual screening.",
      "authors": [
        "Maxwell Kleinsasser",
        "Brayden J. Halverson",
        "Edward Kraft",
        "Sean Francis-Lyon",
        "Sarah E. Hugo",
        "Mackenzie R. Roman",
        "Ben Miller",
        "Andrew D. Blevins",
        "Ian K. Quigley"
      ],
      "primary_category": "q-bio.BM",
      "categories": [
        "q-bio.BM"
      ],
      "published": "2026-02-13 22:27:52+00:00",
      "link": "https://arxiv.org/pdf/2602.13503v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11054v1",
      "title": "A Dynamical Microscope for Multivariate Oscillatory Signals: Validating Regime Recovery on Shared Manifolds",
      "abstract": "Multivariate oscillatory signals from complex systems often exhibit non-stationary dynamics and metastable regime structure, making dynamical interpretation challenging. We introduce a ``dynamical microscope'' framework that converts multichannel signals into circular phase--amplitude features, learns a data-driven latent trajectory representation with an autoencoder, and quantifies dynamical regimes through trajectory geometry and flow field metrics. Using a coupled Stuart--Landau oscillator network with topology-switching as ground-truth validation, we demonstrate that the framework recovers differences in dynamical laws even when regimes occupy overlapping regions of state space. Group differences can be expressed as changes in latent trajectory speed, path geometry, and flow organization on a shared manifold, rather than requiring discrete state separation. Speed and explored variance show strong regime discriminability ($η^2 > 0.5$), while some metrics (e.g., tortuosity) capture trajectory geometry orthogonal to topology contrasts. The framework provides a principled approach for analyzing regime structure in multivariate time series from neural, physiological, or physical systems.",
      "authors": [
        "Łukasz Furman",
        "Ludovico Minati",
        "Włodzisław Duch"
      ],
      "primary_category": "q-bio.NC",
      "categories": [
        "q-bio.NC",
        "nlin.CD"
      ],
      "published": "2026-02-11 17:20:25+00:00",
      "link": "https://arxiv.org/pdf/2602.11054v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16504v1",
      "title": "GRIMM: Genetic stRatification for Inference in Molecular Modeling",
      "abstract": "The vast majority of biological sequences encode unknown functions and bear little resemblance to experimentally characterized proteins, limiting both our understanding of biology and our ability to harness functional potential for the bioeconomy. Predicting enzyme function from sequence remains a central challenge in computational biology, complicated by low sequence diversity and imbalanced label support in publicly available datasets. Models trained on these data can overestimate performance and fail to generalize. To address this, we introduce GRIMM (Genetic stRatification for Inference in Molecular Modeling), a benchmark for enzyme function prediction that employs genetic stratification: sequences are clustered by similarity and clusters are assigned exclusively to training, validation, or test sets. This ensures that sequences from the same cluster do not appear in multiple partitions. GRIMM produces multiple test sets: a closed-set test with the same label distribution as training (Test-1) and an open-set test containing novel labels (Test-2), serving as a realistic out-of-distribution proxy for discovering novel enzyme functions. While demonstrated on enzymes, this approach is generalizable to any sequence-based classification task where inputs can be clustered by similarity. By formalizing a splitting strategy often used implicitly, GRIMM provides a unified and reproducible framework for closed- and open-set evaluation. The method is lightweight, requiring only sequence clustering and label annotations, and can be adapted to different similarity thresholds, data scales, and biological tasks. GRIMM enables more realistic evaluation of functional prediction models on both familiar and unseen classes and establishes a benchmark that more faithfully assesses model performance and generalizability.",
      "authors": [
        "Ashley Babjac",
        "Adrienne Hoarfrost"
      ],
      "primary_category": "q-bio.QM",
      "categories": [
        "q-bio.QM"
      ],
      "published": "2026-02-18 14:46:18+00:00",
      "link": "https://arxiv.org/pdf/2602.16504v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.14583v1",
      "title": "All-pole centroids in the Wasserstein metric with applications to clustering of spectral densities",
      "abstract": "In this work, we propose a method for computing centroids, or barycenters, in the spectral Wasserstein-2 metric for sets of power spectral densities, where the barycenters are restricted to belong to the set of all-pole spectra with a certain model order. This may be interpreted as finding an autoregressive representative for sets of second-order stationary Gaussian processes. While Wasserstein, or optimal transport, barycenters have been successfully used earlier in problems of spectral estimation and clustering, the resulting barycenters are non-parametric and the complexity of representing and storing them depends on, e.g., the choice of discretization grid. In contrast, the herein proposed method yields compact, low-dimensional, and interpretable spectral centroids that can be used in downstream tasks. Computing the all-pole centroids corresponds to solving a non-convex optimization problem in the model parameters, and we present a gradient descent scheme for addressing this. Although convergence to a globally optimal point cannot be guaranteed, the sub-optimality of the obtained centroids can be quantified. The proposed method is illustrated on a problem of phoneme classification.",
      "authors": [
        "Rumeshika Pallewela",
        "Filip Elvander"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-02-16 09:26:22+00:00",
      "link": "https://arxiv.org/pdf/2602.14583v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14411v1",
      "title": "Online Architecture Search for Compressed Sensing based on Hypergradient Descent",
      "abstract": "AS-ISTA (Architecture Searched-Iterative Shrinkage Thresholding Algorithm) and AS-FISTA (AS-Fast ISTA) are compressed sensing algorithms introducing structural parameters to ISTA and FISTA to enable architecture search within the iterative process. The structural parameters are determined using deep unfolding, but this approach requires training data and the large overhead of training time. In this paper, we propose HGD-AS-ISTA (Hypergradient Descent-AS-ISTA) and HGD-AS-FISTA that use hypergradient descent, which is an online hyperparameter optimization method, to determine the structural parameters. Experimental results show that the proposed method improves performance of the conventional ISTA/FISTA while avoiding the need for re-training when the environment changes.",
      "authors": [
        "Ayano Nakai-Kasai",
        "Yusuke Nakane",
        "Tadashi Wadayama"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-02-16 02:38:39+00:00",
      "link": "https://arxiv.org/pdf/2602.14411v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13957v1",
      "title": "Learning-based data-enabled moving horizon estimation with application to membrane-based biological wastewater treatment process",
      "abstract": "In this paper, we propose a data-enabled moving horizon estimation (MHE) approach for nonlinear systems. While the approach is formulated by leveraging Koopman theory, its implementation does not require explicit Koopman modeling. Lifting functions are learned from the state and input data of the original nonlinear system to project the system trajectories into the lifted space, where the resulting trajectories implicitly describe the Koopman representation for the original nonlinear system. A convex data-enabled MHE formulation is developed to provide real-time state estimates of the Koopman representation, from which the states of the nonlinear system can be reconstructed. Sufficient conditions are derived to ensure the stability of the estimation error. The effectiveness of the proposed method is illustrated using a membrane-based biological water treatment process.",
      "authors": [
        "Li Xiaojie",
        "Yin Xunyuan"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-02-15 02:24:36+00:00",
      "link": "https://arxiv.org/pdf/2602.13957v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13108v1",
      "title": "Encoder initialisation methods in the model augmentation setting",
      "abstract": "Nonlinear system identification (NL-SI) has proven to be effective in obtaining accurate models for highly complex systems. Recent encoder-based methods for artificial neural network state-space (ANN-SS) models have shown state-of-the-art performance with improved computational efficiency, where the encoder is used to estimate the initial state allowing for batch optimisation methods. To address the lack of interpretability of these black-box ANN models, model augmentation approaches can be used. These combine prior available baseline models with the ANN learning components, resulting in faster convergence and more interpretable models. The combination of the encoder-based method with model augmentation has shown potential. Thus far, however, the encoder has still been treated as a black-box function in the overall estimation process, while additional information in the form of the baseline model is available to predict the model state from past input-output data. In this paper, we propose novel encoder initialisation approaches based on the available baseline model, resulting in improved noise robustness and faster convergence compared to black-box initialisation. The performance of these initialisation methods is demonstrated on a mass-spring-damper system.",
      "authors": [
        "J. H. Hoekstra",
        "B. Györök",
        "R. Töth",
        "M. Schoukens"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-02-13 17:10:26+00:00",
      "link": "https://arxiv.org/pdf/2602.13108v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.12697v1",
      "title": "From Data $H(jω_i)$ to Balanced Truncation Family: A Projection-based Non-intrusive Approach",
      "abstract": "This paper presents data-driven implementations of balanced truncation and several of its generalizations that rely exclusively on transfer function samples on the imaginary axis. Rather than implicitly approximating the Gramians via numerical quadrature, the proposed approach approximates them implicitly through projection. This enables multiple members of the balanced truncation family to be implemented non-intrusively using practically measurable data, without requiring spectral factorizations. Using this projection-based framework, data-driven implementations are developed for standard balanced truncation, frequency-limited balanced truncation, time-limited balanced truncation, self-weighted balanced truncation, LQG balanced truncation, H-infinity balanced truncation, positive-real balanced truncation, bounded-real balanced truncation, and stochastic balanced truncation. Numerical results demonstrate that the proposed non-intrusive implementations achieve performance comparable to their intrusive counterparts and accurately capture the dominant Hankel singular values.",
      "authors": [
        "Umair Zulfiqar"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-02-13 08:04:31+00:00",
      "link": "https://arxiv.org/pdf/2602.12697v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12016v1",
      "title": "Adaptive Behavioral Predictive Control: State-Free Regulation Without Hankel Weights",
      "abstract": "This paper presents adaptive behavioral predictive control (ABPC), an indirect adaptive predictive control framework operating on streaming data. An LPV--ARX predictor is identified online via kernel--recursive least squares and used to compute closed-form predictive control sequences over a finite horizon, avoiding batch Hankel constructions and iterative optimization. Nonlinear kernel dictionaries extend model expressiveness within a behavioral formulation. Numerical studies on Hammerstein and NARX systems demonstrate effective performance when the dictionary aligns with the plant class and highlight conditioning and feature-selection effects. The paper emphasizes numerical simulation, computational feasibility, and reproducibility.",
      "authors": [
        "Tam W. Nguyen"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-02-12 14:48:40+00:00",
      "link": "https://arxiv.org/pdf/2602.12016v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.11899v1",
      "title": "Gradient-Based Adaptive Prediction and Control for Nonlinear Dynamical Systems",
      "abstract": "This paper investigates gradient-based adaptive prediction and control for nonlinear stochastic dynamical systems under a weak convexity condition on the prediction-based loss. This condition accommodates a broad range of nonlinear models in control and machine learning such as saturation functions, sigmoid, ReLU and tanh activation functions, and standard classification models. Without requiring any persistent excitation of the data, we establish global convergence of the proposed adaptive predictor and derive explicit rates for its asymptotic performance. Furthermore, under a classical nonlinear minimum-phase condition and with a linear growth bound on the nonlinearities, we establish the convergence rate of the resulting closed-loop control error. Finally, we demonstrate the effectiveness of the proposed adaptive prediction algorithm on a real-world judicial sentencing dataset. The adaptive control performance will also be evaluated via a numerical simulation.",
      "authors": [
        "Yujing Liu",
        "Xin Zheng",
        "Zhixin Liu",
        "Lei Guo"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-02-12 12:54:13+00:00",
      "link": "https://arxiv.org/pdf/2602.11899v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.09605v1",
      "title": "A General Formulation for the Teaching Assignment Problem: Computational Analysis Over a Real-World Dataset",
      "abstract": "The Teacher Assignment Problem is a combinatorial optimization problem that involves assigning teachers to courses while guaranteeing that all courses are covered, teachers do not teach too few or too many hours, teachers do not switch assigned courses too often and possibly teach the courses they favor. Typically the problem is solved manually, a task that requires several hours every year. In this work we present a mathematical formulation for the problem and an experimental evaluation of the model implemented using state-of-the-art SMT, CP, and MILP solvers. The implementations are tested over a real-world dataset provided by the Division of Systems and Control at Chalmers University of Technology, and produce teacher assignments with smaller workload deviation, a more even workload distribution among the teachers, and a lower number of switched courses.",
      "authors": [
        "Moa Johannesson",
        "Lina Brink",
        "Alvin Combrink",
        "Sabino Francesco Roselli",
        "Martin Fabian"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-02-10 09:56:09+00:00",
      "link": "https://arxiv.org/pdf/2602.09605v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.16166v1",
      "title": "Discovering Unknown Inverter Governing Equations via Physics-Informed Sparse Machine Learning",
      "abstract": "Discovering the unknown governing equations of grid-connected inverters from external measurements holds significant attraction for analyzing modern inverter-intensive power systems. However, existing methods struggle to balance the identification of unmodeled nonlinearities with the preservation of physical consistency. To address this, this paper proposes a Physics-Informed Sparse Machine Learning (PISML) framework. The architecture integrates a sparse symbolic backbone to capture dominant model skeletons with a neural residual branch that compensates for complex nonlinear control logic. Meanwhile, a Jacobian-regularized physics-informed training mechanism is introduced to enforce multi-scale consistency including large/small-scale behaviors. Furthermore, by performing symbolic regression on the neural residual branch, PISML achieves a tractable mapping from black-box data to explicit control equations. Experimental results on a high-fidelity Hardware-in-the-Loop platform demonstrate the framework's superior performance. It not only achieves high-resolution identification by reducing error by over 340 times compared to baselines but also realizes the compression of heavy neural networks into compact explicit forms. This restores analytical tractability for rigorous stability analysis and reduces computational complexity by orders of magnitude. It also provides a unified pathway to convert structurally inaccessible devices into explicit mathematical models, enabling stability analysis of power systems with unknown inverter governing equations.",
      "authors": [
        "Jialin Zheng",
        "Ruhaan Batta",
        "Zhong Liu",
        "Xiaonan Lu"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-02-18 03:46:02+00:00",
      "link": "https://arxiv.org/pdf/2602.16166v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15618v1",
      "title": "Physics-Informed Anomaly Detection of Terrain Material Change in Radar Imagery",
      "abstract": "In this paper we consider physics-informed detection of terrain material change in radar imagery (e.g., shifts in permittivity, roughness or moisture). We propose a lightweight electromagnetic (EM) forward model to simulate bi-temporal single-look complex (SLC) images from labelled material maps. On these data, we derive physics-aware feature stacks that include interferometric coherence, and evaluate unsupervised detectors: Reed-Xiaoli (RX)/Local-RX with robust scatter (Tyler's M-estimator), Coherent Change Detection (CCD), and a compact convolutional auto-encoder. Monte Carlo experiments sweep dielectric/roughness/moisture changes, number of looks and clutter regimes (gamma vs K-family) at fixed probability of false alarm. Results on synthetic but physically grounded scenes show that coherence and robust covariance markedly improve anomaly detection of material changes; a simple score-level fusion achieves the best F1 in heavy-tailed clutter.",
      "authors": [
        "Abdel Hakiem Mohamed Abbas Mohamed Ahmed",
        "Beth Jelfs",
        "Airlie Chapman",
        "Eric Schoof",
        "Christopher Gilliam"
      ],
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP"
      ],
      "published": "2026-02-17 14:46:25+00:00",
      "link": "https://arxiv.org/pdf/2602.15618v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15596v1",
      "title": "Time-Certified and Efficient NMPC via Koopman Operator",
      "abstract": "Certifying and accelerating execution times of nonlinear model predictive control (NMPC) implementations are two core requirements. Execution-time certificate guarantees that the NMPC controller returns a solution before the next sampling time, and achieving faster worst-case and average execution times further enables its use in a wider set of applications. However, NMPC produces a nonlinear program (NLP) for which it is challenging to derive its execution time certificates. Our previous works, \\citep{wu2025direct,wu2025time} provide data-independent execution time certificates (certified number of iterations) for box-constrained quadratic programs (BoxQP). To apply the time-certified BoxQP algorithm \\citep{wu2025time} for state-input constrained NMPC, this paper i) learns a linear model via Koopman operator; ii) proposes a dynamic-relaxation construction approach yields a structured BoxQP rather than a general QP; iii) exploits the structure of BoxQP, where the dimension of the linear system solved in each iteration is reduced from $5N(n_u+n_x)$ to $Nn_u$ (where $n_u, n_x, N$ denote the number of inputs, states, and length of prediction horizon), yielding substantial speedups (when $n_x \\gg n_u$, as in PDE control).",
      "authors": [
        "Liang Wu",
        "Yunhong Che",
        "Bo Yang",
        "Kangyu Lin",
        "Ján Drgoňa"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-02-17 14:08:41+00:00",
      "link": "https://arxiv.org/pdf/2602.15596v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.13537v2",
      "title": "Cluster-Robust Inference for Quadratic Forms",
      "abstract": "This paper studies inference for quadratic forms of linear regression coefficients with clustered data and many covariates. Our framework covers three important special cases: instrumental variables regression with many instruments and controls, inference on variance components, and testing multiple restrictions in a linear regression. Naïve plug-in estimators are known to be biased. We study a leave-one-cluster-out estimator that is unbiased, and provide sufficient conditions for its asymptotic normality. For inference, we establish the consistency of a leave-three-cluster-out variance estimator under primitive conditions. In addition, we develop a novel leave-two-cluster-out variance estimator that is computationally simpler and guaranteed to be conservative under weaker conditions. Our analysis allows cluster sizes to diverge with the sample size, accommodates strong within-cluster dependence, and permits the dimension of the covariates to diverge with the sample size, potentially at the same rate.",
      "authors": [
        "Michal Kolesár",
        "Pengjin Min",
        "Wenjie Wang",
        "Yichong Zhang"
      ],
      "primary_category": "econ.EM",
      "categories": [
        "econ.EM"
      ],
      "published": "2026-02-14 00:33:20+00:00",
      "link": "https://arxiv.org/pdf/2602.13537v2",
      "tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.15289v1",
      "title": "A Projection Approach to Nonparametric Significance and Conditional Independence Testing",
      "abstract": "This paper develops a novel nonparametric significance test based on a tailored nonparametric-type projected weighting function that exhibits appealing theoretical and numerical properties. We derive the asymptotic properties of the proposed test and show that it can detect local alternatives at the parametric rate. Using the nonparametric orthogonal projection, we construct a computationally convenient multiplier bootstrap to obtain critical values from the case-dependent asymptotic null distribution. Compared with the existing literature, our approach overcomes the need for a stronger compact support assumption on the density of covariates arising from random denominators. We also extend the tailor-made projection procedure to test the conditional independence assumption. The simulation experiments further illustrate the advantages of our proposed method in testing significance and conditional independence in finite samples.",
      "authors": [
        "Xiaojun Song",
        "Jichao Yuan"
      ],
      "primary_category": "econ.EM",
      "categories": [
        "econ.EM"
      ],
      "published": "2026-02-17 01:16:56+00:00",
      "link": "https://arxiv.org/pdf/2602.15289v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15246v1",
      "title": "Learning Against Nature: Minimax Regret and the Price of Robustness",
      "abstract": "We study how a decision-maker (DM) learns from data of unknown quality to form robust, ''general-purpose'' posterior beliefs. We develop a framework for robust learning and belief formation under a minimax-regret criterion, cast as a zero-sum game: the DM chooses posterior beliefs to minimize ex-ante regret, while an adversarial Nature selects the data-generating process (DGP). We show that, in large samples of $n$ signal draws, Nature optimally induces ambiguity by choosing a process whose precision converges to the uninformative signals at the rate $1/\\sqrt{n}$. As a result, learning against the adversarial DGP is nontrivial as well as incomplete: the DM's ex-ante regret remains strictly positive even with an infinite amount of data. However, when the true DGP is fixed and informative (even if only slightly), our DM with a robust updating rule eventually learns the state with enough data. Still, learning occurs at a sub-exponential rate -- quantifying the asymptotic price of robustness -- and it exhibits ''under-inference'' bias. Our framework provides a decision-theoretic dual to the local alternatives method in asymptotic statistics, deriving the characteristic $1/\\sqrt{n}$-scaling endogenously from the signal ambiguity.",
      "authors": [
        "Yeon-Koo Che",
        "Longjian Li",
        "Tianling Luo"
      ],
      "primary_category": "econ.TH",
      "categories": [
        "econ.TH"
      ],
      "published": "2026-02-16 22:56:32+00:00",
      "link": "https://arxiv.org/pdf/2602.15246v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15085v1",
      "title": "Well-being and career instability across genders in the Spanish Astronomical Society",
      "abstract": "We present the results of a comprehensive survey conducted among members of the Spanish Astronomical Society (Sociedad Espanola de Astronomia, SEA) to assess well-being, professional satisfaction, and family-work balance of researchers in astronomy. The survey addressed multiple aspects of professional life, including happiness, career stability, publication pressure, and access to childcare services during scientific meetings. Responses were examined across gender and career stages to identify trends and sources of dissatisfaction.",
      "authors": [
        "Maritza A. Lara-Lopez",
        "I. Rebollido",
        "A. Vidal-Garcia",
        "A. Rouco Escorial",
        "S. Berlanas",
        "I. Garcia-Bernete",
        "B. Agis-Gonzalez",
        "M. Rodriguez-Baras",
        "N. Barrado-Izagirre",
        "I. Pintos Castro",
        "N. Ospina",
        "S. Bonoli"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM",
        "astro-ph.GA",
        "physics.soc-ph"
      ],
      "published": "2026-02-16 12:58:41+00:00",
      "link": "https://arxiv.org/pdf/2602.15085v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15073v1",
      "title": "GW-FALCON: A Novel Feature-Driven Deep Learning Approach for Early Warning Alerts of BNS and NSBH Inspirals in Next-Generation GW Observatories",
      "abstract": "Next-generation GW observatories such as the ET and CE will detect BNS and NSBH inspirals with high SNRs and long in-band durations, making systematic early-warning alerts both feasible and scientifically valuable. Such triggers are essential for coordinating rapid electromagnetic follow-up. In this work, we introduce GW-FALCON, a novel feature-driven DL framework for early-time detection between GW signal+noise and noise-only data in next-generation detectors. Instead of feeding raw time series to CNN or more complex neural network architectures, we first extract a large set of statistical, temporal, and spectral quantities from short observational time windows using the TSFEL library. The resulting fixed-length feature vectors are then used as input to feed-forward ANNs suitable for low-latency operation. We demonstrate the method using simulated BNS and NSBH inspiral waveforms injected into colored Gaussian noise generated from the ET and CE design PSDs. We train separate ANNs on feature sets extracted from partial-inspiral windows characterized by different maximum instantaneous frequencies, enabling early-warning triggers from tens to hundreds of seconds before merger. Across all detector configurations and datasets, the resulting classifiers achieve high accuracy and detection efficiency, with ET-like networks typically reaching test accuracies of order 90% and CE-like ones exceeding 97% at low false-alarm probability. To the best of our knowledge, this work presents the first comprehensive feature-based DL detection framework for Next-generation GW observatories, connecting feature extraction from strain time series data to robust signal-noise classification within a setup that can be extended to real data and to more advanced neural network architectures.",
      "authors": [
        "Grigorios Papigkiotis",
        "Georgios Vardakas",
        "Nikolaos Stergioulas"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM",
        "astro-ph.HE",
        "gr-qc",
        "physics.ins-det"
      ],
      "published": "2026-02-15 23:03:44+00:00",
      "link": "https://arxiv.org/pdf/2602.15073v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13862v1",
      "title": "Measuring Self-Rating Bias in LLM-Generated Survey Data: A Semantic Similarity Framework for Independent Scale Mapping",
      "abstract": "Synthetic survey data generated by large language models (LLMs) suffers from a fundamental circularity: the same model family that generates text responses also maps them to numerical scales. We calibrate and validate Semantic Similarity Rating (SSR; Maier et al., 2024), which decouples generation from scale mapping via embedding-based cosine similarity against predefined anchor statements. Configuration experiments (N=17 pilot, N=69 cross-validation across 8 domains) show that naturalistic behavioral anchors outperform formal jargon by 29 percentage points (pp), and that SSR achieves 65-67% exact match and 91% within plus/minus 1; a cross-model test with OpenAI text-embedding-3-small reaches 77% exact, confirming cross-provider generalization. Direct LLM baselines (Claude 87%, GPT-4o 83%) establish that SSR's contribution is methodological independence, not accuracy superiority. A control condition removing question text from the LLM prompt actually improves LLM accuracy, ruling out information asymmetry as the explanation for SSR's lower accuracy. A pre-registered circularity experiment (N=345) reveals 4x compressed error variance in LLM rating (sigma^2 = 0.21 vs 0.87 for SSR) and systematic directional bias. A cross-model control (GPT-4o rating Claude-generated text) shows nearly identical compression (within/cross ratio = 0.93), indicating variance compression is a general LLM property rather than a within-model artifact. The calibration dataset, anchor library, and source code are publicly available (see Data Availability).",
      "authors": [
        "Eduardo Vera Pichardo"
      ],
      "primary_category": "physics.soc-ph",
      "categories": [
        "physics.soc-ph"
      ],
      "published": "2026-02-14 19:51:33+00:00",
      "link": "https://arxiv.org/pdf/2602.13862v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12808v1",
      "title": "Forecasting emergency department visits in the reference hospital of the Balearic Islands: the role of tourist and weather data",
      "abstract": "Accurate forecasting of patient arrivals at emergency departments (EDs) is vital for efficient resource allocation and high-quality patient care. In this study we investigate the relevance of exogenous variables, namely tourism, weather, calendar and demographic variables, in forecasting ED visits in the reference hospital in Palma de Mallorca, a city with significant seasonal population fluctuations due to tourism. Using a machine learning approach, we develop a model that predicts ED visits based solely on these exogenous variables. We test different machine learning algorithms (random forests, support vector machines, and feedforward neural networks) with different combinations of input variables and compare their symmetric mean average percentage errors (SMAPEs). Our findings reveal that calendar information, resident, and tourist population data are statistically significant for the accuracy of the predictions, while the addition of weather data does not provide any further improvement. Comparison of non-time-series with time-series prediction models reveals that the latter provide better accuracy for short prediction horizons (e.g. shorter than a week). Furthermore, time-series models become less or equally accurate to models relying only on exogenous variables for long prediction horizons (e.g. fortnight or month). Our study highlights the importance of carefully selecting predictive variables to ensure short- and long-term, robust and reliable forecasts. This demonstrates that, despite their lower complexity, non-time-series models with well-chosen input variables can be as effective as time-series models when predicting for long time horizons.",
      "authors": [
        "Paride Crisafulli",
        "Angel del Río Mangada",
        "Juan José Segura Sampedro",
        "Claudio R. Mirasso",
        "Raúl Toral",
        "Tobias Galla"
      ],
      "primary_category": "physics.soc-ph",
      "categories": [
        "physics.soc-ph"
      ],
      "published": "2026-02-13 10:42:54+00:00",
      "link": "https://arxiv.org/pdf/2602.12808v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12420v1",
      "title": "A Transformer-based Model for Rapid Microstructure Inference from Four-Dimensional Scanning Transmission Electron Microscopy Data",
      "abstract": "Properties of crystalline materials are closely linked to microstructure arising from the spatial arrangement, orientation, and phase of nanocrystals. Rapid characterization of crystalline microstructure can accelerate the identification of these links and the development of materials with desired properties. Here, we combine a machine learning framework with four-dimensional scanning transmission electron microscopy (4D-STEM) to enable fast inference of crystalline microstructure over large fields of view. The framework employs a transformer-based architecture to predict crystallographic orientations and phases from 4D-STEM diffraction patterns, yielding spatially resolved maps of microstructural features at the nanoscale. With this framework, crystallographic orientations are inferred up to two orders of magnitude faster than widely used correlative template-matching approaches. This capability enables high-throughput characterization of complex crystalline materials and facilitates the establishment of structure-property relationships central to materials design and optimization.",
      "authors": [
        "Kwanghwi Je",
        "Ellis R. Kennedy",
        "Sungin Kim",
        "Yao Yang",
        "Erik H. Thiede"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci",
        "physics.ins-det"
      ],
      "published": "2026-02-12 21:17:58+00:00",
      "link": "https://arxiv.org/pdf/2602.12420v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12254v1",
      "title": "A Stochastic Cluster Expansion for Electronic Correlation in Large Systems",
      "abstract": "Accurate many-body treatments of condensed-phase systems are challenging because correlated solvers such as full configuration interaction (FCI) and the density matrix renormalization group (DMRG) scale exponentially with system size. Downfolding and embedding approaches mitigate this cost but typically require prior selection of a correlated subspace, which can be difficult to determine in heterogeneous or extended systems. Here, we introduce a stochastic cluster expansion framework for efficiently recovering the total correlation energy of large systems with near-DMRG accuracy, without the need to select an active space a priori. By combining correlation contributions from randomly sampled environment orbitals with an exactly treated subspace of interest, the method reproduces total energies for non-reacting and reactive systems while drastically reducing computational cost. The approach also provides a quantitative diagnostic for molecule-solvent correlation, guiding principled embedding decisions. This framework enables systematically improvable many-body calculations in extended systems, opening the door to high-accuracy studies of chemical processes in condensed phase environments.",
      "authors": [
        "Annabelle Canestraight",
        "Anthony J. Dominic",
        "Andres Montoya-Castillo",
        "Libor Veis",
        "Vojtech Vlcek"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "categories": [
        "cond-mat.mtrl-sci",
        "physics.chem-ph",
        "physics.comp-ph"
      ],
      "published": "2026-02-12 18:42:03+00:00",
      "link": "https://arxiv.org/pdf/2602.12254v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.12052v1",
      "title": "GPU-Accelerated Analytic Simulation of Sparse Signals in Pixelated Time Projection Detector",
      "abstract": "This paper presents a GPU-accelerated simulation package,   TRED, for next-generation neutrino detectors with pixelated   charge readout, leveraging community-driven software ecosystems to   ensure sustainability and extensibility. We introduce two generic   contributions: (i) an effective-charge calculation based on Gaussian   quadrature rules for numerical integration, and (ii) a sparse,   block-binned tensor representation that enables efficient FFT-based   computation of induced signals on readout electrodes for sparsely   activated detector volumes. The former captures sub-grid structure   without requiring dense sampling, while the latter achieves low   memory usage and scalable runtime, as demonstrated in benchmark   studies. The underlying data representation is applicable to   large-scale detectors and to other computational problems involving   sparse activity.",
      "authors": [
        "Yousen Zhang",
        "Brett Viren",
        "Mary Bishai",
        "Sergey Martynenko",
        "Xin Qian",
        "Rado Razakamiandra",
        "Brooke Russell"
      ],
      "primary_category": "physics.ins-det",
      "categories": [
        "physics.ins-det"
      ],
      "published": "2026-02-12 15:17:57+00:00",
      "link": "https://arxiv.org/pdf/2602.12052v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11098v1",
      "title": "Data-Efficient Multidimensional Free Energy Estimation via Physics-Informed Score Learning",
      "abstract": "Many biological processes involve numerous coupled degrees of freedom, yet free-energy estimation is often restricted to one-dimensional profiles to mitigate the high computational cost of multidimensional sampling. In this work, we extend Fokker--Planck Score Learning (FPSL) to efficiently reconstruct two-dimensional free-energy landscapes from non-equilibrium molecular dynamics simulations using different types of collective variables. We show that explicitly modeling orthogonal degrees of freedom reveals insights hidden in one-dimensional projections at negligible computational overhead. Additionally, exploiting symmetries in the underlying landscape enhances reconstruction accuracy, while regularization techniques ensure numerical robustness in sparsely sampled regions. We validate our approach on three distinct systems: the conformational dynamics of alanine dipeptide, as well as coarse-grained and all-atom models of solute permeation through lipid bilayers. We demonstrate that, because FPSL learns a smooth score function rather than histogram-based densities, it overcomes the exponential scaling of grid-based methods, establishing it as a data-efficient and scalable tool for multidimensional free-energy estimation.",
      "authors": [
        "Daniel Nagel",
        "Tristan Bereau"
      ],
      "primary_category": "cond-mat.stat-mech",
      "categories": [
        "cond-mat.stat-mech",
        "physics.chem-ph"
      ],
      "published": "2026-02-11 18:11:49+00:00",
      "link": "https://arxiv.org/pdf/2602.11098v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.13327v1",
      "title": "Uncertainty in space, time, and motion on the special Galilean group",
      "abstract": "Classical mechanics unfolds within absolute time and Euclidean space, yet our knowledge of where events occur, when they occur, and how motion evolves is inherently uncertain. The special Galilean group provides a natural setting for describing classical spacetime, combining absolute time, Euclidean space, and inertial motion within a single Lie group structure. Although this framework is well known, representing and propagating uncertainty on the group has received comparatively little attention. In this work, we bring together existing results on the structure of the Galilean group and use this unified framework to express uncertainty directly on the group manifold. A main contribution is a compact, closed-form expression for the Galilean group Jacobian, which enables principled uncertainty propagation when composing Galilean transformations. We show that uncertainty in spatial position and orientation, temporal displacement, and inertial motion are intrinsically coupled through the underlying group structure. To illustrate the usefulness of the Galilean framework, we consider the problem of estimating a time-varying transformation between inertial frames from noisy observations collected at distinct instants in time. We show that performing estimation directly on the Galilean group yields substantially more statistically consistent estimates than formulations that treat time independently. Together, these results provide a geometric foundation for reasoning about uncertainty in space, time, and motion in classical mechanics, navigation, and robotics.",
      "authors": [
        "Jonathan Kelly",
        "Matthew Giamou"
      ],
      "primary_category": "physics.gen-ph",
      "categories": [
        "physics.gen-ph"
      ],
      "published": "2026-02-11 03:46:24+00:00",
      "link": "https://arxiv.org/pdf/2602.13327v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10405v1",
      "title": "Pivoting as an Adaptive Strategy to Geopolitical Tensions in U.S. Science",
      "abstract": "Geopolitical tensions increasingly reshape the structure and openness of global science, yet we still lack a clear understanding of how successfully scientists adapt their work under such pressures. Using millions of funding and publication datasets across the past ten years, we investigate how U.S. China geopolitical tensions reshaped individual research activities of U.S. based scientists, particularly those collaborating with Chinese peers. We find that although U.S. China geopolitical tensions significantly reduce funding opportunities, many scientists actively respond by pivoting their research portfolios toward alternative topics, and this adaptive reorientation partially mitigates funding losses. Crucially, the effectiveness of this adaptive strategy is highly unequal: for scientists in high risk domains, those of Asian descent, and early-career scientists, pivoting offers only limited protection against funding loss. Our results demonstrate that geopolitical tensions reshape science through shifts in scientists' strategic decisions about their research focus. Understanding this adaptive but uneven reconfiguration is essential for science policies to strengthen the resilience and inclusiveness of the scientific enterprise.",
      "authors": [
        "Moxin Li",
        "Yifang Ma",
        "Yang Wang",
        "Dashun Wang"
      ],
      "primary_category": "physics.soc-ph",
      "categories": [
        "physics.soc-ph"
      ],
      "published": "2026-02-11 01:19:49+00:00",
      "link": "https://arxiv.org/pdf/2602.10405v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09792v1",
      "title": "Variability in Performance of a Machine-Learning Seismicity Catalog: Central Italy, 2016-2017",
      "abstract": "Machine learning (ML) catalogs contain many more earthquakes than routine catalogs, but their performance in phase picking and earthquake detection has not been fully evaluated. We develop station-level detection probabilities using logistic regression and combine them across a seismic network to compute spatial magnitude-of-completeness fields. We apply this approach to two catalogs from the 2016-2017 Central Italy sequence that were constructed from the same seismic network, one routine and one ML based. At the station level, the ML picker increases detection sensitivity by identifying smaller magnitude events and detecting earthquakes at greater distances. Spatially, the magnitude-of-completeness decreases substantially, with median values shifting from 1.6 to 0.5 for P waves and from 1.7 to 0.5 for S waves. However, the ML catalog also shows greater variability in station-level performance than the routine catalog. These results demonstrate that ML-based improvements in detectability are widespread but spatially non-uniform, highlighting their benefits, their limitations, and the potential for further improvements.",
      "authors": [
        "Jaehong Chung",
        "Yifan Yu",
        "Lauro Chiaraluce",
        "Maddalena Michele",
        "Gregory C. Beroza"
      ],
      "primary_category": "physics.geo-ph",
      "categories": [
        "physics.geo-ph"
      ],
      "published": "2026-02-10 13:57:19+00:00",
      "link": "https://arxiv.org/pdf/2602.09792v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09550v1",
      "title": "From Search to GenAI Queries: Global Trends in Physics Information-Seeking Across Topics and Regions",
      "abstract": "The emergence of generative artificial intelligence (GenAI) marks a potential inflection point in the way academic information is accessed, raising fundamental questions about the evolving role of search in student learning. This study examines this shift by analyzing longitudinal trends in physics-related search and page-view activity, using declines in traditional search behavior as a quantitative proxy for changes in independent information-seeking practices. We analyze Google Trends data for core concepts in Classical Mechanics and Electromagnetism across three academic years (2022-2025) in more than 20 countries, and complement this analysis with Wikipedia page-view data across seven major languages to establish platform independence. The results reveal a substantial, systematic, and persistent global decline in search and page-view activity across most examined physics topics. The magnitude of this decline is domain-dependent, with Mechanics-related content exhibiting sharper and more consistent reductions than Electromagnetism-related content. Pronounced geographic and linguistic heterogeneity is observed: while English-speaking regions show relative stability or only moderate declines, non-English-speaking regions exhibit substantially larger reductions in traditional, search-based information-seeking activity. Despite the overall decrease in volume, the seasonal structure characteristic of academic activity remains robust. Taken together, these findings indicate a redistribution of physics-related information-seeking behavior in academic contexts where generative tools are increasingly available.",
      "authors": [
        "Yossi Ben-Zion",
        "Omer Michaeli",
        "Noah D. Finkelstein"
      ],
      "primary_category": "physics.ed-ph",
      "categories": [
        "physics.ed-ph"
      ],
      "published": "2026-02-10 08:57:55+00:00",
      "link": "https://arxiv.org/pdf/2602.09550v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09519v1",
      "title": "Multimode fiber laser cavities as nonlinear optical processors",
      "abstract": "Optical computing provides a promising path toward energy-efficient machine learning, yet implementing nonlinear transformations without complex electronics or high-power sources remains challenging. Here, we demonstrate that continuous-wave multimode fiber laser cavities can function as nonlinear optical processors. Input images encoded as phase patterns on a spatial light modulator undergo high-dimensional transformation through the interplay of multimode interference and gain saturation dynamics. The cavity maps input data into spatially stable, class-separable intensity distributions, enabling a simple linear classifier to achieve accuracies of 85--99\\% across diverse benchmarks -- including medical imaging and remote sensing -- with orders of magnitude fewer trainable parameters than deep neural networks. Our results establish multimode fiber lasers as compact, low-power physical processors for scalable optical machine learning.",
      "authors": [
        "Dilem Eşlik",
        "Bahadır Utku Kesgin",
        "Fatma Nur Kılınç",
        "Uğur Teğin"
      ],
      "primary_category": "physics.optics",
      "categories": [
        "physics.optics"
      ],
      "published": "2026-02-10 08:23:02+00:00",
      "link": "https://arxiv.org/pdf/2602.09519v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16665v1",
      "title": "Optimizing p-spin models through hypergraph neural networks and deep reinforcement learning",
      "abstract": "p-spin glasses, characterized by frustrated many-body interactions beyond the conventional pairwise case (p>2), are prototypical disordered systems whose ground-state search is NP-hard and computationally prohibitive for large instances. Solving this problem is not only fundamental for understanding high-order disorder, structural glasses, and topological phases, but also central to a wide spectrum of hard combinatorial optimization tasks. Despite decades of progress, there still lacks an efficient and scalable solver for generic large-scale p-spin models. Here we introduce PLANCK, a physics-inspired deep reinforcement learning framework built on hypergraph neural networks. PLANCK directly optimizes arbitrary high-order interactions, and systematically exploits gauge symmetry throughout both training and inference. Trained exclusively on small synthetic instances, PLANCK exhibits strong zero-shot generalization to systems orders of magnitude larger, and consistently outperforms state-of-the-art thermal annealing methods across all tested structural topologies and coupling distributions. Moreover, without any modification, PLANCK achieves near-optimal solutions for a broad class of NP-hard combinatorial problems, including random k-XORSAT, hypergraph max-cut, and conventional max-cut. The presented framework provides a physics-inspired algorithmic paradigm that bridges statistical mechanics and reinforcement learning. The symmetry-aware design not only advances the tractable frontiers of high-order disordered systems, but also opens a promising avenue for machine-learning-based solvers to tackle previously intractable combinatorial optimization challenges.",
      "authors": [
        "Li Zeng",
        "Mutian Shen",
        "Tianle Pu",
        "Zohar Nussinov",
        "Qing Feng",
        "Chao Chen",
        "Zhong Liu",
        "Changjun Fan"
      ],
      "primary_category": "cond-mat.dis-nn",
      "categories": [
        "cond-mat.dis-nn",
        "physics.comp-ph"
      ],
      "published": "2026-02-18 18:05:19+00:00",
      "link": "https://arxiv.org/pdf/2602.16665v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.16437v1",
      "title": "Mapping tuberculosis fatalities by region and age group in South Korea: A dataset for targeted health policy optimization",
      "abstract": "In South Korea, age-disaggregated tuberculosis (TB) data at the district level are not publicly available due to privacy constraints, limiting fine-scale analyses of healthcare accessibility. To address this limitation, we present a high-resolution, district-level dataset on tuberculosis (TB) fatalities and hospital accessibility in South Korea, covering the years 2014 to 2022 across 228 districts. The dataset is constructed using a reconstruction method that infers age-disaggregated TB cases and fatalities at the district level by integrating province-level age-specific statistics with district-level spatial and demographic data, enabling analyses that account for both spatial heterogeneity and age structure. Building on an existing hospital allocation framework, we extend the objective function to an age-weighted formulation and apply it to the reconstructed dataset to minimize TB fatalities under different age-weighting schemes. We demonstrate that incorporating age structure can give rise to distinct optimized hospital allocation patterns, even when the total number of minimized fatalities is similar, revealing trade-offs between efficiency and demographic targeting. In addition, the dataset supports temporal analyses of TB burden, hospital availability, and demographic variation over time, and provides a testbed for spatial epidemiology and optimization studies that require high-resolution demographic and healthcare data.",
      "authors": [
        "Yongsung Kwon",
        "Deok-Sun Lee",
        "Mi Jin Lee",
        "Seung-Woo Son"
      ],
      "primary_category": "physics.soc-ph",
      "categories": [
        "physics.soc-ph"
      ],
      "published": "2026-02-18 13:15:56+00:00",
      "link": "https://arxiv.org/pdf/2602.16437v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15990v1",
      "title": "Memristive tabular variational autoencoder for compression of analog data in high energy physics",
      "abstract": "We present an implementation of edge AI to compress data on an in-memory analog content-addressable memory (ACAM) device. A variational autoencoder is trained on a simulated sample of energy measurements from incident high-energy electrons on a generic three-layer scintillator-based calorimeter. The encoding part is distilled into tabular format by regressing the latent space variables using decision trees, which is then programmed on a memristor-based ACAM. In real-time, the ACAM compresses 48 continuously valued incoming energies measured by the calorimeter sensors into the latent space, achieving a compression factor of 12x, which is transmitted off-detector for decompression. The performance result of the ACAM, obtained using the Structural Simulation Toolkit, the SST open source framework, gives a latency value of 24 ns and a throughput of 330M compressions per second, i.e., 3 ns between successive inputs, and an average energy consumption of 4.1 nJ per compression.",
      "authors": [
        "Rajat Gupta",
        "Yuvaraj Elangovan",
        "Tae Min Hong",
        "James Ignowski",
        "John Moon",
        "Aishwarya Natarajan",
        "Stephen Roche",
        "Luca Buonanno"
      ],
      "primary_category": "physics.ins-det",
      "categories": [
        "physics.ins-det",
        "hep-ex",
        "physics.data-an"
      ],
      "published": "2026-02-17 20:26:56+00:00",
      "link": "https://arxiv.org/pdf/2602.15990v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15946v1",
      "title": "On-chip probabilistic inference for charged-particle tracking at the sensor edge",
      "abstract": "Modern scientific instruments operate under increasingly extreme constraints on bandwidth, latency, and power. Inference at the sensor edge determines experimental data collection efficiency by deciding which information to save for further analysis. Particle tracking detectors at the Large Hadron Collider exemplify this challenge: pixelated silicon sensors generate rich spatiotemporal ionization patterns, yet most of this information is discarded due to data-rate limitations. Concurrently, advancements in co-design tools provide rapid turn-around for incorporating machine learning into application-specific integrated circuits, motivating designs for particle detectors with new integrated technologies. We demonstrate that neural networks embedded in the front-end electronics can infer charged-particle kinematic parameters from a single silicon layer. We regress hit positions and incident angles with calibrated uncertainties, while satisfying stringent constraints on numerical precision, latency, and silicon area. Our results establish a path toward probabilistic inference directly at the edge, opening new opportunities for intelligent sensing in high-rate scientific instruments.",
      "authors": [
        "Arghya Ranjan Das",
        "David Jiang",
        "Rachel Kovach-Fuentes",
        "Shiqi Kuang",
        "Ana Sofía Calle Muñoz",
        "Danush Shekar",
        "Jennet Dickinson",
        "Giuseppe Di Guglielmo",
        "Lindsey Gray",
        "Mia Liu",
        "Corrinne Mills",
        "Mark S. Neubauer",
        "Daniel Abadjiev",
        "Anthony Badea",
        "Doug Berry",
        "Karri DiPetrillo",
        "Farah Fahim",
        "Abhijith Gandrakota",
        "Harshul Gupta",
        "James Hirschauer",
        "Eliza Howard",
        "Ron Lipton",
        "Petar Maksimovic",
        "Nick Manganelli",
        "Benjamin Parpillon",
        "Jannicke Pearkes",
        "Ricardo Silvestre",
        "Morris Swartz",
        "Chinar Syal",
        "Nhan Tran",
        "Amit Trivedi",
        "Keith Ulmer",
        "Mohammad Abrar Wadud",
        "Benjamin Weiss",
        "Eric You"
      ],
      "primary_category": "physics.ins-det",
      "categories": [
        "physics.ins-det",
        "hep-ex"
      ],
      "published": "2026-02-17 19:03:17+00:00",
      "link": "https://arxiv.org/pdf/2602.15946v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15780v1",
      "title": "Deep Learning for Point Spread Function Modeling in Cosmology",
      "abstract": "We present the development of a data-driven, AI-based model of the Point Spread Function (PSF) that achieves higher accuracy than the current state-of-the-art approach, \"PSF in the Full Field-of-View'' (PIFF). PIFF is widely used in leading weak-lensing surveys, including the Dark Energy Survey (DES), the Hyper Suprime-Cam (HSC) Survey, and the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST). The PSF characterizes how a point source, such as a star, is imaged after its light traverses the atmosphere and telescope optics, effectively representing the \"blurred fingerprint'' of the entire imaging system. Accurate PSF modeling is essential for weak gravitational lensing analyses, as biases in its estimation propagate directly into cosmic shear measurements -- one of the primary cosmological probes of the expansion history of the Universe and the growth of large-scale structure for dark energy studies. To address the limitations of PIFF, which constructs PSF models independently for each CCD and therefore loses spatial coherence across the focal plane, we introduce a deep-learning-based framework for PSF reconstruction. In this approach, an autoencoder is trained on stellar images obtained with the Hyper Suprime-Cam (HSC) of the Subaru Telescope and combined with a Gaussian process to interpolate the PSF across the telescope's full field of view. This hybrid model captures systematic variations across the focal plane and achieves a reconstruction error of $3.4 \\times 10^{-6}$ compared to PIFF's $3.7 \\times 10^{-6}$, laying the foundation for integration into the LSST Science Pipelines.",
      "authors": [
        "Dayana Andrea Henao Arbeláez",
        "Pierre-François Léget",
        "Andrés Alejandro Plazas Malagón"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO",
        "physics.data-an"
      ],
      "published": "2026-02-17 18:12:53+00:00",
      "link": "https://arxiv.org/pdf/2602.15780v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15743v1",
      "title": "Physics-informed data-driven inference of an interpretable equivariant LES model of incompressible fluid turbulence",
      "abstract": "Restrictive phenomenological assumptions represent a major roadblock for the development of accurate subgrid-scale models of fluid turbulence. Specifically, these assumptions limit a model's ability to describe key quantities of interest, such as local fluxes of energy and enstrophy, in the presence of diverse coherent structures. This paper introduces a symbolic data-driven subgrid-scale model that requires no phenomenological assumptions and has no adjustable parameters, yet it outperforms leading LES models. A combination of a priori and a posteriori benchmarks shows that the model produces accurate predictions of various quantities including local fluxes across a broad range of two-dimensional turbulent flows. While the model is inferred using LES-style spatial coarse-graining, its structure is more similar to RANS models, as it employs an additional field to describe subgrid scales. We find that this field must have a rank-two tensor structure in order to correctly represent both the components of the subgrid-scale stress tensor and the various fluxes.",
      "authors": [
        "Matteo Ugliotti",
        "Brandon Choi",
        "Mateo Reynoso",
        "Daniel R. Gurevich",
        "Roman O. Grigoriev"
      ],
      "primary_category": "physics.flu-dyn",
      "categories": [
        "physics.flu-dyn",
        "physics.comp-ph"
      ],
      "published": "2026-02-17 17:17:09+00:00",
      "link": "https://arxiv.org/pdf/2602.15743v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15345v1",
      "title": "Machine learning electronic structure and atomistic properties from the external potential",
      "abstract": "Electronic structure calculations remain a major bottleneck in atomistic simulations and, not surprisingly, have attracted significant attention in machine learning (ML). Most existing approaches learn a direct map from molecular geometries, typically represented as graphs or encoded local environments, to molecular properties or use ML as a surrogate for electronic structure theory by targeting quantities such as Fock or density matrices expressed in an atomic orbital (AO) basis.   Inspired by the Hohenberg-Kohn theorem, in this work, we propose an operator-centered framework in which the external (nuclear) potential, expressed in an AO basis, serves as the model input. From this operator, we construct hierarchical, body-ordered representations of atomic configurations that closely mirror the principles underlying several popular atom-centered descriptors. At the same time, the matrix-valued nature of the external potential provides a natural connection to equivariant message-passing neural networks. In particular, we show that successive products of the external potential provide a scalable route to equivariant message passing and enable an efficient description of long-range effects. We demonstrate that this approach can be used to model molecular properties, such as energies and dipole moments, from the external potential, or learn effective operator-to-operator maps, including mappings to the Fock matrix and the reduced density matrix from which multiple molecular observables can be simultaneously derived.",
      "authors": [
        "Jigyasa Nigam",
        "Tess Smidt",
        "Geneviève Dusson"
      ],
      "primary_category": "physics.chem-ph",
      "categories": [
        "physics.chem-ph",
        "physics.comp-ph"
      ],
      "published": "2026-02-17 04:23:03+00:00",
      "link": "https://arxiv.org/pdf/2602.15345v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15296v1",
      "title": "GPS constellation search for exotic physics messengers coincident with the binary neutron star merger GW170817",
      "abstract": "The Global Positioning System (GPS) includes a continuously operating, planet-scale network of atomic clocks that, beyond navigation and time dissemination, enables precision tests of fundamental physics. Here we use GPS carrier phase archival data to perform a retrospective search for exotic low-mass fields (ELFs) that might be emitted by the binary neutron-star merger GW170817, complementing gravitational wave and electromagnetic modalitiesnin multi-messenger astronomy. Such ultra-relativistic fields would imprint a dispersive, anti-chirp signature in clock-frequency time series, delayed with respect to the LIGO-Virgo gravitational wave detection. We construct network-median pseudo-frequency data from eighteen Rb satellite clocks referenced to a terrestrial hydrogen maser and conduct a template-bank search spanning ELF pulse duration, arrival delay, and characteristic frequency. No statistically significant signal is observed after accounting for noise statistics and template-bank trials. We derive 95\\% confidence-level lower bounds on the interaction energy scale $Λ_α$ of quadratic couplings driving variations in electromagnetic fine-structure constant. These limits improve upon existing astrophysical and gravity-test constraints across the ELF-energy range $\\approx10^{-18}$--$10^{-14}\\,\\mathrm{eV}$. This demonstrates that mature global satellite-clock networks provide an observational capability for retrospective, multi-messenger searches for new physics using decades of archival timing data.",
      "authors": [
        "Arko P. Sen",
        "Geoffrey Blewitt",
        "Andrey Sarantsev",
        "Paul Ries",
        "Andrei Derevianko"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM",
        "astro-ph.HE",
        "physics.atom-ph",
        "physics.data-an",
        "physics.ins-det"
      ],
      "published": "2026-02-17 01:40:17+00:00",
      "link": "https://arxiv.org/pdf/2602.15296v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12855v1",
      "title": "Lecture notes: From Gaussian processes to feature learning",
      "abstract": "These lecture notes develop the theory of learning in deep and recurrent neuronal networks from the point of view of Bayesian inference. The aim is to enable the reader to understand typical computations found in the literature in this field. Initial chapters develop the theoretical tools, such as probabilities, moment and cumulant-generating functions, and some notions of large deviation theory, as far as they are needed to understand collective network behavior with large numbers of parameters. The main part of the notes derives the theory of Bayesian inference for deep and recurrent networks, starting with the neural network Gaussian process (lazy-learning) limit, which is subsequently extended to study feature learning from the point of view of adaptive kernels. The notes also expose the link between the adaptive kernel approach and approaches of kernel rescaling.",
      "authors": [
        "Moritz Helias",
        "Javed Lindner",
        "Lars Schutzeichel",
        "Zohar Ringel"
      ],
      "primary_category": "cond-mat.dis-nn",
      "categories": [
        "cond-mat.dis-nn"
      ],
      "published": "2026-02-13 12:05:32+00:00",
      "link": "https://arxiv.org/pdf/2602.12855v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12167v1",
      "title": "Time-Structured Tail Probabilities for Ultra-High-Energy Gamma-Hadron Discrimination in Water-Cherenkov Arrays",
      "abstract": "Gamma-hadron discrimination based on shower observables is essential for identifying gamma-ray astrophysical sources at the highest energies. In this work, we introduce $P^{α, T}_{\\rm tail}$, a new discrimination variable for ultra-high-energy photon searches within the framework of a water-Cherenkov detector (WCD) array. The observable extends signal-integrated methods by incorporating the time structure of WCD traces, using cumulative signal distributions.   Using simulated proton- and gamma-induced air showers at energies around $10^{17}\\,\\mathrm{eV}$, we evaluate the performance of $P^{α, T}_{\\rm tail}$ and compare it with established WCD-based observables such as $S_b$, risetime-based variables, and the SWGO-inspired, $P^α_{\\rm tail}$. The new variable attains a background contamination of roughly $2 \\times 10^{-2}$ at $50\\%$ gamma efficiency, improving upon existing WCD-only methods by nearly a factor of five and approaching the performance of an idealized muon-isolating reference. These results demonstrate the effectiveness of exploiting time-resolved signal tails to enhance ultra-high-energy photon searches in sparse surface arrays.",
      "authors": [
        "Ruben Conceição",
        "Pedro J. Costa",
        "Mário Pimenta"
      ],
      "primary_category": "hep-ph",
      "categories": [
        "hep-ph",
        "hep-ex"
      ],
      "published": "2026-02-12 16:52:56+00:00",
      "link": "https://arxiv.org/pdf/2602.12167v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15295v1",
      "title": "Update to the U.S. National Input to the European Strategy Update for Particle Physics",
      "abstract": "In this document we update the status of U.S. community inputs for the European Strategy for Particle Physics Update (ESPPU) since April 1, 2025, and offer responses to the revised questions. Major new inputs include a long-term strategy report from the National Academies of Sciences, Engineering, and Medicine and the formal formation of a U.S. Muon Collider Collaboration.",
      "authors": [
        "André de Gouvêa",
        "Hitoshi Murayama",
        "Mark Palmer",
        "Heidi Schellman"
      ],
      "primary_category": "hep-ex",
      "categories": [
        "hep-ex",
        "hep-ph"
      ],
      "published": "2026-02-17 01:36:58+00:00",
      "link": "https://arxiv.org/pdf/2602.15295v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.15110v1",
      "title": "Measuring Pulsar Distances from Chirping Orbital Periods",
      "abstract": "The observed orbital period time derivative (or orbital \"chirp\") of a millisecond binary pulsar (MSP) encodes information about both the intrinsic properties of the binary system and its environment. Orbital chirp has contributions from intrinsic energy loss due to gravitational wave emission, kinematic effects due to motion in the plane of the sky, and dynamical effects due to galactic acceleration, with the latter two contributions depending on the MSP distance. We use orbital chirp data to infer distances to 21 MSPs; and for four of which we obtain smaller uncertainties than those reported in previous distance measurements. We incorporate multiple realistic galactic acceleration models to assess the sensitivity of the inferred distances to the choice of galactic gravitational potential, finding a significant dependence for four MSPs.",
      "authors": [
        "Brady Egleston",
        "Reza Ebadi",
        "Ronald Walsworth"
      ],
      "primary_category": "astro-ph.HE",
      "categories": [
        "astro-ph.HE",
        "astro-ph.CO",
        "astro-ph.GA",
        "gr-qc",
        "hep-ph"
      ],
      "published": "2026-02-16 19:00:02+00:00",
      "link": "https://arxiv.org/pdf/2602.15110v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.15104v1",
      "title": "ABCMB: A Python+JAX Package for the Cosmic Microwave Background Power Spectrum",
      "abstract": "We present ABCMB, a differentiable Einstein-Boltzmann solver for the cosmic microwave background (CMB). ABCMB is a complete code capturing important effects to linear order in $Λ{\\rm CDM}$ cosmology. It computes the CMB power spectrum and includes effects like lensing, polarization, massive neutrinos, and a state-of-the-art treatment of BBN and recombination. ABCMB has sub-percent-level agreement with CLASS and can be run on a GPU with competitive, and sometimes even faster, run times. It is refactored compared to previous codes and takes advantage of object-oriented programming to improve extensibility, meaning new physics can be added to it without the need for modifying source files. ABCMB provides accurate and stable gradients to the user, making Fisher analyses straightforward, and enabling the use of efficient gradient-based sampling methods.",
      "authors": [
        "Zilu Zhou",
        "Cara Giovanetti",
        "Hongwan Liu"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO",
        "hep-ph"
      ],
      "published": "2026-02-16 19:00:01+00:00",
      "link": "https://arxiv.org/pdf/2602.15104v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12781v1",
      "title": "Statistics of time and frequency-averaged spectra in gravitational-wave background searches",
      "abstract": "Time series analysis from gravitational-wave detectors often relies on the assumption that time chunks, or frequency bins, are uncorrelated. We discuss the validity of this approximation in the context of searches for stochastic gravitational-wave backgrounds. We examine the impact of averaging over time and frequency, a reduction technique commonly employed to minimize the computational expense of likelihood evaluations. We introduce an analytical tool based on Fisher information to quantify the error in parameter inference arising from ignoring these effects. Finally, we address the issue of locally stationary processes and optimal time chunking.",
      "authors": [
        "Quentin Baghi",
        "Nikolaos Karnesis",
        "Jean-Baptiste Bayle"
      ],
      "primary_category": "gr-qc",
      "categories": [
        "gr-qc",
        "astro-ph.IM"
      ],
      "published": "2026-02-13 10:05:02+00:00",
      "link": "https://arxiv.org/pdf/2602.12781v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12011v1",
      "title": "pespace: A new tool of GPU-accelerated and auto-differentiable response generation and likelihood evaluation for space-borne gravitational wave detectors",
      "abstract": "Space-borne gravitational wave detectors will expand the scope of gravitational wave astronomy to the milli-Hertz band in the near future. The development of data analysis software infrastructure at the current stage is crucial. In this paper, we introduce \\texttt{pespace} which can be used for the full Bayesian parameter estimation of massive black hole binaries with detectors including LISA, Taiji, and Tianqin. The core computations are implemented using the high-performance parallel programming framework \\texttt{taichi-lang} which enables automatic differentiation and hardware acceleration across different architectures. We also reimplement the waveform models \\texttt{PhenomXAS} and \\texttt{PhenomXHM} in the separate package \\texttt{tiwave} to integrate waveform generation within the \\texttt{taichi-lang} scope, making the entire computation accelerated and differentiable. To demonstrate the functionality of the tool, we use a typical signal from a massive black hole binary to perform the full Bayesian parameter estimation with the complete likelihood function for three scenarios: including a single detector using the waveform with only the dominant mode; a single detector using the waveform including higher modes; and a detector network with higher modes included. The results demonstrate that higher modes are essential in breaking degeneracies, and coincident observations by the detector network can significantly improve the measurement of source properties. Additionally, automatic differentiation provides an accurate way to obtain the Fisher matrix without manual fine-tuning of the finite difference step size. Using a subset of extrinsic parameters, we show that the approximated posteriors obtained by the Fisher matrix agree well with those derived from Bayesian parameter estimation.",
      "authors": [
        "Rui Niu",
        "Chang Feng",
        "Wen Zhao"
      ],
      "primary_category": "gr-qc",
      "categories": [
        "gr-qc",
        "astro-ph.IM"
      ],
      "published": "2026-02-12 14:41:08+00:00",
      "link": "https://arxiv.org/pdf/2602.12011v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.14547v1",
      "title": "Multiscale feature integration network for inpainting of full-sky CMB $B$-modes",
      "abstract": "Foreground masking and incomplete sky coverage complicate CMB polarization analyses by inducing mode coupling and imperfect E/B separation, with particularly strong impact on searches for primordial $B$-modes. We present SkyReconNet-P, a convolutional neural network for inpainting CMB polarization maps that extends the SkyReconNet framework to jointly reconstruct the polarization $(Q,U)$ maps from partial-sky observations. The method combines regional processing with a hybrid design, utilizing standard convolution and dilated convolution to do a multiscale feature integration. We evaluate performance at both the map and power spectrum level using two masking scenarios: a generated random mask and the Planck 2018 common polarization inpainting mask. For both masking scenarios, SkyReconNet-P reproduces the large-scale morphology of the target maps. In power-spectrum space, we find that the reconstructed $E$-mode spectrum closely tracks the target at low multipoles, while small biases emerge at higher $\\ell$. For $B$-mode, the raw reconstructed spectra exhibit a larger multipole-dependent bias, which we mitigate using a simulation-based linear calibration. We show that the calibrated $B$-mode spectrum preserve more information by comparing it with spectrum estimation using pseudo-$C_\\ell$. Finally, we demonstrate cosmological parameter inference from calibrated reconstructed spectra by fitting $(r, A_{\\rm lens})$ with a Gaussian bandpower likelihood, recovering posteriors consistent with injected parameters across three test ensembles down to $r \\sim 10^{-3}$. These results support inpainting as a complementary route to cut-sky approaches when downstream pipelines can greatly benefit from statistically well-characterized, gap-filled polarization maps.",
      "authors": [
        "Reyhan D. Lambaga",
        "Vipin Sudevan",
        "Pisin Chen"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO"
      ],
      "published": "2026-02-16 08:09:17+00:00",
      "link": "https://arxiv.org/pdf/2602.14547v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13902v1",
      "title": "J-PAS: Semi-Supervised Sim-to-Obs Transfer for Robust Star--Galaxy--Quasar Classification",
      "abstract": "Modern studies in astrophysics and cosmology increasingly rely on simulations and cross-survey analyses, yet differences in data generation, instrumentation, calibration, and unmodeled physics introduce distribution mismatches between datasets (``domain shift''). In machine-learning pipelines, this occurs when the joint distribution of inputs and labels differs between the training (source) and application (target) domains, causing source-trained models to underperform on the target. Transfer learning and domain adaptation provide principled ways to mitigate this effect. We study a concrete simulation-to-observation case: semi-supervised domain adaptation (SSDA) to transfer a four-class spectral classifier -- high-redshift quasars, low-redshift quasars, galaxies, and stars -- from J-PAS mock catalogs based on DESI spectra to real J-PAS observations. Our pipeline pretrains on abundant labeled DESI$\\rightarrow$J-PAS mocks and adapts to the target domain using a small labeled J-PAS subset. We benchmark SSDA against two baselines: a J-PAS--only supervised model trained with the same target-label budget, and a mocks-only model evaluated on held-out J-PAS data. On this held-out J-PAS data, SSDA achieves a macro-F1 score (balancing precision and recall) of $0.82$ and an overall true positive rate of $0.89$, compared to $0.79/0.85$ for the J-PAS--only baseline and $0.73/0.87$ for the mocks-only model. The gains are driven primarily by improved quasar classification, especially in the high-redshift subclass ($\\mathrm{F1}=0.66$ vs.\\ $0.55/0.37$), yielding better-calibrated candidate lists for spectroscopic targeting (e.g., WEAVE-QSO) and AGN searches. This study shows how modest target supervision enables robust, data-efficient simulation-to-observation transfer when simulations are plentiful but target labels are scarce.",
      "authors": [
        "Daniel López-Cano",
        "L. Raul Abramo",
        "L. Nakazono",
        "I. Pérez-Ràfols",
        "G. Martínez-Solaeche",
        "J. Chaves-Montero",
        "Matthew M. Pieri",
        "Jailson Alcaniz",
        "Narciso Benitez",
        "Silvia Bonoli",
        "Saulo Carneiro",
        "Javier Cenarro",
        "David Cristóbal-Hornillos",
        "Simone Daflon",
        "Renato Dupke",
        "Alessandro Ederoclite",
        "Rosa González Delgado",
        "Antonio Hernán-Caballero",
        "Carlos Hernández-Monteagudo",
        "Jifeng Liu",
        "Carlos López-Sanjuan",
        "Antonio Marín-Franch",
        "Claudia Mendes de Oliveira",
        "Mariano Moles",
        "Fernando Roig",
        "Laerte Sodré",
        "Keith Taylor",
        "Jesús Varela",
        "Héctor Vázquez Ramió",
        "Jose Vilchez",
        "Javier Zaragoza-Cardiel"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO"
      ],
      "published": "2026-02-14 21:51:38+00:00",
      "link": "https://arxiv.org/pdf/2602.13902v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13198v1",
      "title": "$\\texttt{GPUmonty}$: A GPU-accelerated relativistic Monte Carlo radiative transfer code",
      "abstract": "We introduce $\\texttt{GPUmonty}$, a CUDA/C-based Monte Carlo radiative transfer code accelerated using graphics processing units (GPUs). $\\texttt{GPUmonty}$ derives from the CPU-based code $\\texttt{grmonty}$ and offloads the most computationally expensive stages of the calculation -- superphoton generation, sampling, tracking, and scattering -- to the GPU. Whereas $\\texttt{grmonty}$ handles photons sequentially, $\\texttt{GPUmonty}$ processes large numbers of superphotons concurrently, leveraging the single-instruction, multiple-thread (SIMT) execution model of modern GPUs. Benchmarks demonstrate a speedup of about $12\\times$ relative to the original CPU implementation on a single GPU, with runtime limited primarily by register pressure rather than compute or memory bandwidth saturation. We validate the implementation through analytic tests for a optically thin synchrotron sphere, as well as comparisons with $\\texttt{igrmonty}$ for scattering synchrotron sphere and GRMHD simulation data. Relative errors remain below a percent level and convergence is consistent with the expected $N_{\\rm s}^{-1/2}$ Monte Carlo scaling. By significantly reducing computational costs, GPUmonty enables the extensive parameter space surveys and faster spectra modeling required to interpret horizon-scale observations of supermassive black holes. $\\texttt{GPUmonty}$ is publicly available under the GNU General Public License.",
      "authors": [
        "Pedro Naethe Motta",
        "Rodrigo Nemmen",
        "Abhishek V. Joshi"
      ],
      "primary_category": "astro-ph.HE",
      "categories": [
        "astro-ph.HE"
      ],
      "published": "2026-02-13 18:59:59+00:00",
      "link": "https://arxiv.org/pdf/2602.13198v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.13036v1",
      "title": "Photometric classification of supernovae detected by the Zwicky Transient Facility using noise augmentation",
      "abstract": "Modern time-domain surveys, such as the Zwicky Transient Facility (ZTF), detect far more extragalactic transients than can be spectroscopically classified. Photometric classification offers a scalable alternative, enabling the identification of larger, fainter, and higher-redshift supernova samples suitable for applications such as Type Ia supernova (SN Ia) cosmology. We present a feature-based photometric classifier for SNe detected by ZTF, with the primary goal of constructing a photometric SN Ia sample for cosmological analyses. Our approach utilises the autoencoder architecture of ParSNIP (Boone 2021) to capture the intrinsic diversity of SN light curves. We trained the model on a spectroscopically classified ZTF SN sample, incorporating a realistic noise augmentation procedure that simulates the flux uncertainties of fainter sources. Light curve features were used to train a gradient-boosted decision tree classifier, implemented in both binary (SN Ia vs. non-Ia) and multi-class configurations. We validated our classifier on independent, fainter ZTF data with and without noise augmentation. To evaluate real-time performance, we also applied our classifier to live ZTF alerts and conducted a spectroscopic classification survey within the ePESSTO+ collaboration. We found that noise augmentation significantly improves classification performance, particularly for fainter sources. Our binary classifier achieves an SN Ia recall of (98.1 $\\pm$ 0.4)%, averaged across five train-test splits. SN Ia recall exceeds 98% for events with a peak apparent magnitude up to 20 and more than 10 detections, and remains above 96% up to magnitude 20.5. Overall, 95% of sources were correctly classified in both binary and multi-class modes. Our classifier performs efficiently on real ZTF data and enables construction of a large photometric SN Ia sample for cosmology.",
      "authors": [
        "A. Townsend",
        "J. Nordin",
        "M. Kowalski",
        "S. Reusch",
        "J. P. Anderson",
        "E. C. Bellm",
        "U. Burgaz",
        "T. X. Chen",
        "T. -W. Chen",
        "G. Dimitriadis",
        "L. Galbany",
        "A. Goobar",
        "M. J. Graham",
        "M. Gromadzki",
        "C. P. Gutiérrez",
        "D. Hale",
        "C. Inserra",
        "M. Kasliwal",
        "Y. -L. Kim",
        "K. Maguire",
        "F. J. Masci",
        "T. E. Müller-Bravo",
        "D. A. Perley",
        "R. L. Riddle",
        "M. Rigault",
        "J. van Santen",
        "S. Schulze",
        "M. Smith",
        "J. Sollerman",
        "S. Yang"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO"
      ],
      "published": "2026-02-13 15:45:35+00:00",
      "link": "https://arxiv.org/pdf/2602.13036v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12955v1",
      "title": "Anomaly Hunter for Alerts (AHA): Anomaly Detection in the ZTF Transient Alert Stream",
      "abstract": "Modern time-domain surveys produce alert streams at a scale that makes exhaustive manual inspection infeasible, requiring automated methods to identify unusual transients for follow-up. In this work, we present an unsupervised anomaly detection pipeline applied to the ZTF alert stream using the Lasair broker. We define normal objects as SN Ia, SN II, and SN Ib/c. Anomalous objects include (i) more exotic transients (AGN, TDEs, SLSNe, CVs, and nuclear transients) and (ii) supernova-labeled objects, either spectroscopically or by Lasair, with anomalous properties, such as incorrect or absent host associations, or non-supernova-like light curves. Our pipeline consists of three independently trained simple autoencoders operating on distinct alert stream data products: object features, triplet image cutouts, and light curves. Each model is trained on predominantly normal transients, and performance is assessed using the recall of exotic objects and the purity of all anomalous objects across both a spectroscopically classified held-out test set and the live alert stream. In the test set, performance is evaluated at a fixed rank corresponding to the top ten scoring candidates, while in the alert stream it is evaluated using an anomaly threshold defined from test set behavior. Across both settings, the algorithms consistently recover exotic transients and anomalous supernovae among their top-ranked candidates. Over 25 days of live alert stream application, we identify 87 unusual supernova candidates for follow-up. The overlap between anomalies flagged by different autoencoders in the test set is non-existent, and in the alert stream is small, with maximum overlap between any two algorithms being 11 objects. The framework is data-efficient, requiring only a few thousand training examples, making it well suited for early and ongoing application to the Rubin Observatory alert stream.",
      "authors": [
        "Leyla Iskandarli",
        "Chris J. Lintott",
        "Steve Croft",
        "Heloise Stevance",
        "Joshua Weston"
      ],
      "primary_category": "astro-ph.SR",
      "categories": [
        "astro-ph.SR",
        "astro-ph.HE",
        "astro-ph.IM"
      ],
      "published": "2026-02-13 14:21:32+00:00",
      "link": "https://arxiv.org/pdf/2602.12955v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.12870v1",
      "title": "GAME: Genetic Algorithms with Marginalised Ensembles for model-independent reconstruction of cosmological quantities",
      "abstract": "Genetic Algorithms (GA) are a powerful tool for stochastic optimisation and non-parametric symbolic regression, already widely used in cosmology. They are capable of reconstructing analytical functions directly from data points without introducing new physical models. A limitation of this approach is that while the reconstructed function is very efficient at reproducing the behaviour of the data points, non-observable quantities involving derivatives are particularly sensitive to stochasticity, hyperparameters, and to the choice of the best-fit function obtained by the GA, which implies the risk of the algorithm getting stuck in a local minimum. In this work we propose an update to the GA methodology for the reconstruction of analytical functions that involves computing a weighted average of an ensemble of GA configurations (\\texttt{GAME}). We define the weights via a quantity that accounts for both the goodness-of-fit of the points and the smoothness of the resulting function. We also present a practical method to analytically estimate and correct the errors on the averaged function by combining a path-integral approach with an ensemble variance. We demonstrate the improvement offered by \\texttt{GAME} methodology on a generic test function. We then apply the new methodology to a non-parametric reconstruction of the Hubble rate $H(z)$ using Cosmic Chronometers data and, assuming a flat Friedmann-Lemaître-Robertson-Walker background and General Relativity, we infer the corresponding dark energy equation of state $w(z)$. Through consistency tests, we show that current data produces results compatible with $Λ$CDM, and that Stage IV cosmology surveys will allow GA reinforced with \\texttt{GAME} methodology to become an even more competitive tool for discriminating between different models.",
      "authors": [
        "Matteo Peronaci",
        "Matteo Martinelli",
        "Savvas Nesseris"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO"
      ],
      "published": "2026-02-13 12:20:46+00:00",
      "link": "https://arxiv.org/pdf/2602.12870v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.11264v1",
      "title": "Time delays and stationarity in quasar light curves",
      "abstract": "We present a fully Bayesian framework for time delay inference and stationarity tests in quasar light curves using marginalised Gaussian processes. The model separates a deterministic, non-stationary drift (piecewise linear mean) from stationary stochastic variability (Matérn and Spectral Mixture kernels), and jointly models multiple images with per-image microlensing. Bayesian evidence and parameter posteriors are obtained via nested sampling and marginalised over model choices. Applied to the quasars WFI J2033 - 4723, B 1608 + 656, and HE 0435 - 1223, we find strong evidence for non-stationarity in B 1608 + 656 and HE 0435 - 1223, while WFI J2033 - 4723 is consistent with stationarity. The stochastic component favours an Markovian exponential kernel for B 1608 + 656 and a non-Markovian Matérn-$\\frac32$ kernel for WFI J2033 - 4723 and HE 0435 - 1223. Multi-length-scale Spectral Mixture kernels are disfavoured. Time delays are shown to be robust to model assumptions and consistent with prior work within the error. We further identify and mitigate a likelihood pathology which biases toward large delays, providing a practical nested sampling convergence protocol.",
      "authors": [
        "Namu Kroupa",
        "David Yallup",
        "Will Handley"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO"
      ],
      "published": "2026-02-11 19:00:02+00:00",
      "link": "https://arxiv.org/pdf/2602.11264v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10207v1",
      "title": "Optimizing Deep Learning Photometric Redshifts for the Roman Space Telescope with HST/CANDELS",
      "abstract": "Photometric redshifts (photo-$z$'s) will be crucial for studies of galaxy evolution, large-scale structure, and transients with the Nancy Grace Roman Space Telescope. Deep learning methods leverage pixel-level information from ground-based images to achieve the best photo-$z$'s for low-redshift galaxies, but their efficacy at higher redshifts with deep, space-based imaging remains largely untested. We used Hubble Space Telescope CANDELS optical and near-infrared imaging to evaluate fully-supervised, self-supervised, and semi-supervised deep learning photo-$z$ algorithms out to $z\\sim3$. Compared to template-based and classical machine learning photometry methods, the fully-supervised and semi-supervised models achieved better performance. Our new semi-supervised model, PITA (Photo-$z$ Inference with a Triple-loss Algorithm), outperformed all others by learning from unlabeled and labeled data through a three-part loss function that incorporates images and colors for all objects as well as redshifts when available. PITA produces a latent space that varies smoothly in magnitude, color, and redshift, resulting in the best photo-$z$ performance even when the redshift training set was significantly reduced. In contrast, the self-supervised approach produced a latent space with significant color and redshift fluctuations that hindered photo-$z$ inference. Looking forward to Roman, we recommend using semi supervised deep learning to take full advantage of the information contained in the hundreds of millions of high-resolution images and color measurements, together with the limited redshift measurements available, to achieve the most accurate photo-$z$ estimates for both faint and bright sources.",
      "authors": [
        "Ashod Khederlarian",
        "Brett H. Andrews",
        "Jeffrey A. Newman",
        "Tianqing Zhang",
        "Biprateep Dey"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM",
        "astro-ph.GA"
      ],
      "published": "2026-02-10 19:01:05+00:00",
      "link": "https://arxiv.org/pdf/2602.10207v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10191v1",
      "title": "Machine Learning Methods for Stellar Collisions. I. Predicting Outcomes of SPH Simulations",
      "abstract": "Stellar collisions can occur frequently in dense cluster environments, and play a crucial role in producing exotic phenomena from blue stragglers in globular clusters to high-energy transients in galactic nuclei. Successive collisions and mergers of massive stars could also lead to the formation of massive black holes, serving as seeds for supermassive black hole in the early universe. While analytic fitting formulae exist for predicting collision outcomes, they do not generalize across different energy scales or stellar evolutionary phases. Smoothed particle hydrodynamics (SPH) simulations are often used to compute the outcomes of stellar collisions, but, even at low resolution, their computational cost makes running on-the-fly calculations during an $N$-body simulation quite challenging. Here we present a new grid of $27,720$ SPH calculations of main-sequence star collisions, spanning a wide range of masses, ages, relative velocities, and impact parameters. Using this grid, we train machine learning models to predict both collision outcomes (merger vs disruption, or flyby) and final remnant masses. We compare the performance of nearest neighbors, support vector machines, and neural networks, achieving classification balanced accuracy of $98.4\\%$, and regression relative errors as low as $0.11\\%$ and $0.15\\%$ for the final stars $1$ and $2$, respectively. We make our trained models publicly available as part of the package collAIder, enabling rapid predictions of stellar collision outcomes in $N$-body models of dense star cluster dynamics.",
      "authors": [
        "Elena González Prieto",
        "James C. Lombardi,",
        "Sanaea C. Rose",
        "Charles F. A. Gibson",
        "Christopher E. O'Connor",
        "Tjitske Starkenburg",
        "Fulya Kıroğlu",
        "Kyle Kremer",
        "Tristan C. Parmerlee",
        "Frederic A. Rasio"
      ],
      "primary_category": "astro-ph.HE",
      "categories": [
        "astro-ph.HE",
        "astro-ph.GA",
        "astro-ph.IM",
        "astro-ph.SR"
      ],
      "published": "2026-02-10 19:00:01+00:00",
      "link": "https://arxiv.org/pdf/2602.10191v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09670v1",
      "title": "Talking with the Latents -- how to convert your LLM into an astronomer",
      "abstract": "Recent advances in Large Language Models (LLMs) offer unique opportunities for scientific tasks, yet their ability to reason over complex numerical data remains largely unexplored. We propose a simple mechanism to introduce domain-specific physical knowledge into LLMs by fusing pre-trained latent physical features with a pre-trained language model. Our method employs a teacher-student knowledge distillation framework where a large LLM (teacher) generates synthetic question-answer supervision to transfer physical reasoning to a smaller LLM (student). The student is conditioned on latent physical features and trained via a lightweight adapter and Low-Rank Adaptation (LoRA). We demonstrate that this approach, applied to models with 1B, 8B, and 32B parameters, enables effective reasoning over real scientific data. Our models substantially outperform strong baselines, such as Gemini 3 Pro, across multiple downstream tasks without task-specific fine-tuning. We show that the model combines latent information with general physical understanding to predict complex properties and can be \"steered\" by identifying physically meaningful directions in the latent space. This allows for explicit physical manipulation and natural language interpretation of latent structures. While our experiments focus on astrophysics, the framework is domain-agnostic and applicable to various scientific fields. Our main contribution is a general framework for using LLMs as interpretable interfaces to scientific latent spaces, enabling a single model to perform diverse tasks through natural language guidance. This work marks a step toward developing scientifically capable and useful LLMs.",
      "authors": [
        "Ilay Kamai",
        "Marc-Huertas Company",
        "Mike J. Smith",
        "Hagai B. Perets"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM"
      ],
      "published": "2026-02-10 11:25:25+00:00",
      "link": "https://arxiv.org/pdf/2602.09670v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.10165v1",
      "title": "The indiscriminate adoption of AI threatens the foundations of academia",
      "abstract": "Artificial intelligence offers much promise, but its use in scientific research should be restrained so that the primary aim of academia -- advancing knowledge for humans -- is safeguarded.",
      "authors": [
        "Roberto Trotta"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM"
      ],
      "published": "2026-02-10 10:51:01+00:00",
      "link": "https://arxiv.org/pdf/2602.10165v1",
      "tags": [
        "query:SR"
      ]
    },
    {
      "id": "2602.09230v1",
      "title": "Modeling Redshift Uncertainties in Roman Weak Lensing Cosmology",
      "abstract": "Cosmological constraints using weak gravitational lensing measurements from the Roman Space Telescope will require a powerful method for modelling uncertainties in the galaxy redshift distribution. In this work, we use an optimized version of the principal component analysis (PCA) to model uncertainties in the full shape of the redshift distributions, a method proposed by \\cite{pca_method} and recently used in the Dark Energy Survey Y6 analysis. Here, we implement this new approach within the Roman High Latitude Imaging Survey (HLIS) Cosmology Project Infrastructure Team (PIT) pipeline, namely Cobaya-Cosmolike Joint Architecture (\\texttt{CoCoA}). To validate the PCA in mitigating biases on cosmological parameters, $S_8$ and $Ω_m$, we use a set of redshift distributions from \\texttt{Cardinal} generated for a variety of Roman configurations. Overall, when the simulated cosmic shear data vector is not strongly miscalibrated relative to the fiducial one, both the mean-shift and the PCA-based approaches produce consistent cosmological constraints when marginalizing over nuisance parameters. For mild to strong miscalibration, including additional PCs progressively mitigates biases in $S_8$ and $Ω_m$, and can achieve comparable performance with fewer parameters than the nine tomographic-bin mean-shift model.",
      "authors": [
        "Diogo H. F. de Souza",
        "Boyan Yin",
        "Tim Eifler",
        "Vivian Miranda",
        "Chun-Hao To",
        "Brett H. Andrews",
        "Katarina Markovič",
        "Eric Huff",
        "Michael A. Troxel",
        "Olivier Doré"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO"
      ],
      "published": "2026-02-09 21:53:49+00:00",
      "link": "https://arxiv.org/pdf/2602.09230v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.09223v1",
      "title": "astromorph: Self-supervised machine learning pipeline for astronomical morphology analysis",
      "abstract": "Modern telescopes generate increasingly large and diverse datasets, often consisting of complex and morphologically rich structures. To efficiently explore such data requires automated methods that can extract and organize physically meaningful information, ideally without the need for extensive manual interaction. We aim to provide a user-friendly implementation of a self-supervised machine learning framework to explore morphological properties of large datasets, based on the BYOL (Bootstrap Your Own Latents) method. By enabling the generation of meaningful image embeddings without manually labelled data, the framework will enable key tasks such as clustering, anomaly detection, and similarity based exploration. In contrast to existing BYOL implementations, astromorph accommodates data of varying dimensions and resolutions, including both single-channel FITS images and multi-channel spectral cubes. The package is built with usability in mind, offering streamlined pipeline scripts for ease of use as well as deeper customization options via PyTorch-based classes. To demonstrate the utility of astromorph, we apply it in two contrasting science cases representing different astronomical domains: images of protoplanetary disks observed with ALMA, and infrared dark clouds observed with Spitzer and Herschel. In both cases, we demonstrate how astromorph produces scientifically meaningful embeddings that capture morphological differences and similarities across large samples. astromorph enables users to apply a robust, label-free approach for uncovering morphological patterns in astronomical datasets. The successful application to two markedly different datasets suggest that the pipeline is broadly applicable across a wide range of imaging-rich astronomical context, providing a user friendly tool for advancing discovery in observational astronomy.",
      "authors": [
        "Per Bjerkeli",
        "Jouni Kainulainen",
        "Maria Carmen Toribio",
        "Leon Boschman",
        "Otoniel Maya Lucas"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM"
      ],
      "published": "2026-02-09 21:46:43+00:00",
      "link": "https://arxiv.org/pdf/2602.09223v1",
      "tags": [
        "keyword:SR"
      ]
    },
    {
      "id": "2602.16128v1",
      "title": "Graph--Based Event Fingerprints for Classifying Geomagnetic Storm--Driven Forbush Decreases",
      "abstract": "Forbush decreases (FDs) are transient depressions in the galactic cosmic-ray flux observed by global neutron-monitor networks and are commonly associated with interplanetary disturbances driven by coronal mass ejections and related shocks. Despite extensive observational work, quantitatively comparing FD morphology across events and linking it to storm severity remains challenging due to heterogeneous station responses, coverage gaps, and the multivariate nature of the network. This work introduces a graph-based event representation in which each FD is mapped to an event network constructed from pairwise dissimilarities between station response time series. A controlled sparse backbone is obtained via the minimum spanning tree, enabling comparable event graphs across cases. From each graph, a compact set of geometric/topological fingerprints is computed, including global integration measures, spectral summaries, mesoscopic structure, centrality aggregates, and complexity descriptors.   Predictive skill is assessed using strict leave-one-event-out validation over a pre-defined grid of distance metrics and distance-domain transformations, with selection criteria fixed \\emph{a priori}. The proposed fingerprints exhibit measurable signal for three tasks: (i) multi-class classification of geomagnetic storm intensity (G3/G4/G5) with moderate but consistent performance and errors dominated by adjacent categories; (ii) stronger binary severity screening ($\\ge$G4 vs.\\ G3) with high sensitivity to severe events; and (iii) drop regression with partial least squares achieving positive explained variance relative to a fold-wise mean baseline.",
      "authors": [
        "Juan D. Perez-Navarro",
        "D. Sierra-Porta"
      ],
      "primary_category": "astro-ph.IM",
      "categories": [
        "astro-ph.IM"
      ],
      "published": "2026-02-18 01:39:10+00:00",
      "link": "https://arxiv.org/pdf/2602.16128v1",
      "tags": [
        "keyword:SR"
      ]
    }
  ],
  "queries": [
    {
      "type": "keyword",
      "tag": "SR",
      "paper_tag": "keyword:SR",
      "query_text": "Symbolic regression for physics or astronomy applications",
      "sim_scores": {
        "2602.13021v2": {
          "score": 0.8176902532577515,
          "rank": 1
        },
        "2602.09223v1": {
          "score": 0.7779627442359924,
          "rank": 2
        },
        "2602.14520v1": {
          "score": 0.777483344078064,
          "rank": 3
        },
        "2602.10576v1": {
          "score": 0.776792049407959,
          "rank": 4
        },
        "2602.15603v1": {
          "score": 0.7737047672271729,
          "rank": 5
        },
        "2602.16015v1": {
          "score": 0.772821307182312,
          "rank": 6
        },
        "2602.15169v1": {
          "score": 0.772812008857727,
          "rank": 7
        },
        "2602.12082v1": {
          "score": 0.7715300917625427,
          "rank": 8
        },
        "2602.14335v1": {
          "score": 0.7704759240150452,
          "rank": 9
        },
        "2602.16436v1": {
          "score": 0.7655037641525269,
          "rank": 10
        },
        "2602.11097v1": {
          "score": 0.7639821767807007,
          "rank": 11
        },
        "2602.16167v1": {
          "score": 0.7629215121269226,
          "rank": 12
        },
        "2602.12143v1": {
          "score": 0.7626197338104248,
          "rank": 13
        },
        "2602.09132v1": {
          "score": 0.7620801329612732,
          "rank": 14
        },
        "2602.12259v1": {
          "score": 0.761853039264679,
          "rank": 15
        },
        "2602.11626v1": {
          "score": 0.7603222131729126,
          "rank": 16
        },
        "2602.14737v1": {
          "score": 0.7601678371429443,
          "rank": 17
        },
        "2602.14020v1": {
          "score": 0.7599745988845825,
          "rank": 18
        },
        "2602.14480v1": {
          "score": 0.7598394155502319,
          "rank": 19
        },
        "2602.14663v1": {
          "score": 0.7595838904380798,
          "rank": 20
        },
        "2602.09530v1": {
          "score": 0.7594612836837769,
          "rank": 21
        },
        "2602.10097v1": {
          "score": 0.7593997120857239,
          "rank": 22
        },
        "2602.09670v1": {
          "score": 0.7588582038879395,
          "rank": 23
        },
        "2602.11747v1": {
          "score": 0.7584466934204102,
          "rank": 24
        },
        "2602.09690v1": {
          "score": 0.758173406124115,
          "rank": 25
        },
        "2602.11320v2": {
          "score": 0.7580558061599731,
          "rank": 26
        },
        "2602.13098v1": {
          "score": 0.7573599815368652,
          "rank": 27
        },
        "2602.13690v1": {
          "score": 0.7570265531539917,
          "rank": 28
        },
        "2602.10605v1": {
          "score": 0.7567622661590576,
          "rank": 29
        },
        "2602.12975v1": {
          "score": 0.756553053855896,
          "rank": 30
        },
        "2602.09959v1": {
          "score": 0.7563755512237549,
          "rank": 31
        },
        "2602.14677v1": {
          "score": 0.7556992769241333,
          "rank": 32
        },
        "2602.15184v1": {
          "score": 0.7555519342422485,
          "rank": 33
        },
        "2602.12177v1": {
          "score": 0.7554610967636108,
          "rank": 34
        },
        "2602.09181v1": {
          "score": 0.7554609775543213,
          "rank": 35
        },
        "2602.14573v1": {
          "score": 0.7553710341453552,
          "rank": 36
        },
        "2602.14024v1": {
          "score": 0.7552967071533203,
          "rank": 37
        },
        "2602.15021v1": {
          "score": 0.7551456093788147,
          "rank": 38
        },
        "2602.11229v1": {
          "score": 0.754693329334259,
          "rank": 39
        },
        "2602.11738v1": {
          "score": 0.7546359300613403,
          "rank": 40
        },
        "2602.10330v2": {
          "score": 0.7545558214187622,
          "rank": 41
        },
        "2602.16264v1": {
          "score": 0.7543829083442688,
          "rank": 42
        },
        "2602.09851v1": {
          "score": 0.7541341781616211,
          "rank": 43
        },
        "2602.11995v1": {
          "score": 0.7539341449737549,
          "rank": 44
        },
        "2602.13362v1": {
          "score": 0.7536612153053284,
          "rank": 45
        },
        "2602.13902v1": {
          "score": 0.75346440076828,
          "rank": 46
        },
        "2602.14440v1": {
          "score": 0.7534359693527222,
          "rank": 47
        },
        "2602.13873v1": {
          "score": 0.7533583045005798,
          "rank": 48
        },
        "2602.09980v1": {
          "score": 0.7531687617301941,
          "rank": 49
        },
        "2602.15164v1": {
          "score": 0.7531687617301941,
          "rank": 50
        },
        "2602.10022v1": {
          "score": 0.7527580261230469,
          "rank": 51
        },
        "2602.12435v1": {
          "score": 0.7526466846466064,
          "rank": 52
        },
        "2602.10670v1": {
          "score": 0.7526266574859619,
          "rank": 53
        },
        "2602.13004v1": {
          "score": 0.7523917555809021,
          "rank": 54
        },
        "2602.15229v1": {
          "score": 0.7522189617156982,
          "rank": 55
        },
        "2602.14272v1": {
          "score": 0.7521184682846069,
          "rank": 56
        },
        "2602.15068v1": {
          "score": 0.7515435218811035,
          "rank": 57
        },
        "2602.09219v1": {
          "score": 0.7514622211456299,
          "rank": 58
        },
        "2602.12982v1": {
          "score": 0.7513948678970337,
          "rank": 59
        },
        "2602.09161v1": {
          "score": 0.751309871673584,
          "rank": 60
        },
        "2602.13485v1": {
          "score": 0.7513083219528198,
          "rank": 61
        },
        "2602.08983v1": {
          "score": 0.7512800097465515,
          "rank": 62
        },
        "2602.13531v1": {
          "score": 0.7511264681816101,
          "rank": 63
        },
        "2602.10191v1": {
          "score": 0.7509104013442993,
          "rank": 64
        },
        "2602.09572v2": {
          "score": 0.7508161664009094,
          "rank": 65
        },
        "2602.09285v1": {
          "score": 0.7507772445678711,
          "rank": 66
        },
        "2602.13550v1": {
          "score": 0.7504152059555054,
          "rank": 67
        },
        "2602.16503v1": {
          "score": 0.7501811981201172,
          "rank": 68
        },
        "2602.11557v1": {
          "score": 0.7501782178878784,
          "rank": 69
        },
        "2602.14280v1": {
          "score": 0.7500447630882263,
          "rank": 70
        },
        "2602.15632v1": {
          "score": 0.7499141693115234,
          "rank": 71
        },
        "2602.16146v1": {
          "score": 0.7496441006660461,
          "rank": 72
        },
        "2602.10378v1": {
          "score": 0.7496243119239807,
          "rank": 73
        },
        "2602.13871v1": {
          "score": 0.7495360374450684,
          "rank": 74
        },
        "2602.12471v1": {
          "score": 0.749176025390625,
          "rank": 75
        },
        "2602.14342v1": {
          "score": 0.7490928173065186,
          "rank": 76
        },
        "2602.13583v1": {
          "score": 0.749040424823761,
          "rank": 77
        },
        "2602.12391v1": {
          "score": 0.7487583160400391,
          "rank": 78
        },
        "2602.12267v1": {
          "score": 0.748432993888855,
          "rank": 79
        },
        "2602.09351v1": {
          "score": 0.7484203577041626,
          "rank": 80
        },
        "2602.12828v1": {
          "score": 0.7484118342399597,
          "rank": 81
        },
        "2602.09314v1": {
          "score": 0.748277485370636,
          "rank": 82
        },
        "2602.11374v1": {
          "score": 0.7482664585113525,
          "rank": 83
        },
        "2602.13184v1": {
          "score": 0.7480826377868652,
          "rank": 84
        },
        "2602.15136v1": {
          "score": 0.7475793957710266,
          "rank": 85
        },
        "2602.13807v1": {
          "score": 0.7475055456161499,
          "rank": 86
        },
        "2602.16091v1": {
          "score": 0.7474364638328552,
          "rank": 87
        },
        "2602.10332v1": {
          "score": 0.7471998929977417,
          "rank": 88
        },
        "2602.13498v1": {
          "score": 0.7471283674240112,
          "rank": 89
        },
        "2602.11841v1": {
          "score": 0.7470434904098511,
          "rank": 90
        },
        "2602.12534v1": {
          "score": 0.746656060218811,
          "rank": 91
        },
        "2602.09762v1": {
          "score": 0.746644914150238,
          "rank": 92
        },
        "2602.14470v1": {
          "score": 0.7466292977333069,
          "rank": 93
        },
        "2602.10905v1": {
          "score": 0.7465229630470276,
          "rank": 94
        },
        "2602.14583v1": {
          "score": 0.7464552521705627,
          "rank": 95
        },
        "2602.15390v1": {
          "score": 0.7463229894638062,
          "rank": 96
        },
        "2602.15297v1": {
          "score": 0.7462946176528931,
          "rank": 97
        },
        "2602.11090v1": {
          "score": 0.7462303638458252,
          "rank": 98
        },
        "2602.11208v1": {
          "score": 0.7462177872657776,
          "rank": 99
        },
        "2602.11825v1": {
          "score": 0.7461050748825073,
          "rank": 100
        },
        "2602.12234v1": {
          "score": 0.745931088924408,
          "rank": 101
        },
        "2602.15150v1": {
          "score": 0.7458131313323975,
          "rank": 102
        },
        "2602.15751v1": {
          "score": 0.7457808256149292,
          "rank": 103
        },
        "2602.16606v1": {
          "score": 0.7457648515701294,
          "rank": 104
        },
        "2602.12164v1": {
          "score": 0.7457517385482788,
          "rank": 105
        },
        "2602.15602v1": {
          "score": 0.745330274105072,
          "rank": 106
        },
        "2602.10420v1": {
          "score": 0.7452681660652161,
          "rank": 107
        },
        "2602.11443v1": {
          "score": 0.7452596426010132,
          "rank": 108
        },
        "2602.14656v1": {
          "score": 0.7451536655426025,
          "rank": 109
        },
        "2602.13104v2": {
          "score": 0.7449943423271179,
          "rank": 110
        },
        "2602.09340v1": {
          "score": 0.7449268102645874,
          "rank": 111
        },
        "2602.14708v1": {
          "score": 0.744504988193512,
          "rank": 112
        },
        "2602.13198v1": {
          "score": 0.7445032000541687,
          "rank": 113
        },
        "2602.13847v2": {
          "score": 0.7444248795509338,
          "rank": 114
        },
        "2602.09809v1": {
          "score": 0.744278609752655,
          "rank": 115
        },
        "2602.14423v1": {
          "score": 0.743832528591156,
          "rank": 116
        },
        "2602.10451v1": {
          "score": 0.7437896728515625,
          "rank": 117
        },
        "2602.11424v1": {
          "score": 0.743762731552124,
          "rank": 118
        },
        "2602.11325v2": {
          "score": 0.7437463402748108,
          "rank": 119
        },
        "2602.09783v1": {
          "score": 0.7437288165092468,
          "rank": 120
        },
        "2602.09539v1": {
          "score": 0.7437009811401367,
          "rank": 121
        },
        "2602.14890v1": {
          "score": 0.743476390838623,
          "rank": 122
        },
        "2602.14547v1": {
          "score": 0.7430701851844788,
          "rank": 123
        },
        "2602.10870v1": {
          "score": 0.743031919002533,
          "rank": 124
        },
        "2602.11107v1": {
          "score": 0.7430239915847778,
          "rank": 125
        },
        "2602.13813v1": {
          "score": 0.7429307699203491,
          "rank": 126
        },
        "2602.14683v1": {
          "score": 0.742897629737854,
          "rank": 127
        },
        "2602.09616v1": {
          "score": 0.7428369522094727,
          "rank": 128
        },
        "2602.11264v1": {
          "score": 0.7427494525909424,
          "rank": 129
        },
        "2602.11408v1": {
          "score": 0.7426349520683289,
          "rank": 130
        },
        "2602.14275v1": {
          "score": 0.7425953149795532,
          "rank": 131
        },
        "2602.13414v1": {
          "score": 0.7420317530632019,
          "rank": 132
        },
        "2602.13413v1": {
          "score": 0.7419725656509399,
          "rank": 133
        },
        "2602.11623v1": {
          "score": 0.7419160008430481,
          "rank": 134
        },
        "2602.13783v1": {
          "score": 0.7419111728668213,
          "rank": 135
        },
        "2602.09240v1": {
          "score": 0.7419052124023438,
          "rank": 136
        },
        "2602.11722v1": {
          "score": 0.7417880892753601,
          "rank": 137
        },
        "2602.12706v1": {
          "score": 0.7415440082550049,
          "rank": 138
        },
        "2602.10480v2": {
          "score": 0.7415226697921753,
          "rank": 139
        },
        "2602.10751v1": {
          "score": 0.7414906620979309,
          "rank": 140
        },
        "2602.11295v1": {
          "score": 0.7414398193359375,
          "rank": 141
        },
        "2602.15951v1": {
          "score": 0.7414368987083435,
          "rank": 142
        },
        "2602.13513v2": {
          "score": 0.741114616394043,
          "rank": 143
        },
        "2602.13063v1": {
          "score": 0.7411123514175415,
          "rank": 144
        },
        "2602.15781v1": {
          "score": 0.7411112785339355,
          "rank": 145
        },
        "2602.12368v1": {
          "score": 0.7410327792167664,
          "rank": 146
        },
        "2602.15457v1": {
          "score": 0.7409758567810059,
          "rank": 147
        },
        "2602.16166v1": {
          "score": 0.740928053855896,
          "rank": 148
        },
        "2602.15676v1": {
          "score": 0.7407509088516235,
          "rank": 149
        },
        "2602.15283v1": {
          "score": 0.7406081557273865,
          "rank": 150
        },
        "2602.14011v1": {
          "score": 0.7405877709388733,
          "rank": 151
        },
        "2602.12851v1": {
          "score": 0.740554928779602,
          "rank": 152
        },
        "2602.11690v1": {
          "score": 0.7405543327331543,
          "rank": 153
        },
        "2602.09988v1": {
          "score": 0.7404229044914246,
          "rank": 154
        },
        "2602.09519v1": {
          "score": 0.7404142022132874,
          "rank": 155
        },
        "2602.14913v1": {
          "score": 0.7402820587158203,
          "rank": 156
        },
        "2602.12170v1": {
          "score": 0.7402229905128479,
          "rank": 157
        },
        "2602.15004v1": {
          "score": 0.7401912212371826,
          "rank": 158
        },
        "2602.14701v1": {
          "score": 0.7400902509689331,
          "rank": 159
        },
        "2602.15423v1": {
          "score": 0.7398476600646973,
          "rank": 160
        },
        "2602.15104v1": {
          "score": 0.7397505640983582,
          "rank": 161
        },
        "2602.09303v1": {
          "score": 0.7397494316101074,
          "rank": 162
        },
        "2602.16530v1": {
          "score": 0.7397091388702393,
          "rank": 163
        },
        "2602.12334v1": {
          "score": 0.7396126985549927,
          "rank": 164
        },
        "2602.15920v1": {
          "score": 0.7396095395088196,
          "rank": 165
        },
        "2602.12592v1": {
          "score": 0.7395796775817871,
          "rank": 166
        },
        "2602.10867v1": {
          "score": 0.7395592927932739,
          "rank": 167
        },
        "2602.16075v1": {
          "score": 0.7395172715187073,
          "rank": 168
        },
        "2602.09182v1": {
          "score": 0.7394185066223145,
          "rank": 169
        },
        "2602.12976v1": {
          "score": 0.7393215298652649,
          "rank": 170
        },
        "2602.12105v1": {
          "score": 0.7392446994781494,
          "rank": 171
        },
        "2602.14881v1": {
          "score": 0.7390227913856506,
          "rank": 172
        },
        "2602.11467v1": {
          "score": 0.7389398217201233,
          "rank": 173
        },
        "2602.11333v1": {
          "score": 0.738799512386322,
          "rank": 174
        },
        "2602.11080v1": {
          "score": 0.7387700080871582,
          "rank": 175
        },
        "2602.15488v1": {
          "score": 0.7387580871582031,
          "rank": 176
        },
        "2602.14284v1": {
          "score": 0.7385903596878052,
          "rank": 177
        },
        "2602.11578v1": {
          "score": 0.7384936213493347,
          "rank": 178
        },
        "2602.12011v1": {
          "score": 0.7384600043296814,
          "rank": 179
        },
        "2602.15328v1": {
          "score": 0.7384162545204163,
          "rank": 180
        },
        "2602.14767v1": {
          "score": 0.7384111881256104,
          "rank": 181
        },
        "2602.15780v1": {
          "score": 0.7383604049682617,
          "rank": 182
        },
        "2602.10247v1": {
          "score": 0.7383246421813965,
          "rank": 183
        },
        "2602.10611v1": {
          "score": 0.7382205128669739,
          "rank": 184
        },
        "2602.15925v1": {
          "score": 0.7381495237350464,
          "rank": 185
        },
        "2602.10847v1": {
          "score": 0.7379980683326721,
          "rank": 186
        },
        "2602.12235v2": {
          "score": 0.7379755973815918,
          "rank": 187
        },
        "2602.11092v1": {
          "score": 0.737973690032959,
          "rank": 188
        },
        "2602.10708v1": {
          "score": 0.7379071116447449,
          "rank": 189
        },
        "2602.10300v1": {
          "score": 0.7379006743431091,
          "rank": 190
        },
        "2602.15110v1": {
          "score": 0.7378305792808533,
          "rank": 191
        },
        "2602.10182v1": {
          "score": 0.7377488613128662,
          "rank": 192
        },
        "2602.16568v1": {
          "score": 0.7377452254295349,
          "rank": 193
        },
        "2602.15128v1": {
          "score": 0.7375476360321045,
          "rank": 194
        },
        "2602.15472v1": {
          "score": 0.7374053001403809,
          "rank": 195
        },
        "2602.10613v1": {
          "score": 0.7373780012130737,
          "rank": 196
        },
        "2602.09720v1": {
          "score": 0.7372312545776367,
          "rank": 197
        },
        "2602.16193v1": {
          "score": 0.737224817276001,
          "rank": 198
        },
        "2602.11794v1": {
          "score": 0.7372088432312012,
          "rank": 199
        },
        "2602.15585v1": {
          "score": 0.7371255159378052,
          "rank": 200
        },
        "2602.14607v1": {
          "score": 0.7370375394821167,
          "rank": 201
        },
        "2602.16400v1": {
          "score": 0.7370030879974365,
          "rank": 202
        },
        "2602.12955v1": {
          "score": 0.7369211912155151,
          "rank": 203
        },
        "2602.15755v1": {
          "score": 0.7368605136871338,
          "rank": 204
        },
        "2602.15731v1": {
          "score": 0.7368381023406982,
          "rank": 205
        },
        "2602.13957v1": {
          "score": 0.7366228103637695,
          "rank": 206
        },
        "2602.12635v1": {
          "score": 0.7365080118179321,
          "rank": 207
        },
        "2602.15089v1": {
          "score": 0.7364006042480469,
          "rank": 208
        },
        "2602.12490v1": {
          "score": 0.7363533973693848,
          "rank": 209
        },
        "2602.15073v1": {
          "score": 0.7362664341926575,
          "rank": 210
        },
        "2602.09167v1": {
          "score": 0.7361852526664734,
          "rank": 211
        },
        "2602.14432v1": {
          "score": 0.7361817359924316,
          "rank": 212
        },
        "2602.16481v1": {
          "score": 0.736171543598175,
          "rank": 213
        },
        "2602.11573v2": {
          "score": 0.736158013343811,
          "rank": 214
        },
        "2602.11920v1": {
          "score": 0.7361258864402771,
          "rank": 215
        },
        "2602.13036v1": {
          "score": 0.73598313331604,
          "rank": 216
        },
        "2602.15820v1": {
          "score": 0.7359748482704163,
          "rank": 217
        },
        "2602.13506v1": {
          "score": 0.7359727025032043,
          "rank": 218
        },
        "2602.12703v1": {
          "score": 0.7359426021575928,
          "rank": 219
        },
        "2602.11087v1": {
          "score": 0.7358596324920654,
          "rank": 220
        },
        "2602.12781v1": {
          "score": 0.7358200550079346,
          "rank": 221
        },
        "2602.12499v1": {
          "score": 0.7357884049415588,
          "rank": 222
        },
        "2602.12697v1": {
          "score": 0.7357704043388367,
          "rank": 223
        },
        "2602.16376v1": {
          "score": 0.7357600927352905,
          "rank": 224
        },
        "2602.10745v1": {
          "score": 0.735712468624115,
          "rank": 225
        },
        "2602.15303v1": {
          "score": 0.7356613874435425,
          "rank": 226
        },
        "2602.11566v1": {
          "score": 0.7356467247009277,
          "rank": 227
        },
        "2602.12870v1": {
          "score": 0.7356020212173462,
          "rank": 228
        },
        "2602.09708v1": {
          "score": 0.7355312705039978,
          "rank": 229
        },
        "2602.09774v1": {
          "score": 0.7354485988616943,
          "rank": 230
        },
        "2602.15306v1": {
          "score": 0.7354077696800232,
          "rank": 231
        },
        "2602.11219v1": {
          "score": 0.7353769540786743,
          "rank": 232
        },
        "2602.09817v1": {
          "score": 0.7353357672691345,
          "rank": 233
        },
        "2602.16537v1": {
          "score": 0.7353351712226868,
          "rank": 234
        },
        "2602.10228v1": {
          "score": 0.7353227734565735,
          "rank": 235
        },
        "2602.12052v1": {
          "score": 0.735316276550293,
          "rank": 236
        },
        "2602.09443v1": {
          "score": 0.735291063785553,
          "rank": 237
        },
        "2602.12218v1": {
          "score": 0.7352856397628784,
          "rank": 238
        },
        "2602.14641v1": {
          "score": 0.7352768778800964,
          "rank": 239
        },
        "2602.13619v1": {
          "score": 0.7351924180984497,
          "rank": 240
        },
        "2602.09613v1": {
          "score": 0.7351846694946289,
          "rank": 241
        },
        "2602.09801v1": {
          "score": 0.7351289987564087,
          "rank": 242
        },
        "2602.11212v1": {
          "score": 0.734963059425354,
          "rank": 243
        },
        "2602.11290v2": {
          "score": 0.7349103689193726,
          "rank": 244
        },
        "2602.15166v1": {
          "score": 0.7348682284355164,
          "rank": 245
        },
        "2602.15649v1": {
          "score": 0.7348001003265381,
          "rank": 246
        },
        "2602.15155v1": {
          "score": 0.7347912192344666,
          "rank": 247
        },
        "2602.09869v1": {
          "score": 0.7347740530967712,
          "rank": 248
        },
        "2602.15946v1": {
          "score": 0.7347468137741089,
          "rank": 249
        },
        "2602.10602v1": {
          "score": 0.7347182035446167,
          "rank": 250
        },
        "2602.12693v1": {
          "score": 0.7346178293228149,
          "rank": 251
        },
        "2602.09395v1": {
          "score": 0.734605610370636,
          "rank": 252
        },
        "2602.15990v1": {
          "score": 0.734603226184845,
          "rank": 253
        },
        "2602.10489v1": {
          "score": 0.7345216274261475,
          "rank": 254
        },
        "2602.10706v3": {
          "score": 0.7344170808792114,
          "rank": 255
        },
        "2602.11843v1": {
          "score": 0.7343419790267944,
          "rank": 256
        },
        "2602.10424v1": {
          "score": 0.7342724204063416,
          "rank": 257
        },
        "2602.13322v1": {
          "score": 0.7342389822006226,
          "rank": 258
        },
        "2602.13442v1": {
          "score": 0.7342179417610168,
          "rank": 259
        },
        "2602.16259v1": {
          "score": 0.7341759204864502,
          "rank": 260
        },
        "2602.10021v1": {
          "score": 0.7341637015342712,
          "rank": 261
        },
        "2602.15586v1": {
          "score": 0.7341396808624268,
          "rank": 262
        },
        "2602.11812v1": {
          "score": 0.7341197729110718,
          "rank": 263
        },
        "2602.15593v1": {
          "score": 0.7340764999389648,
          "rank": 264
        },
        "2602.14791v1": {
          "score": 0.7340171337127686,
          "rank": 265
        },
        "2602.14729v1": {
          "score": 0.7339913845062256,
          "rank": 266
        },
        "2602.16468v1": {
          "score": 0.7338919639587402,
          "rank": 267
        },
        "2602.13128v1": {
          "score": 0.7338875532150269,
          "rank": 268
        },
        "2602.16490v1": {
          "score": 0.7338775396347046,
          "rank": 269
        },
        "2602.10207v1": {
          "score": 0.7338741421699524,
          "rank": 270
        },
        "2602.12605v1": {
          "score": 0.7337995767593384,
          "rank": 271
        },
        "2602.13660v1": {
          "score": 0.733755350112915,
          "rank": 272
        },
        "2602.09190v1": {
          "score": 0.7337523698806763,
          "rank": 273
        },
        "2602.09116v2": {
          "score": 0.7337496280670166,
          "rank": 274
        },
        "2602.11494v1": {
          "score": 0.7336862087249756,
          "rank": 275
        },
        "2602.12869v1": {
          "score": 0.7335523962974548,
          "rank": 276
        },
        "2602.14853v1": {
          "score": 0.7335360646247864,
          "rank": 277
        },
        "2602.12300v1": {
          "score": 0.7335277795791626,
          "rank": 278
        },
        "2602.16328v1": {
          "score": 0.7335276007652283,
          "rank": 279
        },
        "2602.10441v1": {
          "score": 0.7334646582603455,
          "rank": 280
        },
        "2602.11332v1": {
          "score": 0.7334452867507935,
          "rank": 281
        },
        "2602.15965v1": {
          "score": 0.7333155870437622,
          "rank": 282
        },
        "2602.11947v1": {
          "score": 0.7332618236541748,
          "rank": 283
        },
        "2602.09489v1": {
          "score": 0.733221173286438,
          "rank": 284
        },
        "2602.11630v1": {
          "score": 0.7332072854042053,
          "rank": 285
        },
        "2602.16340v1": {
          "score": 0.7331856489181519,
          "rank": 286
        },
        "2602.14997v1": {
          "score": 0.7331640720367432,
          "rank": 287
        },
        "2602.09791v1": {
          "score": 0.7331564426422119,
          "rank": 288
        },
        "2602.15618v1": {
          "score": 0.7331470251083374,
          "rank": 289
        },
        "2602.10457v1": {
          "score": 0.733146071434021,
          "rank": 290
        },
        "2602.10432v1": {
          "score": 0.7331182956695557,
          "rank": 291
        },
        "2602.10640v1": {
          "score": 0.7330870628356934,
          "rank": 292
        },
        "2602.16177v1": {
          "score": 0.7330664396286011,
          "rank": 293
        },
        "2602.11322v1": {
          "score": 0.7329261302947998,
          "rank": 294
        },
        "2602.16316v1": {
          "score": 0.7329107522964478,
          "rank": 295
        },
        "2602.10880v1": {
          "score": 0.7329016923904419,
          "rank": 296
        },
        "2602.12855v1": {
          "score": 0.7328112125396729,
          "rank": 297
        },
        "2602.14934v1": {
          "score": 0.7328107953071594,
          "rank": 298
        },
        "2602.16220v1": {
          "score": 0.7327069044113159,
          "rank": 299
        },
        "2602.11675v1": {
          "score": 0.7326716780662537,
          "rank": 300
        },
        "2602.11698v1": {
          "score": 0.7326689958572388,
          "rank": 301
        },
        "2602.11926v1": {
          "score": 0.7326254844665527,
          "rank": 302
        },
        "2602.12483v1": {
          "score": 0.732624888420105,
          "rank": 303
        },
        "2602.09008v1": {
          "score": 0.7326067090034485,
          "rank": 304
        },
        "2602.10103v1": {
          "score": 0.7325780391693115,
          "rank": 305
        },
        "2602.11054v1": {
          "score": 0.732576847076416,
          "rank": 306
        },
        "2602.09448v1": {
          "score": 0.7325582504272461,
          "rank": 307
        },
        "2602.13152v1": {
          "score": 0.7325246930122375,
          "rank": 308
        },
        "2602.13848v1": {
          "score": 0.732485830783844,
          "rank": 309
        },
        "2602.10408v1": {
          "score": 0.7323728203773499,
          "rank": 310
        },
        "2602.14757v1": {
          "score": 0.7323083877563477,
          "rank": 311
        },
        "2602.10680v1": {
          "score": 0.7322839498519897,
          "rank": 312
        },
        "2602.09003v1": {
          "score": 0.732253909111023,
          "rank": 313
        },
        "2602.16456v1": {
          "score": 0.7322226762771606,
          "rank": 314
        },
        "2602.13327v1": {
          "score": 0.7321683168411255,
          "rank": 315
        },
        "2602.11642v1": {
          "score": 0.7321504354476929,
          "rank": 316
        },
        "2602.16306v1": {
          "score": 0.732139527797699,
          "rank": 317
        },
        "2602.16673v1": {
          "score": 0.7320736646652222,
          "rank": 318
        },
        "2602.11550v1": {
          "score": 0.7319456338882446,
          "rank": 319
        },
        "2602.14630v1": {
          "score": 0.7318933010101318,
          "rank": 320
        },
        "2602.14795v1": {
          "score": 0.7318896651268005,
          "rank": 321
        },
        "2602.09792v1": {
          "score": 0.7318435907363892,
          "rank": 322
        },
        "2602.16449v1": {
          "score": 0.7317322492599487,
          "rank": 323
        },
        "2602.15984v1": {
          "score": 0.7317175269126892,
          "rank": 324
        },
        "2602.10545v1": {
          "score": 0.7315990924835205,
          "rank": 325
        },
        "2602.10532v1": {
          "score": 0.7314639091491699,
          "rank": 326
        },
        "2602.14263v1": {
          "score": 0.7314153909683228,
          "rank": 327
        },
        "2602.11863v1": {
          "score": 0.7313823699951172,
          "rank": 328
        },
        "2602.12117v1": {
          "score": 0.7313721776008606,
          "rank": 329
        },
        "2602.11673v1": {
          "score": 0.7313719987869263,
          "rank": 330
        },
        "2602.15563v1": {
          "score": 0.7313138842582703,
          "rank": 331
        },
        "2602.09509v2": {
          "score": 0.7313070893287659,
          "rank": 332
        },
        "2602.12305v1": {
          "score": 0.7313002347946167,
          "rank": 333
        },
        "2602.13174v1": {
          "score": 0.7312166690826416,
          "rank": 334
        },
        "2602.09985v1": {
          "score": 0.7311992645263672,
          "rank": 335
        },
        "2602.16233v1": {
          "score": 0.7311580181121826,
          "rank": 336
        },
        "2602.10410v1": {
          "score": 0.731124222278595,
          "rank": 337
        },
        "2602.15743v1": {
          "score": 0.7310426831245422,
          "rank": 338
        },
        "2602.16128v1": {
          "score": 0.7310031652450562,
          "rank": 339
        },
        "2602.14320v2": {
          "score": 0.7310004830360413,
          "rank": 340
        },
        "2602.10004v1": {
          "score": 0.7309434413909912,
          "rank": 341
        },
        "2602.13780v1": {
          "score": 0.7308330535888672,
          "rank": 342
        },
        "2602.09581v1": {
          "score": 0.7308328747749329,
          "rank": 343
        },
        "2602.13935v1": {
          "score": 0.7305958867073059,
          "rank": 344
        },
        "2602.14430v1": {
          "score": 0.7305803298950195,
          "rank": 345
        },
        "2602.10195v1": {
          "score": 0.730542778968811,
          "rank": 346
        },
        "2602.13359v1": {
          "score": 0.7304331064224243,
          "rank": 347
        },
        "2602.12681v1": {
          "score": 0.7304267287254333,
          "rank": 348
        },
        "2602.12243v1": {
          "score": 0.7303930521011353,
          "rank": 349
        },
        "2602.14852v1": {
          "score": 0.730313777923584,
          "rank": 350
        },
        "2602.11584v1": {
          "score": 0.7302731275558472,
          "rank": 351
        },
        "2602.09230v1": {
          "score": 0.7302649617195129,
          "rank": 352
        },
        "2602.15277v1": {
          "score": 0.7302098274230957,
          "rank": 353
        },
        "2602.11899v1": {
          "score": 0.7302001714706421,
          "rank": 354
        },
        "2602.14086v1": {
          "score": 0.7301656007766724,
          "rank": 355
        },
        "2602.12589v1": {
          "score": 0.7301430106163025,
          "rank": 356
        },
        "2602.10496v2": {
          "score": 0.730125367641449,
          "rank": 357
        },
        "2602.11139v1": {
          "score": 0.7301072478294373,
          "rank": 358
        },
        "2602.09761v1": {
          "score": 0.7300556898117065,
          "rank": 359
        },
        "2602.11539v1": {
          "score": 0.730025053024292,
          "rank": 360
        },
        "2602.13061v1": {
          "score": 0.7299981713294983,
          "rank": 361
        },
        "2602.09548v1": {
          "score": 0.7299202680587769,
          "rank": 362
        },
        "2602.13532v1": {
          "score": 0.7298402786254883,
          "rank": 363
        },
        "2602.12167v1": {
          "score": 0.7298226356506348,
          "rank": 364
        },
        "2602.10953v1": {
          "score": 0.7298182845115662,
          "rank": 365
        },
        "2602.14411v1": {
          "score": 0.7297976016998291,
          "rank": 366
        },
        "2602.13120v1": {
          "score": 0.7297730445861816,
          "rank": 367
        },
        "2602.11205v1": {
          "score": 0.7297704815864563,
          "rank": 368
        },
        "2602.09956v1": {
          "score": 0.7297062873840332,
          "rank": 369
        },
        "2602.12622v1": {
          "score": 0.7296501398086548,
          "rank": 370
        },
        "2602.12384v2": {
          "score": 0.7296279668807983,
          "rank": 371
        },
        "2602.13024v1": {
          "score": 0.729572057723999,
          "rank": 372
        },
        "2602.15000v1": {
          "score": 0.7295697927474976,
          "rank": 373
        },
        "2602.11289v1": {
          "score": 0.7295012474060059,
          "rank": 374
        },
        "2602.10282v1": {
          "score": 0.7294840812683105,
          "rank": 375
        },
        "2602.13537v2": {
          "score": 0.7294315099716187,
          "rank": 376
        },
        "2602.14938v1": {
          "score": 0.7294287085533142,
          "rank": 377
        },
        "2602.10081v2": {
          "score": 0.7294204235076904,
          "rank": 378
        },
        "2602.12360v1": {
          "score": 0.7292521595954895,
          "rank": 379
        },
        "2602.10387v1": {
          "score": 0.7292516231536865,
          "rank": 380
        },
        "2602.08976v1": {
          "score": 0.7292023301124573,
          "rank": 381
        },
        "2602.12687v1": {
          "score": 0.7291081547737122,
          "rank": 382
        },
        "2602.15296v1": {
          "score": 0.7290893793106079,
          "rank": 383
        },
        "2602.16041v1": {
          "score": 0.7290679812431335,
          "rank": 384
        },
        "2602.11631v1": {
          "score": 0.7290444374084473,
          "rank": 385
        },
        "2602.16218v1": {
          "score": 0.7290365695953369,
          "rank": 386
        },
        "2602.09337v1": {
          "score": 0.7289355397224426,
          "rank": 387
        },
        "2602.16125v1": {
          "score": 0.7288781404495239,
          "rank": 388
        },
        "2602.13961v1": {
          "score": 0.7288458943367004,
          "rank": 389
        },
        "2602.09914v1": {
          "score": 0.7287595868110657,
          "rank": 390
        },
        "2602.13319v1": {
          "score": 0.7287589907646179,
          "rank": 391
        },
        "2602.15756v1": {
          "score": 0.7287255525588989,
          "rank": 392
        },
        "2602.15380v1": {
          "score": 0.7286907434463501,
          "rank": 393
        },
        "2602.10770v1": {
          "score": 0.7286795973777771,
          "rank": 394
        },
        "2602.12365v1": {
          "score": 0.7285853624343872,
          "rank": 395
        },
        "2602.15022v1": {
          "score": 0.7285637259483337,
          "rank": 396
        },
        "2602.14041v1": {
          "score": 0.7285293340682983,
          "rank": 397
        },
        "2602.11639v1": {
          "score": 0.7285231947898865,
          "rank": 398
        },
        "2602.09748v1": {
          "score": 0.7285019755363464,
          "rank": 399
        },
        "2602.10464v1": {
          "score": 0.7284865379333496,
          "rank": 400
        }
      }
    },
    {
      "type": "keyword",
      "tag": "SR",
      "paper_tag": "keyword:SR",
      "query_text": "Symbolic regression and genetic programming for mathematical equation discovery",
      "sim_scores": {
        "2602.13021v2": {
          "score": 0.7721056938171387,
          "rank": 1
        },
        "2602.13513v2": {
          "score": 0.7698601484298706,
          "rank": 2
        },
        "2602.09530v1": {
          "score": 0.758654773235321,
          "rank": 3
        },
        "2602.10905v1": {
          "score": 0.7523519992828369,
          "rank": 4
        },
        "2602.12300v1": {
          "score": 0.7522575259208679,
          "rank": 5
        },
        "2602.16436v1": {
          "score": 0.7476757764816284,
          "rank": 6
        },
        "2602.12350v1": {
          "score": 0.7449991703033447,
          "rank": 7
        },
        "2602.12872v1": {
          "score": 0.7434916496276855,
          "rank": 8
        },
        "2602.10576v1": {
          "score": 0.7430500388145447,
          "rank": 9
        },
        "2602.14573v1": {
          "score": 0.7424887418746948,
          "rank": 10
        },
        "2602.12391v1": {
          "score": 0.7412693500518799,
          "rank": 11
        },
        "2602.13583v1": {
          "score": 0.7408901453018188,
          "rank": 12
        },
        "2602.15725v1": {
          "score": 0.7406171560287476,
          "rank": 13
        },
        "2602.11623v1": {
          "score": 0.7405974864959717,
          "rank": 14
        },
        "2602.16038v1": {
          "score": 0.739768385887146,
          "rank": 15
        },
        "2602.12259v1": {
          "score": 0.739516019821167,
          "rank": 16
        },
        "2602.10097v1": {
          "score": 0.7388081550598145,
          "rank": 17
        },
        "2602.10233v1": {
          "score": 0.737594485282898,
          "rank": 18
        },
        "2602.14506v1": {
          "score": 0.7369283437728882,
          "rank": 19
        },
        "2602.09996v2": {
          "score": 0.7367134690284729,
          "rank": 20
        },
        "2602.16427v1": {
          "score": 0.7362304925918579,
          "rank": 21
        },
        "2602.14154v1": {
          "score": 0.7358362078666687,
          "rank": 22
        },
        "2602.11698v1": {
          "score": 0.73481684923172,
          "rank": 23
        },
        "2602.12164v1": {
          "score": 0.734775185585022,
          "rank": 24
        },
        "2602.13769v1": {
          "score": 0.7347427010536194,
          "rank": 25
        },
        "2602.12471v1": {
          "score": 0.7344145774841309,
          "rank": 26
        },
        "2602.14772v1": {
          "score": 0.7341191172599792,
          "rank": 27
        },
        "2602.11630v1": {
          "score": 0.7336125373840332,
          "rank": 28
        },
        "2602.14737v1": {
          "score": 0.7332877516746521,
          "rank": 29
        },
        "2602.11920v1": {
          "score": 0.7331231236457825,
          "rank": 30
        },
        "2602.09851v1": {
          "score": 0.7330689430236816,
          "rank": 31
        },
        "2602.13506v1": {
          "score": 0.7329027056694031,
          "rank": 32
        },
        "2602.10794v1": {
          "score": 0.7326207160949707,
          "rank": 33
        },
        "2602.13934v1": {
          "score": 0.7318534851074219,
          "rank": 34
        },
        "2602.13169v1": {
          "score": 0.731823205947876,
          "rank": 35
        },
        "2602.11424v1": {
          "score": 0.7316448092460632,
          "rank": 36
        },
        "2602.11566v1": {
          "score": 0.7315053939819336,
          "rank": 37
        },
        "2602.15603v1": {
          "score": 0.7299423813819885,
          "rank": 38
        },
        "2602.08976v1": {
          "score": 0.7293429374694824,
          "rank": 39
        },
        "2602.14890v1": {
          "score": 0.7291024923324585,
          "rank": 40
        },
        "2602.13098v1": {
          "score": 0.7285404205322266,
          "rank": 41
        },
        "2602.10496v2": {
          "score": 0.7285254597663879,
          "rank": 42
        },
        "2602.12674v1": {
          "score": 0.7283812165260315,
          "rank": 43
        },
        "2602.14386v1": {
          "score": 0.7283457517623901,
          "rank": 44
        },
        "2602.11515v1": {
          "score": 0.7275093793869019,
          "rank": 45
        },
        "2602.10037v1": {
          "score": 0.727473795413971,
          "rank": 46
        },
        "2602.14717v1": {
          "score": 0.7266563773155212,
          "rank": 47
        },
        "2602.15738v1": {
          "score": 0.7263494729995728,
          "rank": 48
        },
        "2602.09959v1": {
          "score": 0.7261262536048889,
          "rank": 49
        },
        "2602.11346v1": {
          "score": 0.7260992527008057,
          "rank": 50
        },
        "2602.15632v1": {
          "score": 0.7260259985923767,
          "rank": 51
        },
        "2602.13106v1": {
          "score": 0.7253044247627258,
          "rank": 52
        },
        "2602.11770v1": {
          "score": 0.725009560585022,
          "rank": 53
        },
        "2602.12112v1": {
          "score": 0.7248455882072449,
          "rank": 54
        },
        "2602.11114v1": {
          "score": 0.7246262431144714,
          "rank": 55
        },
        "2602.11320v2": {
          "score": 0.7245786190032959,
          "rank": 56
        },
        "2602.15353v1": {
          "score": 0.7244203090667725,
          "rank": 57
        },
        "2602.11295v1": {
          "score": 0.7243281602859497,
          "rank": 58
        },
        "2602.10991v2": {
          "score": 0.7239623069763184,
          "rank": 59
        },
        "2602.16571v1": {
          "score": 0.7238399386405945,
          "rank": 60
        },
        "2602.14440v1": {
          "score": 0.7235212326049805,
          "rank": 61
        },
        "2602.10387v1": {
          "score": 0.7233354449272156,
          "rank": 62
        },
        "2602.16113v1": {
          "score": 0.7231929302215576,
          "rank": 63
        },
        "2602.09448v1": {
          "score": 0.7230627536773682,
          "rank": 64
        },
        "2602.11639v1": {
          "score": 0.7230623960494995,
          "rank": 65
        },
        "2602.13937v1": {
          "score": 0.7226793766021729,
          "rank": 66
        },
        "2602.15613v1": {
          "score": 0.7223700284957886,
          "rank": 67
        },
        "2602.11899v1": {
          "score": 0.7222111225128174,
          "rank": 68
        },
        "2602.16240v1": {
          "score": 0.7220479249954224,
          "rank": 69
        },
        "2602.16316v1": {
          "score": 0.7219948172569275,
          "rank": 70
        },
        "2602.09620v1": {
          "score": 0.72196364402771,
          "rank": 71
        },
        "2602.13174v1": {
          "score": 0.7219572067260742,
          "rank": 72
        },
        "2602.10597v1": {
          "score": 0.7219370603561401,
          "rank": 73
        },
        "2602.10751v1": {
          "score": 0.7219111919403076,
          "rank": 74
        },
        "2602.14054v1": {
          "score": 0.7218859195709229,
          "rank": 75
        },
        "2602.09572v2": {
          "score": 0.7217750549316406,
          "rank": 76
        },
        "2602.14470v1": {
          "score": 0.721631646156311,
          "rank": 77
        },
        "2602.10450v1": {
          "score": 0.7214702367782593,
          "rank": 78
        },
        "2602.10249v1": {
          "score": 0.7214499115943909,
          "rank": 79
        },
        "2602.13575v1": {
          "score": 0.7214139103889465,
          "rank": 80
        },
        "2602.09605v1": {
          "score": 0.7211756706237793,
          "rank": 81
        },
        "2602.15473v1": {
          "score": 0.7211252450942993,
          "rank": 82
        },
        "2602.15184v1": {
          "score": 0.7211190462112427,
          "rank": 83
        },
        "2602.11665v1": {
          "score": 0.7210472822189331,
          "rank": 84
        },
        "2602.09395v1": {
          "score": 0.7210097312927246,
          "rank": 85
        },
        "2602.16091v1": {
          "score": 0.720872163772583,
          "rank": 86
        },
        "2602.15229v1": {
          "score": 0.7208117246627808,
          "rank": 87
        },
        "2602.16012v1": {
          "score": 0.7207300662994385,
          "rank": 88
        },
        "2602.11491v1": {
          "score": 0.7207197546958923,
          "rank": 89
        },
        "2602.10424v1": {
          "score": 0.7206683158874512,
          "rank": 90
        },
        "2602.13510v1": {
          "score": 0.7205559611320496,
          "rank": 91
        },
        "2602.09720v1": {
          "score": 0.720417857170105,
          "rank": 92
        },
        "2602.15330v1": {
          "score": 0.7202563285827637,
          "rank": 93
        },
        "2602.14814v1": {
          "score": 0.7202446460723877,
          "rank": 94
        },
        "2602.16465v1": {
          "score": 0.7200250625610352,
          "rank": 95
        },
        "2602.13620v1": {
          "score": 0.7200057506561279,
          "rank": 96
        },
        "2602.09394v2": {
          "score": 0.7199697494506836,
          "rank": 97
        },
        "2602.11789v1": {
          "score": 0.7197881937026978,
          "rank": 98
        },
        "2602.15983v1": {
          "score": 0.7197457551956177,
          "rank": 99
        },
        "2602.11863v1": {
          "score": 0.7195234298706055,
          "rank": 100
        },
        "2602.15000v1": {
          "score": 0.7194468975067139,
          "rank": 101
        },
        "2602.11374v1": {
          "score": 0.7193822860717773,
          "rank": 102
        },
        "2602.13695v1": {
          "score": 0.7190651893615723,
          "rank": 103
        },
        "2602.13104v2": {
          "score": 0.7188663482666016,
          "rank": 104
        },
        "2602.11626v1": {
          "score": 0.7188528776168823,
          "rank": 105
        },
        "2602.10538v2": {
          "score": 0.718826413154602,
          "rank": 106
        },
        "2602.11549v1": {
          "score": 0.7187924385070801,
          "rank": 107
        },
        "2602.08980v1": {
          "score": 0.7187651991844177,
          "rank": 108
        },
        "2602.09509v2": {
          "score": 0.7186954021453857,
          "rank": 109
        },
        "2602.11087v1": {
          "score": 0.7186115980148315,
          "rank": 110
        },
        "2602.14295v1": {
          "score": 0.718238115310669,
          "rank": 111
        },
        "2602.14280v1": {
          "score": 0.7182377576828003,
          "rank": 112
        },
        "2602.13155v1": {
          "score": 0.718119740486145,
          "rank": 113
        },
        "2602.16005v1": {
          "score": 0.7180733680725098,
          "rank": 114
        },
        "2602.11570v1": {
          "score": 0.7180620431900024,
          "rank": 115
        },
        "2602.09182v1": {
          "score": 0.7179306745529175,
          "rank": 116
        },
        "2602.12143v1": {
          "score": 0.7178481817245483,
          "rank": 117
        },
        "2602.10512v1": {
          "score": 0.7178454399108887,
          "rank": 118
        },
        "2602.09548v1": {
          "score": 0.717766523361206,
          "rank": 119
        },
        "2602.12665v1": {
          "score": 0.7175395488739014,
          "rank": 120
        },
        "2602.09613v1": {
          "score": 0.7175294756889343,
          "rank": 121
        },
        "2602.15964v1": {
          "score": 0.717525839805603,
          "rank": 122
        },
        "2602.10253v1": {
          "score": 0.7175076603889465,
          "rank": 123
        },
        "2602.14320v2": {
          "score": 0.7172839641571045,
          "rank": 124
        },
        "2602.09181v1": {
          "score": 0.7171919941902161,
          "rank": 125
        },
        "2602.12078v1": {
          "score": 0.7169996500015259,
          "rank": 126
        },
        "2602.13442v1": {
          "score": 0.7169914245605469,
          "rank": 127
        },
        "2602.15390v1": {
          "score": 0.716960072517395,
          "rank": 128
        },
        "2602.10420v1": {
          "score": 0.7168734073638916,
          "rank": 129
        },
        "2602.15164v1": {
          "score": 0.7168545722961426,
          "rank": 130
        },
        "2602.12846v1": {
          "score": 0.7166159749031067,
          "rank": 131
        },
        "2602.09813v1": {
          "score": 0.7163896560668945,
          "rank": 132
        },
        "2602.13864v1": {
          "score": 0.7163829207420349,
          "rank": 133
        },
        "2602.09924v1": {
          "score": 0.7163732647895813,
          "rank": 134
        },
        "2602.10171v1": {
          "score": 0.7161093950271606,
          "rank": 135
        },
        "2602.13704v1": {
          "score": 0.7161010503768921,
          "rank": 136
        },
        "2602.09132v1": {
          "score": 0.7159863710403442,
          "rank": 137
        },
        "2602.12172v1": {
          "score": 0.7159782648086548,
          "rank": 138
        },
        "2602.16335v1": {
          "score": 0.7157553434371948,
          "rank": 139
        },
        "2602.09402v1": {
          "score": 0.715685248374939,
          "rank": 140
        },
        "2602.15365v1": {
          "score": 0.7156375646591187,
          "rank": 141
        },
        "2602.11722v1": {
          "score": 0.7155982851982117,
          "rank": 142
        },
        "2602.12867v1": {
          "score": 0.7155686616897583,
          "rank": 143
        },
        "2602.14037v1": {
          "score": 0.715549886226654,
          "rank": 144
        },
        "2602.16166v1": {
          "score": 0.7155037522315979,
          "rank": 145
        },
        "2602.10368v1": {
          "score": 0.7154287695884705,
          "rank": 146
        },
        "2602.16568v1": {
          "score": 0.7154039144515991,
          "rank": 147
        },
        "2602.13813v1": {
          "score": 0.7154003381729126,
          "rank": 148
        },
        "2602.15531v1": {
          "score": 0.715389609336853,
          "rank": 149
        },
        "2602.10778v1": {
          "score": 0.7153695821762085,
          "rank": 150
        },
        "2602.10541v1": {
          "score": 0.7152809500694275,
          "rank": 151
        },
        "2602.11747v1": {
          "score": 0.7152010202407837,
          "rank": 152
        },
        "2602.11451v1": {
          "score": 0.715199887752533,
          "rank": 153
        },
        "2602.09303v1": {
          "score": 0.7151855230331421,
          "rank": 154
        },
        "2602.10014v1": {
          "score": 0.7150275111198425,
          "rank": 155
        },
        "2602.16435v1": {
          "score": 0.7150115966796875,
          "rank": 156
        },
        "2602.11289v1": {
          "score": 0.714907169342041,
          "rank": 157
        },
        "2602.14607v1": {
          "score": 0.714855432510376,
          "rank": 158
        },
        "2602.15169v1": {
          "score": 0.7148081660270691,
          "rank": 159
        },
        "2602.11594v2": {
          "score": 0.7145051956176758,
          "rank": 160
        },
        "2602.12267v1": {
          "score": 0.7143615484237671,
          "rank": 161
        },
        "2602.10436v1": {
          "score": 0.7143590450286865,
          "rank": 162
        },
        "2602.10470v1": {
          "score": 0.714200496673584,
          "rank": 163
        },
        "2602.11841v1": {
          "score": 0.7141697406768799,
          "rank": 164
        },
        "2602.10867v1": {
          "score": 0.7141191959381104,
          "rank": 165
        },
        "2602.10352v1": {
          "score": 0.7141015529632568,
          "rank": 166
        },
        "2602.16500v1": {
          "score": 0.7140981554985046,
          "rank": 167
        },
        "2602.11229v1": {
          "score": 0.714073896408081,
          "rank": 168
        },
        "2602.09801v1": {
          "score": 0.7140319347381592,
          "rank": 169
        },
        "2602.15068v1": {
          "score": 0.7137658596038818,
          "rank": 170
        },
        "2602.09901v1": {
          "score": 0.713760256767273,
          "rank": 171
        },
        "2602.14423v1": {
          "score": 0.713732898235321,
          "rank": 172
        },
        "2602.11931v1": {
          "score": 0.7136580944061279,
          "rank": 173
        },
        "2602.14058v1": {
          "score": 0.7136353850364685,
          "rank": 174
        },
        "2602.11671v1": {
          "score": 0.7135968208312988,
          "rank": 175
        },
        "2602.09258v1": {
          "score": 0.713547945022583,
          "rank": 176
        },
        "2602.15128v1": {
          "score": 0.7134761810302734,
          "rank": 177
        },
        "2602.14683v1": {
          "score": 0.7133183479309082,
          "rank": 178
        },
        "2602.09116v2": {
          "score": 0.7131494283676147,
          "rank": 179
        },
        "2602.14949v1": {
          "score": 0.7131083607673645,
          "rank": 180
        },
        "2602.15327v1": {
          "score": 0.7130350470542908,
          "rank": 181
        },
        "2602.10885v1": {
          "score": 0.7130118608474731,
          "rank": 182
        },
        "2602.10006v1": {
          "score": 0.7130047082901001,
          "rank": 183
        },
        "2602.10485v1": {
          "score": 0.7129216194152832,
          "rank": 184
        },
        "2602.12123v1": {
          "score": 0.712907075881958,
          "rank": 185
        },
        "2602.14708v1": {
          "score": 0.712904691696167,
          "rank": 186
        },
        "2602.15596v1": {
          "score": 0.7128549218177795,
          "rank": 187
        },
        "2602.09317v1": {
          "score": 0.7128419876098633,
          "rank": 188
        },
        "2602.10891v1": {
          "score": 0.7127791047096252,
          "rank": 189
        },
        "2602.13419v1": {
          "score": 0.7126846313476562,
          "rank": 190
        },
        "2602.16490v1": {
          "score": 0.7126546502113342,
          "rank": 191
        },
        "2602.12243v1": {
          "score": 0.712637722492218,
          "rank": 192
        },
        "2602.14011v1": {
          "score": 0.7124802470207214,
          "rank": 193
        },
        "2602.13485v1": {
          "score": 0.712475061416626,
          "rank": 194
        },
        "2602.13367v1": {
          "score": 0.7124359607696533,
          "rank": 195
        },
        "2602.10583v1": {
          "score": 0.7123852372169495,
          "rank": 196
        },
        "2602.14451v1": {
          "score": 0.7122995853424072,
          "rank": 197
        },
        "2602.10786v1": {
          "score": 0.7122349739074707,
          "rank": 198
        },
        "2602.15972v1": {
          "score": 0.7121334075927734,
          "rank": 199
        },
        "2602.15921v1": {
          "score": 0.7120342254638672,
          "rank": 200
        },
        "2602.14881v1": {
          "score": 0.7118973135948181,
          "rank": 201
        },
        "2602.13069v1": {
          "score": 0.7118146419525146,
          "rank": 202
        },
        "2602.11859v1": {
          "score": 0.7117034196853638,
          "rank": 203
        },
        "2602.10231v1": {
          "score": 0.7115095853805542,
          "rank": 204
        },
        "2602.10587v1": {
          "score": 0.7115012407302856,
          "rank": 205
        },
        "2602.10878v1": {
          "score": 0.7114843130111694,
          "rank": 206
        },
        "2602.09574v1": {
          "score": 0.7110424041748047,
          "rank": 207
        },
        "2602.10461v1": {
          "score": 0.7109698057174683,
          "rank": 208
        },
        "2602.10996v1": {
          "score": 0.7109220027923584,
          "rank": 209
        },
        "2602.10522v1": {
          "score": 0.7108893394470215,
          "rank": 210
        },
        "2602.10585v1": {
          "score": 0.7107882499694824,
          "rank": 211
        },
        "2602.11090v1": {
          "score": 0.7107546329498291,
          "rank": 212
        },
        "2602.13449v1": {
          "score": 0.7107493281364441,
          "rank": 213
        },
        "2602.12753v1": {
          "score": 0.7107383012771606,
          "rank": 214
        },
        "2602.11794v1": {
          "score": 0.710700511932373,
          "rank": 215
        },
        "2602.11881v1": {
          "score": 0.7106752395629883,
          "rank": 216
        },
        "2602.09000v1": {
          "score": 0.7106355428695679,
          "rank": 217
        },
        "2602.16530v1": {
          "score": 0.710585355758667,
          "rank": 218
        },
        "2602.11947v1": {
          "score": 0.7105786204338074,
          "rank": 219
        },
        "2602.11690v1": {
          "score": 0.7105640172958374,
          "rank": 220
        },
        "2602.15472v1": {
          "score": 0.7105571031570435,
          "rank": 221
        },
        "2602.15820v1": {
          "score": 0.7104761600494385,
          "rank": 222
        },
        "2602.09783v1": {
          "score": 0.7104576826095581,
          "rank": 223
        },
        "2602.12889v1": {
          "score": 0.7104562520980835,
          "rank": 224
        },
        "2602.13035v1": {
          "score": 0.7104357481002808,
          "rank": 225
        },
        "2602.09761v1": {
          "score": 0.7103276252746582,
          "rank": 226
        },
        "2602.11917v1": {
          "score": 0.7101761102676392,
          "rank": 227
        },
        "2602.13046v1": {
          "score": 0.7101165652275085,
          "rank": 228
        },
        "2602.14077v1": {
          "score": 0.7100675106048584,
          "rank": 229
        },
        "2602.16177v1": {
          "score": 0.7100403308868408,
          "rank": 230
        },
        "2602.09001v1": {
          "score": 0.7099778652191162,
          "rank": 231
        },
        "2602.15306v1": {
          "score": 0.7099671959877014,
          "rank": 232
        },
        "2602.10632v1": {
          "score": 0.7098926901817322,
          "rank": 233
        },
        "2602.14289v1": {
          "score": 0.7097399234771729,
          "rank": 234
        },
        "2602.11812v1": {
          "score": 0.7097080945968628,
          "rank": 235
        },
        "2602.13103v2": {
          "score": 0.7096544504165649,
          "rank": 236
        },
        "2602.10870v1": {
          "score": 0.7096060514450073,
          "rank": 237
        },
        "2602.10332v1": {
          "score": 0.7095503807067871,
          "rank": 238
        },
        "2602.14868v1": {
          "score": 0.7095308899879456,
          "rank": 239
        },
        "2602.13536v1": {
          "score": 0.7095006704330444,
          "rank": 240
        },
        "2602.10595v1": {
          "score": 0.7093930244445801,
          "rank": 241
        },
        "2602.09774v1": {
          "score": 0.7093734741210938,
          "rank": 242
        },
        "2602.10410v1": {
          "score": 0.7093368768692017,
          "rank": 243
        },
        "2602.14656v1": {
          "score": 0.7092181444168091,
          "rank": 244
        },
        "2602.13957v1": {
          "score": 0.7092093229293823,
          "rank": 245
        },
        "2602.10480v2": {
          "score": 0.7092092633247375,
          "rank": 246
        },
        "2602.14919v1": {
          "score": 0.7091615200042725,
          "rank": 247
        },
        "2602.11557v1": {
          "score": 0.7091301083564758,
          "rank": 248
        },
        "2602.10282v1": {
          "score": 0.7091216444969177,
          "rank": 249
        },
        "2602.15488v1": {
          "score": 0.7088969945907593,
          "rank": 250
        },
        "2602.13567v1": {
          "score": 0.7088031768798828,
          "rank": 251
        },
        "2602.11683v1": {
          "score": 0.708755612373352,
          "rank": 252
        },
        "2602.09757v1": {
          "score": 0.7086842060089111,
          "rank": 253
        },
        "2602.09988v1": {
          "score": 0.7086545825004578,
          "rank": 254
        },
        "2602.08984v1": {
          "score": 0.7086529731750488,
          "rank": 255
        },
        "2602.10489v1": {
          "score": 0.7086070775985718,
          "rank": 256
        },
        "2602.14469v1": {
          "score": 0.708541214466095,
          "rank": 257
        },
        "2602.14480v1": {
          "score": 0.7083331346511841,
          "rank": 258
        },
        "2602.14342v1": {
          "score": 0.7083219289779663,
          "rank": 259
        },
        "2602.09276v1": {
          "score": 0.7082798480987549,
          "rank": 260
        },
        "2602.09487v1": {
          "score": 0.7082160115242004,
          "rank": 261
        },
        "2602.12703v1": {
          "score": 0.708172082901001,
          "rank": 262
        },
        "2602.14275v1": {
          "score": 0.708154022693634,
          "rank": 263
        },
        "2602.14697v1": {
          "score": 0.708059549331665,
          "rank": 264
        },
        "2602.12125v1": {
          "score": 0.7080132961273193,
          "rank": 265
        },
        "2602.10506v1": {
          "score": 0.7080126404762268,
          "rank": 266
        },
        "2602.10680v1": {
          "score": 0.707915723323822,
          "rank": 267
        },
        "2602.10520v2": {
          "score": 0.7078642249107361,
          "rank": 268
        },
        "2602.12821v1": {
          "score": 0.7078362703323364,
          "rank": 269
        },
        "2602.16481v1": {
          "score": 0.7078132033348083,
          "rank": 270
        },
        "2602.16684v1": {
          "score": 0.7077820301055908,
          "rank": 271
        },
        "2602.12706v1": {
          "score": 0.7075691223144531,
          "rank": 272
        },
        "2602.14761v1": {
          "score": 0.7074658870697021,
          "rank": 273
        },
        "2602.11498v1": {
          "score": 0.7073869109153748,
          "rank": 274
        },
        "2602.11675v1": {
          "score": 0.7073807716369629,
          "rank": 275
        },
        "2602.15564v1": {
          "score": 0.7073668241500854,
          "rank": 276
        },
        "2602.14670v1": {
          "score": 0.7073007822036743,
          "rank": 277
        },
        "2602.12049v1": {
          "score": 0.7071008682250977,
          "rank": 278
        },
        "2602.15423v1": {
          "score": 0.7070260047912598,
          "rank": 279
        },
        "2602.08990v1": {
          "score": 0.7069940567016602,
          "rank": 280
        },
        "2602.12642v1": {
          "score": 0.7069836854934692,
          "rank": 281
        },
        "2602.10605v1": {
          "score": 0.7069705724716187,
          "rank": 282
        },
        "2602.16531v1": {
          "score": 0.7069618701934814,
          "rank": 283
        },
        "2602.13891v1": {
          "score": 0.7069472074508667,
          "rank": 284
        },
        "2602.14536v1": {
          "score": 0.7068935632705688,
          "rank": 285
        },
        "2602.11333v1": {
          "score": 0.7068706750869751,
          "rank": 286
        },
        "2602.14580v1": {
          "score": 0.7067628502845764,
          "rank": 287
        },
        "2602.13120v1": {
          "score": 0.7067157030105591,
          "rank": 288
        },
        "2602.11476v1": {
          "score": 0.7066924571990967,
          "rank": 289
        },
        "2602.16473v1": {
          "score": 0.7065976858139038,
          "rank": 290
        },
        "2602.12390v1": {
          "score": 0.7064726948738098,
          "rank": 291
        },
        "2602.09331v1": {
          "score": 0.7064546942710876,
          "rank": 292
        },
        "2602.13935v1": {
          "score": 0.7064064741134644,
          "rank": 293
        },
        "2602.12592v1": {
          "score": 0.7063694596290588,
          "rank": 294
        },
        "2602.09196v1": {
          "score": 0.7061743140220642,
          "rank": 295
        },
        "2602.13807v1": {
          "score": 0.7061644792556763,
          "rank": 296
        },
        "2602.15571v1": {
          "score": 0.7061370611190796,
          "rank": 297
        },
        "2602.11584v1": {
          "score": 0.7061092257499695,
          "rank": 298
        },
        "2602.11149v1": {
          "score": 0.7059726715087891,
          "rank": 299
        },
        "2602.10845v1": {
          "score": 0.705888569355011,
          "rank": 300
        },
        "2602.12968v1": {
          "score": 0.705869197845459,
          "rank": 301
        },
        "2602.11904v1": {
          "score": 0.7057770490646362,
          "rank": 302
        },
        "2602.10374v1": {
          "score": 0.7057486772537231,
          "rank": 303
        },
        "2602.12342v1": {
          "score": 0.70564204454422,
          "rank": 304
        },
        "2602.11937v1": {
          "score": 0.705626368522644,
          "rank": 305
        },
        "2602.10085v2": {
          "score": 0.7055198550224304,
          "rank": 306
        },
        "2602.14517v1": {
          "score": 0.7054635286331177,
          "rank": 307
        },
        "2602.10404v1": {
          "score": 0.7054247856140137,
          "rank": 308
        },
        "2602.12966v1": {
          "score": 0.7054038047790527,
          "rank": 309
        },
        "2602.12273v1": {
          "score": 0.7053655385971069,
          "rank": 310
        },
        "2602.15669v1": {
          "score": 0.7053557634353638,
          "rank": 311
        },
        "2602.15822v1": {
          "score": 0.705321192741394,
          "rank": 312
        },
        "2602.15503v1": {
          "score": 0.7051577568054199,
          "rank": 313
        },
        "2602.11717v1": {
          "score": 0.7051514983177185,
          "rank": 314
        },
        "2602.15206v1": {
          "score": 0.7051308751106262,
          "rank": 315
        },
        "2602.12569v1": {
          "score": 0.7050284147262573,
          "rank": 316
        },
        "2602.09374v1": {
          "score": 0.7050011157989502,
          "rank": 317
        },
        "2602.10388v2": {
          "score": 0.7049888968467712,
          "rank": 318
        },
        "2602.14251v1": {
          "score": 0.7049878835678101,
          "rank": 319
        },
        "2602.09285v1": {
          "score": 0.7049725651741028,
          "rank": 320
        },
        "2602.15602v1": {
          "score": 0.7049689888954163,
          "rank": 321
        },
        "2602.12851v1": {
          "score": 0.7049531936645508,
          "rank": 322
        },
        "2602.15511v1": {
          "score": 0.7049407958984375,
          "rank": 323
        },
        "2602.16606v1": {
          "score": 0.7049312591552734,
          "rank": 324
        },
        "2602.12116v1": {
          "score": 0.7049107551574707,
          "rank": 325
        },
        "2602.16503v1": {
          "score": 0.7048451900482178,
          "rank": 326
        },
        "2602.14089v1": {
          "score": 0.7048244476318359,
          "rank": 327
        },
        "2602.09791v1": {
          "score": 0.7047845125198364,
          "rank": 328
        },
        "2602.14130v1": {
          "score": 0.7047804594039917,
          "rank": 329
        },
        "2602.13413v1": {
          "score": 0.7047743797302246,
          "rank": 330
        },
        "2602.14060v1": {
          "score": 0.7047370076179504,
          "rank": 331
        },
        "2602.12234v1": {
          "score": 0.7046966552734375,
          "rank": 332
        },
        "2602.14701v1": {
          "score": 0.7046698331832886,
          "rank": 333
        },
        "2602.11388v1": {
          "score": 0.7046517133712769,
          "rank": 334
        },
        "2602.10228v1": {
          "score": 0.7045937776565552,
          "rank": 335
        },
        "2602.09598v1": {
          "score": 0.7045045495033264,
          "rank": 336
        },
        "2602.14853v1": {
          "score": 0.7044817805290222,
          "rank": 337
        },
        "2602.15681v1": {
          "score": 0.7044682502746582,
          "rank": 338
        },
        "2602.15481v1": {
          "score": 0.704439640045166,
          "rank": 339
        },
        "2602.09288v1": {
          "score": 0.704404354095459,
          "rank": 340
        },
        "2602.15593v1": {
          "score": 0.7043335437774658,
          "rank": 341
        },
        "2602.11209v1": {
          "score": 0.7042315602302551,
          "rank": 342
        },
        "2602.12114v1": {
          "score": 0.7041080594062805,
          "rank": 343
        },
        "2602.13128v1": {
          "score": 0.7040582895278931,
          "rank": 344
        },
        "2602.12349v1": {
          "score": 0.7039879560470581,
          "rank": 345
        },
        "2602.14969v1": {
          "score": 0.7039490938186646,
          "rank": 346
        },
        "2602.15094v1": {
          "score": 0.7039303779602051,
          "rank": 347
        },
        "2602.12693v1": {
          "score": 0.7039148807525635,
          "rank": 348
        },
        "2602.15260v1": {
          "score": 0.7038746476173401,
          "rank": 349
        },
        "2602.16707v1": {
          "score": 0.7038668990135193,
          "rank": 350
        },
        "2602.09128v1": {
          "score": 0.7038590908050537,
          "rank": 351
        },
        "2602.13962v1": {
          "score": 0.703848123550415,
          "rank": 352
        },
        "2602.16209v1": {
          "score": 0.7037846446037292,
          "rank": 353
        },
        "2602.14243v1": {
          "score": 0.7037289142608643,
          "rank": 354
        },
        "2602.12235v2": {
          "score": 0.7036831378936768,
          "rank": 355
        },
        "2602.09242v1": {
          "score": 0.7036488652229309,
          "rank": 356
        },
        "2602.09314v1": {
          "score": 0.7036236524581909,
          "rank": 357
        },
        "2602.14786v1": {
          "score": 0.7036140561103821,
          "rank": 358
        },
        "2602.11945v1": {
          "score": 0.7035945653915405,
          "rank": 359
        },
        "2602.14896v1": {
          "score": 0.7035750150680542,
          "rank": 360
        },
        "2602.14185v1": {
          "score": 0.7035276293754578,
          "rank": 361
        },
        "2602.14231v1": {
          "score": 0.7034721374511719,
          "rank": 362
        },
        "2602.12275v1": {
          "score": 0.7034705281257629,
          "rank": 363
        },
        "2602.16167v1": {
          "score": 0.7034481167793274,
          "rank": 364
        },
        "2602.15074v1": {
          "score": 0.7033852934837341,
          "rank": 365
        },
        "2602.09842v1": {
          "score": 0.7033510208129883,
          "rank": 366
        },
        "2602.16311v1": {
          "score": 0.7033277750015259,
          "rank": 367
        },
        "2602.12499v1": {
          "score": 0.7033093571662903,
          "rank": 368
        },
        "2602.10699v2": {
          "score": 0.7032457590103149,
          "rank": 369
        },
        "2602.14729v1": {
          "score": 0.7032428979873657,
          "rank": 370
        },
        "2602.13550v1": {
          "score": 0.7031482458114624,
          "rank": 371
        },
        "2602.11097v1": {
          "score": 0.7030922770500183,
          "rank": 372
        },
        "2602.14244v1": {
          "score": 0.7030125856399536,
          "rank": 373
        },
        "2602.10004v1": {
          "score": 0.7028891444206238,
          "rank": 374
        },
        "2602.12187v1": {
          "score": 0.7028586864471436,
          "rank": 375
        },
        "2602.13450v1": {
          "score": 0.70284104347229,
          "rank": 376
        },
        "2602.13486v1": {
          "score": 0.7026733160018921,
          "rank": 377
        },
        "2602.10226v1": {
          "score": 0.7026405334472656,
          "rank": 378
        },
        "2602.11089v1": {
          "score": 0.7025606036186218,
          "rank": 379
        },
        "2602.11057v1": {
          "score": 0.702526867389679,
          "rank": 380
        },
        "2602.12305v1": {
          "score": 0.7023453712463379,
          "rank": 381
        },
        "2602.13571v1": {
          "score": 0.7023398876190186,
          "rank": 382
        },
        "2602.12660v1": {
          "score": 0.7023096680641174,
          "rank": 383
        },
        "2602.09112v1": {
          "score": 0.7022402882575989,
          "rank": 384
        },
        "2602.13359v1": {
          "score": 0.7022310495376587,
          "rank": 385
        },
        "2602.16193v1": {
          "score": 0.7022219896316528,
          "rank": 386
        },
        "2602.11044v1": {
          "score": 0.7022152543067932,
          "rank": 387
        },
        "2602.15283v1": {
          "score": 0.7021813988685608,
          "rank": 388
        },
        "2602.09464v1": {
          "score": 0.7021468877792358,
          "rank": 389
        },
        "2602.16608v1": {
          "score": 0.701999306678772,
          "rank": 390
        },
        "2602.11573v2": {
          "score": 0.7019335031509399,
          "rank": 391
        },
        "2602.15156v1": {
          "score": 0.7019138336181641,
          "rank": 392
        },
        "2602.09302v1": {
          "score": 0.7018691897392273,
          "rank": 393
        },
        "2602.10210v1": {
          "score": 0.7018673419952393,
          "rank": 394
        },
        "2602.09351v1": {
          "score": 0.7018579244613647,
          "rank": 395
        },
        "2602.13562v1": {
          "score": 0.701758623123169,
          "rank": 396
        },
        "2602.13005v1": {
          "score": 0.7017321586608887,
          "rank": 397
        },
        "2602.13362v1": {
          "score": 0.7016971707344055,
          "rank": 398
        },
        "2602.14476v1": {
          "score": 0.7016761302947998,
          "rank": 399
        },
        "2602.16512v1": {
          "score": 0.7016175985336304,
          "rank": 400
        }
      }
    },
    {
      "type": "keyword",
      "tag": "SR",
      "paper_tag": "keyword:SR",
      "query_text": "Deep learning and neural network architectures for symbolic regression tasks",
      "sim_scores": {
        "2602.16177v1": {
          "score": 0.7592624425888062,
          "rank": 1
        },
        "2602.15593v1": {
          "score": 0.7577707767486572,
          "rank": 2
        },
        "2602.14701v1": {
          "score": 0.7525403499603271,
          "rank": 3
        },
        "2602.10598v1": {
          "score": 0.7512378692626953,
          "rank": 4
        },
        "2602.16608v1": {
          "score": 0.7493811249732971,
          "rank": 5
        },
        "2602.10097v1": {
          "score": 0.7470190525054932,
          "rank": 6
        },
        "2602.09613v1": {
          "score": 0.7433837056159973,
          "rank": 7
        },
        "2602.14760v1": {
          "score": 0.7427353858947754,
          "rank": 8
        },
        "2602.16436v1": {
          "score": 0.7420639395713806,
          "rank": 9
        },
        "2602.09304v1": {
          "score": 0.7377341985702515,
          "rank": 10
        },
        "2602.15283v1": {
          "score": 0.7375423908233643,
          "rank": 11
        },
        "2602.12267v1": {
          "score": 0.7373523712158203,
          "rank": 12
        },
        "2602.11424v1": {
          "score": 0.7369077205657959,
          "rank": 13
        },
        "2602.09258v1": {
          "score": 0.7360162138938904,
          "rank": 14
        },
        "2602.12744v1": {
          "score": 0.7358601689338684,
          "rank": 15
        },
        "2602.12851v1": {
          "score": 0.7345099449157715,
          "rank": 16
        },
        "2602.14301v1": {
          "score": 0.7339592576026917,
          "rank": 17
        },
        "2602.11698v1": {
          "score": 0.7338962554931641,
          "rank": 18
        },
        "2602.14280v1": {
          "score": 0.733802080154419,
          "rank": 19
        },
        "2602.09761v1": {
          "score": 0.7336409091949463,
          "rank": 20
        },
        "2602.09395v1": {
          "score": 0.7330244779586792,
          "rank": 21
        },
        "2602.13348v1": {
          "score": 0.7329623699188232,
          "rank": 22
        },
        "2602.13583v1": {
          "score": 0.7328592538833618,
          "rank": 23
        },
        "2602.14919v1": {
          "score": 0.7322739362716675,
          "rank": 24
        },
        "2602.10457v1": {
          "score": 0.7316896915435791,
          "rank": 25
        },
        "2602.14635v1": {
          "score": 0.731414794921875,
          "rank": 26
        },
        "2602.09314v1": {
          "score": 0.7306899428367615,
          "rank": 27
        },
        "2602.10496v2": {
          "score": 0.7304801940917969,
          "rank": 28
        },
        "2602.11841v1": {
          "score": 0.7299469709396362,
          "rank": 29
        },
        "2602.14759v1": {
          "score": 0.7293907403945923,
          "rank": 30
        },
        "2602.15571v1": {
          "score": 0.729289710521698,
          "rank": 31
        },
        "2602.14159v1": {
          "score": 0.729154109954834,
          "rank": 32
        },
        "2602.12384v2": {
          "score": 0.7288620471954346,
          "rank": 33
        },
        "2602.12649v1": {
          "score": 0.7287247180938721,
          "rank": 34
        },
        "2602.13442v1": {
          "score": 0.7286691665649414,
          "rank": 35
        },
        "2602.13021v2": {
          "score": 0.728518545627594,
          "rank": 36
        },
        "2602.11374v1": {
          "score": 0.7279924750328064,
          "rank": 37
        },
        "2602.11204v1": {
          "score": 0.7271890640258789,
          "rank": 38
        },
        "2602.16704v1": {
          "score": 0.7269996404647827,
          "rank": 39
        },
        "2602.09783v1": {
          "score": 0.7263280153274536,
          "rank": 40
        },
        "2602.14440v1": {
          "score": 0.7260680198669434,
          "rank": 41
        },
        "2602.10680v1": {
          "score": 0.7259228825569153,
          "rank": 42
        },
        "2602.11633v1": {
          "score": 0.725344717502594,
          "rank": 43
        },
        "2602.10587v1": {
          "score": 0.7253415584564209,
          "rank": 44
        },
        "2602.12569v1": {
          "score": 0.7252411842346191,
          "rank": 45
        },
        "2602.10602v1": {
          "score": 0.7251116633415222,
          "rank": 46
        },
        "2602.16530v1": {
          "score": 0.7249239087104797,
          "rank": 47
        },
        "2602.09869v1": {
          "score": 0.7248544692993164,
          "rank": 48
        },
        "2602.12952v1": {
          "score": 0.7248184680938721,
          "rank": 49
        },
        "2602.12613v1": {
          "score": 0.7247356176376343,
          "rank": 50
        },
        "2602.09509v2": {
          "score": 0.7242482304573059,
          "rank": 51
        },
        "2602.12556v1": {
          "score": 0.7241722345352173,
          "rank": 52
        },
        "2602.12390v1": {
          "score": 0.7241308093070984,
          "rank": 53
        },
        "2602.15552v1": {
          "score": 0.7240220308303833,
          "rank": 54
        },
        "2602.15814v1": {
          "score": 0.7236158847808838,
          "rank": 55
        },
        "2602.11861v1": {
          "score": 0.7231905460357666,
          "rank": 56
        },
        "2602.15602v1": {
          "score": 0.7231755256652832,
          "rank": 57
        },
        "2602.13087v1": {
          "score": 0.722983717918396,
          "rank": 58
        },
        "2602.10204v1": {
          "score": 0.7228033542633057,
          "rank": 59
        },
        "2602.09530v1": {
          "score": 0.7227176427841187,
          "rank": 60
        },
        "2602.14578v1": {
          "score": 0.7225760221481323,
          "rank": 61
        },
        "2602.11322v1": {
          "score": 0.7224846482276917,
          "rank": 62
        },
        "2602.09489v1": {
          "score": 0.7224608063697815,
          "rank": 63
        },
        "2602.10905v1": {
          "score": 0.7223137617111206,
          "rank": 64
        },
        "2602.15632v1": {
          "score": 0.7216225266456604,
          "rank": 65
        },
        "2602.09717v1": {
          "score": 0.721445620059967,
          "rank": 66
        },
        "2602.09569v1": {
          "score": 0.7214148044586182,
          "rank": 67
        },
        "2602.13128v1": {
          "score": 0.7213481664657593,
          "rank": 68
        },
        "2602.16316v1": {
          "score": 0.721312940120697,
          "rank": 69
        },
        "2602.10408v1": {
          "score": 0.7212808728218079,
          "rank": 70
        },
        "2602.12587v1": {
          "score": 0.7212250232696533,
          "rank": 71
        },
        "2602.13098v1": {
          "score": 0.7209841012954712,
          "rank": 72
        },
        "2602.11320v2": {
          "score": 0.7205695509910583,
          "rank": 73
        },
        "2602.10352v1": {
          "score": 0.7202721834182739,
          "rank": 74
        },
        "2602.15510v1": {
          "score": 0.7201528549194336,
          "rank": 75
        },
        "2602.15353v1": {
          "score": 0.7200868129730225,
          "rank": 76
        },
        "2602.13069v1": {
          "score": 0.7200369834899902,
          "rank": 77
        },
        "2602.11958v1": {
          "score": 0.7199603319168091,
          "rank": 78
        },
        "2602.12962v1": {
          "score": 0.7198864221572876,
          "rank": 79
        },
        "2602.14492v2": {
          "score": 0.7198718786239624,
          "rank": 80
        },
        "2602.10867v1": {
          "score": 0.7198261618614197,
          "rank": 81
        },
        "2602.09190v1": {
          "score": 0.7197302579879761,
          "rank": 82
        },
        "2602.09169v1": {
          "score": 0.71959388256073,
          "rank": 83
        },
        "2602.15521v1": {
          "score": 0.7193871140480042,
          "rank": 84
        },
        "2602.10410v1": {
          "score": 0.7189394235610962,
          "rank": 85
        },
        "2602.12499v1": {
          "score": 0.7188529968261719,
          "rank": 86
        },
        "2602.10489v1": {
          "score": 0.7188028693199158,
          "rank": 87
        },
        "2602.09851v1": {
          "score": 0.7186340093612671,
          "rank": 88
        },
        "2602.10480v2": {
          "score": 0.7186146974563599,
          "rank": 89
        },
        "2602.14896v1": {
          "score": 0.7186137437820435,
          "rank": 90
        },
        "2602.15239v1": {
          "score": 0.7184723019599915,
          "rank": 91
        },
        "2602.09764v1": {
          "score": 0.7184457778930664,
          "rank": 92
        },
        "2602.11940v1": {
          "score": 0.7182161211967468,
          "rank": 93
        },
        "2602.14231v1": {
          "score": 0.7178435325622559,
          "rank": 94
        },
        "2602.13864v1": {
          "score": 0.7170512080192566,
          "rank": 95
        },
        "2602.11491v1": {
          "score": 0.7170459628105164,
          "rank": 96
        },
        "2602.11690v1": {
          "score": 0.7170203924179077,
          "rank": 97
        },
        "2602.09194v1": {
          "score": 0.7168625593185425,
          "rank": 98
        },
        "2602.09501v1": {
          "score": 0.716688871383667,
          "rank": 99
        },
        "2602.11212v1": {
          "score": 0.7166264057159424,
          "rank": 100
        },
        "2602.08976v1": {
          "score": 0.7166101932525635,
          "rank": 101
        },
        "2602.11557v1": {
          "score": 0.7165563702583313,
          "rank": 102
        },
        "2602.16531v1": {
          "score": 0.7165128588676453,
          "rank": 103
        },
        "2602.13910v1": {
          "score": 0.7164177894592285,
          "rank": 104
        },
        "2602.14737v1": {
          "score": 0.716242253780365,
          "rank": 105
        },
        "2602.15206v1": {
          "score": 0.7159508466720581,
          "rank": 106
        },
        "2602.16435v1": {
          "score": 0.7157889008522034,
          "rank": 107
        },
        "2602.12753v1": {
          "score": 0.715700626373291,
          "rank": 108
        },
        "2602.09317v1": {
          "score": 0.7154070734977722,
          "rank": 109
        },
        "2602.12542v1": {
          "score": 0.7153171896934509,
          "rank": 110
        },
        "2602.12642v1": {
          "score": 0.715189516544342,
          "rank": 111
        },
        "2602.15155v1": {
          "score": 0.7150772213935852,
          "rank": 112
        },
        "2602.12009v1": {
          "score": 0.7150334715843201,
          "rank": 113
        },
        "2602.10357v1": {
          "score": 0.7149950265884399,
          "rank": 114
        },
        "2602.14432v1": {
          "score": 0.7148261070251465,
          "rank": 115
        },
        "2602.10585v1": {
          "score": 0.7147959470748901,
          "rank": 116
        },
        "2602.11626v1": {
          "score": 0.7147641181945801,
          "rank": 117
        },
        "2602.13106v1": {
          "score": 0.7146646976470947,
          "rank": 118
        },
        "2602.15897v1": {
          "score": 0.7143006920814514,
          "rank": 119
        },
        "2602.11584v1": {
          "score": 0.7141833305358887,
          "rank": 120
        },
        "2602.13513v2": {
          "score": 0.7140761613845825,
          "rank": 121
        },
        "2602.11881v1": {
          "score": 0.7140452861785889,
          "rank": 122
        },
        "2602.12471v1": {
          "score": 0.7139257192611694,
          "rank": 123
        },
        "2602.12236v1": {
          "score": 0.7136906385421753,
          "rank": 124
        },
        "2602.13024v1": {
          "score": 0.7135944366455078,
          "rank": 125
        },
        "2602.13413v1": {
          "score": 0.7135254144668579,
          "rank": 126
        },
        "2602.10607v1": {
          "score": 0.71281898021698,
          "rank": 127
        },
        "2602.14890v1": {
          "score": 0.7128058075904846,
          "rank": 128
        },
        "2602.11700v1": {
          "score": 0.7127748131752014,
          "rank": 129
        },
        "2602.12846v1": {
          "score": 0.7127591371536255,
          "rank": 130
        },
        "2602.10754v1": {
          "score": 0.71271151304245,
          "rank": 131
        },
        "2602.11146v1": {
          "score": 0.7125734090805054,
          "rank": 132
        },
        "2602.11808v1": {
          "score": 0.712537944316864,
          "rank": 133
        },
        "2602.12235v2": {
          "score": 0.712376594543457,
          "rank": 134
        },
        "2602.12546v1": {
          "score": 0.7122945189476013,
          "rank": 135
        },
        "2602.15738v1": {
          "score": 0.7122725248336792,
          "rank": 136
        },
        "2602.11863v1": {
          "score": 0.7122713923454285,
          "rank": 137
        },
        "2602.14029v1": {
          "score": 0.7121894955635071,
          "rank": 138
        },
        "2602.10031v1": {
          "score": 0.7121118307113647,
          "rank": 139
        },
        "2602.14208v1": {
          "score": 0.7120305299758911,
          "rank": 140
        },
        "2602.13052v1": {
          "score": 0.7118935585021973,
          "rank": 141
        },
        "2602.12146v1": {
          "score": 0.7118722200393677,
          "rank": 142
        },
        "2602.13550v1": {
          "score": 0.7118628621101379,
          "rank": 143
        },
        "2602.12403v1": {
          "score": 0.711768388748169,
          "rank": 144
        },
        "2602.11646v1": {
          "score": 0.7117050886154175,
          "rank": 145
        },
        "2602.11316v1": {
          "score": 0.7116675972938538,
          "rank": 146
        },
        "2602.10420v1": {
          "score": 0.7115628719329834,
          "rank": 147
        },
        "2602.13414v1": {
          "score": 0.7113367319107056,
          "rank": 148
        },
        "2602.12746v1": {
          "score": 0.7111328840255737,
          "rank": 149
        },
        "2602.15313v1": {
          "score": 0.7110337615013123,
          "rank": 150
        },
        "2602.14293v1": {
          "score": 0.7109553813934326,
          "rank": 151
        },
        "2602.14040v1": {
          "score": 0.7108068466186523,
          "rank": 152
        },
        "2602.10796v2": {
          "score": 0.7107341885566711,
          "rank": 153
        },
        "2602.09297v1": {
          "score": 0.7107106447219849,
          "rank": 154
        },
        "2602.10004v1": {
          "score": 0.7106992602348328,
          "rank": 155
        },
        "2602.15405v1": {
          "score": 0.7106332778930664,
          "rank": 156
        },
        "2602.12606v1": {
          "score": 0.7106273174285889,
          "rank": 157
        },
        "2602.12021v1": {
          "score": 0.7105848789215088,
          "rank": 158
        },
        "2602.13073v1": {
          "score": 0.7104957103729248,
          "rank": 159
        },
        "2602.13319v1": {
          "score": 0.7103317975997925,
          "rank": 160
        },
        "2602.14452v1": {
          "score": 0.7103019952774048,
          "rank": 161
        },
        "2602.10953v1": {
          "score": 0.7102057933807373,
          "rank": 162
        },
        "2602.14102v1": {
          "score": 0.7100394368171692,
          "rank": 163
        },
        "2602.13710v1": {
          "score": 0.7100050449371338,
          "rank": 164
        },
        "2602.13594v1": {
          "score": 0.7099289894104004,
          "rank": 165
        },
        "2602.14111v1": {
          "score": 0.7098795771598816,
          "rank": 166
        },
        "2602.09316v2": {
          "score": 0.7098300457000732,
          "rank": 167
        },
        "2602.14423v1": {
          "score": 0.7096381783485413,
          "rank": 168
        },
        "2602.09720v1": {
          "score": 0.709630012512207,
          "rank": 169
        },
        "2602.15238v2": {
          "score": 0.7094478607177734,
          "rank": 170
        },
        "2602.13987v1": {
          "score": 0.709441065788269,
          "rank": 171
        },
        "2602.11410v1": {
          "score": 0.709342360496521,
          "rank": 172
        },
        "2602.11937v1": {
          "score": 0.7093324065208435,
          "rank": 173
        },
        "2602.16012v1": {
          "score": 0.7093126773834229,
          "rank": 174
        },
        "2602.11139v1": {
          "score": 0.7092916965484619,
          "rank": 175
        },
        "2602.10622v1": {
          "score": 0.7092021703720093,
          "rank": 176
        },
        "2602.16340v1": {
          "score": 0.7090662121772766,
          "rank": 177
        },
        "2602.09161v1": {
          "score": 0.7089974880218506,
          "rank": 178
        },
        "2602.10854v1": {
          "score": 0.7089612483978271,
          "rank": 179
        },
        "2602.11090v1": {
          "score": 0.7086640000343323,
          "rank": 180
        },
        "2602.10743v1": {
          "score": 0.7086223363876343,
          "rank": 181
        },
        "2602.14401v1": {
          "score": 0.7086093425750732,
          "rank": 182
        },
        "2602.16204v1": {
          "score": 0.7085880041122437,
          "rank": 183
        },
        "2602.14244v1": {
          "score": 0.708362877368927,
          "rank": 184
        },
        "2602.11578v1": {
          "score": 0.7079232931137085,
          "rank": 185
        },
        "2602.11004v1": {
          "score": 0.7079137563705444,
          "rank": 186
        },
        "2602.09719v1": {
          "score": 0.7078970670700073,
          "rank": 187
        },
        "2602.13325v1": {
          "score": 0.7078350782394409,
          "rank": 188
        },
        "2602.15676v1": {
          "score": 0.7078315019607544,
          "rank": 189
        },
        "2602.13191v1": {
          "score": 0.7076661586761475,
          "rank": 190
        },
        "2602.12855v1": {
          "score": 0.7076594233512878,
          "rank": 191
        },
        "2602.14917v1": {
          "score": 0.707638144493103,
          "rank": 192
        },
        "2602.09953v1": {
          "score": 0.7076063752174377,
          "rank": 193
        },
        "2602.14272v1": {
          "score": 0.7075403928756714,
          "rank": 194
        },
        "2602.10584v1": {
          "score": 0.7073670625686646,
          "rank": 195
        },
        "2602.14655v1": {
          "score": 0.7073622941970825,
          "rank": 196
        },
        "2602.11629v1": {
          "score": 0.7072523236274719,
          "rank": 197
        },
        "2602.15379v1": {
          "score": 0.7072204351425171,
          "rank": 198
        },
        "2602.09901v1": {
          "score": 0.7070984244346619,
          "rank": 199
        },
        "2602.10539v1": {
          "score": 0.7070944309234619,
          "rank": 200
        },
        "2602.11047v3": {
          "score": 0.7069934010505676,
          "rank": 201
        },
        "2602.11451v1": {
          "score": 0.706974983215332,
          "rank": 202
        },
        "2602.10770v1": {
          "score": 0.7068432569503784,
          "rank": 203
        },
        "2602.13670v1": {
          "score": 0.7068048119544983,
          "rank": 204
        },
        "2602.12708v1": {
          "score": 0.7066938877105713,
          "rank": 205
        },
        "2602.10911v1": {
          "score": 0.7066176533699036,
          "rank": 206
        },
        "2602.16144v1": {
          "score": 0.7065403461456299,
          "rank": 207
        },
        "2602.10870v1": {
          "score": 0.7064470052719116,
          "rank": 208
        },
        "2602.10300v1": {
          "score": 0.7064076066017151,
          "rank": 209
        },
        "2602.11812v1": {
          "score": 0.7063190937042236,
          "rank": 210
        },
        "2602.10994v1": {
          "score": 0.7062966227531433,
          "rank": 211
        },
        "2602.11792v1": {
          "score": 0.7062641382217407,
          "rank": 212
        },
        "2602.14814v1": {
          "score": 0.7060900926589966,
          "rank": 213
        },
        "2602.13485v1": {
          "score": 0.7060880064964294,
          "rank": 214
        },
        "2602.12986v1": {
          "score": 0.7060656547546387,
          "rank": 215
        },
        "2602.14506v1": {
          "score": 0.7060299515724182,
          "rank": 216
        },
        "2602.10595v1": {
          "score": 0.7060216069221497,
          "rank": 217
        },
        "2602.13680v1": {
          "score": 0.7059345245361328,
          "rank": 218
        },
        "2602.12601v1": {
          "score": 0.705791711807251,
          "rank": 219
        },
        "2602.11062v1": {
          "score": 0.7057828903198242,
          "rank": 220
        },
        "2602.11550v1": {
          "score": 0.7057085037231445,
          "rank": 221
        },
        "2602.15091v1": {
          "score": 0.705650269985199,
          "rank": 222
        },
        "2602.15423v1": {
          "score": 0.7054901123046875,
          "rank": 223
        },
        "2602.15360v1": {
          "score": 0.7054453492164612,
          "rank": 224
        },
        "2602.09001v1": {
          "score": 0.7054173350334167,
          "rank": 225
        },
        "2602.11149v1": {
          "score": 0.7053791284561157,
          "rank": 226
        },
        "2602.11966v1": {
          "score": 0.7052978277206421,
          "rank": 227
        },
        "2602.16664v1": {
          "score": 0.7051728963851929,
          "rank": 228
        },
        "2602.16188v1": {
          "score": 0.7051361203193665,
          "rank": 229
        },
        "2602.11388v1": {
          "score": 0.7051133513450623,
          "rank": 230
        },
        "2602.09009v1": {
          "score": 0.7049625515937805,
          "rank": 231
        },
        "2602.16024v1": {
          "score": 0.7049283981323242,
          "rank": 232
        },
        "2602.16442v1": {
          "score": 0.7048236131668091,
          "rank": 233
        },
        "2602.15580v1": {
          "score": 0.7046973705291748,
          "rank": 234
        },
        "2602.11022v1": {
          "score": 0.7045360803604126,
          "rank": 235
        },
        "2602.09182v1": {
          "score": 0.7045192718505859,
          "rank": 236
        },
        "2602.13466v1": {
          "score": 0.7045059204101562,
          "rank": 237
        },
        "2602.09519v1": {
          "score": 0.7044766545295715,
          "rank": 238
        },
        "2602.10751v1": {
          "score": 0.7044422626495361,
          "rank": 239
        },
        "2602.11730v1": {
          "score": 0.704434335231781,
          "rank": 240
        },
        "2602.11000v1": {
          "score": 0.7044324278831482,
          "rank": 241
        },
        "2602.13704v1": {
          "score": 0.7043967247009277,
          "rank": 242
        },
        "2602.16429v1": {
          "score": 0.704164445400238,
          "rank": 243
        },
        "2602.16537v1": {
          "score": 0.7041466236114502,
          "rank": 244
        },
        "2602.09555v2": {
          "score": 0.7040473818778992,
          "rank": 245
        },
        "2602.12704v1": {
          "score": 0.7039824724197388,
          "rank": 246
        },
        "2602.09116v2": {
          "score": 0.7036656141281128,
          "rank": 247
        },
        "2602.15089v1": {
          "score": 0.7036349773406982,
          "rank": 248
        },
        "2602.13017v1": {
          "score": 0.7035350203514099,
          "rank": 249
        },
        "2602.11761v1": {
          "score": 0.7034474015235901,
          "rank": 250
        },
        "2602.11623v1": {
          "score": 0.7034096717834473,
          "rank": 251
        },
        "2602.10067v3": {
          "score": 0.7033447027206421,
          "rank": 252
        },
        "2602.14687v1": {
          "score": 0.7033179402351379,
          "rank": 253
        },
        "2602.12391v1": {
          "score": 0.7031674385070801,
          "rank": 254
        },
        "2602.14480v1": {
          "score": 0.7031026482582092,
          "rank": 255
        },
        "2602.16490v1": {
          "score": 0.7029157876968384,
          "rank": 256
        },
        "2602.12243v1": {
          "score": 0.7029088139533997,
          "rank": 257
        },
        "2602.09983v1": {
          "score": 0.7028974294662476,
          "rank": 258
        },
        "2602.14445v1": {
          "score": 0.7028396725654602,
          "rank": 259
        },
        "2602.14482v1": {
          "score": 0.7028254866600037,
          "rank": 260
        },
        "2602.09746v1": {
          "score": 0.7028088569641113,
          "rank": 261
        },
        "2602.14404v1": {
          "score": 0.7027730941772461,
          "rank": 262
        },
        "2602.14169v1": {
          "score": 0.7027656435966492,
          "rank": 263
        },
        "2602.15229v1": {
          "score": 0.7027209997177124,
          "rank": 264
        },
        "2602.14024v1": {
          "score": 0.70267254114151,
          "rank": 265
        },
        "2602.09080v1": {
          "score": 0.7026442289352417,
          "rank": 266
        },
        "2602.16336v1": {
          "score": 0.7024387717247009,
          "rank": 267
        },
        "2602.15128v1": {
          "score": 0.7024127244949341,
          "rank": 268
        },
        "2602.14761v1": {
          "score": 0.7024000883102417,
          "rank": 269
        },
        "2602.12041v1": {
          "score": 0.7023845911026001,
          "rank": 270
        },
        "2602.10623v1": {
          "score": 0.7023365497589111,
          "rank": 271
        },
        "2602.13483v1": {
          "score": 0.7023011445999146,
          "rank": 272
        },
        "2602.09457v1": {
          "score": 0.7022804021835327,
          "rank": 273
        },
        "2602.13093v2": {
          "score": 0.7022285461425781,
          "rank": 274
        },
        "2602.09306v1": {
          "score": 0.7022272944450378,
          "rank": 275
        },
        "2602.10780v1": {
          "score": 0.7021821141242981,
          "rank": 276
        },
        "2602.11717v1": {
          "score": 0.702131450176239,
          "rank": 277
        },
        "2602.11747v1": {
          "score": 0.7020533084869385,
          "rank": 278
        },
        "2602.10056v1": {
          "score": 0.702004611492157,
          "rank": 279
        },
        "2602.16337v1": {
          "score": 0.7019995450973511,
          "rank": 280
        },
        "2602.13738v1": {
          "score": 0.7019714713096619,
          "rank": 281
        },
        "2602.15971v1": {
          "score": 0.7019690275192261,
          "rank": 282
        },
        "2602.12429v1": {
          "score": 0.701874852180481,
          "rank": 283
        },
        "2602.15338v1": {
          "score": 0.7018618583679199,
          "rank": 284
        },
        "2602.13980v1": {
          "score": 0.7018569111824036,
          "rank": 285
        },
        "2602.14983v1": {
          "score": 0.7018453478813171,
          "rank": 286
        },
        "2602.11639v1": {
          "score": 0.7016504406929016,
          "rank": 287
        },
        "2602.10015v2": {
          "score": 0.7015230655670166,
          "rank": 288
        },
        "2602.11087v1": {
          "score": 0.7014844417572021,
          "rank": 289
        },
        "2602.13517v1": {
          "score": 0.7014625072479248,
          "rank": 290
        },
        "2602.14386v1": {
          "score": 0.7014262080192566,
          "rank": 291
        },
        "2602.12681v1": {
          "score": 0.7014175057411194,
          "rank": 292
        },
        "2602.12936v1": {
          "score": 0.7013653516769409,
          "rank": 293
        },
        "2602.11455v1": {
          "score": 0.7013084292411804,
          "rank": 294
        },
        "2602.12245v1": {
          "score": 0.7012508511543274,
          "rank": 295
        },
        "2602.12482v1": {
          "score": 0.7010579705238342,
          "rank": 296
        },
        "2602.10816v1": {
          "score": 0.7009584903717041,
          "rank": 297
        },
        "2602.12222v1": {
          "score": 0.7009468078613281,
          "rank": 298
        },
        "2602.11549v1": {
          "score": 0.7009022235870361,
          "rank": 299
        },
        "2602.15484v1": {
          "score": 0.7009003162384033,
          "rank": 300
        },
        "2602.15755v1": {
          "score": 0.7008988857269287,
          "rank": 301
        },
        "2602.14344v1": {
          "score": 0.7008955478668213,
          "rank": 302
        },
        "2602.14519v1": {
          "score": 0.7008046507835388,
          "rank": 303
        },
        "2602.14938v1": {
          "score": 0.7007445096969604,
          "rank": 304
        },
        "2602.14078v1": {
          "score": 0.7007388472557068,
          "rank": 305
        },
        "2602.12618v1": {
          "score": 0.7007073163986206,
          "rank": 306
        },
        "2602.11899v1": {
          "score": 0.7006657123565674,
          "rank": 307
        },
        "2602.09162v1": {
          "score": 0.7005131840705872,
          "rank": 308
        },
        "2602.13015v1": {
          "score": 0.7003829479217529,
          "rank": 309
        },
        "2602.12798v1": {
          "score": 0.7003070116043091,
          "rank": 310
        },
        "2602.13813v1": {
          "score": 0.7002953290939331,
          "rank": 311
        },
        "2602.16711v1": {
          "score": 0.7002449035644531,
          "rank": 312
        },
        "2602.09288v1": {
          "score": 0.7001659274101257,
          "rank": 313
        },
        "2602.11800v1": {
          "score": 0.7001648545265198,
          "rank": 314
        },
        "2602.15380v1": {
          "score": 0.7000992298126221,
          "rank": 315
        },
        "2602.16146v1": {
          "score": 0.7000360488891602,
          "rank": 316
        },
        "2602.12078v1": {
          "score": 0.7000167965888977,
          "rank": 317
        },
        "2602.13108v1": {
          "score": 0.699989914894104,
          "rank": 318
        },
        "2602.11005v1": {
          "score": 0.6999125480651855,
          "rank": 319
        },
        "2602.15725v1": {
          "score": 0.6999053955078125,
          "rank": 320
        },
        "2602.16456v1": {
          "score": 0.6998814344406128,
          "rank": 321
        },
        "2602.14772v1": {
          "score": 0.6998738646507263,
          "rank": 322
        },
        "2602.11089v1": {
          "score": 0.6998189091682434,
          "rank": 323
        },
        "2602.10303v1": {
          "score": 0.6997659206390381,
          "rank": 324
        },
        "2602.14077v1": {
          "score": 0.6997565031051636,
          "rank": 325
        },
        "2602.12674v1": {
          "score": 0.6997076869010925,
          "rank": 326
        },
        "2602.09621v2": {
          "score": 0.6996622085571289,
          "rank": 327
        },
        "2602.14536v1": {
          "score": 0.6996010541915894,
          "rank": 328
        },
        "2602.12379v1": {
          "score": 0.6995835304260254,
          "rank": 329
        },
        "2602.10794v1": {
          "score": 0.6995606422424316,
          "rank": 330
        },
        "2602.13486v1": {
          "score": 0.699537456035614,
          "rank": 331
        },
        "2602.16092v1": {
          "score": 0.6994994878768921,
          "rank": 332
        },
        "2602.10478v2": {
          "score": 0.6992610692977905,
          "rank": 333
        },
        "2602.13937v1": {
          "score": 0.6992551684379578,
          "rank": 334
        },
        "2602.13040v1": {
          "score": 0.6992465853691101,
          "rank": 335
        },
        "2602.09487v1": {
          "score": 0.6992045044898987,
          "rank": 336
        },
        "2602.13536v1": {
          "score": 0.699201226234436,
          "rank": 337
        },
        "2602.13773v1": {
          "score": 0.6991938352584839,
          "rank": 338
        },
        "2602.09581v1": {
          "score": 0.6991866827011108,
          "rank": 339
        },
        "2602.11565v1": {
          "score": 0.699094295501709,
          "rank": 340
        },
        "2602.11498v1": {
          "score": 0.6989568471908569,
          "rank": 341
        },
        "2602.09375v1": {
          "score": 0.6989040374755859,
          "rank": 342
        },
        "2602.10238v1": {
          "score": 0.6988852024078369,
          "rank": 343
        },
        "2602.09934v1": {
          "score": 0.6988642811775208,
          "rank": 344
        },
        "2602.11945v1": {
          "score": 0.6987622976303101,
          "rank": 345
        },
        "2602.12205v2": {
          "score": 0.6987082958221436,
          "rank": 346
        },
        "2602.15322v1": {
          "score": 0.6986823081970215,
          "rank": 347
        },
        "2602.14885v1": {
          "score": 0.6986197233200073,
          "rank": 348
        },
        "2602.16075v1": {
          "score": 0.6985843777656555,
          "rank": 349
        },
        "2602.10635v1": {
          "score": 0.6985483169555664,
          "rank": 350
        },
        "2602.13529v1": {
          "score": 0.6984865665435791,
          "rank": 351
        },
        "2602.13675v1": {
          "score": 0.6984190940856934,
          "rank": 352
        },
        "2602.14470v1": {
          "score": 0.6983988285064697,
          "rank": 353
        },
        "2602.10388v2": {
          "score": 0.6983919143676758,
          "rank": 354
        },
        "2602.15915v1": {
          "score": 0.6983807682991028,
          "rank": 355
        },
        "2602.11044v1": {
          "score": 0.6982802152633667,
          "rank": 356
        },
        "2602.08984v1": {
          "score": 0.6981883645057678,
          "rank": 357
        },
        "2602.14411v1": {
          "score": 0.6981329321861267,
          "rank": 358
        },
        "2602.10613v1": {
          "score": 0.6980615854263306,
          "rank": 359
        },
        "2602.10217v1": {
          "score": 0.6980305314064026,
          "rank": 360
        },
        "2602.14302v1": {
          "score": 0.6980058550834656,
          "rank": 361
        },
        "2602.10048v1": {
          "score": 0.6980055570602417,
          "rank": 362
        },
        "2602.15902v1": {
          "score": 0.6979846954345703,
          "rank": 363
        },
        "2602.10583v1": {
          "score": 0.6979618668556213,
          "rank": 364
        },
        "2602.16702v1": {
          "score": 0.6979507207870483,
          "rank": 365
        },
        "2602.10506v1": {
          "score": 0.6979252696037292,
          "rank": 366
        },
        "2602.11675v1": {
          "score": 0.6979141235351562,
          "rank": 367
        },
        "2602.14729v1": {
          "score": 0.6978942155838013,
          "rank": 368
        },
        "2602.12593v1": {
          "score": 0.6978719234466553,
          "rank": 369
        },
        "2602.14425v1": {
          "score": 0.6978340148925781,
          "rank": 370
        },
        "2602.16503v1": {
          "score": 0.6977306604385376,
          "rank": 371
        },
        "2602.13075v1": {
          "score": 0.697695255279541,
          "rank": 372
        },
        "2602.14683v1": {
          "score": 0.6976941823959351,
          "rank": 373
        },
        "2602.15156v1": {
          "score": 0.6975661516189575,
          "rank": 374
        },
        "2602.10778v1": {
          "score": 0.6975486278533936,
          "rank": 375
        },
        "2602.09238v3": {
          "score": 0.6975392699241638,
          "rank": 376
        },
        "2602.13891v1": {
          "score": 0.6975030303001404,
          "rank": 377
        },
        "2602.11220v1": {
          "score": 0.6974965333938599,
          "rank": 378
        },
        "2602.13421v1": {
          "score": 0.6974042654037476,
          "rank": 379
        },
        "2602.13823v1": {
          "score": 0.697373628616333,
          "rank": 380
        },
        "2602.12158v1": {
          "score": 0.6973406076431274,
          "rank": 381
        },
        "2602.12469v1": {
          "score": 0.697228193283081,
          "rank": 382
        },
        "2602.14682v1": {
          "score": 0.6971718072891235,
          "rank": 383
        },
        "2602.14767v1": {
          "score": 0.6971426010131836,
          "rank": 384
        },
        "2602.16086v1": {
          "score": 0.6968762278556824,
          "rank": 385
        },
        "2602.15756v1": {
          "score": 0.696840226650238,
          "rank": 386
        },
        "2602.09782v1": {
          "score": 0.6968329548835754,
          "rank": 387
        },
        "2602.15547v1": {
          "score": 0.6968309283256531,
          "rank": 388
        },
        "2602.10437v2": {
          "score": 0.6968042850494385,
          "rank": 389
        },
        "2602.10875v1": {
          "score": 0.6967614889144897,
          "rank": 390
        },
        "2602.09548v1": {
          "score": 0.6966966390609741,
          "rank": 391
        },
        "2602.11715v1": {
          "score": 0.6966700553894043,
          "rank": 392
        },
        "2602.09566v1": {
          "score": 0.6966636776924133,
          "rank": 393
        },
        "2602.16500v1": {
          "score": 0.6965561509132385,
          "rank": 394
        },
        "2602.09870v1": {
          "score": 0.6964922547340393,
          "rank": 395
        },
        "2602.11789v1": {
          "score": 0.6964324116706848,
          "rank": 396
        },
        "2602.12356v1": {
          "score": 0.6964155435562134,
          "rank": 397
        },
        "2602.15603v1": {
          "score": 0.6963235139846802,
          "rank": 398
        },
        "2602.11511v1": {
          "score": 0.6963208913803101,
          "rank": 399
        },
        "2602.11217v1": {
          "score": 0.6961956024169922,
          "rank": 400
        }
      }
    },
    {
      "type": "llm_query",
      "tag": "SR",
      "paper_tag": "query:SR",
      "query_text": "帮我找符号回归与其他学科交叉并且有实证实验的最新论文",
      "sim_scores": {
        "2602.13647v1": {
          "score": 0.7428855895996094,
          "rank": 1
        },
        "2602.11249v1": {
          "score": 0.7389014363288879,
          "rank": 2
        },
        "2602.13629v2": {
          "score": 0.7376030683517456,
          "rank": 3
        },
        "2602.12783v1": {
          "score": 0.7375463247299194,
          "rank": 4
        },
        "2602.10271v3": {
          "score": 0.7338756322860718,
          "rank": 5
        },
        "2602.12528v1": {
          "score": 0.7338653206825256,
          "rank": 6
        },
        "2602.10024v1": {
          "score": 0.7338645458221436,
          "rank": 7
        },
        "2602.16467v1": {
          "score": 0.7330135107040405,
          "rank": 8
        },
        "2602.12911v1": {
          "score": 0.7302975654602051,
          "rank": 9
        },
        "2602.15506v1": {
          "score": 0.7301912307739258,
          "rank": 10
        },
        "2602.11745v1": {
          "score": 0.7296475172042847,
          "rank": 11
        },
        "2602.12639v1": {
          "score": 0.7295879125595093,
          "rank": 12
        },
        "2602.13571v1": {
          "score": 0.728948712348938,
          "rank": 13
        },
        "2602.13650v1": {
          "score": 0.7281955480575562,
          "rank": 14
        },
        "2602.09516v1": {
          "score": 0.7276482582092285,
          "rank": 15
        },
        "2602.11581v1": {
          "score": 0.7270529866218567,
          "rank": 16
        },
        "2602.14819v1": {
          "score": 0.7266467809677124,
          "rank": 17
        },
        "2602.10321v1": {
          "score": 0.7261932492256165,
          "rank": 18
        },
        "2602.14710v1": {
          "score": 0.7260987758636475,
          "rank": 19
        },
        "2602.09961v1": {
          "score": 0.7248005270957947,
          "rank": 20
        },
        "2602.13771v1": {
          "score": 0.7236526608467102,
          "rank": 21
        },
        "2602.14594v1": {
          "score": 0.7229578495025635,
          "rank": 22
        },
        "2602.11968v1": {
          "score": 0.7226380109786987,
          "rank": 23
        },
        "2602.09570v1": {
          "score": 0.7224603295326233,
          "rank": 24
        },
        "2602.11836v1": {
          "score": 0.72230064868927,
          "rank": 25
        },
        "2602.12574v1": {
          "score": 0.7216854095458984,
          "rank": 26
        },
        "2602.09444v1": {
          "score": 0.7211806178092957,
          "rank": 27
        },
        "2602.10454v1": {
          "score": 0.720725953578949,
          "rank": 28
        },
        "2602.13102v1": {
          "score": 0.7205899357795715,
          "rank": 29
        },
        "2602.09373v1": {
          "score": 0.7204604148864746,
          "rank": 30
        },
        "2602.10908v1": {
          "score": 0.7199905514717102,
          "rank": 31
        },
        "2602.12015v1": {
          "score": 0.7198643088340759,
          "rank": 32
        },
        "2602.16607v1": {
          "score": 0.7198222875595093,
          "rank": 33
        },
        "2602.12709v1": {
          "score": 0.7195956707000732,
          "rank": 34
        },
        "2602.16298v1": {
          "score": 0.7193715572357178,
          "rank": 35
        },
        "2602.15210v1": {
          "score": 0.7189927101135254,
          "rank": 36
        },
        "2602.11151v2": {
          "score": 0.718937873840332,
          "rank": 37
        },
        "2602.13870v1": {
          "score": 0.7188636064529419,
          "rank": 38
        },
        "2602.09642v1": {
          "score": 0.7187712788581848,
          "rank": 39
        },
        "2602.13059v1": {
          "score": 0.7186977863311768,
          "rank": 40
        },
        "2602.10414v1": {
          "score": 0.7174493670463562,
          "rank": 41
        },
        "2602.13964v2": {
          "score": 0.71685791015625,
          "rank": 42
        },
        "2602.09388v1": {
          "score": 0.7163786292076111,
          "rank": 43
        },
        "2602.14564v1": {
          "score": 0.7162159085273743,
          "rank": 44
        },
        "2602.09817v1": {
          "score": 0.7162060737609863,
          "rank": 45
        },
        "2602.13543v1": {
          "score": 0.7161122560501099,
          "rank": 46
        },
        "2602.11795v1": {
          "score": 0.716073751449585,
          "rank": 47
        },
        "2602.15898v1": {
          "score": 0.7154965400695801,
          "rank": 48
        },
        "2602.10732v1": {
          "score": 0.7153823971748352,
          "rank": 49
        },
        "2602.11106v2": {
          "score": 0.7153513431549072,
          "rank": 50
        },
        "2602.09552v1": {
          "score": 0.7151494026184082,
          "rank": 51
        },
        "2602.14784v1": {
          "score": 0.7151156663894653,
          "rank": 52
        },
        "2602.09901v1": {
          "score": 0.714968204498291,
          "rank": 53
        },
        "2602.11960v1": {
          "score": 0.7148864269256592,
          "rank": 54
        },
        "2602.14889v1": {
          "score": 0.7144319415092468,
          "rank": 55
        },
        "2602.11938v4": {
          "score": 0.7143880128860474,
          "rank": 56
        },
        "2602.16023v1": {
          "score": 0.7142912745475769,
          "rank": 57
        },
        "2602.09914v1": {
          "score": 0.7138032913208008,
          "rank": 58
        },
        "2602.14238v1": {
          "score": 0.7136662006378174,
          "rank": 59
        },
        "2602.14162v1": {
          "score": 0.7134830355644226,
          "rank": 60
        },
        "2602.12414v1": {
          "score": 0.7127460837364197,
          "rank": 61
        },
        "2602.15013v1": {
          "score": 0.7127382755279541,
          "rank": 62
        },
        "2602.14675v1": {
          "score": 0.7120245695114136,
          "rank": 63
        },
        "2602.12018v1": {
          "score": 0.7118662595748901,
          "rank": 64
        },
        "2602.15769v1": {
          "score": 0.7114653587341309,
          "rank": 65
        },
        "2602.09724v1": {
          "score": 0.7108995318412781,
          "rank": 66
        },
        "2602.12878v1": {
          "score": 0.7108519673347473,
          "rank": 67
        },
        "2602.16432v1": {
          "score": 0.7102636098861694,
          "rank": 68
        },
        "2602.11871v1": {
          "score": 0.7101028561592102,
          "rank": 69
        },
        "2602.12537v1": {
          "score": 0.7095808982849121,
          "rank": 70
        },
        "2602.15156v1": {
          "score": 0.7095293402671814,
          "rank": 71
        },
        "2602.16640v1": {
          "score": 0.7091920375823975,
          "rank": 72
        },
        "2602.13812v2": {
          "score": 0.7090249061584473,
          "rank": 73
        },
        "2602.10661v2": {
          "score": 0.7089005708694458,
          "rank": 74
        },
        "2602.09366v1": {
          "score": 0.7088598012924194,
          "rank": 75
        },
        "2602.15189v1": {
          "score": 0.7085666656494141,
          "rank": 76
        },
        "2602.16469v1": {
          "score": 0.7081298828125,
          "rank": 77
        },
        "2602.11518v1": {
          "score": 0.7078385353088379,
          "rank": 78
        },
        "2602.12064v1": {
          "score": 0.7073391675949097,
          "rank": 79
        },
        "2602.11674v1": {
          "score": 0.7073235511779785,
          "rank": 80
        },
        "2602.13890v1": {
          "score": 0.7068981528282166,
          "rank": 81
        },
        "2602.09866v1": {
          "score": 0.7068119049072266,
          "rank": 82
        },
        "2602.14285v1": {
          "score": 0.7068113684654236,
          "rank": 83
        },
        "2602.10583v1": {
          "score": 0.7066499590873718,
          "rank": 84
        },
        "2602.15540v1": {
          "score": 0.7063396573066711,
          "rank": 85
        },
        "2602.10367v1": {
          "score": 0.7062052488327026,
          "rank": 86
        },
        "2602.09821v2": {
          "score": 0.7061797380447388,
          "rank": 87
        },
        "2602.12779v1": {
          "score": 0.7058280110359192,
          "rank": 88
        },
        "2602.15373v2": {
          "score": 0.7056899070739746,
          "rank": 89
        },
        "2602.13139v1": {
          "score": 0.7054699659347534,
          "rank": 90
        },
        "2602.14279v1": {
          "score": 0.705387532711029,
          "rank": 91
        },
        "2602.12251v1": {
          "score": 0.705136775970459,
          "rank": 92
        },
        "2602.09703v1": {
          "score": 0.7050407528877258,
          "rank": 93
        },
        "2602.11961v1": {
          "score": 0.7049935460090637,
          "rank": 94
        },
        "2602.11040v1": {
          "score": 0.7045501470565796,
          "rank": 95
        },
        "2602.14960v1": {
          "score": 0.7044585943222046,
          "rank": 96
        },
        "2602.14062v1": {
          "score": 0.7042721509933472,
          "rank": 97
        },
        "2602.12778v1": {
          "score": 0.704190194606781,
          "rank": 98
        },
        "2602.10210v1": {
          "score": 0.7038730382919312,
          "rank": 99
        },
        "2602.12278v1": {
          "score": 0.7038285732269287,
          "rank": 100
        },
        "2602.11957v1": {
          "score": 0.7036974430084229,
          "rank": 101
        },
        "2602.15902v1": {
          "score": 0.7036086320877075,
          "rank": 102
        },
        "2602.15958v1": {
          "score": 0.7035130262374878,
          "rank": 103
        },
        "2602.14793v1": {
          "score": 0.7034294605255127,
          "rank": 104
        },
        "2602.10345v1": {
          "score": 0.70313960313797,
          "rank": 105
        },
        "2602.12132v1": {
          "score": 0.7028292417526245,
          "rank": 106
        },
        "2602.09832v1": {
          "score": 0.7028200626373291,
          "rank": 107
        },
        "2602.12410v1": {
          "score": 0.7025192975997925,
          "rank": 108
        },
        "2602.09785v1": {
          "score": 0.7025192975997925,
          "rank": 109
        },
        "2602.12129v1": {
          "score": 0.702476978302002,
          "rank": 110
        },
        "2602.12170v1": {
          "score": 0.7020424604415894,
          "rank": 111
        },
        "2602.15675v1": {
          "score": 0.7018997669219971,
          "rank": 112
        },
        "2602.12424v1": {
          "score": 0.7018535733222961,
          "rank": 113
        },
        "2602.14466v1": {
          "score": 0.7018114328384399,
          "rank": 114
        },
        "2602.11221v1": {
          "score": 0.701664388179779,
          "rank": 115
        },
        "2602.09574v1": {
          "score": 0.7015302181243896,
          "rank": 116
        },
        "2602.10881v1": {
          "score": 0.7014619708061218,
          "rank": 117
        },
        "2602.12192v1": {
          "score": 0.7014365792274475,
          "rank": 118
        },
        "2602.11444v1": {
          "score": 0.7014188170433044,
          "rank": 119
        },
        "2602.13867v1": {
          "score": 0.7013498544692993,
          "rank": 120
        },
        "2602.16019v1": {
          "score": 0.7013324499130249,
          "rank": 121
        },
        "2602.09379v2": {
          "score": 0.7013230323791504,
          "rank": 122
        },
        "2602.13504v1": {
          "score": 0.7013024687767029,
          "rank": 123
        },
        "2602.12137v1": {
          "score": 0.7012783885002136,
          "rank": 124
        },
        "2602.09550v1": {
          "score": 0.7012224197387695,
          "rank": 125
        },
        "2602.15190v1": {
          "score": 0.7011862397193909,
          "rank": 126
        },
        "2602.10017v1": {
          "score": 0.7009149193763733,
          "rank": 127
        },
        "2602.14612v1": {
          "score": 0.7006917595863342,
          "rank": 128
        },
        "2602.14755v1": {
          "score": 0.7004147171974182,
          "rank": 129
        },
        "2602.13352v1": {
          "score": 0.700182318687439,
          "rank": 130
        },
        "2602.15257v1": {
          "score": 0.7000371813774109,
          "rank": 131
        },
        "2602.13521v2": {
          "score": 0.6998909711837769,
          "rank": 132
        },
        "2602.16006v1": {
          "score": 0.699807345867157,
          "rank": 133
        },
        "2602.09907v1": {
          "score": 0.6995920538902283,
          "rank": 134
        },
        "2602.12203v1": {
          "score": 0.6993743777275085,
          "rank": 135
        },
        "2602.15139v1": {
          "score": 0.6990618705749512,
          "rank": 136
        },
        "2602.09147v1": {
          "score": 0.6989501714706421,
          "rank": 137
        },
        "2602.13194v2": {
          "score": 0.698926568031311,
          "rank": 138
        },
        "2602.16050v1": {
          "score": 0.6987302303314209,
          "rank": 139
        },
        "2602.12445v1": {
          "score": 0.6986757516860962,
          "rank": 140
        },
        "2602.15895v1": {
          "score": 0.6985923051834106,
          "rank": 141
        },
        "2602.09624v1": {
          "score": 0.6985825300216675,
          "rank": 142
        },
        "2602.10748v1": {
          "score": 0.6984530687332153,
          "rank": 143
        },
        "2602.13816v1": {
          "score": 0.698307454586029,
          "rank": 144
        },
        "2602.13359v1": {
          "score": 0.6982897520065308,
          "rank": 145
        },
        "2602.12302v1": {
          "score": 0.6981732249259949,
          "rank": 146
        },
        "2602.15921v1": {
          "score": 0.6981328725814819,
          "rank": 147
        },
        "2602.09590v1": {
          "score": 0.6980393528938293,
          "rank": 148
        },
        "2602.14200v1": {
          "score": 0.6979950666427612,
          "rank": 149
        },
        "2602.13452v1": {
          "score": 0.6978241801261902,
          "rank": 150
        },
        "2602.10295v1": {
          "score": 0.6977715492248535,
          "rank": 151
        },
        "2602.13415v1": {
          "score": 0.6976205110549927,
          "rank": 152
        },
        "2602.12004v1": {
          "score": 0.6975414752960205,
          "rank": 153
        },
        "2602.10235v1": {
          "score": 0.6974848508834839,
          "rank": 154
        },
        "2602.10886v1": {
          "score": 0.6973183155059814,
          "rank": 155
        },
        "2602.15413v1": {
          "score": 0.6972979307174683,
          "rank": 156
        },
        "2602.13042v1": {
          "score": 0.6972452402114868,
          "rank": 157
        },
        "2602.11904v1": {
          "score": 0.6972211003303528,
          "rank": 158
        },
        "2602.09760v1": {
          "score": 0.6972171068191528,
          "rank": 159
        },
        "2602.13345v1": {
          "score": 0.6970394253730774,
          "rank": 160
        },
        "2602.10074v1": {
          "score": 0.6969592571258545,
          "rank": 161
        },
        "2602.12727v1": {
          "score": 0.6968186497688293,
          "rank": 162
        },
        "2602.14470v1": {
          "score": 0.6967500448226929,
          "rank": 163
        },
        "2602.09572v2": {
          "score": 0.6967300176620483,
          "rank": 164
        },
        "2602.11841v1": {
          "score": 0.696549654006958,
          "rank": 165
        },
        "2602.16516v1": {
          "score": 0.6964915990829468,
          "rank": 166
        },
        "2602.15753v1": {
          "score": 0.696479320526123,
          "rank": 167
        },
        "2602.13165v1": {
          "score": 0.6962158679962158,
          "rank": 168
        },
        "2602.14158v1": {
          "score": 0.6961066722869873,
          "rank": 169
        },
        "2602.13179v1": {
          "score": 0.6959372758865356,
          "rank": 170
        },
        "2602.15716v1": {
          "score": 0.6958777904510498,
          "rank": 171
        },
        "2602.14488v1": {
          "score": 0.6956350207328796,
          "rank": 172
        },
        "2602.09945v1": {
          "score": 0.6956346035003662,
          "rank": 173
        },
        "2602.13047v1": {
          "score": 0.6954983472824097,
          "rank": 174
        },
        "2602.13318v1": {
          "score": 0.6953834295272827,
          "rank": 175
        },
        "2602.13817v1": {
          "score": 0.6952179670333862,
          "rank": 176
        },
        "2602.14728v1": {
          "score": 0.6951837539672852,
          "rank": 177
        },
        "2602.12235v2": {
          "score": 0.6947957277297974,
          "rank": 178
        },
        "2602.09839v1": {
          "score": 0.6945984959602356,
          "rank": 179
        },
        "2602.16200v1": {
          "score": 0.694587767124176,
          "rank": 180
        },
        "2602.11453v1": {
          "score": 0.6945031881332397,
          "rank": 181
        },
        "2602.11081v1": {
          "score": 0.6944377422332764,
          "rank": 182
        },
        "2602.14879v1": {
          "score": 0.6944360733032227,
          "rank": 183
        },
        "2602.13598v1": {
          "score": 0.6942920684814453,
          "rank": 184
        },
        "2602.15514v1": {
          "score": 0.6942672729492188,
          "rank": 185
        },
        "2602.12833v1": {
          "score": 0.6941062211990356,
          "rank": 186
        },
        "2602.12247v2": {
          "score": 0.6940324902534485,
          "rank": 187
        },
        "2602.16422v1": {
          "score": 0.6939071416854858,
          "rank": 188
        },
        "2602.12806v1": {
          "score": 0.693854570388794,
          "rank": 189
        },
        "2602.15150v1": {
          "score": 0.693710446357727,
          "rank": 190
        },
        "2602.10003v1": {
          "score": 0.693482518196106,
          "rank": 191
        },
        "2602.11874v1": {
          "score": 0.6933692693710327,
          "rank": 192
        },
        "2602.10021v1": {
          "score": 0.6933324337005615,
          "rank": 193
        },
        "2602.13860v1": {
          "score": 0.6931421160697937,
          "rank": 194
        },
        "2602.12921v1": {
          "score": 0.6930413842201233,
          "rank": 195
        },
        "2602.13402v1": {
          "score": 0.6930385828018188,
          "rank": 196
        },
        "2602.12187v1": {
          "score": 0.6930275559425354,
          "rank": 197
        },
        "2602.14733v1": {
          "score": 0.6930038928985596,
          "rank": 198
        },
        "2602.16033v1": {
          "score": 0.6929963827133179,
          "rank": 199
        },
        "2602.09826v1": {
          "score": 0.6929948329925537,
          "rank": 200
        },
        "2602.14406v1": {
          "score": 0.6929897665977478,
          "rank": 201
        },
        "2602.14467v1": {
          "score": 0.6926641464233398,
          "rank": 202
        },
        "2602.10833v1": {
          "score": 0.6923975348472595,
          "rank": 203
        },
        "2602.15085v1": {
          "score": 0.6923599243164062,
          "rank": 204
        },
        "2602.09838v1": {
          "score": 0.6923379898071289,
          "rank": 205
        },
        "2602.15547v1": {
          "score": 0.6922333836555481,
          "rank": 206
        },
        "2602.10749v2": {
          "score": 0.6921408772468567,
          "rank": 207
        },
        "2602.11072v1": {
          "score": 0.6920538544654846,
          "rank": 208
        },
        "2602.14349v1": {
          "score": 0.6920416951179504,
          "rank": 209
        },
        "2602.16124v1": {
          "score": 0.6919703483581543,
          "rank": 210
        },
        "2602.16284v1": {
          "score": 0.6919690370559692,
          "rank": 211
        },
        "2602.11982v1": {
          "score": 0.6919143199920654,
          "rank": 212
        },
        "2602.16136v1": {
          "score": 0.6918920278549194,
          "rank": 213
        },
        "2602.13455v1": {
          "score": 0.6918892860412598,
          "rank": 214
        },
        "2602.10258v1": {
          "score": 0.691559910774231,
          "rank": 215
        },
        "2602.12937v2": {
          "score": 0.6914169192314148,
          "rank": 216
        },
        "2602.15738v1": {
          "score": 0.6912412047386169,
          "rank": 217
        },
        "2602.12876v1": {
          "score": 0.6912038326263428,
          "rank": 218
        },
        "2602.12941v1": {
          "score": 0.6911561489105225,
          "rank": 219
        },
        "2602.09154v1": {
          "score": 0.6911518573760986,
          "rank": 220
        },
        "2602.09246v1": {
          "score": 0.6911351680755615,
          "rank": 221
        },
        "2602.10832v1": {
          "score": 0.6911349892616272,
          "rank": 222
        },
        "2602.12135v2": {
          "score": 0.6911123991012573,
          "rank": 223
        },
        "2602.12889v1": {
          "score": 0.6910856366157532,
          "rank": 224
        },
        "2602.12759v1": {
          "score": 0.6910732388496399,
          "rank": 225
        },
        "2602.14028v1": {
          "score": 0.690959095954895,
          "rank": 226
        },
        "2602.16307v1": {
          "score": 0.6909255981445312,
          "rank": 227
        },
        "2602.16650v1": {
          "score": 0.6908594369888306,
          "rank": 228
        },
        "2602.15312v1": {
          "score": 0.6906888484954834,
          "rank": 229
        },
        "2602.12510v1": {
          "score": 0.6905957460403442,
          "rank": 230
        },
        "2602.16660v1": {
          "score": 0.6904243230819702,
          "rank": 231
        },
        "2602.13704v1": {
          "score": 0.6903905868530273,
          "rank": 232
        },
        "2602.14102v1": {
          "score": 0.6903198957443237,
          "rank": 233
        },
        "2602.10684v1": {
          "score": 0.6902555227279663,
          "rank": 234
        },
        "2602.12723v2": {
          "score": 0.6902548670768738,
          "rank": 235
        },
        "2602.11286v1": {
          "score": 0.6901783347129822,
          "rank": 236
        },
        "2602.13717v1": {
          "score": 0.690142035484314,
          "rank": 237
        },
        "2602.13540v1": {
          "score": 0.6899701356887817,
          "rank": 238
        },
        "2602.16070v1": {
          "score": 0.6898266077041626,
          "rank": 239
        },
        "2602.10995v1": {
          "score": 0.6897507309913635,
          "rank": 240
        },
        "2602.09605v1": {
          "score": 0.6897022724151611,
          "rank": 241
        },
        "2602.15005v1": {
          "score": 0.6895605325698853,
          "rank": 242
        },
        "2602.09346v1": {
          "score": 0.6894544363021851,
          "rank": 243
        },
        "2602.13905v1": {
          "score": 0.6894350051879883,
          "rank": 244
        },
        "2602.16290v1": {
          "score": 0.689418375492096,
          "rank": 245
        },
        "2602.14089v1": {
          "score": 0.6893802285194397,
          "rank": 246
        },
        "2602.14643v2": {
          "score": 0.6893321871757507,
          "rank": 247
        },
        "2602.10518v1": {
          "score": 0.6891942024230957,
          "rank": 248
        },
        "2602.10006v1": {
          "score": 0.6890708208084106,
          "rank": 249
        },
        "2602.15638v1": {
          "score": 0.6889742612838745,
          "rank": 250
        },
        "2602.16039v1": {
          "score": 0.6888881325721741,
          "rank": 251
        },
        "2602.14451v1": {
          "score": 0.6888604164123535,
          "rank": 252
        },
        "2602.14291v1": {
          "score": 0.6888107061386108,
          "rank": 253
        },
        "2602.13713v1": {
          "score": 0.6887467503547668,
          "rank": 254
        },
        "2602.11756v1": {
          "score": 0.6886970400810242,
          "rank": 255
        },
        "2602.08979v1": {
          "score": 0.6886612176895142,
          "rank": 256
        },
        "2602.10023v1": {
          "score": 0.6885039806365967,
          "rank": 257
        },
        "2602.11238v1": {
          "score": 0.6884558796882629,
          "rank": 258
        },
        "2602.12120v1": {
          "score": 0.6884433031082153,
          "rank": 259
        },
        "2602.12705v2": {
          "score": 0.6883344650268555,
          "rank": 260
        },
        "2602.13123v1": {
          "score": 0.6883019208908081,
          "rank": 261
        },
        "2602.14081v1": {
          "score": 0.6881170868873596,
          "rank": 262
        },
        "2602.15778v1": {
          "score": 0.6881129145622253,
          "rank": 263
        },
        "2602.11133v1": {
          "score": 0.6880972385406494,
          "rank": 264
        },
        "2602.10149v1": {
          "score": 0.6880147457122803,
          "rank": 265
        },
        "2602.09469v1": {
          "score": 0.6880061626434326,
          "rank": 266
        },
        "2602.13933v1": {
          "score": 0.6879866719245911,
          "rank": 267
        },
        "2602.14189v1": {
          "score": 0.6879764795303345,
          "rank": 268
        },
        "2602.16231v1": {
          "score": 0.6878454685211182,
          "rank": 269
        },
        "2602.11700v1": {
          "score": 0.6875330209732056,
          "rank": 270
        },
        "2602.10735v1": {
          "score": 0.6872575283050537,
          "rank": 271
        },
        "2602.14743v1": {
          "score": 0.6871418356895447,
          "rank": 272
        },
        "2602.12564v1": {
          "score": 0.6870809197425842,
          "rank": 273
        },
        "2602.15313v1": {
          "score": 0.6869621276855469,
          "rank": 274
        },
        "2602.11091v1": {
          "score": 0.6868457794189453,
          "rank": 275
        },
        "2602.12957v1": {
          "score": 0.6867908239364624,
          "rank": 276
        },
        "2602.09691v1": {
          "score": 0.6867461204528809,
          "rank": 277
        },
        "2602.14274v1": {
          "score": 0.686683177947998,
          "rank": 278
        },
        "2602.10656v1": {
          "score": 0.6865626573562622,
          "rank": 279
        },
        "2602.11509v1": {
          "score": 0.6865378618240356,
          "rank": 280
        },
        "2602.09712v1": {
          "score": 0.6863532066345215,
          "rank": 281
        },
        "2602.14519v1": {
          "score": 0.6863267421722412,
          "rank": 282
        },
        "2602.12843v1": {
          "score": 0.6862164735794067,
          "rank": 283
        },
        "2602.16430v1": {
          "score": 0.6861713528633118,
          "rank": 284
        },
        "2602.15564v1": {
          "score": 0.6861408948898315,
          "rank": 285
        },
        "2602.15650v1": {
          "score": 0.6859906911849976,
          "rank": 286
        },
        "2602.15353v1": {
          "score": 0.6859732866287231,
          "rank": 287
        },
        "2602.11596v1": {
          "score": 0.6859732866287231,
          "rank": 288
        },
        "2602.09653v2": {
          "score": 0.6858768463134766,
          "rank": 289
        },
        "2602.14536v1": {
          "score": 0.6857789754867554,
          "rank": 290
        },
        "2602.12318v1": {
          "score": 0.6857043504714966,
          "rank": 291
        },
        "2602.15915v1": {
          "score": 0.6856979727745056,
          "rank": 292
        },
        "2602.09130v2": {
          "score": 0.6853392720222473,
          "rank": 293
        },
        "2602.14073v2": {
          "score": 0.6853089332580566,
          "rank": 294
        },
        "2602.11609v1": {
          "score": 0.6852208375930786,
          "rank": 295
        },
        "2602.12871v1": {
          "score": 0.6851418018341064,
          "rank": 296
        },
        "2602.12606v1": {
          "score": 0.6849743127822876,
          "rank": 297
        },
        "2602.15378v1": {
          "score": 0.68487149477005,
          "rank": 298
        },
        "2602.15143v1": {
          "score": 0.6848688125610352,
          "rank": 299
        },
        "2602.16189v1": {
          "score": 0.6848577260971069,
          "rank": 300
        },
        "2602.12819v1": {
          "score": 0.6848466396331787,
          "rank": 301
        },
        "2602.13575v1": {
          "score": 0.684829592704773,
          "rank": 302
        },
        "2602.12828v1": {
          "score": 0.6848291158676147,
          "rank": 303
        },
        "2602.14060v1": {
          "score": 0.6847512722015381,
          "rank": 304
        },
        "2602.15078v1": {
          "score": 0.6846041083335876,
          "rank": 305
        },
        "2602.14044v1": {
          "score": 0.684596598148346,
          "rank": 306
        },
        "2602.11908v2": {
          "score": 0.6845555901527405,
          "rank": 307
        },
        "2602.14763v1": {
          "score": 0.6844703555107117,
          "rank": 308
        },
        "2602.13479v1": {
          "score": 0.6843527555465698,
          "rank": 309
        },
        "2602.16400v1": {
          "score": 0.6843481063842773,
          "rank": 310
        },
        "2602.11074v1": {
          "score": 0.6842345595359802,
          "rank": 311
        },
        "2602.10953v1": {
          "score": 0.6842043399810791,
          "rank": 312
        },
        "2602.16497v1": {
          "score": 0.6841856241226196,
          "rank": 313
        },
        "2602.11139v1": {
          "score": 0.6841463446617126,
          "rank": 314
        },
        "2602.15476v1": {
          "score": 0.6841152906417847,
          "rank": 315
        },
        "2602.13680v1": {
          "score": 0.6840369701385498,
          "rank": 316
        },
        "2602.12262v2": {
          "score": 0.6840156316757202,
          "rank": 317
        },
        "2602.14748v1": {
          "score": 0.6840057373046875,
          "rank": 318
        },
        "2602.15681v1": {
          "score": 0.6839867234230042,
          "rank": 319
        },
        "2602.11958v1": {
          "score": 0.6839452385902405,
          "rank": 320
        },
        "2602.16201v1": {
          "score": 0.6834633946418762,
          "rank": 321
        },
        "2602.16703v1": {
          "score": 0.6834496259689331,
          "rank": 322
        },
        "2602.14868v1": {
          "score": 0.6830881834030151,
          "rank": 323
        },
        "2602.13195v1": {
          "score": 0.6830275058746338,
          "rank": 324
        },
        "2602.11305v1": {
          "score": 0.683018684387207,
          "rank": 325
        },
        "2602.16131v1": {
          "score": 0.6830102205276489,
          "rank": 326
        },
        "2602.13308v2": {
          "score": 0.6828718781471252,
          "rank": 327
        },
        "2602.16299v1": {
          "score": 0.6828645467758179,
          "rank": 328
        },
        "2602.10527v1": {
          "score": 0.682857096195221,
          "rank": 329
        },
        "2602.16123v1": {
          "score": 0.6827991008758545,
          "rank": 330
        },
        "2602.10388v2": {
          "score": 0.6827841401100159,
          "rank": 331
        },
        "2602.15295v1": {
          "score": 0.6826550960540771,
          "rank": 332
        },
        "2602.15531v1": {
          "score": 0.68254154920578,
          "rank": 333
        },
        "2602.09587v1": {
          "score": 0.6825384497642517,
          "rank": 334
        },
        "2602.13921v1": {
          "score": 0.6823740005493164,
          "rank": 335
        },
        "2602.16571v1": {
          "score": 0.6823481917381287,
          "rank": 336
        },
        "2602.11361v1": {
          "score": 0.6821508407592773,
          "rank": 337
        },
        "2602.09312v1": {
          "score": 0.68207186460495,
          "rank": 338
        },
        "2602.10238v1": {
          "score": 0.6820533275604248,
          "rank": 339
        },
        "2602.13790v1": {
          "score": 0.6819167137145996,
          "rank": 340
        },
        "2602.10405v1": {
          "score": 0.6818833947181702,
          "rank": 341
        },
        "2602.10384v2": {
          "score": 0.6818754076957703,
          "rank": 342
        },
        "2602.16093v1": {
          "score": 0.6818504333496094,
          "rank": 343
        },
        "2602.15504v1": {
          "score": 0.6816656589508057,
          "rank": 344
        },
        "2602.12237v1": {
          "score": 0.6816308498382568,
          "rank": 345
        },
        "2602.12989v1": {
          "score": 0.6815968155860901,
          "rank": 346
        },
        "2602.14502v1": {
          "score": 0.6815484762191772,
          "rank": 347
        },
        "2602.15813v1": {
          "score": 0.68147873878479,
          "rank": 348
        },
        "2602.14441v1": {
          "score": 0.6814759373664856,
          "rank": 349
        },
        "2602.12206v1": {
          "score": 0.6813026666641235,
          "rank": 350
        },
        "2602.15595v1": {
          "score": 0.6812259554862976,
          "rank": 351
        },
        "2602.16437v1": {
          "score": 0.6810500621795654,
          "rank": 352
        },
        "2602.16151v1": {
          "score": 0.680939257144928,
          "rank": 353
        },
        "2602.12443v1": {
          "score": 0.6808315515518188,
          "rank": 354
        },
        "2602.09930v1": {
          "score": 0.6807920336723328,
          "rank": 355
        },
        "2602.13836v1": {
          "score": 0.6807857751846313,
          "rank": 356
        },
        "2602.14065v1": {
          "score": 0.6807804107666016,
          "rank": 357
        },
        "2602.13484v1": {
          "score": 0.6806871294975281,
          "rank": 358
        },
        "2602.13567v1": {
          "score": 0.6805453896522522,
          "rank": 359
        },
        "2602.14188v1": {
          "score": 0.6804839372634888,
          "rank": 360
        },
        "2602.13773v1": {
          "score": 0.6804646849632263,
          "rank": 361
        },
        "2602.12808v1": {
          "score": 0.6803743839263916,
          "rank": 362
        },
        "2602.15273v1": {
          "score": 0.6801666021347046,
          "rank": 363
        },
        "2602.15730v1": {
          "score": 0.680138111114502,
          "rank": 364
        },
        "2602.14002v1": {
          "score": 0.6800718903541565,
          "rank": 365
        },
        "2602.13377v1": {
          "score": 0.6797811985015869,
          "rank": 366
        },
        "2602.10165v1": {
          "score": 0.6797773241996765,
          "rank": 367
        },
        "2602.11881v1": {
          "score": 0.6797184944152832,
          "rank": 368
        },
        "2602.13748v1": {
          "score": 0.6796929836273193,
          "rank": 369
        },
        "2602.15336v1": {
          "score": 0.6796202659606934,
          "rank": 370
        },
        "2602.13862v1": {
          "score": 0.6795719861984253,
          "rank": 371
        },
        "2602.14516v1": {
          "score": 0.6795202493667603,
          "rank": 372
        },
        "2602.16113v1": {
          "score": 0.6794239282608032,
          "rank": 373
        },
        "2602.10371v1": {
          "score": 0.6794042587280273,
          "rank": 374
        },
        "2602.10802v1": {
          "score": 0.6791950464248657,
          "rank": 375
        },
        "2602.12172v1": {
          "score": 0.6791816353797913,
          "rank": 376
        },
        "2602.13585v1": {
          "score": 0.6791789531707764,
          "rank": 377
        },
        "2602.15019v2": {
          "score": 0.6791702508926392,
          "rank": 378
        },
        "2602.13376v1": {
          "score": 0.6790762543678284,
          "rank": 379
        },
        "2602.16488v1": {
          "score": 0.6790698170661926,
          "rank": 380
        },
        "2602.12640v1": {
          "score": 0.6790350675582886,
          "rank": 381
        },
        "2602.10249v1": {
          "score": 0.6789774298667908,
          "rank": 382
        },
        "2602.16512v1": {
          "score": 0.6789754033088684,
          "rank": 383
        },
        "2602.09159v1": {
          "score": 0.6789674758911133,
          "rank": 384
        },
        "2602.14234v1": {
          "score": 0.6789199113845825,
          "rank": 385
        },
        "2602.12575v1": {
          "score": 0.6786974668502808,
          "rank": 386
        },
        "2602.12530v1": {
          "score": 0.6786624193191528,
          "rank": 387
        },
        "2602.11460v1": {
          "score": 0.6786535382270813,
          "rank": 388
        },
        "2602.14917v1": {
          "score": 0.6786179542541504,
          "rank": 389
        },
        "2602.11671v1": {
          "score": 0.6785310506820679,
          "rank": 390
        },
        "2602.15391v1": {
          "score": 0.6784583926200867,
          "rank": 391
        },
        "2602.14433v1": {
          "score": 0.6784275770187378,
          "rank": 392
        },
        "2602.14812v1": {
          "score": 0.6784052848815918,
          "rank": 393
        },
        "2602.14970v1": {
          "score": 0.6783912181854248,
          "rank": 394
        },
        "2602.09517v1": {
          "score": 0.6783859729766846,
          "rank": 395
        },
        "2602.16138v1": {
          "score": 0.6783707141876221,
          "rank": 396
        },
        "2602.10441v1": {
          "score": 0.6783695220947266,
          "rank": 397
        },
        "2602.12498v1": {
          "score": 0.6782823801040649,
          "rank": 398
        },
        "2602.12413v1": {
          "score": 0.6782736778259277,
          "rank": 399
        },
        "2602.11827v1": {
          "score": 0.6782059669494629,
          "rank": 400
        }
      }
    },
    {
      "type": "llm_query",
      "tag": "SR",
      "paper_tag": "query:SR",
      "query_text": "Recent advances and state-of-the-art methods in symbolic regression",
      "sim_scores": {
        "2602.13021v2": {
          "score": 0.7977672815322876,
          "rank": 1
        },
        "2602.13098v1": {
          "score": 0.780175507068634,
          "rank": 2
        },
        "2602.10097v1": {
          "score": 0.7711752653121948,
          "rank": 3
        },
        "2602.16436v1": {
          "score": 0.7710398435592651,
          "rank": 4
        },
        "2602.12471v1": {
          "score": 0.766506552696228,
          "rank": 5
        },
        "2602.14506v1": {
          "score": 0.7651886940002441,
          "rank": 6
        },
        "2602.14440v1": {
          "score": 0.7641105651855469,
          "rank": 7
        },
        "2602.11424v1": {
          "score": 0.7621322870254517,
          "rank": 8
        },
        "2602.16376v1": {
          "score": 0.7613996267318726,
          "rank": 9
        },
        "2602.14480v1": {
          "score": 0.7605067491531372,
          "rank": 10
        },
        "2602.13362v1": {
          "score": 0.7592167854309082,
          "rank": 11
        },
        "2602.13004v1": {
          "score": 0.7578964233398438,
          "rank": 12
        },
        "2602.09457v1": {
          "score": 0.7576606273651123,
          "rank": 13
        },
        "2602.14969v1": {
          "score": 0.7576475739479065,
          "rank": 14
        },
        "2602.13104v2": {
          "score": 0.7574286460876465,
          "rank": 15
        },
        "2602.14020v1": {
          "score": 0.7573075890541077,
          "rank": 16
        },
        "2602.10905v1": {
          "score": 0.7569147348403931,
          "rank": 17
        },
        "2602.13442v1": {
          "score": 0.7564768195152283,
          "rank": 18
        },
        "2602.11747v1": {
          "score": 0.7561558485031128,
          "rank": 19
        },
        "2602.09530v1": {
          "score": 0.7554490566253662,
          "rank": 20
        },
        "2602.10587v1": {
          "score": 0.7546824216842651,
          "rank": 21
        },
        "2602.13583v1": {
          "score": 0.7540725469589233,
          "rank": 22
        },
        "2602.14423v1": {
          "score": 0.7540550231933594,
          "rank": 23
        },
        "2602.14280v1": {
          "score": 0.7538291215896606,
          "rank": 24
        },
        "2602.16091v1": {
          "score": 0.7534743547439575,
          "rank": 25
        },
        "2602.11623v1": {
          "score": 0.7532780170440674,
          "rank": 26
        },
        "2602.09959v1": {
          "score": 0.7531574964523315,
          "rank": 27
        },
        "2602.10680v1": {
          "score": 0.7527581453323364,
          "rank": 28
        },
        "2602.10282v1": {
          "score": 0.7524749040603638,
          "rank": 29
        },
        "2602.15916v1": {
          "score": 0.7523901462554932,
          "rank": 30
        },
        "2602.14913v1": {
          "score": 0.7523896098136902,
          "rank": 31
        },
        "2602.16531v1": {
          "score": 0.7516729235649109,
          "rank": 32
        },
        "2602.11722v1": {
          "score": 0.7515161633491516,
          "rank": 33
        },
        "2602.16606v1": {
          "score": 0.7513900995254517,
          "rank": 34
        },
        "2602.14683v1": {
          "score": 0.7513014078140259,
          "rank": 35
        },
        "2602.15229v1": {
          "score": 0.7510857582092285,
          "rank": 36
        },
        "2602.13413v1": {
          "score": 0.7507427930831909,
          "rank": 37
        },
        "2602.14029v1": {
          "score": 0.7506246566772461,
          "rank": 38
        },
        "2602.10332v1": {
          "score": 0.7496078014373779,
          "rank": 39
        },
        "2602.10274v1": {
          "score": 0.7495684027671814,
          "rank": 40
        },
        "2602.12693v1": {
          "score": 0.7488548159599304,
          "rank": 41
        },
        "2602.11557v1": {
          "score": 0.7482290267944336,
          "rank": 42
        },
        "2602.11899v1": {
          "score": 0.7481751441955566,
          "rank": 43
        },
        "2602.13152v1": {
          "score": 0.747287392616272,
          "rank": 44
        },
        "2602.15306v1": {
          "score": 0.7472599744796753,
          "rank": 45
        },
        "2602.10408v1": {
          "score": 0.7466809749603271,
          "rank": 46
        },
        "2602.12499v1": {
          "score": 0.7461391687393188,
          "rank": 47
        },
        "2602.11107v1": {
          "score": 0.7460809946060181,
          "rank": 48
        },
        "2602.11290v2": {
          "score": 0.7460479140281677,
          "rank": 49
        },
        "2602.16503v1": {
          "score": 0.7459402084350586,
          "rank": 50
        },
        "2602.09613v1": {
          "score": 0.7453494071960449,
          "rank": 51
        },
        "2602.11333v1": {
          "score": 0.745244026184082,
          "rank": 52
        },
        "2602.10420v1": {
          "score": 0.7450693845748901,
          "rank": 53
        },
        "2602.09167v1": {
          "score": 0.7448157668113708,
          "rank": 54
        },
        "2602.11584v1": {
          "score": 0.7448059320449829,
          "rank": 55
        },
        "2602.09720v1": {
          "score": 0.7446707487106323,
          "rank": 56
        },
        "2602.10580v1": {
          "score": 0.74463951587677,
          "rank": 57
        },
        "2602.13506v1": {
          "score": 0.7445701956748962,
          "rank": 58
        },
        "2602.09351v1": {
          "score": 0.7442532181739807,
          "rank": 59
        },
        "2602.15603v1": {
          "score": 0.7440913915634155,
          "rank": 60
        },
        "2602.11087v1": {
          "score": 0.7440723776817322,
          "rank": 61
        },
        "2602.13485v1": {
          "score": 0.7438831329345703,
          "rank": 62
        },
        "2602.12300v1": {
          "score": 0.7436147332191467,
          "rank": 63
        },
        "2602.09996v2": {
          "score": 0.7434874176979065,
          "rank": 64
        },
        "2602.16537v1": {
          "score": 0.7430065870285034,
          "rank": 65
        },
        "2602.09128v1": {
          "score": 0.742715060710907,
          "rank": 66
        },
        "2602.10424v1": {
          "score": 0.7426093816757202,
          "rank": 67
        },
        "2602.14472v1": {
          "score": 0.7425583004951477,
          "rank": 68
        },
        "2602.09869v1": {
          "score": 0.742419958114624,
          "rank": 69
        },
        "2602.09783v1": {
          "score": 0.7423624992370605,
          "rank": 70
        },
        "2602.15008v1": {
          "score": 0.7422680854797363,
          "rank": 71
        },
        "2602.15925v1": {
          "score": 0.7422271370887756,
          "rank": 72
        },
        "2602.15632v1": {
          "score": 0.7421963214874268,
          "rank": 73
        },
        "2602.09395v1": {
          "score": 0.742094874382019,
          "rank": 74
        },
        "2602.16177v1": {
          "score": 0.7420687675476074,
          "rank": 75
        },
        "2602.15297v1": {
          "score": 0.7418347597122192,
          "rank": 76
        },
        "2602.13813v1": {
          "score": 0.74177485704422,
          "rank": 77
        },
        "2602.13513v2": {
          "score": 0.7417186498641968,
          "rank": 78
        },
        "2602.10480v2": {
          "score": 0.7415720820426941,
          "rank": 79
        },
        "2602.12267v1": {
          "score": 0.7415626645088196,
          "rank": 80
        },
        "2602.10496v2": {
          "score": 0.7413686513900757,
          "rank": 81
        },
        "2602.14573v1": {
          "score": 0.741142988204956,
          "rank": 82
        },
        "2602.09619v1": {
          "score": 0.7411140203475952,
          "rank": 83
        },
        "2602.09851v1": {
          "score": 0.7408936023712158,
          "rank": 84
        },
        "2602.08976v1": {
          "score": 0.7407852411270142,
          "rank": 85
        },
        "2602.11320v2": {
          "score": 0.7406737804412842,
          "rank": 86
        },
        "2602.14760v1": {
          "score": 0.7405180335044861,
          "rank": 87
        },
        "2602.14890v1": {
          "score": 0.7404136657714844,
          "rank": 88
        },
        "2602.11322v1": {
          "score": 0.7398680448532104,
          "rank": 89
        },
        "2602.09161v1": {
          "score": 0.7397390604019165,
          "rank": 90
        },
        "2602.15283v1": {
          "score": 0.7395361661911011,
          "rank": 91
        },
        "2602.08983v1": {
          "score": 0.7394956350326538,
          "rank": 92
        },
        "2602.16608v1": {
          "score": 0.739185094833374,
          "rank": 93
        },
        "2602.09690v1": {
          "score": 0.7390979528427124,
          "rank": 94
        },
        "2602.12972v1": {
          "score": 0.7386521100997925,
          "rank": 95
        },
        "2602.14037v1": {
          "score": 0.7386382818222046,
          "rank": 96
        },
        "2602.10613v1": {
          "score": 0.7384148240089417,
          "rank": 97
        },
        "2602.10576v1": {
          "score": 0.7383515238761902,
          "rank": 98
        },
        "2602.10816v1": {
          "score": 0.7381908893585205,
          "rank": 99
        },
        "2602.11863v1": {
          "score": 0.7381857633590698,
          "rank": 100
        },
        "2602.12391v1": {
          "score": 0.7380337119102478,
          "rank": 101
        },
        "2602.16061v1": {
          "score": 0.7377840280532837,
          "rank": 102
        },
        "2602.16568v1": {
          "score": 0.7374162673950195,
          "rank": 103
        },
        "2602.13684v1": {
          "score": 0.7373424768447876,
          "rank": 104
        },
        "2602.11566v1": {
          "score": 0.7372317314147949,
          "rank": 105
        },
        "2602.10489v1": {
          "score": 0.7371276617050171,
          "rank": 106
        },
        "2602.12534v1": {
          "score": 0.7370637655258179,
          "rank": 107
        },
        "2602.09182v1": {
          "score": 0.7369853854179382,
          "rank": 108
        },
        "2602.10751v1": {
          "score": 0.7368717193603516,
          "rank": 109
        },
        "2602.11794v1": {
          "score": 0.7367295026779175,
          "rank": 110
        },
        "2602.13537v2": {
          "score": 0.7367103695869446,
          "rank": 111
        },
        "2602.10870v1": {
          "score": 0.7366716265678406,
          "rank": 112
        },
        "2602.15380v1": {
          "score": 0.7363773584365845,
          "rank": 113
        },
        "2602.14011v1": {
          "score": 0.7362934350967407,
          "rank": 114
        },
        "2602.14342v1": {
          "score": 0.7362916469573975,
          "rank": 115
        },
        "2602.09314v1": {
          "score": 0.7362170219421387,
          "rank": 116
        },
        "2602.16166v1": {
          "score": 0.7361472249031067,
          "rank": 117
        },
        "2602.16259v1": {
          "score": 0.735984206199646,
          "rank": 118
        },
        "2602.11675v1": {
          "score": 0.7358160614967346,
          "rank": 119
        },
        "2602.11374v1": {
          "score": 0.7356000542640686,
          "rank": 120
        },
        "2602.11690v1": {
          "score": 0.73558109998703,
          "rank": 121
        },
        "2602.14919v1": {
          "score": 0.7355290651321411,
          "rank": 122
        },
        "2602.11550v1": {
          "score": 0.7352900505065918,
          "rank": 123
        },
        "2602.16240v1": {
          "score": 0.7352538108825684,
          "rank": 124
        },
        "2602.10595v1": {
          "score": 0.7351933121681213,
          "rank": 125
        },
        "2602.12350v1": {
          "score": 0.735163688659668,
          "rank": 126
        },
        "2602.15738v1": {
          "score": 0.7350866794586182,
          "rank": 127
        },
        "2602.10253v1": {
          "score": 0.7350491881370544,
          "rank": 128
        },
        "2602.10457v1": {
          "score": 0.7350118160247803,
          "rank": 129
        },
        "2602.15546v1": {
          "score": 0.7349711060523987,
          "rank": 130
        },
        "2602.16274v1": {
          "score": 0.734900712966919,
          "rank": 131
        },
        "2602.16124v1": {
          "score": 0.7347218990325928,
          "rank": 132
        },
        "2602.15206v1": {
          "score": 0.734594464302063,
          "rank": 133
        },
        "2602.13783v1": {
          "score": 0.7345604300498962,
          "rank": 134
        },
        "2602.14772v1": {
          "score": 0.7345112562179565,
          "rank": 135
        },
        "2602.14692v1": {
          "score": 0.7344794273376465,
          "rank": 136
        },
        "2602.12490v1": {
          "score": 0.7344515323638916,
          "rank": 137
        },
        "2602.11789v1": {
          "score": 0.7344090342521667,
          "rank": 138
        },
        "2602.15510v1": {
          "score": 0.7342443466186523,
          "rank": 139
        },
        "2602.12592v1": {
          "score": 0.7341895699501038,
          "rank": 140
        },
        "2602.13486v1": {
          "score": 0.7341219782829285,
          "rank": 141
        },
        "2602.11812v1": {
          "score": 0.7340881824493408,
          "rank": 142
        },
        "2602.09748v1": {
          "score": 0.734041690826416,
          "rank": 143
        },
        "2602.11920v1": {
          "score": 0.7339092493057251,
          "rank": 144
        },
        "2602.08980v1": {
          "score": 0.7338337898254395,
          "rank": 145
        },
        "2602.14814v1": {
          "score": 0.733716607093811,
          "rank": 146
        },
        "2602.15920v1": {
          "score": 0.733697235584259,
          "rank": 147
        },
        "2602.12390v1": {
          "score": 0.7336136698722839,
          "rank": 148
        },
        "2602.09572v2": {
          "score": 0.7335899472236633,
          "rank": 149
        },
        "2602.14024v1": {
          "score": 0.7335109114646912,
          "rank": 150
        },
        "2602.12235v2": {
          "score": 0.7334640622138977,
          "rank": 151
        },
        "2602.12680v1": {
          "score": 0.7334587574005127,
          "rank": 152
        },
        "2602.12613v1": {
          "score": 0.7334421277046204,
          "rank": 153
        },
        "2602.11062v1": {
          "score": 0.733336329460144,
          "rank": 154
        },
        "2602.10037v1": {
          "score": 0.7332462072372437,
          "rank": 155
        },
        "2602.11578v1": {
          "score": 0.7331849336624146,
          "rank": 156
        },
        "2602.10583v1": {
          "score": 0.7330989837646484,
          "rank": 157
        },
        "2602.14737v1": {
          "score": 0.7330589294433594,
          "rank": 158
        },
        "2602.10449v2": {
          "score": 0.7330145835876465,
          "rank": 159
        },
        "2602.13069v1": {
          "score": 0.7329496145248413,
          "rank": 160
        },
        "2602.12468v1": {
          "score": 0.7329122424125671,
          "rank": 161
        },
        "2602.12828v1": {
          "score": 0.7324654459953308,
          "rank": 162
        },
        "2602.09864v1": {
          "score": 0.7324297428131104,
          "rank": 163
        },
        "2602.15593v1": {
          "score": 0.7323607802391052,
          "rank": 164
        },
        "2602.16316v1": {
          "score": 0.7323423027992249,
          "rank": 165
        },
        "2602.09240v1": {
          "score": 0.7323129773139954,
          "rank": 166
        },
        "2602.15602v1": {
          "score": 0.7322635054588318,
          "rank": 167
        },
        "2602.12703v1": {
          "score": 0.7321677803993225,
          "rank": 168
        },
        "2602.12082v1": {
          "score": 0.7319473624229431,
          "rank": 169
        },
        "2602.09761v1": {
          "score": 0.7319438457489014,
          "rank": 170
        },
        "2602.11295v1": {
          "score": 0.7319211959838867,
          "rank": 171
        },
        "2602.11387v1": {
          "score": 0.7319097518920898,
          "rank": 172
        },
        "2602.14708v1": {
          "score": 0.7318610548973083,
          "rank": 173
        },
        "2602.14320v2": {
          "score": 0.7318544387817383,
          "rank": 174
        },
        "2602.12821v1": {
          "score": 0.7317713499069214,
          "rank": 175
        },
        "2602.13807v1": {
          "score": 0.7317709922790527,
          "rank": 176
        },
        "2602.11633v1": {
          "score": 0.7317664623260498,
          "rank": 177
        },
        "2602.14244v1": {
          "score": 0.7316994071006775,
          "rank": 178
        },
        "2602.09651v1": {
          "score": 0.7316833734512329,
          "rank": 179
        },
        "2602.15022v1": {
          "score": 0.73164302110672,
          "rank": 180
        },
        "2602.11841v1": {
          "score": 0.7315623164176941,
          "rank": 181
        },
        "2602.11947v1": {
          "score": 0.7314864993095398,
          "rank": 182
        },
        "2602.11700v1": {
          "score": 0.73143470287323,
          "rank": 183
        },
        "2602.09247v1": {
          "score": 0.7314202785491943,
          "rank": 184
        },
        "2602.16449v1": {
          "score": 0.731372594833374,
          "rank": 185
        },
        "2602.16498v1": {
          "score": 0.7313238382339478,
          "rank": 186
        },
        "2602.11212v1": {
          "score": 0.731231153011322,
          "rank": 187
        },
        "2602.10374v1": {
          "score": 0.7312289476394653,
          "rank": 188
        },
        "2602.12084v1": {
          "score": 0.7311421632766724,
          "rank": 189
        },
        "2602.13729v1": {
          "score": 0.731121838092804,
          "rank": 190
        },
        "2602.12143v1": {
          "score": 0.7309943437576294,
          "rank": 191
        },
        "2602.16146v1": {
          "score": 0.7308109402656555,
          "rank": 192
        },
        "2602.11995v1": {
          "score": 0.7308045625686646,
          "rank": 193
        },
        "2602.11080v1": {
          "score": 0.7307963371276855,
          "rank": 194
        },
        "2602.15972v1": {
          "score": 0.7306852340698242,
          "rank": 195
        },
        "2602.12622v1": {
          "score": 0.7305276989936829,
          "rank": 196
        },
        "2602.09116v2": {
          "score": 0.7304046154022217,
          "rank": 197
        },
        "2602.14791v1": {
          "score": 0.7302943468093872,
          "rank": 198
        },
        "2602.11539v1": {
          "score": 0.7302490472793579,
          "rank": 199
        },
        "2602.16601v1": {
          "score": 0.7302432060241699,
          "rank": 200
        },
        "2602.15571v1": {
          "score": 0.7300881743431091,
          "rank": 201
        },
        "2602.09306v1": {
          "score": 0.7300630807876587,
          "rank": 202
        },
        "2602.16468v1": {
          "score": 0.7300277948379517,
          "rank": 203
        },
        "2602.14470v1": {
          "score": 0.7299169301986694,
          "rank": 204
        },
        "2602.14537v1": {
          "score": 0.7298406958580017,
          "rank": 205
        },
        "2602.15136v1": {
          "score": 0.7298043370246887,
          "rank": 206
        },
        "2602.11219v1": {
          "score": 0.7297415733337402,
          "rank": 207
        },
        "2602.15478v1": {
          "score": 0.7297335267066956,
          "rank": 208
        },
        "2602.12753v1": {
          "score": 0.7296687364578247,
          "rank": 209
        },
        "2602.13704v1": {
          "score": 0.7295812964439392,
          "rank": 210
        },
        "2602.10971v1": {
          "score": 0.7295348644256592,
          "rank": 211
        },
        "2602.14656v1": {
          "score": 0.7294363379478455,
          "rank": 212
        },
        "2602.12567v1": {
          "score": 0.7294109463691711,
          "rank": 213
        },
        "2602.12674v1": {
          "score": 0.7293723821640015,
          "rank": 214
        },
        "2602.14077v1": {
          "score": 0.7292194366455078,
          "rank": 215
        },
        "2602.09181v1": {
          "score": 0.7292143702507019,
          "rank": 216
        },
        "2602.14536v1": {
          "score": 0.7292121648788452,
          "rank": 217
        },
        "2602.10512v1": {
          "score": 0.729204535484314,
          "rank": 218
        },
        "2602.15586v1": {
          "score": 0.7291979789733887,
          "rank": 219
        },
        "2602.11139v1": {
          "score": 0.7291919589042664,
          "rank": 220
        },
        "2602.09548v1": {
          "score": 0.7291902899742126,
          "rank": 221
        },
        "2602.09489v1": {
          "score": 0.7289996147155762,
          "rank": 222
        },
        "2602.15676v1": {
          "score": 0.7288321256637573,
          "rank": 223
        },
        "2602.12889v1": {
          "score": 0.7288217544555664,
          "rank": 224
        },
        "2602.14462v1": {
          "score": 0.7287817001342773,
          "rank": 225
        },
        "2602.16144v1": {
          "score": 0.7287548184394836,
          "rank": 226
        },
        "2602.09593v1": {
          "score": 0.728736162185669,
          "rank": 227
        },
        "2602.10506v1": {
          "score": 0.7286758422851562,
          "rank": 228
        },
        "2602.15559v1": {
          "score": 0.7286735773086548,
          "rank": 229
        },
        "2602.12078v1": {
          "score": 0.7284727096557617,
          "rank": 230
        },
        "2602.10532v1": {
          "score": 0.728449821472168,
          "rank": 231
        },
        "2602.13169v1": {
          "score": 0.7284314632415771,
          "rank": 232
        },
        "2602.13449v1": {
          "score": 0.7283759713172913,
          "rank": 233
        },
        "2602.15964v1": {
          "score": 0.7283749580383301,
          "rank": 234
        },
        "2602.12243v1": {
          "score": 0.7283315658569336,
          "rank": 235
        },
        "2602.14869v1": {
          "score": 0.7282553911209106,
          "rank": 236
        },
        "2602.11843v1": {
          "score": 0.7281583547592163,
          "rank": 237
        },
        "2602.12975v1": {
          "score": 0.7281259298324585,
          "rank": 238
        },
        "2602.15730v1": {
          "score": 0.7280707955360413,
          "rank": 239
        },
        "2602.10212v1": {
          "score": 0.7279790639877319,
          "rank": 240
        },
        "2602.15330v1": {
          "score": 0.7279776930809021,
          "rank": 241
        },
        "2602.15725v1": {
          "score": 0.7279514670372009,
          "rank": 242
        },
        "2602.12851v1": {
          "score": 0.7279419302940369,
          "rank": 243
        },
        "2602.12429v1": {
          "score": 0.7278530597686768,
          "rank": 244
        },
        "2602.12233v1": {
          "score": 0.7278410792350769,
          "rank": 245
        },
        "2602.11861v1": {
          "score": 0.7278236150741577,
          "rank": 246
        },
        "2602.11388v1": {
          "score": 0.7278221249580383,
          "rank": 247
        },
        "2602.13937v1": {
          "score": 0.727741003036499,
          "rank": 248
        },
        "2602.14386v1": {
          "score": 0.7276323437690735,
          "rank": 249
        },
        "2602.11408v1": {
          "score": 0.7276067733764648,
          "rank": 250
        },
        "2602.12039v2": {
          "score": 0.7275084853172302,
          "rank": 251
        },
        "2602.14185v1": {
          "score": 0.7275081276893616,
          "rank": 252
        },
        "2602.09555v2": {
          "score": 0.7273998856544495,
          "rank": 253
        },
        "2602.09258v1": {
          "score": 0.7272810935974121,
          "rank": 254
        },
        "2602.16435v1": {
          "score": 0.7271168231964111,
          "rank": 255
        },
        "2602.13128v1": {
          "score": 0.7270928621292114,
          "rank": 256
        },
        "2602.16505v1": {
          "score": 0.7270680665969849,
          "rank": 257
        },
        "2602.14618v1": {
          "score": 0.7270561456680298,
          "rank": 258
        },
        "2602.10588v1": {
          "score": 0.7270439863204956,
          "rank": 259
        },
        "2602.15820v1": {
          "score": 0.7269999980926514,
          "rank": 260
        },
        "2602.16236v1": {
          "score": 0.7269555330276489,
          "rank": 261
        },
        "2602.12846v1": {
          "score": 0.7269423007965088,
          "rank": 262
        },
        "2602.11594v2": {
          "score": 0.7268550395965576,
          "rank": 263
        },
        "2602.15313v1": {
          "score": 0.7267956733703613,
          "rank": 264
        },
        "2602.16530v1": {
          "score": 0.7267856597900391,
          "rank": 265
        },
        "2602.11149v1": {
          "score": 0.7267587780952454,
          "rank": 266
        },
        "2602.15353v1": {
          "score": 0.7267332077026367,
          "rank": 267
        },
        "2602.13419v1": {
          "score": 0.7266520261764526,
          "rank": 268
        },
        "2602.16481v1": {
          "score": 0.7266464233398438,
          "rank": 269
        },
        "2602.10847v1": {
          "score": 0.7265546321868896,
          "rank": 270
        },
        "2602.10598v1": {
          "score": 0.7264876365661621,
          "rank": 271
        },
        "2602.12389v1": {
          "score": 0.7264869213104248,
          "rank": 272
        },
        "2602.14896v1": {
          "score": 0.7263976335525513,
          "rank": 273
        },
        "2602.13087v1": {
          "score": 0.7263299226760864,
          "rank": 274
        },
        "2602.13759v1": {
          "score": 0.7263243198394775,
          "rank": 275
        },
        "2602.10632v1": {
          "score": 0.726263165473938,
          "rank": 276
        },
        "2602.10329v1": {
          "score": 0.7262074947357178,
          "rank": 277
        },
        "2602.16456v1": {
          "score": 0.7261741161346436,
          "rank": 278
        },
        "2602.11698v1": {
          "score": 0.7260687351226807,
          "rank": 279
        },
        "2602.14789v1": {
          "score": 0.726029098033905,
          "rank": 280
        },
        "2602.12681v1": {
          "score": 0.7259703874588013,
          "rank": 281
        },
        "2602.15634v1": {
          "score": 0.7259641289710999,
          "rank": 282
        },
        "2602.11217v1": {
          "score": 0.7259054183959961,
          "rank": 283
        },
        "2602.10026v1": {
          "score": 0.7258864045143127,
          "rank": 284
        },
        "2602.09842v1": {
          "score": 0.725858211517334,
          "rank": 285
        },
        "2602.14274v1": {
          "score": 0.7258343696594238,
          "rank": 286
        },
        "2602.16427v1": {
          "score": 0.7258254289627075,
          "rank": 287
        },
        "2602.16709v1": {
          "score": 0.7258151769638062,
          "rank": 288
        },
        "2602.13906v1": {
          "score": 0.7257480621337891,
          "rank": 289
        },
        "2602.09901v1": {
          "score": 0.7257256507873535,
          "rank": 290
        },
        "2602.11859v1": {
          "score": 0.7257176041603088,
          "rank": 291
        },
        "2602.09520v1": {
          "score": 0.7256836295127869,
          "rank": 292
        },
        "2602.12170v1": {
          "score": 0.7256516218185425,
          "rank": 293
        },
        "2602.14938v1": {
          "score": 0.7255904674530029,
          "rank": 294
        },
        "2602.12245v1": {
          "score": 0.725578784942627,
          "rank": 295
        },
        "2602.15169v1": {
          "score": 0.7255234718322754,
          "rank": 296
        },
        "2602.09288v1": {
          "score": 0.725396990776062,
          "rank": 297
        },
        "2602.15503v1": {
          "score": 0.7253597974777222,
          "rank": 298
        },
        "2602.09487v1": {
          "score": 0.7252681255340576,
          "rank": 299
        },
        "2602.13319v1": {
          "score": 0.725192666053772,
          "rank": 300
        },
        "2602.15822v1": {
          "score": 0.725164532661438,
          "rank": 301
        },
        "2602.15921v1": {
          "score": 0.7250679731369019,
          "rank": 302
        },
        "2602.13538v1": {
          "score": 0.7250509262084961,
          "rank": 303
        },
        "2602.12982v1": {
          "score": 0.7250308394432068,
          "rank": 304
        },
        "2602.16328v1": {
          "score": 0.7250205874443054,
          "rank": 305
        },
        "2602.16155v1": {
          "score": 0.7250090837478638,
          "rank": 306
        },
        "2602.10450v1": {
          "score": 0.7249840497970581,
          "rank": 307
        },
        "2602.10602v1": {
          "score": 0.724928081035614,
          "rank": 308
        },
        "2602.12016v1": {
          "score": 0.7249194979667664,
          "rank": 309
        },
        "2602.11090v1": {
          "score": 0.7248654365539551,
          "rank": 310
        },
        "2602.15000v1": {
          "score": 0.7248444557189941,
          "rank": 311
        },
        "2602.15983v1": {
          "score": 0.7247729897499084,
          "rank": 312
        },
        "2602.10706v3": {
          "score": 0.7246493101119995,
          "rank": 313
        },
        "2602.12384v2": {
          "score": 0.7246177196502686,
          "rank": 314
        },
        "2602.10464v1": {
          "score": 0.724617600440979,
          "rank": 315
        },
        "2602.14949v1": {
          "score": 0.724610447883606,
          "rank": 316
        },
        "2602.09331v1": {
          "score": 0.7246063351631165,
          "rank": 317
        },
        "2602.14251v1": {
          "score": 0.7245991230010986,
          "rank": 318
        },
        "2602.10217v1": {
          "score": 0.7245593070983887,
          "rank": 319
        },
        "2602.13651v1": {
          "score": 0.7245393395423889,
          "rank": 320
        },
        "2602.11641v1": {
          "score": 0.7245156764984131,
          "rank": 321
        },
        "2602.10623v1": {
          "score": 0.7244706153869629,
          "rank": 322
        },
        "2602.09542v1": {
          "score": 0.7244502305984497,
          "rank": 323
        },
        "2602.10953v1": {
          "score": 0.724388599395752,
          "rank": 324
        },
        "2602.09499v1": {
          "score": 0.7243326306343079,
          "rank": 325
        },
        "2602.14058v1": {
          "score": 0.7243238687515259,
          "rank": 326
        },
        "2602.13864v1": {
          "score": 0.7241634130477905,
          "rank": 327
        },
        "2602.15289v1": {
          "score": 0.7241569757461548,
          "rank": 328
        },
        "2602.14086v1": {
          "score": 0.7241266369819641,
          "rank": 329
        },
        "2602.09304v1": {
          "score": 0.72409987449646,
          "rank": 330
        },
        "2602.09764v1": {
          "score": 0.7240700721740723,
          "rank": 331
        },
        "2602.10936v1": {
          "score": 0.7239665985107422,
          "rank": 332
        },
        "2602.10445v2": {
          "score": 0.7239297032356262,
          "rank": 333
        },
        "2602.13106v1": {
          "score": 0.7239111661911011,
          "rank": 334
        },
        "2602.16099v1": {
          "score": 0.7239072322845459,
          "rank": 335
        },
        "2602.10566v1": {
          "score": 0.7238395810127258,
          "rank": 336
        },
        "2602.10352v1": {
          "score": 0.7238188982009888,
          "rank": 337
        },
        "2602.14492v2": {
          "score": 0.7237562537193298,
          "rank": 338
        },
        "2602.10182v1": {
          "score": 0.7236802577972412,
          "rank": 339
        },
        "2602.15897v1": {
          "score": 0.723602831363678,
          "rank": 340
        },
        "2602.11639v1": {
          "score": 0.7236007452011108,
          "rank": 341
        },
        "2602.13654v1": {
          "score": 0.7235651612281799,
          "rank": 342
        },
        "2602.09196v1": {
          "score": 0.7235611081123352,
          "rank": 343
        },
        "2602.15659v1": {
          "score": 0.7235313653945923,
          "rank": 344
        },
        "2602.14154v1": {
          "score": 0.723438560962677,
          "rank": 345
        },
        "2602.16570v1": {
          "score": 0.7234352827072144,
          "rank": 346
        },
        "2602.14622v2": {
          "score": 0.7234016060829163,
          "rank": 347
        },
        "2602.11410v1": {
          "score": 0.7233933210372925,
          "rank": 348
        },
        "2602.12379v1": {
          "score": 0.7233672142028809,
          "rank": 349
        },
        "2602.09278v1": {
          "score": 0.7233548164367676,
          "rank": 350
        },
        "2602.15266v1": {
          "score": 0.7233508825302124,
          "rank": 351
        },
        "2602.15457v1": {
          "score": 0.7233418226242065,
          "rank": 352
        },
        "2602.11945v1": {
          "score": 0.7233411073684692,
          "rank": 353
        },
        "2602.12968v1": {
          "score": 0.7232736349105835,
          "rank": 354
        },
        "2602.10228v1": {
          "score": 0.7232728004455566,
          "rank": 355
        },
        "2602.11940v1": {
          "score": 0.7232011556625366,
          "rank": 356
        },
        "2602.12593v1": {
          "score": 0.7231122255325317,
          "rank": 357
        },
        "2602.15327v1": {
          "score": 0.7230543494224548,
          "rank": 358
        },
        "2602.10697v1": {
          "score": 0.7230474948883057,
          "rank": 359
        },
        "2602.15304v1": {
          "score": 0.7230206727981567,
          "rank": 360
        },
        "2602.10609v1": {
          "score": 0.7230087518692017,
          "rank": 361
        },
        "2602.10346v1": {
          "score": 0.7229828834533691,
          "rank": 362
        },
        "2602.11630v1": {
          "score": 0.7229557037353516,
          "rank": 363
        },
        "2602.15094v1": {
          "score": 0.7229395508766174,
          "rank": 364
        },
        "2602.10778v1": {
          "score": 0.7229143977165222,
          "rank": 365
        },
        "2602.13586v1": {
          "score": 0.7227575778961182,
          "rank": 366
        },
        "2602.10708v1": {
          "score": 0.7227041125297546,
          "rank": 367
        },
        "2602.14452v1": {
          "score": 0.722685694694519,
          "rank": 368
        },
        "2602.12542v1": {
          "score": 0.7226811051368713,
          "rank": 369
        },
        "2602.11738v1": {
          "score": 0.7226370573043823,
          "rank": 370
        },
        "2602.13184v1": {
          "score": 0.7225990295410156,
          "rank": 371
        },
        "2602.13571v1": {
          "score": 0.7225474715232849,
          "rank": 372
        },
        "2602.09169v1": {
          "score": 0.7225139141082764,
          "rank": 373
        },
        "2602.14767v1": {
          "score": 0.7225081920623779,
          "rank": 374
        },
        "2602.09501v1": {
          "score": 0.722493052482605,
          "rank": 375
        },
        "2602.15337v1": {
          "score": 0.7223927974700928,
          "rank": 376
        },
        "2602.11917v1": {
          "score": 0.722381591796875,
          "rank": 377
        },
        "2602.13177v1": {
          "score": 0.7223343849182129,
          "rank": 378
        },
        "2602.13052v1": {
          "score": 0.7223227620124817,
          "rank": 379
        },
        "2602.11665v1": {
          "score": 0.722274899482727,
          "rank": 380
        },
        "2602.13746v1": {
          "score": 0.7221627235412598,
          "rank": 381
        },
        "2602.10794v1": {
          "score": 0.7221477627754211,
          "rank": 382
        },
        "2602.14592v1": {
          "score": 0.7221434116363525,
          "rank": 383
        },
        "2602.13550v1": {
          "score": 0.7221392393112183,
          "rank": 384
        },
        "2602.14272v1": {
          "score": 0.7220823764801025,
          "rank": 385
        },
        "2602.13315v1": {
          "score": 0.722004234790802,
          "rank": 386
        },
        "2602.09316v2": {
          "score": 0.7218859791755676,
          "rank": 387
        },
        "2602.15538v1": {
          "score": 0.7216182351112366,
          "rank": 388
        },
        "2602.14761v1": {
          "score": 0.7215855717658997,
          "rank": 389
        },
        "2602.14231v1": {
          "score": 0.7215681076049805,
          "rank": 390
        },
        "2602.13091v1": {
          "score": 0.7215103507041931,
          "rank": 391
        },
        "2602.12756v1": {
          "score": 0.7214658260345459,
          "rank": 392
        },
        "2602.13960v1": {
          "score": 0.7214601039886475,
          "rank": 393
        },
        "2602.13532v1": {
          "score": 0.7214304804801941,
          "rank": 394
        },
        "2602.16092v1": {
          "score": 0.7213963270187378,
          "rank": 395
        },
        "2602.13510v1": {
          "score": 0.7213857769966125,
          "rank": 396
        },
        "2602.11453v1": {
          "score": 0.7213397026062012,
          "rank": 397
        },
        "2602.12469v1": {
          "score": 0.7213306427001953,
          "rank": 398
        },
        "2602.12604v1": {
          "score": 0.7212928533554077,
          "rank": 399
        },
        "2602.12589v1": {
          "score": 0.7212784886360168,
          "rank": 400
        }
      }
    },
    {
      "type": "llm_query",
      "tag": "SR",
      "paper_tag": "query:SR",
      "query_text": "Symbolic regression for scientific discovery and physical law extraction",
      "sim_scores": {
        "2602.13021v2": {
          "score": 0.795652449131012,
          "rank": 1
        },
        "2602.15603v1": {
          "score": 0.7689158320426941,
          "rank": 2
        },
        "2602.12259v1": {
          "score": 0.7652649879455566,
          "rank": 3
        },
        "2602.09132v1": {
          "score": 0.7637797594070435,
          "rank": 4
        },
        "2602.10576v1": {
          "score": 0.7603542804718018,
          "rank": 5
        },
        "2602.09801v1": {
          "score": 0.7542214393615723,
          "rank": 6
        },
        "2602.09116v2": {
          "score": 0.7514597177505493,
          "rank": 7
        },
        "2602.16481v1": {
          "score": 0.7498742341995239,
          "rank": 8
        },
        "2602.14890v1": {
          "score": 0.7461177706718445,
          "rank": 9
        },
        "2602.14470v1": {
          "score": 0.7429490685462952,
          "rank": 10
        },
        "2602.16436v1": {
          "score": 0.7429083585739136,
          "rank": 11
        },
        "2602.09163v1": {
          "score": 0.7420252561569214,
          "rank": 12
        },
        "2602.12164v1": {
          "score": 0.7415444850921631,
          "rank": 13
        },
        "2602.11295v1": {
          "score": 0.7413580417633057,
          "rank": 14
        },
        "2602.16091v1": {
          "score": 0.7406612038612366,
          "rank": 15
        },
        "2602.08990v1": {
          "score": 0.7372861504554749,
          "rank": 16
        },
        "2602.16684v1": {
          "score": 0.7371006011962891,
          "rank": 17
        },
        "2602.13583v1": {
          "score": 0.7361291646957397,
          "rank": 18
        },
        "2602.12056v1": {
          "score": 0.735682487487793,
          "rank": 19
        },
        "2602.10081v2": {
          "score": 0.7355667948722839,
          "rank": 20
        },
        "2602.10512v1": {
          "score": 0.7355601787567139,
          "rank": 21
        },
        "2602.15712v1": {
          "score": 0.7352008819580078,
          "rank": 22
        },
        "2602.13769v1": {
          "score": 0.7336853742599487,
          "rank": 23
        },
        "2602.09851v1": {
          "score": 0.7330180406570435,
          "rank": 24
        },
        "2602.09809v1": {
          "score": 0.7327456474304199,
          "rank": 25
        },
        "2602.12828v1": {
          "score": 0.7312408685684204,
          "rank": 26
        },
        "2602.09374v1": {
          "score": 0.7312391996383667,
          "rank": 27
        },
        "2602.12391v1": {
          "score": 0.7311997413635254,
          "rank": 28
        },
        "2602.13104v2": {
          "score": 0.7307939529418945,
          "rank": 29
        },
        "2602.12143v1": {
          "score": 0.73049396276474,
          "rank": 30
        },
        "2602.12170v1": {
          "score": 0.7304480671882629,
          "rank": 31
        },
        "2602.11849v1": {
          "score": 0.7300374507904053,
          "rank": 32
        },
        "2602.12984v1": {
          "score": 0.7299721240997314,
          "rank": 33
        },
        "2602.15984v1": {
          "score": 0.7297853827476501,
          "rank": 34
        },
        "2602.13647v1": {
          "score": 0.7296772003173828,
          "rank": 35
        },
        "2602.14384v1": {
          "score": 0.7292675971984863,
          "rank": 36
        },
        "2602.10023v1": {
          "score": 0.728884756565094,
          "rank": 37
        },
        "2602.11219v1": {
          "score": 0.72845059633255,
          "rank": 38
        },
        "2602.09817v1": {
          "score": 0.7283827066421509,
          "rank": 39
        },
        "2602.15169v1": {
          "score": 0.7278987169265747,
          "rank": 40
        },
        "2602.11920v1": {
          "score": 0.7278329730033875,
          "rank": 41
        },
        "2602.12665v1": {
          "score": 0.7278209924697876,
          "rank": 42
        },
        "2602.09530v1": {
          "score": 0.7278004288673401,
          "rank": 43
        },
        "2602.13855v1": {
          "score": 0.7276144623756409,
          "rank": 44
        },
        "2602.09570v1": {
          "score": 0.727342963218689,
          "rank": 45
        },
        "2602.15306v1": {
          "score": 0.727112889289856,
          "rank": 46
        },
        "2602.10845v1": {
          "score": 0.7266470789909363,
          "rank": 47
        },
        "2602.10097v1": {
          "score": 0.7264204621315002,
          "rank": 48
        },
        "2602.09783v1": {
          "score": 0.7261491417884827,
          "rank": 49
        },
        "2602.13513v2": {
          "score": 0.7261276245117188,
          "rank": 50
        },
        "2602.10021v1": {
          "score": 0.7260801792144775,
          "rank": 51
        },
        "2602.13485v1": {
          "score": 0.7259911298751831,
          "rank": 52
        },
        "2602.16650v1": {
          "score": 0.7259566783905029,
          "rank": 53
        },
        "2602.14795v1": {
          "score": 0.725932776927948,
          "rank": 54
        },
        "2602.11841v1": {
          "score": 0.7255890965461731,
          "rank": 55
        },
        "2602.13419v1": {
          "score": 0.7255409955978394,
          "rank": 56
        },
        "2602.12687v1": {
          "score": 0.7255046367645264,
          "rank": 57
        },
        "2602.14284v1": {
          "score": 0.7253630757331848,
          "rank": 58
        },
        "2602.11874v1": {
          "score": 0.7251399755477905,
          "rank": 59
        },
        "2602.13958v1": {
          "score": 0.7250881791114807,
          "rank": 60
        },
        "2602.10451v1": {
          "score": 0.7244980335235596,
          "rank": 61
        },
        "2602.10228v1": {
          "score": 0.7242474555969238,
          "rank": 62
        },
        "2602.15022v1": {
          "score": 0.7240926027297974,
          "rank": 63
        },
        "2602.13813v1": {
          "score": 0.724044144153595,
          "rank": 64
        },
        "2602.11675v1": {
          "score": 0.723929226398468,
          "rank": 65
        },
        "2602.16640v1": {
          "score": 0.7239105701446533,
          "rank": 66
        },
        "2602.15423v1": {
          "score": 0.7237731218338013,
          "rank": 67
        },
        "2602.12235v2": {
          "score": 0.7236381769180298,
          "rank": 68
        },
        "2602.13098v1": {
          "score": 0.7233742475509644,
          "rank": 69
        },
        "2602.09182v1": {
          "score": 0.7233463525772095,
          "rank": 70
        },
        "2602.12084v1": {
          "score": 0.7233262062072754,
          "rank": 71
        },
        "2602.11123v1": {
          "score": 0.7232892513275146,
          "rank": 72
        },
        "2602.16585v1": {
          "score": 0.7231982946395874,
          "rank": 73
        },
        "2602.11424v1": {
          "score": 0.7229645848274231,
          "rank": 74
        },
        "2602.15330v1": {
          "score": 0.7229642271995544,
          "rank": 75
        },
        "2602.15068v1": {
          "score": 0.7229039669036865,
          "rank": 76
        },
        "2602.09572v2": {
          "score": 0.7228754758834839,
          "rank": 77
        },
        "2602.10905v1": {
          "score": 0.7226338386535645,
          "rank": 78
        },
        "2602.09959v1": {
          "score": 0.7225813865661621,
          "rank": 79
        },
        "2602.12606v1": {
          "score": 0.7225524187088013,
          "rank": 80
        },
        "2602.14189v1": {
          "score": 0.7225251793861389,
          "rank": 81
        },
        "2602.09448v1": {
          "score": 0.722436785697937,
          "rank": 82
        },
        "2602.12082v1": {
          "score": 0.722102165222168,
          "rank": 83
        },
        "2602.16400v1": {
          "score": 0.7218955755233765,
          "rank": 84
        },
        "2602.10538v2": {
          "score": 0.7218733429908752,
          "rank": 85
        },
        "2602.13571v1": {
          "score": 0.7218337059020996,
          "rank": 86
        },
        "2602.10210v1": {
          "score": 0.721815824508667,
          "rank": 87
        },
        "2602.16020v1": {
          "score": 0.7215971946716309,
          "rank": 88
        },
        "2602.16551v1": {
          "score": 0.7215794324874878,
          "rank": 89
        },
        "2602.15725v1": {
          "score": 0.7215018272399902,
          "rank": 90
        },
        "2602.14451v1": {
          "score": 0.7214633226394653,
          "rank": 91
        },
        "2602.13873v1": {
          "score": 0.7214258313179016,
          "rank": 92
        },
        "2602.13004v1": {
          "score": 0.7213506102561951,
          "rank": 93
        },
        "2602.09651v1": {
          "score": 0.7213259935379028,
          "rank": 94
        },
        "2602.11623v1": {
          "score": 0.7212254405021667,
          "rank": 95
        },
        "2602.13830v1": {
          "score": 0.721078634262085,
          "rank": 96
        },
        "2602.12747v1": {
          "score": 0.7210519909858704,
          "rank": 97
        },
        "2602.09340v1": {
          "score": 0.7210211753845215,
          "rank": 98
        },
        "2602.12889v1": {
          "score": 0.7209027409553528,
          "rank": 99
        },
        "2602.09128v1": {
          "score": 0.7208744287490845,
          "rank": 100
        },
        "2602.11881v1": {
          "score": 0.7206026315689087,
          "rank": 101
        },
        "2602.12556v1": {
          "score": 0.7201671600341797,
          "rank": 102
        },
        "2602.16435v1": {
          "score": 0.7200520634651184,
          "rank": 103
        },
        "2602.15184v1": {
          "score": 0.7200078964233398,
          "rank": 104
        },
        "2602.10282v1": {
          "score": 0.7197333574295044,
          "rank": 105
        },
        "2602.10480v2": {
          "score": 0.719704806804657,
          "rank": 106
        },
        "2602.09127v2": {
          "score": 0.7194032669067383,
          "rank": 107
        },
        "2602.09748v1": {
          "score": 0.7193353176116943,
          "rank": 108
        },
        "2602.11722v1": {
          "score": 0.7191574573516846,
          "rank": 109
        },
        "2602.15895v1": {
          "score": 0.7191175818443298,
          "rank": 110
        },
        "2602.09303v1": {
          "score": 0.7187135219573975,
          "rank": 111
        },
        "2602.16503v1": {
          "score": 0.7186418771743774,
          "rank": 112
        },
        "2602.14919v1": {
          "score": 0.718533992767334,
          "rank": 113
        },
        "2602.11700v1": {
          "score": 0.7185285091400146,
          "rank": 114
        },
        "2602.14430v1": {
          "score": 0.7184746265411377,
          "rank": 115
        },
        "2602.09988v1": {
          "score": 0.71836256980896,
          "rank": 116
        },
        "2602.14440v1": {
          "score": 0.7182679176330566,
          "rank": 117
        },
        "2602.11097v1": {
          "score": 0.7182440757751465,
          "rank": 118
        },
        "2602.13807v1": {
          "score": 0.7181982398033142,
          "rank": 119
        },
        "2602.09548v1": {
          "score": 0.7181456089019775,
          "rank": 120
        },
        "2602.09443v1": {
          "score": 0.7181435823440552,
          "rank": 121
        },
        "2602.14020v1": {
          "score": 0.7180451154708862,
          "rank": 122
        },
        "2602.14708v1": {
          "score": 0.7180248498916626,
          "rank": 123
        },
        "2602.11794v1": {
          "score": 0.7180091142654419,
          "rank": 124
        },
        "2602.16125v1": {
          "score": 0.7179855108261108,
          "rank": 125
        },
        "2602.09980v1": {
          "score": 0.7176108360290527,
          "rank": 126
        },
        "2602.11690v1": {
          "score": 0.7175790071487427,
          "rank": 127
        },
        "2602.11098v1": {
          "score": 0.7174921035766602,
          "rank": 128
        },
        "2602.14275v1": {
          "score": 0.7174614667892456,
          "rank": 129
        },
        "2602.12203v1": {
          "score": 0.7174603343009949,
          "rank": 130
        },
        "2602.14060v1": {
          "score": 0.7172914147377014,
          "rank": 131
        },
        "2602.12420v1": {
          "score": 0.7172535061836243,
          "rank": 132
        },
        "2602.10419v1": {
          "score": 0.7172058820724487,
          "rank": 133
        },
        "2602.13184v1": {
          "score": 0.7171753644943237,
          "rank": 134
        },
        "2602.10787v1": {
          "score": 0.7170921564102173,
          "rank": 135
        },
        "2602.09351v1": {
          "score": 0.7169796228408813,
          "rank": 136
        },
        "2602.14295v1": {
          "score": 0.7169530391693115,
          "rank": 137
        },
        "2602.16568v1": {
          "score": 0.716920018196106,
          "rank": 138
        },
        "2602.09276v1": {
          "score": 0.7167313098907471,
          "rank": 139
        },
        "2602.10352v1": {
          "score": 0.7166942358016968,
          "rank": 140
        },
        "2602.13695v1": {
          "score": 0.7166846394538879,
          "rank": 141
        },
        "2602.13442v1": {
          "score": 0.7164730429649353,
          "rank": 142
        },
        "2602.13319v1": {
          "score": 0.7164612412452698,
          "rank": 143
        },
        "2602.12674v1": {
          "score": 0.716277003288269,
          "rank": 144
        },
        "2602.09774v1": {
          "score": 0.7162193655967712,
          "rank": 145
        },
        "2602.13174v1": {
          "score": 0.7160377502441406,
          "rank": 146
        },
        "2602.15136v1": {
          "score": 0.7160336971282959,
          "rank": 147
        },
        "2602.12975v1": {
          "score": 0.7160250544548035,
          "rank": 148
        },
        "2602.09708v1": {
          "score": 0.7159719467163086,
          "rank": 149
        },
        "2602.12414v1": {
          "score": 0.7159006595611572,
          "rank": 150
        },
        "2602.14456v1": {
          "score": 0.7158907055854797,
          "rank": 151
        },
        "2602.15150v1": {
          "score": 0.7157906293869019,
          "rank": 152
        },
        "2602.11904v1": {
          "score": 0.7157902121543884,
          "rank": 153
        },
        "2602.13345v1": {
          "score": 0.7157703042030334,
          "rank": 154
        },
        "2602.10271v3": {
          "score": 0.7157641649246216,
          "rank": 155
        },
        "2602.10708v1": {
          "score": 0.7157126665115356,
          "rank": 156
        },
        "2602.15632v1": {
          "score": 0.7156555652618408,
          "rank": 157
        },
        "2602.16113v1": {
          "score": 0.7156505584716797,
          "rank": 158
        },
        "2602.16041v1": {
          "score": 0.7156369686126709,
          "rank": 159
        },
        "2602.11630v1": {
          "score": 0.7156344056129456,
          "rank": 160
        },
        "2602.11747v1": {
          "score": 0.7156112790107727,
          "rank": 161
        },
        "2602.11215v1": {
          "score": 0.7155660390853882,
          "rank": 162
        },
        "2602.12218v1": {
          "score": 0.7154919505119324,
          "rank": 163
        },
        "2602.14573v1": {
          "score": 0.7153934836387634,
          "rank": 164
        },
        "2602.14469v1": {
          "score": 0.7153195142745972,
          "rank": 165
        },
        "2602.12334v1": {
          "score": 0.7151499390602112,
          "rank": 166
        },
        "2602.15283v1": {
          "score": 0.7151132822036743,
          "rank": 167
        },
        "2602.12693v1": {
          "score": 0.7150943279266357,
          "rank": 168
        },
        "2602.12267v1": {
          "score": 0.7150232195854187,
          "rank": 169
        },
        "2602.11229v1": {
          "score": 0.7150169014930725,
          "rank": 170
        },
        "2602.11467v1": {
          "score": 0.7149016261100769,
          "rank": 171
        },
        "2602.11671v1": {
          "score": 0.7148601412773132,
          "rank": 172
        },
        "2602.16167v1": {
          "score": 0.7148599624633789,
          "rank": 173
        },
        "2602.14626v1": {
          "score": 0.7148442268371582,
          "rank": 174
        },
        "2602.15531v1": {
          "score": 0.7148364782333374,
          "rank": 175
        },
        "2602.11639v1": {
          "score": 0.714688777923584,
          "rank": 176
        },
        "2602.14386v1": {
          "score": 0.7146245241165161,
          "rank": 177
        },
        "2602.12966v1": {
          "score": 0.7146178483963013,
          "rank": 178
        },
        "2602.10332v1": {
          "score": 0.7145354747772217,
          "rank": 179
        },
        "2602.14423v1": {
          "score": 0.7145177721977234,
          "rank": 180
        },
        "2602.10018v1": {
          "score": 0.7143059968948364,
          "rank": 181
        },
        "2602.11374v1": {
          "score": 0.714271605014801,
          "rank": 182
        },
        "2602.11578v1": {
          "score": 0.7141956686973572,
          "rank": 183
        },
        "2602.15353v1": {
          "score": 0.7141673564910889,
          "rank": 184
        },
        "2602.11527v1": {
          "score": 0.7141452431678772,
          "rank": 185
        },
        "2602.12471v1": {
          "score": 0.7141078114509583,
          "rank": 186
        },
        "2602.15602v1": {
          "score": 0.7140865325927734,
          "rank": 187
        },
        "2602.15738v1": {
          "score": 0.7140285968780518,
          "rank": 188
        },
        "2602.10885v1": {
          "score": 0.7140064835548401,
          "rank": 189
        },
        "2602.16015v1": {
          "score": 0.713951051235199,
          "rank": 190
        },
        "2602.16490v1": {
          "score": 0.7138893604278564,
          "rank": 191
        },
        "2602.12846v1": {
          "score": 0.7138756513595581,
          "rank": 192
        },
        "2602.10632v1": {
          "score": 0.7137601375579834,
          "rank": 193
        },
        "2602.10457v1": {
          "score": 0.7137594819068909,
          "rank": 194
        },
        "2602.15303v1": {
          "score": 0.7136164903640747,
          "rank": 195
        },
        "2602.13362v1": {
          "score": 0.7136085629463196,
          "rank": 196
        },
        "2602.10420v1": {
          "score": 0.713578999042511,
          "rank": 197
        },
        "2602.13531v1": {
          "score": 0.71345055103302,
          "rank": 198
        },
        "2602.12300v1": {
          "score": 0.7134038209915161,
          "rank": 199
        },
        "2602.14767v1": {
          "score": 0.7133995294570923,
          "rank": 200
        },
        "2602.13567v1": {
          "score": 0.7132225036621094,
          "rank": 201
        },
        "2602.16665v1": {
          "score": 0.7131760716438293,
          "rank": 202
        },
        "2602.14251v1": {
          "score": 0.7131434082984924,
          "rank": 203
        },
        "2602.14480v1": {
          "score": 0.7131240963935852,
          "rank": 204
        },
        "2602.09219v1": {
          "score": 0.7131031155586243,
          "rank": 205
        },
        "2602.10583v1": {
          "score": 0.7130516767501831,
          "rank": 206
        },
        "2602.11388v1": {
          "score": 0.7130508422851562,
          "rank": 207
        },
        "2602.09616v1": {
          "score": 0.7129663228988647,
          "rank": 208
        },
        "2602.15327v1": {
          "score": 0.7129112482070923,
          "rank": 209
        },
        "2602.09181v1": {
          "score": 0.7129037976264954,
          "rank": 210
        },
        "2602.11364v1": {
          "score": 0.7128763198852539,
          "rank": 211
        },
        "2602.14404v1": {
          "score": 0.7128461599349976,
          "rank": 212
        },
        "2602.09821v2": {
          "score": 0.7128323912620544,
          "rank": 213
        },
        "2602.13937v1": {
          "score": 0.7127962708473206,
          "rank": 214
        },
        "2602.10870v1": {
          "score": 0.7127274870872498,
          "rank": 215
        },
        "2602.13871v1": {
          "score": 0.7127217650413513,
          "rank": 216
        },
        "2602.15925v1": {
          "score": 0.7126725316047668,
          "rank": 217
        },
        "2602.10329v1": {
          "score": 0.7125757932662964,
          "rank": 218
        },
        "2602.16530v1": {
          "score": 0.7125727534294128,
          "rank": 219
        },
        "2602.13704v1": {
          "score": 0.7125695943832397,
          "rank": 220
        },
        "2602.14077v1": {
          "score": 0.7124800682067871,
          "rank": 221
        },
        "2602.09764v1": {
          "score": 0.7124422788619995,
          "rank": 222
        },
        "2602.14913v1": {
          "score": 0.7123292684555054,
          "rank": 223
        },
        "2602.11626v1": {
          "score": 0.7123168706893921,
          "rank": 224
        },
        "2602.14285v1": {
          "score": 0.7123059630393982,
          "rank": 225
        },
        "2602.10422v1": {
          "score": 0.7122271060943604,
          "rank": 226
        },
        "2602.10506v1": {
          "score": 0.7121894955635071,
          "rank": 227
        },
        "2602.14039v1": {
          "score": 0.7120847105979919,
          "rank": 228
        },
        "2602.11087v1": {
          "score": 0.7120361924171448,
          "rank": 229
        },
        "2602.14342v1": {
          "score": 0.7119942903518677,
          "rank": 230
        },
        "2602.14024v1": {
          "score": 0.7119513750076294,
          "rank": 231
        },
        "2602.13758v1": {
          "score": 0.7119362354278564,
          "rank": 232
        },
        "2602.13812v2": {
          "score": 0.7119312286376953,
          "rank": 233
        },
        "2602.10867v1": {
          "score": 0.7119297981262207,
          "rank": 234
        },
        "2602.09499v1": {
          "score": 0.711919903755188,
          "rank": 235
        },
        "2602.13194v2": {
          "score": 0.7118787169456482,
          "rank": 236
        },
        "2602.12247v2": {
          "score": 0.7118120789527893,
          "rank": 237
        },
        "2602.14200v1": {
          "score": 0.7117329835891724,
          "rank": 238
        },
        "2602.15297v1": {
          "score": 0.7116925716400146,
          "rank": 239
        },
        "2602.14503v1": {
          "score": 0.7115469574928284,
          "rank": 240
        },
        "2602.15156v1": {
          "score": 0.7115355730056763,
          "rank": 241
        },
        "2602.11149v1": {
          "score": 0.711439847946167,
          "rank": 242
        },
        "2602.10605v1": {
          "score": 0.711316704750061,
          "rank": 243
        },
        "2602.09161v1": {
          "score": 0.7111945748329163,
          "rank": 244
        },
        "2602.10009v1": {
          "score": 0.7111240029335022,
          "rank": 245
        },
        "2602.13517v1": {
          "score": 0.7110382318496704,
          "rank": 246
        },
        "2602.16612v1": {
          "score": 0.7109849452972412,
          "rank": 247
        },
        "2602.10253v1": {
          "score": 0.7109830379486084,
          "rank": 248
        },
        "2602.10441v1": {
          "score": 0.7108846306800842,
          "rank": 249
        },
        "2602.15332v1": {
          "score": 0.7108821272850037,
          "rank": 250
        },
        "2602.12542v1": {
          "score": 0.7107425928115845,
          "rank": 251
        },
        "2602.10476v1": {
          "score": 0.7106654644012451,
          "rank": 252
        },
        "2602.09720v1": {
          "score": 0.7106435298919678,
          "rank": 253
        },
        "2602.16144v1": {
          "score": 0.7105767726898193,
          "rank": 254
        },
        "2602.14367v1": {
          "score": 0.7105568647384644,
          "rank": 255
        },
        "2602.14972v1": {
          "score": 0.7105410099029541,
          "rank": 256
        },
        "2602.13322v1": {
          "score": 0.7105408906936646,
          "rank": 257
        },
        "2602.13934v1": {
          "score": 0.7104763984680176,
          "rank": 258
        },
        "2602.12172v1": {
          "score": 0.7104758024215698,
          "rank": 259
        },
        "2602.16449v1": {
          "score": 0.7104119062423706,
          "rank": 260
        },
        "2602.09285v1": {
          "score": 0.7104096412658691,
          "rank": 261
        },
        "2602.09517v1": {
          "score": 0.7104019522666931,
          "rank": 262
        },
        "2602.15164v1": {
          "score": 0.7103632688522339,
          "rank": 263
        },
        "2602.11581v1": {
          "score": 0.7103012800216675,
          "rank": 264
        },
        "2602.14743v1": {
          "score": 0.7102540135383606,
          "rank": 265
        },
        "2602.12254v1": {
          "score": 0.7101321220397949,
          "rank": 266
        },
        "2602.15514v1": {
          "score": 0.7101132869720459,
          "rank": 267
        },
        "2602.11217v1": {
          "score": 0.7101082801818848,
          "rank": 268
        },
        "2602.12592v1": {
          "score": 0.7100891470909119,
          "rank": 269
        },
        "2602.14869v1": {
          "score": 0.709995448589325,
          "rank": 270
        },
        "2602.12569v1": {
          "score": 0.7099867463111877,
          "rank": 271
        },
        "2602.11414v1": {
          "score": 0.7099855542182922,
          "rank": 272
        },
        "2602.10680v1": {
          "score": 0.7099752426147461,
          "rank": 273
        },
        "2602.14089v1": {
          "score": 0.7097939848899841,
          "rank": 274
        },
        "2602.16554v1": {
          "score": 0.7097594141960144,
          "rank": 275
        },
        "2602.16498v1": {
          "score": 0.709713876247406,
          "rank": 276
        },
        "2602.11320v2": {
          "score": 0.7096821069717407,
          "rank": 277
        },
        "2602.16432v1": {
          "score": 0.7096487283706665,
          "rank": 278
        },
        "2602.09001v1": {
          "score": 0.709646999835968,
          "rank": 279
        },
        "2602.12389v1": {
          "score": 0.7096357941627502,
          "rank": 280
        },
        "2602.13035v1": {
          "score": 0.7096071243286133,
          "rank": 281
        },
        "2602.09613v1": {
          "score": 0.7095061540603638,
          "rank": 282
        },
        "2602.10532v1": {
          "score": 0.709496021270752,
          "rank": 283
        },
        "2602.14335v1": {
          "score": 0.7094230055809021,
          "rank": 284
        },
        "2602.15313v1": {
          "score": 0.7093691825866699,
          "rank": 285
        },
        "2602.11863v1": {
          "score": 0.7093601226806641,
          "rank": 286
        },
        "2602.09196v1": {
          "score": 0.7093600630760193,
          "rank": 287
        },
        "2602.11304v1": {
          "score": 0.709230899810791,
          "rank": 288
        },
        "2602.15488v1": {
          "score": 0.7091817259788513,
          "rank": 289
        },
        "2602.14428v1": {
          "score": 0.7091453671455383,
          "rank": 290
        },
        "2602.14755v1": {
          "score": 0.7091401815414429,
          "rank": 291
        },
        "2602.09805v1": {
          "score": 0.709126889705658,
          "rank": 292
        },
        "2602.11090v1": {
          "score": 0.7090978622436523,
          "rank": 293
        },
        "2602.10489v1": {
          "score": 0.7090802192687988,
          "rank": 294
        },
        "2602.08983v1": {
          "score": 0.709057092666626,
          "rank": 295
        },
        "2602.16427v1": {
          "score": 0.7090528011322021,
          "rank": 296
        },
        "2602.09314v1": {
          "score": 0.709043562412262,
          "rank": 297
        },
        "2602.14272v1": {
          "score": 0.7089612483978271,
          "rank": 298
        },
        "2602.15345v1": {
          "score": 0.7089602947235107,
          "rank": 299
        },
        "2602.11549v1": {
          "score": 0.708928108215332,
          "rank": 300
        },
        "2602.10595v1": {
          "score": 0.7088764309883118,
          "rank": 301
        },
        "2602.10004v1": {
          "score": 0.7087684869766235,
          "rank": 302
        },
        "2602.11052v1": {
          "score": 0.7087096571922302,
          "rank": 303
        },
        "2602.11208v1": {
          "score": 0.708699107170105,
          "rank": 304
        },
        "2602.09723v1": {
          "score": 0.7085990905761719,
          "rank": 305
        },
        "2602.12703v1": {
          "score": 0.7084643244743347,
          "rank": 306
        },
        "2602.11054v1": {
          "score": 0.7083734273910522,
          "rank": 307
        },
        "2602.12234v1": {
          "score": 0.7083685994148254,
          "rank": 308
        },
        "2602.14130v1": {
          "score": 0.7083397507667542,
          "rank": 309
        },
        "2602.15390v1": {
          "score": 0.7081611156463623,
          "rank": 310
        },
        "2602.10017v1": {
          "score": 0.708104133605957,
          "rank": 311
        },
        "2602.14102v1": {
          "score": 0.7080433964729309,
          "rank": 312
        },
        "2602.14011v1": {
          "score": 0.7080061435699463,
          "rank": 313
        },
        "2602.08984v1": {
          "score": 0.7079634070396423,
          "rank": 314
        },
        "2602.13675v1": {
          "score": 0.7079117894172668,
          "rank": 315
        },
        "2602.10464v1": {
          "score": 0.7078885436058044,
          "rank": 316
        },
        "2602.14960v1": {
          "score": 0.7077948451042175,
          "rank": 317
        },
        "2602.11886v1": {
          "score": 0.7077801823616028,
          "rank": 318
        },
        "2602.09278v1": {
          "score": 0.7076870203018188,
          "rank": 319
        },
        "2602.14761v1": {
          "score": 0.7076828479766846,
          "rank": 320
        },
        "2602.14518v1": {
          "score": 0.7076408863067627,
          "rank": 321
        },
        "2602.15592v1": {
          "score": 0.7074626088142395,
          "rank": 322
        },
        "2602.15304v1": {
          "score": 0.7074222564697266,
          "rank": 323
        },
        "2602.14002v1": {
          "score": 0.7074071764945984,
          "rank": 324
        },
        "2602.11570v1": {
          "score": 0.707401692867279,
          "rank": 325
        },
        "2602.13120v1": {
          "score": 0.7073971033096313,
          "rank": 326
        },
        "2602.11443v1": {
          "score": 0.707353949546814,
          "rank": 327
        },
        "2602.09757v1": {
          "score": 0.7072814106941223,
          "rank": 328
        },
        "2602.15266v1": {
          "score": 0.7072315216064453,
          "rank": 329
        },
        "2602.12714v1": {
          "score": 0.7072027921676636,
          "rank": 330
        },
        "2602.16610v1": {
          "score": 0.7071597576141357,
          "rank": 331
        },
        "2602.10530v1": {
          "score": 0.7070810794830322,
          "rank": 332
        },
        "2602.15079v1": {
          "score": 0.7069790959358215,
          "rank": 333
        },
        "2602.14674v2": {
          "score": 0.7069727182388306,
          "rank": 334
        },
        "2602.11917v1": {
          "score": 0.7069538235664368,
          "rank": 335
        },
        "2602.11092v1": {
          "score": 0.7069411277770996,
          "rank": 336
        },
        "2602.11954v1": {
          "score": 0.7069208025932312,
          "rank": 337
        },
        "2602.14663v1": {
          "score": 0.7068701982498169,
          "rank": 338
        },
        "2602.15736v1": {
          "score": 0.7068299055099487,
          "rank": 339
        },
        "2602.14641v1": {
          "score": 0.706818163394928,
          "rank": 340
        },
        "2602.15921v1": {
          "score": 0.7068098783493042,
          "rank": 341
        },
        "2602.10965v1": {
          "score": 0.7067991495132446,
          "rank": 342
        },
        "2602.10410v1": {
          "score": 0.7067456245422363,
          "rank": 343
        },
        "2602.09850v1": {
          "score": 0.7067255973815918,
          "rank": 344
        },
        "2602.15920v1": {
          "score": 0.706673264503479,
          "rank": 345
        },
        "2602.11760v1": {
          "score": 0.7066450119018555,
          "rank": 346
        },
        "2602.10602v1": {
          "score": 0.7065212726593018,
          "rank": 347
        },
        "2602.13110v1": {
          "score": 0.7064626812934875,
          "rank": 348
        },
        "2602.11139v1": {
          "score": 0.7064040303230286,
          "rank": 349
        },
        "2602.12123v1": {
          "score": 0.7063804864883423,
          "rank": 350
        },
        "2602.12709v1": {
          "score": 0.7063621282577515,
          "rank": 351
        },
        "2602.11106v2": {
          "score": 0.7063583135604858,
          "rank": 352
        },
        "2602.14642v1": {
          "score": 0.7063373327255249,
          "rank": 353
        },
        "2602.09791v1": {
          "score": 0.706290602684021,
          "rank": 354
        },
        "2602.09794v1": {
          "score": 0.706275463104248,
          "rank": 355
        },
        "2602.11289v1": {
          "score": 0.7062399387359619,
          "rank": 356
        },
        "2602.12112v1": {
          "score": 0.7062375545501709,
          "rank": 357
        },
        "2602.15472v1": {
          "score": 0.7061774730682373,
          "rank": 358
        },
        "2602.11361v1": {
          "score": 0.7061720490455627,
          "rank": 359
        },
        "2602.14677v1": {
          "score": 0.7061485648155212,
          "rank": 360
        },
        "2602.11685v1": {
          "score": 0.706108570098877,
          "rank": 361
        },
        "2602.15820v1": {
          "score": 0.7061002254486084,
          "rank": 362
        },
        "2602.11015v1": {
          "score": 0.706048309803009,
          "rank": 363
        },
        "2602.13550v1": {
          "score": 0.7059831023216248,
          "rank": 364
        },
        "2602.12350v1": {
          "score": 0.7059804201126099,
          "rank": 365
        },
        "2602.13421v1": {
          "score": 0.7059661149978638,
          "rank": 366
        },
        "2602.13503v1": {
          "score": 0.7059599161148071,
          "rank": 367
        },
        "2602.09947v1": {
          "score": 0.7059242129325867,
          "rank": 368
        },
        "2602.14231v1": {
          "score": 0.7058817744255066,
          "rank": 369
        },
        "2602.10816v1": {
          "score": 0.7057998776435852,
          "rank": 370
        },
        "2602.09430v1": {
          "score": 0.7057759761810303,
          "rank": 371
        },
        "2602.11642v1": {
          "score": 0.705700159072876,
          "rank": 372
        },
        "2602.16608v1": {
          "score": 0.7056494951248169,
          "rank": 373
        },
        "2602.11022v1": {
          "score": 0.7056346535682678,
          "rank": 374
        },
        "2602.12727v1": {
          "score": 0.7056025266647339,
          "rank": 375
        },
        "2602.16316v1": {
          "score": 0.7055970430374146,
          "rank": 376
        },
        "2602.11114v1": {
          "score": 0.7055585384368896,
          "rank": 377
        },
        "2602.14881v1": {
          "score": 0.7054370045661926,
          "rank": 378
        },
        "2602.13152v1": {
          "score": 0.7054233551025391,
          "rank": 379
        },
        "2602.11136v2": {
          "score": 0.7053391933441162,
          "rank": 380
        },
        "2602.13738v1": {
          "score": 0.7053095102310181,
          "rank": 381
        },
        "2602.13985v1": {
          "score": 0.7053090333938599,
          "rank": 382
        },
        "2602.11557v1": {
          "score": 0.7052919864654541,
          "rank": 383
        },
        "2602.16146v1": {
          "score": 0.7052904367446899,
          "rank": 384
        },
        "2602.15229v1": {
          "score": 0.7052496671676636,
          "rank": 385
        },
        "2602.09170v1": {
          "score": 0.7051229476928711,
          "rank": 386
        },
        "2602.12681v1": {
          "score": 0.7050939202308655,
          "rank": 387
        },
        "2602.14411v1": {
          "score": 0.7050882577896118,
          "rank": 388
        },
        "2602.09316v2": {
          "score": 0.7050747871398926,
          "rank": 389
        },
        "2602.12961v1": {
          "score": 0.7049996852874756,
          "rank": 390
        },
        "2602.10748v1": {
          "score": 0.7049390077590942,
          "rank": 391
        },
        "2602.11325v2": {
          "score": 0.7049360275268555,
          "rank": 392
        },
        "2602.12976v1": {
          "score": 0.7049168944358826,
          "rank": 393
        },
        "2602.13586v1": {
          "score": 0.7048219442367554,
          "rank": 394
        },
        "2602.14855v1": {
          "score": 0.7047780752182007,
          "rank": 395
        },
        "2602.12957v1": {
          "score": 0.7047591209411621,
          "rank": 396
        },
        "2602.11021v1": {
          "score": 0.7047376036643982,
          "rank": 397
        },
        "2602.13024v1": {
          "score": 0.7046210169792175,
          "rank": 398
        },
        "2602.15593v1": {
          "score": 0.7045958042144775,
          "rank": 399
        },
        "2602.09864v1": {
          "score": 0.7045619487762451,
          "rank": 400
        }
      }
    },
    {
      "type": "llm_query",
      "tag": "SR",
      "paper_tag": "query:SR",
      "query_text": "Comparison of genetic programming and neural symbolic regression techniques",
      "sim_scores": {
        "2602.10905v1": {
          "score": 0.7653602361679077,
          "rank": 1
        },
        "2602.13583v1": {
          "score": 0.7623300552368164,
          "rank": 2
        },
        "2602.13864v1": {
          "score": 0.7611014246940613,
          "rank": 3
        },
        "2602.13513v2": {
          "score": 0.7607194185256958,
          "rank": 4
        },
        "2602.11931v1": {
          "score": 0.7568268775939941,
          "rank": 5
        },
        "2602.09317v1": {
          "score": 0.7567911148071289,
          "rank": 6
        },
        "2602.16177v1": {
          "score": 0.7560281753540039,
          "rank": 7
        },
        "2602.14280v1": {
          "score": 0.7557114362716675,
          "rank": 8
        },
        "2602.14701v1": {
          "score": 0.7550256252288818,
          "rank": 9
        },
        "2602.11623v1": {
          "score": 0.7548664808273315,
          "rank": 10
        },
        "2602.15571v1": {
          "score": 0.7542353272438049,
          "rank": 11
        },
        "2602.11841v1": {
          "score": 0.7525143623352051,
          "rank": 12
        },
        "2602.10778v1": {
          "score": 0.7520922422409058,
          "rank": 13
        },
        "2602.11424v1": {
          "score": 0.7520051002502441,
          "rank": 14
        },
        "2602.12851v1": {
          "score": 0.7519835829734802,
          "rank": 15
        },
        "2602.15353v1": {
          "score": 0.7519328594207764,
          "rank": 16
        },
        "2602.13021v2": {
          "score": 0.7508784532546997,
          "rank": 17
        },
        "2602.12569v1": {
          "score": 0.7506498694419861,
          "rank": 18
        },
        "2602.12112v1": {
          "score": 0.7505267858505249,
          "rank": 19
        },
        "2602.15632v1": {
          "score": 0.7497528791427612,
          "rank": 20
        },
        "2602.09258v1": {
          "score": 0.7496252059936523,
          "rank": 21
        },
        "2602.16012v1": {
          "score": 0.7495805025100708,
          "rank": 22
        },
        "2602.10233v1": {
          "score": 0.7491327524185181,
          "rank": 23
        },
        "2602.13575v1": {
          "score": 0.7489054203033447,
          "rank": 24
        },
        "2602.09509v2": {
          "score": 0.7488366961479187,
          "rank": 25
        },
        "2602.12267v1": {
          "score": 0.7487862706184387,
          "rank": 26
        },
        "2602.09720v1": {
          "score": 0.7483820915222168,
          "rank": 27
        },
        "2602.13155v1": {
          "score": 0.748310923576355,
          "rank": 28
        },
        "2602.16500v1": {
          "score": 0.7480641007423401,
          "rank": 29
        },
        "2602.09395v1": {
          "score": 0.7475571632385254,
          "rank": 30
        },
        "2602.09851v1": {
          "score": 0.7469514608383179,
          "rank": 31
        },
        "2602.10097v1": {
          "score": 0.7464221715927124,
          "rank": 32
        },
        "2602.14697v1": {
          "score": 0.7463022470474243,
          "rank": 33
        },
        "2602.16436v1": {
          "score": 0.7461283206939697,
          "rank": 34
        },
        "2602.16113v1": {
          "score": 0.7459967136383057,
          "rank": 35
        },
        "2602.14772v1": {
          "score": 0.745327353477478,
          "rank": 36
        },
        "2602.08976v1": {
          "score": 0.7452300190925598,
          "rank": 37
        },
        "2602.13937v1": {
          "score": 0.7452281713485718,
          "rank": 38
        },
        "2602.12471v1": {
          "score": 0.7451317310333252,
          "rank": 39
        },
        "2602.10585v1": {
          "score": 0.7450703382492065,
          "rank": 40
        },
        "2602.14901v1": {
          "score": 0.7448350787162781,
          "rank": 41
        },
        "2602.12613v1": {
          "score": 0.7448068857192993,
          "rank": 42
        },
        "2602.09761v1": {
          "score": 0.7447038888931274,
          "rank": 43
        },
        "2602.13442v1": {
          "score": 0.7443170547485352,
          "rank": 44
        },
        "2602.16429v1": {
          "score": 0.7439801692962646,
          "rank": 45
        },
        "2602.10598v1": {
          "score": 0.743975818157196,
          "rank": 46
        },
        "2602.15593v1": {
          "score": 0.7438175678253174,
          "rank": 47
        },
        "2602.13106v1": {
          "score": 0.7435207366943359,
          "rank": 48
        },
        "2602.16316v1": {
          "score": 0.7434753775596619,
          "rank": 49
        },
        "2602.11346v1": {
          "score": 0.7429180145263672,
          "rank": 50
        },
        "2602.14506v1": {
          "score": 0.7427558898925781,
          "rank": 51
        },
        "2602.10387v1": {
          "score": 0.742597222328186,
          "rank": 52
        },
        "2602.13104v2": {
          "score": 0.7425960898399353,
          "rank": 53
        },
        "2602.11491v1": {
          "score": 0.7425217628479004,
          "rank": 54
        },
        "2602.10854v1": {
          "score": 0.7424598932266235,
          "rank": 55
        },
        "2602.13934v1": {
          "score": 0.7423786520957947,
          "rank": 56
        },
        "2602.09530v1": {
          "score": 0.7423462867736816,
          "rank": 57
        },
        "2602.10794v1": {
          "score": 0.7422589659690857,
          "rank": 58
        },
        "2602.11690v1": {
          "score": 0.7420024871826172,
          "rank": 59
        },
        "2602.13128v1": {
          "score": 0.7416332364082336,
          "rank": 60
        },
        "2602.15552v1": {
          "score": 0.7415363788604736,
          "rank": 61
        },
        "2602.12872v1": {
          "score": 0.7408490180969238,
          "rank": 62
        },
        "2602.11899v1": {
          "score": 0.7405900359153748,
          "rank": 63
        },
        "2602.15725v1": {
          "score": 0.7403548359870911,
          "rank": 64
        },
        "2602.09996v2": {
          "score": 0.7402714490890503,
          "rank": 65
        },
        "2602.13098v1": {
          "score": 0.7400826215744019,
          "rank": 66
        },
        "2602.10480v2": {
          "score": 0.739967942237854,
          "rank": 67
        },
        "2602.12143v1": {
          "score": 0.7399553060531616,
          "rank": 68
        },
        "2602.12753v1": {
          "score": 0.7398877143859863,
          "rank": 69
        },
        "2602.10603v2": {
          "score": 0.7395869493484497,
          "rank": 70
        },
        "2602.09314v1": {
          "score": 0.739575982093811,
          "rank": 71
        },
        "2602.10171v1": {
          "score": 0.7395114898681641,
          "rank": 72
        },
        "2602.13746v1": {
          "score": 0.7394441366195679,
          "rank": 73
        },
        "2602.12390v1": {
          "score": 0.7390267848968506,
          "rank": 74
        },
        "2602.10037v1": {
          "score": 0.7390230894088745,
          "rank": 75
        },
        "2602.11698v1": {
          "score": 0.7387993335723877,
          "rank": 76
        },
        "2602.09448v1": {
          "score": 0.7379170656204224,
          "rank": 77
        },
        "2602.13348v1": {
          "score": 0.7378998398780823,
          "rank": 78
        },
        "2602.11320v2": {
          "score": 0.7378058433532715,
          "rank": 79
        },
        "2602.10496v2": {
          "score": 0.7376534342765808,
          "rank": 80
        },
        "2602.12499v1": {
          "score": 0.7372861504554749,
          "rank": 81
        },
        "2602.10457v1": {
          "score": 0.7371808290481567,
          "rank": 82
        },
        "2602.16435v1": {
          "score": 0.7370387315750122,
          "rank": 83
        },
        "2602.10680v1": {
          "score": 0.736808717250824,
          "rank": 84
        },
        "2602.15473v1": {
          "score": 0.7361499071121216,
          "rank": 85
        },
        "2602.14890v1": {
          "score": 0.736038327217102,
          "rank": 86
        },
        "2602.10352v1": {
          "score": 0.7359392642974854,
          "rank": 87
        },
        "2602.14295v1": {
          "score": 0.7357255816459656,
          "rank": 88
        },
        "2602.15738v1": {
          "score": 0.7356477975845337,
          "rank": 89
        },
        "2602.16005v1": {
          "score": 0.7356003522872925,
          "rank": 90
        },
        "2602.14737v1": {
          "score": 0.7354815602302551,
          "rank": 91
        },
        "2602.11863v1": {
          "score": 0.735457181930542,
          "rank": 92
        },
        "2602.09613v1": {
          "score": 0.7353817224502563,
          "rank": 93
        },
        "2602.12300v1": {
          "score": 0.735281229019165,
          "rank": 94
        },
        "2602.14440v1": {
          "score": 0.7351232767105103,
          "rank": 95
        },
        "2602.13704v1": {
          "score": 0.7349494695663452,
          "rank": 96
        },
        "2602.16530v1": {
          "score": 0.7347756624221802,
          "rank": 97
        },
        "2602.12391v1": {
          "score": 0.7346794605255127,
          "rank": 98
        },
        "2602.09572v2": {
          "score": 0.7345703840255737,
          "rank": 99
        },
        "2602.14054v1": {
          "score": 0.7344543933868408,
          "rank": 100
        },
        "2602.11374v1": {
          "score": 0.7343888282775879,
          "rank": 101
        },
        "2602.13413v1": {
          "score": 0.7343772649765015,
          "rank": 102
        },
        "2602.14919v1": {
          "score": 0.7343075275421143,
          "rank": 103
        },
        "2602.11089v1": {
          "score": 0.7339755296707153,
          "rank": 104
        },
        "2602.11087v1": {
          "score": 0.7339203357696533,
          "rank": 105
        },
        "2602.16038v1": {
          "score": 0.7337374091148376,
          "rank": 106
        },
        "2602.11549v1": {
          "score": 0.7336808443069458,
          "rank": 107
        },
        "2602.11149v1": {
          "score": 0.7335145473480225,
          "rank": 108
        },
        "2602.13419v1": {
          "score": 0.7334965467453003,
          "rank": 109
        },
        "2602.11557v1": {
          "score": 0.7334394454956055,
          "rank": 110
        },
        "2602.11322v1": {
          "score": 0.7334209084510803,
          "rank": 111
        },
        "2602.10489v1": {
          "score": 0.7334014177322388,
          "rank": 112
        },
        "2602.14814v1": {
          "score": 0.7333251237869263,
          "rank": 113
        },
        "2602.14386v1": {
          "score": 0.7332947254180908,
          "rank": 114
        },
        "2602.15206v1": {
          "score": 0.733264148235321,
          "rank": 115
        },
        "2602.14011v1": {
          "score": 0.733211874961853,
          "rank": 116
        },
        "2602.10357v1": {
          "score": 0.7331016063690186,
          "rank": 117
        },
        "2602.12846v1": {
          "score": 0.7330909967422485,
          "rank": 118
        },
        "2602.14231v1": {
          "score": 0.7329208850860596,
          "rank": 119
        },
        "2602.11451v1": {
          "score": 0.7328966856002808,
          "rank": 120
        },
        "2602.09719v1": {
          "score": 0.7328872680664062,
          "rank": 121
        },
        "2602.16531v1": {
          "score": 0.7328690886497498,
          "rank": 122
        },
        "2602.11114v1": {
          "score": 0.7326638698577881,
          "rank": 123
        },
        "2602.11812v1": {
          "score": 0.7323763370513916,
          "rank": 124
        },
        "2602.12389v1": {
          "score": 0.7322648763656616,
          "rank": 125
        },
        "2602.09182v1": {
          "score": 0.7322437763214111,
          "rank": 126
        },
        "2602.12962v1": {
          "score": 0.7321711778640747,
          "rank": 127
        },
        "2602.15521v1": {
          "score": 0.732075572013855,
          "rank": 128
        },
        "2602.13769v1": {
          "score": 0.7318973541259766,
          "rank": 129
        },
        "2602.15820v1": {
          "score": 0.7318902015686035,
          "rank": 130
        },
        "2602.14423v1": {
          "score": 0.7318850755691528,
          "rank": 131
        },
        "2602.11000v1": {
          "score": 0.7318099737167358,
          "rank": 132
        },
        "2602.12078v1": {
          "score": 0.7317926287651062,
          "rank": 133
        },
        "2602.09001v1": {
          "score": 0.7315213680267334,
          "rank": 134
        },
        "2602.10891v1": {
          "score": 0.731461226940155,
          "rank": 135
        },
        "2602.11937v1": {
          "score": 0.7311570644378662,
          "rank": 136
        },
        "2602.13069v1": {
          "score": 0.7310306429862976,
          "rank": 137
        },
        "2602.09548v1": {
          "score": 0.7309927940368652,
          "rank": 138
        },
        "2602.13791v1": {
          "score": 0.730973482131958,
          "rank": 139
        },
        "2602.15596v1": {
          "score": 0.7308279275894165,
          "rank": 140
        },
        "2602.13367v1": {
          "score": 0.73081374168396,
          "rank": 141
        },
        "2602.10204v1": {
          "score": 0.7307586669921875,
          "rank": 142
        },
        "2602.10300v1": {
          "score": 0.7307497262954712,
          "rank": 143
        },
        "2602.10450v1": {
          "score": 0.7306981682777405,
          "rank": 144
        },
        "2602.11295v1": {
          "score": 0.7306571006774902,
          "rank": 145
        },
        "2602.11700v1": {
          "score": 0.7306455373764038,
          "rank": 146
        },
        "2602.16608v1": {
          "score": 0.7305934429168701,
          "rank": 147
        },
        "2602.10006v1": {
          "score": 0.730575442314148,
          "rank": 148
        },
        "2602.09924v1": {
          "score": 0.7305290102958679,
          "rank": 149
        },
        "2602.09169v1": {
          "score": 0.730495810508728,
          "rank": 150
        },
        "2602.15164v1": {
          "score": 0.7304660081863403,
          "rank": 151
        },
        "2602.11789v1": {
          "score": 0.7304346561431885,
          "rank": 152
        },
        "2602.09489v1": {
          "score": 0.7302128076553345,
          "rank": 153
        },
        "2602.11139v1": {
          "score": 0.7301729321479797,
          "rank": 154
        },
        "2602.11498v1": {
          "score": 0.7300867438316345,
          "rank": 155
        },
        "2602.11584v1": {
          "score": 0.7300418615341187,
          "rank": 156
        },
        "2602.09901v1": {
          "score": 0.7300125956535339,
          "rank": 157
        },
        "2602.11090v1": {
          "score": 0.7299778461456299,
          "rank": 158
        },
        "2602.13550v1": {
          "score": 0.7298706769943237,
          "rank": 159
        },
        "2602.16204v1": {
          "score": 0.7297753095626831,
          "rank": 160
        },
        "2602.15564v1": {
          "score": 0.7297118306159973,
          "rank": 161
        },
        "2602.13485v1": {
          "score": 0.7296223640441895,
          "rank": 162
        },
        "2602.10004v1": {
          "score": 0.7295895218849182,
          "rank": 163
        },
        "2602.11516v1": {
          "score": 0.7295572757720947,
          "rank": 164
        },
        "2602.12243v1": {
          "score": 0.7295148968696594,
          "rank": 165
        },
        "2602.11217v1": {
          "score": 0.7294366359710693,
          "rank": 166
        },
        "2602.11220v1": {
          "score": 0.7293713092803955,
          "rank": 167
        },
        "2602.10754v1": {
          "score": 0.7291066646575928,
          "rank": 168
        },
        "2602.16065v1": {
          "score": 0.7290964126586914,
          "rank": 169
        },
        "2602.12612v1": {
          "score": 0.7290884852409363,
          "rank": 170
        },
        "2602.15983v1": {
          "score": 0.7289381623268127,
          "rank": 171
        },
        "2602.12222v1": {
          "score": 0.7288105487823486,
          "rank": 172
        },
        "2602.15423v1": {
          "score": 0.7284446954727173,
          "rank": 173
        },
        "2602.14717v1": {
          "score": 0.7284218072891235,
          "rank": 174
        },
        "2602.09116v2": {
          "score": 0.7283124327659607,
          "rank": 175
        },
        "2602.10217v1": {
          "score": 0.7282776832580566,
          "rank": 176
        },
        "2602.10388v2": {
          "score": 0.7282739877700806,
          "rank": 177
        },
        "2602.09402v1": {
          "score": 0.7282396554946899,
          "rank": 178
        },
        "2602.12049v1": {
          "score": 0.7281062602996826,
          "rank": 179
        },
        "2602.15510v1": {
          "score": 0.7280539274215698,
          "rank": 180
        },
        "2602.12356v1": {
          "score": 0.7279837727546692,
          "rank": 181
        },
        "2602.09914v1": {
          "score": 0.7279439568519592,
          "rank": 182
        },
        "2602.10332v1": {
          "score": 0.7277646064758301,
          "rank": 183
        },
        "2602.10437v2": {
          "score": 0.7277344465255737,
          "rank": 184
        },
        "2602.13536v1": {
          "score": 0.727705717086792,
          "rank": 185
        },
        "2602.16704v1": {
          "score": 0.7275410890579224,
          "rank": 186
        },
        "2602.09538v1": {
          "score": 0.727500319480896,
          "rank": 187
        },
        "2602.10031v1": {
          "score": 0.7273638248443604,
          "rank": 188
        },
        "2602.12164v1": {
          "score": 0.7272862195968628,
          "rank": 189
        },
        "2602.10420v1": {
          "score": 0.7272360324859619,
          "rank": 190
        },
        "2602.12642v1": {
          "score": 0.7271803617477417,
          "rank": 191
        },
        "2602.09953v1": {
          "score": 0.7271378040313721,
          "rank": 192
        },
        "2602.14293v1": {
          "score": 0.7269134521484375,
          "rank": 193
        },
        "2602.15676v1": {
          "score": 0.726876974105835,
          "rank": 194
        },
        "2602.12276v1": {
          "score": 0.7268032431602478,
          "rank": 195
        },
        "2602.09288v1": {
          "score": 0.726792573928833,
          "rank": 196
        },
        "2602.14573v1": {
          "score": 0.7265821695327759,
          "rank": 197
        },
        "2602.09869v1": {
          "score": 0.7265094518661499,
          "rank": 198
        },
        "2602.10085v2": {
          "score": 0.7264372706413269,
          "rank": 199
        },
        "2602.12556v1": {
          "score": 0.7264140248298645,
          "rank": 200
        },
        "2602.10796v2": {
          "score": 0.7263841032981873,
          "rank": 201
        },
        "2602.12016v1": {
          "score": 0.7262868881225586,
          "rank": 202
        },
        "2602.10885v1": {
          "score": 0.7262635827064514,
          "rank": 203
        },
        "2602.09181v1": {
          "score": 0.7261669039726257,
          "rank": 204
        },
        "2602.11411v1": {
          "score": 0.7261622548103333,
          "rank": 205
        },
        "2602.15229v1": {
          "score": 0.7260620594024658,
          "rank": 206
        },
        "2602.12798v1": {
          "score": 0.72605961561203,
          "rank": 207
        },
        "2602.11630v1": {
          "score": 0.7259984016418457,
          "rank": 208
        },
        "2602.10539v1": {
          "score": 0.7259663939476013,
          "rank": 209
        },
        "2602.14301v1": {
          "score": 0.7259407639503479,
          "rank": 210
        },
        "2602.15921v1": {
          "score": 0.7259137630462646,
          "rank": 211
        },
        "2602.09757v1": {
          "score": 0.725898265838623,
          "rank": 212
        },
        "2602.10595v1": {
          "score": 0.7258832454681396,
          "rank": 213
        },
        "2602.14492v2": {
          "score": 0.7258340120315552,
          "rank": 214
        },
        "2602.11881v1": {
          "score": 0.72579026222229,
          "rank": 215
        },
        "2602.10606v2": {
          "score": 0.7257817983627319,
          "rank": 216
        },
        "2602.10522v1": {
          "score": 0.7256757020950317,
          "rank": 217
        },
        "2602.13473v1": {
          "score": 0.7255588173866272,
          "rank": 218
        },
        "2602.11639v1": {
          "score": 0.7255433797836304,
          "rank": 219
        },
        "2602.10607v1": {
          "score": 0.725541353225708,
          "rank": 220
        },
        "2602.13108v1": {
          "score": 0.7255382537841797,
          "rank": 221
        },
        "2602.10464v1": {
          "score": 0.7255021333694458,
          "rank": 222
        },
        "2602.10410v1": {
          "score": 0.7254867553710938,
          "rank": 223
        },
        "2602.13671v1": {
          "score": 0.7254600524902344,
          "rank": 224
        },
        "2602.10506v1": {
          "score": 0.7254292964935303,
          "rank": 225
        },
        "2602.14760v1": {
          "score": 0.725283145904541,
          "rank": 226
        },
        "2602.12123v1": {
          "score": 0.7252655029296875,
          "rank": 227
        },
        "2602.14078v1": {
          "score": 0.7250595688819885,
          "rank": 228
        },
        "2602.14208v1": {
          "score": 0.725028932094574,
          "rank": 229
        },
        "2602.15602v1": {
          "score": 0.7249876260757446,
          "rank": 230
        },
        "2602.13325v1": {
          "score": 0.7249654531478882,
          "rank": 231
        },
        "2602.16037v1": {
          "score": 0.7247787714004517,
          "rank": 232
        },
        "2602.13730v1": {
          "score": 0.7247704267501831,
          "rank": 233
        },
        "2602.12116v1": {
          "score": 0.7247350215911865,
          "rank": 234
        },
        "2602.09813v1": {
          "score": 0.7247291207313538,
          "rank": 235
        },
        "2602.12350v1": {
          "score": 0.7246941328048706,
          "rank": 236
        },
        "2602.14896v1": {
          "score": 0.7246361970901489,
          "rank": 237
        },
        "2602.11523v1": {
          "score": 0.7246167659759521,
          "rank": 238
        },
        "2602.15091v1": {
          "score": 0.7245539426803589,
          "rank": 239
        },
        "2602.14102v1": {
          "score": 0.7245309352874756,
          "rank": 240
        },
        "2602.15964v1": {
          "score": 0.7245017886161804,
          "rank": 241
        },
        "2602.12867v1": {
          "score": 0.7243611216545105,
          "rank": 242
        },
        "2602.11515v1": {
          "score": 0.7242456674575806,
          "rank": 243
        },
        "2602.10699v2": {
          "score": 0.7241697311401367,
          "rank": 244
        },
        "2602.09681v1": {
          "score": 0.7241658568382263,
          "rank": 245
        },
        "2602.14828v1": {
          "score": 0.7241231799125671,
          "rank": 246
        },
        "2602.16503v1": {
          "score": 0.7240948677062988,
          "rank": 247
        },
        "2602.11675v1": {
          "score": 0.7238851189613342,
          "rank": 248
        },
        "2602.14949v1": {
          "score": 0.7238492369651794,
          "rank": 249
        },
        "2602.09621v2": {
          "score": 0.7238256931304932,
          "rank": 250
        },
        "2602.13813v1": {
          "score": 0.7238149642944336,
          "rank": 251
        },
        "2602.10231v1": {
          "score": 0.7237789630889893,
          "rank": 252
        },
        "2602.12305v1": {
          "score": 0.7237391471862793,
          "rank": 253
        },
        "2602.14696v1": {
          "score": 0.7236536741256714,
          "rank": 254
        },
        "2602.14077v1": {
          "score": 0.7235616445541382,
          "rank": 255
        },
        "2602.14251v1": {
          "score": 0.7235578894615173,
          "rank": 256
        },
        "2602.10602v1": {
          "score": 0.7235392332077026,
          "rank": 257
        },
        "2602.15972v1": {
          "score": 0.7235122919082642,
          "rank": 258
        },
        "2602.14759v1": {
          "score": 0.7234952449798584,
          "rank": 259
        },
        "2602.15330v1": {
          "score": 0.7234892845153809,
          "rank": 260
        },
        "2602.10531v2": {
          "score": 0.7234126329421997,
          "rank": 261
        },
        "2602.09555v2": {
          "score": 0.7233154773712158,
          "rank": 262
        },
        "2602.15156v1": {
          "score": 0.7231600880622864,
          "rank": 263
        },
        "2602.13521v2": {
          "score": 0.7231366038322449,
          "rank": 264
        },
        "2602.13169v1": {
          "score": 0.7231193780899048,
          "rank": 265
        },
        "2602.09598v1": {
          "score": 0.7230954766273499,
          "rank": 266
        },
        "2602.10623v1": {
          "score": 0.7230421304702759,
          "rank": 267
        },
        "2602.10048v1": {
          "score": 0.7229966521263123,
          "rank": 268
        },
        "2602.12013v1": {
          "score": 0.7229821085929871,
          "rank": 269
        },
        "2602.14578v1": {
          "score": 0.7229660153388977,
          "rank": 270
        },
        "2602.12158v1": {
          "score": 0.7229481935501099,
          "rank": 271
        },
        "2602.11210v1": {
          "score": 0.722843587398529,
          "rank": 272
        },
        "2602.10253v1": {
          "score": 0.7228091955184937,
          "rank": 273
        },
        "2602.12681v1": {
          "score": 0.722752571105957,
          "rank": 274
        },
        "2602.10838v1": {
          "score": 0.7227181792259216,
          "rank": 275
        },
        "2602.10210v1": {
          "score": 0.7227127552032471,
          "rank": 276
        },
        "2602.13665v1": {
          "score": 0.7227098941802979,
          "rank": 277
        },
        "2602.09783v1": {
          "score": 0.7226641178131104,
          "rank": 278
        },
        "2602.10587v1": {
          "score": 0.7225850820541382,
          "rank": 279
        },
        "2602.13987v1": {
          "score": 0.7225624918937683,
          "rank": 280
        },
        "2602.09394v2": {
          "score": 0.7225573062896729,
          "rank": 281
        },
        "2602.12187v1": {
          "score": 0.7225366830825806,
          "rank": 282
        },
        "2602.15997v1": {
          "score": 0.7225232124328613,
          "rank": 283
        },
        "2602.11550v1": {
          "score": 0.7224506139755249,
          "rank": 284
        },
        "2602.11333v1": {
          "score": 0.7224258184432983,
          "rank": 285
        },
        "2602.11629v1": {
          "score": 0.722415030002594,
          "rank": 286
        },
        "2602.15246v1": {
          "score": 0.7223848104476929,
          "rank": 287
        },
        "2602.11770v1": {
          "score": 0.7223305702209473,
          "rank": 288
        },
        "2602.10635v1": {
          "score": 0.7222738265991211,
          "rank": 289
        },
        "2602.14029v1": {
          "score": 0.7222648859024048,
          "rank": 290
        },
        "2602.12273v1": {
          "score": 0.7222590446472168,
          "rank": 291
        },
        "2602.09000v1": {
          "score": 0.7222367525100708,
          "rank": 292
        },
        "2602.13073v1": {
          "score": 0.7221819162368774,
          "rank": 293
        },
        "2602.15531v1": {
          "score": 0.7221740484237671,
          "rank": 294
        },
        "2602.16091v1": {
          "score": 0.7221461534500122,
          "rank": 295
        },
        "2602.15365v1": {
          "score": 0.7220650315284729,
          "rank": 296
        },
        "2602.13891v1": {
          "score": 0.7219821810722351,
          "rank": 297
        },
        "2602.11738v1": {
          "score": 0.72190260887146,
          "rank": 298
        },
        "2602.12968v1": {
          "score": 0.7219011783599854,
          "rank": 299
        },
        "2602.11044v1": {
          "score": 0.7218937873840332,
          "rank": 300
        },
        "2602.10911v1": {
          "score": 0.7218891978263855,
          "rank": 301
        },
        "2602.15241v1": {
          "score": 0.7218652963638306,
          "rank": 302
        },
        "2602.11022v1": {
          "score": 0.7218250036239624,
          "rank": 303
        },
        "2602.14470v1": {
          "score": 0.7216558456420898,
          "rank": 304
        },
        "2602.11722v1": {
          "score": 0.7216542959213257,
          "rank": 305
        },
        "2602.10441v1": {
          "score": 0.7216144800186157,
          "rank": 306
        },
        "2602.13040v1": {
          "score": 0.7215640544891357,
          "rank": 307
        },
        "2602.10408v1": {
          "score": 0.7214696407318115,
          "rank": 308
        },
        "2602.11057v1": {
          "score": 0.7214608788490295,
          "rank": 309
        },
        "2602.11920v1": {
          "score": 0.7214239239692688,
          "rank": 310
        },
        "2602.08984v1": {
          "score": 0.7213513851165771,
          "rank": 311
        },
        "2602.10576v1": {
          "score": 0.7213277816772461,
          "rank": 312
        },
        "2602.12674v1": {
          "score": 0.7213107347488403,
          "rank": 313
        },
        "2602.13773v1": {
          "score": 0.7212765216827393,
          "rank": 314
        },
        "2602.16537v1": {
          "score": 0.721240758895874,
          "rank": 315
        },
        "2602.14761v1": {
          "score": 0.7211868762969971,
          "rank": 316
        },
        "2602.11683v1": {
          "score": 0.7211810350418091,
          "rank": 317
        },
        "2602.11209v1": {
          "score": 0.7211796641349792,
          "rank": 318
        },
        "2602.11747v1": {
          "score": 0.7211388349533081,
          "rank": 319
        },
        "2602.09772v1": {
          "score": 0.7211374044418335,
          "rank": 320
        },
        "2602.12889v1": {
          "score": 0.7210859060287476,
          "rank": 321
        },
        "2602.11904v1": {
          "score": 0.7209975123405457,
          "rank": 322
        },
        "2602.11945v1": {
          "score": 0.7209935188293457,
          "rank": 323
        },
        "2602.10226v1": {
          "score": 0.7209721207618713,
          "rank": 324
        },
        "2602.14635v1": {
          "score": 0.7209504842758179,
          "rank": 325
        },
        "2602.14519v1": {
          "score": 0.7207351326942444,
          "rank": 326
        },
        "2602.09190v1": {
          "score": 0.7207103371620178,
          "rank": 327
        },
        "2602.14154v1": {
          "score": 0.7206972241401672,
          "rank": 328
        },
        "2602.13421v1": {
          "score": 0.7206587791442871,
          "rank": 329
        },
        "2602.11800v1": {
          "score": 0.7206112742424011,
          "rank": 330
        },
        "2602.13571v1": {
          "score": 0.7205666303634644,
          "rank": 331
        },
        "2602.12470v1": {
          "score": 0.7205594778060913,
          "rank": 332
        },
        "2602.11719v1": {
          "score": 0.7205336093902588,
          "rank": 333
        },
        "2602.14252v1": {
          "score": 0.7204521894454956,
          "rank": 334
        },
        "2602.12606v1": {
          "score": 0.7204262018203735,
          "rank": 335
        },
        "2602.10238v1": {
          "score": 0.7204013466835022,
          "rank": 336
        },
        "2602.15669v1": {
          "score": 0.7203924655914307,
          "rank": 337
        },
        "2602.14351v1": {
          "score": 0.7203333377838135,
          "rank": 338
        },
        "2602.14111v1": {
          "score": 0.7203043103218079,
          "rank": 339
        },
        "2602.13792v1": {
          "score": 0.7203027009963989,
          "rank": 340
        },
        "2602.10693v1": {
          "score": 0.7202988266944885,
          "rank": 341
        },
        "2602.15089v1": {
          "score": 0.7202820777893066,
          "rank": 342
        },
        "2602.08980v1": {
          "score": 0.7201488018035889,
          "rank": 343
        },
        "2602.13035v1": {
          "score": 0.720138430595398,
          "rank": 344
        },
        "2602.16336v1": {
          "score": 0.720097541809082,
          "rank": 345
        },
        "2602.14728v1": {
          "score": 0.7199385166168213,
          "rank": 346
        },
        "2602.11626v1": {
          "score": 0.7199369072914124,
          "rank": 347
        },
        "2602.11204v1": {
          "score": 0.7199327945709229,
          "rank": 348
        },
        "2602.12976v1": {
          "score": 0.7198909521102905,
          "rank": 349
        },
        "2602.13783v1": {
          "score": 0.7198889255523682,
          "rank": 350
        },
        "2602.14476v1": {
          "score": 0.7198779582977295,
          "rank": 351
        },
        "2602.13075v1": {
          "score": 0.7198779582977295,
          "rank": 352
        },
        "2602.09457v1": {
          "score": 0.7197914123535156,
          "rank": 353
        },
        "2602.11622v1": {
          "score": 0.719761848449707,
          "rank": 354
        },
        "2602.10770v1": {
          "score": 0.719727635383606,
          "rank": 355
        },
        "2602.11666v1": {
          "score": 0.7197205424308777,
          "rank": 356
        },
        "2602.09304v1": {
          "score": 0.7196341753005981,
          "rank": 357
        },
        "2602.10520v2": {
          "score": 0.7195978164672852,
          "rank": 358
        },
        "2602.11594v2": {
          "score": 0.7195640206336975,
          "rank": 359
        },
        "2602.15313v1": {
          "score": 0.7195602655410767,
          "rank": 360
        },
        "2602.14244v1": {
          "score": 0.719555675983429,
          "rank": 361
        },
        "2602.09764v1": {
          "score": 0.7194715142250061,
          "rank": 362
        },
        "2602.16504v1": {
          "score": 0.7194353342056274,
          "rank": 363
        },
        "2602.15360v1": {
          "score": 0.7194116711616516,
          "rank": 364
        },
        "2602.13620v1": {
          "score": 0.7193988561630249,
          "rank": 365
        },
        "2602.15514v1": {
          "score": 0.7193547487258911,
          "rank": 366
        },
        "2602.12540v1": {
          "score": 0.7193454504013062,
          "rank": 367
        },
        "2602.14069v1": {
          "score": 0.7193008661270142,
          "rank": 368
        },
        "2602.12009v1": {
          "score": 0.7192476987838745,
          "rank": 369
        },
        "2602.16490v1": {
          "score": 0.71922767162323,
          "rank": 370
        },
        "2602.12952v1": {
          "score": 0.719226598739624,
          "rank": 371
        },
        "2602.15283v1": {
          "score": 0.7191874980926514,
          "rank": 372
        },
        "2602.15894v1": {
          "score": 0.7191452980041504,
          "rank": 373
        },
        "2602.10816v1": {
          "score": 0.7191018462181091,
          "rank": 374
        },
        "2602.09162v1": {
          "score": 0.7190078496932983,
          "rank": 375
        },
        "2602.13675v1": {
          "score": 0.7190071940422058,
          "rank": 376
        },
        "2602.11229v1": {
          "score": 0.718977689743042,
          "rank": 377
        },
        "2602.12542v1": {
          "score": 0.718961238861084,
          "rank": 378
        },
        "2602.13486v1": {
          "score": 0.7188993692398071,
          "rank": 379
        },
        "2602.14687v1": {
          "score": 0.7188490033149719,
          "rank": 380
        },
        "2602.14729v1": {
          "score": 0.718839704990387,
          "rank": 381
        },
        "2602.15338v1": {
          "score": 0.7187880873680115,
          "rank": 382
        },
        "2602.14607v1": {
          "score": 0.7187129855155945,
          "rank": 383
        },
        "2602.12146v1": {
          "score": 0.7186388969421387,
          "rank": 384
        },
        "2602.14960v1": {
          "score": 0.7186145782470703,
          "rank": 385
        },
        "2602.12342v1": {
          "score": 0.7185596227645874,
          "rank": 386
        },
        "2602.11717v1": {
          "score": 0.7185031771659851,
          "rank": 387
        },
        "2602.11671v1": {
          "score": 0.7184973955154419,
          "rank": 388
        },
        "2602.09517v1": {
          "score": 0.7184749841690063,
          "rank": 389
        },
        "2602.12660v1": {
          "score": 0.7184603214263916,
          "rank": 390
        },
        "2602.15814v1": {
          "score": 0.7184193134307861,
          "rank": 391
        },
        "2602.09351v1": {
          "score": 0.7183291912078857,
          "rank": 392
        },
        "2602.15184v1": {
          "score": 0.7182744145393372,
          "rank": 393
        },
        "2602.11767v1": {
          "score": 0.7182607650756836,
          "rank": 394
        },
        "2602.09620v1": {
          "score": 0.7182267904281616,
          "rank": 395
        },
        "2602.16240v1": {
          "score": 0.7181658148765564,
          "rank": 396
        },
        "2602.15681v1": {
          "score": 0.7180837988853455,
          "rank": 397
        },
        "2602.14869v1": {
          "score": 0.7180670499801636,
          "rank": 398
        },
        "2602.15228v1": {
          "score": 0.7180404663085938,
          "rank": 399
        },
        "2602.13691v1": {
          "score": 0.7179968953132629,
          "rank": 400
        }
      }
    }
  ]
}