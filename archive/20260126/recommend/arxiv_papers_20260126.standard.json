{
  "mode": "standard",
  "generated_at": "2026-01-26T04:11:14.285414+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 2,
    "deep_cap": 8,
    "deep_selected": 2,
    "quick_candidates": 3,
    "quick_skim_target": 13,
    "quick_selected": 3
  },
  "deep_dive": [
    {
      "id": "2601.16849v1",
      "title": "The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics",
      "abstract": "We demonstrate the power of human-LLM collaboration in tackling open problems in theoretical computer science. Focusing on combinatorial optimization, we refine outputs from the FunSearch algorithm [Romera-Paredes et al., Nature 2023] to derive state-of-the-art lower bounds for standard heuristics. Specifically, we target the generation of adversarial instances where these heuristics perform poorly. By iterating on FunSearch's outputs, we identify improved constructions for hierarchical $k$-median clustering, bin packing, the knapsack problem, and a generalization of Lovász's gasoline problem - some of these have not seen much improvement for over a decade, despite intermittent attention. These results illustrate how expert oversight can effectively extrapolate algorithmic insights from LLM-based evolutionary methods to break long-standing barriers.   Our findings demonstrate that while LLMs provide critical initial patterns, human expertise is essential for transforming these patterns into mathematically rigorous and insightful constructions. This work highlights that LLMs are a strong collaborative tool in mathematics and computer science research.",
      "authors": [
        "Henri Nikoleit",
        "Ankit Anand",
        "Anurag Murty Naredla",
        "Heiko Röglin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.DS"
      ],
      "published": "2026-01-23 15:56:31+00:00",
      "link": "https://arxiv.org/pdf/2601.16849v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "evolution of heuristics via FunSearch and LLM",
      "llm_evidence_cn": "通过 FunSearch 和大模型进行启发式算法进化",
      "llm_evidence": "通过 FunSearch 和大模型进行启发式算法进化",
      "llm_tldr_en": "Uses human-AI collaboration and FunSearch to evolve state-of-the-art heuristics for combinatorial optimization.",
      "llm_tldr_cn": "利用人机协作和 FunSearch 进化出组合优化问题的最先进启发式算法。",
      "llm_tldr": "利用人机协作和 FunSearch 进化出组合优化问题的最先进启发式算法。",
      "llm_tags": [
        "keyword:LNS",
        "keyword:EOH",
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.16510v1",
      "title": "Learning to Optimize by Differentiable Programming",
      "abstract": "Solving massive-scale optimization problems requires scalable first-order methods with low per-iteration cost. This tutorial highlights a shift in optimization: using differentiable programming not only to execute algorithms but to learn how to design them. Modern frameworks such as PyTorch, TensorFlow, and JAX enable this paradigm through efficient automatic differentiation. Embedding first-order methods within these systems allows end-to-end training that improves convergence and solution quality. Guided by Fenchel-Rockafellar duality, the tutorial demonstrates how duality-informed iterative schemes such as ADMM and PDHG can be learned and adapted. Case studies across LP, OPF, Laplacian regularization, and neural network verification illustrate these gains.",
      "authors": [
        "Liping Tao",
        "Xindi Tong",
        "Chee Wei Tan"
      ],
      "primary_category": "cs.MS",
      "categories": [
        "cs.MS",
        "cs.LG",
        "math.OC"
      ],
      "published": "2026-01-23 07:18:07+00:00",
      "link": "https://arxiv.org/pdf/2601.16510v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "efficient automatic algorithm design via differentiable programming",
      "llm_evidence_cn": "通过微分编程实现高效自动算法设计",
      "llm_evidence": "通过微分编程实现高效自动算法设计",
      "llm_tldr_en": "Explores learning to design optimization algorithms using automatic differentiation and differentiable programming.",
      "llm_tldr_cn": "探讨利用自动微分和微分编程来学习设计优化算法的新范式。",
      "llm_tldr": "探讨利用自动微分和微分编程来学习设计优化算法的新范式。",
      "llm_tags": [
        "keyword:EAA"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2601.16489v1",
      "title": "EvoConfig: Self-Evolving Multi-Agent Systems for Efficient Autonomous Environment Configuration",
      "abstract": "A reliable executable environment is the foundation for ensuring that large language models solve software engineering tasks. Due to the complex and tedious construction process, large-scale configuration is relatively inefficient. However, most methods always overlook fine-grained analysis of the actions performed by the agent, making it difficult to handle complex errors and resulting in configuration failures. To address this bottleneck, we propose EvoConfig, an efficient environment configuration framework that optimizes multi-agent collaboration to build correct runtime environments. EvoConfig features an expert diagnosis module for fine-grained post-execution analysis, and a self-evolving mechanism that lets expert agents self-feedback and dynamically adjust error-fixing priorities in real time. Empirically, EvoConfig matches the previous state-of-the-art Repo2Run on Repo2Run's 420 repositories, while delivering clear gains on harder cases: on the more challenging Envbench, EvoConfig achieves a 78.1% success rate, outperforming Repo2Run by 7.1%. Beyond end-to-end success, EvoConfig also demonstrates stronger debugging competence, achieving higher accuracy in error identification and producing more effective repair recommendations than existing methods.",
      "authors": [
        "Xinshuai Guo",
        "Jiayi Kuang",
        "Linyue Pan",
        "Yinghui Li",
        "Yangning Li",
        "Hai-Tao Zheng",
        "Ying Shen",
        "Di Yin",
        "Xing Sun"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-23 06:33:01+00:00",
      "link": "https://arxiv.org/pdf/2601.16489v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "self-evolving mechanism for efficient configuration",
      "llm_evidence_cn": "高效配置的自进化机制",
      "llm_evidence": "高效配置的自进化机制",
      "llm_tldr_en": "Introduces EvoConfig, a self-evolving multi-agent system for efficient environment configuration.",
      "llm_tldr_cn": "引入 EvoConfig，一种用于高效环境配置的自进化多智能体系统。",
      "llm_tldr": "引入 EvoConfig，一种用于高效环境配置的自进化多智能体系统。",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16863v1",
      "title": "Mixture-of-Models: Unifying Heterogeneous Agents via N-Way Self-Evaluating Deliberation",
      "abstract": "This paper introduces the N-Way Self-Evaluating Deliberation (NSED) protocol, a Runtime Mixture-of-Models (MoM) architecture that constructs emergent composite models from a plurality of distinct expert agents. Unlike traditional Mixture-of-Experts (MoE) which rely on static gating networks, NSED employs a Dynamic Expertise Broker - a runtime optimization engine that treats model selection as a variation of the Knapsack Problem, binding heterogeneous checkpoints to functional roles based on live telemetry and cost constraints. At the execution layer, we formalize deliberation as a Macro-Scale Recurrent Neural Network (RNN), where the consensus state loops back through a semantic forget gate to enable iterative refinement without proportional VRAM scaling. Key components include an orchestration fabric for trustless N-to-N peer review, a Quadratic Voting activation function for non-linear consensus, and a feedback-driven state update. Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench) demonstrates that this topology allows ensembles of small (less than 20B) consumer-grade models to match or exceed the performance of state-of-the-art 100B+ parameter models, establishing a new hardware arbitrage efficiency frontier. Furthermore, testing on the DarkBench safety suite reveals intrinsic alignment properties, with peer-mediated correction reducing sycophancy scores below that of any individual agent.",
      "authors": [
        "Tims Pecerskis",
        "Aivars Smirnovs"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "eess.SY"
      ],
      "published": "2026-01-23 16:11:54+00:00",
      "link": "https://arxiv.org/pdf/2601.16863v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Dynamic expertise broker treating selection as a Knapsack Problem",
      "llm_evidence_cn": "将模型选择视为背包问题的动态专家经纪人",
      "llm_evidence": "将模型选择视为背包问题的动态专家经纪人",
      "llm_tldr_en": "Introduces a runtime optimization engine for heterogeneous agent selection using a Knapsack Problem formulation.",
      "llm_tldr_cn": "引入了一个运行时优化引擎，通过背包问题建模来选择异构智能体。",
      "llm_tldr": "引入了一个运行时优化引擎，通过背包问题建模来选择异构智能体。",
      "llm_tags": [
        "keyword:EAA",
        "keyword:EOH"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16896v1",
      "title": "How Sequential Algorithm Portfolios can benefit Black Box Optimization",
      "abstract": "In typical black-box optimization applications, the available computational budget is often allocated to a single algorithm, typically chosen based on user preference with limited knowledge about the problem at hand or according to some expert knowledge. However, we show that splitting the budget across several algorithms yield significantly better results. This approach benefits from both algorithm complementarity across diverse problems and variance reduction within individual functions, and shows that algorithm portfolios do NOT require parallel evaluation capabilities. To demonstrate the advantage of sequential algorithm portfolios, we apply it to the COCO data archive, using over 200 algorithms evaluated on the BBOB test suite. The proposed sequential portfolios consistently outperform single-algorithm baselines, achieving relative performance gains of over 14%, and offering new insights into restart mechanisms and potential for warm-started execution strategies.",
      "authors": [
        "Catalin-Viorel Dinu",
        "Diederick Vermetten",
        "Carola Doerr"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-01-23 17:02:22+00:00",
      "link": "https://arxiv.org/pdf/2601.16896v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Focuses on algorithm portfolios and budget allocation for black box optimization",
      "llm_evidence_cn": "关注黑盒优化的算法组合和预算分配",
      "llm_evidence": "关注黑盒优化的算法组合和预算分配",
      "llm_tldr_en": "Shows that sequential algorithm portfolios improve black-box optimization without needing parallel computing.",
      "llm_tldr_cn": "展示了顺序算法组合如何在不需要并行计算的情况下改进黑盒优化。",
      "llm_tldr": "展示了顺序算法组合如何在不需要并行计算的情况下改进黑盒优化。",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "6"
    }
  ]
}