{
  "mode": "standard",
  "generated_at": "2026-02-10T19:46:12.954263+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 1,
    "deep_divecandidates": 4,
    "deep_cap": 6,
    "deep_selected": 4,
    "quick_candidates": 5,
    "quick_skim_target": 11,
    "quick_selected": 5
  },
  "deep_dive": [
    {
      "id": "2602.07834v1",
      "title": "Interpretable Analytic Calabi-Yau Metrics via Symbolic Distillation",
      "abstract": "Calabi--Yau manifolds are essential for string theory but require computing intractable metrics. Here we show that symbolic regression can distill neural approximations into simple, interpretable formulas. Our five-term expression matches neural accuracy ($R^2 = 0.9994$) with 3,000-fold fewer parameters. Multi-seed validation confirms that geometric constraints select essential features, specifically power sums and symmetric polynomials, while permitting structural diversity. The functional form can be maintained across the studied moduli range ($ψ\\in [0, 0.8]$) with coefficients varying smoothly; we interpret these trends as empirical hypotheses within the accuracy regime of the locally-trained teachers ($σ\\approx 8-9\\%$ at $ψ\\neq 0$). The formula reproduces physical observables -- volume integrals and Yukawa couplings -- validating that symbolic distillation recovers compact, interpretable models for quantities previously accessible only to black-box networks.",
      "authors": [
        "D Yang Eng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.DG"
      ],
      "published": "2026-02-08 05:51:35+00:00",
      "link": "https://arxiv.org/pdf/2602.07834v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Symbolic regression for physics and interpretable formulas",
      "llm_evidence_cn": "符号回归用于物理和可解释公式",
      "llm_evidence": "符号回归用于物理和可解释公式",
      "llm_tldr_en": "Uses symbolic regression to distill complex Calabi-Yau metrics into simple, interpretable analytic formulas.",
      "llm_tldr_cn": "利用符号回归将复杂的Calabi-Yau度量蒸馏为简单、可解释的解析公式。",
      "llm_tldr": "利用符号回归将复杂的Calabi-Yau度量蒸馏为简单、可解释的解析公式。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.08270v1",
      "title": "A few-shot and physically restorable symbolic regression turbulence model based on normalized general effective-viscosity hypothesis",
      "abstract": "Turbulence is a complex, irregular flow phenomenon ubiquitous in natural processes and engineering applications. The Reynolds-averaged Navier-Stokes (RANS) method, owing to its low computational cost, has become the primary approach for rapid simulation of engineering turbulence problems. However, the inaccuracy of classical turbulence models constitutes the main drawback of the RANS framework. With the rapid development of data-driven approaches, many data-driven turbulence models have been proposed, yet they still suffer from issues of generalizability and accuracy. In this work, we propose a few-shot, physically restorable, symbolic regression turbulence model based on the normalized general effective-viscosity hypothesis. Few-shot indicates that our model is trained on limited flow configurations spanning only a narrow subset of turbulent flow physics, yet can still outperform the baseline model in substantially different turbulent flows. Physically restorable means our model can nearly revert to the baseline model in regimes satisfying specific physical conditions, using only the symbolic regression training results. The normalized general effective-viscosity hypothesis was proposed in our previous study. Specifically, we first formalize the concept of few-shot data-driven turbulence models. Second, we train our symbolic regression turbulence models using only direct numerical simulation (DNS) data for three-dimensional periodic hill flow slices. Third, we evaluate our models on periodic hill flows, zero pressure gradient flat plate flow, NACA0012 airfoil flows, and NASA Rotor 37 transonic axial compressor flows. One of our symbolic regression turbulence models consistently outperforms the baseline model, and we further demonstrate that this model can nearly revert to baseline behavior in certain flow regimes.",
      "authors": [
        "Ziqi Ji",
        "Penghao Duan",
        "Gang Du"
      ],
      "primary_category": "physics.flu-dyn",
      "categories": [
        "physics.flu-dyn"
      ],
      "published": "2026-02-09 05:08:04+00:00",
      "link": "https://arxiv.org/pdf/2602.08270v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Symbolic regression for physical law extraction in turbulence modeling",
      "llm_evidence_cn": "符号回归在湍流建模中的物理定律提取应用",
      "llm_evidence": "符号回归在湍流建模中的物理定律提取应用",
      "llm_tldr_en": "A few-shot symbolic regression model for turbulence modeling that ensures physical restorability.",
      "llm_tldr_cn": "一种用于湍流建模的少样本符号回归模型，确保了物理可还原性。",
      "llm_tldr": "一种用于湍流建模的少样本符号回归模型，确保了物理可还原性。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.08885v1",
      "title": "Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression",
      "abstract": "Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.",
      "authors": [
        "Paul Saegert",
        "Ullrich Köthe"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SC"
      ],
      "published": "2026-02-09 16:47:00+00:00",
      "link": "https://arxiv.org/pdf/2602.08885v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "amortized neural symbolic regression and expression simplification",
      "llm_evidence_cn": "摊销神经符号回归与表达式简化",
      "llm_evidence": "摊销神经符号回归与表达式简化",
      "llm_tldr_en": "Introduces SimpliPy, a fast rule-based engine to accelerate amortized neural symbolic regression training.",
      "llm_tldr_cn": "引入SimpliPy规则引擎，通过加速表达式简化显著提升摊销神经符号回归的训练速度。",
      "llm_tldr": "引入SimpliPy规则引擎，通过加速表达式简化显著提升摊销神经符号回归的训练速度。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.08733v1",
      "title": "Foundation Inference Models for Ordinary Differential Equations",
      "abstract": "Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.",
      "authors": [
        "Maximilian Mauel",
        "Johannes R. Hübers",
        "David Berghaus",
        "Patrick Seifner",
        "Ramses J. Sanchez"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 14:39:11+00:00",
      "link": "https://arxiv.org/pdf/2602.08733v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "Directly compares to symbolic regression for ODE inference",
      "llm_evidence_cn": "在ODE推断任务中直接与符号回归进行对比",
      "llm_evidence": "在ODE推断任务中直接与符号回归进行对比",
      "llm_tldr_en": "Introduces FIM-ODE, a foundation model for inferring ODE vector fields, outperforming traditional symbolic regression.",
      "llm_tldr_cn": "引入FIM-ODE基础模型用于推断ODE矢量场，在性能上优于传统符号回归方法。",
      "llm_tldr": "引入FIM-ODE基础模型用于推断ODE矢量场，在性能上优于传统符号回归方法。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2602.07709v1",
      "title": "Generative structural elucidation from mass spectra as an iterative optimization problem",
      "abstract": "Liquid chromatography tandem mass spectrometry (LC-MS/MS) is a critical analytical technique for molecular identification across metabolomics, environmental chemistry, and chemical forensics. A variety of computational methods have emerged for structural annotation of spectral features of interest, but many of these features cannot be confidently annotated with reference structures or spectra. Here, we introduce FOAM (Formula-constrained Optimization for Annotating Metabolites), a computational workflow that poses structure elucidation from LC-MS/MS as an iterative optimization problem. FOAM couples a formula-constrained graph genetic algorithm with spectral simulation to explore candidate annotations given an experimental spectrum. We demonstrate FOAM's performance on the NIST'20 and MassSpecGym datasets as both a standalone elucidation pipeline and as a complement to existing inverse models. This work establishes iterative optimization as an effective and extensible paradigm for structural elucidation.",
      "authors": [
        "Mrunali Manjrekar",
        "Runzhong Wang",
        "Samuel Goldman",
        "Jenna C. Fromer",
        "Connor W. Coley"
      ],
      "primary_category": "q-bio.QM",
      "categories": [
        "q-bio.QM",
        "cs.NE"
      ],
      "published": "2026-02-07 21:34:38+00:00",
      "link": "https://arxiv.org/pdf/2602.07709v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Graph genetic algorithm for formula-constrained optimization",
      "llm_evidence_cn": "用于公式约束优化的图遗传算法",
      "llm_evidence": "用于公式约束优化的图遗传算法",
      "llm_tldr_en": "Uses a formula-constrained graph genetic algorithm for molecular structure elucidation from mass spectra.",
      "llm_tldr_cn": "利用公式约束的图遗传算法从质谱中进行分子结构鉴定，属于跨学科实证研究。",
      "llm_tldr": "利用公式约束的图遗传算法从质谱中进行分子结构鉴定，属于跨学科实证研究。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.07800v1",
      "title": "Approximating Matrix Functions with Deep Neural Networks and Transformers",
      "abstract": "Transformers have revolutionized natural language processing, but their use for numerical computation has received less attention. We study the approximation of matrix functions, which map scalar functions to matrices, using neural networks including transformers. We focus on functions mapping square matrices to square matrices of the same dimension. These types of matrix functions appear throughout scientific computing, e.g., the matrix exponential in continuous-time Markov chains and the matrix sign function in stability analysis of dynamical systems. In this paper, we make two contributions. First, we prove bounds on the width and depth of ReLU networks needed to approximate the matrix exponential to an arbitrary precision. Second, we show experimentally that a transformer encoder-decoder with suitable numerical encodings can approximate certain matrix functions at a relative error of 5% with high probability. Our study reveals that the encoding scheme strongly affects performance, with different schemes working better for different functions.",
      "authors": [
        "Rahul Padmanabhan",
        "Simone Brugiapaglia"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.NE",
        "math.NA"
      ],
      "published": "2026-02-08 03:45:25+00:00",
      "link": "https://arxiv.org/pdf/2602.07800v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Transformers for numerical computation and scientific computing",
      "llm_evidence_cn": "用于数值计算和科学计算的 Transformer",
      "llm_evidence": "用于数值计算和科学计算的 Transformer",
      "llm_tldr_en": "Investigates the use of Transformers and neural networks for approximating complex matrix functions.",
      "llm_tldr_cn": "研究了使用 Transformer 和神经网络逼近复杂矩阵函数的方法。",
      "llm_tldr": "研究了使用 Transformer 和神经网络逼近复杂矩阵函数的方法。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.07970v1",
      "title": "Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity",
      "abstract": "Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.",
      "authors": [
        "Zheyuan Hu",
        "Weitao Chen",
        "Cengiz Öztireli",
        "Chenliang Zhou",
        "Fangcheng Zhong"
      ],
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-02-08 13:44:36+00:00",
      "link": "https://arxiv.org/pdf/2602.07970v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Equation discovery and inverse PDE problems",
      "llm_evidence_cn": "方程发现与偏微分方程逆问题",
      "llm_evidence": "方程发现与偏微分方程逆问题",
      "llm_tldr_en": "Explores neural PDE solvers for scientific simulation, including forward solutions and equation discovery.",
      "llm_tldr_cn": "探索用于科学模拟的神经PDE求解器，包括正向求解和方程发现。",
      "llm_tldr": "探索用于科学模拟的神经PDE求解器，包括正向求解和方程发现。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.08857v1",
      "title": "Discovering Interpretable Algorithms by Decompiling Transformers to RASP",
      "abstract": "Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.",
      "authors": [
        "Xinting Huang",
        "Aleksandra Bakalova",
        "Satwik Bhattamishra",
        "William Merrill",
        "Michael Hahn"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-09 16:22:29+00:00",
      "link": "https://arxiv.org/pdf/2602.08857v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Extracting interpretable programs from Transformers",
      "llm_evidence_cn": "从Transformer中提取可解释程序",
      "llm_evidence": "从Transformer中提取可解释程序",
      "llm_tldr_en": "Presents a method to decompile Transformers into interpretable RASP programs for better transparency.",
      "llm_tldr_cn": "提出一种将Transformer反编译为可解释RASP程序的方法以提升透明度。",
      "llm_tldr": "提出一种将Transformer反编译为可解释RASP程序的方法以提升透明度。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.08880v1",
      "title": "Differentiable Logical Programming for Quantum Circuit Discovery and Optimization",
      "abstract": "Designing high-fidelity quantum circuits remains challenging, and current paradigms often depend on heuristic, fixed-ansatz structures or rule-based compilers that can be suboptimal or lack generality. We introduce a neuro-symbolic framework that reframes quantum circuit design as a differentiable logic programming problem. Our model represents a scaffold of potential quantum gates and parameterized operations as a set of learnable, continuous ``truth values'' or ``switches,'' $s \\in [0, 1]^N$. These switches are optimized via standard gradient descent to satisfy a user-defined set of differentiable, logical axioms (e.g., correctness, simplicity, robustness). We provide a theoretical formulation bridging continuous logic (via T-norms) and unitary evolution (via geodesic interpolation), while addressing the barren plateau problem through biased initialization. We illustrate the approach on tasks including discovery of a 4-qubit Quantum Fourier Transform (QFT) from a scaffold of 21 candidate gates. We also report a hardware-aware adaptation experiment on the 133-qubit IBM Torino processor, where the method improved fidelity by 59.3 percentage points in a localized routing task while adapting to hardware failures.",
      "authors": [
        "Antonin Sulc"
      ],
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "published": "2026-02-09 16:40:19+00:00",
      "link": "https://arxiv.org/pdf/2602.08880v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Neuro-symbolic framework for circuit discovery using differentiable logic",
      "llm_evidence_cn": "用于电路发现的神经符号框架，采用可微逻辑",
      "llm_evidence": "用于电路发现的神经符号框架，采用可微逻辑",
      "llm_tldr_en": "A neuro-symbolic framework that optimizes quantum circuits via differentiable logic programming and gradient descent.",
      "llm_tldr_cn": "一种通过可微逻辑编程和梯度下降优化量子电路的神经符号框架。",
      "llm_tldr": "一种通过可微逻辑编程和梯度下降优化量子电路的神经符号框架。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    }
  ]
}