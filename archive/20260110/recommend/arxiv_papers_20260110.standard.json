{
  "mode": "standard",
  "generated_at": "2026-01-10T21:41:46.947814+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 5,
    "deep_dive_selected": 10,
    "deep_dive_cap": 10,
    "deep_dive_divecandidates": 52,
    "quick_skim_selected": 15,
    "quick_skim_target": 15,
    "quick_skim_candidates": 133
  },
  "deep_dive": [
    {
      "id": "2601.03723v1",
      "title": "ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an important paradigm for unlocking reasoning capabilities in large language models, exemplified by the success of OpenAI o1 and DeepSeek-R1. Currently, Group Relative Policy Optimization (GRPO) stands as the dominant algorithm in this domain due to its stable training and critic-free efficiency. However, we argue that GRPO suffers from a structural limitation: it imposes a uniform, static trust region constraint across all samples. This design implicitly assumes signal homogeneity, a premise misaligned with the heterogeneous nature of outcome-driven learning, where advantage magnitudes and variances fluctuate significantly. Consequently, static constraints fail to fully exploit high-quality signals while insufficiently suppressing noise, often precipitating rapid entropy collapse. To address this, we propose \\textbf{E}lastic \\textbf{T}rust \\textbf{R}egions (\\textbf{ETR}), a dynamic mechanism that aligns optimization constraints with signal quality. ETR constructs a signal-aware landscape through dual-level elasticity: at the micro level, it scales clipping boundaries based on advantage magnitude to accelerate learning from high-confidence paths; at the macro level, it leverages group variance to implicitly allocate larger update budgets to tasks in the optimal learning zone. Extensive experiments on AIME and MATH benchmarks demonstrate that ETR consistently outperforms GRPO, achieving superior accuracy while effectively mitigating policy entropy degradation to ensure sustained exploration.",
      "authors": [
        "Shijie Zhang",
        "Kevin Zhang",
        "Zheyuan Gu",
        "Xiang Guo",
        "Rujun Guo",
        "Shaoyu Liu",
        "Guanjun Jiang",
        "Xiaozhao Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-07 09:19:53+00:00",
      "link": "https://arxiv.org/pdf/2601.03723v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 10.0,
      "llm_evidence": "Discusses RLVR and GRPO algorithms used in OpenAI o1 and DeepSeek-R1",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.03590v1",
      "title": "Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions",
      "abstract": "Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant \"spatial gap\" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .",
      "authors": [
        "Zhongbin Guo",
        "Zhen Yang",
        "Yushan Li",
        "Xinyue Zhang",
        "Wenyu Gao",
        "Jiacheng Wang",
        "Chengzhi Li",
        "Xiangrui Liu",
        "Ping Jian"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-01-07 05:13:52+00:00",
      "link": "https://arxiv.org/pdf/2601.03590v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Introduces a benchmark for LLM spatial intelligence and symbolic reasoning",
      "llm_tags": [
        "query:sr-bench",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.04051v1",
      "title": "Symbolic Regression for Shared Expressions: Introducing Partial Parameter Sharing",
      "abstract": "Symbolic Regression aims to find symbolic expressions that describe datasets. Due to better interpretability, it is a machine learning paradigm particularly powerful for scientific discovery. In recent years, several works have expanded the concept to allow the description of similar phenomena using a single expression with varying sets of parameters, thereby introducing categorical variables. Some previous works allow only \"non-shared\" (category-value-specific) parameters, and others also incorporate \"shared\" (category-value-agnostic) parameters. We expand upon those efforts by considering multiple categorical variables, and introducing intermediate levels of parameter sharing. With two categorical variables, an intermediate level of parameter sharing emerges, i.e., parameters which are shared across either category but change across the other. The new approach potentially decreases the number of parameters, while revealing additional information about the problem. Using a synthetic, fitting-only example, we test the limits of this setup in terms of data requirement reduction and transfer learning. As a real-world symbolic regression example, we demonstrate the benefits of the proposed approach on an astrophysics dataset used in a previous study, which considered only one categorical variable. We achieve a similar fit quality but require significantly fewer individual parameters, and extract additional information about the problem.",
      "authors": [
        "Viktor Martinek",
        "Roland Herzog"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-07 16:12:14+00:00",
      "link": "https://arxiv.org/pdf/2601.04051v1",
      "tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Advances in symbolic regression methodology for scientific discovery",
      "llm_tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ]
    },
    {
      "id": "2601.04365v1",
      "title": "Survival Dynamics of Neural and Programmatic Policies in Evolutionary Reinforcement Learning",
      "abstract": "In evolutionary reinforcement learning tasks (ERL), agent policies are often encoded as small artificial neural networks (NERL). Such representations lack explicit modular structure, limiting behavioral interpretation. We investigate whether programmatic policies (PERL), implemented as soft, differentiable decision lists (SDDL), can match the performance of NERL. To support reproducible evaluation, we provide the first fully specified and open-source reimplementation of the classic 1992 Artificial Life (ALife) ERL testbed. We conduct a rigorous survival analysis across 4000 independent trials utilizing Kaplan-Meier curves and Restricted Mean Survival Time (RMST) metrics absent in the original study. We find a statistically significant difference in survival probability between PERL and NERL. PERL agents survive on average 201.69 steps longer than NERL agents. Moreover, SDDL agents using learning alone (no evolution) survive on average 73.67 steps longer than neural agents using both learning and evaluation. These results demonstrate that programmatic policies can exceed the survival performance of neural policies in ALife.",
      "authors": [
        "Anton Roupassov-Ruiz",
        "Yiyang Zuo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-07 20:09:28+00:00",
      "link": "https://arxiv.org/pdf/2601.04365v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Compares neural and programmatic policies in evolutionary reinforcement learning",
      "llm_tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ]
    },
    {
      "id": "2601.03661v1",
      "title": "AMIR-GRPO: Inducing Implicit Preference Signals into GRPO",
      "abstract": "Reinforcement learning has become the primary paradigm for aligning large language models (LLMs) on complex reasoning tasks, with group relative policy optimization (GRPO) widely used in large-scale post-training. However, GRPO faces structural limitations in reasoning-heavy settings: sequence-level advantage normalization introduces systematic length bias, penalties for low-quality trajectories are diluted, and the scalar objective discards rich pairwise preference information embedded in within-group reward rankings. As a result, valuable supervision from costly rollouts remains underutilized.   We propose AMIR-GRPO, which augments GRPO with an implicit DPO-style contrastive regularizer constructed directly from intra-group reward rankings, requiring no additional annotations. This mechanism amplifies suppression of low-reward trajectories, attenuates response-level length bias, and transforms each rollout group into a denser set of supervision constraints. Across multiple mathematical reasoning benchmarks, AMIR-GRPO consistently outperforms strong GRPO baselines, yields clearer separation between correct and incorrect reasoning chains, and delivers broader coverage gains beyond the subset of instances solved by standard GRPO.",
      "authors": [
        "Amir Hossein Yari",
        "Fajri Koto"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-07 07:22:58+00:00",
      "link": "https://arxiv.org/pdf/2601.03661v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Reinforcement learning alignment for large language models using GRPO",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.03699v1",
      "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
      "abstract": "As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against adversarial prompts is paramount. However, existing red teaming datasets suffer from inconsistent risk categorizations, limited domain coverage, and outdated evaluations, hindering systematic vulnerability assessments. To address these challenges, we introduce RedBench, a universal dataset aggregating 37 benchmark datasets from leading conferences and repositories, comprising 29,362 samples across attack and refusal prompts. RedBench employs a standardized taxonomy with 22 risk categories and 19 domains, enabling consistent and comprehensive evaluations of LLM vulnerabilities. We provide a detailed analysis of existing datasets, establish baselines for modern LLMs, and open-source the dataset and evaluation code. Our contributions facilitate robust comparisons, foster future research, and promote the development of secure and reliable LLMs for real-world deployment. Code: https://github.com/knoveleng/redeval",
      "authors": [
        "Quy-Anh Dang",
        "Chris Ngo",
        "Truong-Son Hy"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-07 08:34:17+00:00",
      "link": "https://arxiv.org/pdf/2601.03699v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Universal dataset for comprehensive red teaming and evaluation of LLMs",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.04876v1",
      "title": "ChronosAudio: A Comprehensive Long-Audio Benchmark for Evaluating Audio-Large Language Models",
      "abstract": "Although Audio Large Language Models (ALLMs) have witnessed substantial advancements, their long audio understanding capabilities remain unexplored. A plethora of benchmarks have been proposed for general audio tasks, they predominantly focus on short-form clips, leaving without a consensus on evaluating ALLMs over extended durations. This paper proposes ChronosAudio, the first multi-task benchmark tailored for long-audio understanding in ALLMs. It encompasses six major task categories and comprises 36,000 test instances totaling over 200 hours audio, stratified into short, middle, and long-form categories to comprehensively evaluate length generalization. Extensive experiments on 16 state-of-the-art models using ChronosAudio yield three critical findings: 1.Precipitous Long-Context Collapse: ALLMs exhibit a severe inability to sustain performance, with the transition from short to long contexts triggering a staggering performance degradation of over 90% in specific tasks. 2.Structural Attention Dilution: Performance degradation stems from a fundamental failure in maintaining temporal locality; attention mechanisms suffer from significant diffusion in later sequences. 3.Restorative Ceiling of Mitigation: Current strategies only offer 50% recovery. These findings reveal significant challenges in long-audio, underscoring the urgent need for approaches to achieve robust, document-level audio reasoning.",
      "authors": [
        "Kaiwen Luo",
        "Liang Lin",
        "Yibo Zhang",
        "Moayad Aloqaily",
        "Dexian Wang",
        "Zhenhong Zhou",
        "Junwei Zhang",
        "Kun Wang",
        "Li Sun",
        "Qingsong Wen"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD"
      ],
      "published": "2026-01-08 12:21:09+00:00",
      "link": "https://arxiv.org/pdf/2601.04876v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Describes a comprehensive evaluation benchmark for Audio-Large Language Models",
      "llm_tags": [
        "query:sr-bench",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.03895v1",
      "title": "Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training",
      "abstract": "Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model's exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility https://github.com/chi2liu/ABC-GRPO.",
      "authors": [
        "Chi Liu",
        "Xin Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-07 13:04:52+00:00",
      "link": "https://arxiv.org/pdf/2601.03895v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Refines GRPO reinforcement learning for LLM training stability",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.03986v1",
      "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
      "abstract": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.",
      "authors": [
        "Qi Qian",
        "Chengsong Huang",
        "Jingwen Xu",
        "Changze Lv",
        "Muling Wu",
        "Wenhao Liu",
        "Xiaohua Wang",
        "Zhenghua Wang",
        "Zisu Huang",
        "Muzhao Tian",
        "Jianhan Xu",
        "Kun Hu",
        "He-Da Wang",
        "Yao Hu",
        "Xuanjing Huang",
        "Xiaoqing Zheng"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-07 14:59:03+00:00",
      "link": "https://arxiv.org/pdf/2601.03986v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "presents a framework for systematic evaluation of LLM benchmarks across multiple domains",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.04411v1",
      "title": "Rate or Fate? RLV$^\\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards",
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)?   To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden's index J=TPR-FPR. This yields a sharp phase transition: when J>0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J<0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J>0, noise primarily rescales convergence time (\"rate, not fate\"). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions.",
      "authors": [
        "Ali Rad",
        "Khashayar Filom",
        "Darioush Keivan",
        "Peyman Mohajerin Esfahani",
        "Ehsan Kamalinejad"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-07 21:31:26+00:00",
      "link": "https://arxiv.org/pdf/2601.04411v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Reinforcement learning with verifiable rewards for training LLMs",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2601.04260v1",
      "title": "Towards a Mechanistic Understanding of Propositional Logical Reasoning in Large Language Models",
      "abstract": "Understanding how Large Language Models (LLMs) perform logical reasoning internally remains a fundamental challenge. While prior mechanistic studies focus on identifying taskspecific circuits, they leave open the question of what computational strategies LLMs employ for propositional reasoning. We address this gap through comprehensive analysis of Qwen3 (8B and 14B) on PropLogic-MI, a controlled dataset spanning 11 propositional logic rule categories across one-hop and two-hop reasoning. Rather than asking ''which components are necessary,'' we ask ''how does the model organize computation?'' Our analysis reveals a coherent computational architecture comprising four interlocking mechanisms: Staged Computation (layer-wise processing phases), Information Transmission (information flow aggregation at boundary tokens), Fact Retrospection (persistent re-access of source facts), and Specialized Attention Heads (functionally distinct head types). These mechanisms generalize across model scales, rule types, and reasoning depths, providing mechanistic evidence that LLMs employ structured computational strategies for logical reasoning.",
      "authors": [
        "Danchun Chen",
        "Qiyao Yan",
        "Liangming Pan"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-07 04:20:30+00:00",
      "link": "https://arxiv.org/pdf/2601.04260v1",
      "tags": [
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Mechanistic analysis of Qwen3 architecture and reasoning strategies, directly relevant to LLM technical reports",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "8plus"
    },
    {
      "id": "2601.03486v1",
      "title": "Adaptive Model-Based Reinforcement Learning for Orbit Feedback Control in NSLS-II Storage Ring",
      "abstract": "The National Synchrotron Light Source II (NSLS-II) uses highly stable electron beam to produce high-quality X-ray beams with high brightness and low-emittance synchrotron radiation. The traditional algorithm to stabilize the beam applies singular value decomposition (SVD) on the orbit response matrix to remove noise and extract actions. Supervised learning has been studied on NSLS-II storage ring stabilization and other accelerator facilities recently. Several problems, for example, machine status drifting, environment noise, and non-linear accelerator dynamics, remain unresolved in the SVD-based and supervised learning algorithms. To address these problems, we propose an adaptive training framework based on model-based reinforcement learning. This framework consists of two types of optimizations: trajectory optimization attempts to minimize the expected total reward in a differentiable environment, and online model optimization learns non-linear machine dynamics through the agent-environment interaction. Through online training, this framework tracks the internal status drifting in the electron beam ring. Simulation and real in-facility experiments on NSLS-II reveal that our method stabilizes the beam position and minimizes the alignment error, defined as the root mean square (RMS) error between adjusted beam positions and the reference position, down to ~1$μ$m.",
      "authors": [
        "Zeyu Dong",
        "Yuke Tian",
        "Yu Sun"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-01-07 00:49:57+00:00",
      "link": "https://arxiv.org/pdf/2601.03486v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Applies model-based reinforcement learning to physical control systems",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.03500v1",
      "title": "SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models",
      "abstract": "Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process. We identify that visual statistical bias, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations. To address this, we introduce a training-free algorithm called Structure-Disrupted Contrastive Decoding (SDCD), which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.",
      "authors": [
        "Yuxuan Xia",
        "Siheng Wang",
        "Peng Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-01-07 01:27:58+00:00",
      "link": "https://arxiv.org/pdf/2601.03500v1",
      "tags": [
        "keyword:resnet"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Addresses internal complexities and hallucinations in Large Vision-Language Models",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.04424v1",
      "title": "Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization",
      "abstract": "Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.",
      "authors": [
        "Yao Dou",
        "Wei Xu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-07 22:08:17+00:00",
      "link": "https://arxiv.org/pdf/2601.04424v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Evaluates frontier LLMs like Gemini on long-context benchmarks",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "8plus"
    },
    {
      "id": "2601.03505v1",
      "title": "Beyond Perplexity: A Lightweight Benchmark for Knowledge Retention in Supervised Fine-Tuning",
      "abstract": "Supervised Fine-Tuning (SFT) is a standard approach for injecting domain knowledge into Large Language Models (LLMs). However, relying on validation perplexity to monitor training is often insufficient, as it confounds stylistic mimicry with genuine factual internalization. To address this, we introduce the Knowledge Retention (KR) Test , a lightweight, corpus-grounded evaluation framework designed to distinguish factual learning from linguistics. KR-Test utilizes automatically generated contrastive examples to measure likelihood preferences for correct versus incorrect continuations, requiring no instruction tuning or generative decoding. We validate the framework's integrity through a \"blind vs. oracle\" baseline analysis. Furthermore, we demonstrate the diagnostic capabilities of KR-Test by analyzing the training dynamics of Low-Rank Adaptation (LoRA). By exposing the fine-grained dissociation between linguistic convergence and knowledge retention, KR-Test enhances the interpretability of fine-tuning dynamics.",
      "authors": [
        "Soheil Zibakhsh Shabgahi",
        "Pedram Aghazadeh",
        "Farinaz Koushanfar"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-07 01:34:28+00:00",
      "link": "https://arxiv.org/pdf/2601.03505v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Evaluation framework for knowledge retention in LLMs",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.03520v1",
      "title": "A Reinforcement Learning-Based Model for Mapping and Goal-Directed Navigation Using Multiscale Place Fields",
      "abstract": "Autonomous navigation in complex and partially observable environments remains a central challenge in robotics. Several bio-inspired models of mapping and navigation based on place cells in the mammalian hippocampus have been proposed. This paper introduces a new robust model that employs parallel layers of place fields at multiple spatial scales, a replay-based reward mechanism, and dynamic scale fusion. Simulations show that the model improves path efficiency and accelerates learning compared to single-scale baselines, highlighting the value of multiscale spatial representations for adaptive robot navigation.",
      "authors": [
        "Bekarys Dukenbaev",
        "Andrew Gerstenslager",
        "Alexander Johnson",
        "Ali A. Minai"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.RO"
      ],
      "published": "2026-01-07 02:10:52+00:00",
      "link": "https://arxiv.org/pdf/2601.03520v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Focuses on Reinforcement Learning for autonomous navigation and mapping",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.04537v1",
      "title": "Not All Steps are Informative: On the Linearity of LLMs' RLVR Training",
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.",
      "authors": [
        "Tianle Wang",
        "Zhongyuan Wu",
        "Shenghao Jin",
        "Hao Xu",
        "Wei Chen",
        "Ning Miao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-08 03:06:18+00:00",
      "link": "https://arxiv.org/pdf/2601.04537v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Analyzes reinforcement learning training methodologies for LLM post-training",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ],
      "quick_tier": "8plus"
    },
    {
      "id": "2601.03509v1",
      "title": "Evolving Programmatic Skill Networks",
      "abstract": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.",
      "authors": [
        "Haochen Shi",
        "Xingdi Yuan",
        "Bang Liu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "published": "2026-01-07 01:43:25+00:00",
      "link": "https://arxiv.org/pdf/2601.03509v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Uses LLMs to evolve symbolic programs as skills in a compositional network",
      "llm_tags": [
        "keyword:符号回归（示例）",
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.03543v1",
      "title": "EvolMem: A Cognitive-Driven Benchmark for Multi-Session Dialogue Memory",
      "abstract": "Despite recent advances in understanding and leveraging long-range conversational memory, existing benchmarks still lack systematic evaluation of large language models(LLMs) across diverse memory dimensions, particularly in multi-session settings. In this work, we propose EvolMem, a new benchmark for assessing multi-session memory capabilities of LLMs and agent systems. EvolMem is grounded in cognitive psychology and encompasses both declarative and non-declarative memory, further decomposed into multiple fine-grained abilities. To construct the benchmark, we introduce a hybrid data synthesis framework that consists of topic-initiated generation and narrative-inspired transformations. This framework enables scalable generation of multi-session conversations with controllable complexity, accompanied by sample-specific evaluation guidelines. Extensive evaluation reveals that no LLM consistently outperforms others across all memory dimensions. Moreover, agent memory mechanisms do not necessarily enhance LLMs' capabilities and often exhibit notable efficiency limitations. Data and code will be released at https://github.com/shenye7436/EvolMem.",
      "authors": [
        "Ye Shen",
        "Dun Pei",
        "Yiqiu Guo",
        "Junying Wang",
        "Yijin Guo",
        "Zicheng Zhang",
        "Qi Jia",
        "Jun Zhou",
        "Guangtao Zhai"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-07 03:14:42+00:00",
      "link": "https://arxiv.org/pdf/2601.03543v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Benchmark for evaluating LLM memory capabilities across diverse dimensions",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.04597v1",
      "title": "THaLLE-ThaiLLM: Domain-Specialized Small LLMs for Finance and Thai -- Technical Report",
      "abstract": "Large Language Models (LLMs) have demonstrated significant potential across various domains, particularly in banking and finance, where they can automate complex tasks and enhance decision-making at scale. Due to privacy, security, and regulatory concerns, organizations often prefer on-premise deployment of LLMs. The ThaiLLM initiative aims to enhance Thai language capabilities in open-LLMs, enabling Thai industry to leverage advanced language models. However, organizations often face a trade-off between deploying multiple specialized models versus the prohibitive expense of training a single multi-capability model. To address this, we explore model merging as a resource-efficient alternative for developing high-performance, multi-capability LLMs. We present results from two key experiments: first, merging Qwen-8B with ThaiLLM-8B demonstrates how ThaiLLM-8B enhances Thai general capabilities, showing an uplift of M3 and M6 O-NET exams over the general instruction-following Qwen-8B. Second, we merge Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B. This combination results in further improvements in performance across both general and financial domains, by demonstrating an uplift in both M3 and M6 O-NET, Flare-CFA, and Thai-IC benchmarks. The report showcases the viability of model merging for efficiently creating multi-capability LLMs.",
      "authors": [
        "KBTG Labs",
        ":",
        "Anuruth Lertpiya",
        "Danupat Khamnuansin",
        "Kantapong Sucharitpongpan",
        "Pornchanan Balee",
        "Tawunrat Chalothorn",
        "Thadpong Pongthawornkamol",
        "Monchai Lertsutthiwong"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-08 05:01:07+00:00",
      "link": "https://arxiv.org/pdf/2601.04597v1",
      "tags": [
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Technical report on ThaiLLM architecture and model merging methodologies",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "8plus"
    },
    {
      "id": "2601.03511v1",
      "title": "IntroLM: Introspective Language Models via Prefilling-Time Self-Evaluation",
      "abstract": "A major challenge for the operation of large language models (LLMs) is how to predict whether a specific LLM will produce sufficiently high-quality output for a given query. Existing approaches rely on external classifiers, most commonly BERT based models, which suffer from limited context windows, constrained representational capacity, and additional computational overhead. We propose IntroLM, a method that enables causal language models to predict their own output quality during the prefilling phase without affecting generation using introspective tokens. By introducing token conditional LoRA that activates only for the introspective token, the model learns to predict the output quality for a given query while preserving the original backbone behavior and avoiding external evaluators. On question answering benchmarks, IntroLM applied to Qwen3 8B achieves a ROC AUC of 90 precent for success prediction, outperforming a DeBERTa classifier by 14 precent. When integrated into multi model routing systems, IntroLM achieves superior cost performance tradeoffs, reducing latency by up to 33 precent and large model usage by up to 50 precent at matched reliability.",
      "authors": [
        "Hossein Hosseini Kasnavieh",
        "Gholamreza Haffari",
        "Chris Leckie",
        "Adel N. Toosi"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-07 01:48:17+00:00",
      "link": "https://arxiv.org/pdf/2601.03511v1",
      "tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Self-evaluation methodology for output quality in large language models",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.03597v1",
      "title": "From Chains to Graphs: Self-Structured Reasoning for General-Domain LLMs",
      "abstract": "Large Language Models (LLMs) show strong reasoning ability in open-domain question answering, yet their reasoning processes are typically linear and often logically inconsistent. In contrast, real-world reasoning requires integrating multiple premises and solving subproblems in parallel. Existing methods, such as Chain-of-Thought (CoT), express reasoning in a linear textual form, which may appear coherent but frequently leads to inconsistent conclusions. Recent approaches rely on externally provided graphs and do not explore how LLMs can construct and use their own graph-structured reasoning, particularly in open-domain QA. To fill this gap, we novelly explore graph-structured reasoning of LLMs in general-domain question answering. We propose Self-Graph Reasoning (SGR), a framework that enables LLMs to explicitly represent their reasoning process as a structured graph before producing the final answer. We further construct a graph-structured reasoning dataset that merges multiple candidate reasoning graphs into refined graph structures for model training. Experiments on five QA benchmarks across both general and specialized domains show that SGR consistently improves reasoning consistency and yields a 17.74% gain over the base model. The LLaMA-3.3-70B model fine-tuned with SGR performs comparably to GPT-4o and surpasses Claude-3.5-Haiku, demonstrating the effectiveness of graph-structured reasoning.",
      "authors": [
        "Yingjian Chen",
        "Haoran Liu",
        "Yinhong Liu",
        "Sherry T. Tong",
        "Aosong Feng",
        "Jinghui Lu",
        "Juntao Zhang",
        "Yusuke Iwasawa",
        "Yutaka Matsuo",
        "Irene Li"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-07 05:27:41+00:00",
      "link": "https://arxiv.org/pdf/2601.03597v1",
      "tags": [
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Explores reasoning architectures and graph-structured methodologies for LLMs",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.04670v1",
      "title": "Learning Dynamics in RL Post-Training for Language Models",
      "abstract": "Reinforcement learning (RL) post-training is a critical stage in modern language model development, playing a key role in improving alignment and reasoning ability. However, several phenomena remain poorly understood, including the reduction in output diversity. To gain a broader understanding of RL post-training, we analyze the learning dynamics of RL post-training from a perspective that has been studied in supervised learning but remains underexplored in RL. We adopt an empirical neural tangent kernel (NTK) framework and decompose the NTK into two components to characterize how RL updates propagate across training samples. Our analysis reveals that limited variability in feature representations can cause RL updates to systematically increase model confidence, providing an explanation for the commonly observed reduction in output diversity after RL post-training. Furthermore, we show that effective learning in this regime depends on rapidly shaping the classifier, which directly affects the gradient component of the NTK. Motivated by these insights, we propose classifier-first reinforcement learning (CF-RL), a simple two-stage training strategy that prioritizes classifier updates before standard RL optimization. Experimental results validate our theoretical analysis by demonstrating increased model confidence and accelerated optimization under CF-RL. Additional analysis shows that the mechanism underlying CF-RL differs from that of linear-probing-then-fine-tuning in supervised learning. Overall, our study formalizes the learning dynamics of RL post-training and motivates further analysis and improvement.",
      "authors": [
        "Akiyoshi Tomihari"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-08 07:32:15+00:00",
      "link": "https://arxiv.org/pdf/2601.04670v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Analyzes RL post-training dynamics in language models using NTK framework",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ],
      "quick_tier": "8plus"
    },
    {
      "id": "2601.03515v1",
      "title": "Mem-Gallery: Benchmarking Multimodal Long-Term Conversational Memory for MLLM Agents",
      "abstract": "Long-term memory is a critical capability for multimodal large language model (MLLM) agents, particularly in conversational settings where information accumulates and evolves over time. However, existing benchmarks either evaluate multi-session memory in text-only conversations or assess multimodal understanding within localized contexts, failing to evaluate how multimodal memory is preserved, organized, and evolved across long-term conversational trajectories. Thus, we introduce Mem-Gallery, a new benchmark for evaluating multimodal long-term conversational memory in MLLM agents. Mem-Gallery features high-quality multi-session conversations grounded in both visual and textual information, with long interaction horizons and rich multimodal dependencies. Building on this dataset, we propose a systematic evaluation framework that assesses key memory capabilities along three functional dimensions: memory extraction and test-time adaptation, memory reasoning, and memory knowledge management. Extensive benchmarking across thirteen memory systems reveals several key findings, highlighting the necessity of explicit multimodal information retention and memory organization, the persistent limitations in memory reasoning and knowledge management, as well as the efficiency bottleneck of current models.",
      "authors": [
        "Yuanchen Bei",
        "Tianxin Wei",
        "Xuying Ning",
        "Yanjun Zhao",
        "Zhining Liu",
        "Xiao Lin",
        "Yada Zhu",
        "Hendrik Hamann",
        "Jingrui He",
        "Hanghang Tong"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-07 02:03:13+00:00",
      "link": "https://arxiv.org/pdf/2601.03515v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Benchmarking long-term memory for multimodal LLM agents",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.03604v1",
      "title": "Interleaved Tool-Call Reasoning for Protein Function Understanding",
      "abstract": "Recent advances in large language models (LLMs) have highlighted the effectiveness of chain-of-thought reasoning in symbolic domains such as mathematics and programming. However, our study shows that directly transferring such text-based reasoning paradigms to protein function understanding is ineffective: reinforcement learning mainly amplifies superficial keyword patterns while failing to introduce new biological knowledge, resulting in limited generalization. We argue that protein function prediction is a knowledge-intensive scientific task that fundamentally relies on external biological priors and computational tools rather than purely internal reasoning. To address this gap, we propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation. Instead of relying on long unconstrained reasoning traces, PFUA integrates domain-specific tools to produce verifiable intermediate evidence. Experiments on four benchmarks demonstrate that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.",
      "authors": [
        "Chuanliu Fan",
        "Zicheng Ma",
        "Huanran Meng",
        "Aijia Zhang",
        "Wenjie Du",
        "Jun Zhang",
        "Yi Qin Gao",
        "Ziqiang Cao",
        "Guohong Fu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-07 05:34:38+00:00",
      "link": "https://arxiv.org/pdf/2601.03604v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 6.0,
      "llm_evidence": "explores reinforcement learning and reasoning paradigms in large language models",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ],
      "quick_tier": "6"
    }
  ]
}