Title: Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints

URL Source: https://arxiv.org/pdf/2602.16954v1

Published Time: Fri, 20 Feb 2026 01:15:13 GMT

Number of Pages: 18

Markdown Content:
# Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 

Chuqin Geng 1 2 Li Zhang 2 Mark Zhang 2 Haolin Ye 1 Ziyu Zhao 1 Xujie Si 2

## Abstract 

We challenge black-box purely deep neural approaches for molecules and graph genera-tion, which are limited in controllability and lack formal guarantees. We introduce Neuro-Symbolic Graph Generative Modeling (NSGGM), a neurosymbolic framework that reapproaches molecule generation as a scaffold and interaction learning task with symbolic assembly. An au-toregressive neural model proposes scaffolds and refines interaction signals, and a CPU-efficient SMT solver constructs full graphs while enforc-ing chemical validity, structural rules, and user-specific constraints, yielding molecules that are correct by construction and interpretable con-trol that pure neural methods cannot provide. NSGGM delivers strong performance on both 

unconstrained generation and constrained gener-ation tasks, demonstrating that neuro-symbolic modeling can match state-of-the-art generative performance while offering explicit controlla-bility and guarantees. To evaluate more nu-anced controllability, we also introduce a Logical-Constraint Molecular Benchmark , designed to test strict hard-rule satisfaction in workflows that require explicit, interpretable specifications to-gether with verifiable compliance. 

## 1. Introduction 

Generative modeling of graph-structured data is a fundamen-tal challenge with profound implications for scientific dis-covery and engineering (Brockschmidt et al., 2019). From designing novel molecules and materials (Liu et al., 2018a; Cao & Kipf, 2018) to discovering electronic circuits (Chang et al., 2024), the ability to generate controllable graphs is a 

> *

Equal contribution 1 School of Computer Science, McGill University, Montreal, Canada 2 Department of Computer Sci-ence, University of Toronto, Toronto, Canada . Correspondence to: Chuqin Geng <chuqin.geng@mail.mcgill.ca >, Li Zhang 

<lizhang@cs.toronto.edu >, Xujie Si <six@cs.toronto.edu >.

Preprint. February 20, 2026. 

key tool for innovation. However, molecular graph genera-tion remains challenging due to the non-sequential nature of graphs, combinatorial sparsity, and the need to preserve chemical and guidance control. These difficulties have motivated many generative models. Despite their success, deep molecular generative models (i.e. VAEs, autoregressive models, flows, diffusion) enable the exploration of the chemical space and are commonly evalu-ated using standardized distributional and diversity metrics (Brown et al., 2019) (Polykovskiy et al., 2020). However, as structural constraints are typically learned implicitly rather than specified as explicit rules, these methods generally do not provide formal certificates for user-specified constraints. Additionally, unless constraint compliance is learned during training or enforced during generation, hard user-specified constraints are often handled via post-hoc filtering, which does not scale with increased scaffold size (Maziarz et al., 2022). In high-stakes domains where transparency and in-terpretability is critical (Amann et al., 2020), this motivates methods that allow users to state hard constraints explicitly and verify satisfaction with checkable certificates, rather than relying on implicit neural control. Motivated by these limitations, we introduce Neuro-

Symbolic Graph Generative Modeling (NSGGM), a neu-ral–symbolic framework reframing molecule generation as a dual problem of scaffold and interaction learning and sym-bolic constraint modeling task. Our approach begins by de-composing molecules into a vocabulary of motifs/fragments (subgraph tokens) equipped with interface information and 

neural guidance specifying how they may connect to neigh-boring motifs, while deriving a set of symbolic assembly constraints over these interfaces. Benefiting from symbolic assembly, the neural component can focus on learning data-driven prior and providing high-level guidance, while an SMT solver (Barrett & Tinelli, 2018) enforces hard struc-tural rules, guarantees constraint satisfaction by construc-tion, and provide a transparent, verifiable layer of which constraints are satisfied. While we share the idea of assembling predefined substruc-tures with fragmented-based generative models (Jin et al., 2018; Mercado et al., 2021; Jin et al., 2020b), the key distinc-tion is how assembly is performed. Prior methods typically 1

> arXiv:2602.16954v1 [cs.LG] 18 Feb 2026 Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints

approach assembly with a neural component entirely, where constraint sanctification is handled implicitly with any ad-ditional requirements being enforced via conditioning or post-hoc checks / rules rather than explicit, verifiable rules. To contrast, NSGGM takes a neuro-symbolic approach by separating learned-high level proposal from exact assembly under explicit constraints, allowing for transparent and certi-fiable sanctification of hard requirements. In summary, our key contributions are: • We propose a two-stage neuro-symbolic pipeline where a lightweight neural model proposes subgraph tokens and provides global attachment / local interaction in-fluence, and a symbolic SMT solver assembles a final graph by enforcing hard constraints by construction. • NSGGM offers transparency, user-steerable controlla-bility via explicit user-definable constraints expressed in the symbolic layer, while allowing the neural layer to complete unconstrained regions; this supports two com-plementary modes: (i) conditioning-based completion from user-supplied scaffolds, and (ii) constraint-driven synthesis subject to specified structural or topological constraints. • We demonstrate that NSGGM achieves strong perfor-mance on unconstrained generation benchmarks and scaffold-conditioned generation, and present a case study of hard logical constraints with quantitative eval-uation that highlights explicit constraint satisfaction and the transparency enabled by symbolic specifica-tions. 

## 2. Preliminaries 

2.1. Graph Generation Tasks 

The primary goal of graph generation is to learn the underly-ing distribution from a dataset of graphs G = {G1, . . . , G n}

in order to synthesize new, valid samples. We focus on graphs with categorical node and edge attributes, a common setup in molecular design (Vignac et al., 2023). Formally, we define a graph as a tuple G = ( V, E, x, e), where V

is the set of nodes, E ⊆ V × V is the set of edges, and 

x : V → X and e : E → E assign attributes from discrete label sets X and E, respectively. The task is then to learn a parameterized model Pθ (G) that approximates the true data distribution and to use it for generating novel graphs 

G′ ∼ Pθ (·).Prevailing approaches fall into two families: (i) diffusion models, which learn Pθ (G) by minimizing a divergence (typically KL) and generate graphs via computationally in-efficient, iterative denoising (Yang et al., 2024); and (ii) fragment-based methods that assemble graphs from pre-defined substructures using heuristic search or hard-coded rules. Despite strong generative performance, these mod-els typically offer limited user-steerable control and do not provide explicit, verifiable satisfaction of real-world design constraints. 

2.2. Constraint Satisfaction Problem Solving and SMT Solving 

The deterministic assembly stage of our framework is for-mulated as a Constraint Satisfaction Problem (CSP), neces-sitating a tool with formal guarantees. For this, we turn to Satisfiability Modulo Theories (SMT) solvers. While any off-the-shelf SMT solver could be employed, we choose Z3 (de Moura & Bjørner, 2008) as it is considered state-of-the-art in performance and reliability. A theory T in first-order logic defines a set of symbols (e.g., constants like 0, functions like +) and axioms that constrain their interpretation (e.g., the theory of linear integer arith-metic, TLIA ). An SMT solver’s task is to determine if a given quantifier-free first-order formula ϕ is T-satisfiable —that is, if there exists a model that satisfies both the axioms of T

and the formula ϕ. In this work, the formula ϕ will be the encoding of constraints used to assemble a valid graph from a proposed blueprint. The problem is typically defined by a logical formula ϕ

over a set of variables Z = {z1, . . . , z m}, where each vari-able zi has a corresponding domain Di. The formula is a conjunction of constraints 

C = {c1, . . . , c n} : ϕ := 

> n

^

> j=1

cj (Zj ), Zj ⊆ Z. (1) A solution is a model M , which is an interpretation mapping each variable to a value in its domain, M : Z → D, such that the formula is satisfied ( M |= ϕ). The solver returns one of two outcomes: • SAT (Satisfiable): A valid model M exists and is re-turned. • UNSAT (Unsatisfiable): The solver proves that no such model exists. The model M returned by the solver on a SAT result pro-vides the deterministic instructions for constructing a valid graph. We detail our specific constraint encoding ϕ in Sec-tion 3.2. 

## 3. The NSGGM Framework 

NSGGM treats graph generation as a compositional task, analogous to building with modular components. Our method first decomposes existing molecule scaffolds into a vocabulary of fundamental pieces. A new, valid graph is 2Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints A) Encode     

> Neural
> B) Decode
> Z
> Transformer Decoder
> Motif Token Sequence
> Encoder GNN
> ... ...
> Bi-Directional Refinement & Feature Heads
> Parent Influence
> Merge Influence
> ... ...
> Blueprint Characterization
> User Constraints
> SMT Solver
> Symbolic Assembly
> Final Molecule
> Preprocessing
> Training Data
> Motif Library
> Decomposition
> ,,...

Figure 1. An overview of the NSGGM framework. 

then created by sampling from this vocabulary and assem-bling the pieces while adhering to symbolic assembly con-straints and neural guidance. Figure 1 provides an overview of this neuro-symbolic framework. 

3.1. Decomposition and Vocabulary Structural partitioning Our decomposition leverages the graph’s cycle structure, akin to fragment-based methods (e.g., Jin et al., 2018; Mercado et al., 2021). For G = ( V, E ),we compute a minimum cycle basis C(G) = {c1, . . . , c p}

and split edges into 

EC =

> p

[

> j=1

E(cj ), EA = E \ EC . (2) Let GA = ( V, E A). We define the set of primitives as 

P(G) := C(G) ∪ Components( GA), (3) and enumerate P(G) = {Pi = ( Vi, E i)}mi=1 . By construc-tion, these primitives form an overlapping cover of G, i.e., S 

> i

Vi = V and S 

> i

Ei = E. We make overlaps explicit via 

Vsh =[

> i̸=j

(Vi ∩ Vj ), Esh =[

> i̸=j

(Ei ∩ Ej ). (4) For the acyclic part, we further refine Components( GA)

into recurring tree motifs (Appendix A). Compared to Jin et al. (2018), which uses single edges (plus rings) as acyclic clusters, we admit multi-edge tree motifs and equip them with explicit slot interfaces; overlaps ( Vsh , E sh ) are recon-ciled during assembly by slot matching, without requiring explicit projection maps. 

Molecular interface characterization To enable guided reassembly, we define interfaces over node and edge bond-type slot assignments , σV and σE , enriched with chemistry-aware attributes. Let E be the set of element types and B the set of bond orders (e.g., B = {1, 2, 3} for single/double/triple), and define elem : V → E , val : V → N, and bo : E → B . For a token gi, define the residual bond-type capacities at v as 

ri,b (v) := val( v) − X

> e∈Ei(v)

bo( e)

restricted to bond order b ∈ B and set the node slot assign-ment to 

σV (v, g i) =  elem( v), val( v), (ri,b (v)) b∈B 

,ri,b (v) ≥ 0 ∀b ∈ B .

where Ei(v) are bonds incident to v within token gi.For shared internal bonds, we record their bond order: 

σE (e, g i) := bo( e), e ∈ Ei ∩ Esh .

We prove that Structural Partitioning yields an overlapping cover of G with cycles and tree components as primitives, and uniquely determined interface slots (Proposition B.1, Appendix B.1). 

Vocabulary and blueprint A motif is a subgraph type 

g ∈ V (up to isomorphism), where the global vocabulary 

V is discovered from the training set G. For each primitive occurrence Pi = ( Vi, E i) ∈ P (G), we assign its motif type gi ∈ V and let the neural model select an interface characterization 

si = ( σiV , σ iE ),

A concrete assignment of the interface-slot functions, re-stricted to characterizations observed in the training data. We represent the resulting annotated instance by ti =(gi, s i). The blueprint for G is the multiset of annotated instances 

SG = { ti }mi=1 , (5) where m := |P (G)| is the number of primitive occurrences in G.3Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 

3.2. Assembly Constraints for Graph Synthesis 

Given a blueprint S′ = {t1, . . . , t k} of slotted primitives 

ti = ( gi, s i), determine a merging of nodes across prim-itives such that (i) matched node and edge slots are com-patible (type-equality and residual-capacity satisfaction), and (ii) the induced identifications yield a consistent graph 

G′ respecting all connections. This is encoded as an SMT constraint-satisfaction problem with variables for node iden-tifications and constraints enforcing slot compatibility and global consistency. 

Decision variables Let Vslots = S 

> i

V slot  

> i

be the set of all node-slot atoms in the blueprint. For every pair of distinct primitives i̸ = j and atoms u ∈ V slot  

> i

, v ∈ V slot  

> j

, we introduce a Boolean merge variable 

mu,v ∈ { 0, 1},

interpreted as mu,v = T rue ⇐⇒ u and v are merged in G′. Edge-slot sharing is realized implicitly via end-point merges: two edge-slot copies (u, v ) ∈ Eslot  

> i

and 

(u′, v ′) ∈ Eslot  

> j

are considered edge-merged iff their end-points are merged in a consistent orientation satisfying the hard constraints outlined below and in Appendix B.2 

Hard constraints ( ϕhard ) Hard constraints are inviolable and enforce topological integrity and interface consistency. Let the internal (within-primitive) bond-order degree of an atom u ∈ Vi be deg i(u). We impose: ∀ u, v, mu,v ⇒ u ∈ Vi, v ∈ Vj for some i̸ = j, mu,v ⇒ elem( u) = elem( v),

deg i(u) + deg j (v) ≤ cap  elem( u).

For edge merges, this requires mu,v and mnu,nv to both be true, where nu, nv are neighbours to u, v from the respec-tive subgraphs, that is, they form edges e = ( u, nu ) ∈ ti

and e′ = ( v, nv ) ∈ tj , i ̸ = j. We impose that may be merged only if they (i) have the bo (e) = bo (e′), and (ii) their endpoints are merged consistently, that is mu,v =

mnu,nv = T rue with the fact that bo (e) can be subtracted from valency cap and e ∈ Eslot  

> i

and e′ ∈ Eslot  

> j

. See details in Appendix B.2. 

Neural guidance (Soft constraints (ϕsoft )) We use weighted Max-SMT to satisfy all hard constraints while maximizing soft rewards. Soft constraints are used to en-courage connectivity, that is, encourage subgraph tokens to connect to each other or in certain ways so that the final result is not fragmented. For implementation details see Appendix B.3 

(i) Neural parent guidance (decoder tree). If the decoder pre-dicts a parent primitive for each child, we reward connecting each child to its predicted parent. 

Correctness guarantees. For any blueprint S′, there exists a feasible assignment that reassembles each training graph from its own tokens (Corollary B.2). Moreover, any assign-ment satisfying ϕhard yields a well-defined simple graph; in the molecular case it also enforces element consistency, unique bond orders, and exact valency (Theorem B.3). To-gether, these imply correctness-by-construction and com-pleteness for training graphs (Corollary B.4). Proofs are provided in Appendix B.1. 

User constraints ( ϕuser ) Let s denote the solver deci-sion variables introduced by our assembly encoding (e.g., merge variables {mu,v } and, when applicable, bond/order variables). We optionally introduce auxiliary variables a

that represent higher-level, user-facing properties of the as-sembled graph (e.g., indicators or counts), together with a definitional theory ϕaux (s, a ) that makes each such property a function of s. Users may then supply an additional con-straint formula ϕuser (s, a ) written in the same quantifier-free SMT theory as our encoding. User constraints never weaken hard constraints: the final hard feasibility formula is 

Φhard (s, a ) := ϕhard (s) ∧ ϕaux (s, a ) ∧ ϕuser (s, a ).

Thus ϕuser can only restrict the feasible set (or make it empty). The solver returns either (i) SAT with a model M ,from which we deterministically decode the output graph 

G′, or (ii) UNSAT , certifying that no graph satisfies all hard requirements. The full implementation of these constraints in the solver is given in the Appendix B.3 

3.3. Neural Proposals 

Under our decomposition, each molecule, G, is represented by a finite sequence of motif tokens, T = ( g1, ..., g n), from the global vocabulary V. We employ a VAE with a GNN encoder that operates on a motif graph representation of a molecule, G. Given a primitive decomposition P(G) = 

{Pi}mi=1 , we define the motif graph H(G) = ( P(G), E H )

where (Pi, P j ) ∈ EH iff Pi and Pj overlap (i.e., share at least one node/edge) or are connected by an edge in G. The encoder maps H(G) to a latent code z. We model the condi-tional P (T | z) auto-regressively, followed by a bidirectional refinement network that predicts blueprint interface charac-terizations (s1, ..., s n). The network classifies the blueprint 

of each token gi through P (k|T ) to select the top blueprint prototype from Ωt, a fixed set of top-K blueprint variants observed for that scaffold during training. 

Scaffolds as sequences Let a scaffold proposal sequence 

be an ordered sequence S = ( g1, . . . , g k, <eos> ). Condi-tioned on the latent z, we parametrize the marginal likeli-4Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 

hood as: 

pθ (S); =; 

Z

p(z)·

> k+1

Y

> ℓ=1

Pθ

 gℓ | g<ℓ , z, d z, g k+1 ≡ <eos> .

(6) 

Hierarchical graph encoder We define a GNN encoder 

fθ that maps a molecule G to an embedding in Rd by op-erating on a coarse-grained decomposition graph H(G) = (P(G), E H ). Nodes of H(G) correspond to primitive oc-currences Pi = ( Vi, E i) ∈ P (G) and are featurized by their motif type gi ∈ V (Section 3.1). We define an edge 

(Pi, P j ) ∈ EH whenever Pi and Pj overlap in G, and anno-tate it with a binary relation rij ∈ { 0, 1}, indicating whether the overlap includes a shared edge ( rij = 1 ) or only shared nodes ( rij = 0 ). The latent z is sampled from the global sum-pooling of the final representation hG = P h(L) 

> i

:

z ∼ N  μϕ(hG ), σ 2

> ϕ

(hG ) (7) 

Vocabulary decoder : We model the autoregressive genera-tion of the motif sequence T = ( g1, ..., g L) by approximat-ing P (T | z) via a causal Transformer decoder. Given time step g<ℓ and latent z.

3.4. Neural Guidance (Refinement) Post-decoder refinement Let the hidden decoder states 

{hdec  

> ℓ

}Lℓ=1 ⊂ Rd conditioned on the partial token sequence 

T and latent z. A bidirectional Transformer encoder ϕpost 

refines these states to H = {hℓ}Lℓ=1 :

H = ϕpost 

 Hdec  , hℓ ∈ Rd.

We then model three conditional distributions at each posi-tion ℓ that acts like neural guidance during assembly. 

Blueprint characterization head Recall that each blueprint instance is tℓ = ( gℓ, s ℓ), where sℓ is an inter-face characterization (Section 3.1). We define a discrete characterization vocabulary 

Vb := [

> g∈V

S(g),

where S(g) is the finite set of characterizations for motif type g observed in the training data. At position ℓ, the decoder predicts sℓ from the hidden state hℓ via 

p(sℓ | g≤ℓ, z ) = softmax  Wbhℓ + mask( gℓ),

where mask( gℓ) restricts the softmax support to S(gℓ) ⊆Vb.

Merge-influence head To further guide assembly, we pre-dict a binary merge-type indicator mℓ ∈ { 0, 1} at each position ℓ, where mℓ = 0 denotes a node-merge operation and mℓ = 1 denotes an edge-merge operation. We model this as a Bernoulli distribution parameterized by the refined state hℓ.

Parent-influence head We predict a parent index πℓ ∈{1, . . . , L } \ { ℓ} for each position ℓ (self-parenting is disal-lowed). We compute bilinear attention scores 

sℓj = d−1/2 hℓWQ

  hj WK

⊤,

and normalize over all valid choices j̸ = ℓ:

P (πℓ = j | T , z ) = exp( sℓj )

P 

> p̸=ℓ

exp( sℓp ) , j̸ = ℓ. 

3.5. Training Details Decomposition Each molecule is decomposed as de-scribed in Section 3.1. Then, each interface, h, is mapped to: (1) a structure token tℓ given by a structural Weisfeiler– Lehman (WL) hash hstruct (looked up in a fixed vocabu-lary), and (2) a blueprint characterization variant hash hmeta 

capturing interface/slot instantiation. For every hstruct , we maintain the top-K most frequent metadata variants and supervise a categorical target t′ 

> ℓ

∈ { 1, . . . , K } whenever the ground-truth hmeta falls in this candidate set (otherwise the position is ignored via masking). 

Two-stage decoding Decoding scaffold proposals happens in two stages: (1) an autoregressive Transformer decoder generates scaffold-token sequence T conditioned on latent z, (2) a bidirectional Transformer encoder refines the token-level representations through the full teacher-forced sequence to predict global structural annotations that guides downstream assembly. 

Objective We aim to maximize the marginal log-likelihood of the observed outputs y := (T , π 1: L, m 1: L, t ′

> 1: L

) under both training modes: full-molecule reconstruction and scaffold-conditioned generation: 

max  

> θ,ϕ

E(G)∼D 

h Z

pθ (y | z) pϕ(z | G ) dz, 

i

. (8) which is intractable to optimize directly; in practice we max-imize a variational lower bound (ELBO). The full ELBO and complete training are given in Appendix B.4. 

## 4. Experiments and Results 

We evaluate our neurosymbolic approach along two objec-tives: (i) distribution fidelity and competitive performance 5Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 

Table 1. Unconditional de novo molecule generation on GuacaMol. 

Model Valid ↑ Unique ↑ FCD ↑ KL ↑ Novel ↑

MOLER (2022) 100.0 100.0 62.5 96.4 99.1 MICAM (2023) 100.0 99.4 73.1 98.9 98.6 DIGRESS (2023) 85.2 100.0 68.0 92.9 99.9 LSTM 95.9 100.0 91.3 99.1 91.2 DISCO-GT (2024) 100.0 86.6 59.7 92.6 99.9 NSGGM (ours) 100.0 ± 0.0 100.0 ± 0.0 86.1 ±0.4 95 .5 ± 0.1 98 .9 ± 0.0

Table 2. Unconditional de novo molecule generation on MOSES. We report standard MOSES benchmark metrics. — indicates that the metric is not reported in the original paper. 

Model Valid ↑ Unique ↑ Novel ↑ Filters ↑ FCD ↑ SNN ↑ Scaf ↑

JT-VAE (2018) 100.0 100.0 99.9 97.8 81.8 0.53 10.0 MOLER (2022) 100.0 99.9 97.1 — 85.2 — —DIGRESS (2023) 85.7 100.0 95.0 97.1 78.8 0.52 14.8 GRAPH INVENT (2021) 96.4 99.8 — 95.0 78.3 0.54 12.7 DISCO-GT (2024) 88.3 100.0 97.7 95.6 74.9 0.50 15.1 NSGGM (ours) 100.0 ± 0.0 100.0 ± 0.0 94 .4 ± 0.3 97 .1 ± 0.1 82 .3 ± 0.3 0.5 ± 0.0 11 .2 ± 0.1

Table 3. Unconditioned molecule generation on QM9 implicit hydrogens. — indicates that the metric is not reported in the original paper.                                                           

> Model Valid ↑Unique ↑FCD ↑KL ↑
> JT-VAE (2018) 100.0 54.9 58.8 89.1 MOLER (2022) 100.0 94.0 93.1 96.9 LO-ARM-ST -SEP (2025) 99.9 98.9 95.3 —DIGRESS (2023) 99 .0±0.196 .2±0.1——UNI GEM (2025) 95 .0±3.198.1 ——MICAM (2023) 100.0 93.2 94.5 98.0 DISCO-GT (2024) 99 .3±0.699.5 ——NSGGM (ours) 100 .0±0.096 .1±0.294 .7±0.494 .0±1.0

under unconstrained generation on small and large scale benchmarks, and (ii) exploratory robustness under hard structural constraints at inference time (i.e. logical and scaffold constraints). Accordingly, we test whether exact symbolic constraints can be enforced without costly retraining while preserving generative signals from the neural mode for exploration within the constrained space. 

4.1. Unconstrained Generation Setup We evaluate on QM9 (Wu et al., 2018), MOSES (Polykovskiy et al., 2020), and GuacaMol (Brown et al., 2019); dataset details are in Appendix C. For MOSES and GuacaMol, we use the official training, validation, and test splits and 80/10/10 split for QM9. Unless stated oth-erwise, all results are averaged over three runs, each gen-erating 10k molecules, and we report mean and standard deviation. Also, we report generation throughput and hard-ware in Appendix D.1. In our benchmarks, for distribution fidelity, we use Frechet ChemNet Distance (FCD) (Preuer et al., 2018) and KL Divergence (KL). KL measures how different the property distributions of generated molecules are from the training data. FCD measures the similarity between generated and real molecules using ChemNet embeddings. Additionally, we evaluate on standard fidelity-focused metrics like 

V alidity (Correctness) and U nique (Uniqueness). 

Small scale unconstrained generation We report QM9 with the standard metrics: Validity , Unique 

(Uniqueness), FCD, and KL in Table 3. 

Large-scale generation We evaluate on MOSES (Polykovskiy et al., 2020) and GuacaMol (Brown et al., 2019). For GuacaMol, we report Validity, Uniqueness, Novelty, and standard distributional metrics including FCD and KL divergence in Table 1. For MOSES in Table 2, we report FCD, and standard MOSES evaluation metrics including Filters, SNN, and Scaffold similarity (Scaf) on the TestSF split which is made of separate scaffolds not seen during training. Filters measure the percentage of generated molecules that pass the same chemical constraints of the test set, SNN measures the similarity to the nearest molecule using Tanimoto similarity, and scaffold similarity represents the Bemis–Murcko scaffold overlap with test molecules. 

Analysis As shown in Tables 3, 1, 2, NSGGM demon-strates consistent strong behaviour across the increasing scales. On all benchmarks, NSGGM achieves 100% va-lidity by construction and strong distributional alignment, 6Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 0 5k 10k 15k 20k    

> Attempts
> 0
> 200
> 400
> 600
> 800
> 1000
> Valid Molecules
> Model
> GenMol
> MoLeR
> NSGGM
> Model
> GenMol
> MoLeR
> NSGGM
> Logical Constraint
> φ1φ2φ3φUNSAT

Figure 2. Sample efficiency under logical constraints: cumula-tive number of constraint-satisfying molecules (up to 1000) ver-sus the number of generation attempts. Colours denote model type and markers/linestyles denote constraints φ1–φU N SAT (with 

φU N SAT unsatisfiable by construction). We only plot first 20K attempts for readability as curves beyond 20K are flat. 

confirming the importance of the neural component in guid-ing the symbolic solver. On GuacaMol, NSGGM achieves competitive normalized FCD, outperforming most baselines while maintaining high novelty, indicating that symbolic assembly does not degrade distributional metrics at scale. On MOSES, NSGGM achieves high filter pass rates and low FCD with strong nearest-neighbor similarity (SNN), demonstrating its ability in remaining close to the training distributional despite symbolic structural enforcement. 

Table 4. NSGGM ablations on GuacaMol. Full: complete neuro-symbolic pipeline (neural proposal + neural refinement + sym-bolic solver). W/o neural guidance: remove the neural refine-ment/guidance stage; the solver operates on coarse/high-level pro-posals. W/o solver: remove the symbolic solver; generate directly off of neural component. Variant Valid ↑ FCD ↑

Full 100 .0 ± 0.0 86 .1 ± 0.4

W/o neural guidance 100 .0 ± 0.0 46 .0 ± 1.2

W/o solver 58 .4 ± 2.4 30 .0 ± 1.1

Ablation To further our findings, we perform ablations on GUACA MOL in Table 4 to quantify how much the perfor-mance depends on the neural versus the symbolic solver component. From the table, removing the neural guid-ance component substantially degrades distributional match (FCD), while removing the solver greatly reduces both va-lidity and FCD as local connectivity signal is lost. 

4.2. Logical Constraint Satisfaction 

Next, we evaluate NSGGM on four synthetic, chemistry-inspired, logical constraints ( ρ1, ρ 2, ρ 3, ρ UNSAT ) spanning high, low, and zero training-data support, designed to test whether NSGGM can produce chemically valid molecules that exactly satisfy increasingly difficult specifications. 

Figure 3. Example generated molecule satisfying φ3

Baselines We compare against M OLER (Maziarz et al., 2022) and G EN MOL (Lee et al., 2025) as two recent control-lable generators that can be seeded/conditioned on structural conditions, while many other models would require task-specific fine-tuning to support comparable control. All neural components are trained on GuacaMol. To obtain a comparable baseline for purely neural generators, we use a generate-and-filter evaluation strategy: we sample can-didate molecules from baseline models (biased sampling by seeding with molecules from training set that satisfy the corresponding constraint) and apply exact logical constraint checking, rejecting samples that violate it. For all neural models, we give a sampling budget of 50K molecule at-tempts. For NSGGM, logical constraints are applied only at generation time via SMT-based symbolic verification. We 

do not fine-tune the model on any of the objectives and consider the UNSAT case for our model to count as a fail 

for a fair comparison (otherwise NSGGM will be trivially perfect). 

Constraint semantics Each constraint is a propositional formula over scaffold-presence predicates designed to test non-local logical coupling (i.e., XOR/IFF/AND), varying support/coverage (high vs. low vs. near-zero), and correct UNSAT detection. We give an annotated example satisfying molecule for constraint φ3 from our model in Figure 3. Full specifications of constraints {φ1, φ 2, φ 3, φ UNSAT } are provided in Appendix D.3. 

Results Figure 2 shows the sample efficiency under in-creasing restrictive logical constraints. NSGGM achieves greater satisfaction efficiency on φ1 and φ2 within the fixed budget (exceeding 50% efficiency), while neural baselines lag substantially. Constraint φ3 reveals a major distinction: since φ3 has no satisfying molecules in the training set, un-constrained generation with filtering becomes ineffective where baseline models obtain zero satisfying samples within the sampling budget. On the other hand, NSGGM still achieves a non-trivial satisfaction rate (about 25% ) thanks to the exact symbolic enforcement rather than relying on rare hits. Finally, we evaluate only NSGGM on φU N SAT ,7Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints                                    

> Table 5. Hard scaffold-constrained generation results across scaf-fold sets Σ1–Σ3. NSGGM achieves perfect validity and novelty while delivering leading uniqueness. Scaf. Model Validity ↑Uniqueness ↑Novelty ↑
> Σ1 MOLER100.0 93.2 100.0 GEN MOL 82.4 75.1 70.3 NSGGM (Ours) 100.0 98.9 100.0
> Σ2 MOLER100.0 93.1 93.8 GEN MOL 81.5 70.4 60.2 NSGGM (Ours) 100.0 91.5 100.0
> Σ3 MOLER100.0 96.6 100.0 GEN MOL 82.1 83.3 47.8 NSGGM (Ours) 100.0 99.8 100.0

where it produced no satisfying result, consistent transpar-ent, interpretable, and formal constraint checking. Overall, these results showcase the difficulty of enforcing global, interpretable logical structure when sampling alone, and reveals how a SMT-based verification offers a trans-parent and auditable approach in enforcing exact constraint satisfaction, an important property in controlled molecule generation environments where requirements must be stated and checked explicitly. 

4.3. Scaffold-Constrained Generation 

Next, we evaluate hard constrained scaffold generation, where each molecule must contain a fixed scaffold as a connected subgraph or as part of a logical constraint. This can be represented as a hard structural constraint in sym-bolic logic and enforced by our symbolic SMT-solver rather than being approximated via rewards. Unlike unconstrained generation, under hard constraints, the valid output space is fundamentally restricted, so we will focus on constraint satisfaction and exploratory metrics such as Uniqueness and 

Novelty . Similar to the logical constraints evaluation, we compare against M OLER and G EN MOL as they can explic-itly support scaffold / fragment-conditioned decoration. As all baselines and NSGGM natively enforce the provided scaffold, scaffold retention is 100% across all methods, so we omit it from our results and focus on validity, uniqueness, and novelty. We evaluate by generating 1000 molecules for each core scaffolds, specifically Quinoline ( Σ1) (PubChem CID 7047), Phenazine ( Σ2) (PubChem CID 4757), and a large drug-like core scaffold ( Σ3) (PubChem CID 154108) as fixed scaffolds. As shown in Table 5, NSGGM and M OLERachieve 100% validity by construction. NSGGM outper-foms M OLER on uniquness for Σ1 and Σ3 while remain-ing competitive for Σ2. MOLER is a strong baseline as it is scaffold-seeded by design, yet NSGGM still leads in scaffold exploration, which we attribute to the tunable neural-solver balance: more solver dependence increases exploration. 

## 5. Related Work 

Diffusion models have emerged as a dominant paradigm for graph and molecular generation, focusing on uncon-strained generation and task optimization, from continuous formulations that corrupt adjacency/node representations with Gaussian noise and denoising them back to graphs (Niu et al., 2020; Jo et al., 2022) to discrete diffusion that operates directly over categorical node and edge attributes (Vignac et al., 2023; Haefeli et al., 2023). Some recent diffusion works further improve controllability through conditioning and guidance mechanisms by activating property-aware ob-jectives late in the schedule (after scaffold emerges) (Lee et al., 2025; Feng et al., 2025). Concurrently, Haefeli et al. (2023) study unattributed graphs and also find discrete dif-fusion effective. While highly effective for distribution learning, constraints in diffusion-based generators are typi-cally incorporated implicitly, as a result, they generally do not provide formal guarantees for arbitrary user-specified global structural or logical constraints. Beyond diffusion, VAEs, GANs, and flows have been ex-plored (Zhu et al., 2022; Madhawa et al., 2019; Liu et al., 2018b; Luo et al., 2021), but they typically lag strong autore-gressive models (Liao et al., 2019; Mercado et al., 2021) and motif-based methods (Jin et al., 2020a; Maziarz et al., 2022) that encode domain knowledge. While these models can support practical conditioning i.e. M OLER and G EN MOL 

can be seeded/conditioned on structural inputs, their control is expressed through conditioning variable and guidance examples which makes it difficult to (i) explicitly enforce coupled non-local logical statements exactly and (ii) certify and interpret infeasibility for UNSAT specifications. 

## 6. Conclusion 

We introduce Neuro-Symbolic Graph Generative Modeling (NSGGM), a neuro-symbolic framework for molecule gen-eration that combines scaffold/interaction proposals with SMT-based symbolic assembly. This results in strong gen-erative quality while allowing for explicit, interpretable constraints with SAT/UNSAT detection and correctness-by-construction under encoded rules; we also introduce a 

Logical-Constraint Molecular Benchmark to evaluate strict and explicit rule compliance. While explicitly specifying constraints in logic can be fragile and may not capture under-specified preferences, we believe this work is an important step in bringing transparent, auditable control to a field dom-inated by black-box neural generative models, and we hope it motivates future works that better balance flexibility with verification and interpretability. 8Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 

## Impact Statement 

This work introduces NSGGM, a framework that bridges deep learning with formal symbolic reasoning to ensure structural and chemical validity in molecular generation. By ensuring that generated molecules are correct by construc-tion via an SMT solver, NSGGM significantly reduces the risk of AI-generated hallucinations—chemically impossible or unstable structures that often consume substantial labora-tory resources to disprove. This advancement accelerates the pipeline from computational design to experimental valida-tion, potentially lowering the costs and timelines associated with developing life-saving therapeutics. Furthermore, the introduction of the Logical-Constraint Molecular Bench-mark promotes a necessary shift toward verifiable AI in highly regulated sectors such as pharmacology and materi-als science. By allowing researchers to define explicit safety boundaries and enforce strict hard-rule satisfaction, this approach fosters greater trust among domain experts and regulatory bodies, ensuring that AI-driven designs comply with rigorous safety, ethical, and environmental standards. 

## References 

Amann, J., Blasimme, A., Vayena, E., Frey, D., Madai, V. I., and consortium, P. Explainability for artificial intelligence in healthcare: A multidisciplinary perspective. BMC Med-ical Informatics and Decision Making , 20(1):310, Nov 2020. doi: 10.1186/s12911-020-01332-6. URL https: //doi.org/10.1186/s12911-020-01332-6 .Asai, T., Arimura, H., Uno, T., and Nakano, S.-I. Dis-covering frequent substructures in large unordered trees. In International Conference on Discovery Science , pp. 47–61. Springer, 2003. Barrett, C. and Tinelli, C. Satisfiability modulo theories. In Handbook of model checking , pp. 305–343. Springer, 2018. Brockschmidt, M., Allamanis, M., Gaunt, A. L., and Polo-zov, O. Generative code modeling with graphs. In 

7th International Conference on Learning Representa-tions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .OpenReview.net, 2019. URL https://openreview. net/forum?id=Bke4KsA5FX .Brown, N., Fiscato, M., Segler, M. H. S., and Vaucher, A. C. Guacamol: Benchmarking models for de novo molecular design. Journal of Chemical Information and Modeling , 59(3):1096–1108, 2019. doi: 10.1021/acs.jcim. 8b00839. URL https://pubs.acs.org/doi/10. 1021/acs.jcim.8b00839 .Cao, N. D. and Kipf, T. Molgan: An implicit gen-erative model for small molecular graphs. CoRR ,abs/1805.11973, 2018. URL http://arxiv.org/ abs/1805.11973 .Chang, C., Shen, Y., Fan, S., Li, J., Zhang, S., Cao, N., Chen, Y., and Zhang, X. Lamagic: Language-model-based topology generation for analog integrated circuits. In Forty-first International Conference on Machine Learn-ing, ICML 2024, Vienna, Austria, July 21-27, 2024 .OpenReview.net, 2024. URL https://openreview. net/forum?id=MjGCD8wk1k .de Moura, L. M. and Bjørner, N. S. Z3: an efficient SMT solver. In Ramakrishnan, C. R. and Rehof, J. (eds.), 

Tools and Algorithms for the Construction and Analy-sis of Systems, 14th International Conference, TACAS 2008, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2008, Bu-dapest, Hungary, March 29-April 6, 2008. Proceed-ings , volume 4963 of Lecture Notes in Computer Sci-ence , pp. 337–340. Springer, 2008. doi: 10.1007/ 978-3-540-78800-3 \ 24. URL https://doi.org/ 10.1007/978-3-540-78800-3_24 .9Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 

Feng, S., Ni, Y., yan, L., Ma, Z.-M., Ma, W.-Y., and Lan, Y. UniGEM: A unified approach to generation and property prediction for molecules. In The Thirteenth International Conference on Learning Representations ,2025. URL https://openreview.net/forum? id=Lb91pXwZMR .Geng, Z., Xie, S., Xia, Y., Wu, L., Qin, T., Wang, J., Zhang, Y., Wu, F., and Liu, T.-Y. De novo molecular generation via connection-aware motif mining. In The Eleventh International Conference on Learning Representations ,2023. URL https://openreview.net/forum? id=Q_Jexl8-qDi .Haefeli, K. K., Martinkus, K., Perraudin, N., and Watten-hofer, R. Diffusion models for graphs benefit from dis-crete state spaces, 2023. URL https://arxiv.org/ abs/2210.01549 .Jin, W., Barzilay, R., and Jaakkola, T. S. Junction tree variational autoencoder for molecular graph generation. In Dy, J. G. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm ¨assan, Stockholm, Sweden, July 10-15, 2018 , volume 80 of Proceedings of Machine Learning Research , pp. 2328–2337. PMLR, 2018. URL http:// proceedings.mlr.press/v80/jin18a.html .Jin, W., Barzilay, R., and Jaakkola, T. Hierarchical gener-ation of molecular graphs using structural motifs. In 

Proceedings of the 37th International Conference on Machine Learning (ICML) , 2020a. URL https:// arxiv.org/abs/2002.03230 .Jin, W., Barzilay, R., and Jaakkola, T. S. Hierarchi-cal generation of molecular graphs using structural mo-tifs. In Proceedings of the 37th International Confer-ence on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings of Ma-chine Learning Research , pp. 4839–4848. PMLR, 2020b. URL http://proceedings.mlr.press/v119/ jin20a.html .Jo, J., Lee, S., and Hwang, S. J. Score-based generative modeling of graphs via the system of stochastic differ-ential equations. 2022. URL https://arxiv.org/ abs/2202.02514 .Lee, S., Kreis, K., Veccham, S. P., Liu, M., Reidenbach, D., Peng, Y., Paliwal, S., Nie, W., and Vahdat, A. Gen-mol: A drug discovery generalist with discrete diffu-sion. In Proceedings of the 42nd International Con-ference on Machine Learning , 2025. URL https: //openreview.net/pdf?id=KM7pXWG1xj .Liao, R., Li, Y., Song, Y., Wang, S., Nash, C., Hamilton, W. L., Duvenaud, D., Urtasun, R., and Zemel, R. S. Ef-ficient graph generation with graph recurrent attention networks. 2019. URL https://arxiv.org/abs/ 1910.00760 .Liu, Q., Allamanis, M., Brockschmidt, M., and Gaunt, A. L. Constrained graph variational autoencoders for molecule design. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr ´eal, Canada , pp. 7806– 7815, 2018a. URL https://proceedings. neurips.cc/paper/2018/hash/ b8a03c5c15fcfa8dae0b03351eb1742f-Abstract. html .Liu, Q., Allamanis, M., Brockschmidt, M., and Gaunt, A. L. Constrained graph variational autoencoders for molecule design. 2018b. URL https://arxiv.org/abs/ 1805.09076 .Luo, Y., Yan, K., and Ji, S. Graphdf: A discrete flow model for molecular graph generation. 2021. URL https: //arxiv.org/abs/2102.01189 .Madhawa, K., Ishiguro, K., Nakago, K., and Abe, M. Graph-nvp: An invertible flow model for generating molecu-lar graphs. 2019. URL https://arxiv.org/abs/ 1905.11600 .Martinkus, K., Loukas, A., Perraudin, N., and Watten-hofer, R. Spectre: Spectral conditioning helps to over-come the expressivity limits of one-shot graph genera-tors. In Proceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pp. 15159–15202. PMLR, 2022. URL https://proceedings.mlr.press/ v162/martinkus22a.html .Maziarz, K., Jackson-Flux, H. R., Cameron, P., Sirockin, F., Schneider, N., Stiefl, N., Segler, M., and Brockschmidt, M. Learning to extend molecular scaffolds with struc-tural motifs. In International Conference on Learn-ing Representations (ICLR) , 2022. URL https:// openreview.net/forum?id=nS6YyqpTT6y .McKay, B. D. and Piperno, A. Practical graph isomorphism, ii. Journal of symbolic computation , 60:94–112, 2014. Mercado, R., Rastemo, T., Lindel ¨of, E., Klambauer, G., Engkvist, O., Chen, H., and Bjerrum, E. J. Graph net-works for molecular design. Mach. Learn. Sci. Technol. ,2(2):25023, 2021. doi: 10.1088/2632-2153/ABCF91. URL https://doi.org/10.1088/2632-2153/ abcf91 .10 Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 

Niu, C., Song, Y., Song, J., Zhao, S., Grover, A., and Ermon, S. Permutation invariant graph generation via score-based generative modeling. 2020. URL https://arxiv. org/abs/2003.00638 .Polykovskiy, D., Zhebrak, A., Sanchez-Lengeling, B., Golovanov, S., Tatanov, O., Belyaev, S., Kurbanov, R., Artamonov, A., Aladinskiy, V., Veselov, M., Kadurin, A., Johansson, S., Chen, H., Nikolenko, S., Aspuru-Guzik, A., and Zhavoronkov, A. Molecular sets (moses): A benchmarking platform for molec-ular generation models. Frontiers in Pharmacol-ogy , 11:565644, 2020. doi: 10.3389/fphar.2020. 565644. URL https://www.frontiersin.org/ articles/10.3389/fphar.2020.565644 .Preuer, K., Renz, P., Unterthiner, T., Hochreiter, S., and Klambauer, G. Fr ´echet chemnet distance: A met-ric for generative models for molecules in drug dis-covery. Journal of Chemical Information and Model-ing , 58(9):1736–1741, 2018. doi: 10.1021/acs.jcim. 8b00234. URL https://doi.org/10.1021/acs. jcim.8b00234 . PMID: 30118593. PubChem CID 154108. Dibenzo(c,f)(2,7)naphthyridine [cid 154108]. https://pubchem.ncbi.nlm.nih. gov/compound/154108 . Accessed: 2026-01-29. PubChem CID 4757. Phenazine [cid 4757]. 

https://pubchem.ncbi.nlm.nih.gov/ compound/Phenazine. Accessed: 2026-01-29. PubChem CID 7047. Quinoline [cid 7047]. 

https://pubchem.ncbi.nlm.nih.gov/ compound/Quinoline. Accessed: 2026-01-29. Tarjan, R. Depth-first search and linear graph algo-rithms. SIAM Journal on Computing , 1(2):146–160, 1972. doi: 10.1137/0201010. URL https://doi.org/10. 1137/0201010 .Vignac, C., Krawczuk, I., Siraudin, A., Wang, B., Cevher, V., and Frossard, P. Digress: Discrete denoising dif-fusion for graph generation. In The Eleventh Interna-tional Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023. URL https://openreview.net/forum? id=UaAD-Nu86WX .Wang, Z., Shi, J., Heess, N., Gretton, A., and Titsias, M. Learning-order autoregressive models with ap-plication to molecular graph generation. In Forty-second International Conference on Machine Learning ,2025. URL https://openreview.net/forum? id=EY6pXIDi3G .Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Ge-niesse, C., Pappu, A. S., Leswing, K., and Pande, V. Moleculenet: A benchmark for molecular machine learn-ing, 2018. URL https://arxiv.org/abs/1703. 00564 .Xu, Z., Qiu, R., Chen, Y., Chen, H., Fan, X., Pan, M., Zeng, Z., Das, M., and Tong, H. Discrete-state continuous-time diffusion for graph generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024. URL https://openreview.net/ forum?id=YkSKZEhIYt .Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W., Cui, B., and Yang, M. Diffusion mod-els: A comprehensive survey of methods and applica-tions. ACM Comput. Surv. , 56(4):105:1–105:39, 2024. doi: 10.1145/3626235. URL https://doi.org/10. 1145/3626235 .Zhu, Y., Du, Y., Wang, Y., Xu, Y., Zhang, J., Liu, Q., and Wu, S. A survey on deep graph generation: Methods and applications. 2022. URL https://arxiv.org/ abs/2203.06714 .11 Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 

## A. Further Decomposition of Acyclic Components 

Algorithm 1 Refine Acyclic Components into Tree Motifs 

Input: Graph G = ( V, E ), acyclic edge set EA, minimum sup-port τ

Output: Tree-motif tokens Ttree , residual fragments Rtree  

> 1

Function RefineAcyclic( G, E A, τ ) 

> 2

GA ← (V, E A) // Acyclic remainder subgraph  

> 3

C ← ConnectedComponents (GA) Ttree ← ∅, Rtree ← ∅ 

> 4

foreach C ∈ C do 

// (1) BC decomposition and optional path compression  

> 5

(B, A) ← BlockCutDecompose (C) // Blocks 

B and articulation points A 

> 6

B ← {CompressDegTwoChains (B) | B ∈B} // Contract degree-2 chains within blocks // (2) Subtree enumeration and canonicalization  

> 7

S ← S 

> B∈B

EnumerateSubtrees (B)

// Enumerate candidate subtrees (bounded depth/size)  

> 8

K ← {CanonicalizeSubtree (S) | S ∈S} // Canonical forms for isomorphism handling // (3) Support counting (frequent subtree mining)  

> 9

supp (·) ← CountSupport (K, over all C′ ∈ C)

F ← { K ∈ K | supp (K) ≥ τ } // Frequent patterns // (4) Tokenization and interface slots  

> 10

foreach K ∈ F do  

> 11

g ← RepresentativeSubtree (K) σV , σ E ←

ComputeSlots (g) // Node/edge slots for interfaces  

> 12

Ttree ← T tree ∪ { AssembleToken (g, σ V , σ E )}

// (5) Residuals: pieces not covered by frequent motifs  

> 13

U ← MaximalUncoveredFragments (C, F)

// after covering by F 

> 14

Rtree ← R tree ∪ U // minimal trees /single edges  

> 15

return Ttree , Rtree 

Each primitive gi ∈ P (G) is a subgraph with node set Vi

and edge set Ei. For the acyclic part, we refine connected components into recurring tree motifs using standard steps: 1. BC decomposition: Decompose at articulation points via a block–cut (BC) decomposition (Tarjan, 1972). 2. Path compression: Optionally compress degree-2 chains (a standard tree reduction). 3. Frequent subtree mining: Canonicalize subtrees and select frequent patterns by minimum support, follow-ing frequent (sub)tree mining frameworks (Asai et al., 2003). 4. Canonical labeling: Use canonical labeling for iso-morphism handling (McKay & Piperno, 2014). Residual fragments that do not meet the support threshold remain as minimal trees (or single edges). 

## B. The NSGGM Method 

B.1. Proofs Proposition B.1 (Legitimacy of Structural Partitioning) .

Let G = ( V, E ) be any finite simple graph and let C(G) = 

{c1, . . . , c p} be a minimum cycle basis with edge sets E(cj ).Define 

EC =

> p

[

> j=1

E(cj ), EA = E \ EC , GA = ( V, E A).

Let P(G) = C(G) ∪ Components( GA) and enumerate 

P(G) = {Pi = ( Vi, E i)}mi=1 . Then: 

(i) {Pi} is an overlapping cover of G: S 

> i

Vi = V and S 

> i

Ei = E.

(ii) Every Pi is either a simple cycle cj or an induced connected acyclic subgraph (a tree component of GA). 

(iii) If v ∈ Vi ∩ Vj or e ∈ Ei ∩ Ej for i̸ = j, then the overlap corresponds to a genuine shared item of G

(i.e., no spurious duplication). 

(iv) The blueprint SG = {ti}mi=1 with tokens ti =(Pi, σ V |Vi , σ E |Ei ) is well-defined; in particular, the slot maps σV , σ E can be computed deterministically from G and P(G).Proof. (i) By construction, E = EC ∪ EA and the union is disjoint. The family {E(cj )}j covers EC , while 

Components( GA) covers EA. Hence S 

> i

Ei = E. Since every edge’s endpoints are included, S 

> i

Vi = V follows. 

(ii) Each cj ∈ C (G) is a simple cycle by definition of a cycle basis. Deleting EC breaks all remaining cycles, so 

GA = ( V, E A) is acyclic; its connected components are trees. 

(iii) If an item of G appears in two primitives, it is because that item is simultaneously part of a cycle and of another cycle (fused cycles) or it lies on the interface between a cycle and a tree component (or between fused cycles). In all cases, the overlap is an actual vertex/edge of G present in both subgraphs, not an artifact of the construction. 12 Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 

(iv) The slot assignments σV , σ E are functions of local degrees in G and in each primitive, plus the fused/non-fused tag for cycle primitives (Section 3.1). Since these are determined from (G, P(G)) with no ambiguity, ti is uniquely defined for each Pi.

Corollary B.2 (Canonical Witness for Reassembly) . Let 

SG be the blueprint of G from Proposition B.1. There exists an assignment {zv }v∈Vslots (take zv equal to the original global vertex ID of v) such that all hard constraints ϕhard in Section 3.2 are satisfied. Thus, every training graph admits a feasible assembly consistent with its own blueprint. Proof. Assign each local copy v ∈ Vi the label zv := id G(v) given by its vertex in G. Subgraph integrity holds because distinct local copies within the same primitive cor-respond to distinct vertices. Edge–slot matching holds since each shared edge of G appears with the same end-points across primitives. Type-equality is immediate be-cause copies refer to the same underlying item of G. Hence 

ϕhard is satisfied. 

Theorem B.3 (Soundness of Assembly Under ϕhard ).

Fix a blueprint S′ = {t1, . . . , t k} and any assignment 

{zv }v∈Vslots that satisfies all hard constraints ϕhard of Sec-tion 3.2. Define an equivalence relation on slot-copies by 

v ∼ v′ ⇐⇒ zv = zv′ and let [v] denote its classes. Con-struct a graph G′ = ( V ′, E ′) with V ′ = {[v] : v ∈ Vslots }

and with edges induced from primitive edges subject to edge–slot matching. Then: 

(i) G′ is a well-defined simple graph (no self-loops or parallel edges are introduced by merging). 

(ii) The embedding of each primitive Pi into G′ is injective on its vertices and edges (subgraph integrity). 

(iii) If two local copies are merged, their discrete type fields match exactly (type consistency). If, in addition, S′ is molecular and the molecular hard clauses hold (element consistency, shared-edge order con-sistency, and exact valency balance), then: 

(iv) Every merged vertex in G′ has element type well-defined and unique. 

(v) For each atom u ∈ V ′, the total bond order of edges in-cident to u equals its prescribed valency; in particular, no deficit or surplus valence occurs. 

(vi) Any edge e occurring in multiple primitives has a single bond order in G′.Consequently, G′ is a topologically valid graph; in the molecular case it is chemically valid by construction. Proof. (i) Define V ′ as the quotient by ∼. By subgraph in-tegrity , two distinct vertices of the same primitive never merge, so a primitive cannot collapse onto itself. By 

edge–slot matching , whenever a structural edge is realized by merging endpoints from two primitives, its endpoints map to distinct equivalence classes (no self-loop). Parallel edges cannot arise because matched edge-slots are required to pairwise realize the same underlying adjacency; any du-plicate realization would violate the matching condition. 

(ii) Subgraph integrity is precisely the clause zva̸ = zvb for distinct va, v b of the same primitive; thus the primitive’s vertex set injects into V ′ and its edges map injectively into 

E′.

(iii) This is exactly the type-equality constraint: zvi =

zvj ⇒ σtype  

> V

(vi) = σtype  

> V

(vj ).

(iv)–(vi) Molecular case. Element consistency gives a unique element type per class [v]. Shared-edge order consis-tency ensures any edge e appearing in overlaps has a unique bond order. Finally, valency balance enforces 

X

> w: ([ v],[w]) ∈E′

bo([ v], [w]) = R(v)

for each class [v], where R(v) is the total residual con-tributed by all local copies of that atom. Since residuals are defined as val( v)− (intramolecular bond order already inside primitives), this equality guarantees that inter-token bonds exactly saturate the valence: neither deficits nor over-saturation can occur. 

Corollary B.4 (Correctness by Construction) . If S′ is ob-tained from a valid graph G via the decomposition of Sec-tion 3.1, then by Corollary B.2 there exists a satisfying as-signment of ϕhard that reconstructs a graph G⋆ isomorphic to G. Conversely, by Theorem B.3, any satisfying assign-ment yields a valid G′; in the molecular setting, G′ also satisfies element consistency and valency exactly. Hence the hard constraints are sound (no invalid outputs) and com-plete for reassembling training graphs (every training graph has a satisfying witness). 

B.2. Constraints Hard constraints ( ϕhard ): Below are the formal hard con-straints we enforce: 

(i) Subgraph integrity. Nodes from the same subgraph/token never merge. That is, ∀u, v, mu,v =⇒ u ∈ Vi, v ∈ Vj

for some i̸ = j

(ii) Element consistency. Merged atoms must have equal element types: 

∀u, v : mu,v ⇒ elem( u) = elem( v).

13 Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 

(iii) Valency cap. Let the internal (within-primitive) bond-order degree of an atom u ∈ Vi be 

deg i(u) = X

> (u,x )∈Ei

bo( u, x ).

If two node-slot atoms u ∈ V slot  

> i

and v ∈ V slot  

> j

are merged via mu,v , then the resulting atom cannot exceed the valency cap for its element: 

mu,v ⇒ deg i(u) + deg j (v) ≤ cap(elem( u)) .

(Here cap( ·) is the allowed maximum valence for each ele-ment under the dataset’s chemistry conventions.) 

(iv) Edge-slot merges (fused/shared edges). Edge sharing is only allowed for edges in Eslot  

> i

and Eslot  

> j

. Two edge-slot copies e = ( u, v ) ∈ Eslot  

> i

and e′ = ( u′, v ′) ∈ Eslot  

> j

, i ̸ = j

may be merged only if they (i) have the same bond order, and (ii) their endpoints are merged consistently: 

bo( u, v ) = bo( u′, v ′) ∧  mu,u ′ ∧mv,v ′ ∨ mu,v ′ ∧mv,u ′

.

When such an edge merge occurs, we enforce endpoint element equality and valency caps accounting for the dupli-cated edge :

(mu,u ′ ∧ mv,v ′ ) ⇒

elem( u) = elem( u′),

deg i(u) + deg j (u′) − bo( u, v ) ≤ cap(elem( u)) ,

deg i(v) + deg j (v′) − bo( u, v ) ≤ cap(elem( v)) .

The swapped orientation (mu,v ′ ∧ mv,u ′ ) uses the same constraints. This subtraction of bo( u, v ) removes double-counting of the shared bond: it appears once in each primitive but becomes a single edge in G′.

(v) Edge-slot restriction. Edge merges are permitted only for edge-slot edges: 



(u, v ) /∈ Eslot 

> i



∨



(u′, v ′) /∈ Eslot 

> j



⇒¬



(mu,u ′ ∧ mv,v ′ ) ∨ (mu,v ′ ∧ mv,u ′ )



.

In particular, an edge may be shared even if its endpoints are not node slots, provided the edge itself lies in Eslot .

B.3. The NSGGM algorithms and constraints 

This appendix gives concise, implementation-ready pseu-docode for training and inference. 

Soft Constraints For Connectivity: Define par [i] as the parent identified for subgraph token i in S′. Define 

active i = T rue if the ith subgraph token is used in the final molecule, that is, any mu,v = T rue where u ∈ Vi.To encourage connectivity, we use hard constraints to define a valid reachability structure; actual connectivity is encouraged only via soft rewards. Introduce Boolean reachability vars reach i, root indicators 

isRoot i, and integer distances dist i Add hard constraints: if any active i then exactly one root; roots are active and have dist = 0 Add hard constraints: active i ⇒ reach i

and every reachable non-root i has a reachable parent j

with conn i,j and dist i = dist j + 1 Add soft rewards to O

favoring larger connected assemblies: 

max X

> i

wreach (i) · 1[reach i] + X

> i<j

wconn · 1[conn i,j ],

and if parents par [i] are given, add an extra reward for 

conn i,par [i].

Algorithm 2 SMT-based assembly of a blueprint 

Input: Blueprint S′ = {Pi}ki=1 ; optional ϕuser 

Output: Selected merges {mu,v } 

> 16

Function ASSEMBLE SMT (S′, ϕ user ) 

> 17

O ← Z3O PTIMIZE () ; M ← ∅ // M maps candidate pairs to Booleans mu,v 

// (1) Candidate merges + local validity clauses  

> 18

M ← AddNodeMergeCandidates (S′, O)

M ← M ∪ AddEdgeSlotCandidates (S′, O)

// Forced Merges/Structures/Rules from user  

> 19

foreach user rule r ∈ ϕuser do  

> 20

Identify the set of required atom pairs F(r) induced by r

prefix r ⇔ V 

> (u,v )∈F (r)

mu,v  

> 21

if ϕuser provides logical constraints then  

> 22

O ← parseLogic(constraint , prefix i) // where parselogic recursively parses the prefixes and S’ required for logical constraints and adds corresponding clause for solver between them  

> 23

foreach subgraph i ∈ { 1, . . . , k } do  

> 24

Define active i ⇔ W{mu,v ∈ M : u ∈ Pi or v ∈ Pi}

// Connectivity (reachability) + soft objective defined in B.3 // Solve and decode  

> 25

Solve O (Max-SMT) return return all true mu,v 

Practical notes. (i) Masking : BuildFeasibilityMasks pre-computes per-position masks using only local interface checks (degree stubs, fused-cycle tuples), which is linear in the number of candidate tokens at each step. (ii) User con-trol : ϕuser may add hard clauses (e.g., ring-count bounds, forbidden motifs) or weighted soft terms that are optimized in the final SMT objective without retraining Mθ . (iii) 14 Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 

Caching : Since ϕstruct depends only on V, its encoding can be cached across runs. 

B.4. Training ELBO. We weight the contributions of the autoregres-sive token decoder and the three structural heads by 

λtok , λ π , λ m, λ meta , and weight the KL regularizer by β.

LELBO = Ez∼qϕ(z|G )

Rλ(z)| {z }

> generator loss

− β KL( qϕ ∥ pϕ)

| {z }

> latent alignment

, (9) Here KL( qϕ ∥ p) denotes 

KL( qϕ(z | G ) ∥ p(z)) , with p(z) = N (0 , I ).

, and the weighted reconstruction term is 

Rλ(z) = λtok Ltok (z)

| {z }

> AR tokens

+ λπ Lπ (z)

| {z }

> parent pointers

+ λmLm(z)

| {z }

> merge/interface

+ λmeta Lmeta (z)

| {z }

> metadata

.

(10) The per-head log-likelihoods are 

L{tok ,π,m, meta }(z) = 

> L

X

> ℓ=1

log pθ (xℓ | cℓ), (11) where 

(xℓ, c ℓ) ∈



(gℓ, (g<ℓ , z )) ,

(πℓ, (T , z )) ,

(mℓ, (T , z )) ,

(tℓ, (T , z )) .

(12) 

## C. Dataset details 

C.1. Molecular graph datasets 

QM9 (Wu et al., 2018), MOSES and GuacaMol are molecu-lar datasets, where nodes represent atoms and edges corre-spond to bonds. Planar dataset (Martinkus et al., 2022) is a non-molecular graph. 

QM9. QM9 is a supervised, property-rich benchmark of small organic molecules (up to nine heavy atoms among 

{C,N,O,F }), each paired with a single low-energy 3D con-formation and a suite of quantum-chemical targets (e.g., dipole moment, polarizability, energies). Its compact chem-ical space and dense labels make it ideal for studying mes-sage passing, geometry-aware encoders, and multi-task re-gression. QM9’s strength is label depth (node/edge/3D infor-mation and many targets), not breadth of chemotypes; it is less suitable for evaluating large-scale distribution learning. 

MOSES. MOSES is a large, cleaned corpus for gener-ative modeling drawn from drug-like ZINC subsets with standardized filters (e.g., MW, logP, allowed atom types). It ships with train/test splits and a unified metric suite (valid-ity, uniqueness, FCD, KL, internal diversity), encouraging apples-to-apples comparison across unconditional genera-tors. Unlike QM9, MOSES prioritizes scale and distribu-tional realism over supervised labels; molecules are pro-vided primarily as SMILES without per-atom/bond targets. 

GuacaMol. GuacaMol is a broad, medicinal-chemistry benchmark that evaluates both distribution learning and goal-directed generation (e.g., multi-objective property op-timization, rediscovery). It emphasizes realistic scaffold diversity and challenge tasks rather than supervised labels, providing standardized splits, metric implementations, and leaderboards. Practically, the dataset includes many chemi-cally complex structures. 

## D. Experimental Details 

D.1. Unconstrained Generation Details. Hardware. Unconstrained generation experiments were ran on CPU-based compute only. Specifically, a Google Colab CPU runtime (Google Compute Engine VM) with an AMD EPYC 7B12 CPU (8 vCPUs) and 50 GB RAM, running Linux 6.6.105. 

Throughput. Below (Table 6) we report the throughput on each benchmark, rounded to the nearest 10 molecules per second due to run-to-run variability.        

> Table 6. Inference throughput (mol/s) on the CPU runtime. Values are rounded to the nearest 10 mol/s.
> Metric QM9 GuacaMol MOSES
> Inference (mol/s) 20 40 100

D.2. Unconstrained Generation Examples. D.3. Logical Constraints Satisfaction Specifications Overview. We define formulations that evaluate global logical constraints over substructure presence in candidate molecules. Each instance is a propositional formula where each atom correspond to whether a molecule contains the corresponding scaffold. The tests contain: (i) set of drug-15 Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints   

> Figure 4. Example outputs from QM9 implicit. Samples were randomly chosen.
> Figure 5. Example outputs from MOSES. Samples were randomly chosen.

like scaffolds (6–7 distinct pieces per formula) while re-maining non-trivial, (ii) genuinely global/non-local logic via XOR parity, IFF statements, implications with compos-ite antecedents, and conditional negations, (iii) ”sequential picking” behavior, where scaffold choice constrains com-patible substituent families, and finally (iv) four instances: two moderately difficult satisfiable, higher-coverage con-straints; one difficult and satisfiable but near-zero coverage on broad drug-like scaffolds; and one pure-logic UNSAT sanity check. We emphasize that these logical expressions are not intended as medicinal-chemistry specifications or direct applicable drug-discovery constraints. These serve as controlled, synthetic, stress tests for evaluating our model.  

> Figure 6. Example outputs from GuacaMol. Samples were ran-domly chosen.

Objects: pieces, atoms, and expression language. A

piece is a named scaffold which can be composed by some 

motifs , g1, ..., g n, where gi ∈ V . Let S = {s1, . . . , s n} de-note the set of pieces. Each piece si induces a Boolean atom 

xi which is interpreted as “the output molecule, G′, con-tains si.“ Formulas are built over these atoms using Boolean operators: negation ¬, conjunction ∧, and disjunction ∨.No cardinality primitives are assumed; all “exactly-one” constraints are expressed compositionally (via XOR). 

Core scaffolds. The core scaffolds are single-ring, non-fused heteroaromatics: • Pyridine : ( PubChem CID 1049 )• Pyrimidine (PubChem CID 9260 )• Imidazole : ( PubChem CID 795 )• Thiazole :( PubChem CID 9256 )

Functional motifs (branches). The functional motifs are compact, generic patterns. Their corresponding SMILES string is noted below: • Nitrile : C#N 

• Trifluoromethyl : C(F)(F)F 

• Amide : C(=O)N 

• Sulfonamide : S(=O)(=O)N 

• Tert-Butyl group : CC(C)(C)C 

16 Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints 

Logical operators macros. We define the following oper-ators over Boolean expressions a, b :

IMP( a, b ) := ¬a ∨ b (13) 

XOR2( a, b ) := ( a ∨ b) ∧ ¬ (a ∧ b) (14) 

IFF( a, b ) := IMP( a, b ) ∧ IMP( b, a ). (15) All benchmarks below are expressed using only ¬, ∧, ∨ plus these macros. 

Assignment mapping and expression satisfaction. 

Given a candidate molecule m and piece set S, we map 

m to a Boolean assignment αm = ( x1(m), . . . , x n(m)) 

through its presence in the final molecule i.e. 

xi(m) = 1 ⇐⇒ m contains the substructure query si.

A molecule m satisfies a benchmark formula φ if φ(αm) = True . Then, we state m |= φ.

Expression 1 (φ1):Scaffolds used (7): Pyridine (P ), Pyrimidine (Q), Nitrile 

(N ), Trifluoromethyl (F ), Amide (A), Sulfonamide (S), Tert-Butyl (T ). We define three exactly-one selections: 

CORE12 X := XOR2( P, Q ), (16) 

EWG X := XOR2( N, F ), (17) 

POL X := XOR2( A, S ). (18) Full expression: 

φ1 := CORE12 X ∧ EWG X ∧ POL X 

∧ IMP( P, N ∨ (F ∧ T )) 

∧ IMP( Q, F ∨ (N ∧ A)) 

∧ IMP( T, S )

∧ IMP( A, ¬T ). (19) Intuitively, scaffold choice gates feasible EWG/polar modes, and bulk steers the polar choice which results in global coupling despite the small number of atoms. 

Expression 2 (φ2) : 

Scaffolds used (6): Imidazole ( I), Thiazole ( H), Nitrile (N ), Amide ( A), Sulfonamide ( S), tert -Butyl ( T ). This expression enforces exactly-one core and exactly-one polar: 

CORE34 X := XOR2( I, H ), (20) 

POL2 X := XOR2( A, S ). (21) Full expression: 

φB2 := CORE34 X ∧ POL2 X 

∧ IFF( T, S )

∧ IMP( A, N )

∧ IMP( H, A ∧ ¬ T )

∧ IMP( I, S ∨ N ) . (22) This instance pushes on IFF wiring and implications with composite consequents. 

Expression 3 (φ3): Scaffolds used (7): Pyridine (P ), 

Pyrimidine (Q), Thiazole (H), Nitrile (N ), Trifluoromethyl 

(F ), Amide (A), Sulfonamide (S). This expression requires the presence of three cores simultaneously, then select ex-actly one EWG and one polar, and finally lock consistent modes: 

φB3 := ( P ∧ Q ∧ H)

∧ XOR2( N, F ) ∧ XOR2( A, S )

∧ IFF( A, N ) ∧ IFF( S, F ). (23) Although it is logically satisfiable, the multi-core conjunc-tion is intended to have a low hit rate in the training dataset. 

Expression 4 (UNSAT) (φU N SAT ) : 

Scaffolds used (3): Pyridine ( P ), Pyrimidine ( Q), Imidazole (I). The expression is: 

φUNSAT := XOR2( P, Q ) ∧ XOR2( Q, I ) ∧ XOR2( P, I ).

(24) This formula is unsatisfiable as pairwise XOR constraints among three variables cannot hold simultaneously. This serves as a sanity check of the logical layer. 

Experiment chemistry constraints. Across all experi-ments, we apply a valency-only feasibility check. Bonds contribute their nominal order to the atom valence. Aro-matic bonds are counted as 1.5. Default atomic valence limits follow common organic chemistry conventions (i.e, H = 1, C = 4, N = 3 / 5 with charge, ...). Formal charges adjust the allowed valence accordingly. 

Transformer decoder sampler. For each dataset, we trained the sampler for 20 epochs using the Adam opti-mizer (learning rate 5 × 10 −3, batch size 128). The model is a lightweight GPT-style decoder with four layers, four atten-tion heads, and 128 hidden dimensions. At inference time, we employed diverse beam search with nucleus sampling (top-p = 0.99) and top-k sampling (k = 1000). A global frequency of 1.0 is applied to discourage over-sampling frequent fragments. 

## E. LLM Usage 

Large Language Models (LLMs) were used as a general-purpose assistive tool in the preparation of this work. Specif-ically, LLMs supported tasks such as refining the clarity of writing, suggesting alternative phrasings, and checking the consistency of technical terminology. They were not used for generating research ideas, conducting experiments, or 17 Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints  

> Table 7. Unconditional generation on SBM and planar graphs. V.U.N.: valid, unique & novel graphs.

Model Deg ↓ Clus ↓ Orb ↓ V.U.N. ↑

Stochastic block model 

GraphRNN 6.9 1.7 3.1 5% GRAN 14.1 1.7 2.1 25% GG-GAN 4.4 2.1 2.3 25% SPECTRE 1.9 1.6 1.6 53% ConGress 34.1 3.1 4.5 0% DiGress 1.6 1.5 1.7 74% NSGGM (ours) 1.3 1.5 1.9 77% 

Planar graphs 

GraphRNN 24.5 9.0 2508 0% GRAN 3.5 1.4 1.8 0% SPECTRE 2.5 2.5 2.4 25% ConGress 23.8 8.8 2590 0% DiGress 1.4 1.2 1.7 75% 

NSGGM (ours) 1.3 1.1 2.0 72% producing original scientific contributions. All substantive research decisions, analysis, and results presented in this paper are the responsibility of the authors. The authors have carefully reviewed and verified all LLM-assisted text to ensure accuracy and originality. 

## F. Non-Molecular Graph Dataset 

The non-molecular benchmark of Martinkus et al. (2022) consists of two datasets of 200 graphs: (a) stochastic block models (SBM; up to 200 nodes) and (b) planar graphs (64 nodes). Following the protocol, we assess how well gen-erated graphs match degree distributions (Deg), clustering coefficients (Clus), orbit counts (Orb), and the proportion of valid, unique, and novel graphs (V.U.N.). NSGGM is used in an unconditional setting. The sam-pler proposes discrete construction sequences, which the symbolic solver assembles under task-specific constraints (e.g., simple connectivity for SBM and planarity for planar graphs). On SBM , NSGGM attains the lowest Deg error (1.3) and ties the best Clus (1.5), while remaining compet-itive on Orb (1.9), yielding the highest V.U.N. (77%). On 

Planar , NSGGM achieves the best Deg (1.3) and Clus (1.1) and competitive Orb (2.0), with V.U.N. 72%—close to Di-Gress (75%)—indicating strong fidelity to planar structure with minimal loss in novelty. These outcomes illustrate that explicit, verifiable constraints during assembly can improve structural metrics without sacrificing diversity. 18