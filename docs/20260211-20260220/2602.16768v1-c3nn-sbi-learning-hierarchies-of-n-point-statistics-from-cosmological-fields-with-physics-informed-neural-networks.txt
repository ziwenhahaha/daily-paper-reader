Title: C3NN-SBI: Learning Hierarchies of $N$-Point Statistics from Cosmological Fields with Physics-Informed Neural Networks

URL Source: https://arxiv.org/pdf/2602.16768v1

Published Time: Fri, 20 Feb 2026 01:02:15 GMT

Number of Pages: 17

Markdown Content:
> Draft version February 20, 2026
> Typeset using L ATEX twocolumn style in AASTeX7.0.1

C3NN-SBI: Learning Hierarchies of N -Point Statistics from Cosmological Fields with Physics-Informed Neural Networks 

Kai Lehman, 1, 2, 3, ∗ Zhengyangguang Gong, 1, 4, 5, ∗ David Gebauer, 4, 6 Stella Seitz, 1, 4 and Jochen Weller 1, 3, 4 

> 1Universit¨ ats-Sternwarte M¨ unchen, Fakult¨ at f¨ ur Physik, Ludwig-Maximilians-Universit¨ at, Scheinerstr. 1, 81679 M¨ unchen, Germany
> 2Center for Computational Astrophysics, Flatiron Institute, 162 5th Avenue, New York, NY, 10010, USA
> 3Excellence Cluster ORIGINS, Boltzmannstr. 2, 85748 Garching, Germany
> 4Max-Planck-Institut f¨ ur extraterrestrische Physik, Giessenbachstr. 1, 85748 Garching, Germany
> 5Steward Observatory, University of Arizona, 933 North Cherry Avenue, Tucson, AZ 85721, USA
> 6Fakult¨ at f¨ ur Physik, Universit¨ at Bielefeld, Postfach 100131, 33501 Bielefeld, Germany

ABSTRACT Cosmological analyses are moving past the well understood 2-point statistics to extract more infor-mation from cosmological fields. A natural step in extending inference pipelines to other summary statistics is to include higher order N-point correlation functions (NPCFs), which are computationally expensive and difficult to model. At the same time it is unclear how many NPCFs one would have to include to reasonably exhaust the cosmological information in the observable fields. An efficient alternative is given by learned and optimized summary statistics, largely driven by overparametriza-tion through neural networks. This, however, largely abandons our physical intuition on the NPCF formalism and information extraction becomes opaque to the practitioner. We design a simulation-based inference pipeline, that not only benefits from the efficiency of machine learned summaries through optimization, but also holds on to the NPCF program. We employ the heavily constrained Cosmological Correlator Convolutional Neural Network (C3NN) which extracts summary statistics that can be directly linked to a given order NPCF. We present an application of our framework to simulated lensing convergence maps and study the information content of our learned summary at various orders in NPCFs for this idealized example. We view our approach as an exciting new avenue for physics-informed simulation-based inference. 

Keywords: Astrostatistics techniques (1886) — Astrostatistics tools (1887) — Convolutional neural networks (1938) — Cosmological parameters from large-scale structure (340) 

1. INTRODUCTION Extracting maximal cosmological information from current (G. J. Hill et al. 2008; J. T. A. de Jong et al. 2013; K. S. Dawson et al. 2013; M. Levi et al. 2013; L. E. Bleem et al. 2015; S. Miyazaki et al. 2015; Dark En-ergy Survey Collaboration et al. 2016; G.-B. Zhao et al. 2016; P. Predehl et al. 2021) and upcoming ( LSST Sci-ence Collaboration et al. 2009; R. Laureijs et al. 2011; R. S. de Jong et al. 2012; M. Takada et al. 2014; R. Maartens et al. 2015; D. Spergel et al. 2015) large-scale structure surveys increasingly requires moving beyond the standard 2-point correlation function (2PCF, or its Fourier counterpart the power spectrum) framework (  

> Email: kai.lehman@physik.lmu.de, zgong@arizona.edu
> ∗Equal Contribution; Corresponding authors

The Beyond-2pt Collaboration et al. 2025). Two gen-eral strategies have emerged as promising approaches. The first holds on to the use of summary statistics, both analytically motivated such as R. Scoccimarro & H. M. P. Couchman (2000); E. Sefusatti et al. (2007); J. M. Kratochvil et al. (2012); J. Fluri et al. (2018); A. Halder et al. (2021); Z. Gong et al. (2023); Y. Wang & P. He (2025), and machine learning optimized (e.g. T. Charnock et al. (2018); N. Jeffrey et al. (2021); P. Lemos et al. (2023)) 7. These approaches aim to compress non-Gaussian information into low-dimensional statistics.  

> 7The ideas of such compression methods for use in Approximate Bayesian Computation (D. B. Rubin 1984; M. A. Beaumont et al. 2002; J. Akeret et al. 2015) go back to M. G. B. Blum & O. Francois (2008); P. Fearnhead & D. Prangle (2010); M. G. B. Blum et al. (2012); B. Jiang et al. (2015)
> arXiv:2602.16768v1 [astro-ph.CO] 18 Feb 2026

2Recent comparisons (D. Lanzieri et al. 2025) have shown that summary-based methods can capture substantial amount of information beyond the 2-point level, partic-ularly when higher-order statistics are included. As an alternative to the use of summary statistics, field-level inference has gained attention in which analyses are per-formed directly on the full density field rather than on compressed summaries (N. Porqueres et al. 2023; A. J. Zhou et al. 2024; N.-M. Nguyen et al. 2024; F. Leclercq 2025). Such approaches can, in principle, access the complete information content of the data, including all higher-order correlation functions encoded in the field. Both strategies have demonstrated impressive gains. Summary-based approaches can recover large amounts of previously inaccessible information (D. Lanzieri et al. 2025), while field-level methods have shown even larger potential improvements in constraining power (N.-M. Nguyen et al. 2024). However, each method comes with significant limitations. Field-level inference typically re-quires differentiable forward models, repeated simula-tions, and computationally expensive frameworks with high-dimensional sampling algorithms for the initial con-ditions, making it a challenging task to scale to realistic survey volumes on extremely high dimensional parame-ter spaces. Due to the computational expense and the inherent fidelity of such analyses, the use of summary statistics is still of value as a cheaper and more robust al-ternative. Conversely, optimized summary statistics, es-pecially those produced by neural networks, often suffer from limited interpretability, as the learned compression is a high dimensional non linear function with a large number of learned parameters. Overfitting is another concern, especially given the imperfect nature of cosmo-logical simulations, one would like to produce summary statistics which are not contaminated by learned arti-facts of a given simulation. To address these limitations, C. Miles et al. (2021) in-troduced the Correlator Convolutional Neural Network (CCNN) architecture in the context of quantum matter, imposing structural constraints on convolution kernels such that each learned summary can be expressed ex-plicitly as a weighted sum of NPCFs at a given order. This yields a transparent connection between network output and analytically interpretable statistics, bridging the gap between interpretability and flexibility. Build-ing on this idea, Z. Gong et al. (2024) adapted and ex-tended the CCNN architecture for cosmological appli-cations, and developed the so called Cosmological Cor-relator Convolutional Neural Network (C3NN). Their advances included: 1. Enforcing rotational and translational invariance appropriate for cosmological density fields, thus encoding the cosmological principles of homogene-ity and isotropy into the architecture. 2. Implementing an auxiliary algorithm (feature se-lection process) that can hierarchically distinguish contributions from different orders of NPCFs to a given binary classification task. 3. As a proof of concept, demonstrating that higher-order statistics, particularly bispectrum-related information learned by C3NN, significantly im-prove the discrimination between Gaussian and lognormal fields as well as simulated weak lensing convergence maps with different underlying equa-tion of state parameter w0 of dark energy, proving that C3NN captures the information following its design. With the above features, their work provided the first cosmological toy demonstration that C3NN can iso-late higher-order information in a controlled and inter-pretable way. In this work, we extend this program from a pure classification task to mock cosmological parameter in-ference. We integrate a C3NN summary extractor with a simulation-based inference (SBI) framework, and train both components jointly to infer cosmological parame-ters directly from simulated weak lensing convergence maps. This joint training ensures that the summary extraction is optimized specifically for the downstream inference task. Our approach provides two key advantages: 1. Efficient extraction of higher-order information. Direct computation and measurement of NPCFs beyond the 3-point correlation function (3PCF) is prohibitively expensive, and current cosmological analyses rarely consider orders above N = 3. The constrained architecture of C3NN enables us to incorporate higher-order information at negligible computational cost while retaining interpretabil-ity: each component of the summary can be ex-plicitly associated with a specific N -point order. 2. Quantifying information beyond a given N -point order. As we can clearly separate the summary statistic in orders of NPCFs, we can systemat-ically bound the incremental information gained from each additional order. This enables us to forecast whether improvements are more efficiently achieved by adding higher NPCF orders or by transitioning to unconstrained machine learning optimized summaries. The remainder of this paper is organized as follows. Section 2 reviews the overall architecture of C3NN-SBI, 3its convolutional constraints and the explicit correspon-dence between the model output and NPCFs. Section 3 describes our mock simulations, targeted cosmological parameter space and training data preparation. Then Section 4 presents our results on the information con-tent of successive NPCF orders as approximated by our framework and the residual information not captured beyond 4th order. We discuss our validation tests, as well as limitations and caveats in the current framework in Section 5. Finally we present our conclusions and discuss future work in Section 6. 

2. C3NN-SBI MODEL ARCHITECTURE 2.1. Optimizable N-point Information Extraction with C3NN 

We briefly introduce the C3NN embedding network here. For a more detailed discussion, as well as tests on cosmological toy models in a classification task setting, we refer the readers to C. Miles et al. (2021); Z. Gong et al. (2024). The C3NN architecture starts as a stan-dard single-layer Convolutional Neural Network (CNN) without an activation function. Beyond the usual convo-lutional feature maps, we define so-called moment maps ,which can be constructed analytically from the output of the first convolutional layer alone. By taking spatial average of these moment maps, we obtain a set of scalar summary statistics, referred to as 

N th-order moments . These moments can be written an-alytically in terms of the corresponding NPCF estima-tors of the input fields. The resulting summary statis-tics therefore admit a clear interpretation as weighted integrals (to be more precise, summation in the case of discrete pixels) over NPCFs, where the weights are de-termined by the convolutional filters. We optimize these weights by maximizing the variational mutual informa-tion between the summary statistics and the inferred cosmological parameters, using standard gradient-based optimization techniques. Further details are provided below. 2.1.1. Moment Maps and Their Connection to NPCFs 

We define the first operation on an input image Sk(xxx)with pixel coordinates xxx in the kth channel as the first convolutional layer and produce the linear feature map 

C(1)  

> α

given by 

C(1)  

> α

(xxx) = X

> aaa,k

wα,k (aaa) Sk(xxx + aaa), (1) where wα,k (aaa) denotes the learnable weight of filter α at offset aaa from xxx. This operation is the same as the first layer in a traditional convolutional neural network. In analogy, we define the N th-order moment map for 

N ∈ N as 

C(N ) 

> α

(xxx) = 1

N !  

> (aaa1,k 1)̸=... ̸=( aaaN,k N)

X  

> (aaa1,k 1),..., (aaaN,k N)
> N

Y

> j=1

wα,k j (aaaj ) Skj (xxx + aaaj ).

(2) where ( aaa1, k 1)̸ = . . . ̸ = ( aaaN , k N ) above the summation does not denote an upper limit, but rather an exclusion of self-counting. We retain such a notation convention below unless otherwise stated. As a concrete example, the 2nd-order moment map reads 

C(2)  

> α

(xxx) = 12

> (aaa1,k 1)̸=( aaa2,k 2)

X

> aaa1,a aa2,k 1,k 2

wα,k 1 (aaa1) wα,k 2 (aaa2)

× Sk1 (xxx + aaa1) Sk2 (xxx + aaa2).

(3) Taking the spatial average of this map yields the cor-responding 2nd-order moment ,

c(2)  

> α

= 1

Npix 

X

> xxx

C(2)  

> α

(xxx)= 12

> (aaa1,k 1)̸=( aaa2,k 2)

X

> aaa1,a aa2,k 1,k 2

wα,k 1 (aaa1) wα,k 2 (aaa2)

×

"

1

Npix 

X

> xxx

Sk1 (xxx + aaa1) Sk2 (xxx + aaa2)

#

.

(4) The term in brackets is the real-space 2-point corre-lation function (2PCF) estimator between channels k1

and k2,ˆξk1k2 (rrr) = 1

Npix 

X

> xxx

Sk1 (xxx + aaa1) Sk2 (xxx + aaa2) , (5) with separation rrr = aaa2 −aaa1. Substituting this definition, the second-order moment can be written compactly as 

c(2)  

> α

= 12

> (aaa1,k 1)̸=( aaa1+rrr,k 2)

X

> aaa1,r rr,k 1,k 2

wα,k 1 (aaa1) wα,k 2 (aaa1+rrr) ˆξk1k2 (rrr) .

(6) This construction generalizes straightforwardly to higher orders. For instance, the third-order moment can be expressed as a weighted sum over the corresponding 3PCF estimator ˆζk1k2k3

c(3)  

> α

= 13! 

> (aaa1,k 1)̸=( aaa1+rrr,k 2)̸=( aaa1+rrr′,k 3)

X

> aaa1,r rr,r rr′,k 1,k 2,k 3

wα,k 1 (aaa1) wα,k 2 (aaa1 + rrr)

× wα,k 3 (aaa1 + rrr′)ˆζk1k2k3 (rrr, r rr′, −rrr − rrr′) .

(7) 42.1.2. Efficient Computation via a Recursion Relation 

As the computational cost of evaluating higher-order moment maps grows rapidly with order, a direct cal-culation quickly becomes infeasible. To address this, we instead exploit the recursion relation derived in C. Miles et al. (2021), which allows higher-order moment maps to be computed based on the lower-order maps. The N th-order moment map can be expressed as 

C(N ) 

> α

(xxx) = 1

N

> N

X

> l=1

(−1) l−1

" X

> aaa,k

wlα,k (aaa) Slk(xxx + aaa)

#

× C(N −l) 

> α

(xxx) ,

(8) where we define C(0)  

> α

(xxx) = 1. This formulation significantly reduces the computa-tional burden from O(( KP )N ) per input map pixel, where K is the number of input data channels and 

P is the number of pixels in the convolution filter, to 

O(N 2KP ) and thus enabling efficient scaling to large maps and high moment orders. 2.1.3. Isotropic Filters and Inductive Biases 

Under standard cosmological assumptions, the under-lying fields are statistically isotropic. To improve train-ing efficiency, it is therefore advantageous to encode this known symmetry directly into the machine learning ar-chitecture. We achieve this by enforcing isotropy in our C3NN filters, such that all filter pixels at the same radial distance from the central pixel share a common value. This additional form of weight sharing, beyond that already imposed by the convolutional structure of the network, substantially reduces the number of trainable parameters while explicitly incorporating prior physical knowledge about the data. Our implementation lever-ages the escnn library 8 (M. Weiler & G. Cesa 2019; G. Cesa et al. 2022), which provides a principled framework for symmetry-aware neural networks. In the machine learning literature, architectural con-straints such as isotropy are commonly referred to as inductive biases . The performance and interpreta-tion of an optimized model are inherently conditioned on these (sometimes implicit) design choices. Conse-quently, when assessing the information content of cos-mological fields within our optimization framework in the following sections, it is important to keep in mind that the results are conditional on the imposed induc-tive biases. We return to this point explicitly in our discussion of the results.  

> 8https://github.com/QUVA-Lab/escnn

2.2. SBI and Loss Function 

We learn summary statistics to be used for SBI (J. Alsing et al. 2019; K. Cranmer et al. 2020). Generally, in this framework, neural networks are trained on simu-lated data to extract informative, low-dimensional rep-resentations of the observations. The choice of loss func-tion in this setting is not unique. Several learning strate-gies for the extraction of summary statistics have been proposed in the literature. A common option is to train networks using a mean-squared error (MSE) loss with respect to the parameters (P. Fearnhead & D. Prangle 2010; B. Jiang et al. 2015; P. Lemos et al. 2023). Other approaches include so-called moment networks (N. Jef-frey & B. D. Wandelt 2020), which are designed to learn informative moments of the marginalized posterior dis-tribution, as well as end-to-end SBI methods that di-rectly optimize a neural density estimator (S. T. Radev et al. 2020; N. Jeffrey et al. 2021; K. Lehman et al. 2024). Following the results of D. Lanzieri et al. (2025), which demonstrate superior performance of the latter strategy, we adopt a full SBI approach in this work. For a com-prehensive introduction to the SBI paradigm, we refer the reader to K. Cranmer et al. (2020). Applications of SBI in cosmology include, for example, N. Jeffrey et al. (2021); B. Tucci & F. Schmidt (2023); M. von Wietersheim-Kramsta et al. (2024); K. Lehman et al. (2024); D. Gebauer et al. (2025); A. Thomsen et al. (2025) 9.In our setup, the C3NN acts as an embedding network ,producing a learned summary representation of the data that is passed directly to the SBI inference model. For the density estimation stage, we employ masked autore-gressive flows (MAFs) (G. Papamakarios et al. 2017) to model the posterior distribution over parameters, an approach commonly referred to as neural posterior esti-mation (NPE). The corresponding training objective is given by the negative log-posterior, 

LNPE = − X

> i

log qϕ(θi | di) , (9) where qϕ denotes the neural posterior density estima-tor with trainable parameters ϕ. The network is trained on a set of simulated parameter data pairs {θi, d i}Ni=1 

by maximizing the posterior probability assigned to the true parameters. An alternative SBI strategy is to learn the likelihood function instead of the posterior directly. This approach, known as neural likelihood estimation (NLE), has the  

> 9We further refer to https://github.com/smsharma/awesome-neural-sbi for an extensive list of SBI applications.

5advantage that the prior distribution can be modified after training. In this case, the loss function is given by 

LNLE = − X

> i

log qϕ(di | θi) . (10) Once the likelihood has been learned, standard likelihood-based inference techniques, such as Markov Chain Monte Carlo (MCMC), can be used to obtain posterior samples using the approximate likelihood. We present a schematic overview of our C3NN-SBI ar-chitecture in Figure 1. The resulting summaries can be interpreted as a hierarchy of learned, filtered N -point statistics, providing a physically motivated and inter-pretable bridge between traditional moment-based anal-yses and modern SBI. By optimizing these summaries end-to-end for posterior inference, the network learns to combine the information at all orders in NPCFs consid-ered to produce posterior estimates.  

> 3.

SIMULATIONS AND DATA In this work, we utilize a suite of simulated weak lens-ing convergence maps originally generated for the anal-ysis presented in D. Gebauer et al. (2025). These simu-lations are designed to emulate the survey properties of the Dark Energy Survey Year 3 (DES Y3) analysis (M. Gatti et al. 2021; J. Myles et al. 2021) while providing a vast training set suitable for simulation-based infer-ence. We describe the underlying N-body simulations, the lensing methodology, and the data preparation pro-cess below. 3.1. Data & Lensing 

Weak gravitational lensing provides an unbiased probe of the total matter distribution in the Universe. As light from background sources travels towards the observer, it is deflected by the gravitational potentials of the inter-vening large-scale structure. In this work, we focus on the weak lensing convergence κ, a scalar field which de-scribes the isotropic magnification of the source images and corresponds to the weighted line-of-sight projection of the matter density contrast δ.Under the Born approximation, assuming a spatially flat Universe, the convergence κ(θθθ) at an angular po-sition θθθ on the sky is given by (M. Bartelmann & P. Schneider 2001): 

κ(θθθ) = 3H20 Ωm

2c2

Z χlim 

> 0

dχ χa(χ) g(χ)δ(χθ θθ, χ ), (11) where H0 is the Hubble constant, Ω m is the current mat-ter density parameter, c is the speed of light, and a(χ)is the scale factor at comoving distance χ. The lensing efficiency kernel g(χ) depends on the source galaxy red-shift distribution n(z) and is defined as (M. Bartelmann & P. Schneider 2001): 

g(χ) = 

Z χlim 

> χ

dχ′ n(χ′) χ′ − χχ′ . (12) To generate mock convergence maps from N-body sim-ulations, we rely on the particle-shell approximation (R. J. Sgier et al. 2019; R. Sgier et al. 2021; A. Reeves et al. 2024). Instead of continuous integration, the sim-ulation volume is discretized into concentric spherical shells of finite thickness. The effective convergence is then computed as a weighted sum over the particle den-sities projected onto Healpix maps (K. M. G´ orski et al. 2005) within these shells. 3.2. The Cosmogrid Sims 

We utilize the CosmoGridV1 dataset (T. Kacprzak et al. 2023), a large suite of N-body simulations designed for simulation-based inference in cosmology. The simu-lations were evolved using the Pkdgrav3 code (D. Pot-ter et al. 2017) within periodic cubic boxes of side length 900 h−1Mpc containing 832 3 dark matter particles. The particle mass resolution varies between 3 .5×10 10 h−1M⊙

and 17 .5 × 10 10 h−1M⊙ depending on the specific cos-mology. The grid spans a wide parameter space of flat wCDM cosmologies, varying the total matter density Ω m, bary-onic density Ω b, Hubble constant H0, spectral index ns,fluctuation amplitude σ8, and the dark energy equation of state parameter w0. These parameters are sampled using a Sobol sequence to ensure efficient coverage of the high-dimensional space. The suite consists of a wide and a narrow grid with 1250 distinct cosmologies each, with 7 independent realizations per cosmology to sup-press cosmic variance. Additionally, 200 independent realizations are provided at a fiducial cosmology to al-low for accurate covariance estimation and validation. For the exact distribution of simulation cosmologies in the above parameter space, we would like to refer the reader to Figure 1 in T. Kacprzak et al. (2023). The CosmoGridV1 suite incorporates baryonic effects via the baryonification method (A. Schneider & R. Teyssier 2015; S. K. Giri & A. Schneider 2021), ap-plied directly at the map level using particle shells. This method displaces dark matter particles to mimic the re-shaping of density profiles due to gas cooling, star for-mation, and feedback mechanisms. While the full model depends on several parameters, weak lensing observables are primarily sensitive to the mass dependence of the gas profile, parametrized by Mc. We adopt the extended parametrization used in the grid: 6                  

> Figure 1. Cartoon illustration of our summary extraction and neural posterior estimation (NPE) pipeline. The input conver-gence maps ( κ, with zs 1 to zs 3 indicating source galaxy tomographic bins, i.e. different channels) are convolved once with a set of isotropic filters αwith learnable weights, producing the first-order moment maps C(1)
> α(xxx). Higher-order moment maps C(N)
> α(xxx)are then computed recursively using the relation in Eq. 8, avoiding the need for repeated convolutions. Spatial averaging of each moment map yields the corresponding moment estimates c(N)
> α, which serve as our summary statistics. These summaries, together with the associated simulation parameters, are passed to a masked autoregressive flow (MAF)–based neural posterior estimator to infer the posterior distribution p(θ| { c(N)
> α}). The entire pipeline is trained end-to-end, with gradients propagating from the posterior estimator through the summary network.

Mc(z) = M 0 

> c

(1 + z)ν , (13) where M 0 

> c

and ν are varied as free parameters in the simulation grid alongside the cosmological parameters. 3.3. Training Data Preparation 

After generating the full-sky simulated weak-lensing convergence maps, we prepare the training data for the C3NN model by partitioning these maps into smaller, square patches. Specifically, we follow the map-partitioning strategy exploited in Z. Gong et al. (2024), which was originally introduced in F. Ferlito et al. (2023). In this approach, we use each full-sky map to produce non-overlapping square maps of size 50 × 50 pixels. For each of the simulation nodes in the CosmogridV1 wide grid, we use the same full-sky realization and par-tition it into 150 non-overlapping square maps following the procedure described above. Each square map con-tains four channels, corresponding to the four DES Y3 source redshift bins, and it also preserves the angular resolution of the original full-sky simulation, which is generated at NSIDE = 512. Consequently, the angular size of each square patch is comparable to the largest an-gular separation considered in the DES Y3 cosmic shear 2-point correlation function analysis (A. Amon et al. 2022; L. F. Secco et al. 2022), and the total footprint of the 150 square maps summed up approximately equals to that of DES Y3 footprint. This choice ensures that the spatial extent of the patches remains relevant for direct comparison with scales commonly used in con-temporary weak-lensing analyses. The entire simulations suite consists of 2500 simula-tions, out of which 1300 are drawn according to a rather wide prior, whereas the remaining 1200 are drawn from a significantly tighter distribution, we refer readers to Figure 1 in T. Kacprzak et al. (2023) for details of the prior distribution in the parameter space. For our main analysis, we restrict ourselves to the simulations stem-ming from the wide prior (i.e. 1300) but we later in-vestigate the impact increasing the amount of simula-tions. Furthermore, to construct an independent test data set in our main analysis, we randomly select 250 out of the 1300 cosmological nodes and reserve all of their corresponding square maps exclusively for testing. In addition to the wide-grid simulations, we apply the same partitioning method to the 200 full-sky realizations generated at the fiducial cosmology, where Ω m = 0 .26, 

σ8 = 0 .84, w0 = −1.0, ns = 0 .9649, Ω b = 0 .0493 and 

H0 = 67 .3 km /s/Mpc (T. Kacprzak et al. 2023). The resulting square maps are subsequently used to produce the results presented in our information content analy-sis. 7The motivation for pushing the pixel size of the square maps to the native angular resolution of the simula-tions is twofold. First, the simulations already incor-porate baryonic feedback effects through the baryoni-fication technique (A. Schneider & R. Teyssier 2015) as described above, making the small-scale information physically meaningful. Second, retaining these small-scale modes allows us to include as much cosmological information as possible in the training data, thereby en-abling the C3NN–SBI framework to optimally exploit both large- and small-scale features present in the con-vergence fields. While observations rely on the estima-tion of shear from galaxy ellipticities, which introduces shape noise and requires shape measurement calibration, we note that we do not add realistic DES Y3–like shape noise to the training data. The primary goal of this work is not to derive cosmological parameter constraints from observational data, but rather to construct and validate our inference framework and to study the theoretical information content encoded in different orders of corre-lation functions and in map-level information extractors such as CNNs. Omitting shape noise therefore allows us to isolate the intrinsic information content of the lensing fields themselves.  

> 4.

RESULTS 4.1. Physically Meaningful Hyperparameters and Architectural Choices 

Throughout this work, we only present the best of a few trained networks and defer an exhaustive hyper-parameter search to further study. There are, however, some hyperparameters that are of particular importance to our networks as they have clear physical meaning. We motivate the choices of these hyperparameters here. Firstly, the filter size directly determines the largest scale that the network can probe. This is in contrast to traditional CNNs which convolve on the extracted fea-ture map multiple times, effectively also learning scales larger than the initial filter scale. At the same time we want to refrain from setting the filter size too large, as this only adds additional parameters to the network un-necessarily due to the fact that on large scales higher order NPCFs do not contribute significantly to the cos-mological information capture. To facilitate symmetries more easily, we further restrict the filter size to be odd in pixel numbers. This provides us with a defined cen-tral pixel after each convolution. We can also apply an integer number of padding of zeros to keep the output 

C(1)  

> α

(xxx) maps the same size as the input maps. The filter size chosen for the models presented here is 37, which corresponds to a maximum scale of rmax ≈ 248 ′. This scale is approximately equivalent to the largest angular separation probed by the DES Y3 cosmic shear analyses (A. Amon et al. 2022; L. F. Secco et al. 2022). A second important hyperparameter in our models is the number of filters. The amount of scales and configurations that a single filter can reasonably learn is finite. The amount of filters therefore gives an upper limit on how much information can be extracted. Our strategy for this hy-perparameter consists of increasing the filter number at a given order in moments, until the constraining power no longer improves. We have found empirically that the 2nd order C3NN embedding network already con-verged with only a few filters, as opposed to higher order models. This is consistent with our intuition that the possible amount of configurations drastically increases beyond the second order in NPCFs. For models at 2nd order we use 4 filters, at 3rd order 25, and at 4th or-der we use 40 filters. Increasing the number of filters at any order beyond these numbers has not resulted in any significant improvement. As our data consists of 150 maps that add up to the size of DES Y3 footprint, our architecture computes the moments for each of these maps. We then take the mean of the moments as the summary statistic. We further-more found it very effective, to not only use the means, but also the standard deviation of the moments. This is because in principle, the standard deviation of a given order N moment can already access part of the infor-mation contained in the connected 2 N -point correlation function, though the square root operation would en-sure that the standard deviation has the same physical units and scaling behavior as the moment itself. The final length of our summary statistic vector, the set of all moment estimates {c(N ) 

> α

} is therefore given by: dim( {c(N ) 

> α

}) = 2 × Nfilter × N. (14) As we always use all filters at a given order, we only denote the N -point order as a subscript when showing our results in the following. 4.2. Information Gain at Different Orders 

In Figure 2a we show the resulting contours of C3NNs trained at different orders for mock inference on a pre-viously withheld simulation. Generally, we find that we can overcome the prior in Ω m, σ8 and w0. As we have found in preliminary tests that the other parameters are not constrainable within the prior, we do not explicitly infer them here and instead implicitly marginalize over them, including Ω b, H0, ns, and M 0 

> c

, ν controlling the baryonic effects (Eq. 13). In the constrainable parame-ters we encouragingly find a monotone increase in con-straining power as we move to higher orders of moment maps, i.e. to higher order in NPCFs. We did not find 8any improvement in constraining power after the 4th or-der. However, we suspect this to be an artifact of the overall low training simulation budget at our disposal and we will investigate this further in the following sec-tions. Concerning inference at 5th order, which in the-ory should be achievable within our framework, we defer such investigations to future work. We provide in Table 1 the relative improvement in terms of the fractional difference on the marginalized standard deviation of each of the three parameters as well as the 3-dimensional Figure of Merit (FoM) relative improvement when we move to higher orders. All the numbers presented in the table are acquired by running the models on a single fiducial cosmology realization.                    

> Model Ωmσ8w0FoM
> {c1, c 2, c 3}27 .92% 28 .44% 19 .11% 96 .59%
> {c1, c 2, c 3, c 4}12 .89% 19 .99% 3.68% 34 .38%
> Table 1. The relative improvement in terms of the fractional difference of the 1D marginalized parameter standard devi-ation and the FoM from the 3D parameter covariance. The percentage numbers of model {c1, c 2, c 3}are with respect to model {c1, c 2}, and those of model {c1, c 2, c 3, c 4}are with respect to model {c1, c 2, c 3}.

We further run the same test as the one shown in Ta-ble 1, but on 200 independent simulation realizations on fiducial cosmology. Figure 3 shows histograms of the improvements in the FoM in terms of fractional dif-ference from adding the c3 statistic on top of c2 (Fig-ure 3a) and adding the c4 statistic on top of c3 (Fig-ure 3b). While the improvement varies for different re-alizations, the improvements for the posterior shown in Figure 2a are rather characteristic of and close to the median improvement, and thus are representative of the improvement one would get from adding higher order N -point information using the C3NN model. The analysis demonstrates a substantial and consistently positive im-pact from including the higher-order N -point informa-tion, with a median FoM improvement of 75 .30% from adding c3, and a median FoM improvement of 30 .69% from adding c4.4.3. Comparison to a Traditional CNN 

We further compare the information that can be ex-tracted with a standard CNN, in order to assess the quality of information extraction from the models pre-sented thus far while drawing the comparison at the same amount of training budget. It is important to note that this test is not run to assess if all information is ex-tracted, as the CNN itself may fail to do so. The CNN model implemented in this section consists of 3 convolu-tional layers with ReLU activation function in between, followed by one max pooling layer after each nonlinear activation. We then take the average over the output realizations and apply an adaptive pooling layer. This result is then fed into a fully connected 3-layer neural network with ReLU between it. Our implementation of C3NN at 4th order only has about 3000 trainable parameters, and the CNN model approximately has 165000 parameters. For this test we train the CNN on a mean square error loss regressing to the standardized cosmological parameters. This re-sults in an estimate of the posterior mean (see e.g. K. P. Murphy 2022), which is then fed as a summary statistic into an NPE network. The latter is trained in a sepa-rate process. In Figure 2b we show the comparison of the contours obtained from the CNN model above with such an optimization strategy against our best C3NN model in 4th order, when trained on the same low num-ber of simulations. We find that the CNN model has a better performance on the interested cosmological pa-rameters than the C3NN at 4th order, even with this limited number of training simulations. We identify two possible contributions to this difference: 1. The CNN has access to higher-order information that is not probed by a chosen finite set of corre-lation functions in the C3NN case. 2. On the orders of NPCFs that the two models do share, C3NN is limited to learning weighted inte-grals of NPCFs while the CNN has more freedom. This freedom comes at the cost of opaque infor-mation extraction, whereas C3NN retains a clear analytical connection to the NPCF program. Nevertheless, we remind the readers that these results are subject to the limited training data used. While we find the difference in constraining power intriguing as to the uncaptured information by C3NN in the field, this is merely a hint and could be driven by the impact of the limited amount of training simulations. Thus the robust quantitative investigation of the amount of cosmological information remained uncaptured by C3NN to a given correlation function order, but existent in the field, is left to future studies. 4.4. Comparison to the 2PCF and Hybrid Summaries 

Given that higher-order statistics aim to outperform the 2PCF, we explicitly compare the two in this section. For this we first measure the convergence 2PCFs on the same 1250 wide grid cosmology projected square maps used for training C3NN in 10 logarithmically spaced bins between 10 and 250 arcminutes. We train NPE with 9{c1, c 2}{c1, c 2, c 3}{c1, c 2, c 3, c 4}

> 0.60.81.01.2
> σ8
> 0.20.30.4

Ωm

> −1.6
> −1.2
> −0.8
> w0
> 0.60.81.01.2

σ8

> −1.6
> −1.2
> −0.8

w0

(a) 2PCF 

{c1, c 2, c 3, c 4}

CNN MSE 

> 0.75 0.90 1.05
> σ8
> 0.18 0.24 0.30 0.36

Ωm

> −1.5
> −1.2
> −0.9
> −0.6
> w0
> 0.75 0.90 1.05

σ8

> −1.5
> −1.2
> −0.9
> −0.6

w0 (b) 

Figure 2. Mock inference on a withheld test simulation at the fiducial cosmology. In (a) we compare the constraining power of C3NNs at different orders. As we increase the order, i.e. up to the 4th order which is the highest order of correlation function considered, the constraining power increases due to the non-gaussian information in the convergence map. In (b) we compare a traditional CNN with our C3NN at 4th order. We find that at the same low amount of training simulations, the CNN model can extract more cosmological information possibly drawing from even higher orders in NPCFs. We also add the mock inference results from measured 2PCFs, and its constraining power is comparable to our C3NN at 4th order, due to the limited training simulation budget. 0.0 0.5 1.0 1.5Improvement in FoM Ω m, σ 8, w 

0510 15 Count Median : 76 .80% Example Posterior : 97 .00% 

(a) 0.0 0.5 1.0 1.5Improvement in FoM Ω m, σ 8, w 

0510 15 20 25 Count Median : 31 .34% Example Posterior : 34 .08% (b) 

Figure 3. Histograms of the improvement in the FoM in terms of fractional difference for 200 simulation realizations at the fiducial cosmology. We also show the median improvement in black, and the improvement seen in the posterior of the selected fiducial simulation inference shown in Figure 2a in red. 10 a linear, one layer deep, feed forward embedding net-work on the measured 2PCFs directly from simulation maps. Figure 2b shows the comparison of this poste-rior with our best C3NN model at 4th order (here the 2PCF model exploits the same amount of training simu-lations as the C3NN, i.e. 1000 nodes). Surprisingly, the 4th order C3NN model only adds little information on top of the 2PCFs. We trace this back to the limiting small simulation budget available here. This is formally explained by the Dodelson-Schneider correction (S. Do-delson & M. D. Schneider 2013) which is also present in neural posterior analyses (J. Homer et al. 2025). We assess two strategies to still outperform the 2PCFs. One is that of hybrid, 2PCF aware, summary statistics(T. L. Makinen et al. 2024; T. Lucas Makinen et al. 2025), the other is an increase in training budget by switching to NLE which we perform in the following section. In order to facilitate easier information extraction, based on the knowledge of the 2PCF we first investi-gate a hybrid approach, as introduced by T. L. Maki-nen et al. (2024); T. Lucas Makinen et al. (2025). They have shown, that if the density estimator is not only conditioned on the learned summary, but also an al-ready known summaries (such as the 2PCF estimator in this case), the learned summary improves in infor-mation content as the machine can “concentrate” solely on information beyond the known summary. We train our 4th order C3NN model as a hybrid model condi-tioned also on the NPE compressed 2PCFs and show the results in Figure 4a. In this framework the C3NN can learn additional information and leads to the out-performance of the 2PCFs (Tab. 2). As with our other results thus far, we conjecture that the amount of infor-mation gained is also still limited by the small amount of available simulations. 4.5. Increasing the Simulation Budget through NLE 

We can also use the trained C3NN summary net-work to retrain an inference network based on NLE. In that case, we can include more simulations from Cosmo-GridV1 suite, as the training simulations are no longer bound to follow the prior distribution. We present the NLE alternative to the NPE posteriors here, in order to investigate whether our pipeline thus far is really simulation limited as conjectured. NLE presents a few downsides in our use case. Most notably, the previously learned summary statistic is no longer optimal under the prior, as the training distribution for the C3NN em-bedding net is now different from the actual one. In our particular case, the learned summary is also quite high dimensional (see Eq. 14). As the density estimation task is now data dimensional, as opposed to parameter dimensional, this would introduce the curse of dimen-sionality. In order to make the density estimation fea-sible, we train NLE on the principal components of the learned summary, and only keep 20 of them. We empha-size that this additional linear compression introduces 

heavy losses in information . It does, however, enable us to now investigate the dependency of our machinery on the simulation budget. As this subsection only constitutes such a test, we accept the highly non-optimal postprocessing of the learned summary and our mock posteriors are to be interpreted as lower bounds on the actual information content of the summary. We present our NLE posteri-ors with twice the amount of training data in Figure 4b. We find that the increase in constraining power is sig-nificant, just from the increased simulation budget. The tightened NLE contours at 4th order easily outperform the 2PCF. We report detailed improvements in Table 2. These results confirm our conjecture that we are in fact simulation limited in our particular case.               

> Model Ωmσ8w0FoM Hybrid vs. 2PCF 9.46% 10 .36% −10 .79% 7.29% NLE vs. NPE 36 .67% 79 .70% −9.41% 347 .73%
> Table 2. Same as Table 1 but comparing (a) the hybrid trained C3NN model at 4th order with the 2PCF at the same simulation budget and (b) the NLE result at 4th order using double the simulation budget with respect to the NPE result at 4th order. 5.

VALIDATION In this section we validate our pipeline from two dif-ferent perspectives. On one hand, given that our NPE posteriors do not significantly outperform the 2 point function (even though the NLE posteriors do), we con-struct a test to ensure that we are in fact picking up on higher order information. We do so by artificially removing higher order information and comparing the remaining constraining power of our pipeline. On the other hand, we run standard Bayesian inference tests and validate the resulting marginal posteriors with the popular simulation-based calibration (sbc) test (S. Talts et al. 2018). 5.1. Phase Reshuffling 

Furthermore, we perform a test focusing on phase in-formation in simulated weak lensing convergence maps. We conduct a controlled test where the phase com-ponents of our original training data maps are ran-domized while preserving their amplitude, and then re-evaluate the performance of our C3NN models on these 11 2PCF 

{c1, c 2, c 3, c 4} hybrid 

> 0.70.80.91.0
> σ8
> 0.18 0.24 0.30 0.36

Ωm

> −1.6
> −1.2
> −0.8
> −0.4
> w0
> 0.70.80.91.0

σ8

> −1.6
> −1.2
> −0.8
> −0.4

w0

(a) {c1, c 2, c 3, c 4}{c1, c 2, c 3, c 4} NLE 

> 0.70.80.91.0
> σ8
> 0.18 0.24 0.30 0.36

Ωm

> −1.5
> −1.2
> −0.9
> −0.6
> w0
> 0.70.80.91.0

σ8

> −1.5
> −1.2
> −0.9
> −0.6

w0 (b) 

Figure 4. (a) Comparison of a hybrid (T. L. Makinen et al. 2024; T. Lucas Makinen et al. 2025) C3NN at 4th order to the measured 2PCF with NPE. The hybrid model outperforms the 2PCF NPE posterior. (b) Comparison of our best C3NN 4th order NPE result with a corresponding C3NN 4th order NLE model using twice the training data for the inference network. By including more simulations in the training of the NLE inference network, the constraining power increases substantially. 

phase-randomized datasets. Figure 5 shows that c2

(2PCF/power spectrum) retains nearly all of its cos-mological information content relative to the original training case. In contrast, the information gain derived from c3 (3PCF/bispectrum) and c4 (4PCF/trispectrum) is diminished by a large amount (on certain marginal-ized parameter dimension there is even information loss) when comparing the results between Table 1 and 3. This outcome aligns with theoretical predictions for random Gaussian and non-Gaussian fields: the power spectrum, as a second-order statistic, only encodes amplitude-dependent information about density fluctuations and is thus invariant to phase permutations, while the bis-pectrum and trispectrum, as higher-order correlation functions, are inherently sensitive to the phase coher-ence of the field. These phase-sensitive statistics capture the non-random, physically meaningful alignments of structure (e.g. filamentary connections or halo cluster-ing) that encode unique cosmological signatures. Their degradation under phase randomization confirms that our C3NN models are indeed leveraging higher-order phase information to constrain cosmological parameters beyond the limits of power spectrum. {c1, c 2}{c1, c 2, c 3}{c1, c 2, c 3, c 4}

> 0.60.81.01.2
> σ8
> 0.20.30.4

Ωm

> −1.6
> −1.2
> −0.8
> −0.4
> w0
> 0.60.81.01.2

σ8

> −1.6
> −1.2
> −0.8
> −0.4

w0

Figure 5. The constraining power of C3NN at different orders where the training maps are on the same cosmological nodes as used in Figure 2a but phase-randomized : The fields are first Fast Fourier Transformed (FFT), then their phases randomized following a uniform distribution within 0 and 2 π,and then with inverse FFT transformed back to configuration space. 12                      

> Model Ωmσ8w0FoM
> {c1, c 2, c 3}7.70% 14 .21% −3.93% 34 .89%
> {c1, c 2, c 3, c 4}6.28% −4.90% 2.63% −8.23%
> Table 3. The relative improvement of the 1D marginal-ized parameter standard deviation and the FoM from the 3D parameter covariance. Here the training data have the same size but are randomized on their phases . The percent-age numbers of model {c1, c 2, c 3}are with respect to model
> {c1, c 2}, and those of model {c1, c 2, c 3, c 4}are with respect to model {c1, c 2, c 3}.All models trained here possess the same hyperparameters as the corresponding ones previously.

5.2. Simulation-Based Calibration (SBC) test 

We show the SBC test results for the three models in Section 4.2 with different orders of correlators in Fig-ure 6. The test data set is initially separated from the training data set in the CosmogridV1 simulation wide grid. SBC is a validation method for Bayesian Inference methods, designed to assess whether a model’s pre-dicted posterior distributions are statistically consis-tent (S. Talts et al. 2018). SBC makes use of a so called rank statistic which is used to compare the (test) data averaged marginal posterior to the prior. For a well-calibrated model, these rank statistics should be uni-formly distributed over the interval [0 , 1]. Instead of showing these histograms directly, we show the cumula-tive distribution function (CDF) which makes it easier to spot deviations. The behavior of the CDF of the rank statistics pro-vides direct insight into the properties of the trained model: (i) If the posterior distributions are overly broad, the rank CDF would first lie below the identity line, then cross, and lie above for large ranks. This indicates that the model is underconfident. (ii) If the posterior distri-butions are too narrow, the corresponding CDF curves yield the opposite pattern compared to the undercon-fident case, implying that the model is overconfident, and (iii) a biased posterior shifts the rank distribution toward one side, which means the CDF curves will lie systematically either below or above the identity line. From the SBC results shown in Figure 6 for the three cosmological parameters of interest, Ω m, σ 8 and w0, we find that the empirical CDFs of the rank statistics for all three independently trained C3NN models closely follow the identity function and the histograms them-selves therefore are uniform. This demonstrates that, when validated on the held-out test dataset, the models exhibit no signs of underconfidence, overconfidence, or systematic bias on the level of their marginal posteriors. 5.3. Caveats Concerning Information Content 

We would like to clearly point out the caveats of our analysis. Most prominently, our analysis is conditioned on the inductive biases of our architecture. Beyond the isotropy enforcement mentioned in Section 2.1.3, there are many more constraints we set here, for example: 

• The convolutional layer implicitly assumes trans-lational invariance of the features 

• Our summary statistic is always a weighted in-tegral over the given NPCF which of course is a massive compression of the information 

• By choosing MAFs as density estimators we are inherently biased toward a certain family of distri-butions for the posterior Beyond the inductive biases we would like to point out additional caveats: As neural networks are always a non-convex optimization problem we can never find the global optimum which means that our findings should be seen as lower bounds only. We are furthermore tied to a finite training set which we optimize for. We have already shown in this work, that the available simulation budget has a large impact on the resulting constraints on cosmological parameters within our framework. While we therefore consider our architecture to be an excellent tool to approximate/bound the information content in NPCF, we ask the reader to keep in mind that this is subject to these conditions.  

> 6.

CONCLUSIONS We have presented a novel framework for SBI that enables the efficient extraction of N -point information using learned, physically interpretable summary statis-tics with C3NN. Our approach combines the flexibility and performance of modern machine learning pipelines with a clear connection to traditional statistical de-scriptors of cosmological fields. By construction, each learned summary can be interpreted as a weighted sum of NPCFs at a given order, providing a transparent link between the network output and familiar correlation function–based analyses. In this sense, our method con-stitutes a physics-informed neural network, where strong architectural constraints encode prior physical knowl-edge directly into the model. The interpretability is achieved through constraining design choices: the use of a single convolutional layer with rotationally symmetric filters, together with a re-cursive construction of higher-order moment maps. As a consequence, even architectural hyperparameters ad-mit a physical interpretation. For example, the filter 13 0 6 12 Posterior Rank 00.51Empirical CDF 

Ωm

σ8

w0

(a) 0 6 12 Posterior Rank 00.51Empirical CDF 

Ωm

σ8

w0 (b) 0 6 12 Posterior Rank 00.51Empirical CDF 

Ωm

σ8

w0

(c) 

Figure 6. SBC test for the three C3NN models on a separate test dataset composed of 250 randomly sampled simulations from the CosmogridV1 wide grid for the (a) second order (b) third order and (c) fourth order model. Blue, orange and green curves denote empirical CDF curves of rank statistics for Ω m, σ 8 and w0 respectively. The gray bands represent the overall statistical (95%) uncertainties of the estimated empirical CDF. 14 scale determines the largest spatial scales probed by the summaries, while the number of filters controls the res-olution of scales that can be learned. These constraints substantially reduce the number of trainable parameters compared to standard CNN architectures. In the limit of infinite training data, our architecture provides a principled framework to probe the informa-tion content of arbitrary NPCFs. In practice, using a modest set of 1250 simulations of simplified DES Y3-sized convergence maps, we are able to train models in-corporating information up to the 4-point order. We find that the information content increases monotoni-cally from 2nd to 4th order. While we do not observe a further gain from including the summary statistics of 5-point or even higher orders, we attribute this saturation to the limited simulation budget rather than an intrinsic lack of higher-order information in the simulated weak lensing convergence fields. To explicitly verify that our model is sensitive to genuinely higher-order information, we performed infer-ence on phase-reshuffled mock convergence maps. Phase reshuffling preserves the 2-point statistics information while erasing higher-order correlations. We find that, after reshuffling, the constraining power of all higher-order models collapse to that of the 2nd-order model. This provides a direct numerical confirmation of the an-alytic connection between our summaries and NPCFs, and demonstrates that the improved performance at higher order is indeed driven by information beyond the 2PCF. We further compared our approach to a traditional 2PCF-based SBI analysis. In the baseline setup, we do not observe a statistically significant improvement over the 2PCF alone. However, we demonstrate that this limitation is purely driven by the small number of available simulations. We verify this conclusion in two complementary ways. First, we construct hybrid sum-maries in which the network is explicitly informed of the 2PCF and is trained only to learn additional in-formation beyond it. Even with the original, limited simulation budget, these hybrid summaries outperform the 2PCF baseline. Secondly, we increase the training set by incorporating additional simulations not drawn from the prior. This necessitates a switch from neu-ral posterior estimation (NPE) to neural likelihood es-timation (NLE), allowing the prior to be changed after training. Doubling the number of training simulations in this manner leads to a substantial increase in constrain-ing power. Together, these tests strongly support the conclusion that our method is capable of extracting ad-ditional information beyond the 2-point level, and that its ultimate performance is currently limited by the sim-ulation budget rather than by the expressiveness of the architecture. Our summary extraction framework opens several promising avenues for future work. One immediate di-rection is the interpretation of the learned filters them-selves: after training, these filters may provide direct in-sight into the specific configurations and scales that are most informative at a given order. This could enable configuration-targeted inference tasks, such as searches for primordial non-Gaussianity in specific limits. On the methodological side, extending the architecture to other symmetry groups represents a natural next step. While a generalization to three-dimensional Euclidean data ap-pears straightforward, a particularly compelling direc-tion is the extension to non-Euclidean domains, such as the sphere. Since cosmological observations are natu-rally defined on S2, this would allow for a more direct and natural application of our framework to real obser-vational data. 15 ACKNOWLEDGEMENTS We would like to thank Annabelle Bohrdt, Anik Halder, Jed Homer, Tomasz Kacprzak, Yoann Launay, and Sankarshana Srinivasan for stimulating and insightful discussions. KL acknowledges support via the KISS consortium (05D23WM1) funded by the German Fed-eral Ministry of Education and Research BMBF in the ErUM-Data action plan. We are also grateful for sup-port from the Cambridge-LMU strategic partnership. DG was also supported by the European Union (ERC StG, LSS BeyondAverage, 101075919). ZG and DG are deeply indebted to Stella Seitz for in-troducing and inspiring them to pursue research in the field of gravitational lensing, in which she made several foundational contributions. She recognized early on the potential of wide-area weak lensing surveys, advanced statistical tools, and machine learning methods, all of which are central to this work. Stella was deeply enthu-siastic and actively involved in this project, but sadly did not live to see its final form. May she rest in peace, knowing that her legacy will live on through this and future works. REFERENCES 

Akeret, J., Refregier, A., Amara, A., Seehars, S., & Hasner, C. 2015, JCAP, 2015, 043, doi: 10.1088/1475-7516/2015/08/043 Alsing, J., Charnock, T., Feeney, S., & Wandelt, B. 2019, MNRAS, 488, 4440, doi: 10.1093/mnras/stz1960 Amon, A., Gruen, D., Troxel, M. A., et al. 2022, PhRvD, 105, 023514, doi: 10.1103/PhysRevD.105.023514 Bartelmann, M., & Schneider, P. 2001, PhR, 340, 291, doi: 10.1016/S0370-1573(00)00082-X Beaumont, M. A., Zhang, W., & Balding, D. J. 2002, Genetics, 162, 2025 Bleem, L. E., Stalder, B., de Haan, T., et al. 2015, ApJS, 216, 27, doi: 10.1088/0067-0049/216/2/27 Blum, M. G. B., & Francois, O. 2008, arXiv e-prints, arXiv:0809.4178, doi: 10.48550/arXiv.0809.4178 Blum, M. G. B., Nunes, M. A., Prangle, D., & Sisson, S. A. 2012, arXiv e-prints, arXiv:1202.3819, doi: 10.48550/arXiv.1202.3819 Cesa, G., Lang, L., & Weiler, M. 2022, in International Conference on Learning Representations. https://openreview.net/forum?id=WE4qe9xlnQw Charnock, T., Lavaux, G., & Wandelt, B. D. 2018, PhRvD, 97, 083004, doi: 10.1103/PhysRevD.97.083004 Cranmer, K., Brehmer, J., & Louppe, G. 2020, Proceedings of the National Academy of Science, 117, 30055, doi: 10.1073/pnas.1912789117 Dark Energy Survey Collaboration, Abbott, T., Abdalla, F. B., et al. 2016, MNRAS, 460, 1270, doi: 10.1093/mnras/stw641 Dawson, K. S., Schlegel, D. J., Ahn, C. P., et al. 2013, AJ, 145, 10, doi: 10.1088/0004-6256/145/1/10 de Jong, J. T. A., Verdoes Kleijn, G. A., Kuijken, K. H., & Valentijn, E. A. 2013, Experimental Astronomy, 35, 25, doi: 10.1007/s10686-012-9306-1 de Jong, R. S., Bellido-Tirado, O., Chiappini, C., et al. 2012, in Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, Vol. 8446, Ground-based and Airborne Instrumentation for Astronomy IV, ed. I. S. McLean, S. K. Ramsay, & H. Takami, 84460T, doi: 10.1117/12.926239 Dodelson, S., & Schneider, M. D. 2013, PhRvD, 88, 063537, doi: 10.1103/PhysRevD.88.063537 Fearnhead, P., & Prangle, D. 2010, arXiv e-prints, arXiv:1004.1112, doi: 10.48550/arXiv.1004.1112 Ferlito, F., Springel, V., Davies, C. T., et al. 2023, MNRAS, 524, 5591, doi: 10.1093/mnras/stad2205 Fluri, J., Kacprzak, T., Sgier, R., Refregier, A., & Amara, A. 2018, JCAP, 2018, 051, doi: 10.1088/1475-7516/2018/10/051 Gatti, M., Sheldon, E., Amon, A., et al. 2021, MNRAS, 504, 4312, doi: 10.1093/mnras/stab918 Gebauer, D., Halder, A., Seitz, S., & Anbajagane, D. 2025, arXiv e-prints, arXiv:2510.13805, doi: 10.48550/arXiv.2510.13805 Giri, S. K., & Schneider, A. 2021, JCAP, 2021, 046, doi: 10.1088/1475-7516/2021/12/046 Gong, Z., Halder, A., Barreira, A., Seitz, S., & Friedrich, O. 2023, JCAP, 2023, 040, doi: 10.1088/1475-7516/2023/07/040 Gong, Z., Halder, A., Bohrdt, A., Seitz, S., & Gebauer, D. 2024, ApJ, 971, 156, doi: 10.3847/1538-4357/ad582e G´ orski, K. M., Hivon, E., Banday, A. J., et al. 2005, ApJ, 622, 759, doi: 10.1086/427976 Halder, A., Friedrich, O., Seitz, S., & Varga, T. N. 2021, MNRAS, 506, 2780, doi: 10.1093/mnras/stab1801 Hill, G. J., Gebhardt, K., Komatsu, E., et al. 2008, in Astronomical Society of the Pacific Conference Series, Vol. 399, Panoramic Views of Galaxy Formation and Evolution, ed. T. Kodama, T. Yamada, & K. Aoki, 115, doi: 10.48550/arXiv.0806.0183 16 

Homer, J., Friedrich, O., & Gruen, D. 2025, A&A, 699, A213, doi: 10.1051/0004-6361/202453339 Jeffrey, N., Alsing, J., & Lanusse, F. 2021, MNRAS, 501, 954, doi: 10.1093/mnras/staa3594 Jeffrey, N., & Wandelt, B. D. 2020, arXiv e-prints, arXiv:2011.05991, doi: 10.48550/arXiv.2011.05991 Jiang, B., Wu, T.-y., Zheng, C., & Wong, W. H. 2015, arXiv e-prints, arXiv:1510.02175, doi: 10.48550/arXiv.1510.02175 Kacprzak, T., Fluri, J., Schneider, A., Refregier, A., & Stadel, J. 2023, JCAP, 2023, 050, doi: 10.1088/1475-7516/2023/02/050 Kratochvil, J. M., Lim, E. A., Wang, S., et al. 2012, PhRvD, 85, 103513, doi: 10.1103/PhysRevD.85.103513 Lanzieri, D., Zeghal, J., Lucas Makinen, T., et al. 2025, A&A, 697, A162, doi: 10.1051/0004-6361/202451535 Laureijs, R., Amiaux, J., Arduini, S., et al. 2011, arXiv e-prints, arXiv:1110.3193, doi: 10.48550/arXiv.1110.3193 Leclercq, F. 2025, arXiv e-prints, arXiv:2509.13435, doi: 10.48550/arXiv.2509.13435 Lehman, K., Krippendorf, S., Weller, J., & Dolag, K. 2024, arXiv e-prints, arXiv:2411.08957, doi: 10.48550/arXiv.2411.08957 Lemos, P., Parker, L. H., Hahn, C., et al. 2023, in Machine Learning for Astrophysics, 18, doi: 10.48550/arXiv.2310.15256 Levi, M., Bebek, C., Beers, T., et al. 2013, arXiv e-prints, arXiv:1308.0847, doi: 10.48550/arXiv.1308.0847 LSST Science Collaboration, Abell, P. A., Allison, J., et al. 2009, arXiv e-prints, arXiv:0912.0201, doi: 10.48550/arXiv.0912.0201 Lucas Makinen, T., Heavens, A., Porqueres, N., et al. 2025, JCAP, 2025, 095, doi: 10.1088/1475-7516/2025/01/095 Maartens, R., Abdalla, F. B., Jarvis, M., & Santos, M. G. 2015, arXiv e-prints, arXiv:1501.04076, doi: 10.48550/arXiv.1501.04076 Makinen, T. L., Sui, C., Wandelt, B. D., Porqueres, N., & Heavens, A. 2024, arXiv e-prints, arXiv:2410.07548, doi: 10.48550/arXiv.2410.07548 Miles, C., Bohrdt, A., Wu, R., et al. 2021, Nature Communications, 12, 3905, doi: 10.1038/s41467-021-23952-w Miyazaki, S., Oguri, M., Hamana, T., et al. 2015, ApJ, 807, 22, doi: 10.1088/0004-637X/807/1/22 Murphy, K. P. 2022, Probabilistic Machine Learning: An introduction (MIT Press). http://probml.github.io/book1 Myles, J., Alarcon, A., Amon, A., et al. 2021, MNRAS, 505, 4249, doi: 10.1093/mnras/stab1515 Nguyen, N.-M., Schmidt, F., Tucci, B., Reinecke, M., & Kosti´ c, A. 2024, PhRvL, 133, 221006, doi: 10.1103/PhysRevLett.133.221006 Papamakarios, G., Pavlakou, T., & Murray, I. 2017, arXiv e-prints, arXiv:1705.07057, doi: 10.48550/arXiv.1705.07057 Porqueres, N., Heavens, A., Mortlock, D., Lavaux, G., & Makinen, T. L. 2023, arXiv e-prints, arXiv:2304.04785, doi: 10.48550/arXiv.2304.04785 Potter, D., Stadel, J., & Teyssier, R. 2017, Computational Astrophysics and Cosmology, 4, 2, doi: 10.1186/s40668-017-0021-1 Predehl, P., Andritschke, R., Arefiev, V., et al. 2021, A&A, 647, A1, doi: 10.1051/0004-6361/202039313 Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., & K¨ othe, U. 2020, arXiv e-prints, arXiv:2003.06281, doi: 10.48550/arXiv.2003.06281 Reeves, A., Nicola, A., Refregier, A., Kacprzak, T., & Machado Poletti Valle, L. F. 2024, JCAP, 2024, 042, doi: 10.1088/1475-7516/2024/01/042 Rubin, D. B. 1984, The Annals of Statistics, 12, 1151 , doi: 10.1214/aos/1176346785 Schneider, A., & Teyssier, R. 2015, JCAP, 2015, 049, doi: 10.1088/1475-7516/2015/12/049 Scoccimarro, R., & Couchman, H. M. P. 2000, Monthly Notices of the Royal Astronomical Society, 325, 1312, doi: 10.1046/j.1365-8711.2001.04662.x Secco, L. F., Samuroff, S., Krause, E., et al. 2022, PhRvD, 105, 023515, doi: 10.1103/PhysRevD.105.023515 Sefusatti, E., Crocce, M., Pueblas, S., & Scoccimarro, R. 2007, Physical Review D, 75, 083522, doi: 10.1103/PhysRevD.75.083522 Sgier, R., Fluri, J., Herbel, J., et al. 2021, JCAP, 2021, 047, doi: 10.1088/1475-7516/2021/02/047 Sgier, R. J., R´ efr´ egier, A., Amara, A., & Nicola, A. 2019, JCAP, 2019, 044, doi: 10.1088/1475-7516/2019/01/044 Spergel, D., Gehrels, N., Baltay, C., et al. 2015, arXiv e-prints, arXiv:1503.03757, doi: 10.48550/arXiv.1503.03757 Takada, M., Ellis, R. S., Chiba, M., et al. 2014, PASJ, 66, R1, doi: 10.1093/pasj/pst019 Talts, S., Betancourt, M., Simpson, D., Vehtari, A., & Gelman, A. 2018, arXiv e-prints, arXiv:1804.06788, doi: 10.48550/arXiv.1804.06788 The Beyond-2pt Collaboration, Krause, E., Kobayashi, Y., et al. 2025, ApJ, 990, 99, doi: 10.3847/1538-4357/ad781d Thomsen, A., Bucko, J., Kacprzak, T., et al. 2025, arXiv e-prints, arXiv:2511.04681, doi: 10.48550/arXiv.2511.04681 17 

Tucci, B., & Schmidt, F. 2023, arXiv e-prints, arXiv:2310.03741, doi: 10.48550/arXiv.2310.03741 von Wietersheim-Kramsta, M., Lin, K., Tessore, N., et al. 2024, arXiv e-prints, arXiv:2404.15402, doi: 10.48550/arXiv.2404.15402 Wang, Y., & He, P. 2025, PhRvD, 111, L041302, doi: 10.1103/PhysRevD.111.L041302 Weiler, M., & Cesa, G. 2019, in Conference on Neural Information Processing Systems (NeurIPS). https://arxiv.org/abs/1911.08251 Zhao, G.-B., Wang, Y., Ross, A. J., et al. 2016, MNRAS, 457, 2377, doi: 10.1093/mnras/stw135 Zhou, A. J., Li, X., Dodelson, S., & Mandelbaum, R. 2024, PhRvD, 110, 023539, doi: 10.1103/PhysRevD.110.023539