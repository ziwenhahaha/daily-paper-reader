Title: RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution

URL Source: https://arxiv.org/pdf/2602.16932v1

Published Time: Fri, 20 Feb 2026 01:13:44 GMT

Number of Pages: 9

Markdown Content:
# RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution 

# Jinming Nian 

Santa Clara University Santa Clara, CA, USA jnian@scu.edu 

# Fangchen Li 

Independent Researcher Bothell, WA, USA fangchen.li@outlook.com 

# Dae Hoon Park 

Walmart Global Tech Sunnyvale, CA, USA dae.hoon.park@walmart.com 

# Yi Fang 

Santa Clara University Santa Clara, CA, USA yfang@scu.edu 

## Abstract 

Retrieval algorithms like BM25 and query likelihood with Dirich-let smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which can-didate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankE-volve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effec-tive, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms. Code is available here. 

## CCS Concepts 

â€¢ Information systems â†’ Retrieval models and ranking .

## Keywords 

Lexical Retrieval, Evolutionary Search, LLM-as-optimizer, Auto-mated Algorithm Discovery 

## 1 Introduction 

Lexical retrieval algorithms such as BM25 [ 17 ] and query-likelihood (QL) [ 16 ] remain remarkably strong and efficient first-stage rankers. Over the decades, numerous variants have been proposed, including BM25T [ 6], BM25-adapt [ 11 ], BM25+ [ 12 ], BM25L [ 13 ], and alter-native QL smoothing methods such as Dirichlet smoothing and Jelinekâ€“Mercer [ 23 ]. Yet these improvements have largely relied on parameter tuning and human intuition over individual scoring components. This raises a natural question: can we automate the discovery of improved lexical retrieval algorithms? Recent devel-opments in large language models (LLMs) for automated scientific discovery offer a promising path for such automation. Systems like AlphaEvolve [ 15 ], Evolution of Heuristics [ 9 ], and ShinkaEvolve [ 7 ]   

> 1BEIR: ArguAna, FiQA, NFCorpus, SciFact, SciDocs, TREC-COVID. BRIGHT: Biology, Earth Science, Economics, Pony, StackOverflow, TheoremQA. 0100 200 300

Evolution Step   

> 0.41
> 0.42
> 0.43
> 0.44
> Combined Score
> Seed Program:
> Pyserini BM25 Equivalent
> 0100 200

Evolution Step 

> Seed Program:
> Pyserini QL-Dir Equivalent

Figure 1: Combined score over evolution steps for two seed programs. The combined score is the optimization target, de-fined as 0.8Ã—Avg Recall@100 +0.2Ã—Avg nDCG@10 , averaged across 12 IR datasets 1.

represent candidate solutions as executable code and leverage LLMs to iteratively mutate and recombine programs, with performance-driven selection. These methods have produced strong results in mathematics and combinatorial optimization, but have not yet been applied to information retrieval. We introduce RankEvolve, a pro-gram evolution setup in which each candidate ranking algorithm is a self-contained Python program ( âˆ¼300 lines) scored by an evalu-ator across multiple datasets. The evaluatorâ€™s per-dataset metrics, a single fitness score, sampled top-performing and inspirational candidates, and prior attempted changes together form the prompt for a coding LLM to propose the next mutation. Based on AlphaE-volveâ€™s description, the candidates are managed by a combination of MAP-Elites [14] and island-based evolutionary databases [20]. Starting from two seed programs, BM25 and query likelihood with Dirichlet smoothing, we evolve for several hundred steps. The resulting algorithms are novel and effective, introducing scoring mechanisms absent from either seed family. To assess generaliza-tion, we evaluate the evolved programs on held-out BEIR [ 22 ] and BRIGHT [ 19 ] subsets, as well as TREC DL 2019 [ 3], and DL 2020 [ 2 ], and find promising transfer beyond the datasets used during evo-lution. Our contributions are: (1) RankEvolve is the first attempt at evolving entire retrieval algorithms via LLM-guided program search; (2) we study the importance of seed program design, exam-ining how structural freedom and abstraction affect the evolution  

> arXiv:2602.16932v1 [cs.IR] 18 Feb 2026 Conferenceâ€™17, July 2017, Washington, DC, USA Nian et al.

outcome; and (3) we show that RankEvolve can discover algorithms with novel scoring motifs that transfer to unseen datasets, sug-gesting that evaluator-guided program evolution with LLMs is a promising way forward for automating IR research. 

## 2 Related Work 

The idea of automatically discovering ranking functions has been explored through genetic programming (GP). Fan et al . [5] pro-posed ARRANGER, a framework that uses GP to discover ranking functions from arithmetic operators ( +, Ã—, log ) applied to IR fea-tures (tf, idf, dl). Cummins and Oâ€™Riordan [4] evolved local and global term-weighting schemes using a similar GP setup, producing functions that competed with BM25. A simpler alternative is grid search over existing hyperparameters [ 21 ], though this is limited to tuning a fixed function. RankEvolve differs in a fundamental way: classical GP evolves expression trees of arithmetic primitives by randomly swapping subtrees without understanding what the expression computes. RankEvolve uses an LLM as the mutation operator over a Python program, enabling reason-informed edits (e.g., recognizing that a signal for the documentâ€™s coverage of the query is missing and introducing one) and a far richer search space. The learning-to-rank (LTR) paradigm [ 10 ] is also related, though the key distinction is that LTR combines existing features through learned weights, while RankEvolve proposes entirely new features and also evolves the relevance scoring function itself. 

## 3 Method 

We frame the discovery of improved retrieval algorithms as pro-gram synthesis via LLM-guided evolutionary search. RankEvolve iteratively mutates a seed program using an LLM and selects among variants based on retrieval performance. The overall pipeline con-sists of four components: (1) a seed program and a system prompt, which together define the search space; (2) a population database maintaining diversity via island-based evolution [ 20 ] and MAP-Elites [ 14 ]; (3) mutation guided by structured prompts; and (4) an evaluator that computes fitness over retrieval datasets. 

## 3.1 Search Space 

The seed program and system prompt jointly define the search space. The seed program defines the evolvable interface: code regions that the LLM is permitted to modify. To maximize evolutionary free-dom, we decompose each ranking function into a small number of abstract components. For the BM25 seed, we define three evolvable components: document representation, query representation, and scoring function. The initial behavior reproduces classic BM25: BM25 (ğ‘, ğ‘‘ ) =âˆ‘ï¸  

> ğ‘¡ âˆˆğ‘

IDF (ğ‘¡ ) Â· tf (ğ‘¡, ğ‘‘ ) Â· ( ğ‘˜ 1 + 1)

tf (ğ‘¡, ğ‘‘ ) + ğ‘˜ 1 Â·



1 âˆ’ ğ‘ + ğ‘ Â· |ğ‘‘ |

> avgdl

 (1) IDF (ğ‘¡ ) = log 

 ğ‘ âˆ’ df (ğ‘¡ ) + 0.5

df (ğ‘¡ ) + 0.5



, (2) where tf (ğ‘¡, ğ‘‘ ) is the term frequency of ğ‘¡ in document ğ‘‘ , df (ğ‘¡ ) is the number of documents that contain ğ‘¡ , |ğ‘‘ | is the document length, avgdl is the average document length, and ğ‘˜ 1 = 0.9 and ğ‘ = 0.4

following Pyserini [ 8 ] defaults. For the QL-Dir seed, we add a fourth component, the collection language model, to the evolvable inter-face. The initial program implements: QL-Dir (ğ‘, ğ‘‘ ) =âˆ‘ï¸  

> ğ‘¡ âˆˆğ‘

log 

 tf (ğ‘¡, ğ‘‘ ) + ğœ‡ Â· ğ‘ƒ (ğ‘¡ | ğ¶ )|ğ‘‘ | + ğœ‡ 



, (3) where ğ‘ƒ (ğ‘¡ | ğ¶ ) = tf (ğ‘¡, ğ¶ )/| ğ¶ | is the collection language model probability of term ğ‘¡ , ğ¶ is the corpus, and ğœ‡ = 2000 following Pyserini. The system prompt guides how to evolve the seed pro-gram. It details the design principles that encourage exploration of information-theoretic, probabilistic, and fundamentally novel ideas while discouraging ad-hoc constraints without justification. It also specifies the optimization objective, metrics, datasets, and evolvable versus restricted components. Together, these elements aim to maximize the LLMâ€™s freedom while ensuring every candidate remains a valid, executable retrieval system. 

## 3.2 Population Management 

We maintain a population of candidate programs using a combi-nation of island-based evolution [ 20 ] and MAP-Elites [ 14 ]. For the island-based model, the population is partitioned into ğ¾ indepen-dently evolving islands. Each island maintains its own MAP-Elites grid, including a record of its best programs. Programs inherit their parentâ€™s island, preserving lineage isolation. Within each island, MAP-Elites maps programs to a grid defined by two dimensions: complexity (code length) and diversity (edit distance from the pop-ulation). Each dimension is divided into ğµ bins, yielding ğµ Ã— ğµ cells per island. A new program is accepted into a cell only if the cell is unoccupied or the new program has strictly higher optimization tar-get score than the current occupant. This mechanism is crucial for avoiding local minima, because always picking the best-performing candidate for mutation would quickly stagnate the search. Every ğ‘€ 

iterations, the top ğ›¾ fraction of programs from each island migrate to adjacent islands to encourage new variants. Programs that have migrated previously are excluded to prevent duplication. 

## 3.3 Mutation Proposal 

At each iteration a parent program is selected from the current island via one of three strategies, chosen at random: (1) exploration (probability ğ‘ ğ‘’ ): uniform random sampling from the island; (2) exploitation (probability ğ‘ ğ‘¥ ): sampling from the elite archive; or (3) weighted (probability 1 âˆ’ ğ‘ ğ‘’ âˆ’ ğ‘ ğ‘¥ ): performance-proportional sampling from the island. Additionally, we include ğ‘‡ best programs and ğ‘† randomly sampled programs from the same island to provide the LLM with diverse reference points. Each program is paired with its detailed evaluation metrics described in Section 3.4. These, together with the system prompt, are presented to the LLM, which proposes a mutation in a SEARCH/REPLACE diff format. 

## 3.4 Evaluator 

The evaluator imports a candidate program, executes its full pipeline (tokenization, indexing, and retrieval) on a set of evaluation datasets, and returns per-dataset nDCG@10, Recall@100, and latency mea-surements, all of which are stored alongside the program in the population database. For population management, sampling pri-ority, and as the optimization target, we use a single fitness score: 

0.8 Ã— Avg Recall@100 + 0.2 Ã— Avg nDCG@10 , where the averages RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution Conferenceâ€™17, July 2017, Washington, DC, USA 

Table 1: Macro-averaged results on unseen BRIGHT (6), BEIR (8), and TREC DL 19/20 datasets. Best per-group scores bolded. 

> â˜…

Evolved with RankEvolve. â€ Significant over the respective seed (per-query paired ğ‘¡ -test, ğ‘ < 0.05 ). 

BRIGHT BEIR TREC DL Method nDCG@10 R@100 nDCG@10 R@100 nDCG@10 R@100 BM25 [17] 10.55 32.11 48.16 70.95 62.16 46.02 BM25+ [12] 9.72 31.54 45.71 69.93 61.07 46.69 BM25-adpt [11] 11.05 34.67 44.62 69.55 60.90 48.23 

BM25 â˜… 11.79 â€  37.51 â€  47.90 72.43 â€  64.57 â€  47.10 QL-Dir [23] 9.52 32.48 44.15 68.72 57.03 46.69 QL-JM [23] 10.17 31.08 40.61 65.79 56.65 41.05 QL-Dir â˜… 11.42 â€  36.33 â€  46.46 â€  70.22 â€  60.68 â€  47.96 

are taken across datasets. The weighting reflects the first-stage retrieval setting. The primary objective is to maximize recall of relevant documents for a downstream reranker, while nDCG@10 serves as a secondary signal for ranking quality of the retrieved set. 

## 4 Experiments 4.1 Baselines 

BM25 [ 17 ] is the classical TF-IDF based ranking function with saturation and document length normalization. BM25+ [ 12 ] in-troduces a lower-bound term frequency normalization to address BM25â€™s deficiency to overly penalize long documents, ensuring that additional occurrences of a query term always contribute posi-tively to the score. BM25-adpt [ 11 ] extends this further by using document-adaptive ğ‘˜ 1 parameters instead of a single global value. For language model baselines, QL-Dir [ 23 ] scores documents via query likelihood with Dirichlet smoothing (Section 3.1), while QL-JM [ 23 ] applies Jelinek-Mercer smoothing, where the query like-lihood ğ‘ƒ (ğ‘ | ğ‘‘ ) = Ãğ‘¡ âˆˆğ‘ (1 âˆ’ ğ›¼ ) ğ‘ƒ (ğ‘¡ | ğ‘‘ ) + ğ›¼ ğ‘ƒ (ğ‘¡ | ğ¶ ) is a linear combination between the maximum-likelihood document language model and the collection language model. 

## 4.2 Setup 

RankEvolve builds on OpenEvolve [ 18 ], an open-source implemen-tation of AlphaEvolve. We use GPT-5.2 via API with temperature 0.85, and set ğ¾ = 3, ğµ = 12 , ğ‘€ = 20 , ğ›¾ = 0.15 , ğ‘‡ = 4, and ğ‘† = 4

(Sections 3.2 and 3.3). RankEvolve ran for 300 steps from the BM25 seed and 200 steps from the QL-Dir seed, yielding the best-scoring programs at steps 293 and 182 respectively. Figure 1 shows the opti-mization target trajectory over the course of evolution. We evaluate on three benchmark suites: 14 datasets from BEIR [ 22 ], all 12 sub-sets of BRIGHT [ 19 ], and TREC Deep Learning 2019 [ 3 ]/2020 [ 2 ], totaling 28 datasets. We exclude BioASQ, Signal-1M (RT), TREC-NEWS, and Robust04 as they are not publicly available on BEIR GitHub, and omit MSMARCO as it serves as the corpus for TREC DL. Of these 28 datasets, only 12 are used during evolution (from BEIR: ArguAna, FiQA, NFCorpus, SciFact, SciDocs, TREC-COVID; from BRIGHT: Biology, Earth Science, Economics, Pony, StackOver-flow, TheoremQA). The remaining 16 are held out entirely and used solely to test generalization. 

## 4.3 Results 

Table 1 presents the macro-averaged results for each benchmark on the 16 held-out datasets. BM25 â˜… outperforms all BM25 base-lines on BRIGHT and BEIR Recall@100 and achieves the highest nDCG@10 on TREC DL. QL-Dir â˜… consistently outperforms both QL-Dir and QL-JM across all three benchmarks. Gains are statis-tically significant over the respective seeds on most evaluation groups, and extend to datasets not seen during evolution, indi-cating that RankEvolve discovers better retrieval algorithms that generalize well rather than overfitting to the evaluator signal. Figure 2 shows the Recall@100 and nDCG@10 trajectories across both evolutionary runs. Recall@100 improves nearly monotonically, while nDCG@10 occasionally regresses. This is expected: the opti-mization target weights Recall ( 0.8Ã—) far more heavily than nDCG (0.2Ã—), so RankEvolve will accept any mutation that trades a small nDCG loss for a larger Recall gain. The pattern is visible in both seeds, with nDCG dipping at exactly the steps where Recall makes its largest jumps. The combined score 1 is monotonically increasing throughout optimization. When we optimize a weighted sum, we implicitly authorize the optimizer to sacrifice the pawn to advance the queen. RankEvolve simply makes this trade visible. To understand what RankEvolve discovered, we now analyze the best-evolved programs in detail. 

## 4.4 The Evolved BM25 Algorithm 

After 293 evolution steps, the best evolved algorithm converges to a multi-channel, modulated scoring function that operates entirely over lexical features, yet has substantially departed from BM25 in structure. The top-level scoring function is: 

ğ‘† (ğ‘, ğ‘‘ ) = ğ‘… (ğ‘ base , ğ‘‘ base ) + ğ‘¤ pfx Â· ğ‘… (ğ‘ pfx , ğ‘‘ pfx )+ ğ‘¤ bi Â· ğ‘… (ğ‘ bi , ğ‘‘ bi ) + ğ‘¤ mic Â· ğº (ğ‘ ) Â· ğ‘… (ğ‘ mic , ğ‘‘ mic ), (4) where ğ‘… is a shared core scoring function applied across four parallel token spaces. Base tokenization uses the standard Lucene tokenizer. Prefix tokenization ( ğ‘¤ pfx = 0.1) truncates each token to its first 5 characters, acting as a cheap stemming approximation. Bigram tokenization ( ğ‘¤ bi = 0.08 ) concatenates consecutive token pairs. Micro tokenization ( ğ‘¤ mic = 0.12 ) uses rolling character 3-grams for sub-word matching, gated by ğº (ğ‘ ) = ğœ (( IDF (ğ‘ ) âˆ’ 2.2)/ 1.0), a sigmoid of the queryâ€™s mean IDF that activates sub-word matching only for rare or technical queries. Conferenceâ€™17, July 2017, Washington, DC, USA Nian et al. 0 100 200 300 

Evolution Step    

> 0.45
> 0.46
> 0.47
> 0.48
> Recall@100
> BM25 Seed
> Avg Recall@100
> 0100 200 300

Evolution Step   

> 0.270
> 0.275
> 0.280
> 0.285
> 0.290
> 0.295
> nDCG@10
> BM25 Seed
> Avg nDCG@10
> 0100 200

Evolution Step   

> 0.455
> 0.460
> 0.465
> 0.470
> 0.475
> Recall@100
> QL-Dir Seed
> Avg Recall@100
> 0100 200

Evolution Step 

> 0.260
> 0.265
> 0.270
> 0.275
> 0.280
> nDCG@10
> QL-Dir Seed
> Avg nDCG@10

Figure 2: Evolution trajectories for the BM25 seed (left) and Dirichlet seed (right). Recall@100 improves nearly monotonically in both runs, while nDCG@10 occasionally regresses at the same steps, reflecting deliberate trades made by the evolutionary process to maximize the optimization target. 

The shared scoring function ğ‘… combines a base evidence term with a chain of bounded multipliers: 

ğ‘… (ğ‘, ğ‘‘ ) = ln (1 + ğ¸ ) Â· ğµ cov Â· ğµ spec Â· ğµ coord Â· ğµ anc 

ğµ len 

, (5) where ğ¸ = Ãğ‘¡ âˆˆğ‘€ ğ‘¤ (ğ‘¡ ) Â· ln (1 + tf (ğ‘¡, ğ‘‘ )) accumulates weighted log-TF evidence over matched query terms ğ‘€ , and ğµ âˆ— are multipliers of the form 1 +ğ›¾ Â· (Â·) with small ğ›¾ that modulate the score based on query-term coverage, topical specificity, term coordination, rare-term anchoring, and document length. Their individual effects are gentle, but their combined effect can be substantial. The outer ln (1 + ğ¸ )

applies a second layer of saturation, making the scoring robust to outlier term frequencies through double log-compression. Full definitions of all components appear in Appendix A. We highlight three aspects of the evolved design that are es-pecially notable. First, the composite term weight ğ‘¤ (ğ‘¡ ) multiplies three separate functions of IDF (ğ‘¡ ), which together suppress stopword-like terms (low IDF) while leaving rare terms nearly unaffected. 

RankEvolve has learned a soft stopword filter without ever being told about stopwords. Second, the specificity multiplier ğµ spec uses pointwise mutual information between each query term and the candidate document to reward documents where matched terms appear with higher-than-expected frequency, an idea closely related to the collection language model in probabilistic retrieval. Third, the length dampener ğµ len = 1 + 0.15 Â· ln (1 + (| ğ‘‘ | + 1)/( avgdl + 1)) 

replaces BM25â€™s linear normalization with a gentler logarithmic form, aligning with findings from Lv and Zhai [12] that BM25 over-penalizes long documents. RankEvolve arrived at all of these components through evolu-tionary search alone, without any explicit guidance toward them. The prefix channel approximates stemming, the multi-layered IDF weighting acts as a soft stopword filter, the PMI-based multiplier (ğµ spec ) resembles a collection language model, and the logarith-mic length normalization independently addresses a known BM25 weakness. To the best of our knowledge, no exact formulation in the existing literature matches the evolved scoring function, yet these well-studied concepts emerged naturally from a purely metric-driven search process. This suggests that such ideas may have been abundant enough in the LLMâ€™s training data to leave a strong im-pression in its parametric knowledge, or that they are perhaps inevitable solutions to the lexical retrieval problem, or both. 

## 4.5 The Evolved Query Likelihood Algorithm 

After 182 evolution steps starting from classical query likelihood with Dirichlet smoothing, the best evolved algorithm retains the probabilistic language-modeling foundation but departs substan-tially from the standard formulation. The top-level scoring function is: 

ğ‘† (ğ‘, ğ‘‘ ) =âˆ‘ï¸  

> ğ‘¡ âˆˆğ‘ ğ‘¢

ğœ” (ğ‘¡ ) Ëœğ‘  (ğ‘¡, ğ‘‘ ) + âˆ‘ï¸  

> ğ‘¡ âˆˆğ‘ ğ‘¢

ğ‘š (ğ‘¡, ğ‘‘ ) + AND (ğ‘, ğ‘‘ ) + LP (ğ‘‘ ),

(6) where ğ‘ ğ‘¢ is the set of unique query terms in the vocabulary. The score decomposes into four additive components: a weighted sum of per-term relevance scores Ëœğ‘  , a missing-term penalty ğ‘š , a soft-AND coverage bonus, and a log-normal document length prior. Unlike the evolved BM25, which restructures scoring into a product of bounded multipliers (Equation 5), the evolved query likelihood algorithm retains an additive architecture, but augments it with coordination and penalty mechanisms that standard query likeli-hood lacks entirely. Full definitions of all components appear in Appendix B. We highlight four aspects that represent the most significant departures from the seed. First, RankEvolve replaces the standard collection language model ğ‘ƒ (ğ‘¡ | ğ¶ ) with a three-stage enriched estimate. The raw collection probabilities are raised to a power 

ğœ = 0.85 and renormalized (flattening the distribution to give more mass to rare terms), then interpolated with a document-frequency language model ğ‘ƒ df (ğ‘¡ ) = df (ğ‘¡ )/ ğ‘ that is robust to bursty docu-ments, and finally mixed with a small uniform component as a safety floor. RankEvolve has independently discovered that flatten-ing the collection language model improves retrieval , related to the information-based retrieval models of Clinchant and Gaussier [1]. Second, raw term frequency is replaced by tf (ğ‘¡, ğ‘‘ )ğ›½ (ğ‘¡ ) , where the exponent ğ›½ (ğ‘¡ ) âˆˆ [ 0.70 , 1.0] is a function of normalized IDF. Common terms saturate aggressively ( ğ›½ â‰ˆ 0.70 ), while rare terms RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution Conferenceâ€™17, July 2017, Washington, DC, USA 

Table 2: Ablation on code structure design (BM25 family). Each row reports the best program found by RankEvolve under a given level of structural freedom; evolve step and total search budget are shown in parentheses. The optimization target is 0.8 Ã— R@100 + 0.2 Ã— nDCG@10 , macro-averaged over the 12 datasets used during evolution; unseen evaluation covers 16 held-out datasets (see Section 4.2). BRIGHT BEIR TREC DL Optimization Target Code Structure nDCG@10 R@100 nDCG@10 R@100 nDCG@10 R@100 Seen (12 datasets) Unseen (16 datasets) Original (step 0) 10.55 32.11 48.16 70.95 62.16 46.02 40.30 49.77 Constrained (step 115/200) 11.36 34.68 48.27 70.75 61.86 45.44 42.87 50.47 Composable (step 185/200) 11.97 37.27 46.95 70.30 63.07 48.62 43.31 51.33 Freeform (step 177/200) 11.39 36.92 46.87 71.27 61.17 45.40 44.35 51.20 Freeform (step 293/300) 11.79 37.51 47.90 72.43 64.57 47.10 44.58 52.22 

preserve the full TF signal ( ğ›½ â‰ˆ 1.0). Standard Dirichlet smooth-ing applies no TF saturation; BM25 applies uniform saturation; the evolved BM25 also uses uniform log-saturation. The per-term adaptive exponent is strictly more expressive than all of them. Third, the evolved algorithm applies a leaky rectifier to per-term scores. Where standard implementations discard negative per-term contributions, the evolved function retains them at 12% strength. Together with a separate missing-term penalty that applies the Dirichlet smoothing-only score (scaled to 7%) for completely absent query terms, this creates a layered penalty architecture at two granularities: per-term weakness and per-term absence. The small magnitudes suggest the evolutionary process found that recall is fragile and penalties must be conservative. Fourth, the document length prior LP (ğ‘‘ ) = âˆ’0.06 Â· ( log (| ğ‘‘ |) âˆ’ 

log (avgdl )) 2 penalizes deviation in both directions from the corpus average on a log scale. This contrasts with the evolved BM25â€™s length dampener, which only penalizes long documents. The qua-dratic form reflects the insight that very short documents are also problematic. 

## 4.6 Convergent Principles Across Seeds 

The evolved query likelihood algorithm retains a recognizable language-modeling structure: the core score is still a log-likelihood ratio with Dirichlet smoothing, and the missing-term penalty is lit-erally the Dirichlet score at tf = 0. The evolutionary process chose to augment the probabilistic foundation rather than abandon it. This contrasts with the evolved BM25, which restructured the scor-ing function more radically into a product of bounded multipliers applied to a double-log-compressed evidence term. Despite starting from fundamentally different baselines, one from a probabilistic lan-guage model and the other from a TF-IDF based retrieval function, the two evolved algorithms exhibit convergence in their high-level strategies. Both independently discovered term-frequency satura-tion, soft stopword filtering, explicit coordination mechanisms, and gentle length normalization. Yet they implement these strategies through different architectural paradigms: multiplicative modula-tion versus additive penalties, multi-channel tokenization versus single-space gating, uniform log-saturation versus per-term adap-tive exponents. The fact that both trajectories converged to the same abstract principles from very different starting points suggests that these are not artifacts of any particular scoring framework, but rather fundamental requirements of effective lexical retrieval. 

## 5 Ablation Study 

Effect of seed structure on search space and convergence. We study how the structural freedom of the seed program shapes evolution. We design three functionally identical BM25 seeds with increas-ing degrees of freedom, and evolve each using the procedure from Section 3. The â€œconstrainedâ€ seed fixes the BM25 formula and per-mits only hyperparameter tuning and selection among predefined component variants (e.g., Lucene vs. Robertson IDF). This setting is analogous to automated grid search. The â€œcomposableâ€ seed de-composes retrieval into modular primitives whose formulas may be rewritten or extended, but keeps the overall pipeline structure fixed. The â€œfreeformâ€ seed, used in our main experiments, defines only query representation, document representation, and scoring function. It fixes the evaluator interface and leaves everything else evolvable. Table 2 shows a clear trend: greater structural freedom yields monotonically higher optimization target scores on both the 12 datasets used by the evaluator and the 16 held-out datasets. Con-strained evolution converges earliest but produces the smallest gains, confirming that parameter tuning alone has limited potential. The composable variant improves further by introducing novel scor-ing primitives, yet its fixed pipeline prevents deeper architectural changes. The freeform variant converges last but achieves the best scores on both seen and unseen datasets, demonstrating again that the improvements generalize rather than overfit to the development suite. These results suggest that seed program design sets an upper bound on what RankEvolve can discover. Restrictive seeds bias the search towards local optima near the original formulation, whereas seeds with more structural freedom expand the search space and allow evolution to identify non-obvious improvements. 

Complementary strengths across structures. Beyond the aggre-gate trend, Table 2 reveals that the best programs evolved from differently structured seeds exhibit complementary per-benchmark strengths. The freeform variant (step 293/300) achieves the high-est scores on BRIGHT Recall@100, BEIR Recall@100, and TREC DL nDCG@10, yet it is not uniformly dominant: its TREC DL Re-call@100 (47.10) lags behind the composable variant (48.62); its BEIR nDCG@10 (47.90) falls short of the constrained variant (48.27); and notably also under-perform the original BM25 baseline (48.16). These per-metric differences are not random noise. They reflect the distinct inductive biases each seed structure imposes on the Conferenceâ€™17, July 2017, Washington, DC, USA Nian et al. 

Table 3: Average per-document indexing latency and per-query retrieval latency across all 28 datasets. Lowest val-ues are bolded, second lowest are underlined . â˜…Evolved with RankEvolve.                       

> Method Indexing (ms/doc) Query (ms/query) BM25 1.79 56.72
> BM25 â˜…Constrained 1.85 58.50 BM25 â˜…Composable 1.77 111.49 BM25 â˜…Freeform (step 177) 2.37 171.52 BM25 â˜…Freeform (step 293) 2.81 648.89 QL-Dir 2.02 178.26
> QL-JM 1.95 212.24 QL-Dir â˜…Freeform (step 182) 2.02 325.41

search trajectory. The constrained seed, limited to selecting among predefined component variants and tuning their parameters, pre-serves the original BM25 formula almost intact. This conservatism prevents it from discovering novel scoring primitives, but it also shields against degradations on benchmarks where the classical formulation is almost near-optimal. The composable seed allows evolution to rewrite individual scoring primitives while keeping the pipeline skeleton fixed. This additional freedom lets it discover recall-oriented modifications (e.g., aggressive document-length nor-malization and alternative term-frequency saturation curves) that boost TREC DL Recall@100beyond what either the constrained or freeform variants achieve. The freeform seed removes even the pipeline constraint, enabling evolution to restructure the scoring architecture itself. This produces the largest aggregate gains but oc-casionally trades precision on narrow evaluation slices for broader improvements across the board. 

Influence on the discovery process. The fact that no single struc-tural configuration dominates on every metric underscores a key insight: RankEvolveâ€™s value lies not only in the single best program it returns, but also in the diverse family of high-performing pro-grams it is able to discover. Each seed structure guides evolution through a different region of program space, surfacing solutions that emphasize different trade-offs between nDCG and Recall, or between robustness on short web queries (TREC DL) and complex reasoning-intensive queries (BRIGHT). In practice, one could run RankEvolve from multiple seed structures and select or ensemble the best-performing variant per deployment scenario, treating the evolutionary trajectories as a form of structured exploration rather than a single-objective optimization. Despite these per-benchmark variations, the freeform seed re-mains the strongest choice on average: it achieves the highest op-timization target on both the 12 seen datasets and the 16 unseen datasets, confirming that maximal structural freedom provides the best riskâ€“award trade-off when a single general-purpose retrieval function is needed. 

## 6 Latency 

Table 3 reports per-document indexing and per-query retrieval latency averaged across all 28 datasets. Indexing overhead is neg-ligible across all variants. Query latency increases with program complexity: the best performing BM25 â˜… (step 293) is roughly 11 Ã—

slower than the seed BM25. This is entirely expected. While la-tency statistics are visible to the LLM during evolution, they are never part of the optimization target nor explicitly mentioned to be mindful of in the system prompt, so the search process has no pressure to favor efficient solutions. Incorporating latency as an explicit optimization objective is a straightforward extension that we leave to future work. Despite this, the latency profile offers a useful lens into how RankEvolve discovers improvements at different levels of structural freedom. The constrained variant adds virtually no overhead (58.50 vs. 56.72 ms/query), confirming that its gains stems almost entirely from parameter tuning rather than algorithmic breakthrough. Yet even this minimal-complexity evolution meaningfully improves the optimization target (Table 2), showing that the simplest form of RankEvolve, which is effectively an automated parameter search, already provides value. Among the freeform variants, the trajectory from step 177 to step 293 is very interesting. At step 177 the program had already achieved strong effectiveness (Table 2), with query latency still within a modest 3Ã— of the baseline. The following 116 steps of evolution continued to improve recall and nDCG, but at the cost of a 3.8Ã— further increase in query latency, indicating that later evolutionary steps exploit increasingly complex scoring mecha-nisms whose marginal effectiveness gains carry disproportionate computational cost. This mirrors a common pattern in program synthesis: early mutations tend to explore high-return structural changes, while later mutations add refinements that are effective but expensive. 

## 7 Conclusion and Future Work 

We introduce RankEvolve, a framework that applies LLM-guided program evolution to discover lexical retrieval algorithms. Evolved from BM25 and QL-Dir seeds, the resulting functions consistently outperform their seeds and established variants on held-out bench-mark datasets. The evolved programs independently rediscover and reformulate well-studied IR concepts. Our ablation confirms that the structural freedom of the seed program determines the ceiling of what RankEvolve can discover. The evolved algorithms, while effective, are much more complex than their seeds. Defin-ing and encouraging elegance is a natural next step. More broadly, RankEvolve optimizes whatever objective the evaluator defines, which suggests the framework could extend beyond lexical retrieval to dense retrieval, learned sparse representations, and even LLM reranking methods. A straightforward extension is to incorporate efficiency constraints as an explicit optimization objective. We hope RankEvolve motivates further exploration of LLM-guided program evolution as a tool for automated IR research. RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution Conferenceâ€™17, July 2017, Washington, DC, USA 

## References 

[1] StÃ©phane Clinchant and Eric Gaussier. 2010. Information-based models for ad hoc IR. In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Geneva, Switzerland) (SIGIR â€™10) . Association for Computing Machinery, New York, NY, USA, 234â€“241. doi:10. 1145/1835449.1835490 [2] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020. Overview of the TREC 2020 Deep Learning Track. In Proceedings of the Twenty-Ninth Text REtrieval Conference, TREC 2020, Virtual Event [Gaithersburg, Maryland, USA], November 16-20, 2020 (NIST Special Publication, Vol. 1266) , Ellen M. Voorhees and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST). https://trec.nist.gov/pubs/trec29/papers/OVERVIEW.DL.pdf [3] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen M Voorhees, and Ian Soboroff. 2025. Overview of the TREC 2022 deep learning track. arXiv preprint arXiv:2507.10865 (2025). [4] Ronan Cummins and Colm Oâ€™Riordan. 2006. Evolving local and global weighting schemes in information retrieval. Inf. Retr. 9, 3 (2006), 311â€“330. doi:10.1007/ S10791-006-1682-6 [5] Weiguo Fan, Michael D. Gordon, and Praveen Pathak. 2004. A generic ranking function discovery framework by genetic programming for information retrieval. 

Inf. Process. Manag. 40, 4 (2004), 587â€“602. doi:10.1016/J.IPM.2003.08.001 [6] Mathias GÃ©ry and Christine Largeron. 2012. BM25t: a BM25 extension for focused information retrieval. Knowl. Inf. Syst. 32, 1 (2012), 217â€“241. doi:10.1007/S10115-011-0426-0 [7] Robert Tjarko Lange, Yuki Imajuku, and Edoardo Cetin. 2025. ShinkaE-volve: Towards Open-Ended And Sample-Efficient Program Evolution. arXiv:2509.19349 [cs.CL] https://arxiv.org/abs/2509.19349 [8] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A Python Toolkit for Reproducible In-formation Retrieval Research with Sparse and Dense Representations. In SIGIR â€™21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 , Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai (Eds.). ACM, 2356â€“2362. doi:10.1145/3404835.3463238 [9] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. 2024. Evolution of Heuristics: Towards Efficient Auto-matic Algorithm Design Using Large Language Model. In Forty-first Interna-tional Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024 (Proceedings of Machine Learning Research, Vol. 235) , Ruslan Salakhutdi-nov, Zico Kolter, Katherine A. Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (Eds.). PMLR / OpenReview.net, 32201â€“32223. https://proceedings.mlr.press/v235/liu24bs.html [10] Tie-Yan Liu. 2011. Learning to Rank for Information Retrieval . Springer. doi:10. 1007/978-3-642-14267-3 [11] Yuanhua Lv and ChengXiang Zhai. 2011. Adaptive term frequency normalization for BM25. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management (Glasgow, Scotland, UK) (CIKM â€™11) . Association for Computing Machinery, New York, NY, USA, 1985â€“1988. doi:10.1145/2063576. 2063871 [12] Yuanhua Lv and ChengXiang Zhai. 2011. Lower-bounding term frequency nor-malization. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management (Glasgow, Scotland, UK) (CIKM â€™11) . Association for Computing Machinery, New York, NY, USA, 7â€“16. doi:10.1145/2063576.2063584 [13] Yuanhua Lv and ChengXiang Zhai. 2011. When documents are very long, BM25 fails!. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (Beijing, China) (SIGIR â€™11) . Association for Computing Machinery, New York, NY, USA, 1103â€“1104. doi:10.1145/2009916. 2010070 [14] Jean-Baptiste Mouret and Jeff Clune. 2015. Illuminating search spaces by mapping elites. CoRR abs/1504.04909 (2015). arXiv:1504.04909 http://arxiv.org/abs/1504. 04909 [15] Alexander Novikov, NgÃ¢n V Ëœu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, and Matej Balog. 2025. AlphaEvolve: A coding agent for scientific and algorithmic discovery. arXiv:2506.13131 [cs.AI] https://arxiv.org/abs/2506.13131 [16] Jay M. Ponte and W. Bruce Croft. 2017. A Language Modeling Approach to Information Retrieval. SIGIR Forum 51, 2 (2017), 202â€“208. doi:10.1145/3130348. 3130368 [17] Stephen E. Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (2009), 333â€“389. doi:10.1561/1500000019 [18] Asankhaya Sharma. 2025. OpenEvolve: an open-source evolutionary coding agent .https://github.com/codelion/openevolve [19] Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan Ã–. Arik, Danqi Chen, and Tao Yu. 2025. BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 . OpenReview.net. https://openreview.net/forum?id=ykuc5q381b [20] Reiko Tanese. 1989. Distributed genetic algorithms for function optimization . Ph. D. Dissertation. University of Michigan, USA. https://hdl.handle.net/2027.42/162372 [21] Michael Taylor, Hugo Zaragoza, Nick Craswell, Stephen Robertson, and Chris Burges. 2006. Optimisation methods for ranking functions with multiple param-eters. In Proceedings of the 15th ACM international conference on Information and knowledge management . 585â€“593. [22] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evalua-tion of Information Retrieval Models. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual , Joaquin Vanschoren and Sai-Kit Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ 65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html [23] Chengxiang Zhai and John Lafferty. 2004. A study of smoothing methods for lan-guage models applied to information retrieval. ACM Transactions on Information Systems (TOIS) 22, 2 (2004), 179â€“214. Conferenceâ€™17, July 2017, Washington, DC, USA Nian et al. 

## A Full Evolved BM25 Scoring Function 

This appendix provides the complete definitions of all components in the evolved BM25 algorithm described in Section 4.4. The top-level scoring function (Equation 4) applies a shared core function 

ğ‘… across four parallel token spaces, and the core function ğ‘… (Equa-tion 5) combines base evidence with a chain of bounded multipliers. 

Term weight. The term weight ğ‘¤ (ğ‘¡ ) determines how much each query term can contribute to the evidence term ğ¸ :

ğ‘¤ (ğ‘¡ ) = qtf (ğ‘¡, ğ‘ )0.5 Â· IDF (ğ‘¡ ) Â· 

 IDF (ğ‘¡ )

IDF (ğ‘¡ ) + 1

0.6

Â· IDF (ğ‘¡ )

IDF (ğ‘¡ ) + 1.25 (7) where qtf (ğ‘¡, ğ‘ ) is the query term frequency, and IDF (ğ‘¡ ) = âˆ’ ln 

 df (ğ‘¡ ) + 1

ğ‘ + 2



, (8) which is closer to the traditional TF-IDF formulation than to BM25â€™s IDF. The first factor qtf (ğ‘¡, ğ‘ )0.5 gives diminishing credit to repeated query terms. The remaining three factors are all functions of IDF .Acting together, they behave like a stopword removal filter: a stopword-like term with IDF â‰ˆ 1 is suppressed by all three fac-tors and ends up with a very small weight, whereas a rare term with IDF â‰ˆ 8 passes through nearly unaffected. 

Coverage multiplier. The coverage multiplier rewards breadth of match: 

ğµ cov = 1 + 0.25 Â· ğ‘Š ğ‘€ 

ğ‘Š , (9) where ğ‘Š ğ‘€ = Ãğ‘¡ âˆˆğ‘€ ğ‘¤ (ğ‘¡ ) is the total weight of matched terms and 

ğ‘Š = Ãğ‘¡ âˆˆğ‘ ğ‘¤ (ğ‘¡ ) is the total weight of all query terms. The ratio 

ğ‘Š ğ‘€ /ğ‘Š âˆˆ [ 0, 1] measures what fraction of the queryâ€™s importance the document satisfies. A document matching all query terms re-ceives a 1.25 Ã— boost. BM25 has no explicit equivalent; it relies solely on additive accumulation to implicitly reward multi-term matches. 

Specificity multiplier. The specificity multiplier rewards docu-ments where matched terms appear with higher-than-expected frequency: 

ğµ spec = 1 + 0.10 Â·

Ãğ‘¡ âˆˆğ‘€ + ğ‘¤ (ğ‘¡ ) Â· min (PMI (ğ‘¡, ğ‘‘ ), 3.0)

ğ‘Š , (10) where ğ‘€ + = {ğ‘¡ âˆˆ ğ‘€ : PMI (ğ‘¡, ğ‘‘ ) > 0} and PMI (ğ‘¡, ğ‘‘ ) = ln 

 tf (ğ‘¡, ğ‘‘ ) Â· ğ‘ 

max (| ğ‘‘ |, 25 ) Â· df (ğ‘¡ )



(11) is the pointwise mutual information between term ğ‘¡ and docu-ment ğ‘‘ . A positive PMI means the term is over-represented in the document relative to its corpus-wide rate. The cap at 3.0 and the document length floor of 25 prevent short documents from pro-ducing extreme values. ğµ spec captures how strongly the document focuses on query terms. 

Coordination multiplier. The coordination multiplier provides a second, calibrated signal for multi-term matching: 

ğµ coord = 1 + 0.20 Â· ğœ coord 

ğœ coord + ln (1 + ğ‘Š ) Â· |ğ‘€ ||ğ‘ | , (12) where |ğ‘€ | is the number of matched query terms, |ğ‘ | is the number of unique query terms, and ğœ coord = 2.5. The calibration factor   

> ğœ coord
> ğœ coord +ln (1+ğ‘Š )

adapts the bonus to query difficulty. For short, rare-term queries, ğ‘Š is small and the calibration factor is close to 1, so the bonus is modest because high per-term evidence already implicitly rewards coordination. For long, common-word queries, ğ‘Š is large and the calibration factor shrinks, preventing double-counting with 

ğµ cov .

Anchor multiplier. The anchor multiplier is a recall safeguard for rare terms: 

ğµ anc = 1 + 0.14 Â· ln (1 + ğ´ ) (13) 

ğ´ = max    

> ğ‘¡ âˆˆğ‘€ :IDF (ğ‘¡ )>4.2

IDF (ğ‘¡ ) âˆ’ 4.2

IDF (ğ‘¡ ) (14) If a document matches even one very rare query term (IDF above 

4.2), it receives a small boost proportional to that termâ€™s rarity. Only the single rarest matched term contributes, keeping the boost bounded. 

Length dampener. The length dampener replaces BM25â€™s per-term linear normalization with a single global logarithmic penalty: 

ğµ len = 1 + 0.15 Â· ln 



1 + |ğ‘‘ | + 1

avgdl + 1



(15) This is gentler than BM25â€™s 1 âˆ’ ğ‘ + ğ‘ Â· | ğ‘‘ |/ avgdl . A document twice the average length is penalized by roughly 10% , compared to up to 40% under BM25 with ğ‘ = 0.4. The logarithmic form means the penalty grows very slowly for extremely long documents, which benefits heterogeneous corpora with high length variance. 

## B Full Evolved Query Likelihood Scoring Function 

This appendix provides the complete definitions of all components in the evolved query likelihood algorithm described in Section 4.5. 

Enriched collection language model. The standard collection lan-guage model ğ‘ƒ (ğ‘¡ | ğ¶ ) = tf ğ¶ (ğ‘¡ )/| ğ¶ | is replaced by a three-stage estimate. First, the raw collection probabilities are raised to a power 

ğœ = 0.85 and renormalized: 

ğ‘ƒ ğœ (ğ‘¡ ) = ğ‘ƒ (ğ‘¡ | ğ¶ )ğœ 

Ãğ‘¡ â€² âˆˆğ‘‰ ğ‘ƒ (ğ‘¡ â€² | ğ¶ )ğœ , (16) where ğ‘¡ â€² is a dummy variable indicating that the denominator sums across all terms in the vocabulary ğ‘‰ . This flattens the distribution, transferring probability mass from frequent terms to rare ones. Sec-ond, the tempered model is interpolated with a document-presence language model ğ‘ƒ df (ğ‘¡ ) = df (ğ‘¡ )/ ğ‘ :

ğ‘ƒ mix (ğ‘¡ ) = 0.90 Â· ğ‘ƒ ğœ (ğ‘¡ ) + 0.10 Â· ğ‘ƒ df (ğ‘¡ ). (17) The document-frequency model counts each document once re-gardless of internal repetition, providing a stabler estimate than the token-frequency model for terms concentrated in bursty docu-ments. Third, a small uniform component is mixed in to ensure no term has zero background probability: 

ğ‘ƒ ğ¶ (ğ‘¡ ) = 0.97 Â· ğ‘ƒ mix (ğ‘¡ ) + 0.03 

|ğ‘‰ | . (18) 

Adaptive term-frequency saturation. Raw term frequency is re-placed by tf eff (ğ‘¡, ğ‘‘ ) = tf (ğ‘¡, ğ‘‘ ) ğ›½ (ğ‘¡ ) , where 

ğ›½ (ğ‘¡ ) = 1 âˆ’ 0.30 Â· ( 1 âˆ’ IDF 01 (ğ‘¡ )) (19) RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution Conferenceâ€™17, July 2017, Washington, DC, USA 

IDF 01 (ğ‘¡ ) =

log 

 ğ‘ +1 

> df (ğ‘¡ )+ 1



max ğ‘¡ â€² âˆˆğ‘‰ log 

 ğ‘ +1  

> df (ğ‘¡ â€²)+ 1

 . (20) The normalized IDF IDF 01 (ğ‘¡ ) âˆˆ [ 0, 1] maps each term to its rel-ative rarity. For common terms ( IDF 01 â‰ˆ 0), ğ›½ (ğ‘¡ ) â‰ˆ 0.70 and term-frequency saturates aggressively. For rare terms ( IDF 01 â‰ˆ 1), 

ğ›½ (ğ‘¡ ) â‰ˆ 1.0 and the full term-frequency signal is preserved. 

Gated Dirichlet log-likelihood. The core per-term relevance score is: 

ğ‘  base (ğ‘¡, ğ‘‘ ) = ğ‘” (ğ‘¡ ) Â· log Â©Â«

1 + tf (ğ‘¡,ğ‘‘ ) ğ›½ (ğ‘¡ )   

> ğœ‡ Â·ğ‘ƒ ğ¶ (ğ‘¡ )|ğ‘‘ |+ ğœ‡ ğœ‡

ÂªÂ®Â¬

, (21) with ğœ‡ = 1750 , lower than the standard default of ğœ‡ = 2000 because the tempered background model already provides more uniform smoothing. The gate ğ‘” (ğ‘¡ ) is the Entity Dispersion Ratio (EDR): 

ğ‘” (ğ‘¡ ) = 1 + 0.45 Â· clip 



log 

 ğ‘ƒ df (ğ‘¡ )

ğ‘ƒ ğ¶ (ğ‘¡ )



, âˆ’2.5, 2.5



. (22) The ratio ğ‘ƒ df (ğ‘¡ )/ ğ‘ƒ ğ¶ (ğ‘¡ ) measures whether a termâ€™s document spread matches its token frequency. Terms that appear across many doc-uments but with low total frequency (high ratio) are upweighted as independently informative. Terms with high token frequency concentrated in few documents (low ratio) are downweighted. The clipping keeps the gate bounded in [âˆ’ 0.125 , 2.125 ].

Query term weight. Each query term receives a composite weight: 

ğœ” (ğ‘¡ ) =  qtf (ğ‘¡ ) Â· ğ‘Ÿ (ğ‘¡ )0.6 (23) 

ğ‘Ÿ (ğ‘¡ ) = 1 + 0.9 Â·

clip 



log 

 ğ‘ƒ df (ğ‘¡ )  

> ğ‘ƒ ğ¶ (ğ‘¡ )



, 0, 2.5



2.5 (24) The residual weight ğ‘Ÿ (ğ‘¡ ) âˆˆ [ 1.0, 1.9] boosts query terms that are spread broadly rather than concentrated in few documents. The outer power 0.6 applies sub-linear damping to repeated query terms, preventing verbose or repetitive queries from over-counting. 

Leaky rectifier. Per-term scores are passed through a leaky recti-fier: 

Ëœğ‘  (ğ‘¡, ğ‘‘ ) =

(

ğ‘  base (ğ‘¡, ğ‘‘ ) if ğ‘  base (ğ‘¡, ğ‘‘ ) â‰¥ 0,

0.12 Â· ğ‘  base (ğ‘¡, ğ‘‘ ) otherwise. (25) Negative evidence is retained at 12% of its full strength, enough to break ties and mildly penalize documents that partially match but are weak on specific terms. 

Missing-term penalty. An explicit penalty applies for each query term that is completely absent from the document: 

ğ‘š (ğ‘¡, ğ‘‘ ) = 1[tf (ğ‘¡, ğ‘‘ ) = 0] Â· 0.07 Â· ğœ” (ğ‘¡ ) Â· log 

 ğœ‡ Â· ğ‘ƒ ğ¶ (ğ‘¡ )|ğ‘‘ | + ğœ‡ 



. (26) This is precisely the Dirichlet log-likelihood score evaluated at tf = 0, scaled down by 0.07 . Together with the leaky rectifier, this creates a layered penalty architecture at two granularities (per-term weakness and per-term absence). 

Soft-AND coverage bonus. A bonus rewards documents matching more distinct query terms: AND (ğ‘, ğ‘‘ ) = 0.14 Â· 1

|ğ‘ ğ‘¢ |âˆ‘ï¸ ğ‘¡ âˆˆğ‘ ğ‘¢ 

tanh ğœ” (ğ‘¡ ) Â· max ( Ëœğ‘  (ğ‘¡, ğ‘‘ ), 0)

3.0 . (27) The tanh approximates a binary â€œthis term was matched wellâ€ signal. Summing and normalizing by |ğ‘ ğ‘¢ | gives a coverage fraction in [0, 1].The tanh design is robust to the scale of per-term scores, so the coverage signal reflects purely breadth, not depth. 

Document length prior. A Gaussian prior on log-length penalizes deviation from the corpus average in both directions: LP (ğ‘‘ ) = âˆ’0.06 Â·   log (| ğ‘‘ |) âˆ’ log (avgdl )) 2. (28) A document 10 Ã— the average length receives a penalty of approxi-mately âˆ’0.32 , a nudge rather than a decisive factor.