Title: Learning-based augmentation of first-principle models: A linear fractional representation-based approach

URL Source: https://arxiv.org/pdf/2602.17297v1

Published Time: Fri, 20 Feb 2026 01:43:13 GMT

Number of Pages: 16

Markdown Content:
# Learning-based augmentation of first-principle models: A linear fractional representation-based approach ⋆

# Jan H. Hoekstra a, Bendegúz M. Györök b, Roland Tóth a,b, Maarten Schoukens a

> a

Control Systems Group, Eindhoven University of Technology, The Netherlands 

> b

Systems and Control Lab, HUN-REN Institute for Computer Science and Control, Budapest, Hungary 

Abstract 

Nonlinear system identification has proven to be effective in obtaining accurate models from data for complex real-world systems. In particular, recent encoder-based methods with artificial neural network state-space (ANN-SS) models have achieved state-of-the-art performance on various benchmarks, using computationally efficient methods and offering consistent model estimation in the presence of noisy data. However, inclusion of prior knowledge of the system can be further exploited to increase (i) estimation speed, (ii) accuracy, and (iii) interpretability of the resulting models. This paper proposes a model augmentation method that incorporates prior knowledge from first-principles (FP) models in a flexible manner. We introduce a novel linear-fractional-representation (LFR) model structure that allows for the general representation of various augmentation structures including the ones that are commonly used in the literature, and an encoder-based identification algorithm for estimating the proposed structures together with appropriate initialisation methods. The performance and generalisation capabilities of the proposed method are demonstrated on the identification of a hardening mass-spring-damper system in a simulation study and on the data-driven modelling of the dynamics of an F1Tenth electric car using measured data. 

Key words: Nonlinear System Identification; Model Augmentation; Physics-Based Learning. 

1 Introduction 

As control systems are becoming more complex and per-formance requirements surge, the need for accurate nonlin-ear models capable of efficiently capturing complicated be-haviours of physical systems is rapidly increasing. It is com-mon practice to derive baseline models using first-principle 

(FP) methods, e.g., rigid body dynamics [30]; however, these models provide only an approximate system description. Al-though more accurate FP models can be developed, this is a labour-intensive process, especially when additional phys-ical effects—such as friction or aerodynamic forces—are included. Modelling these phenomena from first principles 

⋆ This paper was not presented at any IFAC meeting. Implementa-tion of the proposed method is available at https://github. com/JanHHoekstra/Model-Augmentation-Public 

This work is funded by the European Union (Horizon Europe, ERC, COMPLETE, 101075836) and has also been supported by the Air Force Office of Scientific Research under award number FA8655-23-1-7061. Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. 

Email addresses: j.h.hoekstra@tue.nl (Jan H. Hoekstra), gyorokbende@sztaki.hun-ren.hu (Bendegúz M. Györök), r.toth@tue.nl (Roland Tóth), 

m.schoukens@tue.nl (Maarten Schoukens). 

often requires dedicated experimental campaigns to identify and estimate the associated unknown parameters. Further-more, the resulting models may become too complex to be handled analytically. In some cases, reliable FP descriptions of the to-be-modelled effects may not even exist, resulting in approximations with varying levels of fidelity. To overcome these issues, nonlinear system identification 

(NL-SI) methods offer an alternative option to estimate mod-els directly from measurement data [30]. Black-box models, particularly those that incorporate artificial neural networks 

(ANNs), have achieved unprecedented accuracy in captur-ing complex behaviours. In control applications, ANN-based 

state-space (SS) models have proven to be effective in han-dling high-order systems and capturing complex nonlinear dynamics [5]. Although black-box methods may result in accurate mod-els, they also have serious downsides. First, flexible function approximators are difficult to interpret, leading to model be-haviour that is not well understood. This in turn limits the reliability of the model during, e.g., extrapolation beyond the training data. This is a significant drawback for control applications, where interpretable models are preferred in the design process [10, 25]. The second drawback is the signifi-cant time spent learning expected behaviour that has already been modelled thoroughly, e.g., FP-based understanding of the rigid-body-dynamics of the system. 

Preprint submitted to Automatica 20 February 2026 

> arXiv:2602.17297v1 [eess.SY] 19 Feb 2026

Physics-informed neural networks [28] and physics-guided neural networks [11] embed the prior knowledge of the physics in the form of equations (algebraic of partial dif-ferential) in the cost function, enforcing the learnt functions to fit to known physics behaviour. This leads to more in-terpretable models as the known physics are enforced, and shows faster learning convergence. These methods, however, require the knowledge of such physics equations and still rely on a black-box model to capture the entire system. A promising approach is model augmentation, e.g., [16, 17, 33,35]. This method combines baseline models with flexible function approximators, such as ANNs, in a combined model structure. As a result of this structural combination, the prior knowledge is directly captured in the baseline model and the learning components only need to model unknown dy-namics. For control engineering, such a structure is benefi-cial, as it is clear how a well-understood baseline model is combined with black-box elements. In the literature, there are a variety of different model aug-mentation structures, such as parallel [35] and series [16,17, 33] interconnections. These interconnections reflect in what form the known baseline model is combined with the learn-ing component that models the unknown behaviour of the system. Although different interconnections may result in an equally accurate model, model complexity and conver-gence speed also need to be considered, especially when the final model is utilised for control purposes. One intercon-nection may be equally accurate while having a less com-plex parameterisation compared to others. It is not trivial to determine which interconnection is the most advantageous for a specific baseline model and data-generating system, as the choice of the optimal interconnection depends on the unknown dynamics of the system. To address this, further research into model augmentation methods is required to develop, e.g., automatic model selection methods. To facil-itate this research, a general model augmentation structure is required, such that model augmentation methods can be developed efficiently and compared across different works in literature. Such a general model augmentation structure is currently lacking in the literature. To solve this problem, a general model augmentation struc-ture based on a Linear Fractional Representation (LFR) is proposed, which has been chosen for its modular and flexible nature, enabling a generalised form for augmenting the FP or the already known dynamics. The formulation of LFRs al-lows for systematic model augmentation while maintaining a clear separation between the baseline and learning compo-nents. The proposed model augmentation structure is able to express a wide range of model augmentation structures used in literature, and thus is a unified representation. Further-more, LFRs are commonly used in the robust control field for uncertainty modelling in a generalised plant format. Thus, in addition to the general representation, the proposed struc-ture also ensures compatibility with well-established control methodologies for classical LFRs [40], making them a ver-satile choice for a wide range of applications. The price of the uniform model structure is that well-posedness problems may arise. To address this, we examine the computational graph of the proposed structure and provide well-posedness conditions based on this graph. We also propose an identifi-cation algorithm capable of handling the general LFR model augmentation structure with consistency guarantees, while also addressing the joint identification of both the baseline model and learning component parameters and managing overparameterisation through a physics-guided regularisa-tion method. The main contributions are summarised as 1 :

• We present a novel, general LFR-based model augmen-tation structure with two possible parameterisations: one fully parameterised, offering high flexibility in es-timation at the cost of model interpretability, and the other structured and sparsely parameterised, promot-ing transparency, but requiring more prior knowledge to define the structure; 

• We provide proof of the representation capability of the proposed LFR-based model augmentation struc-ture, demonstrating that it is capable of representing all commonly used model augmentation structures in the existing literature; 

• We provide conditions under which well-posedness of the proposed model structure is guaranteed; 

• We provide an efficient identification algorithm for data-driven estimation of the augmented models under the proposed structure with consistency guarantees. 

• We perform rigorous simulation and real-world mea-surement based studies to validate the proposed methodology. The paper is organised as follows: first, the system iden-tification problem and available model augmentation ap-proaches are presented in Section 2. Then, Section 3 intro-duces the LFR-based model augmentation structure and its computational graph. Section 4 provides conditions under which well-posedness of the model is proven. Next, Sec-tion 5 provides an identification algorithm with consistency guarantees. A hardening mass-spring-damper (MSD) simu-lation example and real-world experiments with an F1Tenth electric car are used to demonstrate the performance of the proposed identification method in Sections 6 and 7, respec-tively. The conclusions are given in Section 8. 

2 Model Augmentation Problem Setting 

We consider the system to be identified given by the discrete-time (DT) nonlinear representation 

xk+1 = f (xk, uk), (1a) 

yk = h(xk, uk) + ek, (1b) where xk ∈ Rnx is the state, uk ∈ Rnu is the input, yk ∈ Rny is  

> 1A preliminary version of the methods discussed in this work was presented in [20]. Compared to [20], the key differences are the full parameterisation of the LFR, the computational graph, a detailed well-posedness condition, an extended description of the identification algorithm, a more in-depth simulation study, and a real-world identification example using measured data.

2Nonlinear System Baseline FP Data 

Model Augmentation structure Learning components Fig. 1. Learning-based model augmentation concept. 

the output signal of the system at time moment k ∈ Z with 

ek an i.i.d. white noise process with finite variance, repre-senting measurement noise, f : Rnx ×Rnu → Rnx is the state-transition function and h : Rnx → Rny is the output function. This state-space representation is a general form that can describe a wide range of dynamics encountered in practice. We assume a baseline model of (1) is available in the form 

xb,k+1 = fbase 

 θbase , xb,k, uk

 , (2a) ˆyk = hbase 

 θbase , xb,k, uk

 , (2b) where xb,k ∈ Rnxb is the baseline model state, ˆ yk ∈ Rny is the model output, and fbase : Rnxb × Rnu → Rnxb with hbase :

Rnxb × Rnu → Rny are the baseline state-transition and out-put readout functions respectively, parameterised by θbase ∈

Rnθbase . The parameters θbase correspond to the physical pa-rameters associated with system (1). In model augmentation, the baseline model is combined with learning components in a combined model structure. The parameters of this model augmentation structure are then estimated using data measurements of the system as shown in Fig. 1. The general model augmentation structure is de-scribed as 

xb,k+1 =   fbase ⋆ faug 

   xb,k, xa,k, uk

 , (3a) 

xa,k+1 = gaug 

 xb,k, xa,k, uk

 , (3b) ˆyk =  hbase ⋆ haug 

   xb,k, xa,k, uk

 , (3c) where xa,k are additional states added for dynamic augmen-tation structures, and faug , gaug and haug are the learning components parameterised by θaug ∈ Rnθaug . For notational simplicity, both θbase and θaug are not written out in (3). The operator ⋆ represents an interconnection between two func-tions. This interconnection can represent a variety of differ-ent forms of model augmentation structure used in the liter-ature, such as static parallel [35] and static series [16,17,33] structures. Due to the state-space form of the model struc-ture, augmentations can occur at the state and/or output level. We show a selection of possible state level augmenta-tions in Table 1 and output level augmentations in Table 2. Here, static refers to augmentations that do not add new state dimensions beyond the baseline model states xb. Dynamic augmentation structures [7, 20], on the other hand, add new augmentation states xa to model missing dynamics. A broad range of further augmentations are possible. In this work, we restrict attention to the elementary augmentations listed in Tables 1 and 2, which will be discussed through this paper. As discussed in the introduction, a general augmentation structure is desired. For this, a parameterisation of the op-erator ⋆ is required, capable of characterising the intercon-

Table 1 Classes of state model augmentation structures. static parallel (S-SP) xb,k+1 = fbase 

 xb,k, uk

 + faug 

 xb,k, uk



static series output (S-SSO) xb,k+1 = faug 

 xb,k, uk, fbase 

 xb,k, uk

 

static series input (S-SSI) xb,k+1 = fbase 

  faug 

 xb,k, uk

 

dynamic parallel (S-DP) 

xb,k+1 = fbase 

 xb,k, uk

 + faug 

 xb,k, xa,k, uk



xa,k+1 = gaug 

 xb,k, xa,k, uk



dynamic series output (S-DSO) 

xb,k+1 = faug 

 xb,k, xa,k, uk, fbase 

 xb,k, uk

 

xa,k+1 = gaug 

 xb,k, xa,k, uk



dynamic series input (S-DSI) 

xb,k+1 = fbase 

  faug 

 xb,k, xa,k, uk

 

xa,k+1 = gaug 

 xb,k, xa,k, uk



Table 2 Classes of output model augmentation structures. static parallel (O-SP) ˆyk = hbase 

 xb,k, uk

 + haug 

 xb,k, uk



static series output (O-SSP) ˆyk = haug 

 xb,k, uk, hbase 

 xb,k, uk

 

static series input (O-SSI) ˆyk = hbase 

 haug 

 xb,k, uk

 

dynamic parallel (O-DP) ˆyk = hbase 

 xb,k, uk

 + haug 

 xb,k, xa,k, uk



xa,k+1 = gaug 

 xb,k, xa,k, uk



dynamic series output (O-DSO) ˆyk = haug 

 xb,k, xa,k, uk, hbase 

 xb,k, uk

 

xa,k+1 = gaug 

 xb,k, xa,k, uk



dynamic series input (O-DSI) ˆyk = hbase 

 haug 

 xb,k, xa,k, uk

 

xa,k+1 = gaug 

 xb,k, xa,k, uk



nection between the baseline model and the learning com-ponents. This would realise a fully parameterised general augmentation structure of (3). Additionally, an identifica-tion algorithm able to estimate the parameters of this gen-eral model augmentation structure is proposed, under the restriction that the model is well-posed. 

3 LFR-based augmentation structure 

In this section, we formulate a general representation of (3) in an LFR-based augmentation structure. Next, we derive the graph based representation of the proposed model structure, which is to be used to introduce sparsity to the LFR-based augmentation structure as well as for deriving conditions for well-posedness in Section 4. Finally, we introduce the parameterisation of the learning component that we will use to formulate our augmentation approach. 

3.1 General LFR-based augmentation structure 

As discussed in Section 2, many model augmentation struc-tures are available in the literature, and now we propose a 3unified structure based on the Linear Fractional Represen-tation (LFR) that can represent all augmentation arrange-ments. The flexibility of this representation has made it pop-ular in the field of robust control [40] and linear parameter-varying-control [37]. Furthermore, an LFR can also include nonlinear components in the interconnections [38], which has made LFRs useful for black-box nonlinear system repre-sentations [32,34] and implicit learning [12]. Recent results have also shown that stability properties can be enforced on these black-box LFRs in a constraint-free manner at the cost of some representation capability [15, 29]. Introduce the following notation for the baseline terms: 

φbase (θbase , zb,k) = 

" fbase (θbase , zb,k)

hbase (θbase , zb,k)

#

. (4) Moreover, we denote the learning component as φaug , which can be represented by any universal function approxima-tor. We assume that it is implemented as a function with 

θaug ∈ Rnθaug collecting its parameters. Latent variables 

wb,k ∈ Rnxb +ny , wa,k ∈ Rnwa , za,k ∈ Rnza , and zb,k ∈ Rnxb +nu

are introduced, and expressed as 

"zb,k

za,k

#

=

"Cb

> z

Ca

> z

#

ˆxk +

"Db

> zu

Da

> zu

#

uk +

"Dbb  

> zw

Dba 

> zw

Dab  

> zw

Daa 

> zw

#| {z }

> Dzw

"wb,k

wa,k

#

, (5) where Cb 

> z

, Ca 

> z

, . . . , Da 

> zw

are real matrices with dimensions compatible with the signal dimensions, and their elements are parameters that are optimised during model learning. The state transition and output equations are expressed as 

"xb,k+1

xa,k+1

#| {z }

> ˆxk+1

=

"Abb Aba 

Aab Aaa 

#| {z }

> A

"xb,k

xa,k

#| {z } 

> ˆxk

+

"Bb

> u

Ba

> u

#| {z } 

> Bu

uk +

"Bbb  

> w

Bba 

> w

Bab  

> w

Baa 

> w

#| {z }

Bb 

> w

Ba

> w
> 

"wb,k

wa,k

#| {z } 

> wk

,

ˆyk =

h

Cb 

> y

Ca

> y

i| {z }

> Cy

ˆxk + Dyu uk +

h

Db 

> yw

Da

> yw

i

wk,

where A, Bu, Cy, and Dyu are real matrices with appropi-rate signal dimensions representing the linear parts of the unmodeled dynamics, the baseline part participates in the relation through matrices Bb 

> w

and Db

> yw

, while the nonlinear black-box terms effect the dynamics through Ba 

> w

and Da

> yw

.Finally, the LFR-based model augmentation structure (shown in Fig. 2) can be expressed in a compact form, as 



ˆxk+1

ˆyk

zb,k

za,k



=



A Bu Bb 

> w

Ba

> w

Cy Dyu Db 

> yw

Da

> yw

Cb 

> z

Db 

> zu

Dbb  

> zw

Dba 

> zw

Ca 

> z

Da 

> zu

Dab  

> zw

Daa 

> zw

| {z } 

> W(θLFR )



ˆxk

uk

wb,k

wa,k



, (6a) 

wb,k = φbase 

 θbase , zb,k

 , (6b) 

wa,k = φaug 

 θaug , za,k

 , (6c) 

Fig. 2. LFR-based augmentation structure of the baseline model characterised by φbase with learning components φaug .

where W is the LFR matrix, and all parameters determining the matrices A, Bu, . . . , Daa  

> zw

are collected into θLFR . Since 

θLFR is included in the (tunable) model parameters, the final interconnection structure of the LFR-based augmentation is formed throughout model learning, hence the flexibility of the approach. However, an inherent challenge of LFR model structures is ensuring well-posedness of the model structure. As the model structure allows algebraic loops, it is possible the retrieve ill-posed realisations. We define the 

well-posedness (WP) property as 

Property 1 (Well-posedness) For any value of ˆxk ∈ Rnx

and u k ∈ Rnu , the signal relations in (6) admit a unique so-lution z k =

h

z⊤ 

> b,k

z⊤

> a,k

i⊤

.

Conditions to ensure the well-posedness are introduced in Section 4. The unified representation capability of the pro-posed LFR structure is shown by the following theorem. 

Theorem 2 (Unified representation) Given a baseline model (2) with parameters θbase connected to learning com-ponents f aug , g aug and h aug , each parametrised with θaug ,in terms of (3) , where the operator ⋆ corresponds to any of the interconnections listed in Table 1 and 2. Then, for the considered model structure (6) , there exists a θLFR ∈ RnθLFR 

and choices of latent dimensions n wa , nza ≥ 0 and a φaug 

function, such that (3) and (6) are equivalent representa-tions of the same dynamic behaviour. 

PROOF. See Appendix A. ■

3.2 Computational graph of the interconnection 

The full parameterisation of W in the proposed LFR-based model augmentation structure gives a general representation of (3). However, due to full parameterisation, the actual in-terconnection of the learning and baseline model compo-nents is represented in a black-box fashion compared to the cases listed in Tables 1 and 2. To be able to detect or even enforce a particular configuration of these components, we investigate the computational graph of the augmentation interconnection (3). This graph makes clear what each edge, i.e., element of θLFR , does in the augmentation structure, and thus also what removing it for sparsification will do. Additionally, the graph representation 4(a) Full computational graph 

(b) Computational graph with subgraph φ

Fig. 3. Computational graph representation of the function 

w1 = φ (θ , z1, z2) = ( z1 + z2) θ + σ (z2).

will provide general conditions on the well-posedness of the proposed model structure in Section 4. First, we introduce computational graphs, then present the graph representations of the baseline model and the learning component, which are finally combined into the LFR model graph. Here we make use of the following graph notions: 

G = ( V, E) is a graph with nodes V and edges E, deg − is the indegree of a node, deg + is the outdegree of a node, disjoint (V,W ) is the disjoint union of set between two sets V

and W defined as V ∪W with V ∩W = /0, source (W ) defines a set of nodes V = {v ∈ W |deg 9 (v) = 0}, and sink (W ) defines a set of nodes V = v ∈ W |deg + (v) = 0 . We also introduce the shorthand notation outvar (V,W ) to define the set of edges 

Evw = (v, w) |v ∈ V, w ∈ W, deg + (v) = deg 9 (w) = 1 , and 

Gc = ( Vc, Ec) = G\Vs for vertex contraction with G = ( V, E),

Vs ⊂ V , Vc = V \Vs ∪ vc and vc being the contracted vertex. 

3.2.1 Computational graphs 

We take the formulation of the computational graph as de-fined in [3]. First, take the computational problem defined as the set of M functions 

wi = φi = ( z1, . . . , zK ) , i = 1, . . . , M, (7) of K real variables z1, . . . , zK ∈ R from which M real quanti-ties wi ∈ R are obtained. Each function φi : RK → R can be described by a computational process consisting of a num-ber of primitive operators 

ζic,0 := ρc

 ζic,1 , . . . , ζic,s

 , c = 1, . . . , L (8) where the values ζic, j ∈ R and ρc : Rs → R. For all j ∈

[1, . . . , s], ζic, j ∈ [z1, . . . , zK , ζib,0 ] with b < c, i.e., the inputs to 

ρc can be the input of φi or output from a previous operator 

ρb. For j = 0, ζic, j ∈ [w1, . . . , wM , ζib,1,..., ζib,s ] with b > c, i.e., the output of ρc can be the output of φi or input to a next operator ρb. We consider primitive operators ρc : Rs → R,with s ∈ { 1, 2}, to be either: addition, multiplication or a nonlinear injective function (where s = 1). The computational process defines and it is characterised by a computational graph , where both ρc and ζi,c, j are nodes in the graph. This computational graph is defined as follows: 

Definition 3 (Computational graph) A directed graph de-noted by G = ( V, E) with V = disjoint  Vζ ,Vρ

 the set of ver-tices and E = disjoint  Eζ ρ , Eρζ 

 the set of edges is called a computational graph if (a) V ρ = {ρ1, . . . , ρL}, V ζ = {ζ1, . . . , ζK }

(b) V z = source  Vζ

, V w = sink  Vζ



(c) E ζ ρ =

n

(ρ, ζ ) |ρ ∈ Vρ , ζ ∈ Vζ and ∂ ρ ∂ ζ ̸ = 0

o

(d) E ρζ = outvar  Vρ ,Vζ \Vz



(a) defines the sets of operator and variable nodes respec-tively. (b) defines subsets of the variables representing the inputs and outputs respectively (i.e., sources and sinks in graph nomenclature). Lastly, (c) defines what variable nodes are inputs to which operators, and (d) defines the connec-tion of the output variables for each operator. In Fig. 3a we show the computational graph for the example function 

w1 = φ (θ , z1, z2) = ( z1 + z2) θ + σ (z2).By defining a subgraph Gφi , we can formulate a more com-pact notation. The computational subgraph Gφi , contains the nodes Vφi =  Vζi ∪Vρi

 \ Vwi \ Vzi and the edges internal to these nodes. Then by vertex contraction G \Vφi we get the contracted noted vφi that represents the multivariate function 

φi in (7). Such a contracted node vφi is shown for the exam-ple function in Fig. 3b. For each function φi in the compu-tational problem, a computational graph Gφi can be defined. By taking the union of these graphs as 

Gφ =  Vφ , Eφ

 =  Vφ1 ∪ . . . ∪VφM , Eφ1 ∪ . . . ∪ EφM

 (9) we obtain the computational graph Gφ of the entire compu-tational problem (7) with K inputs and M outputs, as visu-alised in Fig. 5. Note that due to the union, nodes and edges may be shared between the computational graphs Gφi .

3.2.2 Graph of baseline model 

The baseline model (2) can be represented with a computa-tional graph as 



wb,1

...

wb,nxb

wb,nxb +1

...

wb,nxb +ny



=



φbase 1 (θbase ,1, zb,k,1)

...

φbase nx b (θbase ,nxb , zb,k,nxb )

φbase nx b +1 (θbase ,nxb +1, zb,k,nxb +1)

...

φbase nx b +ny (θbase ,nxb +ny , zb,k,nxb + ny)



=

" fbase (θbase , zb,k)

hbase (θbase , zb,k)

#

, (10) where we describe zb,k = vec (xb,k, uk) and θbase as input variables, wb,k = vec (xb,k+1, ˆyk) as output variables, and 

θbase ,1, . . . , θbase ,nxb +ny ∈ θbase as the parameters (i.e., the 5Fig. 4. Computational graph for a computation problem consisting of M functions φ1 . . . φM .

parameters may be shared between functions φbase i ), and similarly zb,k,1, . . . , zb,k,nxb +ny ∈ zb,k.Which subset of θbase and zb,k are input to each function 

φbase i , which follows from the baseline model (2) considered. After deriving a computational graph Gφbase i for each func-tion φbase 1 , . . . , φbase nx b +ny , the computational graph Gφbase is derived by the graph union as in (9), with the resulting nodes 

Vzb , Vwb and Vφbase . Notably, Ezbφbase is sparse, e.g., the num-ber of edges between Vzb and Vφbase is smaller than would be permissible by any arbitrary chosen baseline model (2). 

3.2.3 Graph of the learning component 

Similarly as for the baseline model, we describe the learning function graph for the computational problem as 



w1

...

wnwa 

=



φaug 1 (θaug ,1, za,k,1)

...

φaug nwa (θaug ,nwa , za,k,nwa )

= φaug (θaug , za,k),

Then the computational graph Gφaug is derived as before, with the resulting nodes Vza , Vwa and Vφaug .

3.2.4 LFR model graph 

With the baseline model graph Gφbase and the learning com-ponent graph Gφaug , we can define the interconnection struc-ture between these graphs and the signals ˆ xk, uk, ˆ xk+1 and ˆyk of the model. For ease of notation, we will leave out the 

θ nodes, as these are not altered any further. We start by defining how the model signals are matched to Gφbase and 

Gφaug , by taking the following parameterised summation 

ς = 

> deg 9(ς)

# ∑

> i=1

θivi, (11) where ς ∈ disjoint  Vx+ ,Vy,Vzb ,Vza

 and vi ∈ disjoint (Vx,Vu,

Vwb ,Vwa

 and θi is the weight of the summation. These sum-mation operations are then represented by the summation nodes Vς . Then the interconnect graph can be defined as 

Definition 4 (Interconnection graph) The computational graph of the interconnection between G φbase and G φaug 

is a directed graph denoted by GLFR = ( VLFR , ELFR ),where V LFR = disjoint  Vx+ ,Vy,Vzb ,Vza ,Vx,Vu,Vwb ,Vwa

 and ELFR = disjoint  Eςbzb , Eςaza , Eςxx+ , Eςyy, Eφbase , Eφaug 



(a) V ςb = {ς1, . . . , ςnzb }, V ςa = {ς1, . . . , ςnza },Vςx = {ς1, . . . , ςnx }, V ςy = {ς1, . . . , ςny }

Fig. 5. Computational graph of the interconnection of Gb and Ga.

(b) E ςbzb = outvar  Vςb ,Vzb

, E ςaza = outvar  Vςa ,Vza



Eςxx+ = outvar  Vςx ,Vx+

, E ςyy = outvar  Vςy ,Vy



(c) ∀v ∈ Vx ∪Vu, deg 9 (v) = 0, ∀v ∈ Vx+ ∪Vy, deg + (v) = 0

(d) E vς = (v, ς ) |v ∈ disjoint  Vzb ,Vza ,Vxk ,Vuk

 , ς ∈ Vς

(a) defines the summation nodes for the baseline, augmenta-tion, state, and output nodes and (b) defines the one-to-one outgoing edges from these summation nodes to the respec-tive output nodes ρ. (c) defines existing nodes as inputs and outputs respectively (i.e., sources and sinks). Finally, (d) de-fines the edges between the state, input, baseline, and aug-mentation and the summation nodes. By applying vertex contraction with the nodes ςi ∈ Vςi and the nodes to which the corresponding outgoing edge of ς

leads, either Vz, Vx+ or Vy, on on GLFR , and also applying vertex contraction with node sets Vρb and Vρa on GLFR , we retrieve a graph with only the variable nodes. Considering 

Pi to be the adjacency matrix for the edge set Ei, then the adjacency matrix for the interconnected graph is 

P =



ˆxk uk zb za wb wa ˆxk+1 ˆyk

ˆxk 0 0 0 0 0 0 0 0

uk 0 0 0 0 0 0 0 0

zb Pzbx Pzbu 0 0 Pzbwb Pzbwa 0 0

za Pzax Pzau 0 0 Pzawb Pzawa 0 0

wb 0 0 Pb 0 0 0 0 0

wa 0 0 0 Pa 0 0 0 0

ˆxk+1 Pxx Pxu 0 0 Pxw b Pxw a 0 0

ˆyk Pyx Pyu 0 0 Pyw b Pyw a 0 0



. (12) By considering all edge sets maximal, i.e., fully connected where allowed, the summations in (11) recover the proposed LFR structure (6). The augmentation structures in Tables 1 and 2 correspond to sparse adjacency matrices. Thus, we can enforce or detect these augmentation structures by these sparse patterns in the adjacency matrix. 63.3 Parametrisation of the learning component 

The proposed LFR-based augmentation structure allows for the use of any parameterised learning function φaug without loss of generality. For the remainder of this report, however, we will consider φaug to be parameterised by an ANN. An ANN with q hidden layers, each composed of mi neurons and activation function ρ : R → R, is defined as 

ξi, j = ρ

> mi−1

# ∑

> l=1

θw,i, j,l ξi−1,l + θb,i, j

!

(13) where ξi = col (ξi,1, . . . , ξi,mi ) is the latent variable represent-ing the output of layer 1 ≤ i ≤ q, θw,i, j,l and θb,i, j are the weight and bias parameters of the network. Here, col () de-notes the composition of a column vector. For a φaug with q

hidden layers and linear input and output layers, this gives 

φaug (θaug , za,k) = θw,q+1ξq(k) + θb,q+1 (14a) 

ξ0(k) = za,k (14b) This can be extended to residual neural network (ResNet) [19], which adds a linear bypass given as 

φaug (θaug , za,k) = θw,q+1ξq(k) + θb,q+1 +Waza,k (15a) 

ξ0(k) = za,k (15b) where W is a parameterised residual weight matrix of appro-priate dimensions. The additional linear bypass can capture unknown linear dynamics in the learning functions. This is preferable over capturing these dynamics through nonlinear activation functions, since this is more likely to extrapolate better and has shown better learning performance in similar settings [31]. It additionally allows for stable initialisations of the proposed LFR structure as discussed in Section 5.4. This finalises the structure of the fully parameterised general augmentation structure of (3). We continue with providing well-posedness conditions and an identification algorithm. 

4 LFR structure well-posedness 

We now propose conditions under which Property 1 is guar-anteed. To derive conditions, we rewrite (5) into 

" zb,k

za,k

#

− Dzw φ  zb,k, za,k

| {z }

> v

(zb,k,za,k)=

" Cb 

> z

Db

> zu

Ca 

> z

Da

> zu

# " ˆxk

uk

#

, (16) where col  φbase 

 θbase , zb,k

 , φaug 

 θaug , za,k

 = φ  zb,k, za,k

.Then, if the inverse v−1  zb,k, za,k

 exists, 

" zb,k

za,k

#

= v−1

" Cb 

> z

Db

> zu

Ca 

> z

Da

> zu

#" ˆxk

uk

#! 

. (17) Substitution into (6a) then eliminates (6b) and (6c). The existence of the inverse v−1  zb,k, za,k

 can be guaranteed by the following theorem. For this, we introduce the notation 

Cn to denote the class of functions whose derivatives up to order n exist and are continuous. 

Theorem 5 (Hadamard’s global inverse function [23]) 

Let the function v (z) : RN → RN be a C 2 mapping. Sup-pose that the determinant of the Jacobian det (Dv (z)) ̸ =

0, ∀z ∈ RN . In addition, suppose that v (z) is proper, i.e., 

∥v(z)∥22 → ∞ as ∥z∥22 → ∞. Then, there exists an inverse function v −1(z) such that v (v−1(z)) = z and v −1(v(z)) = z. 

The conditions of this theorem can be met through a vari-ety of parameterisations of the interconnect matrix and the functions φi(zi) [29, 39]. We prove that the conditions of the theorem can be met by the following conditions on the computational graph GLFR and the functions φbase and φaug .

Condition 6 (Directed acyclic graph) The computational graph G LFR is acyclic, i.e., it contains no cycles. 

Remark 7 For a given adjacency matrix P, the presence of a topological ordering, and thus the acyclic property, can be computed in linear time O(n) [2]. 

Condition 8 (Differentiability) The functions φbase and 

φaug are C 2.

Under these conditions, the following theorem holds: 

Theorem 9 (Well-posedness of the augmentation) Given a parameterisation θ = ( θLFR , θbase , θaug ) of the model augmentation structure (6) , the parameterised structured is well-posed if Conditions 6 and 8 hold. 

PROOF. By Condition 8, φbase and φaug are C2, and also 

v  zb,k, za,k

 =

"zb,k

za,k

#

+ Dzw 

" φbase 

 θbase , zb,k



φaug 

 θaug , za,k

#

(18) is C2. This satisfies the first condition of Hadamard’s global inverse function theorem. Second, the properness of v  zb,k, za,k

. By Condition 6, we can, by elementary row and column operations, retrieve a strict lower triangular of Dzw Dφ (z). This implies that the function ¯ φ (z) = Dzw φ (z) has without loss of generality, the structure ¯φi(z) = ¯φi(z1, . . . , zi−1), i = 1, . . . , nz,

with ¯ φ (z) = col   ¯φ1, . . . , ¯φnz

. Each component of v is given by vi(z) = zi + ¯φi(z1, . . . , zi−1), i = 1, . . . , nz. (19) We then prove that ∥v(z)∥22 → ∞ as ∥z∥22 → ∞. From 

∥z∥22 → ∞, we have that at least one |zi| → ∞ while the remainder may remain bounded. We thus prove that this zi

induces ∥v(z)∥22 → ∞. For i = 1, ¯ φ1(z) is constant, therefore, 

v1(z) = z1 + c1 for some constant c1 ∈ R. Then |v1(z)| → ∞

as |z1| → ∞. For i = 2, . . . , nz, if ¯ φi is bounded, then by (19) 

|vi(z)| → ∞ as |zi| → ∞. Thus if any |zi| → ∞, and thus 

∥z∥22 → ∞, then ∥v(z)∥22.Third, we prove the Jacobian determinant condition. The Jacobian determinant of v  zb,k, za,k

 is written as det  Dv  zb,k, za,k

 = det  I − Dzw Dφ  zb,k, za,k

 . (20) By Condition 6, we know that all subgraphs of GLFR 

are acyclic. Thus the feedback connection, given as 7Dzw diag (Pb, Pa), must be acyclic and thus nilpotent. Then, by the construction of Pb and Pa, also the matrix 

Dzw diag (Dφbase , Dφaug ) is nilpotent, implying det I − Dzw 

"Dφbase 00 Dφaug 

#! ̸

= 0. (21) Then v  zb,k, za,k

 has an inverse v−1 and the algebraic loop can be eliminated, proving the well-posedness of (6). ■

Remark 10 Condition 6, which ensures the well-posedness of the LFR-based structure in (6) , can be satisfied through several strategies. The simplest approach is to restrict Dzw ≡ 0 during model training. However, this constraint limits the generality of the method by reducing the variety of model augmentation structures that the LFR-based rep-resentation can capture. A more flexible alternative is to in-clude only one of the components, either D ba  

> zw

or D ab 

> zw

, in the parameter vector θLFR , while setting all other elements of Dzw to zero. This approach preserves the acyclic condition while allowing for a richer class of model augmentation structures to be represented by the LFR-based formulation. The selection between tuning D ba  

> zw

or D ab  

> zw

is a modelling choice and should be guided by prior physical insights. It is left to future work to develop parameterisations that directly guarantee well-posedness of the model structure. 

5 Identification Algorithm 

Given the proposed model augmentation structure (6), our goal is to estimate the parameters of the model structure based on measured data in order to capture the behaviour of the data-generating system (1). To this end, we specify an identification algorithm consisting of identification criterion, baseline parameter regularisation, data and baseline model normalisation, and parameter initialisation. 

5.1 Truncated loss function 

We adapt the multiple-shooting-based truncated objective function [5]. This is a truncated prediction loss based ob-jective function. The data DN is split into N subsections of length T . This allows for the use of computationally effi-cient batch optimisation methods popular in machine learn-ing, while also increasing data efficiency [5]. This truncated objective function is given as 

Vtrunc (θ ) = 1

N − T + 1 

> N−T+1

# ∑

> i=1

1

T 

> T−1

# ∑

> ℓ=0

ˆyki+ℓ|ki −yki+ℓ 22 (22a) 



ˆxki+ℓ+1|ki

ˆyki+ℓ|ki

wb,ki+ℓ|ki

wa,ki+ℓ|ki



:= W (θLFR )



ˆxki+ℓ|ki

uki+ℓ

zb,ki+ℓ|ki

za,ki+ℓ|ki



(22b) 

wb,ki+ℓ|ki := φbase 

 θbase , zb,k

 (22c) 

wa,ki+ℓ|ki := φaug 

 θaug , za,k

 (22d) ˆxk|k := ψ



θencoder , yk−1 

> k−na

, uk−1

> k−nb



, (22e) 

Fig. 6. SUBNET structure: the subspace encoder ψθ estimates the initial state at time k based on past inputs and outputs, then it is propagated through φθ multiple times until a simulation length T .

where θ = col (θbase , θaug , θLFR , θencoder ) is the joint parame-ter vector, k +ℓ|k indicates the state ˆ xk or the output ˆ yk at time 

k + ℓ simulated from the initial state ˆ xk|k at time k. The sub-sections start at a randomly selected time ki ∈ { n+1, . . . , N −

T }. The initial state of these subsections is estimated by an encoder function ψ using past input-output data, i.e., ˆ xk|k =

ψ(θencoder , yk−1 

> k−na

, uk−1 

> k−nb

) where uk−1 

> k−nb

= [ u⊤ 

> k−nb

· · · u⊤

> k−1

]⊤

for τ ≥ 0 and yk−1 

> k−na

is defined similarly. The model structure with the encoder ψ included, is shown in Fig. 6. The existence of the encoder ψ has been shown in [4] for state-space models. We give a brief overview of the under-lying mechanism. We derive the n-step ahead predictor of the data generating system (1), resulting in 

yn+kk =



h(xk, uk) + ek

(h ◦ f )( xk, uk+1 

> k

) + ek+1

...

(h ◦n f )( xk, un+kk ) + ek+n

| {z }

> Γn

(xk,un+kk )+en+kk

. (23) where ◦n stands for n times recursive function composition. If Γn is partially invertible as xk = Φn(un+kk , yn+kk − en+kk ),then the reconstructability map [21] is given as 

xk = ( ◦n f )( xk−n, ukk−n) (24a) 

= ( ◦n f )( Φn(ukk−n, ykk−n − ekk−n), ukk−n) (24b) 

= Ψn(ukk−n, ykk−n − ekk−n). (24c) However, the noise sequence ekk−n is not available in practice. Under the assumption that ek is i.i.d. white noise, we can use the conditional expectation of (24c) as an estimate of xk:¯xk = Eek

h

xk|ukk−n, ykk−n

i

= ¯Ψ (ukk−n, ykk−n), (25) which is an unbiased estimator of xk [22]. This estimator is difficult to compute in practice due to the required analytical inversion to obtain Φn, which varies with the choices of θbase ,

θaug and θLFR . Instead the parameterised function estimator 

ψ is used to co-estimate ¯Ψ with W (θLFR ), φbase and φaug .

5.2 Baseline parameter regularisation 

The proposed LFR-based model structure (6) is overpa-rameterised and, as a result, the optimal parameter values 8 θ ∗

> base

, θ ∗

> aug

, θ ∗

> LFR

 that minimise (22) are not unique. There-fore, the joint identification of all parameters can result in the learning components representing or cancelling out part of the baseline model dynamics. These non-unique parame-terisations have similar model performance; however, some parameterisations may generate physical parameters that de-viate wildly from the expected parameters, even leading to physically unrealistic parameter values. To address this is-sue and retain the interpretability of the baseline model, we adapt the regularisation cost term from [8, 9]: 

V reg (θ ) = Λ  θbase − θ 0base 

 22 (26) where Λ = λ diag (θ 0base )−1 with λ ∈ R≥0 as a tunable pa-rameter. This regularisation term (26) penalises deviations of the baseline parameters from the a priori selected values 

θ 0base , with the diagonal element normalising the importance of the parameters compared to each other. The cost function with the regularisation term becomes 

VDN (θ ) = V trunc  

> DN

(θ ) + V reg (θ ). (27) The tunable variable λ determines how much the baseline parameters can deviate from the nominal parameter set θ 0base ,relative to the change in T -step-ahead prediction cost (22). 

5.3 Data and baseline model normalisation 

For the estimation of model structures containing ANNs, normalisation of the input and output data to zero mean and to a standard deviation of 1 has been shown to improve model estimation [6]. Therefore, we normalise u and y in 

DN and aim to initialise the model structure and (6) so that ˆx is also normalised as in [5]. For this, the to-be-augmented baseline model φbase needs to be considered in the normali-sation process. We take φbase into consideration based on the work in [32]. This results in a model that takes normalised input and state, and returns a normalised output while not altering its dynamics. For a baseline model operating around zero mean ˆ x, u, ˆ y, this can be achieved by the transformation ¯fbase = Tx fbase 

 θbase , T −1 

> x

xb, T −1 

> u

u (28a) ¯hbase = Tyhbase 

 θbase , T −1 

> x

xb, T −1 

> u

u , (28b) where Tu ∈ Rnu is a diagonal matrix composed as T −1u =

diag (σ −1 

> u1

, . . . , σ −1 

> unu

), where σu is the sample-based standard deviation of each input signal, computed based on the data set DN . The transformation matrix Ty is defined in a sim-ilar way. For Tx, the standard deviation σx is determined on a sequence of baseline states xb. For this, we simulate the baseline model with θ 0base for the given input sequence resulting in ˆxb,k. We define an extended data set ˆDN =

{ yk, ˆxb,k, uk

}Nk=1 for initialisation of the encoder later. 

5.4 Model structure initialisation 

Next, we consider the initialisation of the learning compo-nents, LFR matrix, and the encoder. A common approach in the literature for initialising ANNs is to randomly assign weights and biases. However, for baseline models, random initialisation can be unstable and result in poor optimisation results. Furthermore, initialisation based on prior informa-tion, e.g., baseline model behaviour, can improve conver-gence rate and enhance model accuracy. 

5.4.1 Model behaviour at initialisation 

We propose to initialise the parameters θ so that the LFR-based model structure with the encoder behaves equivalent to the baseline model on initialisation. We note the model structure (6) with zb,k and za,k eliminated as Ω. Then the initialisation of θ should realise the following behaviour 



xb,k+1

xa,k+1

ˆyk

 = Ω  θ , zb,k, za,k

 =



fbase (θ 0base , zb,k)

φaug (θ 0aug , za,k)

hbase (θ 0base , zb,k)

 , (29) where θ 0aug are the initialised parameters for the augmenta-tion. It is not trivial to determine such an initialisation for any arbitrary baseline model, learning function, and LFR matrix combination. We propose here a method that can achieve this initialisation under the following conditions: (a) The computational graph GLFR is acyclic (b) The learning functions are ResNets (15). Condition (a) is the same condition as for the well-posedness proof and thus is not restrictive. Condition (b) is required to feasibly create series augmentations that can have baseline model behaviour. 

5.4.2 Encoder initialisation 

The encoder is parameterised by an ANN, e.g., a ResNET. To guarantee baseline behaviour at initialisation, this encoder should at initialisation output the baseline state sequence as in the extended dataset ˆDN . This could be derived analyti-cally as in Section 5.1, but this is complicated on not fea-sible for all ANNs. Instead, we consider a data-driven ap-proach. We fit a baseline encoder ψbase on this dataset using the following loss function during the initialisation step 

Venc (θ ) = 1

N

> N

# ∑

> k=1

ψbase 



θbase , ˆyk−1 

> k−na

, uk−1

> k−nb



− ˆxb,k 

> 22

, (30) where ˆ xb,k is the forward simulated state of the baseline model in the extended dataset ˆDN . If augmented states are considered, the baseline encoder ψbase is extended with an augmented state encoder ψaug 

"xb,k|k

xa,k|k

#

=

ψb



θbase , ˆyk−1 

> k−na

, uk−1

> k−nb



ψa



θaug , ˆyk−1 

> k−na

, uk−1

> k−nb

 , (31) where the weights and biases of ψaug are initialised by the Xavier approach. The loss function (30) is no longer con-sidered after initialisation. 

5.4.3 LFR matrix and learning component initialisation 

We now initialise the LFR matrix and the learning compo-nents so that (29) holds for initialisation. We can simplify this equation with Condition (b) by only considering the lin-ear component of the learning function and initialising the NL component to be zero, i.e., φaug 

 za,k

 = 0 +Waza,k. We 9further assume under the acyclic property, without loss of generality, that Dzw is lower block diagonal with respect to the learning functions and the baseline model. This means that Daa  

> zw

= 0, Dbb  

> zw

= 0, and either Dba  

> zw

= 0 or Dab  

> zw

= 0. Sub-stituting the linear component of the learning function into (16) and eliminating za,k, gives 

zb,k =

h

Cb 

> z

+ Dba 

> zw

WaCa 

> z

Db 

> zu

+ Dba 

> zw

WaDa

> zu

i| {z }

> ˜C

" ˆxk

uk

#

, (32) and the prediction equation 

" ˆxk+1

ˆyk

#

=

" A + Ba

> w

WaCa 

> z

Bu + Ba

> w

WaDa

> zu

Cy + Da

> yw

WaCa 

> z

Dyu + Da

> yw

WaDa

> zu

#| {z }

> ˜A

" ˆxk

uk

#

+

" Bb 

> w

+ Ba

> w

WaDab 

> zw

Db 

> yw

+ Da

> yw

WaDab 

> zw

#| {z }

> ˜B

φbase 

 zb,k

 . (33) Thus, to have an initialisation satisfying (29), we require ˜B = Inx+ny and ˜C = Inx+nu . We repeat here that we assume either Dba  

> zw

= 0 or Dab  

> zw

= 0 to satisfy Condition 6. The choice between these assumptions results in initialisation similar to the model structures derived in Appendix A, with Dba  

> zw

= 0resulting in series output augmentations, Dab  

> zw

= 0 in series-input and Dba  

> zw

= Dab  

> zw

= 0 in parallel augmentation. All matrices not required to set the baseline model behaviour at initialisation (29) have all elements m of the matrix ini-tialised randomly, according to [32], i.e., m ∼ U (−1, 1)

where U (a, b) denotes a uniform distribution with support from a to b.

5.5 Convergence and Consistency 

Next, we can analyse the statistical properties of the intro-duced augmentation approach in terms of convergence and consistency [24]. Convergence implies that, as the number of samples in DN tends to infinity, the empirical identifica-tion criterion approaches its expected value. An estimator is 

consistent if, as N → ∞, the estimated model converges to an equivalent representation of the true system (1). Under Conditions 2.1–2.4 in [5] on the data-generating pro-cess (1), model structure (6), and identification criterion (22), the convergence and consistency are proven. Condition 2.1 requires the data-generating system to be incrementally ex-ponential output stable, which becomes an assumption on the considered system (1). Similarly, Conditions 2.3 on pre-dictor convergence and 2.4 on persistence of excitation are commonly assumed to hold. However, Condition 2.2 war-rants additional consideration, as it demands differentiability of the model structure, including the encoder in (22), with respect to θ . This is not trivial due to the feedback connec-tion in the model structure. If we take the acyclic condition (Condition (a)), then differentiability of v(zb,k, za,k) is en-sured, and it remains only to assume differentiability of the prediction mappings (6a), φbase , and φaug —a standard and                                            

> Table 3 Physical parameters of the 3-DOF MSD systems. Body Mass miSpring kiDamper ciHardening ai
> 10.5 kg 100 Nm0.5Ns m100 Nm3
> 20.4 kg 100 Nm0.5Ns m-30.1 kg 100 Nm0.5Ns m-Table 4 Approximate physical parameters 2-DOF MSD baseline model. Body Mass miSpring kiDamper ci
> 10.5 kg 95 Nm0.45 Ns m
> 20.4 kg 95 Nm0.45 Ns m

non-restrictive assumption in system identification. Thus, all conditions in [5] are fulfilled to imply convergence and con-sistency of the proposed model augmentation approach. 

6 Simulation Study 

In this section, we analyse the performance of different model augmentation realisations of the LFR model when applied to various systems using a common baseline model. The objective is to demonstrate that the proposed LFR aug-mentation structure together with the proposed identifica-tion methods can effectively estimate system behaviour start-ing from the baseline model, while achieving faster estima-tion and offering greater transparency compared to the fully ANN-SS model. We focus here on structured parameterisa-tions of the LFR in the form of those presented in Table 1 and Table 2. In Section 7, this analysis is extended to flex-ible parameterisations, applied to a real-world setup with a more complex baseline model, where the most suitable form of augmentation is not known a priori. 

6.1 Mass-Spring-Damper system and data generation 

As a simulation example, a mass–spring–damper (MSD) system consisting of three masses is considered under three different configurations, as illustrated in Fig. 7. The corre-sponding physical parameters are listed in Table 3. Config-uration (a) consists of three masses and a hardening spring nonlinearity. Configurations (b) and (c) add to this an in-put saturation and a first-order low-pass filter (LPF) to the output, respectively. Configuration (a) is described in terms of a state-space rep-resentation with a total of 6 states as the positions pi and velocities ˙pi of the masses m1, m2, and m3. The harden-ing spring nonlinearity is simulated as a cubic stiffness term multiplied by the parameter a1. The measured output cor-responds to the position p2, and the external input force is applied to the first mass m1. For Configuration (b) the sat-uration is 30 tanh ( u 

> 30

) which results in a reduction of the applied multisine input signal RMS from 10 .0 N to 9 .11 N. For Configuration (c), the LPF is simulated such that the Bode plot of the LPF component is as shown in Fig. 10. The system is simulated using a 4th order Runge-Kutta 

(RK4) numerical integration with step size Ts = 0.02 s and 10 m1 m2 m3               

> a1
> k1
> k2k3
> c1
> c2c3
> Fu
> p1
> y
> p2p3(a) 3-DOF MSD with nonlinear cubic spring (b) 3-DOF MSD with nonlinear cubic spring and input saturation
> (c) 3-DOF MSD with nonlinear cubic spring and low-pass filter
> (d) 2-DOF MSD linear baseline model Fig. 7. Considered baseline model in blue and black and the MSD data generating systems with additional dynamics in red. Table 5 Hyperparameters for identifying the LFR-based augmentation and ANN-SS models. hidden layers nodes nanbTepochs batch size 287200 3000 2000

synchronised zero-order-hold actuation and sampling. The values of the DT input signal uk are generated by a random-phase multisine with 1666 frequency components in the range [0, 25 ] Hz with a uniformly distributed phase in [0, 2π).The sampled output measurements yk are perturbed by an additive white noise process ek ∼ N (0, σ 2e ). Here, σe is cho-sen so that the signal-to-noise ratio is equal to 30 dB. The generated sampled output yk for the input uk is collected in the data set DN . Separate data sets are created for estima-tion, validation and testing of different realisations of sizes 

Nest = 2 · 10 4, Nval = 10 4, Ntest = 10 4, respectively. The esti-mation data comprise two periods, while the validation and testing data each contain a single period. 

6.2 Baseline model 

The baseline model is chosen to represent the linear 2-DOF MSD dynamics shown in Fig. 7d. We consider two initial-isations for the baseline model parameters: the ideal val-ues from Table 3 and the approximate values from Table 4. The root mean squared error (RMSE) of the simulation re-sponses of the baseline model for these initial parameter val-ues is shown in Table 6 for configurations (a-c). Both initial-isations perform relatively poorly, despite (approximately) representing a large part of the dynamics. 

6.3 Config. (a): MSD with added cubic spring and mass 

First, we consider the augmentation of the baseline model in a structured form using the introduced LFR model struc-ture. For this we consider various parameterisations of the LFR matrix W corresponding to the configurations in Ta-ble 1 (not including S-SSI or S-DSI as we know a-priori that these will not model the system given the baseline model). For parallel augmentation structures, the learning compo-nents are chosen as feedforward neural networks, and for series augmentation structures, we choose ResNets to have a feasible initialisation (see Section 5.4). For all learning components, the number of hidden layers and neurons are listed in Table 5. The activation function is chosen as tanh. For the dynamic augmentation, we add two additional states to the baseline model states for a total of 6 states. This is the minimum number of states required to completely model the 3-DOF MSD system. The baseline model, learning com-ponent, and encoder parameters are jointly estimated as de-scribed in Section 3. The hyperparameters for these estima-tions are shown in Table 5, with 16 nodes and 2 hidden layers used for the encoder. The regularisation tuning parameter λ

in the joint identification cost function was set at λ = 1 for ideal initialisation. The identification criterion is optimised using Adam. As a comparison to a black-box approach, an ANN-SS model parameterised by ResNets is estimated with the SUBNET method [4] and hyperparameters from Table 5. The simulation RMSE on the test data for these estimated models is shown in Table 6. The dynamic augmentations are able to capture the dynamics accurately, while the static augmentations result in slightly less accurate models. In Fig. 8 we show the validation loss curves of select esti-mated models. The convergence speed of the ANN-SS and S-DSO models is slower than the S-DP model, while all models achieve similar RMSE scores. The estimated physical parameters remain very close to the initialisation values for both the ideal and the approximate case. This is not desired behaviour for the approximate initialisations, indicating that the learning components are learning parts of the system dynamics that could be repre-sented by the baseline model. In Fig. 9, we show the comparison between the states ˆ x of the S-DP model ( ) and the outputs of the learning components 11 0 100 200 300 400 500 epochs 10 −2

> 10 −1
> RMSE

Fig. 8. Validation loss over first 500 training epochs for S-DP ( ), S-DSO ( ) and black-box ANN-SS ( ) models estimated for the MSD system with configuration (a). The noise floor is shown with a red dashed line ( ). 0.02.5   

> x1
> −2.50.02.5
> x2
> 0.02.5
> x3
> −2.50.02.5
> x4
> −0.25 0.00 0.25
> x5
> 100 200 300 400
> k
> −0.50.0
> x6

Fig. 9. Comparison of the augmented model with dynamic parallel configuration states ˆ x ( ) and the outputs of the learning compo-nents φaug ( ) for a simulation with test data of the MSD system with configuration (a). States x1 - x4 are states of the augmented model based on the sum of fbase and faug , while states x5 - x6 are the output of the dynamic augmentation component gaug .

φaug ( ). Here, xb = [ x⊤ 

> 1

. . . x⊤ 

> 4

]⊤ and xa = [ x⊤ 

> 5

x⊤ 

> 6

]⊤. The effect of the learning components is relatively small for xb,while xa is modelled solely by the learning components. From this, we can conclude that the learning components are augmenting the baseline and not replacing the baseline model with their own dynamics. 

6.4 Config. (b): MSD with added input saturation 

For the MSD system in configuration (b), we applied similar identification steps as in Section 6.3. We again estimate S-SP and S-DP as described in Table 1. We, however, further combine these parameterisations with a series input augmen-tation to characterise the input saturation, which we note as S-SP-I with the following structure 

xb,k+1 = fbase 

 xb,k, gaug (uk) + faug 

 xb,k, uk

 . (34) A similar structure is used for a S-DP with series input augmentation noted as S-DP-I. The remaining parameteri-sations and hyperparameters are as in Section 6.3. We again estimate an ANN-SS model parameterised by ResNets as a black-box comparison. The simulation RMSE on the test data for these estimated models are shown in Table 6. All selected augmentations re-

Fig. 10. Bode plot of both O-DSO estimated with the S-DP ( )and S-SP ( ), compared against the LPF included in system configuration (c) ( ). 

sult in accurate models. However, the series input augmen-tation specifically models the input saturation as a function of uk and thus results in a more interpretable model. The convergence of the estimated models are similar as in Fig. 8, with the S-SP-I and S-DP-I models converging faster than the backbox ANN-SS, S-SP and S-DP models. 

6.5 Config. (c): MSD with added output low-pass-filter 

For the MSD system in configuration (c), we applied similar identification steps as in Section 6.3. We again estimate the S-SP and S-DP described in Table 1. We now further esti-mate these state augmentations with and O-DSO described in Table 2. The O-DSO is parameterised by an LTI model with a single state, which is capable of modelling the dy-namics of the first-order LPF. The remaining parameterisa-tions and hyperparameters as in Section 6.3. We again es-timate an ANN-SS model parameterised by ResNets as a black-box comparison. The simulation RMSE on the test data for these estimated models is shown in Table 6. All selected augmentations re-sult in accurate models. The convergence of the estimated models are similar as in Fig. 8, with the O-DSO models, S-DP and S-SP converging faster than the backbox ANN-SS model. In Fig. 10, the Bode plot of haug from both the estimated O-DSO model, with S-SP and S-DP state models, are shown compared against the true system LPF. We can see that the output augmentations model behaviour similar to the sys-tem LPF thus enhancing the interpretability of the estimated model compared to the model augmentations without the output augmentation where this behaviour will have to be modelled in the state augmentation. Ensuring that output augmentations model the LPF exactly is an identifiability problem left to future research. 

7 Experimental study 

In this section, we demonstrate the capabilities of the gen-eral LFR-based model augmentation structure and the pro-posed estimation approach by identifying the dynamics of an F1Tenth electric vehicle, using experimental data. 

7.1 F1Tenth vehicle 

F1tenth is a 1/10 scale model of an electric car, which has been mainly developed as a test platform for various auto-motive applications [1]. To demonstrate the capabilities of the proposed LFR-based model augmentation structure, the dynamics of such a vehicle are identified in this section. In contrast to Section 6, measurements from a real F1tenth are 12 Table 6 RMSE of the simulated responses from the estimated models evaluated on the test sets generated by the MSD system with configurations (a), (b), and (c). Model Config. (a) Config. (b) Config. (c) Ideal Approx. Ideal Approx. Ideal Approx. baseline 1.97 · 10 −1 1.77 · 10 −1 1.96 · 10 −1 1.86 · 10 −1 2.22 · 10 −1 2.13 · 10 −1                                                                                                         

> S-SP 6.73 ·10 −36.61 ·10 −35.53 ·10 −36.03 ·10 −35.86 ·10 −35.53 ·10 −3
> S-DP 5.54 ·10 −35.77 ·10 −35.79 ·10 −35.80 ·10 −35.46 ·10 −35.39 ·10 −3
> S-SSO 7.07 ·10 −36.80 ·10 −3––––S-DSO 5.33 ·10 −35.57 ·10 −3––––S-SP-I ––5.40 ·10 −35.78 ·10 −3––S-DP-I ––5.44 ·10 −35.41 ·10 −3––S-SP & O-DSO ––––5.45 ·10 −35.42 ·10 −3
> S-DP & O-DSO ––––5.39 ·10 −35.38 ·10 −3
> blackbox ANN-SS 5.72 ·10 −36.37 ·10 −35.55 ·10 −3

used instead of simulation data. An in-depth description of the used vehicle and test environment is available in [14]. 

7.2 Baseline model of the F1Tenth vehicle 

To develop a baseline model of the F1Tenth platform, the so-called single-track model has been used [27]. The model is illustrated in Fig. 11, and can be expressed by using six state variables. The baseline states are the position of the center-of-gravity (CoG) in the (X,Y ) plane (px, py), the orientation of the vehicle ϕ, which is measured from the X axis, the longitudinal and lateral velocities of the vehicle, and the yaw rate. The control inputs are the steering angle δ , and the PWM percentage of the electric motor that provides the main propulsion of the vehicle. The equations of the single-track model can be derived in continuous time, and the resulting model is discretised using the RK4 scheme. Since the used OptiTrack motion capture system measures the position and orientation of the vehicle, while the built-in IMU and speed sensors provide information regarding the velocity components, full state measurements are available. Hence, the baseline output function becomes ˆ yk = xb,k. To model the longitudinal tire force component Fξ , an empirical drivetrain model [14] is applied, while the linearised Magic Formula [26] has been utilised to model the lateral tire force components ( Ff,η and Fr,η ). For a detailed derivation and discussion of the baseline model, refer to [13, 18]. The applied tire models (especially the empirical drivetrain model) are highly approximative and are the primary sources of inaccuracy in the baseline model; hence, identifying the dynamics of the F1Tenth vehicle is challenging even when incorporating existing physical knowledge into the model structure. Moreover, there are 9 baseline parameters (such as mass, inertia, distance of the rear and front end from the CoG, and parameters corresponding to the tire models) that need to be estimated. Initial values of these parameters were determined in [13]. However, some elements of this initial parameter vector θ 0base are highly approximative. Hence, to achieve an accurate representation of the true dynamics, θbase 

is tuned jointly with the model parameters. 

> Fig. 11. Illustration of the single track model.

7.3 Data acquisition 

A lemniscate-shaped trajectory has been selected for gener-ating measurement data because, by following it, the head-ing angle traverses the whole operational domain, and the resulting motion has quick changes in velocity. A second tra-jectory has been chosen to be a circle-shaped path because it is also a typical maneuver for this type of vehicle. Measure-ment data has been collected with a sampling frequency of 

fs = 40 Hz. To acquire data with various motor PWM inputs, the velocity references have been varied for both trajectories, ranging from 0.45 m/s to 1 m/s with step increments of 0.05 m/s, resulting in a total of 24 measurement records. Half of these measurements have been separated into training and test sets. Both trajectories with alternating reference veloci-ties have been included in all data sets. Before concatenating the measurement signals for the training data set, contigu-ous segments (corresponding to 20% of the length of each of the 12 training signals) were randomly selected to form the validation data set. A total of 6467 samples are used for estimation, 1669 for validation, and 8041 for testing. 

7.4 Estimated models 

As proposed by [18, 36], to simplify the neural network structure, we only identify the input-to-velocity relationship 13 Table 7 Hyperparameters of the LFR-based model augmentation structure for identifying the dynamics of the F1Tenth vehicle. hidden layers nodes na nb T epochs batch size 2 128 12 40 3000 256 

by detaching the integrators. Hence, the outputs (and con-sequently, the baseline states) become the longitudinal and lateral velocity components, as well as the yaw rate of the vehicle. Then, after identification, by putting back the inte-grator dynamics, the position and orientation values can also be obtained during model simulation. As all baseline model states are measured, the initial val-ues of xb are known for all subsections when calculating the truncated prediction loss, i.e., the encoder network only estimates the augmented states in case a dynamic model augmentation structure is applied. For the latter scenario, a fully-connected feedforward ANN with 2 hidden layers and 64 nodes per layer has been selected for the encoder network with the tanh activation function. Based on previous black-box identification results on the same dataset (see [36]), an encoder lag of na = nb = 12 has been applied. The aug-mented state dimension has been selected based on physical insight and a short trial-and-error period as nxa = 2. For the LFR-based structure, nza = 4 has been applied, while nwa = 3and nwa = 5 were selected for the static and dynamic model augmentations, respectively. A regularisation coefficient of 

λ = 0.01 has been applied, as a result of a line-search. All other hyperparameters are summarised in Table 7. As discussed in Remark 10, there are a few possible strate-gies to ensure the well-posedness of the LFR-based struc-ture (6). The most straightforward one is to restrict Dzw ≡ 0, while a more general approach is to select either Dba  

> zw

or Dab 

> zw

to be tuned freely, while the rest of Dzw is set to zero. To demonstrate these approaches, we have trained models with different options regarding the structure of the Dzw matrix. Furthermore, we demonstrate the enforcing of model struc-tures in the LFR matrix W by constraining the Cb 

> z

and Db

> zu

matrices such that zb,k ≡ vec  xb,k, uk

, we have also trained models with that setting. The results are summarised in Ta-ble 8, where the augmented models are compared with state-of-the-art black-box identification results and the baseline model with nominal parameters. As the integrator dynamics make it difficult to obtain accurate long-term predictions in practice, the presented error values only consider the veloc-ity components of the output. This is in line with previous black-box results using the same data set, see [36]. Still, to demonstrate the accuracy of the simulated position and ori-entation values, Fig. 12 presents these signals obtained using the best-performing static and dynamic LFR-based models for an arbitrarily selected test trajectory. Notably, both mod-els demonstrate remarkable accuracy compared to the real measured data, even over extended open-loop simulations lasting 16–18 seconds. It is important to note that the re-ported errors are also influenced by the employed numerical integration scheme. This effect is particularly visible in the case of the dynamic LFR model, where the simulated tra-                                    

> Table 8 Normalised root mean squared simulation error of the estimated models on the F1Tenth identification study. Model Test NRMS error static LFR-based ( Dzw ≡0) 10.71% static LFR-based ( Dab
> zw tuned) 9.27%
> static LFR-based ( Dba
> zw tuned) 10.44% static LFR-based ( Dzw ≡0, zbfixed) 9.42% static LFR-based ( Dab
> zw tuned, zbfixed) 9.90% dynamic LFR-based ( Dzw ≡0) 8.79% dynamic LFR-based ( Dab
> zw tuned) 8.52% dynamic LFR-based ( Dba
> zw tuned) 8.99% dyn. LFR-based ( Dzw ≡0, zbfixed) 8.25%
> dyn. LFR-based ( Dab
> zw tuned, zbfixed) 8.41% Initial baseline model 49.12% DT SUBNET (black-box, [36]) 8.53% CT SUBNET (black-box, [36]) 8.99%

jectory initially closely follows the measured data, but the accuracy continuously deteriorates due to the accumulation of integration errors and unknown input disturbances. Further analysing the results shown in Table 8, it is visi-ble that the most general options (not fixing zb, and tuning 

Dab 

> zw

) have resulted in the best model accuracy for the static structure. As the applied approximative baseline model only expresses the dominant high-order dynamics of the real sys-tem, all dynamic augmentation structures have outperformed the static models. Introducing the augmented states in the LFR-based structure increases the model DoF compared to static structures. This helps explain why the highest accu-racy for the dynamic LFR augmentation is obtained when certain constraints are imposed on the LFR matrix. This example highlights the importance of selecting the optimal model complexity: richer parametrisations can improve ex-pressiveness, but overly flexible models may suffer from re-duced robustness. Introducing suitable restrictions can lead to improved performance by mitigating variance effects. It is also worth noting that multiple dynamic LFR-based aug-mented models have resulted in better model accuracies than the black-box methods. All model augmentation structures have shown similar con-vergence properties as the DT SUBNET approach (a state-of-the-art black-box identification method); however, the dy-namic LFR-based method performed the best in terms of convergence speed. Hence, the proposed model augmenta-tion structure was able to generate more accurate results with better convergence properties than black-box methods. 

8 Conclusion 

In this paper a novel general LFR-based model augmenta-tion has been proposed that provides unified representation 14 −1 0 1

x pos. [m] 

−2

−1

0

1

2

> y pos. [m]

0.00 

0.25 

> |ex| [m]

0.0

0.2

> |ey | [m]

0 10 Time [s] 

0.0

0.2

> |eϕ| [rad]

Meas. Static LFR Dyn. LFR Fig. 12. Comparing the simulated model response with measure-ments on the test data. 

structure. The model was also expressed in graph form, of-fering insight into the structural patterns that characterise and enable the detection of specific augmentation structures. In addition, we established conditions ensuring the well-posedness of the proposed model structure. To provide reli-able estimation of the proposed model structure, an adapta-tion of the SUBNET approach was implemented, inheriting the consistency guarantees of the SUBNET approach. By analysing various augmentation configurations with simu-lation and experimental data, we have shown that suitable positioning of the learning component provides faster con-vergence and can lead to more accurate models compared to state-of-the-art black-box approaches. 

Acknowledgements 

We thank Mircea Lazar, Max Bolderman and Chris Verhoek for the helpful discussions for this work. 

References 

[1] A. Agnihotri et al. Teaching Autonomous Systems at 1/10th-scale: Design of the F1/10 Racecar, Simulators and Curriculum. In Proc. of the ACM Tech. Symp. on Comp. Sci. Edu. , pages 657–663, 2020. [2] J. Bang-Jensen and G. Z. Gutin. Digraphs: theory, algorithms and applications . Springer, 2008. [3] F. L. Bauer. Computational graphs and rounding error. SIAM Journal on Numerical Analysis , 11(1):87–96, 1974. [4] G. I. Beintema. Data–driven Learning of Nonlinear Dynamic Systems: A Deep Neural State–Space Approach . Phd thesis, Eindhoven University of Technology, 2024. [5] G. I. Beintema, M. Schoukens, and R. Tóth. Deep subspace encoders for nonlinear system identification. Automatica , 156:111210, 2023. [6] C. M. Bishop. Neural networks for pattern recognition . Oxford university press, 1995. [7] T. P. Bohlin. Practical grey-box process identification: theory and applications . Springer, 2006. [8] M. Bolderman et al. Physics-guided neural networks for feedforward control with input-to-state-stability guarantees. Control Engineering Practice , 145:105851, 2024. [9] M. Bolderman, M. Lazar, and H. Butler. On feedforward control using physics–guided neural networks: Training cost regularization and optimized initialization. In Proc. of the European Control Conf. ,pages 1403–1408, 2022. [10] M. Chioua (moderator). Machine learning and control. https:// www.youtube.com/watch?v=Jb7oU81QJPU (42:15), 2021. [11] A. Daw et al. Physics-guided neural networks (pgnn): An application in lake temperature modeling. In Knowledge Guided Machine Learning , pages 353–372. Chapman and Hall/CRC, 2022. [12] L. El Ghaoui et al. Implicit deep learning. SIAM Journal on Mathematics of Data Science , 3(3):930–958, 2021. [13] K. Floch. Model-based motion control of the F1TENTH autonomous electrical vehicle . Bachelor’s thesis, Budapest University of Technology and Economics, 2022. [14] K. Floch et al. Gaussian-process-based adaptive tracking control with dynamic active learning for autonomous ground vehicles. IEEE Transactions on Control Systems Technology , pages 1–13, 2025. [15] D. Frank et al. Robust recurrent neural network to identify ship motion in open water with performance guarantees–technical report. 

arXiv preprint arXiv:2212.05781 , 2022. [16] R.-S. Götte and J. Timmermann. Composed physics-and data-driven system identification for non-autonomous systems in control engineering. In Proc. of the 3rd Int. Conf. on Artificial Intelligence, Robotics and Control , pages 67–76, 2022. [17] W. D. Groote et al. Neural network augmented physics models for systems with partially unknown dynamics: Application to slider-crank mechanism. IEEE/ASME Transactions on Mechatronics , 27:103–114, 2 2022. [18] B. M. Györök et al. Orthogonal projection-based regularization for efficient model augmentation. In Proc. of the 7th Annual Learning for Dynamics & Control Conf. , pages 166–178, 2025. [19] K. He et al. Deep residual learning for image recognition. In Proc. of the IEEE Conf. on computer vision and pattern recognition , pages 770–778, 2016. [20] J. H. Hoekstra et al. Learning-based model augmentation with LFRs. 

European Journal of Control , 86(A):101304, 2025. [21] A. Isidori. Nonlinear control systems: an introduction . Springer, 1985. [22] S. Janny et al. Learning reduced nonlinear state-space models: an output-error based canonical approach. In Proc. of the 61st IEEE Conf. on Decision and Control , pages 150–155, 2022. [23] S. G. Krantz and H. R. Parks. The implicit function theorem: history, theory, and applications . Springer, 2002. [24] L. Ljung. Convergence analysis of parametric identification methods. 

IEEE Transactions on Automatic Control , 23, 1978. [25] L. Ljung. Perspectives on system identification. Annual Reviews in Control , 34(1):1–12, 2010. [26] H. B. Pacejka. Chapter 4 - Semi-Empirical Tire Models. In Tire and Vehicle Dynamics (Third Edition) , pages 149–209. Butterworth-Heinemann, Oxford, 2012. [27] B. Paden et al. A survey of motion planning and control techniques for self-driving urban vehicles. IEEE Transactions on Intelligent Vehicles , 1(1):33–55, 2016. [28] M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. 

Journal of Computational Physics , 378:686–707, 2019. [29] M. Revay and I. Manchester. Contracting implicit recurrent neural networks: Stable models with improved trainability. In Proc. of the 2nd Conf. on Learning for Dynamics and Control , pages 393–403, 2020. [30] J. Schoukens and L. Ljung. Nonlinear system identification: A user-oriented road map. IEEE Control Systems , 39:28–99, 12 2019. 

15 [31] M. Schoukens. Improved initialization of state-space artificial neural networks. In Proc. of the 2021 European Control Conf. , pages 1913–1918, 2021. [32] M. Schoukens and R. Tóth. On the initialization of nonlinear LFR model identification with the best linear approximation. IFAC-PapersOnLine , 53(2):310–315, 2020. [33] P. Shah et al. Deep neural network-based hybrid modeling and experimental validation for an industry-scale fermentation process: Identification of time-varying dependencies among parameters. 

Chemical Engineering Journal , 441:135643, 8 2022. [34] M. F. Shakib et al. Computationally efficient identification of continuous-time lur’e-type systems with stability guarantees. 

Automatica , 136:110012, 2022. [35] B. Sun et al. A comprehensive hybrid first principles/machine learning modeling framework for complex industrial processes. 

Journal of Process Control , 86:30–43, 2020. [36] M. Szécsi et al. Deep learning of vehicle dynamics. IFAC-PapersOnLine , 58(15):283–288, 2024. [37] R. Tóth. Modeling and identification of linear parameter-varying systems , volume 403. Springer, 2010. [38] J. Veenman, C. W. Scherer, and H. Köro˘ glu. Robust stability and performance analysis based on integral quadratic constraints. 

European Journal of Control , 31:1–32, 2016. [39] E. Winston and J. Z. Kolter. Monotone operator equilibrium networks. 

Advances in neural information processing systems , 33:10718–10728, 2020. [40] K. Zhou, J. Doyle, and K. Glover. Robust and Optimal Control .Feher/Prentice Hall Digital. Prentice Hall, 1996. 

A Proof of Theorem 2 

We provide the proof by parts: 

Parallel augmentation at the state level: Choose nza = nxb +

nu + nxa and nwa = nxb + nxa . Take zb,k ≡ vec  xb,k, uk

 and 

za,k ≡ vec  xb,k, xa,k, uk

. For zb,k, this is achieved by setting 

Cb 

> z

=

" Inxb 0nxb ×nxa

0nu×nxb 0nxb ×nxa

#

, Db 

> zu

=

"0nxb ×nu

Inu

#

, (A.1) while Dbb 

> zw

, and Dba  

> zw

are set to zero, while for za,k, this is achieved by setting 

Ca 

> z

=

h

I⊤  

> nxb+nxa

0⊤  

> nu×(nxb+nxa)

i⊤

, Da 

> zu

=

h

0⊤   

> (nxb+nxa)×nu

I⊤

> nu

i⊤

,

and setting Dab 

> zw

, and Daa  

> zw

as zeros. Next, we take ˆ xk+1 ≡

wb,k + wa,k. This is achieved by setting Bb 

> w

= Inxb , and setting 

Abb , Aba , Bb

> u

, and Bba  

> w

as zeros. Then, we have 

"xb,k+1

xa,k+1

#

=

" fbase (xb,k, uk)

0nxa ×nxa

#

+ φaug (xb,k, xa,k, uk), (A.2) which is equivalent to the dynamic parallel state augmenta-tion structure. Moreover, since the dynamic parallel struc-ture is a generalisation of the static parallel, selecting nxa = 0results in the static parallel state augmentation structure 

xb,k+1 = fbase (xb,k, uk) + φaug (xb,k, uk). (A.3) 

Series output augmentation at the state level: Choose nza =

2nxb +nxa +nu, and nwa = nxb +nxa . Take zb,k ≡ vec  xb,k, uk

,and za,k ≡ vec  xb,k, xa,k, uk, fbase (xb,k, uk). The former can be achieved by setting Cb 

> z

, Db 

> zu

as in (A.1), and Dbb 

> zw

, Dba  

> zw

as zeros. The latter can be realised by restricting 

Ca 

> z

=

h

I⊤  

> nxb+nxa

0⊤   

> (nu+nxb)×(nxb+nxa)

i⊤

, (A.4) 

Da 

> zu

=

h

0⊤   

> (nxb+nxa)×nu

I⊤ 

> nu

0⊤ 

> nxb×nu

i⊤

, (A.5) 

Dab  

> zw

=

h

0⊤  

> (nxb+nxa+nu)×nxb

In⊤

> xb

i⊤

, (A.6) and selecting Daa  

> zw

as a zero matrix. Then, by selecting A, Bu,and Bb 

> w

as zero matrices and setting Ba 

> w

an identity matrix, the resulting state transition function is ˆxk+1 = φaug (xb,k, xa,k, uk, fbase (xb,k, uk)) . (A.7) By universal approximation properties, there exist such weights and biases for φaug =

h

(φ baug )⊤ (φ aaug )⊤

i⊤

such that 

"xb,k+1

xa,k+1

#

=

"φ baug (xb,k, xa,k, uk, fbase (xb,k, uk)) 

φ aaug (xb,k, xa,k, uk)

#

, (A.8) which is equivalent to the dynamic series output state aug-mentation. Moreover, as we have shown previously, dy-namic augmentation is a generalisation of static augmen-tation; hence, the structure also represents the static series output augmentation form at the state level. 

Static series input augmentation at the state level: Choose 

nza = nxb + nu and nxa = 0. Take za,k ≡ vec  xb,k, uk,  by set-ting Ca 

> z

, Da

> zu

, Dab 

> zw

,Dbb 

> zw

, A, Bu, Bb 

> w

and Daa  

> zw

as for the parallel augmentation at the state level structure. Set Ba 

> w

≡ 0, Cb 

> z

≡ 0, 

Db 

> zu

≡ 0 and Dba  

> zw

as an identity matrix. This achieves 

zb,k = φaug (xb,k, uk), (A.9) 

xb,k+1 = fbase (zb,k), (A.10) which is equivalent to the static series input augmentation form at the state transition level. 

Dynamic series input augmentation on the state level: 

Choose nzb = nxb + nxa + nu. Take za,k ≡ vec  xb,k, xa,k, uk, 

by setting Ca 

> z

, Da

> zu

, Dab 

> zw

,Dbb 

> zw

, A, Bu and Daa  

> zw

as for the par-allel augmentation at the state level structure. Set Cb 

> z

≡ 0, 

Db 

> zu

≡ 0 and set Dba  

> zw

as an identity matrix. Consider the output of the learning component to be split into two parts 

φaug =

h

(φ baug )⊤ (φ aaug )⊤

i⊤

. This achieves 

"xb,k+1

xa,k+1

#

=

"xb,k+1 = fbase (φ baug (xb,k, xa,k, uk)) 

φ aaug (xb,k, xa,k, uk)

#

, (A.11) which is equivalent to the dynamic series input augmentation form at the state transition level. 

Output augmentation structures: The output augmentation formulations are similar in structure to the state augmenta-tion cases, but the connection of the baseline and learning components happens at the output level. Following the pre-vious arguments, it is straightforward to derive that all struc-tures in Table 2 can be represented by (6), similarly to the structures in Table 1, which concludes the proof. 16