Title: Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases

URL Source: https://arxiv.org/pdf/2602.17001v1

Published Time: Fri, 20 Feb 2026 01:19:20 GMT

Number of Pages: 22

Markdown Content:
# Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

Zhao Tan 1 2 Yiji Zhao 3 Shiyu Wang Chang Xu 4 Yuxuan Liang 5

Xiping Liu 2 Shirui Pan † 1 Ming Jin † 1

## Abstract 

Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS , a neuro-symbolic framework that tackles NLQ4TSDB via a “Search-Then-Verify” pipeline. Analogous to active sonar, it utilizes a feature index to “ping” candidate windows via SQL, fol-lowed by generated Python programs to “lock on” and verify candidates against raw signals. To en-able effective evaluation, we introduce NLQTS-Bench , the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experi-ments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where tradi-tional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a gen-eral framework and evaluation standard to facili-tate future research. 

## 1. Introduction 

The volume of time series data is increasing rapidly across domains such as IoT monitoring, financial trading, and AIOps. To handle this scale, specialized Time Series Databases (TSDBs) have become the standard solution for storage (Pelkonen et al., 2015). However, querying these massive records for meaningful insights remains a signifi-

> 1

Griffith University 2Jiangxi University of Finance and Eco-nomics 3Yunnan University 4Microsoft Research Asia 5The Hong Kong University of Science and Technology (Guangzhou). Corre-spondence to: Ming Jin <ming.jin@griffith.edu.au >, Shirui Pan 

<s.pan@griffith.edu.au >.NLQ SQL     

> Massive Time Series Database
> Multi -scale Feature Tables
> (Queryable Index)
> Candidates
> Verify
> SQL Search
> Python
> Verification
> Sonar -TS
> Text -to -SQL Time Series
> Model
> Failure:
> Limited
> Context
> NLQ SQL
> Failure:
> Morphology
> Gap

Figure 1. Comparison of querying paradigms. While Text-to-SQL fails to express morphological intents and Time Series Models are limited by context length, Sonar-TS adopts a “Search-Then-Verify” pipeline: it uses SQL to search a symbolic index for candidates and Python to verify them on raw data. 

cant barrier for non-expert users. Unlike simple numerical lookups (e.g., “maximum value in May”), users often pri-oritize morphological characteristics, such as identifying a specific day where data shows “a rapid rise followed by a slow fall.” The profound semantic gap between such abstract natural language descriptions and the continuous numerical data constitutes the fundamental challenge in TSDB querying. Existing attempts to bridge this gap can be broadly cat-egorized into two distinct paradigms. On the one hand, from a database-centric perspective, Text-to-SQL (Pourreza & Rafiei, 2023; Tan et al., 2024; Liu et al., 2025a) aims to translate natural language into executable SQL queries. While this field has demonstrated significant success in han-dling complex schema linking within relational databases, it encounters an expressivity bottleneck in the time series domain: standard SQL lacks native primitives to describe continuous morphological concepts (e.g., shapes or trends), making it arduous to formulate qualitative intents using rigid operators. On the other hand, from a data-centric perspec-tive, Time Series Question Answering (TSQA) (Jin et al., 2024; Langer et al., 2025; Divo et al., 2025) focuses on aligning textual modalities with raw temporal signals, en-1

> arXiv:2602.17001v1 [cs.AI] 19 Feb 2026 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases

abling models to interpret morphological patterns directly. However, these models face a severe scalability bottleneck. Constrained by finite context windows, end-to-end models cannot ingest the ultra-long-horizon histories (often millions of points) stored in real-world TSDBs. To overcome these limitations, we formally define the task of Natural Language Querying for Time Series Databases (NLQ4TSDB). Unlike standard TSQA which typically op-erates on short context windows, this task demands that a system ground high-level semantic intents into executable operations over massive, unsegmented temporal records. To facilitate rigorous evaluation, we introduce NLQTSBench, a comprehensive benchmark featuring a hierarchical taxon-omy spanning four levels of complexity: Level–1 tests basic numerical retrieval and windowing; Level–2 focuses on morphological pattern recognition (e.g., shapelets identifica-tion); Level–3 evaluates semantic reasoning (e.g., composite trends and causal anomalies); and Level–4 requires holistic insight synthesis for complex report generation. In this work, we propose Sonar-TS , a neuro-symbolic framework tailored to address the unique challenges of NLQ4TSDB. As illustrated in Figure 1, Sonar-TS reframes the TSDB querying process as a “Search-Then-Verify” pipeline, drawing an analogy to active sonar. Instead of scanning the entire raw history (which is computationally prohibitive) or relying solely on SQL (which lacks mor-phological expressivity), our system first “pings” the mas-sive search space using a multi-scale feature index via SQL queries to localize candidate windows. Subsequently, it “locks on” to these candidates using generated Python pro-grams to verify the raw signals against the user’s semantic intent. The framework is structured into three key mod-ules: (1) Offline Data Processing , which constructs com-pact feature tables (e.g., SAX) to render continuous shapes queryable; (2) Online Querying , where an LLM-driven planner synthesizes hybrid SQL-Python execution plans; and (3) Post-processing , which formats execution artifacts into user-friendly insights and visualizations. Our contributions are summarized as follows: • New Problem: We formally define the NLQ4TSDB task, highlighting its necessity in practice, the dual chal-lenges of semantic grounding, and scalability that dif-ferentiate it from traditional Text-to-SQL and TSQA. • New Benchmark: We introduce NLQTSBench, the first large-scale benchmark for complex time series question-answering on TSDBs. Distinguished from existing settings that are restricted to short context win-dows, it necessitates active evidence localization over long-horizons rather than passive context processing. • Novel Framework: We propose Sonar-TS , a frame-work that orchestrates a “Search-Then-Verify” work-flow. Our experiments demonstrate that Sonar-TS ef-fectively manages complex temporal queries where traditional methods struggle, laying a strong ground-work for future research in DB-grounded time series analysis. 

## 2. Problem Formulation 

The goal of NLQ4TSDB is to retrieve insights from a Time Series Database (TSDB) via natural language. For-mally, let D denote the TSDB instance containing massive timestamped records. Its structure is defined by a schema 

{Mi}ni=1 , consisting of n measurements (e.g., tables). We define each measurement as a tuple: 

Mi = ( ni, Ti, Fi), (1) where ni is the measurement name, Ti is a set of tags (cate-gorical), and Fi is a set of fields (numerical). 

Input. The input consists of two components: (1) Textual Schema (S), a serialized token sequence of the metadata 

{Mi}ni=1 , formally defined as S = Serialize( {Mi}ni=1 ).This provides structural context without exposing the raw data in D. (2) Natural Language Query (Q), a sentence spec-ifying the user intent, typically targeting temporal patterns rather than simple relational lookups. 

Objective. We formulate the task as a two-stage process. A solver f first synthesizes an executable query plan π (e.g., SQL or Python code) based on the schema, which is then executed against the instance D to yield the answer A:

π = f (Q, S), A = Exec( π, D). (2) 

Output. The answer A can be a scalar, timestamps, time intervals, or descriptive text, depending on the query intent. 

## 3. NLQTSBench 

Existing TSQA benchmarks (Kong et al., 2025; Chen et al., 2025) typically restrict evaluation to short snippets (often fewer than 500 points), a setting that fails to reflect the complexity of practical TSDB querying. In contrast, real-world TSDB interactions operate on massive, continuously accumulated histories (Naqvi et al., 2017). Even when users specify a time range, the requested window is often sufficiently long to impose significant challenges in both computation and semantic grounding. To bridge the gap, we first introduce NLQTSBench , a benchmark tailored for nat-ural language interaction with TSDBs. In NLQTSBench ,global time series comprise over 170,000 points, and individ-ual queries typically involve processing windows averaging approximately 11,000 points. Crucially, this scale is not just about “more data points.” This magnitude enforces a paradigm shift from passive 2Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases Level 1: Basic Operations    

> [Question]: What is the median value of channel 1 in
> 2021?
> [Answer]: 0.479
> [Question]: Which 10 -day period in 2022 had the highest
> average for channel 2?
> [Answer]: [2022 -02 -18 17:15:00 to 2022 -02 -28 17:00:00]

Level 2: Pattern Recognition 

Sliding Window Atomic Retireval      

> [Question]: Find the interval in channel 5 within
> [Search Range] most similar to the pattern in
> [Ref Range] .
> [Answer]: [2021 -07 -27 12:45:00 to 2021 -07 -28
> 03:30:00]
> Search Range Ref Range
> Basic
> Operations
> 23%
> Pattern
> Recongnition
> 29%
> Semantic
> Reasoning
> 36%
> Insight
> Synthesis
> 12%
> [Question]: Identify the time range of the longest
> plateau (stable period) in channel 3 within [2022 -
> 09 -11 08:00:00 to 2022 -09 -20 12:00:00].
> [Answer]: [2022 -09 -11 19:15:00 to 2022 -09 -13
> 04:00:00]

Shape Indentification Periodicity Detection   

> [Question]: What is the dominant cycle period
> (in data points) of channel 4 within [2023 -11 -05
> 09:15:00 to 2023 -11 -08 11:45:00]?
> [Answer]: 90

Subsequence Matching 

Level 3: Semantic Reasoning     

> [Question]: Channel 8 is the upstream source of
> channel 9, identify the time period in 2019 where
> where 9 shows a significant causal anomaly.
> [Answer]: [2021 -07 -27 12:45:00 to 2021 -07 -28
> 03:30:00]
> [Question]: Identify the top -10 dates in channel
> 6 during 2023 that exhibit the most significant
> 'rapid fall followed by a stable plateau' trend.
> [Answer]: ['2023 -06 -21’, ..., '2023 -02 -05']

Composite Trend Contextual Anomaly     

> [Question]: Identify the period in channel 7
> during 2023 that experienced historically high
> water levels.
> [Answer]: [2023 -09 -09 to 2023 -11 -10]

Causal Anomaly 

Level 4: 

Insight Synthesis           

> [Question]:
> Analyze the behavior of channel 9
> for the period 2020 -07.
> Please use ONLY the following
> phrases for trend description: rapid
> rise, gradual rise, rapid fall, gradual
> fall, steady stable, fluctuating stable.
> Provide a structured report
> covering:
> 1. Trend Segmentation: Describe
> each stage with precise
> start/end timestamps using the
> phrases above.
> 2. Outlier Audit: Identify only
> significant outliers that deviate
> sharply from the local trend.
> [Answer]:
> 1. Trend Segmentation: from
> 2020 -07 -01 00:00:00 to 2020 -
> 07 -14 06:45:00, the trend
> showed a steady stable; from
> 2020 -07 -14 06:45:00 to 2020 -
> 07 -20 22:00:00, the trend
> showed a fluctuating stable;
> from 2020 -07 -20 22:00:00 to
> 2020 -07 -25 08:15:00, the trend
> showed a gradual rise; from
> 2020 -07 -25 08:15:00 to 2020 -
> 07 -31 23:45:00, the trend
> showed a fluctuating stable.
> 2. Outlier Audit: A significant
> spike was detected at 2020 -07 -
> 08 00:45:00 (value: 13.61).

Figure 2. The hierarchical taxonomy of tasks in NLQTSBench. The benchmark ranges from Level 1 (Basic Operations) which tests numerical filtering, to Level 2 (Pattern Recognition) for morphological grounding, Level 3 (Semantic Reasoning) for logical composition, and finally Level 4 (Insight Synthesis) for narrative reporting. 

context processing to active database evidence localiza-tion. Since preprocessing such as downsampling or slicing can distort micro-signals or break temporal dependencies, NLQTSBench compels the solver to grapple with the uncut, original texture of the data, pinpointing precise evidence within massive, noisy histories. 

3.1. Task Taxonomy 

We structure NLQTSBench into a four-level taxonomy, il-lustrated in Figure 2. This hierarchy systematically evalu-ates a solver’s ability to ground abstract linguistic concepts into continuous time series data at scale, progressing from atomic operations to holistic reasoning. 

Level 1: Basic Operations. This level evaluates the solver’s precision in translating natural language constraints into ex-ecutable numerical filters. It includes: (1) Atomic Retrieval ,involving statistical calculation, precise localization, and value-based range selection; and (2) Sliding Window , a task that identifies target intervals across a massive search space. 

Level 2: Pattern Recognition. This level assesses the ability to ground abstract morphological concepts (e.g., V-shape) into continuous numerical series. It includes: (1) 

Shape Identification , recognizing qualitative patterns; (2) 

Periodicity Detection , capturing dominant cycles in oscillat-ing data; and (3) Subsequence Matching , retrieving periods that morphologically resemble a reference pattern. 

Level 3: Semantic Reasoning. This level evaluates the capacity for logical composition and relational reasoning in-volving temporal sequences and inter-channel dependencies. It includes: (1) Composite Trend , handling multi-stage tran-sitions; (2) Contextual Anomaly , diagnosing abnormalities relative to local history; and (3) Causal Anomaly , identify-ing contradictions between correlated series (e.g., source rises while target flatlines). 

Level 4: Insight Synthesis. The final level evaluates the ability to synthesize holistic insights from multi-regime con-texts. It focuses exclusively on Report Generation , requiring the solver to autonomously perform comprehensive Trend Segmentation and Outlier Auditing (as shown in Figure 2) to synthesize a structured intelligence report. 

3.2. Data Construction Pipeline 

Building a benchmark for NLQ over massive TSDBs re-quires balancing realism with verifiable ground truth. Purely synthetic series lack the irregular noise and drift observed in real deployments, while manual annotation on long streams is costly and prone to subjectivity due to fuzzy semantic boundaries (e.g., where exactly a “gradual rise” begins). To 3Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

address this, we adopt a controlled injection strategy on top of the real-world CausalRivers dataset (Stein et al., 2025), making labels auditable by construction while preserving the complexity of real backgrounds. We follow a three-stage pipeline: (1) Template formula-tion , instantiating parameterized task schemas into concrete constraints (e.g., window length, pattern family, and cross-channel relations); (2) Data synthesis , sampling real back-ground windows and injecting mathematically specified patterns, with injection magnitude calibrated to local vari-ability (e.g., by local dispersion statistics) to be salient yet in-distribution; and (3) Human visual verification , where experts inspect rendered charts to ensure the evidence is dis-tinguishable and physically plausible, discarding ambiguous or artifact-prone cases. More details are in Appendix A 

3.3. Benchmark Variants. 

The complete NLQTSBench contains 831 validated queries. Each instance is paired with an executable evaluation pro-gram and deterministic metrics (e.g., interval IoU and Set-F1; see Section 5 for details). We additionally release 

NLQTSBench-Lite with fixed 512-point windows as a compatibility set for evaluating those context-limited time series models such as Time-LLM (Jin et al., 2024) and ChatTS (Xie et al., 2025), covering Shape Identification, Composite Trend, and Causal Anomaly. 

## 4. The Sonar-TS Framework 

4.1. Overview 

As formulated in Section 2, NLQ4TSDB involves creating an executable plan π from a natural language query Q and a textual schema S, to derive the answer by executing π

on the TSDB instance D. Although formally similar to Text-to-SQL, NLQ4TSDB is substantially harder for two reasons: (1) Queries are often morphology-driven, requiring compositions of time series operators to interpret concepts like “volatile drop”; and (2) answering them necessitates long-horizon evidence localization, i.e., searching lengthy histories for sparse, relevant segments. This combination of deep temporal reasoning and large-scale search exceeds the capabilities of most time series language models (Jin et al., 2024; Xie et al., 2025; Langer et al., 2025) that directly read raw streams through a fixed-length context window. To close the gap, we propose Sonar-TS , a neuro-symbolic framework that reframes querying as a Search-Then-Verify 

pipeline. We name the framework Sonar-TS by drawing an analogy to active sonar : We first “ping” a lightweight multi-scale feature index to retrieve candidate “echoes” (time windows), and then “lock on” by executing programs on raw data to verify the target. As shown in Figure 3, the system makes NLQ4TSDB tractable by organizing the workflow into three functional stages: • Offline Data Processing. We preprocess native time-series data into multi-scale feature tables. Each win-dow is represented by identifiers and window metadata, statistical summaries, and a symbolic shape signature for pattern-oriented retrieval. • Online Querying. Given a query Q and a textual schema S, Sonar-TS performs task planning and gener-ates hybrid SQL and Python programs. The generated SQL first queries the feature tables to obtain candidate windows, and then zooms into selected windows by fetching raw slices and executing Python operators to compute task-specific metrics and checks. Addition-ally, we incorporate a prompt cold-start initialization and continuously improve the query process through experience summarization and updates. • Post-processing. Finally, we format executable results into canonical outputs (e.g., scalars, timestamps, time windows, or structured reports), along with visualiza-tions and summaries for inspection. 

4.2. Offline Data Processing 

To tackle the computational challenges associated with querying massive high-frequency time series records, we introduce an offline indexing mechanism. Instead of per-forming costly full scans during queries, we precompute compact multi-scale Feature Tables . These tables act as a 

Queryable Semantic Index , summarizing raw signals into window-level descriptors that facilitate rapid, approximate evidence localization. Importantly, the raw TSDB remains the definitive source of truth, accessed subsequently only for exact computations on localized regions. Concretely, for each numeric channel, we materialize fea-ture rows using a hierarchical windowing scheme across multiple granularities (e.g., Year/Month/Day) with con-figurable phase offsets. Each row corresponds to a spe-cific time window w = [ ts, t e) and stores metadata (e.g., Channel ID) alongside two categories of descriptors: (1) 

Statistical Primitives: We compute lightweight statis-tics (e.g., slope , std val ) to provide cheap, high-recall pruning signals. This enables the system to prior-itize candidate windows using simple SQL (e.g., ORDER BY std val DESC LIMIT 1 ) rather than repeatedly aggregating over raw points. (2) Morphological To-kens: To accommodate shape-driven intents (e.g., “V-shape”) that scalar values cannot capture, we generate Symbolic Aggregate approXimation (SAX) (Lin et al., 2007) strings as discrete signatures. We treat SAX as a Symbolic Search Handle , allowing users to approxi-mate shapes via standard SQL regex (e.g., approximat-4Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases Sax_Shape ... slope end start Channel 

> acead... ... -0.36 2020 -01 2019 -01 channel_1
> ... ... ... ... ... ...
> Sax_Shape ... slope end start Channel
> acead... ... -0.36 2020 -01 2019 -01 channel_1
> ... ... ... ... ... ...
> Visualization

Question 

Offline Data Processing  

> Native Time Series Data Feature Tables

... 

Online Querying Post -processing    

> Database Schema
> # Table: data [channel_1, ...]
> # Table: feature_daily [...]
> # Table: feature_monthly [...]
> ...
> Question
> Find dates in channel 1 during
> 2019 daily trend: first rapid
> fall, then volatile rise, then
> steady, then gradual fall.
> Code
> Generation
> Task
> Planning
> Experiences
> Execute
> Results
> lTime Window
> lTimestamp
> lScalar
> Sub -Steps
> # Stpe 1
> # Step 2
> ...
> Reward
> Model
> Experiences
> Summarize
> Experiences
> Update
> Trajectory
> Prompt Cold Start

Answer    

> Question
> Sub -Steps Results
> SAX ... slope end start Channel
> acead... ... -0.36 2020 -01 2019 -01 channel_1
> ... ... ... ... ... channel_2
> Yearly
> Monthly
> Daily
> Tags Statistical Pattern

Time Series 

Database   

> SQL Python
> Operators
> Query
> Tasks
> Comprehensive
> Reporting
> 1. For simple shape detection,
> use SAX Regex, and...
> ...
> Answer Summary

[...,'2019 -04 -28',...]  

> Search Verify

Figure 3. The overview of the Sonar-TS framework. The workflow is organized into three stages: (1) Offline Data Processing constructs compact multi-scale Feature Tables to serve as a queryable index; (2) Online Querying , where the Task Planner and Code Generator synthesize SQL for rapid candidate search and Python for exact verification, supported by a closed-loop Prompt Cold Start mechanism that evolves analysis insights; and (3) Post-processing translates execution artifacts into a user-friendly interface. 

ing a “monotonic rise” via WHERE regexp like(sax, ’[ab]+.*[de]+’ )). While currently using SAX for its SQL compatibility, our framework is agnostic to the specific tokenization method. By exposing statistical and morphological properties as structured columns, the multi-scale feature tables bridge the semantic gap between user intents and raw data, making subsequent online querying computationally tractable. 

4.3. Online Querying 

Querying over TSDBs faces two core challenges: (1) The Semantic-Signal Mismatch : Users’ intents often describe high-level morphology (e.g., “steady → sharp drop”) but databases only index low-level raw signals. Expressing such composite intents as a single SQL predicate is often impos-sible. (2) Computational Infeasibility : Verifying these intents directly on raw data requires full-history scanning, which is prohibitively expensive for online interaction. To resolve these issues, Sonar-TS adopts a Search-Then-Verify workflow that orchestrates collaboration between coarse-grained database search and fine-grained algorithmic verification. 4.3.1. S EARCH -T HEN -V ERIFY WORKFLOW 

The pipeline processes the input query Q through three distinct phases, utilizing the database schema (including offline feature tables) and a set of Experiences (a concise summary of high-level past guidelines; Section 4.3.2) as inputs. 

Step 1: Task Planning. Complex temporal queries are difficult to execute in a single step. We employ a backbone LLM as a Task Planner to decompose Q into a sequence of sub-steps. This step determines the logical execution order and selects the necessary operator types for the subsequent generation. 

Step 2: Code Generation (Search-Then-Verify). Based on the plan, the Code Generator instantiates the ab-stract logic into executable forms via two distinct out-puts: (1) SQL-based Search: Targeting the TSDB, the system serves as a High-Recall Filter, generat-ing SQL to narrow the search space using SAX to-kens as symbolic handles. Consider a composite trend like “first rapidly rise, then falls”; the system formu-lates a fuzzy regex (e.g., WHERE regexp like(sax, ’[ab]+.*[de]+.*[ab]+’) ) to retrieve the approxi-mate trajectory. This strategy efficiently eliminates struc-turally irrelevant windows, yielding a set of candidate meta-data. (2) Operator-based Verification: For the localized candidates, the system synthesizes executable programs to perform execution-grounded validation and computation. The generation is guided by a set of canonical domain primi-tives, which serve as flexible algorithmic specifications (e.g., the CHANGEPOINT segmentation logic). This ensures that fuzzy linguistic concepts are effectively grounded in precise mathematical operations. 5Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

Step 3: Execution & Self-Correction. The generated SQL and Python are executed sequentially. This stage han-dles runtime failures (e.g., SQL returning empty results or Python syntax errors). If an error occurs, execution feed-back (a traceback or a result summary) is returned to the Code Generator for refinement. The system allows up to 3 retries to adjust query logic or program content before returning the final result. 4.3.2. P ROMPT COLD START 

NLQ4TSDB is knowledge-intensive, requiring domain heuristics beyond standard schemas and generic Text-to-SQL skills. To inject such expertise without heavyweight fine-tuning, Sonar-TS maintains Experiences (compact, high-level insights distilled from past executions). These guidelines instruct the LLM on correct operator usage, effec-tive utilization of feature tables (e.g., SAX matching), and the avoidance of common pitfalls (e.g., irregular sampling gaps). During inference, Experiences are directly injected into the Planner and Generator prompts, acting as a dynamic “expert manual.” The Experiences evolve through a closed-loop Reward-Summarize-Update process (Cai et al., 2025). First, after execution, a deterministic script assigns a soft score based on answer type, employing Intersection over Union (IoU) for time windows, relative precision for timestamps/scalars, and F1-score for sets. Subsequently, an Experience Summa-rizer (LLM) analyzes the full execution trajectory (including plan, code, and reward) to distill 1–3 generalizable insights. Finally, an Experience Updater (LLM) refines the global Experience set via semantic operations (add/merge/delete), enforcing a strict size limit (e.g., < 20 insights) to prevent context overflow. Overall, this mechanism keeps Experiences compact and non-redundant while progressively accumulating domain know-how, improving robustness without expensive model fine-tuning. 

4.4. Post-processing 

The Online Querying module returns raw mathematical primitives (timestamps, time windows, and scalars) that are not always suitable for direct user presentation. The Post-processing module converts these artifacts into user-facing outputs, and is required for text-centric tasks such as REPORT GENERATION .Given the original Question , the planned Sub-Steps , and the execution Results , Post-processing decides whether a final textual response is needed. If the task calls for nar-rative synthesis, it invokes an LLM to compose a concise answer by integrating the retrieved values into a coherent de-scription aligned with the user’s intent. Otherwise, it skips summarization and proceeds directly to visualization. In addition, Post-processing optionally provides alightweight visualization for human inspection. A Visu-alization Agent (LLM) generates a Python plotting script that renders the relevant time-series context and explicitly highlights the retrieved results. This component is intended as a user-friendly interface for quick verification. 

4.5. Implementation Details 

Sonar-TS is a training-free framework utilizing DeepSeek-V3 as the backbone for both task planning and code gen-eration. To enhance robustness, we incorporate a runtime self-correction mechanism allowing up to 3 retries upon fail-ure. The current implementation targets SQL-compatible TSDBs (e.g., InfluxDB). We construct multi-scale feature tables (Year/Month/Day) via sliding windows. Each entry encodes statistical primi-tives (e.g., slope) and a morphological SAX signature. We set the SAX alphabet size to α = 5 and employ hierarchical segmentation widths (e.g., w = 24 for daily windows) to align symbolic tokens with temporal semantics. Compre-hensive prompt templates, the full operator library, and addi-tional implementation details are provided in Appendix B. 

## 5. Experiments 

5.1. Experimental Setup Datasets. We evaluate Sonar-TS on NLQTSBench, com-prising 831 queries over long-horizon histories. Due to the context window limitations of end-to-end time series mod-els, we perform a dual-track evaluation: (1) NLQTSBench-Lite: A subset with cropped 512-point windows to assess the short-term pattern recognition capabilities of time series model baselines. (2) NLQTSBench (Default): The com-plete benchmark containing massive histories to evaluate the retrieval scalability of query-based baselines. 

Baselines. We compare Sonar-TS against two categories of methods. First, for time series models evaluated on the Lite setting, we select ChatTS (Xie et al., 2025), a multi-modal framework aligning signals with text features; IT-Former (Wang et al., 2025b), a Transformer-based encoder for temporal modeling; and Time-R1 (Luo et al., 2025), a reasoning-enhanced time series model. Second, for Text-to-SQL methods evaluated on the default benchmark, we com-pare with MAC-SQL (Wang et al., 2025a), a classic multi-agent decomposition framework. We also include Xiyan-SQL (Liu et al., 2025b) and Omini-SQL (Li et al., 2025), representing leading end-to-end SQL generation models. 

Evaluation Metrics. As our benchmark involves diverse output formats, we employ tailored quantitative metrics. We calculate the Intersection over Union (IoU) for time in-6Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases                                                                                                                    

> Table 1. Main experimental results on NLQTSBench. Abbreviations: AR (Atomic Retrieval), SW (Sliding Window), SI (Shape Ident.),
> PD (Periodicity Det.), SM (Subseq. Matching), CT (Composite Trend), CxA (Contextual Anomaly), CsA (Causal Anomaly), IS (Insight Synthesis). Best results are bolded , and the best baseline results are underlined.
> Category Method L1: Basic Ops L2: Pattern Rec. L3: Semantic Reasoning L4 Avg. AR SW SI PD SM CT CxA CsA IS
> Performance on NLQTSBench-Lite (Short Context)
> Time Series Models ChatTS-14B --0.1768 --0.2431 -0.1229 -0.1818 ITFormer-7B --0.0736 --0.1500 -0.1953 -0.1529 Time-R1 --0.0320 --0.1395 -0.1878 -0.1374
> Ours Sonar-TS --0.2491 --0.2680 -0.3615 -0.3016
> Performance on NLQTSBench (Long History)
> Text-to-SQL Methods MAC-SQL 0.4735 0.0457 0.0419 0.3928 0.0598 0.0020 0.0346 0.0152 -0.1562 Xiyan-SQL-32B 0.1082 00.0588 0.2263 0.0068 0.0021 0.0168 0.0020 -0.0526 Omini-SQL-32B 0.4245 0.0086 0.0185 000.0038 0.0154 0-0.0589
> Ours Sonar-TS 0.8489 0.7417 0.3169 0.8529 0.9535 0.3033 0.5130 0.3422 0.7482 0.6168

tervals, measure accuracy for scalars and timestamps, and report the F1-score for sets of dates. Specifically, for insight synthesis tasks, we enforce strict output formatting to en-able precise component parsing and evaluation. Detailed configurations are provided in Appendix A.3. 

5.2. Main Results 

Table 1 presents the comparative performance on NLQTS-Bench. Overall, Sonar-TS achieves consistent improve-ments across most metrics. The results highlight that NLQ4TSDB imposes a unique challenge: it requires both precise signal grounding and complex logical reasoning, a combination that neither current time series models nor Text-to-SQL methods address. 

Performance on NLQTSBench-Lite. Time series models perform poorly, even on tasks they are expected to solve well, such as Shape Identification (SI). Interestingly, they often score lower on simple SI tasks than on Composite Trend (CT). Qualitative analysis (see Section 5.4) suggests that while these models can recognize general shapes, they struggle with strict semantic filtering. For instance, given a query to “Identify the longest plateau”, the model effec-tively locates a plateau but ignores the “longest” constraint. However, Sonar-TS shows reduced accuracy on morphology tasks (SI, CT) under the short-context setting relative to the long-history benchmark. A likely cause is the information loss introduced by SAX compression, which is sufficient for coarse trends but may discard fine local details in short windows. 

Performance on NLQTSBench (Default). Within the Text-to-SQL category, the training-free MAC-SQL outperforms fine-tuned SOTA models like Xiyan-SQL and Omini-SQL. This trend runs contrary to general benchmarks such as BIRD (Li et al., 2023). We attribute this divergence to a fundamental paradigm gap: standard relational query-ing focuses on discrete record filtering, whereas time se-ries analysis demands reasoning over continuous tempo-ral. Consequently, SQL baselines struggle significantly on morphology-dependent tasks (e.g., SI, CT). For instance, to identify a “slow rise followed by rapid ascent”, models typi-cally generate static range constraints (e.g., WHERE value > threshold ), which fundamentally fail to capture the dynamic rate of change. 

5.3. Ablation Study 

Table 2 validates the contribution of each component. First, removing the Feature Table severely impacts morphology-driven tasks (e.g., CT drops 0.30 → 0.04). This confirms that the symbolic index serves as the primary discrimina-tor for shape retrieval, without which Sonar-TS struggles to localize target patterns. Second, removing Experiences leads to broad degradation, particularly on tasks requiring domain-specific reasoning, such as Contextual Anomaly. The module bridges the gap between generic LLM logic and domain heuristics (e.g., comparing against historical baselines), guiding the planner to mimic expert workflows. Finally, removing the Verification stage causes the most severe collapse (Avg. 0.62 → 0.25), especially on algorith-mic tasks like PD and IS. This indicates that verification is not merely a filter but a necessary computational engine for tasks that rely on rigorous algorithms (e.g., change-point detection) that are inexpressible in SQL. 

5.4. Case Study 

Figure 4 presents a qualitative analysis of the Shape Iden-tification task, specifically targeting the “longest plateau”. Text-to-SQL baselines struggle to express continuous mor-phological traits, often hallucinating rigid numerical filters 7Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

Table 2. Ablation study on component contributions. We analyze the impact of removing the Feature Table, Experiences, and the Verification phase. Abbreviations follow Table 1. Cells highlighted in blue indicate tasks most significantly impacted by the ablation. 

Variant L1: Basic L2: Pattern L3: Reasoning L4 Avg. AR SW SI PD SM CT CxA CsA IS Sonar-TS 0.8489 0.7417 0.3169 0.8529 0.9535 0.3033 0.5130 0.3422 0.7482 0.6168 

w/o Feature Tables 0.8658 0.6923 0.1561 0.8804 0.9309 0.0422 0.4968 0.3918 0.7163 0.5572 w/o Experiences 0.8426 0.5591 0.1186 0.8311 0.9138 0.0677 0.2501 0.3612 0.3415 0.4592 w/o Verify 0.8294 0.1870 0.2992 0.0154 0.0396 0.2826 0.0818 0.0592 0.0261 0.2528 Query:                                    

> Identify the index range of the
> longest plateau (stable period)
> in channel 94. (Output format:
> [start_index, end_index]).
> SELECT MIN( timestamp )AS start ,
> MAX( timestamp )AS end
> FROM (SELECT timestamp
> FROM l2_t1_data WHERE 94 =
> (SELECT 94 FROM l2_t1_data
> WHERE timestamp
> BETWEEN '0' AND '512'
> GROUP BY 94
> ORDER BY COUNT( 94 )
> DESC LIMIT 1)
> AND timestamp
> BETWEEN '0' AND '512' )
> Time Series
> Model
> （ChatTS ）
> Text -to -SQL
> (Xiyan -SQL)
> Sonar -TS
> Verify
> Search
> SELECT window_start, window_end
> FROM feature_table
> WHERE regexp_like(sax,
> '[a -b]+[c]*[d -e]{2,}[c]*[a -b]+' )
> [380, 500] Answer
> SQL
> def verify_longest(data)

Figure 4. Case Study. Text-to-SQL (Left) lacks morphological expressivity, and TS Models (Right) fail the logical constraint. Sonar-TS (Middle) succeeds via Search-Then-Verify. 

that fail to capture the geometric pattern. Conversely, time series models correctly recognize typical plateau shapes within short contexts but fail to align with the “longest” intent, lacking the global reasoning to compare durations. Sonar-TS bridges this gap by first retrieving candidates via fuzzy SAX matching and subsequently employing Python verification to rigorously enforce the constraint, ensuring precise grounding where pure model-based approaches fail. 

## 6. Related Work 

NLQ4TSDB is related to (1) Text-to-SQL and (2) TSQA. 

Text-to-SQL. Text-to-SQL translates natural language ques-tions into executable SQL queries (Li et al., 2023; Qin et al., 2022), evolving from rule-based matching to LLM-driven agentic pipelines. To address the dual challenges of mas-sive schemas and complex reasoning, recent state-of-the-art methods employ decomposition and collaborative agents. For instance, DIN-SQL (Pourreza & Rafiei, 2023) and MAC-SQL (Wang et al., 2025a) break down Text-to-SQL into sub-problems such as schema linking and logic refinement, while CHASE-SQL (Pourreza et al., 2025) leverages test-time computation to iteratively generate and verify candidate queries. Similarly, CHESS (Talaei et al., 2024) introduces hierarchical retrieval to filter irrelevant schema items in industrial-scale databases. However, standard Text-to-SQL is fundamentally centered on relational algebra. It lacks native primitives for continuous morphological matching, such as shape or trend detection. Consequently, expressing concepts like “V-shaped ” or “fluctuation” remains a seman-tic gap, necessitating a neuro-symbolic extension beyond plain SQL generation. 

Time Series Question Answering. TSQA aims to endow models with the ability to perceive and reason over numeri-cal signals, shifting the focus from forecasting to semantic understanding (Chang et al., 2025). Pioneering approaches like Time-LLM (Jin et al., 2024) reprogram LLMs by trans-lating series into textual prototypes. More recently, native multimodal frameworks such as ChatTS (Xie et al., 2025) align continuous signals with linguistic concepts via context-aware encoders. Despite the expansion of reasoning tasks through benchmarks like Time-MQA (Kong et al., 2025) and QuAnTS (Divo et al., 2025), current TSQA models are constrained by the context window of Transformers. This bottleneck renders them computationally prohibitive for scanning massive, unsegmented histories in industrial TS-DBs. Applying TSQA in this setting necessitates an explicit Search-Then-Verify mechanism to localize sparse evidence from long horizons before reasoning. 

## 7. Conclusion 

In this paper, we formally define the NLQ4TSDB task. To facilitate evaluation, we introduced NLQTSBench , a hierar-chical benchmark that necessitates reasoning over year-scale histories. To address this challenge, we proposed Sonar-TS ,a neuro-symbolic framework implementing a “Search-Then-Verify” pipeline. By orchestrating coarse-grained symbolic search and fine-grained algorithmic verification, the system effectively bridges the gap between abstract user intents and raw numerical data. Our experiments demonstrate that Sonar-TS successfully handles complex temporal reasoning tasks where traditional paradigms fail. This work serves as a foundational step toward DB-grounded time series in-telligence, offering a robust baseline for future research in semantic indexing and intelligent data monitoring. 8Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

## Impact Statement 

This paper presents a framework for natural language query-ing of time series databases. The primary positive impact is lowering the technical barrier for analysts and operators to retrieve events, intervals, and summaries from TSDBs, which may improve decision-making in domains such as monitoring and operations. However, deploying such systems introduces potential risks. Privacy and confidentiality concerns may arise if the model is applied to sensitive industrial data. While we do not foresee immediate negative societal consequences, we en-courage practitioners to maintain strict access controls and validation protocols when applying this technology to sensi-tive or critical environments. 

## References 

Cai, Y., Cai, S., Shi, Y., Xu, Z., Chen, L., Qin, Y., Tan, X., Li, G., Li, Z., Lin, H., et al. Training-free group relative policy optimization. arXiv preprint arXiv:2510.08191 ,2025. Chang, C., Shi, Y., Cao, D., Yang, W., Hwang, J., Wang, H., Pang, J., Wang, W., Liu, Y., Peng, W.-C., et al. A sur-vey of reasoning and agentic systems in time series with large language models. arXiv preprint arXiv:2509.11575 ,2025. Chen, J., Feng, A., Zhao, Z., Garza, J., Nurbek, G., Qin, C., Maatouk, A., Tassiulas, L., Gao, Y., and Ying, R. Mtbench: A multimodal time series benchmark for tem-poral reasoning and question answering. arXiv preprint arXiv:2503.16858 , 2025. Divo, F., Kraus, M., Nguyen, A. Q., Xue, H., Razzak, I., Salim, F. D., Kersting, K., and Dhami, D. S. Quants: Question answering on time series. arXiv preprint arXiv:2511.05124 , 2025. Jin, M., Wang, S., Ma, L., Chu, Z., Zhang, J., Shi, X., Chen, P.-Y., Liang, Y., Li, Y.-F., Pan, S., and Wen, Q. Time-llm: Time series forecasting by reprogramming large language models. In International Conference on Learning Repre-sentations , volume 2024, pp. 23857–23880, 2024. Kong, Y., Yang, Y., Hwang, Y., Du, W., Zohren, S., Wang, Z., Jin, M., and Wen, Q. Time-MQA: Time series multi-task question answering with context enhancement. In 

Proceedings of the 63rd Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: Long Papers) , pp. 29736–29753, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1437. Langer, P., Kaar, T., Rosenblattl, M., Xu, M. A., Chow, W., Maritsch, M., Verma, A., Han, B., Kim, D. S., Chubb, H., et al. Opentslm: Time-series language models for reasoning over multivariate medical text-and time-series data. arXiv preprint arXiv:2510.02410 , 2025. Li, H., Wu, S., Zhang, X., Huang, X., Zhang, J., Jiang, F., Wang, S., Zhang, T., Chen, J., Shi, R., et al. Omnisql: Synthesizing high-quality text-to-sql data at scale. arXiv preprint arXiv:2503.02240 , 2025. Li, J., Hui, B., Qu, G., Yang, J., Li, B., Li, B., Wang, B., Qin, B., Geng, R., Huo, N., et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems , 36:42330–42357, 2023. Lin, J., Keogh, E., Wei, L., and Lonardi, S. Experiencing sax: a novel symbolic representation of time series. Data Mining and knowledge discovery , 15(2):107–144, 2007. Liu, X., Shen, S., Li, B., Ma, P., Jiang, R., Zhang, Y., Fan, J., Li, G., Tang, N., and Luo, Y. A survey of text-to-sql in the era of llms: Where are we, and where are we going? 

IEEE Transactions on Knowledge and Data Engineering ,2025a. Liu, Y., Zhu, Y., Gao, Y., Luo, Z., Li, X., Shi, X., Hong, Y., Gao, J., Li, Y., Ding, B., et al. Xiyan-sql: A novel multi-generator framework for text-to-sql. arXiv preprint arXiv:2507.04701 , 2025b. Luo, Y., Zhou, Y., Cheng, M., Wang, J., Wang, D., Pan, T., and Zhang, J. Time series forecasting as reasoning: A slow-thinking approach with reinforced llms. arXiv preprint arXiv:2506.10630 , 2025. Naqvi, S. N. Z., Yfantidou, S., and Zim ´anyi, E. Time series databases and influxdb. Studienarbeit, Universit ´e Libre de Bruxelles , 12:1–44, 2017. Pelkonen, T., Franklin, S., Teller, J., Cavallaro, P., Huang, Q., Meza, J., and Veeraraghavan, K. Gorilla: a fast, scalable, in-memory time series database. Proc. VLDB Endow. , 8(12):1816–1827, August 2015. ISSN 2150-8097. doi: 10.14778/2824032.2824078. Pourreza, M. and Rafiei, D. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Ad-vances in Neural Information Processing Systems , 36: 36339–36348, 2023. Pourreza, M., Li, H., Sun, R., Chung, Y., Talaei, S., Kakkar, G. T., Gan, Y., Saberi, A., Ozcan, F., and Arik, S. Chase-sql: Multi-path reasoning and preference optimized candi-date selection in text-to-sql. In International Conference on Learning Representations , volume 2025, pp. 60385– 60415, 2025. 9Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

Qin, B., Hui, B., Wang, L., Yang, M., Li, J., Li, B., Geng, R., Cao, R., Sun, J., Si, L., et al. A survey on text-to-sql parsing: Concepts, methods, and future directions. arXiv preprint arXiv:2208.13629 , 2022. Stein, G., Shadaydeh, M., Blunk, J., Penzel, N., and Den-zler, J. Causalrivers - scaling up benchmarking of causal discovery for real-world time-series. In International Conference on Learning Representations , volume 2025, pp. 64516–64531, 2025. Talaei, S., Pourreza, M., Chang, Y.-C., Mirhoseini, A., and Saberi, A. Chess: Contextual harnessing for efficient sql synthesis. arXiv preprint arXiv:2405.16755 , 2024. Tan, Z., Liu, X., Shu, Q., Li, X., Wan, C., Liu, D., Wan, Q., and Liao, G. Enhancing text-to-sql capabilities of large language models through tailored promptings. In Proceedings of the 2024 Joint International Confer-ence on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) , pp. 6091–6109, 2024. Wang, B., Ren, C., Yang, J., Liang, X., Bai, J., Chai, L., Yan, Z., Zhang, Q.-W., Yin, D., Sun, X., et al. Mac-sql: A multi-agent collaborative framework for text-to-sql. In Proceedings of the 31st International Conference on Computational Linguistics , pp. 540–557, 2025a. Wang, Y., Lei, P., Song, J., Hao, Y., Chen, T., Zhang, Y., Jia, L., Li, Y., and Wei, Z. Itformer: Bridging time series and natural language for multi-modal qa with large-scale multitask dataset. arXiv preprint arXiv:2506.20093 ,2025b. Xie, Z., Li, Z., He, X., Xu, L., Wen, X., Zhang, T., Chen, J., Shi, R., and Pei, D. Chatts: Aligning time series with llms via synthetic data for enhanced understanding and reasoning. Proc. VLDB Endow. , 18(8):2385–2398, April 2025. ISSN 2150-8097. doi: 10.14778/3742728. 3742735. 10 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

## A. Benchmark Details 

This appendix provides comprehensive specifications for the construction, distribution, and evaluation of NLQTSBench. We detail the query templates, data injection mechanisms, and the rigorous metric definitions used to benchmark solver performance. 

A.1. Dataset Statistics and Examples 

Table 3 summarizes the distribution of the 831 queries across the four capability levels. The dataset is balanced to ensure significant coverage of both morphological pattern recognition and logical reasoning tasks.                                

> Table 3. Statistics of NLQTSBench. The dataset comprises 831 queries spanning four complexity levels. Window Size denotes the typical length of the time series context provided to the solver.
> Level Task Sub-Type Count Avg. Length L1: Basic Operations
> Atomic Retrieval 144 ∼2k Sliding Window 47
> L1 Subtotal 191 -
> L2: Pattern Recognition
> Shape Identification 80
> ∼0.6k Periodicity Detection 80 Subsequence Matching 79
> L2 Subtotal 239 -
> L3: Semantic Reasoning
> Composite Trend 140
> ∼30k Contextual Anomaly 83 Causal Anomaly 78
> L3 Subtotal 301 -
> L4: Insight Report Generation 100 ∼3k
> Total All Tasks 831 Avg. ≈11.2k

A.2. Data Construction Details 

We employ a dual-strategy pipeline to construct NLQTSBench: extraction for basic operations (Level 1) to ensure fidelity to real-world raw data, and injection for advanced pattern tasks (Levels 2-4) to ensure controllable ground truth. A.2.1. P IPELINE OVERVIEW 

The data construction process is formally defined as a generative mapping G, which synthesizes a dataset instance from templates and background data: 

(Qtpl , Dbg , Θ) G

−→ (X, Q, A ) (3) where: Qtpl denotes the semantic query template with parameter slots. Dbg is the repository of real-world background time series. Θ represents the sampling parameters (e.g., window size, noise level). The outputs consist of the final time series X,the natural language query Q, and the ground truth answer A.To ensure diversity and rigorous quality, the pipeline executes in three sequential stages: 1. Template Formulation: We define semantic templates containing parameter slots (e.g., {window size },

{threshold }). These slots are dynamically filled using distribution-aware sampling (e.g., percentiles) to instantiate concrete query intents. 2. Data Synthesis: This is the core generation phase. Depending on the task level, G branches into two distinct strategies to produce (X, A ):• Strategy I: Direct Extraction (for Level 1) prioritizes fidelity to raw data. • Strategy II: Signal Injection (for Levels 2-4) prioritizes controllability via mathematical signal superimposition. 11 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

3. Human Visual Verification: Finally, synthesized samples undergo a rigorous audit. We apply automated SNR checks to ensure pattern distinctness, followed by expert human review to filter out ambiguous cases or artifacts. A.2.2. T EMPLATE FORMULATION 

In the first stage of the pipeline, we define semantic templates that encapsulate specific analytical intents. Each template T

consists of a fixed linguistic structure and variable parameter slots (denoted by brackets {... }). These slots are populated via distribution-aware sampling strategies (e.g., matching thresholds to data percentiles) to ensure that the generated queries are both linguistically natural and mathematically solvable. Below, we present the instantiated templates for each complexity level. 

Level 1: Basic Operations This level evaluates the solver’s precision in translating natural language constraints into executable numerical filters. We categorize these tasks into two primary types: Atomic Retrieval , which involves precise calculations or localization based on strict conditions, and Sliding Window , which requires searching for statistical optima across moving contexts. 

## Level 1: Basic Operations Templates 

Type 1: Atomic Retrieval 

• Global Aggregation Template: What is the {agg } value of channel {channel } in {time }?

Args: {agg } ∈ [”maximum”, ”minimum”, ”average”, ”median”, ”range”] 

• Temporal Localization Template: At what exact timestamp did channel {channel } { action } in {time }?

Args: {action } ∈ [”reach its maximum value”, ”first rise above {threshold }”, ...] 

• Interval Discovery Template: Find the longest period where channel {channel } remained above {threshold } in {time }.

Args: {threshold } → P80 (Dynamic 80th percentile of data) 

Type 2: Rolling Window Analysis 

• Sliding Window Statistics Template: Which {window desc } in {time } had the {metric } for channel {channel }?

Args: {metric } ∈ [”highest/lowest average”, ”highest variance”, ”largest range”];   

> {window }→Random Integer [3D, 60D]

Level 2: Pattern Recognition While Level 1 focuses on explicit numerical filters, Level 2 evaluates the model’s ability to perceive morphological and structural characteristics of time series. These tasks require the solver to bridge the semantic gap between visual concepts (e.g., ”plateau”, ”spike”) and raw signal fluctuations, as well as to perform complex comparisons like periodicity detection and subsequence similarity search. 

## Level 2: Pattern Recognition Templates 

• Shape Identification Template: Identify the time range of the {superlative } { pattern name } in channel {channel } within 

{time window }.

Args: {pattern name } ∈ [”plateau”, ”upward spike”, ”deep valley”, ”step ascent/descent”];   

> {superlative }∈[”longest”, ”highest”, ”deepest”, ”largest”]

• Periodicity Detection Template: What is the dominant cycle period (in data points) of channel {channel } within {time window }?12 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

Args: Target Signal ∈ [”sine”, ”cosine”, ”composite”]; Period Range → Random Integer [30, 120] points 

• Similarity Search Template: Analyze the reference pattern in {query window }. Find the time interval where channel {channel }

exhibits the most similar pattern within the search context {search window }.

Args: Injected Prototypes ∈ [”bell curve”, ”step pattern”, ”double-peak (M-shape)”, ”sharp spike”] 

Level 3: Semantic Reasoning This level escalates the challenge from local pattern recognition to high-level semantic reasoning over long-horizon histories (typically year-scale). Tasks here simulate expert analyst workflows, requiring the model to contextualize events against global baselines (e.g., distinguishing a ”flood” from normal high tide) or infer causal discrepancies between correlated time series. 

## Level 3: Semantic Reasoning Templates 

• Composite Trend Search (Top-K) Template: Identify the top-{k} dates in channel {channel } during {year } that exhibit the most significant 

{pattern desc } trend. 

Args: {pattern desc } ∈ [”rapid rise then fall”, ”gradual reversal”, ”step ascent”, ...]; Injection Intensity → Linear Decay (ensures strict ranking) 

• Contextual Anomaly Detection Template: Identify the period in channel {channel } during {year } that experienced the most significant 

{anomaly desc }.

Args: {anomaly desc } ∈ [”severe flood” ( > 3σ surge), ”severe drought” (variance → 0)]; Context → Full Calendar Year ( >30k points) 

• Causal Anomaly Detection Template: Given that channel {upstream } causes {downstream }, identify the time period in {year } where 

{downstream } shows a significant causal anomaly, such as an {break desc }.

Args: {break desc } ∈ [”inverse trend against source”, ”flat line during activity”]; Mechanism → Correlation Break Injection 

Level 4: Insight Synthesis At the apex of the benchmark, Level 4 assesses the model’s ability to act as an automated data analyst. Unlike previous levels that target atomic outputs (intervals or values), this task demands a holistic linguistic summary. The model must segment a complex, multi-stage time series into semantically meaningful regimes and conduct a rigorous audit of local anomalies using a standardized professional vocabulary. 

## Level 4: Insight Synthesis Templates 

• Trend & Anomaly Audit (Report Generation) Template: Analyze channel {channel } for the period {target month }.

Constraint: Use ONLY standardized phrases: [”rapid/gradual rise”, ”rapid/gradual fall”, ”steady/fluctuating stable”]. 

Requirement: Provide a structured report covering: 1. Trend Segmentation (with precise HH:MM:SS timestamps). 2. Significant Outlier Audit (ignoring minor noise). 

Args: Scenario Generation → Chained injection of 2–4 distinct primitives (e.g., Linear Trend → Stable → Oscillation);   

> Ground Truth →Structured Fact Sheet defining the exact start/end and type of every stage.

13 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

A.2.3. D ATA SYNTHESIS 

We propose a unified generative framework G to construct the benchmark. To balance data realism with ground truth accuracy, G employs two complementary strategies: Solver-based Extraction for explicit numerical facts, and Compositional Injection for semantic pattern reasoning. 

Strategy I: Solver-based Extraction (Level 1). For tasks governed by strict numerical logic (e.g., aggregation, extrema), we directly derive answers from raw data to preserve its original characteristics. Let Xraw ∈ RT be a multivariate time series. A query-answer pair (Q, A ) is derived by executing a deterministic solver S over a sampled context window W ⊂ Xraw :

A = S(W, θ ), where θ ∼ Pdata (W ) (4) Crucially, the query parameters θ (e.g., threshold τ ) are sampled from the data’s empirical distribution Pdata (e.g., τ is drawn from the local percentiles of W ) rather than fixed constants. This ensures that the generated queries are mathematically valid and pragmatically non-trivial (i.e., avoiding null result sets). 

Strategy II: Compositional Injection (Levels 2–4). For tasks requiring morphological cognition or causal reasoning, we utilize a Hierarchical Injection Protocol to construct controlled signals from atomic mathematical primitives. 

1. Atomic Primitives ( P). We define a library of differentiable base functions f (t) to model fundamental visual concepts. These primitives are parametric and continuous: • Transient Patterns: Modeled via Gaussian kernels, e.g., fspike (t) = A · e−λt 2

.• State Shifts: Modeled via dual-Sigmoid activations, e.g., fbox (t) = σ(k(t − ts)) − σ(k(t − te)) .• Oscillations: Modeled as composite sinusoids with noise, fwave (t) = P Ai sin( ωit + ϕi) + ϵ.

2. Additive Composition Model. Complex scenarios are synthesized by recursively composing these primitives. The synthetic series Xsyn is formulated as: 

Xsyn (t) = Xbg (t) + α · T (f (t)) (5) where Xbg is a background window selected for stationarity (minimizing variance), T represents temporal transformations (scaling, shifting), and α is an adaptive gain factor. The factor α is dynamically calibrated to maintain the Signal-to-Noise Ratio (SNR) above a rigorous threshold (e.g., SNR > 1.0), ensuring that the injected patterns are distinguishable from background noise. This hierarchical approach allows for flexible complexity scaling: • Level 2 (Atomic): Injects a single instance of a primitive (e.g., one ”Plateau”) to test local pattern recognition. • Level 3 (Semantic): Chains primitives to form logical trends (e.g., frise ⊕ ff all ) or introduces correlation breaks for causal anomaly detection. • Level 4 (Narrative): Composes multi-stage sequences with distractors to simulate full-lifecycle analytical reports. 

Quality Assurance. We enforce a post-synthesis audit to filter out low-quality samples. Candidates are rejected if (1) the injected pattern is statistically indistinguishable from the background (low SNR), or (2) the random background noise coincidentally mimics the target pattern (ambiguity check). A.2.4. H UMAN VISUAL VERIFICATION 

Although our synthesis pipeline includes automated statistical checks (e.g., SNR and ACF), we employ a human-in-the-loop audit as the final safeguard to ensure that the injected patterns are perceptually distinct and unambiguously aligned with the natural language queries. Figure 5 illustrates our custom verification interface, where expert annotators review the generated samples to filter out artifacts or edge cases that satisfy numerical constraints but lack visual clarity. 14 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases  

> Figure 5. The human verification interface. Annotators inspect both global context and local details (where the injected signal in orange is overlaid on the raw data in blue) to validate the ground truth.

A.3. Evaluation Implementation 

Given the diversity of output formats (scalars, intervals, sets, and natural language reports), we implement a robust evaluation suite consisting of four specialized metrics. 

1. Scalar & Timestamp Accuracy. For atomic retrieval tasks (Level 1) returning numerical values or timestamps, we employ a tolerance-based accuracy metric. • Scalars: We calculate the Relative Accuracy to handle varying scales. Given a ground truth y and prediction ˆy:Score scalar (y, ˆy) = max 



0, 1 − |y − ˆy||y| + ϵ



(6) where ϵ = 10 −9 prevents division by zero. • Timestamps: We use a binary Hit Rate with strict tolerance. A prediction is correct if |tpred − tgt | < δ , where δ

corresponds to the sampling resolution (typically 0). 

2. Interval Intersection over Union (IoU). For time range identification tasks (Level 1 & 2), we evaluate the temporal overlap between the predicted interval Ip = [ tpstart , t pend ] and the ground truth Ig = [ tgstart , t gend ]:IoU (Ip, I g ) = duration (Ip ∩ Ig )

duration (Ip ∪ Ig ) (7) where duration (·) denotes the time difference in seconds. Non-overlapping intervals yield a score of 0. 15 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

3. Set F1-Score. For tasks requiring a list of discrete dates (e.g., Top-K search in Level 3), we treat the output as an unordered set and compute the F1-score: 

F 1 = 2 · Precision · Recall Precision + Recall (8) where Precision is the fraction of correct dates in the prediction, and Recall is the fraction of ground truth dates successfully retrieved. 

4. Composite Report Score (Level 4). Evaluating natural language reports is challenging. Instead of relying on generic text metrics like BLEU or ROUGE, we utilize a structure-aware Composite Score . Crucially, as defined in Level 4 (see §A.2.2), our prompts explicitly enforce a strict output schema and a controlled vocabulary (e.g., mandating phrases like “rapid rise” or “steady stable”). This instructional constraint allows us to deterministically parse the generated text into structured semantic components (Trend Segments and Outliers), which are then evaluated via a weighted sum of four sub-metrics: 

Stotal = w1Strend + w2Sinterval + w3Sadj + w4Soutlier (9) Based on our experimental calibration, we set the weights as follows: • Trend Sequence ( w1 = 0 .4): Measures the alignment of trend types (e.g., ”Rise” vs. ”Fall”) using Longest Common Subsequence (LCS) matching. • Segmentation Accuracy ( w2 = 0 .3): The average IoU of the time intervals for the matched trend segments. • Semantic Precision ( w3 = 0 .2): Measures whether the descriptive adjectives (e.g., ”Rapid” vs. ”Gradual”) match the ground truth. • Outlier Detection ( w4 = 0 .1): The F1-score of the detected significant anomaly timestamps (within a 4-hour tolerance window). This composite metric ensures that a model is rewarded only if it correctly identifies what happened (Trend), when it happened (Interval), how it happened (Adjective), and where the anomalies are (Outlier). 

## B. Implementation Details of Sonar-TS 

B.1. Offline Data Processing Implementation Database Selection and Design Philosophy. The Sonar-TS framework is designed with a decoupled architecture, separating data storage from analytical logic. For the underlying TSDBs, we currently target systems that support standard SQL, such as InfluxDB. To ensure broad compatibility and portability, the SQL generation module avoids proprietary extensions or dialect-specific advanced functions (e.g., specific time-windowing clauses unique to TimescaleDB). Instead, SQL is utilized primarily for efficient data retrieval and initial filtering, while complex analytical processing is offloaded to the flexible Python environment. Although specialized TSDB features could offer latency benefits in production deployments, our current design prioritizes architectural generality. 

Multi-scale Feature Tables. To facilitate efficient multi-granular querying, we materialize feature tables aligned with natural temporal hierarchies: Yearly , Monthly , and Daily views. Each row in these tables summarizes a specific time window w = [ tstart , t end ) for a unique time series channel. The schema includes: • Metadata: series id , view type (e.g., ’daily’), window start , window end .• Statistical Primitives: min val , max val , avg val , std val , and slope .• Morphological Signature: sax (Symbolic Aggregate approXimation string) and sax len .16 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

B.1.1. S TATISTICAL FEATURES 

For each window, we precompute a set of lightweight descriptive statistics. While the current version of the NLQ4TSDB benchmark focuses heavily on complex pattern recognition, making simple statistics less central to the primary evaluation tasks, these primitives remain critical for optimizing broader query types. For instance, a query such as “Find the longest daily interval with a continuous upward trend” can be accelerated significantly by filtering on precomputed slope values (e.g., slope > 0 ), thereby pruning the search space before accessing raw high-frequency data. Similarly, std val 

serves as an efficient proxy for filtering volatile or stable periods. B.1.2. M ORPHOLOGICAL FEATURES (SAX I MPLEMENTATION )To support shape-based retrieval, we implement SAX, which discretizes time series windows into string signatures. This transforms complex shape matching into efficient string operations (e.g., regex matching). 

1. Piecewise Aggregate Approximation (PAA). We first reduce the dimensionality of the raw time series sequence 

C = {c1, . . . , c n} within a window into a vector of length w, denoted as ¯C = {¯c1, . . . , ¯cw}. The choice of w adapts to the temporal granularity to preserve semantic interpretability: • Yearly View: w = 12 , aligning with months. • Monthly View: w is dynamic, equal to the number of days in that specific month (e.g., 28, 30, or 31). • Daily View: w = 24 , aligning with hours. The i-th PAA coefficient is computed as the mean of the corresponding segment: 

¯ci = wn 

> nwi

X  

> j=nw(i−1)+1

cj (10) 

2. Symbolic Mapping. The PAA vector ¯C is Z-normalized to have a mean of zero and a standard deviation of one. We then map each coefficient ¯ci to a symbol si from an alphabet Σ of size α (in our implementation, α = 5 , Σ = {′a′,′ b′,′ c′,′ d′,′ e′}). The mapping is defined by a set of breakpoints β = {β0, . . . , β α}, which divide the area under the Normal distribution 

N (0 , 1) into α equiprobable regions. The mapping function is: 

si = char (j) if βj−1 ≤ ¯ci < β j (11) where β0 = −∞ and βα = ∞. This results in a discrete signature string ˆC = s1s2 . . . s w that robustly encodes the shape of the time series window. Figure 6 illustrates the SAX transformation process across different granularities. 

B.2. Online Querying Implementation 

This section details the runtime components of the Sonar-TS Online Querying module.The system operates on a Search-Then-Verify workflow. While the database search relies on standard SQL, the subsequent verification phase requires precise mathematical computations that are often cumbersome to implement from scratch within a limited context window. To address this, we implement a library of Domain-Specific Verification Operators and a dynamic prompt construction mechanism to guide the Code Generator. B.2.1. D OMAIN -S PECIFIC VERIFICATION OPERATORS 

To bridge the gap between linguistic intent and numerical execution, we implement a library of atomic Verification Operators . These operators serve as computational primitives that the Code Generator can import and compose to verify high-level logic. Critically, these functions are designed to be atomic and stateless , performing a specific mathematical operation on input data frames (e.g., pandas.DataFrame or numpy.ndarray ) without coupling to specific query templates. This decoupling allows the LLM to flexibly combine them—for instance, using a change point detection operator 17 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

Figure 6. Visualization of Multi-Scale SAX Representations. The framework discretizes time series data across hierarchical granularities to support pattern matching at different resolutions: (Top) The Daily View captures high-frequency local fluctuations; (Middle) The 

Monthly View summarizes intermediate trends; (Bottom) The Yearly View abstracts long-term seasonality. The colored horizontal bars represent the assigned SAX symbols (from level a to e) overlaid on the raw data (gray line). 

followed by a slope calculation to characterize a multi-stage trend(i.e., the level-4 Insight Synthesis task). Table 4 details the core operator signatures exposed to the model. 

Table 4. List of Atomic Verification Operators. These primitives encapsulate standard time-series algorithms, serving as the executable vocabulary for the Code Generator. 

Operator Signature Functionality & Atomicity Core Algorithm 

detect period(data, max lag) 

Periodicity Estimation. Computes the dominant period length of the series. Used as a primitive for analyzing cyclic behaviors. Autocorrelation Function (ACF) 

find best match(query seq, search seq, metric) 

Subsequence Matching. Scans a search window to find the interval most similar to a reference pattern. Atomic support for “shape matching” or “pattern retrieval” tasks. Dynamic Time Warping (DTW) 

detect changepoints(data, penalty) 

Structural Segmentation. Identifies timestamps where the statistical properties (mean/variance) of the series shift abruptly. Essential for breaking continuous streams into dis-crete logical segments. PELT 

calc trend slope(data) Trend Quantification. Fits a robust linear model to a specific window. The resulting slope is interpreted relative to the local slope distribution to determine semantic labels (e.g., “Rapid Rise”), avoiding rigid thresholds. Theil-Sen Estimator 

calc correlation(seq a, seq b, lag) 

Relationship Measurement. Computes the statistical corre-lation between two aligned series windows. Used to verify causal links or detect “correlation breaks” (anomalies where expected synchrony disappears). Pearson Correlation 

18 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

B.2.2. P ROMPT DESIGN 

We implement a structured prompting pipeline to orchestrate the Search-Then-Verify workflow. This process dissociates high-level logical planning from low-level code implementation, featuring a dual-mode code generator capable of iterative self-correction. 

1. Task Planner Prompt. The Planner is responsible for selecting the execution mode and the data source. As shown in the prompt below, the model is explicitly guided to distinguish between the “High IO” raw data table (Wide) and the “Low IO” feature tables (Long) to optimize retrieval efficiency. 

## Task Planner Prompt 

You are a Task Planner for Time Series Analysis. Decompose the question into a JSON plan. No code. 

— DATABASE SCHEMA — 

{database schema }

1. Raw Data (data): High I/O cost. Use ONLY when the time range is explicitly known and precise values are needed. 2. Feature Tables (feature *): Low I/O cost. Contains precomputed stats (avg, std) and shape descriptors (sax). Use for searching patterns or scanning large historical ranges. 

— EXPERIENCES — 

{experiences }

— User Question — 

{Input Question }

— PLANNING STRATEGY — 

Analyze the query constraints to decide the Pipeline Mode: Mode A: DIRECT ACCESS (Fetch → Compute) Criteria: 1. Time range is fixed (e.g., ”last 24h”, ”in 2023”). 2. Task involves calculation on a specific segment. Strategy: Direct SQL query on ”data” → Python Processing. Mode B: SEARCH THEN VERIFY (Prune → Verify) Criteria: 1. Time range is UNKNOWN/OPEN (e.g., ”Find the period where...”). 2. Logic is MORPHOLOGICAL (e.g., ”V-shape”) or requires historical context. Strategy: Search ”feature *” filters → Get Candidates → Python Verification. 

— Output Format (Strict JSON) — 

{

”reasoning”: ”Analysis of time range constraints and search intent”, ”pipeline mode”: ”DIRECT ACCESS” or ”SEARCH THEN VERIFY”, ”step 1 retrieval”: {

”target table”: ”data” or ”feature daily” or ”feature monthly”, ”sql logic hint”: ”Description of the SQL filter/logic” 

},”step 2 computation”: {

”needs python”: true, ”logic description”: ”Description of the math/verification logic” 

}}

2. Code Generator Prompt. Following the plan, the Code Generator constructs the executable queries. It operates in two modes: Generation (synthesizing the initial code) and Refinement (fixing errors based on runtime feedback). In the generation phase, strict environmental constraints and a domain-specific operator library are enforced to ground the model’s logic. Subsequently, the refinement phase acts as a runtime debugger, allowing the agent to iteratively self-correct against dynamic exceptions (e.g., empty SQL results) that are unpredictable during the initial generation. 19 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

## Code Generation Prompt (Generation Mode) 

You are the Code Generator. Translate the structured Task Plan into executable Python code. 

— DATABASE SCHEMA — 

{database schema }

— EXPERIENCES — 

{experiences }

— User Question — 

{Input Question }

— TASK PLAN (JSON) — 

{JSON Plan generated by Task Planner }

— CODE GENERATION STRATEGY — 

Generate a single Python code block to execute the plan. 1. Execution Environment: You have access to pandas (pd), numpy (np), and scipy. The variable ”conn” is already defined and connected to the database. - DO NOT create a new connection. DO NOT close ”conn”. - Execute SQL using: df = pd.read sql query(sql, conn). 2. Operator Library (Module: sonar ops): Prioritize using the following pre-defined atomic operators for verification. If a task cannot be solved by these operators, synthesize standard pandas/numpy logic. - detect period(data, max lag): Estimates the dominant cycle length. - find best match(query, search, metric): Finds the most similar subsequence via DTW. - detect changepoints(data, penalty): Identifies structural break points (PELT). - calc trend slope(data): Computes robust slope (Theil-Sen) for trend description. - calc correlation(seq a, seq b, lag): Measures statistical relationship between series. 

— Output Format — 

Return ONLY the executable Python code block. No markdown explanation. The code must define a final variable final answer containing the result. 

## Code Generation Prompt (Refinement Mode) 

You are the Code Refinement Agent. The previous execution failed. Analyze the error trace and modify the code to make it executable. 

— SHARED CONTEXT — 

(Includes Database Schema, Experiences, Input Question, Task Plan, and Code Generation Strategy) 

— HISTORY: TURN 1 — 

Code: 

{Previous Python Code }

Execution Feedback: 

{Error Traceback OR ”Empty Result” }

... (History stacks up to 3 turns) ... — Output Format — 

Return ONLY the corrected executable Python code block. No markdown explanation. The code must define a final variable final answer containing the result. 

3. Experience Summarizer Prompt. This module acts as a ”Technical Lead” conducting a post-mortem. It analyzes the full execution trajectory—including the initial plan, the challenges faced (error history), and the final working code—to extract a single, high-value insight. This ensures that the system learns not just from success, but from the corrections applied during the process. 20 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

## Experience Summarization Prompt 

You are a Technical Lead conducting a Post-Mortem. Summarize the technical solution. 

— Question — 

{Input Question }

— Plan — 

{JSON Plan generated by Task Planner }

— CODE & EXECUTION HISTORY — 

{Full Trace of Code Generations and Execution Feedbacks }

— Task — 

Extract ONE concise, reusable technical insight. - How was the task resolved? - Analyze the reasons for success or failure. - Generalize the finding. 

— Output Format — 

Insight: [Your 1-sentence summary] 

4. Experience Updater prompt To prevent the experience list from growing indefinitely, the Updater functions as a ”Knowledge Curator.” It takes the newly extracted insight and merges it into the existing Global Experience Pool. The model is instructed to perform semantic operations— Add (if new), Merge (if similar), or Discard (if redundant)—ensuring the knowledge base remains compact and highly relevant. 

## Experience Updater Prompt 

You are the Knowledge Base Curator for Sonar-TS. Manage the global experience list. 

— EXISTING EXPERIENCES — 

{Current List of N Insights }

— NEW INSIGHT — 

{Output from Summarizer }

— UPDATE STRATEGY — 

Compare the New Insight with the Existing List and apply one operation: 1. ADD : If the insight covers a new edge case. 2. MERGE : If a similar insight exists, combine them into a more robust rule. 3. DISCARD : If the insight is trivial or fully covered. Constraint: Keep the total list size under 20 items to preserve context window. 

— Output Format — 

Return the updated list of experiences (JSON list of strings). 

5. Experience Snapshot. Finally, to illustrate the nature of the learned knowledge, we display a subset of the actual experiences. These insights range from low-level syntax corrections to high-level robustness strategies. 

## Snapshot of Injected Experiences 

• For structural trend reporting, do not calculate a global slope. Instead, use a divide-and-conquer approach: first apply de-tect changepoints to decompose the trend into segments, then use calc trend slope to quantify the direction of each specific segment.” • For symbolic pattern matching, utilize the 5-level SAX alphabet (’a-b’ for Low, ’d-e’ for High). E.g., to detect a ’low plateau’ (bottom out), use the fuzzy SQL regex: SELECT * FROM feature daily WHERE sax REGEXP ’.*[a-b] {3, }.*’ • When working with timestamps in pandas, ALWAYS convert them immediately after loading: df[’ts’] = pd.to datetime(df[’ts’]). Never assume they are datetime objects. Never call .strftime() on a numpy array without conversion. 

21 Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases 

• If Step 1 (SAX filtering) returns an empty result, you MUST automatically fallback to fetching raw data for the entire requested time range. Do not simply return ’No data found’—the SAX approximation might have been too strict. • When detecting continuous time periods (Interval Discovery), use dynamic gap detection based on the dataset’s median sampling interval. • ... (others) 

22