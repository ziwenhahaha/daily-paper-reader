Title: Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity

URL Source: https://arxiv.org/pdf/2602.07970v1

Published Time: Tue, 10 Feb 2026 02:28:52 GMT

Number of Pages: 25

Markdown Content:
# LEARNING -GUIDED KANSA COLLOCATION FOR FOR -

# WARD AND INVERSE PDE S BEYOND LINEARITY 

Zheyuan Hu 1, Weitao Chen 2, Cengiz ¨ Oztireli 1, Chenliang Zhou 1∗, Fangcheng Zhong 1∗ 

> 1

Department of Computer Science and Technology,  

> 2

Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK. ∗Co-corresponding authors. 

{zh369, wc358 }@cam.ac.uk, {chenliang.zhou, fangcheng.zhong }@cst.cam.ac.uk 

## ABSTRACT 

Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent Zhong et al. (2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a com-prehensive survey of neural PDE solvers and scientific simulation applications. 

## 1 INTRODUCTION 

PDEs are useful in different domains of scientific computing, including physics, graphics and biol-ogy. They describe how quantities change over space and time, making them essential for modeling phenomena such as fluid dynamics, electromagnetic fields, heat transfer, and population dynamics. Zhong et al. (2023) proposed extension to Kansa method, which is a mesh-free Radial Basis Func-tions (RBFs) PDE solver. They introduced auto-tuning of the shape parameters of RBFs. How-ever, their work focuses only on single-variable linear PDEs . Therefore, this paper extends CNFs backend solver to multi-dependent-variable u and nonlinear PDEs , and apply the framework to specific scientific simulation problems, including forward computation and inverse problems. It’s unknown how (extended) CNF solvers Zhong et al. (2023) compared with other classical and neural PDE solvers. Hence, we also implement and evaluate selected prior methods on the bench-marks on their effectiveness with different quality metrics (e.g. L1, L2, errors) against ground truth solutions, efficiency , computation resource, convergence speed, method complexity , and finally 

utility in research , i.e. their scientific simulation applications or integration with other methods, e.g. differentiable rendering to solve inverse physics-related problems in Graphics. 

## 2 RELATED WORK 

PDE benchmarks. We identified several representative equations Takamoto et al. (2022) in Table 14. They are different in linearity of the operator and solution u dimensionality. 

PDE solvers . Numerical methods, e.g. Finite Difference Method (FDM) and Finite Element Method (FEM), are widely used to solve PDEs. However, they suffer from the curse of dimensionality, high computation costs and domain-specific discretization. Recently, neural network based solvers have shown promising results in addressing these issues. For example, Physics-Informed Neural Networks (PINNs) Raissi et al. (2019) and Fourier Neural Operators (FNOs) Li et al. (2020) have demonstrated the ability to generalize to unseen scenarios and handle high dimension effectively. 1

> arXiv:2602.07970v1 [cs.CE] 8 Feb 2026

Inverse problem , i.e. estimating unknown parameters or inputs of a variable x from given solution observations u, is crucial. However, it’s unclear how CNFs can be applied to these problems, includ-ing connecting with differentiable rendering pipelines Spielberg et al. (2023) in Visual Computing. 

## 3 METHODOLOGY 

3.1 GENERAL FORM OF PDE S

With spatial domain Ω ⊂ Rd, where its dimension is d, and the unknown field u(x, t ) ∈ U : D → R

defined on the spatio-temporal domain D = Ω × [t0, t f ] ⊂ Rd+1 , the general form of PDEs is, 

 D[u] = f, x ∈ Ω, t ∈ [t0, t f ],

Bi[u] = gi, x ∈ ∂Ωi, t ∈ [t0, t f ]. ⇔

 D[u]( x, t ) = f (x, t ), x ∈ Ω, t ∈ [t0, t f ],

Bi[u]( x, t ) = gi(x, t ) x ∈ ∂Ωi, t ∈ [t0, t f ]..

(1) where D : U → Y is the differential operator and f ∈ Y : D → Rm is source function, e.g. the external force in dynamics, with m being the output dimension of f 1. The differential operators D

include the gradient ∇, Laplace ∆, divergence ∇· , etc. For boundary conditions, Bi : U → Z i is each boundary operator with gi ∈ Z i : ∂Ωi × [t0, t f ] → Rni and ni as the output dimension of gi.3.2 KANSA COLLOCATION 

Kernel functions . Radial Basis Function (RBF) inversely relates the distance r between the input x

and a fixed origin point c to the output value. 

ψc(r) = ψc(|| x − c|| ). (2) There are various infinitely smooth RBFs, among which we choose Gaussian RBF for its effective-ness in approximating smooth functions, 

ψc(r) = 



e−(ϵr )2

, Gaussian , 

> 11+( ϵr )2

, Inverse quadratic ,

p1 + ( ϵr )2, Multiquadrics ,

(3) where Gaussian shape parameter ϵ = 1√2σ , and σ is the standard deviation. 

Kansa method Kansa (1990) approximates the solution u(x, t ) with a linear combination of kernel functions ψi(|| x − xi|| ) ∈ Rd → R centered at each collocation points {xi ∈ D}Ni=1 ,

u(x, t ) ≈ ˆu(x) = 

> N

X

> i=1

αi · ψi(|| x − xi|| ), x ∈ D, (4) where αi ∈ R are the coefficients to be solved. The time dimension t is omitted, which can be treated as an additional spatial dimension here. Equation equation 4 is expressed as, by rewriting the kernel functions into matrix form, 



ˆu(x1)ˆu(x2)

...

ˆu(xN )

| {z }

> u∈RN

=



ψ1(|| x1 − x1|| ) ψ2(|| x1 − x2|| ) · · · ψN (|| x1 − xN || )

ψ1(|| x2 − x1|| ) ψ2(|| x2 − x2|| ) · · · ψN (|| x2 − xN || )

... ... . . . ...

ψ1(|| xN − x1|| ) ψ2(|| xN − x2|| ) · · · ψN (|| xN − xN || )

| {z }  

> kernel matrix K∈RN×N

·



α1

α2

...

αN

| {z } 

> a∈RN

. (5) The PDE general form equation 1 can be summarized as a single equation, 

F[ˆ u]( xi) = h(xi), xi ∈ D, (6) where the operator F = {D , Bi} and h = {f, g i} represent both the initial and boundary conditions. 

> 1

Note that U and Y are two function spaces, and we require they are Banach spaces. 

23.2.1 LINEAR OPERATOR CASE 

By plugging in the approximation of u equation 4 and assuming the operator F is linear 2, the PDE equation 6 can be simplified as, 

F[ˆ u]( xi) = F[

> N

X

> i=1

αi · ψi]( xi) = 

> N

X

> i=1

αi · F [ψi]( xi) = h(xi). (7) By expanding in matrix form, the above equation is, 



F[ψ1]( x1) F[ψ2]( x1) · · · F[ψN ]( x1)

F[ψ1]( x2) F[ψ2]( x2) · · · F[ψN ]( x2)

... ... . . . ...

F[ψ1]( xN ) F[ψ2]( xN ) · · · F[ψN ]( xN )

| {z }  

> operator-evaluated kernel matrix F∈RN×N

·



α1

α2

...

αN

| {z } 

> a∈RN

=



h(x1)

h(x2)

...

h(xN )

| {z } 

> constraint values h∈RN

. (8) Concretely, when the kernel function is Gaussian RBF defined in equation 2, and xi = ( xi, t i),

ψi = e− r2 

> i
> 2σ2

, r2 

> i

= ( x − xi)2 + ( t − ti)2. (9) Take F = ∂∂t  

> 3

and by the chain rule, the element Fj,i in the matrix equation 8 is thus, 

F[ψi]( xj ) = ∂ψ i(xj )

∂t = ∂ψ i(xj )

∂r 2

> i

· ∂r 2

> i

∂t = − 12σ2 e− r2 

> i
> 2σ2

· 2( tj − ti) = − tj − ti

σ2 e− r2 

> i
> 2σ2

. (10) 

Simultaneous equations. When there are Neq equations of different constraints to be satisfied, the collocation points {xi ∈ D}Ntotal  

> i=1

are distributed among all equations 4, where Ntotal = PNeq  

> j=1

Nj .

Fj [ˆ u]( xi) = hj (xi), ∀j ∈ { 1, . . . , N eq }, xi ∈ D. (11) Equation equation 8 can be extended by stacking each matrix F(j) ∈ RNtotal ×Ntotal and constraint vector h(j) ∈ RNtotal vertically for all equations Zhong et al. (2023). The block matrix form is, 



F(1) 

...

F(Neq )

| {z }  

> stacked F∈R(Neq ·Ntotal )×Ntotal

·



α1

...

αNtotal 

| {z }

> a∈RNtotal

=



h(1) 

...

h(Neq )

| {z }  

> stacked h∈RNeq ·Ntotal

. (12) 

The solution of u equation 4 depends on the coefficients a = [ α1, α 2, . . . , α N ], which can be solved by the linear system Fa = h. The general form is given by the least squares approximation, i.e. minimizing the norm of the error vector and setting the gradient to zero, 

aopt = min  

> a

(|| Fa − h|| )2,

∇a(Fa − h)T (Fa − h) = 0 = ⇒ (FT F)aopt = FT h.

(13) If matrix F is full rank, FT F is invertible, thus one can derive aopt = ( FT F)−1FT h. Should the matrix F be square and invertible, equation 13 can be further simplified as aopt = F−1h. Whichever conditions occurs, the final solution for u is approximated by plugging in the optimal coefficients 

aopt into equation 5, i.e. ˆu(x) = K · aopt .When testing on unseen data points {x⋆j ∈ D}Mj=1 , the kernel functions are constructed between the test points and the collocation points {xi ∈ D}Ni=1 . The solution is thus, 

u(x, t ) ≈ ˆu(x⋆) = 

> N

X

> i=1

αi · ψi(|| x⋆ − xi|| ), x ∈ D, (14) 

> 2

For linear operators, F[α · ψ] = α · F [ψ], as defined in § A.   

> 3∂ψ i
> ∂t

: phi t = torch.autograd.grad(phi, t, create graph=True)[0] 

> 4

Note that repeated collocation points are forced to be repeated here for distinct constraints. 

3Test-time solution equation 14 can be formulated to matrix form, 



ˆu(x⋆

> 1

)ˆu(x⋆

> 2

)

...

ˆu(x⋆M )

| {z }

> u∈RM

=



ψ1(|| x⋆ 

> 1

− x1|| ) ψ2(|| x⋆ 

> 1

− x2|| ) · · · ψN (|| x⋆ 

> 1

− xN || )

ψ1(|| x⋆ 

> 2

− x1|| ) ψ2(|| x⋆ 

> 2

− x2|| ) · · · ψN (|| x⋆ 

> 2

− xN || )

... ... . . . ...

ψ1(|| x⋆M − x1|| ) ψ2(|| x⋆M − x2|| ) · · · ψN (|| x⋆M − xN || )

| {z }  

> kernel matrix K⋆∈RM×N

·



α1

α2

...

αN

| {z } 

> a∈RN

. (15) 3.2.2 EXTENSION 1: COUPLED SOLUTION FIELDS OF PDE S

Coupled multi-dimensional PDE solution fields. Assuming there are ND solution dimensions, i.e. u = [ u1, u 2, . . . , u ND ], the Kansa approximation equation 4 for each dimension is, 

ˆud(x) = 

> N

X

> i=1

α(d) 

> i

· ψ(d) 

> i

(|| x − xi|| ), ∀d ∈ { 1, . . . , N D }. (16) The couple PDE equation is thus formulated as applying the coupling, or governing operator G on all dimensions of solution, which each has its own operator F(d),

G



F(1) [ˆ u1], . . . , F(ND )[ˆ uND ]



(xi) = h(xi), xi ∈ D. (17) Here we assume the coupling operator G is linear with each dimension of solution, 

G (ˆ v1, . . . , ˆvND ) ( x) = 

> ND

X

> d=1

βd · ˆvd(x), (18) where βd ∈ R is the per-dimension weight. Equation equation 8 can be extended by stacking each matrix F(d) ∈ RN ×N horizontally for all dimensions of solution. The block matrix form is, 

[β1IN · · · βND IN ]

| {z }   

> β∈RN×(ND·N)

◦ F(1) · · · F(ND )| {z }    

> coupling F∈RN×(ND·N)

·



a(1) 

...

a(ND )

| {z }  

> a∈R(ND·N)

=



h(x1)

...

h(xN )

| {z } 

> stacked h∈RN

, (19) where ◦ is element-wise or Hadamard product and IN is the identity matrix of size N . For simulta-neous coupled PDE equations, similar to equation 11, they are indexed by j ∈ { 1, . . . , N eq },

Gj



F(1)  

> j

[ˆ u1], . . . , F(ND ) 

> j

[ˆ uND ]



(xi) = hj (xi), ∀j ∈ { 1, . . . , N eq }, xi ∈ D. (20) With Ntotal defined as in equation 12, the block matrix form is, 



β(1) 

...

β(Neq )

| {z }  

> β∈R(Neq ·Ntotal )×(ND·Ntotal )

◦



F(1 ,1) · · · F(1 ,N D )

... F(j,d ) ...

F(Neq ,1) · · · F(Neq ,N D )

| {z }   

> coupling F∈R(Neq ·Ntotal )×(ND·Ntotal )

·



a(1) 

...

a(ND )

| {z } 

> a∈R(ND·Ntotal )

=



h(1) 

...

h(Neq )

| {z }  

> stacked h∈R(Neq ·Ntotal )

.

(21) 3.2.3 EXTENSION 2: NONLINEAR OPERATOR CASE 

When the operator F is nonlinear , we can no longer simplify equation 6 as in equation 7. However, we can still derive the relation between the solution u and its linear transformed version as below. 

Differentiable matrix helps decompose the general non-linear operator F into a series of linear operators. Take any linear operator, e.g. ∂∂x , it relates the relation between unknown u and its derivative u′ = Dx · u. We derive Dx from Kansa equation 4, by linearity and equation 7, 

∂∂x u(x) = 

> N

X

> i=1

αi · ∂∂x ψi(|| x − xi|| ). (22) 4In matrix form, we have u′ = Kx · a, where the matrix Kx ∈ RN ×N is constructed by evaluating  

> ∂∂x

ψi(|| x − xi|| )|x=xj for all i, j ∈ { 1, . . . , N } as row and column indices. We can invert equation 5 a = K−1 · u, assuming K invertibility from independent basis. By substituting a into u′ = Kx · a, one gets u′ = Kx · K−1 · u. The differentiable matrix is thus, 

Dx = Kx · K−1 ∈ RN ×N . (23) For viscous Burgers’ equation equation 63 F[u] = ∂u ∂t + u ∂u ∂x − ν ∂2u∂x 2 . Its differentiable matrix form, which follows the same formulation as in equation 23 by replacing the operator accordingly, is, 

F[u] = Dt · u + u ◦ (Dx · u) − ν(Dxx · u). (24) Here we present two categories of Kansa approaches (Table 1). The first consists of four time-stepping schemes, including two per-step linear and another two nonlinear systems. The second employs a fully nonlinear solver on the PDE residuals, without explicit time discretization. Table 1: Summary of different non-linear Kansa solver features, ∆t is the time step size, Nx and 

Nt are the number of collocation points in spatial and temporal dimensions respectively. Features forward IMEX backward Crank–Nicolson fully non-linear 

Time-step explicit semi-explicit implicit implicit ×

Error O(∆ t) O(∆ t) O(∆ t) O(∆ t2) O(1) 

Stability unstable stable stable stable N/A Memory O(N 2 

> x

) O(N 2 

> x

) O(N 2 

> x

) O(N 2 

> x

) O(N 2 

> x

N 2 

> t

)

Time-stepping approach with linear system. We can remove the non-linearity by discretizing the time derivative via finite difference method, for a special case of time-dependent PDEs. One solution is to use the (1) explicit forward Euler scheme, 

∂u ∂t + D[u] = 0 = ⇒ un+1 − un

∆t + O(∆ t) + D[un] = 0 , (25) where D is the spatial operator. A more stable solution is to use the (2) implicit-explicit (IMEX) 

scheme, which splits the stiff and non-stiff parts of the operator D = Istiff + Enon-stiff , which stiffness means the numerical instability incurred by the operator and needs to be treated implicitly, 

un+1 − un

∆t + O(∆ t) + Istiff [un+1 ] + Enon-stiff [un] = 0 . (26) Despite the non-linear spatial operator D or Enon-stiff , we already know the solution un at time step 

n. Thus, with differentiable matrices, one can evaluate D[un] or Enon-stiff [un] directly, so as to derive the solution un+1 at the next time step n + 1 .

Time-stepping approach with nonlinear solver. If we discretize the time derivative via the (3) 

backward Euler scheme, the non-linearity remains in the formulation, 

∂u ∂t + D[u] = 0 = ⇒ un+1 − un

∆t + O(∆ t) + D[un+1 ] = 0 . (27) We can directly replace linear system solver by a non-linear system solver, e.g. Newton-Raphson method Ypma (1995), to minimize the residual vector and derive the unknown solution at next time step, 

un+1 = arg min  

> un+1

rn+1 , where rn+1 = un+1 − un + ∆ t · D [un+1 ]. (28) Alternatively, (4) Crank-Nicolson scheme can be used to discretize second-order accurate in time, 

∂u ∂t + D[u] = 0 = ⇒ un+1 − un

∆t + 12

 D[un+1 ] + D[un] + O(∆ t2) = 0 . (29) Similar with equation 28, the unknown solution at next time step is derived by minimizing the residual vector as stated in equation 29. 5Fully nonlinear solver without time-stepping. This approach directly minimizes the PDE residuals equation 6 over all collocation points, without explicit time discretization. After plugging in the differentiable matrix form of the non-linear operator F, the objective function is therefore, 

α = arg min 

> αN

X

> i=1

(F[ˆ u]( xi) − h(xi)) 2 . (30) By plugging in Kansa approximation equation 4, we derive unknown solution u over entire domain. 3.2.4 AUTO -TUNING OF KANSA HYPERPARAMETERS 

To tune the key Kansa method hyperparameter, kernel shape parameter ϵ in equation 3, Zhong et al. (2023) proposed one of the self-tuning methods for ϵ, by minimizing the variation of the solution field u over all collocation points, and the condition number of the kernel matrix F,

ϵ⋆ = arg min  

> ϵ

ω1 · cond (F) + ω2 ·

Z

> D

||∇ u(x)|| 2dx, (31) where cond (F) is the condition number of matrix F defined in equation 8. The integral term can be approximated by summing over all collocation points by Monte Carlo integration. This approach works for linear, including coupled and multi-dimensional, PDEs. For non-linear operator case, the solution u depends on ϵ implicitly via the coefficients αi. The matrix F no longer exists explicitly. Here, we propose to directly minimize the PDE residuals over all collocation points, the total variation of the solution field u, and the training L2 loss between the predicted solution u and the ground truth solution ugt if available, 

ϵ⋆ = arg min  

> ϵ

ω1 ·

> N

X

> i=1

(F[ˆ u]( xi) − h(xi)) 2 + ω2 ·

Z

> D

||∇ u(x)|| 2dx + ω3 · || u − ugt || 2, (32) where ω1, ω 2, and ω3 are the weights for each penalty term. 3.3 SOLUTIONS OF INVERSE PDE PROBLEMS 

Inverse PDE problems. When given observations of solution field uobs , we infer the unknown PDE parameters π that minimize the discrepancy L between the predicted upred (π) and uobs ,

π⋆ = arg min  

> π

L(uobs , u pred (π)) . (33) We adopt the SciPy implementation of the least squares and root finding algorithms, which are either gradient-based or gradient-free, detailed in the evaluation section. 

## 4 EVALUATION 

4.1 PERFORMANCE METRICS 

Accuracy . Given the numerical solution ˆui from PDE solvers, and the corresponding ground truth 

ui, the L2 risk RL2 is the average discretized error over all Ntest test points, 

ˆRL2 = 1

Ntest 

> Ntest

X

> i=1

|| ˆui − ui|| 2, ˆRrelative L2 = 1

Ntest 

> Ntest

X

> i=1

|| ˆui − ui|| 2

|| ui|| 2

. (34) The relative L2 risk is computed from RL2 and normalized by the ground truth ui L2 norm, 4.2 EVALUATION OF SOLVERS FOR THE ADVECTION EQUATION 

For the 1D advection equation defined in equation 45, we set the number of domain quadrature points NR = 100 × 10 , i.e. initial condition (IC) points Nd = 10 and the boundary condition (BC) points NB = 100 × 2. The advection equation is initialized as per Table 2. 6Table 2: 1D advection equation experimental setup. 

domain time range parameter IC BC 

x0 = 0 , x f = 1 t0 = 0 , t f = 1 β = 0 .4 u0(x) = sin(2 πx ) per equation 46 FNO requires multiple instances of PDEs for training. Hence, we generate Npde = 100 instances by varying only the initial condition as, given ck ∼ N (0 , 1) ,

u0(x) := u0(x)max x |u0(x)| , where u0(x) = 

> 5

X

> k=1

ck sin(2 πkx ). (35) For training, PINN and FNO are trained via learning rate η = 10 −3 until convergence, i.e. with epoch iterations Niter = 3000 for PINN and Niter = 100 for FNO. For evaluation, the test points 

Ntest = 64 × 8. The error is measured by relative L2 risk ˆRrelative L2 equation 34. 4.2.1 FORWARD PROBLEM 

Since FNO is trained on Npde = 100 instances of PDEs, we compensate more training data for single-instance solvers for a fair comparison. The adjustment factor is defined as Cscale ∈

[1 , N pde ] ⊂ R+. Hence, the domain points is N ′R = Cscale × NR and methods denoted as FDM Cscale 

and PINN Cscale . The test-time results are summarized in Table 3. Table 3: Models accuracy ˆRrelative L2 ×10 −3, on 1D advection relative to the data domain resolution. 

Cscale FDM PINN FNO KM 

1 36 .63 300 .2 744 .3 1.918 22 17 .05 20 .68 58 .71 0.0028 42 7.478 8.654 37 .68 N/A 

Npde = 10 2 3.228 6.457 13 .37 N/A average 16 .10 ± 12 .87 83 .99 ± 124 .9 213 .5 ± 306 .9 0.9603 ± 0.9576 

From Table 3, we conclude that all solvers are sensitive to the number of training data, where larger 

Cscale leads to better precision on test points. Kansa outperforms other methods in both accuracy and convergence speed, achieving the least error (up to 10 −6) with only Cscale = 4 2. However, due to the increasing computational cost above Cscale = 10 2, memory limit was exceeded. 4.2.2 INVERSE PROBLEM 

For the 1D advection equation equation 45 initialized in Table 2, we set up the inverse PDE problem to infer the initial parameter β from the observation data uobs at all time steps. All methods are evaluated at their best performance from the forward problem. The results are summarized in Table 4, with the initial parameter β0 set. Table 4: Inverse predictions of β on advection equation, where the ground truth β = 0 .4.

β0 FDM PINN FNO KM β0 FDM PINN FNO KM 

0.2 0.402 0.39987 0.3985 0.402 1.0 1.000 0.40446 1.2267 0.402 

For local optimization methods when searching for the optimal parameter, they stuck at different local minima depending on the initial guess β0. With different runs of initial guesses, they give more precise predictions with more computational cost. 74.3 EXTENSION 1: K ANSA METHOD FOR COUPLED PDE S

The Lotka-Volterra equations equation 47 are initialized as per Table 5, where the number of domain quadrature points NR = 100 , and initial condition points Nd = 1 . For evaluation, the test points 

Ntest = 64 . The results from Kansa method are summarized in Table 7, where the Gaussian RBF shape parameters, as defined in equation 3, are set as ϵ = 0 .2 for both x(t) and y(t).Table 5: 1D Lotka-Volterra equations experimental setup. 

time range parameter initial conditions 

t0 = 0 , t f = 200 α = 0 .1, β = 0 .02 , δ = 0 .01 , γ = 0 .1 x(0) = 40 , y (0) = 9 

The 1D Maxwell’s equations as defined in equation 58 are initialized per Table 6, where the speed of time propagation c = 1 , the number of domain quadrature points NR = 12 × 12 , and initial condition points Nd = 24 . For evaluation, the test points Ntest = 10 × 10 . The shape parameter of Gaussian RBF, as defined in equation 3, is set as ϵx = 0 .21 and ϵy = 0 .2 for Lotka-Volterra equations and ϵE = 16 and ϵB = 16 for Maxwell’s equations, respectively. Table 6: 1D Maxwell’s equations experimental setup. 

domain time span parameter initial conditions 

x0 = 0 ,xf = 1 

t0 = 0 ,tf = 12

c = 1 Ez (x, 0) = sin(2 πx ) + 12 sin(4 πx )

By (x, 0) = cos(2 πx ) + 12 cos(4 πx )

4.3.1 FORWARD PROBLEM 

Table 7: ˆRrelative L2 error of Lotka-Volterra and Maxwell’s equations using Kansa method. 

Cscale x(t) y(t) Ez (x, t ) By (x, t )1 0.1279353 0.055667494 0.8049189 0.5894967 

4 0.04539858 0.06230465 0.4383743 0.3830594 

Accuracy . The results from Kansa method are summarized in Table 7. Both errors converge with increasing Cscale . Efficiency . The training time and inference time of Kansa method on Lotka-Volterra equations are 0.4034 and 0.0001 seconds, respectively. The training time and inference time of Kansa method on Maxwell’s equations are 0.4486 and 0.0005 seconds. 4.3.2 INVERSE PROBLEM 

For the Lotka-Volterra defined in equation 47 initialized in Table 5, we set up the inverse problem to infer the initial parameter α, β, δ and γ from observation xobs (t) and yobs (t) at all time steps. Table 8: Inverse predictions of α, β, δ and γ on Lotka-Volterra equations. 

α β δ γ α β δ γ

reference 0.1 0.02 0.01 0.1 prediction 0.102 0.0207 0.0100 0.0994 

With the initial guess all set to 1, the results are summarized in Table 8. Despite the four-dimensional search space, the optimization algorithm SciPy Powell method successfully infers the parameters with high accuracy and decent computational cost. 84.4 EXTENSION 2: K ANSA METHOD FOR NONLINEAR PDE S

The Burgers’ equation defined in equation 63 is initialized as per Table 9, where the number of domain quadrature points NR = 64 ×16 , i.e. initial condition (IC) points Nd = 64 and the boundary condition (BC) points NB = 16 × 2. For evaluation, the test points Ntest = 48 × 12 . The Gaussian RBF shape parameter, as defined in equation 3, is set as ϵ = 0 .9.Table 9: Burgers’ equation experimental setup. 

domain time span param. ICs BCs 

x0 = −10 , x f = 10 t0 = 0 , t f = 4 ν = 0 .5 per equation 74 u(x0) = 1 , u (xf ) = 0 

4.4.1 FORWARD PROBLEM 

Stability . For four time discretization schemes, only forward Euler scheme is unstable (Table 10), where time step ∆t exceeds the stability limit when Ct 

> scale

= 1 and 2 according to CFL condition. Table 10: Stability test of forward Euler Kansa method on Burgers’ equation. 

Ct 

> scale

1 2 4 10 

ˆRrelative L2 3.74 × 10 29 NaN 4.31 × 10 −3 3.11 × 10 −3

Stability unstable unstable stable stable Accuracy . From Table 11, we observe that fully non-linear approach outperforms other time-stepping schemes. It’s hard to determine whether IMEX or backward Euler is more accurate theoret-ically. However, Crank-Nicolson scheme is definitely more accurate than both IMEX and backward Euler, since it’s second-order accurate in time while the other two are only first-order accurate. Table 11: ˆRrelative L2 × 10 −2 error of Burgers’ equation using Kansa methods. forward IMEX backward Crank–Nicolson fully non-linear 

3.74 × 10 31 1.68 1.33 1.29 0.012 

Computational efficiency . We measure the training and inference time of different Kansa methods on Burgers’ equation in Table 12. The non-linear solver used is the SciPy least-squares. Table 12: Train or infer time of Burgers’ equation using Kansa methods (in seconds). forward IMEX backward Crank–Nicolson fully non-linear 

Training 0.34 0.47 2.33 1.66 99 .2

Inference 0.436 0.272 1.053 1.109 0.005 

Training time for fully nonlinear approach is longer because each step involves heavier computation with substantial memory (Table 1), further compounded by nonlinear solvers. Four time-stepping schemes have much less training time. The forward Euler is unstable when the stability condition is not satisfied. Inference time of fully non-linear approach is significantly reduced, due to the reuse of coefficient from the training phase. Despite a full test-time recomputation from scratch, the inference time of four time-stepping schemes remains acceptable for most practical applications. 94.4.2 INVERSE PROBLEM 

For the Burgers’ equation defined in equation 63 initialized in Table 9, we set up the inverse PDE problem to infer the initial parameter ν from the observation data uobs at all time steps. The results are summarized in Table 13, with the initial parameter ν0 set as 0.1.Table 13: Inverse predictions of ν on Burgers’ equation, where the ground truth ν = 0 .5.forward IMEX backward Crank–Nicolson fully non-linear 

0.388 0.535 0.467 0.502 0.500 

Accuracy . Under same optimizer and initial guess, the Crank-Nicolson scheme confirms its the-oretical advantage (Table 1) over both IMEX and backward Euler, which stuck at local minima. 

Computational efficiency . Fully non-linear approach requires retraining for each new parameter, which is computationally expensive (Table 12). To speed up the per-run training time, it is trained with a maximum iteration. Stability . The forward Euler scheme is unstable when given large ∆t.

## 5 CONCLUSIONS 

This paper extends Zhong et al. (2023) RBF framework solver beyond original scope of linear PDEs. In particular, we generalize its PDE solver to handle multi-dependent-variable and nonlinear 

PDEs, addressing the loss property of linear reordering. These broaden the applicability of CNF-driven self-tuning mesh-free solvers to both forward modeling and inverse problem formulations. In addition, this work contributes a systematic empirical study of how CNF solvers compare with established classical and neural PDE solvers. By implementing representative prior methods and evaluating them across benchmark problems, we assess their relative performance in terms of solu-tion accuracy, efficiency, convergence and complexity. Such comparisons clarify the strengths and limitations of CNF-based approaches within the broader landscape of PDE solvers. Overall, this paper demonstrates that learning-guided Kansa solvers can serve as a promising and flexible tool for coupled or nonlinear PDE systems. Future work includes theoretical analysis of error and convergence properties, application to neural field in computing, and integration with differentiable pipelines in scientific domains. 10 REFERENCES 

Robert A. Adams and John J. F. Fournier. Sobolev spaces , volume 140 of Pure and Applied Math-ematics . Academic Press, Boston, MA, 2 edition, 2003. ISBN 978-0-12-044143-3. Originally published in 1975. Michael Athanasopoulos, Hassan Ugail, and Gabriela Gonz´ alez Castro. Parametric design of aircraft geometry using partial differential equations. Advances in Engineering Software , 40(7):479–486, 2009. ISSN 0965-9978. doi: https://doi.org/10.1016/j.advengsoft.2008.08.001. URL https: //www.sciencedirect.com/science/article/pii/S0965997808001531 .Nicolas Baca¨ er. Lotka, Volterra and the predator–prey system (1920–1926). In A Short History of Mathematical Population Dynamics , pp. 71–76. Springer, London, 2011. doi: 10.1007/ 978-0-85729-115-8 13. URL https://doi.org/10.1007/978-0-85729-115-8_ 13 .Adam W. Bargteil and Tamar Shinar. An Introduction to Physics-based Animation. ACM SIG-GRAPH 2018 Courses , 1(1):1–57, August 2018. doi: 10.1145/3214834.3214849. HARRY BATEMAN. Some recent researches on the motion of fluids. Monthly Weather Review , 43(4):163 – 170, 1915. doi: 10.1175/1520-0493(1915)43 ⟨163:SRROTM ⟩2.0. CO;2. URL https://journals.ametsoc.org/view/journals/mwre/43/4/ 1520-0493_1915_43_163_srrotm_2_0_co_2.xml .Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in Machine Learning: a survey. Journal of Machine Learning Research ,18(153):1–43, 2018. URL http://jmlr.org/papers/v18/17-468.html .Richard E. Bellman. Dynamic programming . Princeton University Press, Princeton, NJ, 1957. ISBN 978-0-691-07951-6. Prepared for the Rand Corporation. Richard Courant, Kurt Friedrichs, and Hans Lewy. ¨Uber die partiellen Differenzengleichungen der mathematischen Physik. Mathematische Annalen , 100(1):32–74, 1928. doi: 10.1007/ BF01448839. L.C. Evans. Partial Differential Equations . Graduate studies in mathematics. American Mathemati-cal Society, 2010. ISBN 9780821849743. URL https://books.google.co.uk/books? id=Xnu0o_EJrCQC .Walter Greiner. Maxwell’s equations , pp. 250–275. Springer New York, New York, NY, 1998. ISBN 978-1-4612-0587-6. doi: 10.1007/978-1-4612-0587-6 13. URL https://doi.org/ 10.1007/978-1-4612-0587-6_13 .Eberhard Hopf. The partial differential equation ut + u u x = μ u xx . Communications on Pure and Applied Mathematics , 3(3):201–230, 1950. doi: https://doi.org/10.1002/cpa.3160030302. URL 

https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160030302 .Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural Networks , 2(5):359–366, January 1989. doi: 10.1016/ 0893-6080(89)90020-8. Arieh Iserles. A first course in the Numerical Analysis of Differential Equations . Cambridge Uni-versity Press, Cambridge, 2 edition, 2008. ISBN 978-0-521-73490-5. URL http://www. cambridge.org/9780521734905 .E. J. Kansa. Multiquadrics—a scattered data approximation scheme with applications to com-putational fluid dynamics—II solutions to parabolic, hyperbolic and elliptic partial differen-tial equations. Computers & Mathematics with Applications , 19(8–9):147–161, 1990. doi: 10.1016/0898-1221(90)90271-K. Gitta Kutyniok. The Mathematics of Artificial Intelligence, 2022. URL https://arxiv.org/ abs/2203.08890 .11 Jean le Rond D’Alembert. Recherches sur la courbe que forme une corde tendu¨ ee mise en vibration. 

Histoire de l’acad´ emie royale des sciences et belles lettres de Berlin , 3:214–219, 1747. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, An-drew Stuart, and Anima Anandkumar. Neural Operator: Graph Kernel Network for Partial Differ-ential Equations. In ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations (ODE/PDE+DL) , 2020. URL https://arxiv.org/abs/2003.03485 . Poster presentation. Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. Physics-Informed Neural Operator for learning Partial Differential Equations. ACM / IMS J. Data Sci. , 1(3), May 2024. doi: 10.1145/3648506. URL 

https://doi.org/10.1145/3648506 .Giuseppe Orlando and Mario Sportelli. Growth and cycles as a struggle: Lotka–Volterra, Goodwin and Phillips. In Giuseppe Orlando, Alexander N. Pisarchik, and Ruedi Stoop (eds.), Nonlinearities in Economics: An Interdisciplinary Approach to Economic Dynamics, Growth and Cycles , pp. 191–208. Springer International Publishing, Cham, 2021. doi: 10.1007/978-3-030-64234-0 10. URL https://doi.org/10.1007/978-3-030-64234-0_10 .S. V. Patankar. Numerical heat transfer and fluid flow . Taylor & Francis, 1980. ISBN 978-0-89116-522-4. Patrick P´ erez, Michel Gangnet, and Andrew Blake. Poisson image editing. In ACM SIGGRAPH 2003 Papers , pp. 313–318, New York, NY, USA, 2003. Association for Computing Machinery. ISBN 1-58113-709-5. doi: 10.1145/1201775.882269. URL https://doi.org/10.1145/ 1201775.882269 .M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equa-tions. Journal of Computational Physics , 378:686–707, 2019. ISSN 0021-9991. doi: https://doi. org/10.1016/j.jcp.2018.10.045. URL https://www.sciencedirect.com/science/ article/pii/S0021999118307125 .Walter Rudin. Principles of Mathematical Analysis . McGraw-Hill, New York, 3 edition, 1976. ISBN 978-0-07-054235-8. Tim De Ryck and Siddhartha Mishra. Error analysis for physics-informed neural networks (PINNs) approximating Kolmogorov PDEs. Advances in Computational Mathematics , 48(6):79, 2022. ISSN 1572-9044. doi: 10.1007/s10444-022-09985-9. URL https://doi.org/10.1007/ s10444-022-09985-9 .Andrew Spielberg, Fangcheng Zhong, Kwang Moo Rematas, and et al. Differentiable visual computing for inverse problems and machine learning. Nature Machine Intelligence , 5:1189– 1199, 2023. doi: 10.1038/s42256-023-00743-0. URL https://doi.org/10.1038/ s42256-023-00743-0 .Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani, Dirk Pfl¨ uger, and Mathias Niepert. PDEBench: an extensive benchmark for scientific machine learn-ing. In Proceedings of the 36th International Conference on Neural Information Processing Sys-tems , NIPS ’22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. Sifan Wang, Hanwen Wang, and Paris Perdikaris. On the eigenvector bias of fourier feature net-works: From regression to solving multi-scale pdes with physics-informed neural networks. Com-puter Methods in Applied Mechanics and Engineering , 384:113938, 2021. ISSN 0045-7825. doi: https://doi.org/10.1016/j.cma.2021.113938. URL https://www.sciencedirect.com/ science/article/pii/S0045782521002759 .Dmitry Yarotsky. Optimal approximation of continuous functions by very deep ReLU networks. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Annual Conference on Learning Theory , volume 75 of Proceedings of Machine Learning Research , pp. 1– 11, Stockholm, Sweden, 2018. PMLR. URL https://proceedings.mlr.press/v75/ yarotsky18a.html .12 Tjalling J. Ypma. Historical development of the newton–raphson method. SIAM Review , 37(4): 531–551, 1995. doi: 10.1137/1037125. URL https://doi.org/10.1137/1037125 .Fangcheng Zhong, Kyle Fogarty, Param Hanji, Tianhao Wu, Alejandro Sztrajman, Andrew Spiel-berg, Andrea Tagliasacchi, Petra Bosilj, and Cengiz Oztireli. Neural fields with hard constraints of arbitrary differential order. In Advances in Neural Information Processing Systems (NeurIPS) ,2023. 13 A LINEAR OPERATOR 

A linear operator Iserles (2008) is a function F : V → W that maps one vector space V ∈

R to another, or itself 5, W ∈ R, and preserving the operations of vector addition and scalar multiplication , also known as homogeneity . Thus, for all vectors ui ∈ V and all scalars c, the following features hold: 

F(

> n

X

> i=1

ui) = 

> n

X

> i=1

F(ui), vector additivity ,

F(c · u) = c · F (u), scalar multiplication .

(36) Linear operators are fundamental in Linear Algebra for processing matrices, Quantum Mechanics for observables, Machine Learning, and Signal Processing. This forms the basis for Kansa method for linear PDEs. Here are several commonly used examples of linear operators below, among which some are used in this work for PDE solver algorithms. • Matrix multiplication : For a matrix A, the function A : Rn → Rm is a linear operator, 

F(x) = Ax. (37) • Integral operator : The operator that integrates a function over a fixed interval [a, b ] is a linear operator, 

I(f ) = 

Z ba

f (x) dx. (38) • Differentiation : The operator taking the derivative in a function space is a linear operator, because differentiation preserves addition and scalar multiplication, 

Dx(f ) = ∂f ∂x . (39) • Gradient operator : In multivariable calculus, the gradient operator ∇ is a linear operator that maps a scalar field to a vector field, 

∇f =

 ∂f ∂x , ∂f ∂y , ∂f ∂z 



. (40) • Divergence operator : In vector calculus, the divergence operator ∇· is a linear operator that maps a vector field to a scalar field, 

∇ · F = ∂F x

∂x + ∂F y

∂y + ∂F z

∂z . (41) • Laplace operator : In the context of partial differential equations, the Laplace operator ∆

is a linear operator that maps a scalar field to another scalar field, 

∆f = ∇ · (∇f ) = ∇2f = ∂2f∂x 2 + ∂2f∂y 2 + ∂2f∂z 2 . (42) • Curl operator : In vector calculus, the curl operator ∇× is a linear operator that maps a vector field to another vector field, 

∇ × F =

 ∂F z

∂y − ∂F y

∂z , ∂F x

∂z − ∂F z

∂x , ∂F y

∂x − ∂F x

∂y 



=

ˆi ˆj ˆk

> ∂∂x ∂∂y ∂∂z

Fx Fy Fz

. (43)         

> 5If the domain and codomain are the same vector space, i.e., F:V→V, it’s called a linear transformation or operator on V.

14 B PARTIAL DIFFERENTIAL EQUATIONS 

B.1 BOUNDARY AND INITIAL CONDITIONS (BC S AND IC S)Since solution to differential equations contain integration constants, which is non-unique, addi-tional conditions are required to enforce uniqueness. The boundary conditions (BCs) specify the function u behavior on the domain boundary ∂Ω, whereas the initial conditions (ICs) from time scale perspective are given at t = 0 . The formulation is defined in equation 1. There are some common boundary conditions, defined over the boundary Ω = [ x0, x f ] in 1D space, where {gi}4 

> i=1

are given closed-form functions, Zero BC: u(x0, t ) = 0 , u (xf , t ) = 0 ,

Dirichlet BC: u(x0, t ) = g1(t), u (xf , t ) = g2(t),

von Neumann BC: ∂u ∂x (x0, t ) = g3(t), ∂u ∂x (xf , t ) = g4(t).

(44) B.2 SUMMARY OF PDE S

Table 14: Summary of PDEs with different characteristics. 

Equation Domains Linearity Solution dim. 

Advection Physics, Graphics Linear 1Wave Physics, Graphics Linear 1Lotka-Volterra Biology Linear 2

Maxwell Physics Linear 2

Burgers Physics, Graphics Nonlinear 1B.3 1D A DVECTION EQUATION 

The advection equation Takamoto et al. (2022) models the linear transport of a scalar quantity 

u(x, t ), which is changed over time t and space x, as follows: 

( ∂u (x,t ) 

> ∂t

+ β ∂u (x,t ) 

> ∂x

= 0 , x ∈ [x0, x f ], t ∈ [t0, t f ],u(x, 0) = u0(x), x ∈ [x0, x f ], initial condition , (45) where parameter β ∈ R is the advection velocity, and u0(x) is the initial condition given at t = 0 .The analytical solution of equation 45 is, 

u(x, t ) = u0(x − βt ). (46) The positivity of parameter β indicates the direction of wave propagation. From equation 46, when 

β > 0, the wave propagates rightwards, and vice versa. The solution is visualized in Figure 1, with initial condition u0(x) = sin(2 πx ), x ∈ [0 , 1] .

Figure 1: Advection equation solution visualization in 1D, 2D and 3D. 15 B.4 LOTKA -V OLTERRA PREDATOR -PREY MODEL 

Lotka-Volterra predator-prey model Baca¨ er (2011) relates the populations of prey x(t) and predators 

y(t) at time t in a dynamic biological system via coupled differential equations, also applicable to other fields, e.g. the unemployment rate with respect to wage growth Orlando & Sportelli (2021) and many more, (

x′(t) := dx (t) 

> dt

= αx (t) − βx (t) · y(t),y′(t) := dy (t) 

> dt

= δx (t) · y(t) − γy (t). , t ∈ [t0, t f ]. (47) where α is the prey growth rate, β is the predation rate, δ is the ratio of neonate predators to eaten prey, and γ is the predator death rate. It assumes that there would be unlimited food supply for the prey, and thus exponential growth αx (t). The multiplicative term x(t) · y(t) represents the encounters between prey and predators statistically. The system has no explicit analytical solution, but the implicit solution exists. After scaling of variables, 

x∗(t) = δγ x(t), y∗(t) = βα y(t), τ = αt, (48) By plugging into equation 47, and dividing the first equation by the second, 

dy ∗

dx ∗ = γα · y∗(x∗ − 1) 

x∗(y∗ − 1) , (49) The implicit solution is given by integration separation of variables, for which CL-V ∈ R is the integration constant, 

ln( y∗) − y∗ − γα [ln( x∗) − x∗] = CL-V . (50) Figure 2 shows the solution with phase space given by the above implicit solution, which depends on the initial conditions x(0) and y(0) . 

> (a) Prey and predator solution populations (b) Phase space trajectory

Figure 2: Lotka-Volterra predator-prey model solution and phase space. B.5 MAXWELL ’S EQUATIONS 

In electromagnetism, Maxwell’s equations Greiner (1998) relate the electric field E(r, t ) and mag-netic field B(r, t ) with spatial position r ∈ R3 and time t, to the electric charge density ρ(r, t ) ∈ R

and current density J(r, t ) ∈ R3. The differential form is as follows, 



∇ · E = ρϵ0 , Gauss’s law ,

∇ · B = 0 , Gauss’s law for magnetism ,

∇ × E = − ∂B 

> ∂t

, Faraday’s law of induction ,

∇ × B = μ0J + μ0ϵ0 ∂E 

> ∂t

, Amp` ere-Maxwell law ,

(51) where constants μ0, ϵ 0 ∈ R+ are the vacuum permeability and permittivity respectively. Their product is the reciprocal of the square of the speed of light c ≈ 3 × 10 8 m s −1 in vacuum, 

μ0ϵ0 = 1

c2 . (52) 16 The first two equations state that the electric field E sourced by electric charges, and no magnetic monopoles exist. The last two equations depict how a time-varying magnetic field B induces an electric field E, and vice versa with the addition of current density J. In general, Maxwell’s equa-tions are linear with respect to E and B.Taking the curl of Faraday’s law and Amp` ere-Maxwell law respectively, 

∇ × (∇ × E) = − ∂∂t (∇ × B),

∇ × (∇ × B) = μ0(∇ × J) + μ0ϵ0 ∂∂t (∇ × E). (53) By vector calculus identity of ∇ × (∇ × E) = ∇(∇ · E) − ∆E, and plugging in Amp` ere-Maxwell law on the right-hand side of the first equation, 

(

∇(∇ · E) − ∆E = −μ0 ∂J 

> ∂t

− μ0ϵ0 ∂2E 

> ∂t 2

,

∇(∇ · B) − ∆B = μ0(∇ × J) + μ0ϵ0 ∂∂t (∇ × E). (54) By substituting Gauss’s law for ∇ · E in the first equation, Gauss’s law for magnetism for ∇ · B

and Faraday’s law for ∇ × E in the second equation, the two equations after rearrangement are inhomogeneous, i.e. including source terms F(r, t ), wave equations, taking the forms of c2∆u −

utt = F, (

∆E − μ0ϵ0 ∂2E 

> ∂t 2

= ∇( ρϵ0 ) + μ0 ∂J 

> ∂t

,

∆B − μ0ϵ0 ∂2B 

> ∂t 2

= −μ0(∇ × J). (55) To simplify the problem, we take the one-dimensional (1D) electromagnetic wave propagating along 

x-axis without sources, i.e. ρ = 0 and J = 0 , with the electric field E(r, t ) = (0 , 0, E z (x, t )) along 

z-axis and magnetic field B(r, t ) = (0 , B y (x, t ), 0) along y-axis respectively. By expanding the defintion of curl ∇× operators, and removing the zero terms, 

∇ × E =

 ∂E z

∂y − ∂E y

∂z , ∂E x

∂z − ∂E z

∂x , ∂E y

∂x − ∂E x

∂y 



=



0, − ∂E z

∂x , 0



, (56) thus the reduced last two equations of Maxwell’s equations equation 51 are, 

( ∂E z 

> ∂x

= − ∂B y 

> ∂t

, 

> ∂B y
> ∂x

= −μ0ϵ0 ∂E z 

> ∂t

. (57) By taking partial derivatives ∂x and ∂t, and simplifying, the 1D wave solutions are 6,

( ∂2Ez 

> ∂x 2

− μ0ϵ0 ∂2Ez 

> ∂t 2

= 0 , 

> ∂2By
> ∂x 2

− μ0ϵ0 ∂2By 

> ∂t 2

= 0 . (58) The initial conditions at t = 0 are given as follows, 

Ez (x, 0) = f (x), By (x, 0) = g(x), x ∈ [x0, x f ]. (59) Let u = Ez + By and v = Ez − By and with change of variables xt = xt=0 ± ct , where c defined in equation 52 is the speed of light in vacuum. According to d’Alembert’s formula le Rond D’Alembert (1747), u(x, t ) = u(x − ct, 0) = f (x − ct ) + g(x − ct ),v(x, t ) = v(x + ct, 0) = f (x + ct ) − g(x + ct ). (60) By reversing the change of variables, the analytical solutions to equation 58 are, 

Ez = 12 (u + v) = 12 [f (x − ct ) + f (x + ct )] + 12 [g(x − ct ) − g(x + ct )] ,By = 12 (u − v) = 12 [f (x − ct ) − f (x + ct )] + 12 [g(x − ct ) + g(x + ct )] . (61) The solution is visualized in Figure 3, with initial condition given as, 

f (x) = sin(2 πx ) + 0 .5 sin(4 πx ), g (x) = cos(2 πx ) + 0 .5 cos(4 πx ) x ∈ [0 , 1] , t ∈ [0 , 0.5] . (62) 

> 6

The 1D wave equation aligned with the general inhomogeneous wave equation equation 55, with F = 0 .

17 Figure 3: Maxwell’s equations solution visualization in 1D, 2D and 3D. B.6 VISCOUS BURGERS ’ EQUATION 

Viscous Burgers’ equation Takamoto et al. (2022) captures both non-linear advection, also known as convection and diffusion phenomena in dynamics, 

( ∂u (x,t ) 

> ∂t

+ u(x, t ) ∂u (x,t ) 

> ∂x

= ν ∂2u(x,t ) 

> ∂x 2

, x ∈ [x0, x f ], t ∈ [t0, t f ],u(x, 0) = u0(x), x ∈ [x0, x f ], initial condition , (63) where viscosity ν ∈ R+ is the positive constant, and u0(x) is the initial condition given at t = 0 .By Cole-Hopf transformation Hopf (1950), unknown function u(x, t ) is converted into ϕ(x, t ) via, 

u(x, t ) = −2ν ∂∂x ln ϕ(x, t ) = −2ν 1

ϕ(x, t )

∂ϕ (x, t )

∂x ≡ − 2ν ϕx

ϕ . (64) By chain rule and quotient rule of differentiation, the first-order and second-order spatial or temporal derivatives of u(x, t ) are, 

∂u (x, t )

∂x = 2 ν

 ϕ2

> x

ϕ2 − ϕxx 

ϕ



, ∂u (x, t )

∂t = 2 ν

 ϕxϕt

ϕ2 − ϕxt 

ϕ



,∂2u(x, t )

∂x 2 = 2 ν

 3ϕxϕxx 

ϕ2 − 2ϕ3

> x

ϕ3 − ϕxxx 

ϕ



.

(65) By plugging equation 65 into equation 63 and simplifying, 

2ν

 ϕxϕt

ϕ2 − ϕxt 

ϕ − ν ϕxϕxx 

ϕ2 + ν ϕxxx 

ϕ



= 0 , x ∈ [x0, x f ], t ∈ [t0, t f ], (66) With the inversion of quotient rule, equation 66 is rearranged as, 

2ν ∂∂x 

 νϕ xx − ϕt

ϕ



= 0 , x ∈ [x0, x f ], t ∈ [t0, t f ]. (67) By integrating equation 67 with respect to x and introducing an integration function f (t),

νϕ xx − ϕt

ϕ = f (t), x ∈ [x0, x f ], t ∈ [t0, t f ]. (68) Now introduce f (t) = dF (t) 

> dt

and ˜ϕ = ϕ · eF (t), thus the derivatives of ˜ϕ are, 

∂ ˜ϕ∂t = eF (t)



ϕt + ϕ dF (t)

dt 



, ∂2 ˜ϕ∂x 2 = eF (t)ϕxx , (69) 18 by plugging them into equation 68. The resulting equation is reduced to the standard heat equation, 

ν ∂2 ˜ϕ(x, t )

∂x 2 − ∂ ˜ϕ(x, t )

∂t = 0 , x ∈ [x0, x f ], t ∈ [t0, t f ]. (70) The solution of equation 70 is formed by heat kernel Φ( x, t ) convolved with the initial condition 

˜ϕ0(x) = ˜ϕ(x, 0) Evans (2010), 

˜ϕ(x, t ) = 

Z ∞−∞ 

Φ( x − x′, t ) ˜ϕ0(x′) dx ′, where Φ( x, t ) = 1

√4πνt e− x24νt . (71) Note that the transformation from ϕ to ˜ϕ does not change the Cole-Hopf transformation equation 64, since the additional multiplicative term eF (t) is independent of x,

u(x, t ) = −2ν ∂∂x ln ϕ(x, t ) = −2ν ∂∂x ln ˜ϕ(x, t ). (72) From the Cole-Hopf equation 72 at t = 0 and via integration, the initial condition for ˜ϕ(x, 0) is thus, 

˜ϕ0(x) = − 12ν

Z x

> 0

u0(x′) dx ′. (73) The analytical solution of equation 63 is thus plugging equation 73 into equation 71 and then into equation 72. In this work, we consider when u0(−∞ ) and u0(∞) exist and u′

> 0

(x) < 0 for all x ∈ R, the explicit expression BATEMAN (1915) is then a steadily propagating wave as below, 

u(x, t ) = c−∆u0 tanh 

 ∆u0

2ν (x − ct )



, where c = u0(−∞ ) + u0(∞)2 , ∆u0 = u0(−∞ ) − u0(∞)2 .

(74) The solution is visualized in Figure 4, with initial condition set by equation 74, u0(−∞ ) = 1 ,

u0(∞) = 0 , and ν = 0 .5.

Figure 4: Burgers’ equation solution visualization in 1D, 2D and 3D. B.7 PDE SOLVERS 

There are many attempts to solve the PDE solution field u. Among which, we categorize PDE solvers into two types, i.e. numerical analysis methods and neural-based methods. 

Constrained optimization. In PDE solvers, constraints are boundary conditions, initial conditions, or PDE residuals. They can be in the form of either soft or hard constraints. The former is the cost functions that are penalised, while the latter is that can not be violated, e.g. (in)equality forms. Table 15: Summary of different PDE solvers. 

method motivation training supervised constraint 

FDM grid-based × N/A hard PINN physics-driven ✓ × soft 19 method motivation training supervised constraint 

NO data-driven ✓ ✓ soft - FNO data-driven ✓ ✓ soft - PINO hybrid ✓ ✓ soft KM mesh-free grid × N/A hard - CNF KM on neural fields ✓ × hard B.7.1 FINITE DIFFERENCE METHOD (FDM) Considering a discretized sequence u ∈ RN ×M of the continuous function u(x, t ) as in equation 45. Along the spatial dimension x and temporal dimension t, there are N and M sampled points respec-tively. The finite difference operators Iserles (2008) defined on per element, are as follows: 

(∆ u)i =



(∆ +u)i = uji+1 − uji , forward difference. 

(∆ −u)i = uji − uji−1, backward difference. 

(∆ 0u)i = uji+1 − uji−1

2 , central difference. 

, (75) for which i ∈ { 0, 1, . . . , N − 1} is the spatial index , and j ∈ { 0, 1, . . . , M − 1} is the temporal index of the sequence u.The partial equations often involve full and/or partial derivatives, where differential operators can be discretized into difference operators equation 75 via finite difference method Bargteil & Shinar (2018). By Taylor expansion of u(x ± ∆x, t ) around u(x, t ) up to the first order error, the corre-sponding examples for the spatial derivative are, 

∂u ji (x, t )

∂x =

 

> (∆ −u)i
> ∆x

+ O(∆ x) ≈ (∆ −u)i 

> ∆x

= uji −uji−1 

> ∆x

if β > 0, 

> (∆ +u)i
> ∆x

+ O(∆ x) ≈ (∆ +u)i 

> ∆x

= uji+1 −uji 

> ∆x

if β < 0., upwind scheme. 

= (∆ 0u)i

∆x + O(∆ x2) ≈ (∆ 0u)i

∆x = uji+1 − uji−1

2∆ x , central difference. (76) where ∆x is the spatial spacing, with spatial index i and temporal index j defined above. The 

upwind scheme Patankar (1980) considers where the information comes from, e.g. when β > 0,the wave propagates rightwards, and thus uji is influenced by uji−1, and vice versa for downwind scheme. Under the upwind scheme, the advection equation equation 45 is therefore as the following ODE, 

∂u (x, t )

∂t + β[Iβ> 0

(∆ −u)i

∆x + Iβ< 0

(∆ +u)i

∆x ] = 0 , (77) where the indicator function Iβ> 0 =

1 if β > 0,

0 otherwise. is for controlling different cases of β7.By forward Euler method for ODEs, the temporal derivative is discretized via the forward differ-ence operator. After which, the advection equation equation 45 is simplified as, with ∆t being the temporal spacing, 

uj+1  

> i

− uji

∆t + β[Iβ> 0

(∆ −u)i

∆x + Iβ< 0

(∆ +u)i

∆x ] = 0 . (78) With algebraic reordering, the upwind scheme update rule is thus, 

uj+1  

> i

= uji − β∆t

∆x [Iβ> 0(∆ −u)i + Iβ< 0(∆ +u)i]. (79) 

Stability condition. For implicit numerical schemes, e.g. the backward Euler method, the solution is unconditionally stable. However, for explicit numerical schemes, e.g. the forward Euler method above, stability conditions must be satisfied to avoid numerical instability, which we briefly intro-duce below. 

> 7

Alternatively, one may use max( β, 0) and min( β, 0) to replace βIβ> 0 and βIβ< 0 respectively. 

20 In the 1D space, the scalar Courant number C, also known as the CFL stability criteria, measures the ratio of how far the wave propagates in one time interval ∆t to the spatial spacing ∆x. The CFL condition Courant et al. (1928) states that C must satisfy, 

C = |β|∆t

∆x ≤ Cmax , (80) where Cmax is a problem-dependent constant. It sets the maximum allowable time step ∆t for a given ∆x, for numerical stability. B.7.2 PHYSICS -INFORMED NEURAL NETWORK (PINN) Physics-informed neural network (PINN) Raissi et al. (2019) is a data-driven approach for func-tional PDE approximation, which requires a large labeled dataset but has the ability to generalize. Consider the general form of PDEs defined in equation 1, PINNs approximate the unknown solution 

u(x, t ) ∈ U with a neural network ˆuθ (x, t ) ∈ U , i.e. ˆuθ (x, t ) ≈ u(x, t ), parameterized by updatable parameters θ ∈ Θ.

Residual Rθ of the PDEs is calculated without supervised data for the neural network ˆuθ , which is minimized via automatic differentiation Baydin et al. (2018) 8 during training for generalizability, 

Rθ (x, t ) ∈ Y = D[ˆ uθ ]( x, t ) − f (x, t ), x ∈ Ω, t ∈ [t0, t f ]. (81) The residual loss LR9, also known as the physics-informed loss, is defined to be the p-norm of the residual Rθ in equation 81. During training, NR quadrature points are sampled, where the integral loss is approximated by the discretized loss LR with weights ωk at each sample index k and training error ET (θ),

LR := ( ||R θ || p)p := [( 

Z

> D

|R θ |p dx dt ) 1 

> p

]p

| {z } 

> integral LR

=

Z

> D

|R θ |p dx dt 

By quadrature, =

> NR

X

> k=1

ωk|R θ (xk, t k)|p

| {z } 

> discretized LR

+ET (θ) ≈ L R, where ET (θ) = LR − L R.

(82) If considering the boundary conditions, the residual for the i-th boundary condition RBi 

> θ

is calculated via equation 1 as well, after which the boundary condition loss LBC is defined accordingly, 

RBi 

> θ

(x, t ) ∈ Z i = Bi[ˆ uθ ]( x, t ) − gi(x, t ), x ∈ ∂Ωi, t ∈ [t0, t f ]. (83) As defined in equation 99, the total error between the optimal solution from the network ˆuθ and the ground truth u is, by expanding equation 97, 

EPINN (θ) = ( || ˆuθ − u|| p)p. (84) During training, the network is optimized on supervised dataset {(xn, t n), u (xn, t n)}Nd

> n=1

, with Nd

being the total number of data. The supervised loss Ldata 10 approximates the total error equation 84, 

Ldata = 1

NdNdX

> n=1

(|ˆuθ (xn, t n) − u(xn, t n)|p). (85) 

Training . PINN approximates the solution as ˆuθ = uθopt (x, t ). To avoid overfitting due to the 

limited supervised data, the main goal is to minimize the unsupervised residual error LR equa-tion 82. With the addition of the supervised loss equation 85 and the boundary condition residual 

> 8

Example of ∂u ∂x : u x = torch.autograd.grad(outputs=u, inputs=x, create graph=True)[0] 

> 9

Note that LR is the same as the risk R defined in equation 97, but for the residual Rθ instead of the solution 

u. 10 Note that when the supervised data is only sampled on the boundary, the supervised loss and the boundary condition loss are the same. 

21 equation 83, the optimized theta is θopt ≈ arg min θ∈Θ L, where the total training loss LPINN is, 

LPINN =

> NR

X

> k=1

ωk|R θ (xk, t k)|p

| {z } 

> Discretized residual loss LR

+λ1

1

NdNdX

> n=1

(|ˆuθ (xn, t n) − u(xn, t n)|p)

| {z } 

> Supervised loss Ldata

+λ2

X

> iNBi

X

> b=1

ωBi 

> b

|R Bi 

> θ

(xb, t b)|p

| {z } 

> BC loss LBC

,

(86) with weights ωBi 

> b

at each sample index b for i-th boundary condition and regularization parameters 

λ1, λ 2 > 0 for combining different losses. 

Algorithm 1 Physics-Informed Neural Network training pseudocode. 

1: Input: Initial parameters θ for network ˆuθ .

2: Output: Optimized parameters θopt for network ˆuθ .

3: Hyperparameters: Learning rate η, number of training iterations Niter .

4: while number of iterations < N iter do 

5: Sample PDE points xk ∈ Ω, t k ∈ [t0, t f ] and boundary points xb ∈ ∂Ωi, t b ∈ [t0, t f ].

6: Compute the network outputs ˆuθ and their derivatives D[ˆ uθ ] and boundary Bi[ˆ uθ ].

7: Compute loss LPINN = LR + Ldata + LBC by equation 86. 

8: By gradient descent, update θ ← θ − η ∇θ L.

9: end while 

B.7.3 NEURAL OPERATOR (NO) 

Operator learning . From the general form of PDEs equation 1, we assume that within the D

operator, the source function f or the initial conditions, there is a parameter a of the same dimension as the solution u. In this subsection, we denote the differential operator D as Da, where the PDE is thus Da[u] = f .Given the dataset {(aji , f ji ), u ji |i = 1 , ..., N R}Npde  

> j=1

with Npde PDE instances each NR quadrature points, the idea of operator learning Li et al. (2020) is to learn the operator G mapping input a ∈A : D → R to the solution u ∈ U : D → R, i.e. G(a, f ) = u connecting two function spaces A

and U with infinite dimensions, which is challenging for neural networks since they are for finite 

dimensions instead. 

Solution A. To solve this challenge, one solution is to parameterize the PDE u = u(t, x, μ ),assuming that a is measureable in finite dimension, i.e. a = a(μ), μ ∈ Rdy . It’s a techinique widely used in aircraft design and manufacturing Athanasopoulos et al. (2009), image processing P´ erez et al. (2003). The training process is therefore to minimize the supervised loss Ldata from data with p-norm, which measures how much the predicted solution Gθ (ai, f i) deviates from the ground truth ui for each data point i,

Ldata = 1

Npde × NR

> Npde

X

> j=1
> NR

X

> i=1

(||G θ (aji , f ji ) − uji || p)p. (87) The approximated solution is therefore ˆuθ = Gθopt (a, f ), where θopt ≈ arg min θ∈Θ Ldata . There is no addition of the PDE residual loss LR equation 81 as in PINNs for basic operator learning. An extension, termed as physics-informed neural operator (PINO) Li et al. (2024), combines the supervised loss Ldata and the residual loss LR as the total training loss, 

LPINO = Ldata + λ 1

Npde × NR

> Npde

X

> j=1
> NR

X

> i=1

(||D a[Gθ (aji , f ji )] − f ji (xi, t i)|| p)p

| {z } 

> Residual loss LR

, (88) Despite the simplicity in ideas, the parameterization suffers from how to sample from the given space, non-uniqueness, and low generalization to unseen a.

Solution B. Interpolation from the discretized grid, including a neural network based interpolator or traditional methods (linear, cubic, spline, etc). However, it suffers from inconsistency between the discretized and continuous functions. 22 Solution C. Generalize the neural network from discrete to continuous function space. 

(Nlv)( x) = σ[Alv(x) + Bl(x) + 

Z

> D

Kl(x, y )v(y) dy ], x ∈ D. (89) Fast implemenentation via FFT. 

## C ARCHITECTURE DETAILS 

We adopt a multi-scale feed-forward neural network architecture for PINN Wang et al. (2021), which augments the original feed-forward architecture with input encoding layers of multiple frequency scales. There are three hidden layers each with 64 neurons. For FNO, we adopt the architecture with 16 modes retained in the spectral convolution layer, and the latent feature dimension is 64. 

## D ENVIRONMENT SETUP 

All the measurements are conducted on a Mac M1, with a single-core CPU running at 3.2 GHz. In the following sections, the data points are uniformly sampled unless otherwise specified. 

## E LEARNING THEORY 

E.1 FUNCTIONAL ANALYSIS 

We introduce basic functional analysis concepts here for PDE solvers and later learning theory (Appendix § E). The p-norm of a function f : Ω ⊆ Rd → R is defined as, 

|| f || p = ( 

Z

> Ω

|f (x)|p dx ) 1 

> p

, for 1 ≤ p < ∞. (90) The p-integrable function f ∈ Lp(Ω) , is defined as 

(|| f || p)p =

Z

> Ω

|f (x)|p dx < ∞. (91) For additional concepts used for learning theory, please refer to Appendix § E.2. E.2 FUNCTIONAL ANALYSIS ADDENDUM 

Smoothness Rudin (1976) of a function is defined to be the number of continuous derivatives it has. The class of function f with smoothness k ∈ N+ has at least a k-th derivative, and is denoted as 

f ∈ Ck,

Ck(Ω) = {f : Ω ⊆ Rd → R | ∀ α ≤ k. ∂ αf exists and is continuous }. (92) When k = ∞, the function is differentiable at all orders. While not every function is not smooth, there is a generalization of smooth functions, i.e. Sobolev functions. The weak derivative f ′ generalizes to include functions that are not differentiable, but locally integrable on bounded domain [a, b ]. The f ′ definition is for all smooth test functions ϕ, with 

ϕ(a) = ϕ(b) = 0 ,

Z ba

f (x)ϕ′(x) dx = [ f (x)ϕ(x)] ba −

Z ba

f ′(x)ϕ(x) dx, by integration by parts ,

= −

Z ba

f ′(x)ϕ(x) dx, as ϕ(a) = ϕ(b) = 0 .

(93) 

Sobolev spaces Adams & Fournier (2003) W k,p (Ω) is a function space where all functions f having weak derivatives up to order k and every derivate is p-integrable via equation 91, 

W k,p (Ω) ⊂ Lp(Ω) = {f : Ω ⊆ Rd → R | ∀ α ≤ k. ∃ ∂αf ∈ Lp(Ω) }, (94) When k = 2 , it forms a Hilbert space, i.e. W k, 2(Ω) = Hk(Ω) .23 E.3 APPROXIMATION THEORY OF NEURAL NETWORKS 

We quote some known bounds for neural networks from theoretical machine learning field here, which are relevant to PDE solvers analysis later. 

Universal approximation theorem Hornik et al. (1989) states that neural networks ˆuθ , for which parameters θ ∈ Θ, can approximate any continuous functions u : Rd → R with little error ϵ > 0 in the p-norm of function space U, with an extension to their differential operator D,

∃ θ ∈ Θ. || ˆuθ − u|| p < ϵ =⇒ ||D [ˆ uθ ] − D [u]|| p < ϵ. (95) 

Optimal DNN functions approximation theorem Yarotsky (2018). Assuming a continuous func-tion u ∈ W s,p as defined in equation 94, where s ∈ N+ is the smoothness of u, there exists a neural network ˆuθ with M parameters, such that the error bound is, 

|| ˆuθ − u|| p = O(M − sd ), (96) where d is the input dimension of u. It means intuitively that the smoother and lower-dimensional the u is, the easier for it to be approximated by a neural network ˆuθ . For a fixed error ϵ, the required number of parameters is M = O(ϵ− ds ), which suffers from the exponential growth of d, i.e. the curse of dimensionality Bellman (1957). E.4 ERROR ANALYSIS 

Error and risk estimation . Define the risk of the approximation ˆuθ against the ground truth func-tion u : Ω → R with p-norm integral, 

R(ˆ uθ ) = ( || ˆuθ − u|| p)p := 

Z

> Ω

|ˆuθ (x) − u(x)|p dx, (97) For discretized computation, quadrature ˆR is used to approximate the integral risk R with N sample points from the dataset, where ωk is the weight at each sample index k,

Z

> Ω

|ˆuθ (x) − u(x)|p dx 

| {z }  

> integral R(ˆ uθ)

=

> N

X

> k=1

ωk|ˆuθ (xk) − u(xk)|p

| {z }  

> discretized ˆR(ˆ uθ)

+ET (θ) ≈ ˆR(ˆ uθ ), ET (θ) := R(ˆ uθ ) − ˆR(ˆ uθ ),

(98) where the training, also known as generalization or out-of-sample, error ET (θ) measures the differ-ence between the integral risk and the discretized risk due to quadrature. 

Error decomposition Kutyniok (2022). When approximating a continuous function u with a neural network ˆuθ , the total error E(θ)11 is decomposed into three parts, with the risk R and its quadrature 

ˆR defined in equation 97 and equation 98 respectively, 

E(θ) := R(ˆ uθ ) ≤ inf  

> θ⋆∈Θ

R(uθ⋆ )

| {z }

> EA(θ)

+ ˆR(ˆ uθ ) − inf  

> θ⋆∈Θ

R(uθ⋆ )

| {z } 

> EO(θ)

+ R(ˆ uθ ) − ˆR(ˆ uθ )

| {z } 

> ET(θ)

, (99) the approximation error EA measures the risk between the best network approximation uθ⋆ and ground truth u, optimization error EO measures the trained network result ˆuθ deviation from the best network approximation, and training error ET defined in equation 98. E.5 PINN LEARNING THEORY 

We briefly analyze the PINN error bound 12 . The total error between the optimal solution ˆuθ and the ground truth u is shown in equation 84. However, during training, the network doesn’t have access to the exact ground truth for u. Therefore, we aim to reduce the PDE residual instead. 

ER(θ) = ( ||R θ || p)p = ( ||D [ˆ uθ ] − f || p)p, by equation 81 .

= ||D [ˆ uθ ] − D [u]|| p, by equation 1.

= || ˆf − f || p, by the definition of ˆf , 

(100) 

> 11

inf θ is the infimum over all possible network parameters θ, which might not be attained. 

> 12

The PDE residual is considered here, whereas boundary and initial conditions are omitted for simplicity. 

24 where ˆf = D[ˆ uθ ] is the approximated source function. In practice, this integral is approximated via quadrature, with training error defined in equation 82. From the theoretical perspective, the goal is to derive that the total error EPINN equation 84 is suf-ficiently small. To prove this, a sufficient condition is that the total error is bounded by the residual error ER equation 100, i.e. we can prove that the smallest residual error ensures the smallest total error. 

∀θ ∈ Θ. EPINN (θ) ≤ CER(θ), (101) where C is a constant. By expansion of EPINN equation 84 and ER equation 100, the abovementioned inequality equation 101 is equivalent to the following coercivity condition Ryck & Mishra (2022), 

∀θ ∈ Θ. || ˆuθ − u|| ≤ C|| ˆf − f || p, (102) By quadrature bound Iserles (2008), the smallest practical training error ET equation 82 ensures the smallest residual error ER equation 100, where C′ is a constant, 

∀θ ∈ Θ. ER(θ) ≤ C′[ET (θ) + Eu(NR)] , (103) and the extra term Eu(NR) converges faster than 1 

> NR

, thus can be ignored given the increasing sampled quadrature points NR,

Eu(NR) ∼ o( 1

NR

) = ⇒ Eu(NR)

> 1
> NR

= 0 , n → ∞ , by the definition of little-o notation .

=⇒ lim  

> NR→∞

NREu(NR) = 0 , by the definition of limit .

(104) By the above two inequalities equation 101 and equation 103, the total error EPINN equation 84 converges as the training error ET equation 82 converges, 

∀θ ∈ Θ. EPINN (θ) ≤ CC ′[ET (θ) + o( 1

NR

)] . (105) By Universal approximation theorem equation 95, the smoothness of the solution u ensures that the residual error ER(θ) < ϵ is sufficiently small. Given sufficient quadrature points NR, and smooth activation functions in the neural network ˆuθ Iserles (2008), 

min  

> θ∈Θ

ET (θ) ≤ E R(θ) + o( 1

NR

), (106) Hence, the training error ET (θ) < ϵ + o( 1 

> NR

) is sufficiently small, according to equation 104. So is the total error EPINN (θ) < CC ′[ϵ + o( 1 

> NR

)] , by equation 105, which concludes the proof. From the practical perspective, the common failure modes, from the above theoretical analysis, are (1) few quadrature points NR leading to large training error ET in equation 98, (2) insufficient train-ing resulting in large optimization error EO in equation 99, (3) violation of the coercivity condition equation 102 for PDEs, and (4) large constant C in equation 101 or C′ in equation 103. 25