Title: Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression

URL Source: https://arxiv.org/pdf/2602.08885v1

Published Time: Tue, 10 Feb 2026 03:44:05 GMT

Number of Pages: 23

Markdown Content:
# Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

Paul Saegert 1 Ullrich K¨ othe 1

# Abstract 

Symbolic regression (SR) aims to discover in-terpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast re-duction of equivalent expressions to a concise nor-malized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy , a rule-based simplification engine achieving a 100 -fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scala-bility to much larger training sets, more efficient use of the per-expression token budget, and sys-tematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amor-tized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget. 

# 1. Introduction 

Symbolic regression (SR) occupies a unique position in sci-entific machine learning by enabling the discovery of inter-pretable, closed-form laws from observational data (Schmidt & Lipson, 2009). Unlike standard deep learning, SR seeks the analytical expressions that govern the data-generating process. Traditionally, this is framed as a combinatorial op-timization problem solved via genetic programming (GP) 

> 1

Computer Vision and Learning Lab, Heidelberg University, Germany. Correspondence to: Paul Saegert <bc226@uni-heidelberg.de >, Ullrich K ¨othe <ullrich.koethe@iwr.uni-heidelberg.de >.

Preprint. February 10, 2026. 

(Koza, 1994; Cranmer, 2023). While GP remains the gold standard for precision, it treats every dataset as a tabula rasa 

search instance, failing to transfer structural knowledge be-tween tasks and acquire experience. Pioneered by Lample & Charton (2019); Biggio et al. (2021), amortized SR solves this by learning the posterior 

p(expression |data ) over millions of examples, aiming to shift the computational burden to a one-time pre-training phase. Meanwhile, neural scaling laws dictate that robust generalization requires vast and diverse (Kaplan et al., 2020), and high quality (Lample & Charton, 2019; Lee et al., 2022; Gunasekar et al., 2023) data. This presents a fundamental challenge: randomly generated mathematical expressions are rife with redundancies (e.g., x + x vs. 2x) that must be simplified to ensure high-quality, normalized training targets. Current state-of-the-art approaches face a dilemma regard-ing this simplification step. Standard Computer Algebra Systems (CAS) like SymPy (Meurer et al., 2017) rely on heavy object-oriented parsing and tree traversals. When in-tegrated into training loops (Bendinelli et al., 2023; Yu et al., 2025), this creates a simplification bottleneck , where data generation becomes orders of magnitude slower than gra-dient updates. To maintain feasible training times, many compromise on the amount or diversity of the training data, or abandon simplification entirely (Kamienny et al., 2022). We introduce S IMPLI PY1, an engine that reduces symbolic simplification to fast pattern matching, achieving speedups of up to 100 × over SymPy at comparable quality. This breakthrough enables F LASH -ANSR 2 (Figure 1), a frame-work for Transformer-based training on a continuously gen-erated stream of high-quality expressions. Freed from the common CAS bottleneck, F LASH -ANSR can be scaled to learn to approximate the posterior over a much broader distributions of mathematical functions and data. 

Contributions: 

1. We identify the simplification bottleneck as a key ob-stacle to scaling the training and inference of amortized SR and introduce S IMPLI PY, a hash-based simplifica-

> 1

https://github.com/psaegert/simplipy 

> 2

https://github.com/psaegert/flash-ansr 

1    

> arXiv:2602.08885v1 [cs.LG] 9 Feb 2026 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression SetTransfor me r
> Transfor me r
> Dec ode r
> On-The -Fly Data Gene ration
> Figure 1. The F LASH -ANSR training pipeline. Following the established standard encoder-decoder paradigm, our framework integrates SimpliPy (top center) into the loop for synchronous simplification of on-the-fly generated training expressions.

tion engine to resolve it. 2. We present F LASH -ANSR, which leverages S IMPLI PY

to train on 512M on-the-fly generated and simplified data-expression pairs, scaling to higher-dimensional inputs and broader operator sets than prior work. 3. We demonstrate that F LASH -ANSR dominates the inference-time-recovery-rate Pareto frontier against both static (NeSymReS) and unsimplified (E2E) base-lines, and matches state-of-the-art GP methods (PySR) with superior inference-time-parsimony scaling. 4. Addressing the pervasive lack of rigor regarding data leakage and evaluation standards in the field, we im-plement strict symbolic and numeric decontamination 

of training data, ensuring our results reflect actual gen-eralization. We follow a rigorous test-time compute 

evaluation protocol, exposing the inference time bud-get trade-offs often obscured in prior literature. 

# 2. Related Work 

Symbolic regression (SR) has traditionally been formulated as a combinatorial optimization problem. While genetic pro-gramming (GP) approaches like PySR (Cranmer, 2023) and Operon (Burlacu et al., 2020) remain the gold standard for precision, they treat every task as a new search instance. Amortized methods aim to overcome this by learning a parameterized approximation of the posterior distribution 

p(τ |D ) of expressions τ given tabular data D over a mas-sive training corpus. However, existing approaches are com-promised by the trade-off between expression quality (i.e. simplification), and data diversity and scale. 

2.1. Static Corpora and Finite Support 

To ensure high-quality training targets, many amortized methods rely on SymPy (Meurer et al., 2017) for simplifica-tion. Its computational cost, however, leads many to perform data generation offline, resulting in fixed, pre-computed datasets on the order of 100M expressions (Biggio et al., 2021; Vastl et al., 2024). These expensive and limited train-ing sets ultimately force a trade-off between coverage, di-versity, and dimensionality: to maintain a representative density of samples over the prior, training is restricted to limited operator sets and low-dimensional settings ( D ≤ 3). Furthermore, these static datasets are confined to iterating over a fixed subset of the prior, fundamentally restricting the diversity of functional forms the model is exposed to during training compared to on-the-fly expression generation. 

2.2. Lack of Normalization and Syntactic Redundancy 

To escape the limitations of expensive static datasets, recent works have moved toward on-the-fly procedural generation. However, to maintain training throughput, several methods abandon simplification as a consequence, and train on vast streams of unsimplified expressions (Kamienny et al., 2022; Li et al., 2025). This forces the model to approximate a redundant one-to-many mapping during training where a single underlying function has infinite valid representations (e.g., x, x+0 , 1·x) which dilutes the probability mass across syntactically distinct but semantically identical targets. This learned syntactic redundancy severely hampers inference efficiency. The generative search dissipates its budget pro-ducing syntactic rewrites rather than distinct functional hy-potheses, and, since they are not filtered, the constants have to be redundantly re-optimized for each variant. 

2.3. The SymPy Bottleneck in Dynamic Generation 

A third category of methods attempts to combine on-the-fly generation with CAS-based simplification. NSRwH (Bendinelli et al., 2023) and MDLformer (Yu et al., 2025) apply SymPy within the training loop to simplify generated skeletons. This raises the data quality but introduces a severe computational bottleneck for large scale training. The over-head of object-oriented parsing and tree traversals in SymPy restricts the throughput and maximum complexity of expres-sions that can be generated in real-time. Consequently, these methods are often limited to lower-dimensional problems. FLASH -ANSR resolves this dilemma via S IMPLI PY, en-abling the high-throughput generation of simplified expres-sions with high dimensionality and broad operator support, significantly exceeding the scale of prior dynamic methods. 2Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

2.4. Methodological Deficiencies in Evaluation. 

We identify a pervasive lack of rigor in SR concerning data handling and evaluation. Many works report point-estimates without confidence intervals (Bendinelli et al., 2023) or use lenient success thresholds ( R2 > 0.9) that mask failures (d’Ascoli et al., 2023). Analyses often ignore inference time budgets, comparing methods with vastly different compute budgets (Valipour et al., 2021). Most alarmingly, we find that with the partial exception of (Biggio et al., 2021) virtu-ally no prior work performs rigorous data decontamination. This risks performance overestimation (Carlini et al., 2023) and violates fundamental evaluation principles (Hastie et al., 2009). Our work establishes a strict evaluation protocol: We demand machine-precision recovery ( FVU < 10 −7), ana-lyze the test-time compute Pareto frontier with respect to a diverse set of metrics, and perform rigorous and conserva-tive decontamination to prevent test set leakage. 

# 3. Method 

We formally cast symbolic regression as a set-to-sequence task. S is our set of mathematical symbols : variables V =

{x1, . . . , x D }, operators O ∈ { +, −, ∗, /, pow , sin , . . . }

and ⋄ as a placeholder for constants. Very common sub-expressions like pow2 and div3 are abbreviated by special-ized symbols. We represent a mathematical expression as a sequence of tokens τ = ( τ1, . . . , τ L), where each token rep-resents a single symbol in pre-order notation, see Appendix H for details. Our goal is to learn the posterior p(τ |D ), map-ping observed datasets D = {(xm ∈ RD , y m ∈ R)}Mm=1 to plausible (i.e. high probability) generating expressions τ .Our framework relies on three synergistic components: (1) SIMPLI PY, a novel pattern-matching engine that reduces al-gebraic simplification to efficient lookups and cancellations; (2) a synchronous, high-throughput data generation pipeline that leverages S IMPLI PY to produce a vast stream of simpli-fied training samples; and (3) a specialized encoder-decoder Transformer architecture. 

3.1. SimpliPy: Amortized Symbolic Simplification 

To enable high-throughput generation of concise expres-sions, we introduce S IMPLI PY. It reduces complex expres-sions to equivalent normalized forms: s(τ ) → τ ∗. Standard computer algebra solves this problem from first principles, but this is overkill for the kind of expressions encountered in SR training and therefore unnecessarily slow. Instead, we apply an amortization strategy to the simplification process itself and determine normalized forms for (sub)expressions with up to four variables and seven symbols offline up-front. This one-time investment allows us to reduce runtime sim-plification to fast table lookups. SIMPLI PY operates directly on the tokenized prefix se-quence produced by the generator and consumed by the model, allowing us to bypass additional parsing overhead caused by conversions to object-oriented structures. 

Phase 1: Amortized Rule Discovery (Offline). We employ a variation of Kruskal’s algorithm (Kruskal, 1956) to find a partial, heuristically bounded directed minimum spanning forest over a dynamically generated graph G with expres-sions as nodes and simplification rules as edges with weight 

w(τa, τb) = max( |τa|, |τb|). We consider symbols S′ with 

D = 4 and additional literals {0, ±1, π, e, ±∞ , NaN } to allow for informative intermediate representations. 1. Implicit Sorting: For each length L ∈ { 1, . . . , L max }

in ascending order, we add every prefix expression τ

over S′ with |τ | = L as nodes to the graph G.2. Find & Compress: We attempt to find a node ˜τ ∗ =

SIMPLI PYR(τ ) with the current edges (rules) R.3. Selection: If no such simplification exists, we test ev-ery node τ ′ of smaller length j ∈ { 1, . . . , L − 1}

in ascending order satisfying Vars (τ ′) ⊆ Vars (τ ) for functional equivalence with τ using Algorithm 1. 4. Pre-Union: Once all equivalent candidate nodes of length j′ have been found, we cache the one with the least amount of constants as a new rule (τ → τ ′) into a temporary Rnew and skip longer nodes for which 

j > j ′.5. Union: After iterating through all edge candidates go-ing out from all nodes of length L, we update the graph with the cached edges: R ← R ∪ R new .We thus create a Minimum Spanning Forest capturing the least amount of rules needed for expression simplification, minimizing the computational cost of pattern matching. 

Phase 2: Online Pattern Matching & Cancellation. At runtime, rules are loaded and sorted into operator-specific buckets keyed by pattern length and root node. Explicit rules (those without variables) are hash-indexed for O(1) 

lookup while rules with variables are stored as trees for subtree matching. Given a prefix expression, our algorithm first attempts exact replacements considering only the most specific patterns without variables on the current subtree. It then widens the search including patterns with variables and searches pattern buckets from largest to smallest pattern length. If no pattern matches the expression, we simplify children and re-consider simplification of the parent. An optional Lmax caps the longest pattern considered, providing control over the trade-off between speed and quality. Between rule passes, the engine runs a multiplicity-based cancellation pass that merges associative and 3Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

commutative clusters (e.g., rewriting [ + , x1 , x1 ] to 

[ mult2 , x1 ]) and cancels inverse pairs. Both steps are interleaved and repeat alternatingly until convergence or a small iteration budget is reached. In the final expression, operands of commutative operators are sorted to a canonical order. As an example, S IMPLI PY simplifies 

x1

2

> 2

+ c1 · e− 1 

> x2−x2

+ c2 −→ c1 +

 x1

2

2

(1) effectively cancelling the exponential term, removing the redundant absolute value, and substituting c1 ← c1 + c2.

3.2. Architecture 

FLASH -ANSR builds on an encoder-decoder Transformer architecture inspired by Biggio et al. (2021). We opt for the pre-RMSNorm Transformer decoder architecture (Vaswani et al., 2023; Zhang & Sennrich, 2019; Xiong et al., 2020) with multi-head FlashAttention (Dao, 2023). To encode data sets of variable size, we use a variant of the Set Transformer (Lee et al., 2019) to efficiently encode the input data and use I = S = 128 induction and seed points respectively. Inspired by Child et al. (2019); Xiong et al. (2020), and Zhang & Sennrich (2019); Zhang et al. (2022), we introduce the pre-norm paradigm to the Set Transformer architecture. We introduce a masked RMSSetNorm based on Zhang & Sennrich (2019); Zhang et al. (2022) to replace the standard LayerNorm (Ba et al., 2016) or SetNorm (Zhang et al., 2022), allowing us to standardize over the same number of axes as SetNorm while only requiring half the number of statistics and re-scaling parameters, and accounting for padding (see Appendix C for derivation). As in NeSymReS (Biggio et al., 2021), we preprocess inputs to the Set Transformer by representing each scalar by a multi-hot encoding over a fixed bit representation following the IEEE-754 standard (IEEE, 2008). We use 32 bits instead of just 16 bits to represent each scalar value, leading to an input tensor Z(0) ∈ RN ×M ×32 ·(D+1) with batch size N .

3.3. Data Generation 

Training data is generated fully on-the-fly following a multi-step process. For each instance, we sample the desired num-ber of operators of the expression nops ∼ pops (nops ) ∝

exp[ nα

> ops

/λ ] with α = 0 .7 and λ = 1 limited to numbers in the range nops ∈ [0 , 17] . A prefix skeleton is then generated with the Lample & Charton algorithm (Lample & Charton, 2019), creating skeletons with up to 35 symbols. Operators are then sampled and assigned to non-leaf nodes with rela-tive weights 10 for the operators + , - , * and / , and weight 1 for all other operators. The leaf nodes are populated by sampling the number of unique variables to be present in the skeleton nvar ∗ ∼ U (1 , min( nleaves , D )) limited by the number of available leaves nleaves or the considered maxi-mum dimensionality D. The set of unique variables is then drawn uniformly without replacement from the union of all variables and the constant placeholder Vsample ∗ ⊆ V ∪ {⋄} .We then randomly duplicate elements of Vsample ∗ until reach-ing multiset Vsample with nleaves elements before shuffling and substituting them into the leaf nodes of the skeleton. Next, the skeletons are simplified with S IMPLI PY (Lmax =4) and rejected if the simplification produced non-finite sym-bols, or if they symbolically or numerically match one of the test skeletons. To that end, we structurally prune all con-stant nodes from both candidate and test skeletons before computing and comparing their images ycheck on a fixed input Xcheck ∼ U (−10 , 10) 512 ×D using Algorithm 1, effec-tively excluding whole families of expressions from train-ing. Skeletons are held out from training if all point-wise absolute differences fall within 10 −4. Non-numeric values (NaN) in the image are treated as matches, contributing to a conservative decontamination strategy. In the final step, we sample the number of data points 

M ∼ U (1 , 1024) and their values in each dimension j

as xmj ∼ U (aj , b j ) with aj , b j ∼ N (0 , σ = 10) . Each constant present in the expression is sampled independently from N (0 , σ = 5) , and the expression is compiled and evaluated on X via y = f¯τ (X; c). If y includes complex, non-finite, or non-numeric elements, all data is re-sampled up to 4 times after which the entire instance is rejected. An illustrative example is provided in Appendix I. 

3.4. Training 

We minimize the cross-entropy loss over the joint distribu-tion of normalized skeletons and datasets p( ¯ τ ∗, D):

ˆθ = arg min  

> θ

E ¯τ ∼p( ¯ τ )  

> D∼ p(D| ¯τ)

"

−

> L

X

> t=1

log pθ (¯ τ ∗ 

> t

| ¯τ ∗

> <t

, D)

#

(2) where pθ is the likelihood assigned by the Transformer decoder at step t, conditioned on the previous tokens ¯τ ∗

> <t

and the dataset D encoded by the Set Transformer. Encoder and decoder are trained jointly and end-to-end. We train four models (3M, 20M, 120M, and 1B parameters) using the hyperparameters listed in Appendix B. 

3.5. Inference 

We opt for autoregressive softmax sampling to generate candidate skeletons ˆ¯τ ∼ p(ˆ ¯τ |D ) at inference time and en-courage diversity by capturing the multi-modal nature of the posterior distribution of expressions. Integer multiplication and division operator tokens in ¯τ are replaced with multipli-cations by free constants at a slightly increased optimization cost to allow for fine-grained numerical optimization of the 4Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

expression, e.g. [..., mult2 , · , ... ] → [..., * , ⋄ , · , ... ].All generated skeletons are then simplified with SimpliPy (Lmax = 4 ), deduplicated symbolically, and compiled before their constants are optimized with the Levenberg-Marquardt algorithm (Levenberg, 1944; Marquardt, 1963) under a least-squares objective using SciPy.optimize.curve fit (Vir-tanen et al., 2020), yielding optimized expressions ˆτ . Opti-mizations are run in parallel on multiple CPU cores and are restarted 8 times with initial values sampled from 

c0 ∼ N (0 , σ = 5) , matching the training prior. Inspired by many other methods (Biggio et al., 2021; Cranmer, 2023), the predicted expressions are then sorted by their fit quality and a parsimony regularization 

ˆτ ⋆ = arg min  

> ˆτ

log 10 FVU ( ˆ τ ) + γ · | ˆτ | (3) where FVU ( ˆ τ ) is the fraction of variance unexplained (Equation 8) achieved by an expression ˆτ , and γ is a small constant penalizing the complexity of a predicted expres-sion. Thus, an expression ˆτb is considered better than an expression ˆτa if FVU ( ˆ τb)

FVU ( ˆ τa) < 10 −γ(| ˆτb|−| ˆτa|). (4) By default, we set γ = 0 .05 , meaning that each additional symbol should decrease the FVU by 11% relative to the expression without that symbol. 

# 4. Experiments 

We conduct a comprehensive empirical evaluation of FLASH -ANSR against established baselines in amortized SR (NeSymReS, E2E) and direct optimization (PySR) on 115 expressions and data derived from the F AST SRB bench-mark (Martinek, 2025) 3. Compared to other commonly used benchmarks (e.g. Feynman (Udrescu & Tegmark, 2020), Nguyen (Hoai et al., 2002; Johnson, 2009; Keijzer, 2003; Uy et al., 2011; Petersen et al., 2021)), which restrict the domain of input data to small and arbitrary intervals around zero, FAST SRB adopts the physically motivated data domains of SRSD (Matsubara et al., 2024), ensuring that methods are evaluated on data distributions that reflect real scientific applications. We evaluate under strict machine-precision recovery metrics and analyze performance as a function of three key scaling axes: test-time compute, data sparsity, and noise robustness. 

4.1. Time-Normalized Recovery Rate 

We take inspiration from Biggio et al. (2021) and evaluate the inference-time-recovery-rate Pareto frontier by sweep-ing over a characteristic scaling parameter for each method 

> 3We exclude 5 equations (B4, B7, B17, II.24.17 and III.14.14) due to complications with data sampling

(Table 1), varying the inference time budget from the mil-lisecond regime ( < 0.1s) up to a medium-horizon search (≈ 1000 s) in powers of 2. No artificial restrictions are im-posed on hardware acceleration (see Appendix F), and no model is stopped early during inference. For each test ex-pression, we evaluate on 10 distinctly sampled datasets for short-term runs and 5 for medium-horizon runs. Baselines are configured as close to their default settings as possible (see Appendix G for details).                            

> Table 1. Ranges for characteristic inference-time scaling param-eters used to analyze performance across orders of magnitude in inference time. Evaluation of our 1B parameter model (Appendix L) is limited to 64k samples to account for additional inference time induced by the larger model size. METHOD PARAMETER RANGES
> Short ( 10 ×)Medium ( 5×)NeSymReS Beam Width 1. . . 32 128 ,512
> E2E Beam Width 1. . . 512 –PySR Iterations 1. . . 1024 4096 ,16 kFlash-ANSR Samples 1. . . 16 k64 k,256 k

4.2. Data Sparsity 

We evaluate the ability of F LASH -ANSR to recover physical laws from sparse data by varying the number of points to fit, M ∈ { 20, . . . , 211 }. To ensure a fair comparison, we fix the inference budget to approximately 10 seconds per expression for all methods. The specific hyperparameters corresponding to this budget are detailed in Table 2.                  

> Table 2. Fixed test time compute settings for the sparse data and noisy data experiments. METHOD PARAMETER VALUE
> NeSymReS Beam Width 4E2E Beam Width 256 PySR Iterations 128 Flash-ANSR v23.0-3M Samples 4096 Flash-ANSR v23.0-20M Samples 2048 Flash-ANSR v23.0-120M Samples 2048 Flash-ANSR v23.0-1B Samples 512

4.3. Noisy Data 

To assess robustness to noise, we introduce additive Gaus-sian noise to the target variables following a protocol sim-ilar to Cava et al. (2021). For the relative noise levels 

η ∈ { 10 −3, 10 −2, 10 −1}, we corrupt the data via 

˜ym = ym + ϵm, ϵm ∼ N (0 , σ = ( η · σy )) (5) where σy is the standard deviation of the clean targets. 5Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

4.4. Metrics 

We assess model performance based on two central met-rics capturing the goodness of fit and the parsimony of the predicted expressions. Relying on exact symbolic match criteria for evaluation is fundamentally flawed for two reasons. Incomplete symbolic canonicalizations of functionally equivalent expressions are not identified as such without expensive manual intervention, leading to misleadingly low recovery rates. Purely symbolic criteria also disregard the vast amount of diverse, function-ally distinct expressions that can fit the data equally well as the ground truth. Hence, we focus on numeric recovery as the primary criterion for success. 

Numeric Recovery Rate (NRR). Since problems can have widely varying output scales, we base our numeric criterion on the Fraction of Variance Unexplained (FVU) between the ground truth values y and the values obtained from a predicted expression ˆy. We define the NRR as the percent-age of test problems for which the model finds a solution that fits the data up to the machine epsilon for 32-bit float-ing point numbers, i.e. FVU( y, ˆy) ≤ ϵ32 ≈ 1.19 × 10 −7.Furthermore, distinguish between the recovery rate on the fit data split (fNRR) and an unseen validation data split of equal size (vNRR) for each test instance. 

Expression Length Ratio. As a proxy for symbolic qual-ity and interpretability, we evaluate the parsimony of the predicted expressions using the ratio of lengths between predicted and ground truth expressions | ˆτ |/|τ |, where |τ |

denotes the number of nodes in the expression tree of τ .We indicate the objective of each metric, e.g. NRR ↑[0 ,100] 

and use bootstrapping to estimate the 95% confidence inter-vals for all statistics (Riezler & Hagmann, 2024). 

4.5. Symbolic Simplification Efficiency 

We directly compare the effectiveness and efficiency of simplifications with SymPy (Meurer et al., 2017) and S IM -

> PLI

PY based on 216 (64k) expressions sampled from our training distribution (Section 3.3). For SymPy, we imple-ment a compatibility layer that converts our tokenized prefix representation to the corresponding infix string. Then, to work around SymPy’s inherent inability to combine sym-bols representing free constants, we temporarily substitute randomly sampled values ck ∼ U (−10 , 10) into the expres-sion before parsing it with SymPy, measuring the time of its 

simplify routine, and masking numerical values again with the placeholder ⋄. The simplified expression is then converted back to our prefix format with S IMPLI PY’s parse method. 

# 5. Results 

5.1. Time-Normalized Recovery Rate 

We present the results of the test-time normalized perfor-mance on the F AST SRB benchmark in Figure 2.          

> Figure 2. Left: Validation Numeric Recovery Rate (vNRR) as a function of inference time (log scale). F LASH -ANSR mod-els (shades of blue) scale monotonically with compute, with the 120M model partially surpassing the PySR baseline (red). Base-lines NeSymReS and E2E fail to generalize to the benchmark.
> Right: Expression Length Ratio |ˆτ|/|τ|versus compute. We ob-serve a parsimony inversion : while PySR increases complexity to minimize error over time, F LASH -ANSR converges toward sim-pler, more canonical expressions as the sampling budget increases. Shaded regions denote 95% confidence intervals.

Baselines. Consistent with the limitations identified in Sec-tion 2, prior amortized methods struggle to generalize to the realistic domains of F AST SRB. NeSymReS saturates at a recovery rate of approximately 10% , while E2E fails to achieve meaningful recovery ( < 2.5% ) across all budgets. 

Accuracy Scaling. FLASH -ANSR demonstrates strong power-law scaling with respect to inference compute. While the smallest model (3M) lags behind the genetic program-ming baseline, the larger variants and the 1B model (Ap-pendix L) effectively bridge the gap. Notably, the 120M parameter model achieves parity with PySR in the medium-compute regime ( ≈ 10 s) and partially surpasses it at higher budgets, reaching a peak recovery rate of 55 .8% compared to PySR’s 50 .0% . This shows that with sufficient scale and sampling budget, an amortized prior can match or exceed the search efficiency of mature evolutionary algorithms. 

The Parsimony Inversion. Crucially, we observe a fun-damental divergence in how the methods navigate the accuracy-complexity trade-off (Figure 2, right). PySR em-ploys a sophisticated model selection strategy: it maintains a Pareto front of the best expressions found at every complex-ity level and selects the final candidate by maximizing the negative gradient of the log-loss with respect to complexity (finding the “knee” of the curve). Without any parsimony penalty beyond this default strategy, however, PySR exhibits a positive correlation between time and expression complex-ity: as the evolutionary search progresses, the length ratio increases from 0.94 to 1.85 . This suggests that even with parsimony pressure, the GP optimizes residual error by ap-6Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

pending corrective terms, resulting in expressions that fit the data via complexity rather than structural discovery. Conversely, F LASH -ANSR exhibits an inverse scaling trend. As the sampling budget increases, the average length ratio of the best-found solution decreases , converging towards the ground truth length (e.g., the 120M model improves from 1.40 to 1.27 ), while simultaneously improving recov-ery rates. With more compute, the model successfully sam-ples these rarer, concise “needles in the haystack”, rather than constructing complex approximations. As a realistic application of our method, we present the identification of mathematical test-time scaling laws on the FAST SRB benchmark in Figure 3.             

> Figure 3. FLASH -ANSR fits to its own and PySR’s scaling curves log 10 (T)vs vNRR from Figure 2 (left) using v23.0-120M,
> γ= 0 .15 , 128k choices ≈10 min. Extrapolation suggests an asymptotic vNRR ∝log Tscaling for F LASH -ANSR, and an asymptotic upper limit for PySR around 53% .

5.2. Symbolic Simplification Efficiency 

Figure 4 characterizes the performance of our S IMPLI PY

engine and the SymPy simplify routine.              

> Figure 4. Left: Empirical Cumulative Distribution Functions (ECDFs) of simplification wall-clock time. Our S IMPLI PYrewrit-ing engine (shades of blue, varying Lmax ) operates in the low to moderate millisecond regime, orders of magnitude faster than the SymPy baseline (orange, red). Right: ECDF of the simplification ratio |τ∗|/|τ|. The inset highlights the tail of the distribution. Our method with Lmax ≥5achieves simplification ratios comparable to the SymPy baseline while maintaining high throughput.

Speedup. Our measurements (Figure 4, left) reveal a sig-nificant acceleration with respect to the SymPy baseline which exhibits a median simplification time of approxi-mately 100 ms per expression, rendering it prohibitive for real-time applications or large-batch training. In contrast, our pattern-based engine operates in the millisecond regime for Lmax = 4 (used to train and evaluate F LASH -ANSR). 

Quality Preservation. Critically, this speed only comes at a minimal cost of simplification quality. Figure 4 (right) plots the distribution of the simplification ratio |τ ∗|/|τ |. We observe that the distributions for Lmax ≥ 5 even exceed the baseline in terms of conciseness, indicating that our local rewriting rules and cancellations capture the vast majority of reductions found by the more complex heuristics of SymPy. While S IMPLI PY results in strictly shorter or at most equal lengths, SymPy’s simplification paradoxically increases the length of about 38% to 52% of expressions (even with the “ratio” parameter set to 1), and fails to complete within a 1-second timeout for 9% of expressions. 

5.3. Data Sparsity 

We investigate the impact of data sparsity on model perfor-mance by varying the number of support points M from 1 to 

2048 (Figure 5). While the amortized baselines (NeSymReS,                 

> Figure 5. Left: Validation Numeric Recovery Rate (vNRR) vs. the number of support points M. F LASH -ANSR (120M) outperforms PySR in the dense data regime ( M > 64 ). Right: Expression Length Ratio |ˆτ|/|τ|. We observe a distinct “Complexity Peak” at M≈8, where the model generates expressions significantly longer than the ground truth. This peak coincides with a regime of high uncertainty (low log-probability) and excess constant usage, suggesting the model is interpolating the sparse points via complex aliasing rather than identifying the underlying law.

E2E) remain largely ineffective across all regimes, F LASH -ANSR and PySR demonstrate strong scaling behavior. The structural evolution of the predicted expressions re-veals a phase transition analogous to (Deep) Double Descent 

(Belkin et al., 2018; Nakkiran et al., 2019). At the critical threshold of M ≈ 8 points, we observe a “complexity peak” where the average expression length overshoots the ground truth by up to 2 times. This correlates with a surge in ex-cess constants (using degrees of freedom to minimize the fit error) and a dip in the model’s log-probability (Appendix L.2), suggesting a transition through three distinct regimes: 

The Parsimonious Regime ( M < 8): The data is insuffi-cient to contradict the model’s bias toward simplicity and the 7Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

scoring function’s preference for parsimonious expressions (Equation 3), resulting in confident, concise approximations with high bias but low variance. 

The Interpolation Regime ( M ≈ 16 ): The data density is sufficient to rule out trivial forms but insufficient to con-strain the search to the true symbolic law. To maintain low fit error, the method saturates its complexity budget, insert-ing excess constants and operators to construct “aliasing” approximations. The drop in log-probability confirms this as a regime of high uncertainty, where probability mass is spread across many complex candidates. 

The Identification Regime ( M → 2048 ): The symbolic cost of explaining the data via complex approximation ex-ceeds the model’s generative capabilities, encouraging a posterior collapse to the ground truth – the only remaining solution that fits the data and is concise enough to be pre-dicted by the model. We observe that distribution shifts in the data can disrupt this delicate balance, preventing suc-cessful identification even in the dense data regime. 

5.4. Noisy Data 

We demonstrate the effects of distribution shifts induced by additive Gaussian noise on the target values in Figure 6.          

> Figure 6. Left: Validation Numeric Recovery Rate (vNRR) vs. the noise level η(log scale). While F LASH -ANSR (shades of blue) achieves competitive performance in the noiseless regime (circles), PySR (red) retains higher recovery rates at larger noise levels.
> Right: Expression Length Ratio |ˆτ|/|τ|. On noisy data, PySR (red) and NeSymReS (orange) produce expressions of similar com-plexity to the ground truth whereas F LASH -ANSR consistently predicts longer expressions without any apparent trend.

In low-noise settings, F LASH -ANSR demonstrates superior performance with the 120M parameter model surpassing the genetic programming baseline PySR (Figure 6, left). At higher noise levels η ≥ 10 −2, we observe a distinct perfor-mance crossover. Here, PySR demonstrates better robust-ness, retaining a higher recovery rate than F LASH -ANSR. Since F LASH -ANSR was trained exclusively on noiseless data, the injection of noise constitutes a distributional shift. The encoder, having learned that all variance in y carries se-mantic meaning, misinterprets the noise as high-frequency signals. Consequently, it attempts to “explain” the noise by continuing to generate expressions with rich functional structures, none of which can perfectly fit the intricate vari-ations induced by the noise (Figure 6, right). 

5.5. Training Prior Ablations 

Figure 7 highlights the importance of a well-designed train-ing prior. Restricting constant values to a narrow positive range U(1 , 5) as in prior work (Biggio et al., 2021) severely limits the model’s ability to generalize to test expressions with constants near zero or negative values. By expanding the training prior to include negative values and values near zero, the model learns to associate subtle variations in data with corresponding structural changes in the expression, significantly improving recovery rates.                  

> Figure 7. Ablations of F LASH -ANSR fitting a Gaussian function with varying μand σ.Left: When trained with constants sampled from U(1 ,5) as in prior work (Biggio et al., 2021), the model struggles to correctly identify the function’s shape and constants around the standard values ( μ= 0 ,σ= 1 ). Right: Training with a broader prior of U(−5,5) improves the ability to recover the correct expression under subtle variations from the standard values.

# 6. Conclusion 

In this work, we introduced S IMPLI PY, a high-throughput simplification engine that breaks a significant bottleneck preventing amortized SR from scaling to scientific complex-ity. We demonstrate its effectiveness through our F LASH -ANSR framework, which enables large-scale training on diverse and high-quality expressions, and efficient inference. In our large-scale evaluation on the F AST SRB benchmark, FLASH -ANSR dominates existing amortized baselines and effectively competes with state-of-the-art genetic program-ming (PySR) on the inference-time vs recovery-rate Pareto frontier in realistic scientific data domains, while recovering more concise expressions as the inference budget increases. Limitations remain, particularly regarding robustness to noisy data, which acts as an out-of-distribution shift for our models. Future work will focus on further improving the training prior by incorporating noisy data into the training process, widening the range of the generated training data distributions, and exploring alternative encoding and de-8Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

coding paradigms. By establishing rigorous evaluation stan-dards and demonstrating scalable performance, our work positions amortized SR as a powerful, high-speed comple-ment to classical optimization in the toolkit of scientific discovery. 

# Impact Statement 

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. 

# References 

Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voz-nesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang, X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G., Wu, P., and Chintala, S. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 , pp. 929–947. ACM, 2024. doi: 10.1145/3620665.3640366. Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normaliza-tion, 2016. URL https://arxiv.org/abs/1607. 06450 .Belkin, M., Hsu, D. J., Ma, S., and Mandal, S. Reconciling modern machine learning and the bias-variance trade-off. ArXiv , abs/1812.11118, 2018. URL https://api. semanticscholar.org/CorpusID:57189215 .Bendinelli, T., Biggio, L., and Kamienny, P.-A. Control-lable neural symbolic regression, 2023. URL https: //arxiv.org/abs/2304.10336 .Biggio, L., Bendinelli, T., Neitz, A., Lucchi, A., and Paras-candolo, G. Neural symbolic regression that scales. In 

International Conference on Machine Learning , pp. 936– 945. Pmlr, 2021. Burlacu, B., Kronberger, G., and Kommenda, M. Operon c++: An efficient genetic programming framework for symbolic regression. In Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion , GECCO ’20, pp. 1562–1570, New York, NY, USA, 2020. Association for Computing Machin-ery. ISBN 9781450371278. doi: 10.1145/3377929. 3398099. URL https://doi.org/10.1145/ 3377929.3398099 .Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tram `er, F., and Zhang, C. Quantifying memorization across neural language models. In The Eleventh International Confer-ence on Learning Representations , 2023. URL https: //openreview.net/forum?id=P282diP8Cq .Cava, W. L., Orzechowski, P., Burlacu, B., de Fran c¸ a, F. O., Virgolin, M., Jin, Y., Kommenda, M., and Moore, J. H. Contemporary symbolic regression methods and their relative performance, 2021. URL https://arxiv. org/abs/2107.14351 .Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transform-ers. ArXiv , abs/1904.10509, 2019. URL https: //api.semanticscholar.org/CorpusID: 129945531 .Cranmer, M. Interpretable machine learning for science with pysr and symbolicregression.jl, 2023. URL https: //arxiv.org/abs/2305.01582 .Dao, T. Flashattention-2: Faster attention with better par-allelism and work partitioning, 2023. URL https: //arxiv.org/abs/2307.08691 .d’Ascoli, S., Becker, S., Mathis, A., Schwaller, P., and Kilbertus, N. Odeformer: Symbolic regression of dy-namical systems with transformers. arXiv preprint arXiv:2310.05573 , 2023. Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Giorno, A. D., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., Salim, A., Shah, S., Behl, H. S., Wang, X., Bubeck, S., Eldan, R., Kalai, A. T., Lee, Y. T., and Li, Y. Textbooks are all you need, 2023. URL 

https://arxiv.org/abs/2306.11644 .H ¨agele, A., Bakouch, E., Kosson, A., Allal, L. B., von Werra, L., and Jaggi, M. Scaling laws and compute-optimal training beyond fixed training dura-tions. ArXiv , abs/2405.18392, 2024. URL https: //api.semanticscholar.org/CorpusID: 270068102 .Hastie, T., Tibshirani, R., and Friedman, J. The Elements of Statistical Learning: Data Mining, Inference, and Predic-tion . Springer, New York, NY, 2 edition, 2009. Hoai, N. X., McKay, R. I., Essam, D., and Chau, R. Solv-ing the symbolic regression problem with tree-adjunct grammar guided genetic programming: The comparative 9Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

results. In Proceedings of the 2002 Congress on Evo-lutionary Computation. CEC’02 (Cat. No. 02TH8600) ,volume 2, pp. 1326–1331. IEEE, 2002. Hu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z., Fang, Y., Huang, Y., Zhao, W., Zhang, X., Thai, Z. L., Zhang, K., Wang, C., Yao, Y., Zhao, C., Zhou, J., Cai, J., Zhai, Z., Ding, N., Jia, C., Zeng, G., Li, D., Liu, Z., and Sun, M. Minicpm: Unveiling the potential of small language models with scalable training strategies, 2024. URL https://arxiv.org/abs/2404.06395 .IEEE. Ieee standard for floating-point arithmetic. IEEE Std 754-2008 , pp. 1–70, 2008. doi: 10.1109/IEEESTD.2008. 4610935. Johnson, C. G. Genetic programming crossover: Does it cross over? In Genetic Programming: 12th European Conference, EuroGP 2009 T ¨ubingen, Germany, April 15-17, 2009 Proceedings 12 , pp. 97–108. Springer, 2009. Kamienny, P.-A., d’Ascoli, S., Lample, G., and Charton, F. End-to-end symbolic regression with transformers, 2022. URL https://arxiv.org/abs/2204.10532 .Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language mod-els, 2020. URL https://arxiv.org/abs/2001. 08361 .Keijzer, M. Improving symbolic regression with interval arithmetic and linear scaling. In European Conference on Genetic Programming , pp. 70–82. Springer, 2003. Koza, J. R. Genetic programming as a means for pro-gramming computers by natural selection. Statistics and Computing , 4:87–112, 1994. URL https://api. semanticscholar.org/CorpusID:8750149 .Kruskal, J. B. On the shortest spanning subtree of a graph and the traveling salesman problem. Proceed-ings of the American Mathematical Society , 7(1):48– 50, 1956. ISSN 00029939, 10886826. URL http: //www.jstor.org/stable/2033241 .Lample, G. and Charton, F. Deep learning for symbolic mathematics, 2019. URL https://arxiv.org/ abs/1912.01412 .Lee, J., Lee, Y., Kim, J., Kosiorek, A. R., Choi, S., and Teh, Y. W. Set transformer: A framework for attention-based permutation-invariant neural networks, 2019. URL 

https://arxiv.org/abs/1810.00825 .Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N. Deduplicating train-ing data makes language models better, 2022. URL 

https://arxiv.org/abs/2107.06499 .Levenberg, K. A method for the solution of certain non – linear problems in least squares. Quarterly of Applied Mathematics , 2:164–168, 1944. URL https: //api.semanticscholar.org/CorpusID: 124308544 .Li, Y., Liu, J., Wu, M., Yu, L., Li, W., Ning, X., Li, W., Hao, M., Deng, Y., and Wei, S. Mmsr: Symbolic regression is a multi-modal information fusion task. Information Fusion , 114:102681, February 2025. ISSN 1566-2535. doi: 10.1016/j.inffus.2024.102681. URL http://dx. doi.org/10.1016/j.inffus.2024.102681 .Loshchilov, I. and Hutter, F. Decoupled weight decay regu-larization, 2019. URL https://arxiv.org/abs/ 1711.05101 .Marquardt, D. W. An algorithm for least-squares estimation of nonlinear parameters. Journal of the Society for In-dustrial and Applied Mathematics , 11(2):431–441, 1963. doi: 10.1137/0111030. URL https://doi.org/10. 1137/0111030 .Martinek, V. Fast symbolic regression benchmarking, 2025. URL https://arxiv.org/abs/2508.14481 .Matsubara, Y., Chiba, N., Igarashi, R., and Ushiku, Y. Re-thinking symbolic regression datasets and benchmarks for scientific discovery, 2024. URL https://arxiv. org/abs/2206.10540 .Meurer, A., Smith, C. P., Paprocki, M., ˇCert ´ık, O., Kirpichev, S. B., Rocklin, M., Kumar, A., Ivanov, S., Moore, J. K., Singh, S., Rathnayake, T., Vig, S., Granger, B. E., Muller, R. P., Bonazzi, F., Gupta, H., Vats, S., Johansson, F., Pedregosa, F., Curry, M. J., Terrel, A. R., Rou ˇcka, v., Saboo, A., Fernando, I., Kulal, S., Cimrman, R., and Scopatz, A. Sympy: symbolic computing in python. PeerJ Computer Science , 3:e103, January 2017. ISSN 2376-5992. doi: 10.7717/peerj-cs.103. URL https://doi. org/10.7717/peerj-cs.103 .Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I. Deep double descent: Where big-ger models and more data hurt, 2019. URL https: //arxiv.org/abs/1912.02292 .Petersen, B. K., Landajuela, M., Mundhenk, T. N., Santi-ago, C. P., Kim, S. K., and Kim, J. T. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. In Proc. of the International Conference on Learning Representations ,2021. Riezler, S. and Hagmann, M. Validity, Reliability, and Sig-nificance: Empirical Methods for NLP and Data Sci-ence . Synthesis Lectures on Human Language Tech-10 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

nologies. Springer Cham, 2024. ISBN 978-3-031-57065-0. doi: 10.1007/978-3-031-57065-0. URL https: //doi.org/10.1007/978-3-031-57065-0 .Schmidt, M. D. and Lipson, H. Distilling free-form nat-ural laws from experimental data. Science , 324:81 – 85, 2009. URL https://api.semanticscholar. org/CorpusID:7366016 .Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/ 2104.09864 .Udrescu, S.-M. and Tegmark, M. Ai feynman: a physics-inspired method for symbolic regression, 2020. URL 

https://arxiv.org/abs/1905.11481 .Uy, N. Q., Hoai, N. X., O’Neill, M., McKay, R. I., and Galv ´an-L ´opez, E. Semantically-based crossover in ge-netic programming: application to real-valued symbolic regression. Genetic Programming and Evolvable Ma-chines , 12:91–119, 2011. Valipour, M., You, B., Panju, M., and Ghodsi, A. Sym-bolicgpt: A generative transformer model for symbolic regression. arXiv preprint arXiv:2106.14131 , 2021. Vastl, M., Kulh ´anek, J., Kubal ´ık, J., Derner, E., and Babu ˇska, R. Symformer: End-to-end symbolic regression using transformer-based architecture. IEEE Access , 12:37840– 37849, 2024. doi: 10.1109/ACCESS.2024.3374649. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023. URL https://arxiv.org/ abs/1706.03762 .Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I., Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A., Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa, F., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods , 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2. Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T.-Y. On layer normalization in the transformer architec-ture. ArXiv , abs/2002.04745, 2020. URL https: //api.semanticscholar.org/CorpusID: 211082816 .Yu, Z., Ding, J., Li, Y., and Jin, D. Symbolic regression via mdlformer-guided search: from minimizing predic-tion error to minimizing description length, 2025. URL 

https://arxiv.org/abs/2411.03753 .Zhang, B. and Sennrich, R. Root mean square layer nor-malization. ArXiv , abs/1910.07467, 2019. URL https: //api.semanticscholar.org/CorpusID: 113405151 .Zhang, K. and Shasha, D. Simple fast algorithms for the edit-ing distance between trees and related problems. SIAM journal on computing , 18(6):1245–1262, 1989. Zhang, L. H., Tozzo, V., Higgins, J. M., and Ran-ganath, R. Set norm and equivariant skip con-nections: Putting the deep in deep sets. Proceed-ings of machine learning research , 162:26559–26574, 2022. URL https://api.semanticscholar. org/CorpusID:250048548 .11 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

# A. Examples 

Figure 8. FLASH -ANSR fits (v23.0-120M, γ = 0 .05 , 32k samples ≈ 100 s) (red curves) on uniformly sampled data points (black dots) from several 1D expressions ( M = 64 , η = 0 ). Many ground truth expressions (bold) are recovered exactly, while others are recovered in alternative forms (e.g. ˆy = ( x + 1) 2 for y = x2 + 2 x + 1 ). 

Figure 9. Diverse F LASH -ANSR solutions (v23.0-120M, γ = 0 .05 , 32k samples ≈ 100 s) (red curves) on uniformly sampled data points (black dots) depicting the trajectory of a damped harmonic oscillator ( M = 150 , η ≈ 0.04 ). Multiple distinct expressions fit the data equally well given the noisy data. The ground truth expression (top left, bold) is exactly recovered with high probability (top right). Other solutions (bottom row) achieve similar fit quality while differing structurally. 

12 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression         

> Figure 10. FLASH -ANSR fits (v23.0-120M, γ= 0 .05 , 32k samples ≈100 s) (middle columns) on uniformly sampled data points (black dots) from several 2D expressions from Biggio et al. (2021) ( M= 100 ,η= 0 ). Many ground truth expressions (left columns, bold) are recovered exactly, while few others have additional terms that do not affect the relative fit error (right columns)

# B. Hyperparameters 

We provide a detailed specification of the hyperparameters and training configurations used for the F LASH -ANSR model family to ensure reproducibility below and in Table 3. 

Training. All models are trained end-to-end using the AdamW optimizer (Loshchilov & Hutter, 2019) with β1 = 0 .9. We utilize a Warmup-Stable-Decay learning rate schedule (Hu et al., 2024; H ¨agele et al., 2024). The learning rate linearly warms up from 0 to a maximum of ηmax over the first 10% of optimization steps, remains constant for the subsequent 70% of steps, and follows a linear decay to 0 over the final 20% of steps. We apply gradient clipping with a maximum global norm of 2.0.

Architecture & Implementation. Our models are implemented in PyTorch 2.9. To maximize training throughput on modern GPU architectures, we use Automatic Mixed Precision (AMP) with bf16 precision and torch.compile (Ansel et al., 2024). We apply dropout with p = 0 .1 to the output of Multi-Head Attention blocks and Feed-Forward networks. In the decoder, we utilize Rotary Positional Embeddings (RoPE) (Su et al., 2023) applied to the queries and keys of the self-attention mechanism. The Set Transformer encoder uses I = 128 inducing points and S = 128 seed vectors across all model sizes. 13 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

Table 3. Hyperparameters for the Flash-ANSR model series. HYPERPARAMETER V23.0-3M V23.0-20M V23.0-120M V23.0-1B 

Encoder 

Dimension denc 192 384 640 1280 Heads 3 6 10 20 ISAB Layers 1 2 5 10 SAB Layers 1 2 5 10 

Decoder 

Dimension ddec 192 384 640 1280 Heads 3 6 10 20 Layers 3 6 10 20 Feed Forward Dim 576 1152 1920 3840 

Training 

Optimizer Steps 1M 2M 3M 4M Learning Rate ηmax 10 −4 10 −4 10 −4 3 · 10 −5

Weight Decay 0.01 0.01 0.01 0.025 

β2 0.95 0.95 0.95 0.99995 Logical CPUs 4 32 8 8GPU RTX 2080Ti RTX 4090 A100 H200 Time Taken 112h 56h 340h 657h 

Parameters 

Total 3.2M 23M 122M 955M Encoder 1.6M 10.4M 64.4M 494M Decoder 1.6M 12.4M 57.5M 459M 

Simplification. We employ our S IMPLI PY symbolic simplification engine (version dev 7-3 ) with a maximum pattern length of Lmax = 4 during training and inference. 

Ablations. For Figure 7, we use an older version of our model (v7.0 trained with constants sampled from U(1 , 5) and v7.4 trained with constants sampled from U(−5, 5) but otherwise identical). 

# C. Masked RMSNorm for Sets 

Let Z ∈ RN ×M ×d be a tensor representing a batch of N sets, each containing up to M points with dimension d padded with arbitrary values where necessary. Let Ξ ∈ { 0, 1}N ×M be a binary mask where Ξn,m = 1 indicates a valid point and 

0 indicates padding. We introduce a learnable scaling parameter β ∈ Rd. For each set n ∈ { 1, . . . , N }, we compute the masked root mean square statistic νn over all valid points and features: 

νn =

vuutPMm=1 

Pdj=1 (Zn,m,j )2 · Ξn,m 

max(1 , d · PMm=1 Ξn,m ) + ϵ (6) where ϵ is a small constant for numerical stability. The normalized output ˆZ is then obtained by scaling the input element-wise: 

ˆZn,m,j = Zn,m,j 

νn

· βj (7) 14 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

# D. Model Architecture 

Figure 11. Visual depiction of the Flash-ANSR model architecture. The Set Transformer encoder ingests a variable-sized set of input-output pairs and produces a fixed-size latent representation via Induced Set Attention Blocks (ISAB) and Set Attention Blocks (SAB). The Transformer decoder autoregressively generates a symbolic expression token-by-token, attending to the encoded dataset at each step. 

# E. Expression Equivalence Checking & SimpliPy Rule Discovery 

Algorithm 1 Expression Equivalence Check 

1: Input: source τ , candidate τ ′, data X, challenges K = 16 , retries R = 16 

2: Let C be constants of τ ; C′ be constants of τ ′

3: for k = 1 to K do 

4: Sample r ∼ N (0 , σ = 5) |C|

5: for s ∈ {− 1, 0, 1}|C| do 

6: yτ ← fτ (X; |r| ⊙ s)

7: if |C′| = 0 then 

8: match ← allclose (fτ ′ (X), y τ )

9: else 

10: match ← False 

11: for j = 1 to R do 

12: p0 ∼ N (0 , σ = 5) |C′|

13: if ∃ˆc s.t. allclose (fˆτ (X; ˆ c), y τ ) with Levenberg-Marquardt initialized at p0 then 

14: match ← True; break 

15: end if 

16: end for 

17: end if 

18: if ¬match then 

19: return False 

20: end if 

21: end for 

22: end for 

23: return True 15 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

Algorithm 2 SimpliPy Rule Discovery 

1: Input: Operators O (Appendix H), Variables V, Literals C (Section 3.1), Limits Lmax = 7 , L tgt = 3 , Data X ∼N (0 , 5) 1024 ×D , number of challenges K = 16 and retries R = 16 

2: Initialize rule set R ← ∅ and pre-compute EL, the set of expressions of length L using O, V, C for L = 1 to Lmax 

3: for length L = 1 to Lmax do 

4: Initialize temporary rule cache Rnew ← ∅ 

5: for each expression τ ∈ E L do 

6: τ ∗ ← SIMPLI PYR(τ ); found ← False ; τ ′ 

> best

← None 

7: for length j = 1 to min( |τ ∗| − 1, L tgt ) do 

8: for each τ ′ ∈ E j where Vars (τ ′) ⊆ Vars (τ ) do 

9: if Equivalent (τ , τ ′; X, K, R ) then 

10: if τ ′ 

> best

= None or |C (τ ′)| < |C (τ ′

> best

)| then 

11: τ ′ 

> best

← τ ′

12: end if 

13: found ← True 

14: end if 

15: end for 

16: if found then 

17: Rnew ← R new ∪ { τ → τ ′

> best

}; break 

18: end if 

19: end for 

20: end for 

21: R ← R ∪ R new 

22: Update simplification engine with new rules in R

23: end for 

24: return R

# F. Test-Time Compute System 

All runtime-sensitive experiments for Section 5.1 (Time-Normalized Recovery Rate) were conducted on a single representa-tive high-end consumer workstation hardware setup (Table 4) to ensure comparability. 

Table 4. System specifications for Time-Normalized Recovery Rate experiment series. COMPONENT SPECIFICATION 

CPU AMD Ryzen 9 9950X (16C, 32T) GPU NVIDIA RTX 4090 (24 GB) RAM 64 GB DDR5 6000MT/s OS Ubuntu 24.04.3 LTS Python 3.13.7 PyTorch 2.9.1 + CUDA 13.0 

# G. Baselines 

Baseline methods are configured at or as close to their defaults as possible while matching the problem setting. For PySR, we allow the same operator set as F LASH -ANSR, excluding explicit integer multiplication and division operators. For NeSymReS, we evaluate the “100M” model trained on 100M expressions with the default 4 optimizer restarts. For E2E, we employ the released model1.pt with default refinement parameters (10 trees, 100 bags, 200 points per bag). To attempt evaluation at higher inference budgets (beam width ≥ 512 ), we apply a dynamic reduction of the maximum generation length to satisfy internal batching constraints. We could not get E2E to function beyond a beam width of 512. For both, we apply minimal compatibility patches to the official implementation to ensure basic functionality in the evaluation 16 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

environment. These patches are documented and made publicly available alongside our codebase. 

# H. Operators 

The set of operators supported by F LASH -ANSR and S IMPLI PY is summarized in Table 5. During training, we sample + ,

- , * , and / with weight 10 and all others with weight 1. 

Table 5. Flash-ANSR operator vocabulary. TOKEN DESCRIPTION TOKEN DESCRIPTION TOKEN DESCRIPTION 

+ Addition pow1 2 √x asinh arcsinh( x)

- Subtraction pow1 3 3

√x acosh arccosh( x)

* Multiplication pow1 4 4

√x atanh arctanh( x)

/ Division pow1 5 5

√x exp ex

abs |x| sin sin( x) log log( x)

inv 1 

> x

cos cos( x) mult2 2 · x

neg −x tan tan( x) mult3 3 · x

pow Exponentiation asin arcsin( x) mult4 4 · x

pow2 x2 acos arccos( x) mult5 5 · x

pow3 x3 atan arctan( x) div2 x

> 2

pow4 x4 sinh sinh( x) div3 x

> 3

pow5 x5 cosh cosh( x) div4 x

> 4

tanh tanh( x) div5 x

> 5

# I. Data Generation Example 

We sketch a single draw from our training sampler to illustrate how simplification, test-set guarding, and resampling work. 

1) Skeleton draw. We sample nops = 4 and produce the prefix skeleton with a free constant placeholder ⋄ :

¯τ (1) = [ + , mult2 , sin , x1 , pow2 , ⋄ ].

2) Simplification. SimpliPy ( Lmax = 4 ) absorbs pow2 into the constant placeholder and simplifies the expression to 

¯τ (1) ∗ = [ + , mult2 , sin , x1 , ⋄ ].

3) Test-set guard. We drop constants from both train candidates and held-out expressions, reducing the candidate to 

˜τ (1) = [ mult2 , sin , x1 ].Assume the held-out pool contains the expression as its i-th entry 

τ (i) 

> −

= [ mult2 , sin , / , x1 , ⋄ ].which reduces to 

˜τ (i) 

> −

= [ mult2 , sin , x1 ].Since ˜τ (1) = ˜ τ (i) 

> −

, we reject this instance and proceed to resampling. 

4) Resampling. A new skeleton is drawn with nops = 6 ,

¯τ (2) = [ * , / , x1 , pow2 , x2 , * , ⋄ , + , x1 , neg , x1 ].17 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

Simplification yields 

¯τ (2) ∗ = [ * , / , x1 , pow2 , x2 , ⋄ ].which, after removing constants, becomes 

˜τ (2) = [ / , x1 , pow2 , x2 ].No held-out expression matches this skeleton, so we proceed to data sampling. 

5) Constant and data draw. The constant is sampled from N (0 , σ = 5) as c1 = 1 .15 . We sample M = 137 data points with Xj ∼ U (aj , b j ) where aj , b j ∼ N (0 , σ = 10) . Evaluating y = f¯τ (2) (X; c1) yields finite, real values, so the instance is accepted without further resampling. 

# J. Data Statistics 

Figure 12. Training data statistics. Left: Empirical Cumulative Distribution Functions (ECDFs) of the number of constants, unique variables, variable nodes and the expression length in the training data. Half of the simplified training expressions contain at least 5 unique variables and 24 symbols. Right: 2D histogram showing the number of unique variables and the number of variable nodes. The training expressions cover both low and high-dimensional regimes with complexity arising both due to the coupling of like and unlike variables. 

# K. Training Dynamics and Scaling 

Figure 13. Training dynamics and scaling behavior. Left: Validation Cross-Entropy loss (log-scale) as a function of estimated training FLOPs (log-scale) We use the approximation FLOPS = 6 N D following Kaplan et al. (2020), and count FLOPS separately for the encoder with its dataset input and the decoder with its tokenized expression input before adding both for the total FLOPs. The performance envelope generally follows a power-law scaling relationship ( L ∝ FLOPs −0.14 ). Right: Evolution of gradient norms (pre-clipping). Optimization is stable across the 3M–120M regime. The 1B model exhibits one gradient spike late in training from which it fully recovers over the subsequent training steps. Further investigation is required to determine if this is a systemic issue at large model sizes. 

18 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

# L. Additional Metrics 

We use the following definition for the Fraction of Variance Unexplained (FVU): FVU (y, ˆy) = 

PNi=1 (yi − ˆyi)2

PNi=1 (yi − ¯y)2 (8) with ¯y = 1

> N

PNi=1 yi.Beyond the primary results, we report the following diagnostics with 95% confidence intervals derived from 1000 -fold bootstrapping. 

Log 10 FVU (non-perfect fits). For cases that are not numerically perfect ( FVU > ϵ 32 ), we summarize log 10 (FVU) . Exact fits are excluded before aggregation. 

Numeric Recovery Rate (NRR). For a test set of size K, the fraction of cases where the predicted expression achieves a numerically perfect fit: NRR = 1

K

> K

X

> k=1

I



FVU (y(k), ˆy(k)) ≤ ϵ32 



(9) with ϵ32 ≈ 1.19 × 10 −7 being the machine epsilon for 32-bit floating point numbers. 

Symbolic Recovery Rate (SRR). Exact structural match between simplified predicted and ground-truth prefix skeletons: SRR = 1

K

> K

X

> k=1

I



simplify (ˆ ¯τ (k)) = simplify ( ¯ τ (k))



. (10) 

Expression F1. Token-level F1 between the predicted prefix skeleton and the simplified ground-truth skeleton. 

ZSS Tree Edit Distance. Zhang–Shasha tree edit distance (Zhang & Shasha, 1989) between predicted and ground-truth skeleton trees. 

Variable Identification. Recall on the set of unique variables present in the simplified predicted vs. ground-truth expressions. 

Log Probability. Mean log-probability of the generated sequence under the model. 

Excess Constants. Difference in the number of constants between prediction and ground truth. Positive values indicate an overuse of constants to fit the data. 

Expression Length Ratio. Length of the predicted prefix skeleton divided by the length of the simplified ground-truth skeleton. 

Total Nestedness. Sum of consecutive nestings contributed by unary operators over the predicted expression as a proxy for naturalness and interpretability. For example, y = sin(log(arccosh( x))) has a total nestedness of 2, and y = sin(log( x1 +⋄)) 

has a total nestedness of 1.

Relative Fit Time. Fit-time multiplier relative to the hyperparameter configuration for the test-time compute experiment. 

Fit Success. Fraction of test cases where the fitting procedure completed without errors. 19 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

L.1. Detailed Test Time Scaling Results 

Figure 14. Detailed comparison various time-normalized numerical and symbolic metrics. With increasing time budget, F LASH -ANSR consistently generates more concise (lower expression length ratio), specific (fewer excess constants), and natural (lower total nestedness) expressions than PySR. 

L.2. Detailed Data Sparsity Results 

Figure 15. Detailed results for varying number of data points M on the F AST SRB benchmark. In the Interpolation Regime ( N ≈ 16 ), FLASH -ANSR maintains a lower mean total nestedness than PySR. 

20 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

L.3. Detailed Noise Level Scaling Results 

Figure 16. Detailed results for varying noise levels η on the F AST SRB benchmark. Across all noise levels, expressions by F LASH -ANSR maintain similar qualities. With higher noise levels, PySR still achieves shorter and more specific expressions. 

# M. SimpliPy Rules 

Table 6. Random subset of the simplification rules from the S IMPLI PY engine organized into pattern-replacement pairs. The shown expressions are one-dimensional, hence we use x as the variable token. The constant placeholder ⋄ represents an arbitrary finite constant. Infinities are carried through the simplification process to enable simplifications such as exp( − 1 

> x−x

) Cancel 

−−−→ exp( −∞ ) Rule 37 

−−−−→ 0.PATTERN → REPLACEMENT PATTERN → REPLACEMENT 

  −∞ 

> 4

 3√π → ∞ 4xπ → ⋄ · x

(−∞ )x · (e · x) → (−∞ )x (x · ∞ ) + x 

> NaN

→ NaN  

> 1/π
> 0/(−∞ )

→ ∞ x+( −∞ )sinh( π) → −∞ 

(∞ · 1) · π 

> −1

→ −∞ e 

> −1

· (−∞ )∞ → −∞ 

sinh( π)x−∞ → 0 (x · 0) −1/e → ∞

arcsin(0) 3√π → 0 (π · π) · 2( −∞ ) → −∞ 

(−1 − π) · (∞ · π) → −∞ sinh( −∞ )arctan( e) → ∞ 

> x+∞
> 5( −1)

→ −∞ (1 /2) x/ 3 → ⋄x 

> ∞π
> π+( −1)

→ ∞ arcsin( −1) · ∞ 4 → −∞  

> √−∞
> 4π

→ ∞ (ππ )∞3

→ ∞

21 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

# N. Per-Expression Results 

Figure 17. Median and 95% confidence intervals of the FVU with v23.0-120M, γ = 0 .05 , 8 optimizer restarts, 265k samples ≈ 1000s across individual expressions in the F AST SRB benchmark stratified by difficulty level following (Matsubara et al., 2024). All but three “easy” expressions are solved perfectly (left, green dots). In both “medium” and “hard” categories, we observe a large number of approximate fits (orange) and failures (red) around FVU ≈ 0 (gray dashed line) with increasing difficulty. 

22 Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression 

# O. Token Embeddings 

(a) 3M (b) 20M 

(c) 120M (d) 1B 

Figure 18. Token embeddings for the Flash-ANSR model family (v23.0) across model sizes. Across all scales, we observe high similarity of semantically related operators (e.g., binary operators, trigonometric functions, exponentials and powers) and variable tokens, and low similarity between different types of tokens (e.g. operators vs variables). The overall strength of these relationships varies across model sizes with no discernible pattern. 

23