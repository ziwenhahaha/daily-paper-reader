---
title: Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression
title_zh: 突破摊销神经符号回归中的简化瓶颈
authors: "Paul Saegert, Ullrich Köthe"
date: 2026-02-09
pdf: "https://arxiv.org/pdf/2602.08885v1"
tags: ["keyword:SR", "query:SR"]
score: 10.0
evidence: 摊销神经符号回归与表达式简化
tldr: 符号回归（SR）旨在发现可解释的解析表达式，但摊销神经符号回归在处理复杂科学问题时受限于表达式简化的计算瓶颈。本文提出 SimpliPy，一个比 SymPy 快 100 倍的规则化简化引擎，并构建了 Flash-ANSR 框架。该方法显著提升了训练和推理效率，在 FastSRB 基准测试中准确率超过同类摊销模型，且在保持高精度的同时能生成比 PySR 更简洁的表达式。
motivation: 现有的摊销符号回归方法依赖于计算缓慢的通用代数系统（如 SymPy）进行表达式归一化，严重限制了模型的训练规模和推理速度。
method: 开发了高性能规则化简化引擎 SimpliPy，并将其集成到 Flash-ANSR 框架中，实现了大规模训练集去重和高效的表达式标记预算利用。
result: SimpliPy 在保持简化质量的同时实现了百倍加速，使 Flash-ANSR 在准确率上显著优于 NeSymReS 和 E2E，并达到了与 PySR 相当的性能。
conclusion: 通过突破表达式简化的计算瓶颈，可以显著提升摊销符号回归的可扩展性，使其在处理复杂科学发现任务时兼具高效性与简洁性。
---

## 摘要
符号回归 (SR) 旨在发现能够准确描述观测数据的可解释解析表达式。摊销 SR 有望比主流的遗传编程 SR 方法更高效，但目前在扩展到现实科学复杂度方面仍面临挑战。我们发现，一个关键障碍是缺乏将等效表达式快速归约为简洁规范形式的方法。摊销 SR 曾通过 SymPy 等通用计算机代数系统 (CAS) 来解决这一问题，但高昂的计算成本严重限制了训练和推理速度。我们提出了 SimpliPy，这是一种基于规则的简化引擎，在质量相当的情况下，其速度比 SymPy 快 100 倍。这使得摊销 SR 得到了实质性的改进，包括对更大训练集的扩展性、对每个表达式 token 预算的更高效利用，以及针对等效测试表达式的系统性训练集去污染。我们在 Flash-ANSR 框架中展示了这些优势，该框架在 FastSRB 基准测试上的准确率远高于摊销基准模型（NeSymReS、E2E）。此外，它的性能与最先进的直接优化方法 (PySR) 相当，且随着推理预算的增加，它能恢复出更简洁而非更复杂的表达式。

## Abstract
Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.

---

## 论文详细总结（自动生成）

这篇论文针对摊销神经符号回归（Amortized Neural Symbolic Regression）中的计算瓶颈提出了创新方案。以下是对该论文的结构化总结：

### 1. 核心问题与整体含义
*   **研究动机**：符号回归（SR）旨在从数据中发现数学公式。传统的遗传编程（GP）方法虽精确但速度慢且无法利用先验知识；而“摊销 SR”通过预训练神经网络来快速预测公式。
*   **核心瓶颈**：摊销 SR 的性能高度依赖于训练数据的质量和多样性。为了保证公式简洁且唯一，必须进行“表达式简化”。然而，现有的计算机代数系统（如 SymPy）处理速度极慢，成为了实时数据生成和训练的瓶颈，导致现有模型要么训练集规模受限，要么放弃简化导致预测冗余。
*   **整体含义**：本文旨在通过开发极速的简化引擎，打破这一瓶颈，从而实现大规模、高质量的摊销符号回归训练。

### 2. 方法论
*   **SimpliPy 简化引擎**：
    *   **核心思想**：将复杂的代数归约转化为高效的模式匹配。
    *   **离线规则发现**：利用 Kruskal 算法变体，在离线阶段对短表达式空间进行搜索，构建一个“最小生成森林”形式的简化规则库。
    *   **在线模式匹配**：在训练/推理时，通过哈希索引和树匹配技术，对表达式进行快速替换和抵消。其速度比 SymPy 快约 100 倍，且质量相当。
*   **Flash-ANSR 框架**：
    *   **架构**：采用 Encoder-Decoder 结构。Encoder 使用改进的 **Set Transformer**（引入了 Masked RMSSetNorm）处理变长数据集；Decoder 使用 **Transformer Decoder**（配合 FlashAttention 和旋转位置编码 RoPE）生成公式。
    *   **实时数据生成**：利用 SimpliPy 的高速特性，在训练过程中实时生成、简化并过滤公式。
    *   **严格去污染（Decontamination）**：在训练前通过数值和符号双重检查，确保训练集中不包含任何与测试集等效的表达式，保证实验的严谨性。

### 3. 实验设计
*   **基准测试（Benchmark）**：主要使用 **FastSRB**，这是一个基于物理科学背景、具有真实数据域分布的挑战性基准。
*   **对比方法**：
    *   **摊销类基准**：NeSymReS、E2E。
    *   **优化类基准**：PySR（目前最先进的遗传编程方法）。
*   **评估维度**：推理时间与恢复率的 Pareto 前沿、数据稀疏性影响、噪声鲁棒性、公式简洁度（Parsimony）。

### 4. 资源与算力
论文详细列出了不同规模模型的训练资源：
*   **模型规模**：提供了 3M、20M、120M 和 1B（10 亿）四种参数规模。
*   **硬件与时长**：
    *   **3M**：RTX 2080Ti，耗时 112 小时。
    *   **20M**：RTX 4090，耗时 56 小时。
    *   **120M**：A100，耗时 340 小时。
    *   **1B**：H200，耗时 657 小时。
*   **训练数据量**：总计在 5.12 亿个实时生成的简化表达式对上进行训练。

### 5. 实验数量与充分性
*   **实验规模**：在 FastSRB 的 115 个复杂表达式上进行了测试，每个测试点重复多次实验（短时运行 10 次，长时运行 5 次）并计算 95% 置信区间。
*   **消融实验**：对训练先验（如常数采样范围）进行了消融分析。
*   **充分性评价**：实验设计非常充分且严谨。特别是引入了“推理时间标准化”对比，避免了不同方法因计算资源不均导致的评估偏差；同时，严格的去污染处理在同类论文中属于极高标准。

### 6. 主要结论与发现
*   **性能超越**：Flash-ANSR 在准确率上全面碾压了之前的摊销模型（NeSymReS, E2E），并在 120M 规模下达到了与 PySR 相当的水平。
*   **简洁度反转（Parsimony Inversion）**：这是一个重要发现。传统 GP 方法（如 PySR）随着搜索时间增加，公式往往变得更复杂（为了拟合残差）；而 Flash-ANSR 随着推理预算增加，能从后验分布中采样到更简洁、更接近真值的公式。
*   **缩放定律**：模型性能随计算量（FLOPs）呈现明显的幂律缩放关系。

### 7. 优点
*   **效率突破**：SimpliPy 解决了困扰该领域已久的简化速度问题，使大规模在线训练成为可能。
*   **工程严谨**：在数据去污染、推理时间公平对比、物理域评估等方面树立了新的学术标准。
*   **简洁性**：模型倾向于发现结构正确的简洁公式，而非通过增加复杂度来过拟合。

### 8. 不足与局限
*   **噪声鲁棒性**：在面对高噪声数据（$\eta \ge 10^{-2}$）时，性能下降比 PySR 快。这是因为模型在无噪声数据上训练，将噪声误认为是需要解释的复杂信号（分布偏移）。
*   **算力需求**：1B 规模模型的训练成本极高，普通研究机构难以负担。
*   **算子限制**：虽然支持广泛，但仍受限于预定义的算子集，对于极其罕见的特殊函数可能无法直接处理。

（完）
