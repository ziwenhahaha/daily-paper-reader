Title: Discovering Interpretable Algorithms by Decompiling Transformers to RASP

URL Source: https://arxiv.org/pdf/2602.08857v1

Published Time: Tue, 10 Feb 2026 03:43:54 GMT

Number of Pages: 101

Markdown Content:
## Discovering Interpretable Algorithms by Decompiling Transformers to RASP 

Xinting Huang * 1 Aleksandra Bakalova * 1 Satwik Bhattamishra 2 William Merrill 3 Michael Hahn 1

Abstract 

Recent work has shown that the computations of Transformers can be simulated in the RASP fam-ily of programming languages. These findings have enabled improved understanding of the ex-pressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. How-ever, it remains open whether trained models ac-tually implement simple interpretable programs. In this paper, we present a general method to ex-tract such programs from trained Transformers. The idea is to faithfully re-parameterize a Trans-former as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs. 1

1. Introduction 

Understanding the computations in trained Transformers is a central goal of mechanistic interpretability. Over the past few years, a complementary theoretical line has sharp-ened this question by giving symbolic characterizations of Transformer computation. In particular, the RASP language (Weiss et al., 2021) and its dialects (e.g. Strobl et al., 2025; Yang & Chiang, 2024; Zhou et al., 2024) simulate broad classes of Transformer architectures, and have been used to explain when and why models generalize on specific tasks. A recurring thesis in this literature is that simple programs imply robust generalization : if a task admits a short RASP 

> *

Equal contribution 1Saarland Informatics Campus, Saarland University 2University of Oxford 3Allen Institute for AI. Corre-spondence to: Xinting Huang <xhuang@lst.uni-saarland.de >,Aleksandra Bakalova <abakalov@lst.uni-saarland.de >.

Preprint. February 10, 2026. 

> 1

Code link: https://github.com/lacoco-lab/ decompiling_transformers 

program, then a Transformer trained on shorter inputs will generalize to longer inputs (RASP length generalization conjecture, Zhou et al., 2024; Huang et al., 2025). Yet, this story has a missing piece: Theoretical results show that Transformers can realize RASP-like computations, but they do not show that trained models do so in a human-interpretable way. Interpretability methods such as circuit discovery (e.g. Conmy et al., 2023) identify sparse computa-tion graphs supporting particular behaviors, but the resulting objects are circuits rather than programs : they are usually tied to specific input lengths and input templates, and do not directly yield an algorithm uniformly across input lengths. Lindner et al. (2023) map RASP programs to Transformers, but the inverse problem—recovering a compact symbolic program from a trained Transformer—has remained open. This gap is especially salient for the length-generalization conjecture: if length-generalizing models succeed because they internally implement short RASP programs, we would like to extract those programs and inspect them. In this work we introduce a general decompilation pipeline for recovering interpretable RASP-like programs from trained Transformers. Our starting point is a faithful repa-rameterization: we define a target dialect, Decompiled RASP (D-RASP) , whose primitives mirror Transformer computation while exposing intermediate variables in inter-pretable spaces such as token and position bases. Under a linearization assumption for layer normalization, we show that a GPT-2 style Transformer can be translated into a D-RASP program that exactly preserves its input–output behavior. Faithful translation alone does not yield inter-pretability: the naive program is exponentially large in depth, reflecting the many paths through the residual stream. Our key methodological contribution is thus a second step dis-covering a minimal sufficient sub-program . Using causal interventions, we prune program components while preserv-ing behavioral match to the original model, and replace learned components with simple primitives when possible. The result is often a short, readable program that implements the same algorithmic behavior as the trained Transformer. We validate this reparameterize-and-simplify approach on small GPT-2 style models trained on algorithmic and formal-language benchmarks. On models that length-generalize, de-compilation often recovers compact programs aligning with known hypotheses from theory and interpretability—for 1

> arXiv:2602.08857v1 [cs.LG] 9 Feb 2026 Interpretable Algorithms by Decompiling Transformers

example, histogram-based majority computation (cf Weiss et al., 2021), induction-head-based copying (Olsson et al., 2022; Zhou et al., 2024), and bracket counting in bounded-depth Dyck languages (Yao et al., 2021; Wen et al., 2023). In contrast, on non-length-generalizing models, decompila-tion does not succeed, suggesting that these models rely on more entangled, non-program-like mechanisms. Our results provide direct evidence that, at least in the con-trolled setting of small models trained on formal problems, length-generalizing Transformers can internally implement simple RASP-like algorithms—and that these algorithms can be extracted automatically as symbolic programs. 

2. Decompiled RASP (D-RASP) 

Here, we define our target dialect of RASP, Decompiled RASP (D-RASP). It is centered around selectors, aggre-gation, and element-wise operations as the original RASP (Weiss et al., 2021), but with adapted definitions. Let Σ be the (finite) alphabet, Σ = {σ1, . . . , σ |Σ|}, and let T ∈ N be the maximum context size of the model. We also use N to denote the length of a given input ( 1 ≤ N ≤ T )D-RASP has two types of variables: The first type is acti-vation variables , v ∈ Rd×N where d = d(v) is specific to this variable and specified in the program. For a variable v,we write d(v) for its dimensionality, so that v ∈ Rd(v)×N .In our decompiled programs, typical values for d(v) may be |Σ| (for encoding token information) or T (for encod-ing position information). We write v(i) for v·,i ∈ Rd(v).The second type is selector variables , α ∈ RN ×N ; these determine the aggregation of information across positions. Each program has access to two initial variables holding one-hot vectors for positions and tokens (Figure 2): 

pos (i)j =δj,i = I[j = i] pos ∈ RT ×N

token (i)j =δσj ,x i = I[σj = xi] token ∈ R|Σ|× N

where, for an input string x ∈ ΣN , xi ∈ Σ indicates the to-ken at position i. Other variables are created by operations. 

Selectors and Aggregation Self-attention operations are captured using select and aggregate operators. Given activation variables vk, v q , we produce a selector α ∈

RN ×N

α =select 



k= vk, q= vq , op= A



(A ∈ Rd(vq )×d(vk ))

α =select 



k= vk, op= b



(b ∈ R1×d(vk )),

evaluating to the tensor ( 1 ≤ s ≤ i ≤ N ): 

α(i, s ) = vq (i)⊤A vk(s) or 

α(i, s ) = b vk(s)

The A, b tensors are part of the program; they might be one of a set of hard-coded constructs from a library of primitives (e.g., the identity matrix), or something arbitrary. Selectors can only be used as input to an aggregation operation: 

v = aggregate(s= α1 + · · · + αp, v= w)

where αi are selector variables, w an activation variable; v

an activation variable; with output defined as 

v(i) = X

> j≤i

ai,j w(j) where 

ai,s = exp( α(i, s )1 + · · · + α(i, s )p)

P 

> s′≤i

exp( α(i, s ′)1 + · · · + α(i, s ′)p)

Next, elementwise operations capture MLPs. They take a set of variables and produce a a new variable: 

v = element wise op (v1, . . . , v s, func= f ) (1) where f : Rd(v1)×···× d(vs) → Rd(v) is an arbitrary function provided with the program; v and the inputs vj are activation variables. It evaluates to the tensor v ∈ Rd(v)×N :

v(i) = f (v1(i), . . . , v s(i)) (2) At the end, we compute next-token logits , p =

project (inp= v, op= A) defined as 

p(j) = A · v(j) (3) where A ∈ R|Σ|× d(v), d(p) = |Σ|; or p =

project(op = b) defined as p = b (b ∈ R|Σ|). We obtain the output as 

prediction (j) := Φ( p1(j) + · · · + ps(j)) (4) where Φ ∈ { softmax , sigmoid } depending on the task; each d(pi) = |Σ| and d(output ) = |Σ|.

Primitives D-RASP programs can use arbitrary tensors 

A, b for selectors and next-token projections, and arbitrary maps f for elementwise operations. We provide a library of tensors A, b for selector and next-token logits, and element-wise operations f . We provide the full library in Appendices G and H. Examples for A in select include the identity matrix (“ k == q ”) and an off-by-one shifted identity ma-trix (“ k == q-1 ”). We find it useful to define separate primitives for normal tokens and special tokens (BOS, EOS, SEP, PAD), provided using separate op= (for normal to-kens) and special op= (for special tokens) arguments. We only do this as syntactic sugar when using primitives; for a general non-primitive matrix A, we just write op= A.Examples for f in element wise op include a “hard-max” operation ( fharden (x) := earg max j x, i.e., creating a one-hot vector at the maximal component of the vector x). In decompilation, where possible, we will attempt to use these primitives, as detailed below. 2Interpretable Algorithms by Decompiling Transformers 

Decompiled program for MOST FREQUENT 

1. a1 = aggregate(s=[], v=token) 2. logits1 = project(inp=a1, op=(q==k), special_op=(uniform selection)) 3. prediction = softmax(logits1) 

(a) Code <bos>  

> o
> b
> r
> o
> <sep>
> o
> a
> b
> ...
> o
> p
> q
> r
> ...
> <bos>
> <sep>
> <eos>
> <pad>
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(b) Line 1: a1 a 

> b
> c
> d
> e
> ...
> <pad>
> a
> b
> c
> d
> e
> ...
> <pad> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(c) Line 2: op=(q==k), special op=(uniform selection) 

Figure 1. Program (a) for finding the most frequent character in a string, extracted from a real transformer. An example input is 

BOS o b r o SEP o ; the model is trained to predict the last token “ o”. Line 1 computes, at each position, the relative frequencies of symbols at preceding positions ( a1 ∈ R|Σ|× N , in (b). x-axis = input string, y-axis = variable dims., by aggregating over 

token ∈ R|Σ|× N (Figure 2b). In Line 1, s=[] indicates that no selector is used, i.e., the weights ai,j in aggregation are constant. Line 2 projects a1 to output logits via the matrix in (c) (rows = input dims., columns = output dims.). For ordinary tokens (covered by op= ), each token receives an output logit proportional to its frequency, corresponding to a temperature-scaled identity matrix. Logits for special tokens are excluded ( special op=(uniform selection) ), precluding outputting BOS/SEP on length-1 strings (App. Figure 43b has logits1 ). Overall, the program assigns the highest output logit to the most frequent non-BOS/SEP token. The program is extracted from a 1-layer 4-head transformer (App. J.6); an equivalent program results from a 4-layer 4-head transformer (App. J.7). <bos>  

> o
> b
> r
> o
> <sep>
> o
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> ...
> 153 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(a) pos <bos> 

> o
> b
> r
> o
> <sep>
> o
> a
> b
> ...
> o
> p
> q
> r
> ...
> <bos>
> <sep>
> <eos>
> <pad>
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(b) token 

Figure 2. Initial variables on the example input used in Figure 1. x-axis = input string ∈ ΣN ; y-axis = activation dimensions 

Comparison to other RASP dialects D-RASP differs from other RASP dialects (e.g. Weiss et al., 2021; Yang & Chiang, 2024; Strobl et al., 2025) in using softmax for aggregation, in keeping with Transformers. This is useful for enabling direct and faithful translation from Trans-formers; it enables both non-uniform attention (unlike C-RASP, Yang & Chiang (2024)) and attention over an unbounded number of position (unlike B-RASP, Yang et al. (2024)). The most relevant comparison for us here is to C-RASP, which has been linked to length generalizability of transformers (Yang & Chiang, 2024; Yang et al., 2025; Huang et al., 2025; Jobanputra et al., 2025; Chen et al., 2025; Jiang et al., 2025). Superficially, there is a major difference: C-RASP programs only produce counts and boolean outputs, whereas D-RASP produces general real-valued activations. However, if we slightly alter the semantics of D-RASP so that the outcome of 

aggregate or element wise op is rounded to some 

p-bit precision (which in practice happens on real-world hardware), and also enforce a rationality constraint on the parameters, then we identify a fragment equivalent to C-RASP (Theorem 2.1, proof in App. C). Merrill & Sabharwal (2023) show that log-precision transformers can be expressed in the logic FO(M). As C-RASP defines a strict subset of FO(M) (Huang et al., 2025), this fragment also defines a strict subset for FO(M). 

Theorem 2.1 (Correspondence of D-RASP and C-RASP) .

Let D be the class of D-RASP programs not using pos 

where all tensor entries Aij , b i (for the parameters of 

select , project ) are in {−∞ , } ∪ { log q : q ∈ Q+}.Then the programs in D, using the rounded semantics, define exactly the same functions as C-RASP. 

3. Decompilation Method 

Our pipeline applies to GPT-2 style models with absolute positional encodings. We focus on this architecture because the theory of length generalization is especially well de-veloped for such models (Zhou et al., 2024; Huang et al., 2025). Here, we describe our decompilation method. Our method consists of two steps: reparamaterization and sim-plification. The first part translates the transformer into a D-RASP program. The second one simplifies the program while empirically aiming to preserve faithfulness. 

3.1. Step 1: From Transformers to Programs 

We view a transformer as a length-preserving function that maps a sequence x ∈ Σ∗ with |x| = N (N ≤ T ) to a sequence of next-token logits in ∈ R|Σ|× N . Our reparame-terization applies to transformers satisfying: 

Assumption 3.1 (Linear Layer Norm Assumption) . A trans-former T satisfies the Linear Layer Norm Assumption 

(LLNA) iff each layer norm operation can be replaced with a linear operation for some constant parameter γ′ with neg-ligible change to the input-output behavior: 

x − x

pσ2(x) + ϵ γ + β ⇒ (x − x) γ′ + β (5) 3Interpretable Algorithms by Decompiling Transformers 

Motivation for this assumption comes from Baroni et al. (2025), who empirically find that trained language models can be finetuned to satisfy LLNA with little performance drop. We then show: 

Theorem 3.2. Consider a GPT-2-style transformer satisfy-ing LLNA. There is a D-RASP program defining the same input-output map; it can be explicitly obtained from the transformer’s parameters. Indeed, the residual stream at each layer can be recovered linearly from a set of variables in the D-RASP program. 

We provide the full formal proof in Appendix B.3. Here, we give an intuitive discussion of the construction. Each path in the transformer leading up to a given layer is rep-resented by a variable. Here, we provide an example. The variables pos , token simulate the input layer, which can be recovered as Etoken + P pos , where E ∈ Rd×| Σ| is the token embedding matrix, and P ∈ Rd×T is the position embedding matrix. We simulate the first layer as follows. For each attention head h = 1 , . . . , H , we define four se-lectors, describing the four possible key-query interactions between the two variables from the input layer:                                         

> α1,h =select (k=pos , q =token , op =ETQT
> 1,h K1,h P)
> α′
> 1,h =select (k=token , q =token , op =ETQT
> 1,h K1,h E)
> α′′
> 1,h =select (k=pos , q =pos , op =PTQT
> 1,h K1,h P)
> α′′′
> 1,h =select (k=token , q =pos , op =PTQT
> 1,h K1,h E)

where Kl,h , Ql,h ∈ Rdh×d project the residual stream onto the key or query activation of the h-th head in the l-th layer. We further define two output variables, reflecting aggrega-tion of token or position counts:                 

> v⟨pos, ⟨1,h ⟩⟩ =aggregate (α1,h +α′
> 1,h +α′′
> 1,h +α′′′
> 1,h ,pos )
> v⟨tok, ⟨1,h ⟩⟩ =aggregate (α1,h +α′
> 1,h +α′′
> 1,h +α′′′
> 1,h ,token )

By construction, the output of the original attention head can be recovered as 2

V1,h P v⟨pos, ⟨1,h ⟩⟩ + V1,h Ev⟨tok, ⟨1,h ⟩⟩ (6) The pre-MLP residual stream in layer 1 is recovered as the sum of these terms for all attention heads, plus Etoken +

P pos . The 2H variables defined in layer 1, plus token ,

pos jointly serve as input to an element wise op repre-senting the MLP, mapping to Rd. We thus have represented the residual stream in layer 1 in terms of 2+2 H +1 variables. This construction can be repeated recursively layer by layer. Now, by induction, any layer can be described as a linear combination of variables v1, . . . , v k, with some coefficient matrices C1, . . . , Ck, (Ci ∈ Rd×d(vi)) obtained from the         

> 2For simplicity of notation, we subsume the Vand Omatrices of GPT-2 into a single V∈Rd×dmatrix (App. B.1).

transformer parameters: Pki=1 Civi. For each of these vari-ables in the top layer, we add a project statement, with 

A defined in terms of the original model’s unembedding matrix U ∈ R|Σ|× d: for i = 1 , . . . , k . By construction, the original next-token logits equal Pki=1 pi ∈ R|Σ|× N . We show a full example (one-layer transformer) in Appendix Figure 9. The discussion above disregards layer norm. Un-der LLNA, the weights γ′ and β can be absorbed into the 

op= tensors. 

3.2. Step 2: Discovering Minimal Sufficient Programs 

In Step 2, we aim to simplify the program by (i) pruning components that are causally irrelevant, (ii) simplifying the matrices and vectors that appear in the program by referring, where possible, to primitives from the library. We mea-sure faithfulness by quantifying the match accuracy , i.e., the fraction of input instances where the program gives the same response as the original model. Formally, the match accuracy is the proportion of inputs where the pruned model (with MLP replaced) makes the same prediction (next to-ken with highest probability) as the original model in all 

token positions on which the model receives training signal. Throughout, we deem decompilation to be causally faithful 

when the match accuracy is ≥ 90% .

Step 2.1: Causal Pruning & Linearizing Layer Norm 

For each aggregate operator, we try to replace selector inputs with 0 or key-only selector if such replacements have negligible causal effect. For each element wise op op-erator, we try to remove vs inputs; each removed input is replaced with a constant absorbed into the transformation f .Meanwhile, we also try to remove inputs from the final soft-max, replacing them with a constant absorbed into the bias .Variables that do not enter into downstream components are removed. We show an example in Appendix, Figure 9. This Causal Pruning step is similar to standard causal ablation methods in interpretability and especially circuit discovery research (Conmy et al., 2023; Li & Janson, 2024; Bhaskar et al., 2024): they remove parts of the program that are not causally relevant to the final output. They can be viewed as removing edges in the computation graph. We caution that Causal Pruning is nontrivial, because the size of the program guaranteed by Theorem 3.2 is exponential in the original transformer’s size. To avoid unfolding the full pro-gram and to make pruning scalable, we (i) unfold and prune the program in a multi-stage procedure, and (ii) prune using a trainable gate, where the objective function consists of the KL divergence between the pruned model and the original model, along with a sparsity penalty. The technique is based on Li & Janson (2024). We give full details in Appendix F. Meanwhile, we also train γ′ (5) for each layer norm to minimize the KL divergence. We then replace all layer 4Interpretable Algorithms by Decompiling Transformers 

norms in a model with their linearized form: LayerNorm (x) ≈ Lx + β, L =



I − 11 T

d(x)



γ′

where 1 ∈ Rd(x) is a vector with all 1s. L parameters are absorbed into the attention and MLP parameters (App. B.2). 

Step 2.2: Explaining Per-Position Transforma-tions We try to explain element wise op by replacing them with known primitives. Before that, we try to simplify per-position transforma-tions v = element wise op (v1, . . . , v s, func= f )

as a sum of single-input operations wj =

element wise op (vj , func= fj ) (j = 1, . . . , s ), where f1, . . . , f s are re-fitted as MLPs to recover the original downstream model outputs as v(i) ≈ P 

> j

wj (v).The motivation for this is that single-input operations are likely to be, in general, easier to interpret. Any downstream appearance of v is replaced by w1, . . . , w s; these can then be individually pruned using causal pruning. In practice, we integrate this process with the causal pruning stage. We then try to replace element wise op operations with primitives in our library shown in Appendix G. The new output of element wise op has dimension determined by the selected primitive. The output w stays interpretable since the f and input v are interpretable. For example, if 

f is fno −op (x) = x and v is token , then the output di-mension is |Σ| instead of d. We fit a matrix C ∈ Rd×d(w)

to minimize ∥v − Cw∥2 

> F

, where C has a closed-form so-lution. We then replace all downstream occurrences of v

by w, and absorb C into the matrices associated with those downstream occurrences. For example, when v appears in project (v, A), we replace it by project (w, AC );similar change applies when v appears in select or 

element wise op . In sum, whereas the nonlinearity originally applies to activations ∈ Rd, it now maps between interpretable variables. If no primitive achieves high enough match accuracy, we interpret the element wise op by inspecting its inputs and outputs as follows. On the input side, we store the interpretable activation variables that are fed into the 

element wise op . On the output side, if the operation feeds into unembedding, then we can linearly transform it to have output dimension |Σ| and see which output tokens are promoted given different activation variable as input; if it feeds as k (similarly q) into select , we check what activation variables of q (similarly k) are associated with dif-ferent inputs; in other words, we inspect pairs of activation variables that result in high attention logit. This technique can be viewed as a principled adaptation of LogitLens (nos-talgebraist, 2020), but unlike that technique also ensures that downstream effects through paths other than the residual stream are correctly accounted for (App. G.5). 

Step 2.3: Explaining tensors in select and project 

To explain A, b tensors in select and project opera-tors, and the bias in the final softmax, we replace them with a matrix from the library, whenever this is causally faithful. When this is not possible, we optimize the matrices to be sparse and have integer values. See Appendix H for the library of primitives and details of the optimization process. 

4. Results 

We train small GPT2-like models on a suite of algorithmic problems based on Huang et al. (2025); Zhou et al. (2024), and on a battery of finite-state languages from Bhattamishra et al. (2020) (list in Appendix D). We view the algorithmic problems as sequence prediction, predicting a single token or a sequence ending in EOS. For the formal languages, we instead have the model at each step output the set of allowed next tokens (Bhattamishra et al., 2020; Sarrof et al., 2024; Huang et al., 2025), which we implement by feeding the logits into sigmoid to obtain a validity label ∈ [0 , 1] 

for each symbol. As formal languages have no inherent probability distribution over strings, this is a simpler strategy than language modeling. Following the setup of Huang et al. (2025), we train models at input lengths ≤ 50 and evaluated length generalization at input lengths in [51 , 150] , because we are interested in how length-generalization relates to interpretability of models. Random offsets ensure that all position embeddings are trained. See details in Appendix D.2. The decompilation pipeline (Step 2 in Sec. 3) is run on inputs of length ≤

150 , the match accuracy is measured at length ≤ 150 on random i.i.d. samples. For each task, we train models with different hyperparameters (e.g., number of layers and heads 

∈ { 1, 2, 4}, model dimensions ∈ { 16 , 64 , 256 }, etc.). We keep at least one representative model for each task, and for some algorithmic tasks also have additional models with different architecture and different length-generalization performance. For each task, we keep the model with best length generalization performance across all models trained with different hyperparameters. For each model, we trace out a Pareto frontier (match accuracy vs. program length) by varying the sparsity coefficients in Step 2.1. We declare decompilation to be successful if some programs have match accuracy ≥ 0.9, and select the shortest one. 

When does Decompilation succeed? Eight algorithmic tasks showed length generalization at least at some hyperpa-rameters, three did not. Results (App. Table 1) show that the length-generalizing models can be decompiled, and models that do not length-generalize could not be decompiled. This agrees with and supports a connection between the existence 

of (short) RASP program and length-generalization hypoth-esized previously (Zhou et al., 2024; Huang et al., 2025). A 5Interpretable Algorithms by Decompiling Transformers 

Decompiled program for UNIQUE COPY 

1. s1 = select(q=pos, k=pos, op=(k==q-1)) 2. a1 = aggregate(s=s1, v=token) 3. s2 = select(q=token, k=a1, op=(k==q), special_op=(k==BOS)) 4. a2 = aggregate(s=s2, v=token) 5. logits1 = project(inp=a2, op=(inp==out), special_op=(uniform selection)) 6. logits2 = project(op= i ) 7. prediction = softmax(logits1+ logits2) 0 

> 1
> 2
> 3
> 4
> ...
> 302
> 0
> 1
> 2
> 3
> 4
> ...
> 302 0
> 2000
> 4000
> 6000
> 8000
> 10000

(a) k==q-1 <bos>  

> 1
> 3
> 4
> 2
> <sep>
> 1
> 3
> 4
> 2
> <eos>
> <bos>
> 1
> 3
> 4
> 2
> <sep>
> 1
> 3
> 4
> 2
> <eos>
> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(b) Line 1: s1 <bos>   

> 1
> 3
> 4
> 2
> <sep>
> 1
> 3
> 4
> 2
> <eos>
> 0
> 1
> 2
> 3
> 4
> ...
> <bos>
> <sep>
> <eos>
> <pad> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(c) a1 0 

> 1
> 2
> 3
> 4
> ...
> <pad>
> 0
> 1
> 2
> 3
> 4
> ...
> <pad> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(d) k==q <bos>  

> 1
> 3
> 4
> 2
> <sep>
> 1
> 3
> 4
> 2
> <eos>
> <bos>
> 1
> 3
> 4
> 2
> <sep>
> 1
> 3
> 4
> 2
> <eos>
> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(e) Line 3: s2 <bos>   

> 1
> 3
> 4
> 2
> <sep>
> 1
> 3
> 4
> 2
> <eos>
> 0
> 1
> 2
> 3
> 4
> ...
> <bos>
> <sep>
> <eos>
> <pad> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(f) a2 0 

> 1
> 2
> 3
> 4
> ...
> <pad>
> 0
> 1
> 2
> 3
> 4
> ...
> <pad> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(g) inp==out <bos>   

> 1
> 3
> 4
> 2
> <sep>
> 1
> 3
> 4
> 2
> <eos>
> 0
> 1
> 2
> 3
> 4
> ...
> <pad> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(h) Line 5: logits1 ...  

> 144
> 145
> 146
> 147
> 148
> 149
> <bos>
> <sep>
> <eos>
> <pad>
> 2.5
> 0.0
> 2.5

(i) Line 6: logits2 

Figure 3. An extracted program for copying a string without repeti-tions: The input BOS 1 3 4 2 SEP is completed with the string 1 3 4 2 EOS . The primitive “ k == q-1 ” (line 1) selects the position immediately preceding the query position; it is given by the matrix in (a) (rows = query dimen-sions; columns = key dimensions) and yields the selector in (b) (x-axis = key positions; y-axis = query positions). The variable 

a1 (c) (x-axis = positions in the input; y-axis = dimensions of the variable) stores the previous token at each position, as a one-hot vector. The variables token and a1 jointly encode the bigrams appearing in the string, which is sufficient for copying a string without repetitions. The variable a2 (f) then stores the token to be output next. It is obtained by finding the position j where a1(j) 

equals token(i) , and retrieving token(j) into a2(i) . Here, the primitive “ k==q ” selects a matching token; it is given by the matrix in (d). In these cases, there is special behavior on the spe-cial tokens, ensuring that the transformer produces the first token in the given sequence after ⟨sep ⟩. a2 is directly forwarded into 

logits1 (h) by a scaled identity matrix (g). The bias in line 6 (i) favors EOS in the absence of other outputs, ensuring EOS is output at the end. The program recapitulates the “induction head” motif; importantly, our decompilation pipeline provides it fully automatically. 

Figure 4. Causal Pruning (Step 2.1): Pareto frontiers over lines of code vs. match accuracy for models satisfying LLNA. 

Decompiled program for SORT 

1. s1 = select(q=token, k=token, op= a ) 2. a1 = aggregate(s=s1, v=token) 3. new_a1 = element_wise_op(a1) 4. logits1 = project(inp=new_a1) 5. prediction = softmax(logits1) 0

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> ...
> <sep>
> <eos>
> <pad>
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> ...
> <sep>
> <eos>
> <pad>
> 200
> 0
> 200
> 400

(a) Line 1: op= a in s1 <bos>  

> 10
> 15
> 4
> 9
> <sep>
> 4
> 9
> 10
> 15
> <eos>
> ...
> 4
> ...
> 9
> 10
> ...
> 15
> ...
> <bos>
> <sep>
> <eos>
> <pad>
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(b) Line 1: a1 3        

> 4
> ...
> 9
> 10
> 11
> 12
> ...
> 67
> 68
> ...
> 76
> 0.96
> 0.96
> 0.88 0.08
> 0.99
> 0.63 0.21 0.09
> 0.07 0.10 0.56 0.21
> 0.96
> 0.68 0.32
> 0.75 0.17
> Input Features
> 9
> 10
> 11
> Output
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 5
> 0
> 5
> 10
> 15

(c) Example for Per-Position Transformation 

Figure 5. An extracted program for sorting a list of integers: The input BOS 10 15 4 9 SEP is completed with the string 4 9 10 15 EOS . Line 1 uses a select 

operation given by the matrix in (a). Entries are largest when k

is slightly larger than q, and then fall off. As a result, a1 collects tokens bigger than the current one, and among those favoring the smallest one (b). SEP always collects the smallest number to start counting. Line 3 performs a nonlinear operation f : R|Σ| → Rd,not replaced by a primitive. We inspect behavior on twelve sample inputs a1 (i) ∈ R|Σ|, each denoted by a row in (c, left), together with the associated output vector new a1 (i) ∈ R|Σ| (c, right). The operation “hardens” the input, selecting the dimension for which entries are largest in the input (e.g., 9, 10, 11 in the examples), effectively making the vector one-hot. When there is no number bigger than the current one, the attention is diffuse (as shown in the matrix in (a), entries are very similar when k is ≤ q), making the input a1 also diffuse. The same operation in Line 3 then maps this type of inputs to EOS (Fig. 51 shows that EOS is promoted when none of the non-BOS entry contains decisively high value). App. J.8 has more heatmaps. 

6Interpretable Algorithms by Decompiling Transformers 

similar trend is also observed for formal languages (App. Table 2). 14 languages showed length generalization; de-compilation succeeded on 9 of them. 3 languages did not show length generalization, and decompilation failed. Thus, on the formal languages, decompilation succeeds in the majority of length-generalizing models. Moreover, we find that, when decompilation succeeds, we can usually vastly cut down the size of programs compared to the initial D-RASP reparameterization (Figure 4). For instance, for finding the most frequent character in a string, we trained 1-layer 4-head and 4-layer 4-head models, with D-RASP translations of 56 and 16,201,616 lines, respec-tively. Causal pruning results in equivalent 3-line programs in both cases. In contrast, for non-length-generalizing mod-els, even if keeping layer norm on, pruning usually has only very limited success (App. Figure 12). Thus, interestingly, even though all models are transformers of similar sizes, generalizable models tend to be more interpretable. 

4.1. Decompilation recovers interpretable algorithms 

We show all decompiled programs in App. J–K. We show a program extracted for finding the most frequent character in a string in Figure 1: a select +aggregate operation aggregates a histogram of tokens, which directly determines the output logits (Figure 1(c), so that the most frequent symbol has the highest output logit. We show a program extracted from a transformer trained to 

copy a string in which each token is unique (Unique Copy) in Figure 3. In this case, all operations can be expressed in terms of predefined primitives. The first 

select +aggregate annotates each position with the directly preceding symbol; the second one then corresponds to an “induction head” (Olsson et al., 2022): it retrieves whichever prior symbol was preceded by the current sym-bol. This indeed is the RASP mechanism hypothesized to perform copying without repetition in Zhou et al. (2024); Huang et al. (2025). For a transformer trained to copy back-wards , we find a corresponding program implementing an “anti-induction” head operation (App. J.12). For transformers trained to copy strings in which each bi-gram is unique , we find (i) a program that encodes trigram statistics into a variable of dimension |Σ|3 via a per-position operation, and (ii) a different program that only uses selec-tion/aggregation operations (App. J.9, J.10). We show an extracted program for sorting a list of integers 

in Figure 5. Here, operations are not expressed in terms of primitives, but the output of decompilation is nonethe-less interpretable: there is a select operation favoring the smallest keys larger then the query, and a per-position operation making the aggregated histogram vector one-hot. 0 

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> ...
> <pad>
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> ...
> <pad>
> 0
> 5
> 10
> 15
> Figure 6. See text. Rows denote input dimensions; Columns denote output dimensions.

In a counting task (Zhou et al., 2024), the model completes e.g. 

⟨bos ⟩ 12 16 ⟨sep ⟩ with 

12 13 14 15 16 ⟨eos ⟩.Incrementing counts while counting is done by a projection matrix acting directly on token : p1 =

project(token), op= Figure 6 )); it maps “0” to “1”, “1” to “2”, etc. Simultaneously, aggregate 

operations control behavior at the beginning and end of counting (App. J.4). We next discuss formal languages. For a few strictly local languages , where each symbol determines the next possible symbols, no aggregate is needed. For instance, for a+b+c+d+e+, the program obtains the output logits as project(token) where the op= 

matrix (Figure 7) maps each symbol to the possible next symbols: “a” and “b” can follow “a”; “b” and “c” can follow “b”; “e” and EOS can follow “e”. a 

> b
> c
> d
> e
> <bos>
> <sep>
> <eos>
> <pad>
> a
> b
> c
> d
> e
> <bos>
> <sep>
> <eos>
> <pad>
> 10
> 5
> 0
> 5
> Figure 7. See text. Rows denote input dimensions; Columns denote output di-mensions

The bench-mark includes 

bounded-depth Dyck languages 

at various depths. The models for D2, D4, D12 were decompiled; the model for D3 failed in the layer norm linearization. The algorithms are similar, but the shortest program (Figure 8) is found for D4, i.e., the language of well-nested strings over one pair of parentheses (opening “a” and closing “b”) with depth at most four (equivalently, the formal language 

(a(a(a(ab )∗b)∗)b)∗): uniform aggregation creates the histogram of BOS, opening, and closing brackets; a per-position activation then performs a threshold calculation: e.g., EOS is allowed only when a and b are balanced. At two other depths (D2, D12), the aggegation intriguingly assigns different select weights to “a” and “b”; this nonuniformity cascades into somewhat more complicated variants of the same algorithm (App. K.1,K.2). 

5. Discussion 

Our results support and strengthen the RASP length gener-alization conjecture (Zhou et al., 2024; Huang et al., 2025): 7Interpretable Algorithms by Decompiling Transformers 

Decompiled program for D4 

1. a1 = aggregate(s=[], v=token) 2. new_a1 = element_wise_op(a1) 3. logits1 = project(inp=new_a1, op=(inp==out)) 4. prediction = sigmoid(logits1) <bos> 

> a
> a
> a
> a
> b
> b
> b
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad> 0.0 

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(a) Line 1: a1 a                                            

> b
> <bos>
> 0.50 0.00 0.50
> 0.67 0.00 0.33
> 0.51 0.48 0.01
> 0.51 0.49 0.01
> 0.50 0.25 0.25
> 0.00 0.00 1.00
> 0.33 0.33 0.33
> 0.80 0.00 0.20
> 0.71 0.14 0.14
> 0.67 0.22 0.11
> 0.64 0.27 0.09
> Input Features
> a
> b
> <eos>
> 52.32 17.94 -20.81
> 51.20 7.66 -8.02
> 2.67 7.77 -10.40
> 2.54 8.19 -10.92
> 32.85 10.34 -12.54
> 270.38 -95.60 246.57
> 83.41 -32.74 79.60
> -9.56 6.22 -9.10
> -7.44 6.36 -9.06
> -6.65 6.45 -9.09
> -6.28 6.53 -9.15
> Output
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(b) Line 2: element wise op 

Figure 8. Decompiled program for depth-4 Dyck language, on sam-ple input BOS a a a a b b b b EOS .The program at each step outputs the set of allowed next char-acters. a1 contains the relative frequencies of a, b, BOS. Line 2 applies an element-wise operation, whose output is directly forwarded to next-token predictions in Line 3 via the identity ma-trix ( project(...,op=(inp==out)) ). We show the input-output behavior in line 2 on 11 sample inputs a1 ∈ R|Σ| in (b). “a” is allowed (i.e., receives positive logit) only when #a-#b <

4 · #BOS; “b” is allowed when #a > #b, and “EOS” is allowed when the string is exactly balanced (#a = #b). More in App. K.3. 

Prior work suggests that the presence of length generaliza-tion tracks (C-)RASP expressiveness (Huang et al., 2025). Our work strengthens this link, confirming that, when trans-formers show length generalization on synthetic tasks, they in fact often internally implement simple RASP algorithms. Translations from transformers to logics from the theoreti-cal literature on transformers, such as logic with majority quantifiers (Merrill & Sabharwal, 2023), C-RASP (Yang et al., 2025), and first-order logic (Li & Cotterell, 2025), usu-ally create one formula for each dimension of the residual stream, which means that the size of the program will grow with the hidden dimension, and the meaning of the individ-ual formulas is not directly interpretable, as the dimensions of the residual stream are not in general individually in-terpretable. A key advantage of our D-RASP translation over these prior theoretical translations is that the size of the resulting program is independent of the transformer’s hidden dimension, with embedding, query, key, value, un-embedding transformations all absorbed into matrices; in many cases, these have interpretable dimensions such as 

|Σ|. This is a powerful property especially when models are overparameterized for optimization reasons. Tracr (Lindner et al., 2023) compiles RASP programs to transformers. Multiple works aim to recover RASP pro-grams from Tracr-generated models (Baker, 2023; Thurn-herr & Riesen, 2024; Langosco et al., 2024). Our work dif-fers by decompiling models trained with standard methods, rather than models specifically constructed to mirror RASP algorithms. Friedman et al. (2023); Lai-Dang et al. (2025); Zhang et al. (2026) train transformer variants amenable to symbolic interpretation, e.g. with discrete representations and attention weights. Our work instead applies to normal transformers trained with standard methods. The most prominent approach to understanding the internal computations of Transformers is circuit discovery (e.g. El-hage et al., 2021; Wang et al., 2023; Conmy et al., 2023). This approach views the transformer’s computation as a computation graph and finds a subgraph faithfully repre-senting the computation of the full transformer. The major difference to our work is that we discover programs rather than computation graphs . With this, we address a major challenge to circuit discovery, namely providing abstrac-tions of model behavior that are valid across a broad range of input strings. The reason is that computation graphs dis-covered by circuit discovery are tightly linked to specific input templates; generalizing beyond individual templates is nontrivial. Programs over sequences are more natural representations of algorithms across input lengths, whereas circuits are generally specific to a given input length. De-compilation addresses this challenge, at least in the setting of small transformers trained on algorithmic problems: pro-grams apply to the full input space. An example is the unique copy task (Figure 3), where standard circuit discov-ery would lead to a circuit where the final prediction is connected to all positions with redundant connections, as the bigram relevant to the next prediction might be located anywhere in the input. In contrast, programs abstract these connections into a single aggregation operation. This dif-ference is even more pronounced in the context of length generalization: a circuit is tied to a specific input length, whereas our method allows us to find length-generalizing RASP programs. An intriguing question is whether our method can also be used for interpreting LLMs as a comple-ment to circuit discovery; we leave this to future work. 

6. Conclusion 

We introduce a method for extracting simple programs from transformers trained on individual synthetic tasks. Our re-8Interpretable Algorithms by Decompiling Transformers 

sults constitute the most direct evidence so far that length-generalizing transformers trained on algorithmic and formal language problems often implement interpretable RASP programs. It is an interesting question if this also applies to language models, which are effectively trained to perform a large variety of tasks simultaneously. It is plausible that language models might implement modular sub-programs for different subtasks; testing this is an interesting problem for future research. 

Impact Statement 

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. 

Acknowledgments 

This research is funded by the Deutsche Forschungsgemein-schaft (DFG, German Research Foundation) – Project-ID 232722074 – SFB 1102 “Information Density and Linguis-tic Encoding”; Project-ID 471607914 – GRK 2853/1 “Neu-roexplicit Models of Language, Vision, and Action”. MH thanks the participants of the Dagstuhl Seminar “Theory of Neural Language Models” for useful discussion. 

References 

Baker, W. Inverse tracr: Mapping neural network weights to code. Master’s thesis, University of Cambridge, 2023. URL https://william-baker.github. io/Inverse_Tracr.pdf .Baroni, L., Khara, G., Schaeffer, J., Subkhankulov, M., and Heimersheim, S. Transformers don’t need LayerNorm at inference time: Scaling LayerNorm removal to GPT-2 XL and the implications for mechanistic interpretability. 

arXiv preprint arXiv:2507.02559 , 2025. URL https: //arxiv.org/abs/2507.02559 .Bhaskar, A., Wettig, A., Friedman, D., and Chen, D. Finding transformer circuits with edge pruning. Advances in Neu-ral Information Processing Systems , 37:18506–18534, 2024. URL https://openreview.net/forum? id=8oSY3rA9jY .Bhattamishra, S., Ahuja, K., and Goyal, N. On the ability and limitations of transformers to rec-ognize formal languages. In Proceedings of the 2020 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP) , pp. 7096–7116, 2020. URL https://aclanthology.org/2020. emnlp-main.576/ .Butoi, A., Khalighinejad, G., Svete, A., Valvoda, J., Cot-terell, R., and DuSell, B. Training neural networks as recognizers of formal languages. In The Thirteenth Inter-national Conference on Learning Representations , Sin-gapore, April 2025. URL https://openreview. net/forum?id=aWLQTbfFgV .Chen, T., Ma, T., and Li, Z. Non-asymptotic length general-ization. In Forty-second International Conference on Ma-chine Learning , 2025. URL https://openreview. net/forum?id=WZlq625BWD .Chiang, D. and Cholak, P. Overcoming a theoretical lim-itation of self-attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 7654–7664, 2022. URL https://aclanthology.org/2022. acl-long.527/ .Conmy, A., Mavor-Parker, A., Lynch, A., Heimersheim, S., and Garriga-Alonso, A. Towards automated circuit discovery for mechanistic interpretability. Advances in Neural Information Processing Systems , 36:16318– 16352, 2023. URL hhttps://openreview.net/ forum?id=89ia77nZ8u .Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. A math-ematical framework for transformer circuits. Trans-former Circuits Thread , 2021. https://transformer-circuits.pub/2021/framework/index.html. Friedman, D., Wettig, A., and Chen, D. Learning trans-former programs. In Oh, A., Naumann, T., Glober-son, A., Saenko, K., Hardt, M., and Levine, S. (eds.), 

Advances in Neural Information Processing Systems ,volume 36, pp. 49044–49067. Curran Associates, Inc., 2023. URL https://openreview.net/forum? id=Pe9WxkN8Ff .Haklay, T., Orgad, H., Bau, D., Mueller, A., and Belinkov, Y. Position-aware automatic circuit discovery. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Pro-ceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,pp. 2792–2817, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.141. URL https: //aclanthology.org/2025.acl-long.141/ .Hanna, M., Liu, O., and Variengien, A. How does gpt-2 compute greater-than?: Interpreting mathematical abili-ties in a pre-trained language model. Advances in Neu-ral Information Processing Systems , 36:76033–76060, 9Interpretable Algorithms by Decompiling Transformers 

2023. URL https://openreview.net/forum? id=p4PckNQR8k .Huang, X., Yang, A., Bhattamishra, S., Sarrof, Y., Krebs, A., Zhou, H., Nakkiran, P., and Hahn, M. A formal framework for understanding length generalization in transformers. In The Thirteenth International Conference on Learning Representations , 2025. URL https:// openreview.net/forum?id=U49N5V51rU .Izzo, Z., Nichani, E., and Lee, J. D. Quantitative bounds for length generalization in transformers. In High-dimensional Learning Dynamics 2025 , 2025. URL 

https://arxiv.org/abs/2510.27015 .Jiang, H., Hahn, M., Zetzsche, G., and Lin, A. W. Soft-max transformers are turing-complete. arXiv preprint arXiv:2511.20038 , 2025. URL https://arxiv. org/abs/2511.20038 .Jobanputra, M., Veitsman, Y., Sarrof, Y., Bakalova, A., Dem-berg, V., Pavlick, E., and Hahn, M. Born a Transformer – always a Transformer? On the effect of pretraining on architectural abilities. In The Thirty-ninth Annual Conference on Neural Information Processing Systems ,2025. URL https://openreview.net/forum? id=Huw15LqglI .Lai-Dang, Q.-V., Kang, T., and Son, S. Adaptive trans-former programs: Bridging the gap between performance and interpretability in transformers. In The Thirteenth International Conference on Learning Representations ,2025. URL https://openreview.net/forum? id=W8K8slZ73R .Langosco, L., Baker, W., Alex, N., Bradley, H., Quarel, D., and Krueger, D. Towards meta-models for automated interpretability, 2024. URL https://openreview. net/forum?id=1zDOkoZAtl .Li, J. and Cotterell, R. Characterizing the expressivity of fixed-precision transformer language models. In 

The Thirty-ninth Annual Conference on Neural Infor-mation Processing Systems , 2025. URL https:// openreview.net/forum?id=29LwAgLFpj .Li, M. and Janson, L. Optimal ablation for interpretability. 

Advances in Neural Information Processing Systems , 37: 109233–109282, 2024. URL https://openreview. net/forum?id=opt72TYzwZ .Lindner, D., Kram ´ar, J., Farquhar, S., Rahtz, M., Mc-Grath, T., and Mikulik, V. Tracr: Compiled transformers as a laboratory for interpretability. Advances in Neu-ral Information Processing Systems , 36:37876–37899, 2023. URL https://openreview.net/forum? id=tbbId8u7nP .Liu, B., Ash, J., Goel, S., Krishnamurthy, A., and Zhang, C. Exposing attention glitches with flip-flop language modeling. Advances in Neural Information Process-ing Systems , 36:25549–25583, 2023. URL https: //openreview.net/forum?id=VzmpXQAn6E .Merrill, W. and Sabharwal, A. A logic for express-ing log-precision transformers. Advances in neu-ral information processing systems , 36:52453–52463, 2023. URL https://openreview.net/forum? id=uR8TtWCIsr .nostalgebraist. interpreting gpt: the logit lens. LESS-WRONG , 2020. https://www.lesswrong. com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens .Olsson, C., Elhage, N., Nanda, N., Joseph, N., Das-Sarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. In-context learning and induction heads. CoRR ,abs/2209.11895, 2022. URL https://doi.org/10. 48550/arXiv.2209.11895 .Sarrof, Y., Veitsman, Y., and Hahn, M. The expressive capac-ity of state space models: A formal language perspective. 

Advances in Neural Information Processing Systems , 37: 41202–41241, 2024. URL https://openreview. net/forum?id=eV5YIrJPdy .Strobl, L., Angluin, D., Chiang, D., Rawski, J., and Sabhar-wal, A. Transformers as transducers. Transactions of the Association for Computational Linguistics , 13:200–219, 2025. URL https://aclanthology.org/2025. tacl-1.9/ .Thurnherr, H. and Riesen, K. Neural Decompiling of Tracr Transformers , pp. 25–36. Springer Nature Switzer-land, 2024. ISBN 9783031716027. doi: 10.1007/ 978-3-031-71602-7 3. URL http://dx.doi.org/ 10.1007/978-3-031-71602-7_3 .Wang, K. R., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. Interpretability in the wild: a circuit for in-direct object identification in gpt-2 small. In The Eleventh International Conference on Learning Representations ,2023. URL https://openreview.net/forum? id=NpsVSN6o4ul .Weiss, G., Goldberg, Y., and Yahav, E. Thinking like transformers. In International Conference on Machine Learning , pp. 11080–11090. PMLR, 2021. URL http://proceedings.mlr.press/v139/ weiss21a.html .10 Interpretable Algorithms by Decompiling Transformers 

Wen, K., Li, Y., Liu, B., and Risteski, A. Transform-ers are uninterpretable with myopic methods: a case study with bounded dyck grammars. Advances in Neu-ral Information Processing Systems , 36:38723–38766, 2023. URL https://openreview.net/forum? id=OitmaxSAUu .Yang, A. and Chiang, D. Counting like transformers: Compiling temporal counting logic into softmax trans-formers. In First Conference on Language Modeling ,2024. URL https://openreview.net/forum? id=FmhPg4UJ9K .Yang, A., Chiang, D., and Angluin, D. Masked hard-attention transformers recognize exactly the star-free languages. Advances in Neural Information Process-ing Systems , 37:10202–10235, 2024. URL https: //openreview.net/forum?id=FBMsBdH0yz .Yang, A., Cadilhac, M., and Chiang, D. Knee-deep in c-RASP: A transformer depth hierarchy. In The Thirty-ninth Annual Conference on Neural Information Processing Systems , 2025. URL https://openreview.net/ forum?id=jPduiyxyfw .Yao, S., Peng, B., Papadimitriou, C., and Narasimhan, K. Self-attention networks can process bounded hierarchical languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 3770–3785, 2021. URL https://aclanthology.org/2021. acl-long.292/ .Yin, K. and Steinhardt, J. Which attention heads matter for in-context learning? In Forty-second International Conference on Machine Learning , 2025. URL https: //openreview.net/forum?id=C7XmEByCFv .Zhang, Y., Bi, W., Zhang, K., Jin, D., Fu, J., and Jin, Z. Weights to code: Extracting interpretable algorithms from the discrete transformer, 2026. URL https://arxiv. org/abs/2601.05770 .Zhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O., Susskind, J. M., Bengio, S., and Nakkiran, P. What Algorithms can Transformers Learn? A Study in Length Generalization. In International Conference on Learning Representations , 2024. URL https://openreview. net/forum?id=AssIuHnmHX .11 Interpretable Algorithms by Decompiling Transformers 

A. FAQ 

1. Why use APE, rather than positional encodings more popular in modern LLMs? 

We use APE mainly because theoretical understanding of length generalization in terms of RASP is best-developed for APE (Zhou et al., 2024; Huang et al., 2025; Izzo et al., 2025). We also note that a lot of interpretability research has taken GPT-2 as a reference (e.g. Wang et al., 2023; Hanna et al., 2023; Baroni et al., 2025), and that APE subsumes NoPE; in fact our programs in App. J, K often don’t include pos .2. Why isn’t the method applied to LLMs? 

The goal of this paper is to answer a foundational question about transformers, i.e., whether they learn interpretable RASP-like algorithms. While this has been suggested by theoretical studies of transformer’s learning abilities (Zhou et al., 2024; Huang et al., 2025), our work provides the most direct empirical test of this idea so far. Our work thus targets transformers specifically trained on individual algorithmic problems. Language models in contrast are trained on highly varied data, and effectively are exposed to many different kinds of algorithmic tasks. It appears unlikely that language models as a whole would be expressible as simple RASP programs due to the extremely rich variation of their training data (e.g., grammar, world knowledge). It is an interesting question if language models also develop interpretable sub-programs for individual algorithmic tasks. The presence of induction heads in real-world language models (e.g. Olsson et al., 2022; Wang et al., 2023; Yin & Steinhardt, 2025), effectively a subprogram isomorphic to the program in Figure 3, suggests this might be case. Testing this is a nontrivial and interesting question for future work, for which the present work provides foundations. 3. Why can’t one just use existing circuit discovery methods? 

While circuit discovery and D-RASP decompilation are methodologically closely connected, their outputs differ in a few meaningful and important ways. A D-RASP program by design assigns an interpretable operation to each of the attention heads and MLPs that stay part of the program, while circuit discovery methods only show which pathways are important in a computation graph and do not address a question of interpreting the operations. Moreover, operations in a D-RASP program act across all tokens in the input sequence, treating tokens and positions as variables, while circuit discovery methods are highly template-dependent and aligned to specific positions in the input (e.g. Haklay et al., 2025). Importantly, existing circuit discovery methods define circuits usually on component level, which is not aligned with variables in D-RASP. A vertex in component-level circuits can be an aggregation of many D-RASP variables, making it harder to perform interpretation. 4. Why not build on existing translations from transformers to logic? 

As explained in the Discussion section, existing translations typically create one formula for each dimension in the residual stream, which in general are not individually interpretable. While it is conceivable that pruning might cut this down, our strategy circumvents such a blow-up. For instance, it directly results in isomorphic programs for the “Most Frequent” task (Figure 1, 256 dimensions in residual stream of original transformer) and the binary variant “Majority” (App. J.1, 16 dimensions in residual stream of original transformer). 5. Can decompilation be applied to any model? When is decompilation expected to work? 

Decompilation requires LLNA to hold. In practice, it also requires the model to allow pruning of a large fraction of the program lines (i.e., to have a sparse sufficient sub-program), as small programs ease interpretation. Our findings suggest that these properties are often satisfied in length-generalizing models, but not in models that fail to length-generalize. 6. Is there a guarantee that all variables will end up with interpretable dimensionalities such as |Σ|?

Theoretically, there can be cases where the decompilation will result in variables of dimension equal to that of the original model, e.g., when the output of an MLP feeds into attention and then again into an MLP. We do not observe such cases in our experiments, where decompiled variables all have interpretable dimensions such as |Σ|, T .7. Isn’t the decompilation pipeline complicated? Are all steps of the decompilation pipeline needed? 

The pipeline has three steps: (i) causal pruning and linearizing layer norm, (ii) explaining per-position transformations, (iii) explaining tensors in select and project . The first steps is needed in order to obtain problems of manageable size. The second and third steps are helpful in automating part of the interpretation. Whenever no primitive is found, interpretation via manual inspection is still feasible. 12 Interpretable Algorithms by Decompiling Transformers 

8. How are the hyperparameters justified? 

In most cases, we observed in preliminary experiments that the choice of hyperparameters does not significantly affect the result. This is supported by the fact that we use the same set of hyperparameters for all models. For sparsity coefficient, we always sweep over various values to trace out a Pareto frontier (Figure 4). An important hyperparameter is the accuracy threshold, which defines whether the program is decompiled or not, which we chose based on common sense. 9. Why do we need a new RASP dialect? Why not decompile into an existing dialect? 

The most important feature of D-RASP is that it uses softmax (unlike RASP, B-RASP, C-RASP, S-RASP), which makes it suited to faithfully representing transformer computations. 10. Isn’t the exponential size of the D-RASP programs from Theorem 3.2 a big problem? 

The D-RASP program has exponential number of lines only after the reparametrization step. This is not a problem for interpreting final programs as we perform simplification , which significantly reduces the number of lines (Figure. 4) This is also not a big problem for pruning, because we do not unroll all lines of D-RASP to perform pruning. rather, pruning is done in a efficient manner, where we use scalable gradient-based approach, and in initial stages prune over groups of lines instead of single line. This is outlined in the main paper, and described in detail in App. F. 11. How does D-RASP compare to Transformer Programs (Friedman et al., 2023; Lai-Dang et al., 2025) and the Discrete Transformer (Zhang et al., 2026)? 

On a technical level, there are some similarities; in particular, our definition of separate variables for each path through the residual stream is similar to the disentangled residual stream where the output of the attention heads is concatenated rather than added to the residual stream. That said, there are key differences: first, our decompilation procedure is designed to apply to normal transformers trained using standard methods; second, causal pruning enables small programs despite a theoretically exponential number of paths through the residual stream. 12. Why don’t we train and decompile the model on the same length? Would that make more models decompilable? 

We also considered decompiling models to programs that match the original model only on training lengths. Indeed for some non-length-generalizing models, this enables decompilation (though the program might not match the model’s behavior beyond training lengths). But there are some other models that are still not decompilable, meaning that their inner computational mechanism may be complex and uninterpretable even if we only consider model behavior on a limited length range on which the model does perform well. An important consideration is that programs obtained by decompiling on only the training length so would not help us on understanding why generalization fails, as they do not match model behavior on longer length. See details in Appendix E. 

B. D-RASP Construction 

B.1. Notation for Transformers 

Here, we introduce our notation for GPT-2-style transformers, closely following Huang et al. (2025). By LLNA, we assume layer norm has been linearized and absorbed into the parameters (Section B.2). For simplicity of notation, we omit biases in attention, and merge value and output matrices V , O into a single V matrix per head. Our implementation, however, takes all such details into account faithfully. 

Computation of Activations and Outputs If L is the number of layers, then we write the output of layer l = 1 , . . . , L at position i = 1 , . . . , N (T ) as y(l) 

> i

∈ Rd. We set 

y(0)  

> i

= Exi + Pi i = 1 , . . . , |x| (7) where xi ∈ Σ is the input symbol at position i, E is the word embedding matrix, and P is the position embedding matrix. Attention logits, at query position i and key position j are computed as 

a(l,h ) 

> i,j

= ( y(l−1)  

> i

)T QTl,h Kl,h y(l−1)  

> j

for 1 ≤ j ≤ i ≤ | x|; l = 1 , . . . , L ; h = 1 , . . . , H (8) 13 Interpretable Algorithms by Decompiling Transformers 

We assume standard softmax attention: 

Y (l) 

> i

:= y(l−1)  

> i

+

> H

X

> h=1

Pij=1 exp 



a(l,h )

> i,j



Vl,h y(l−1) 

> j

Pij=1 exp 



a(l,h )

> i,j

 (9) After each attention block, the activations are passed through a one-layer MLP: 

y(l) 

> i

:= Y (l) 

> i

+ ψl(Y (l) 

> i

) (10) where ψl is an MLP. A transformer maps strings x ∈ Σ∗ (|x| ≤ T ) to vectors of next-token prediction logits, ∈ R|Σ|×| x|, obtained as U y (L)

> i

(i = 1 , . . . , |x|) for the unembedding matrix U ∈ R|Σ|× d.

B.2. Reparameterizing Layer Norm when LLNA is Satisfied 

When LLNA is satisfied (for some constant parameter γ′): 

x − x

pσ2(x) + ϵ γ + β ⇒ (x − x) γ′ + β

we can replace all layer norm operations in a model with their linearized form: LayerNorm (x) ≈ Lx + β, L =



I − 11 T

d(x)



γ′

where 1 is a vector of all ones. L parameters can be absorbed into the attention parameters: 

Ql,h ← Ql,h Ll,h ; Kl,h ← Kl,h Ll,h ; Vl,h ← Vl,h Ll,h 

and similarly for the MLP. 

B.3. Detailed Definition of D-RASP Translation 

Here, we prove Theorem 3.2, using the notation from Appendix B.1. Let L be the number of layers of the transformer. Let H be the number of heads. By a “path” p, we refer to a sequence over {1, . . . , L } × { 1, · · · H} that is strictly decreasing in the first component. We set 

layer (p) := max ( {0} ∪ { i : ( i, h ) ∈ p}); that is, the maximum layer involved in the path. We use ⟨. . . ⟩ to indicate ordered tuples. We will accumulate a set of variables defined in each layer. Some variables will be in token space, others will be in position space, and (when necessary) others will be left in the model’s original activation space. 

Definition B.1. We define a set V indexing the variables of the D-RASP program as follows. Let start =

{tok, pos, mlp 1, . . . , mlp L}. Then V is the smallest set such that: 1. For each x ∈ start , ⟨x, ⟨⟩⟩ ∈ V .2. For any l ∈ [L], h ∈ [H], whenever layer (⟨x, p ⟩) < l and ⟨x, p ⟩ ∈ V , then ⟨x, ⟨(l, h )⟩ ⊕ p⟩ ∈ V .where 3. layer (⟨tok, p ⟩) := layer (p)

4. layer (⟨pos, p ⟩) := layer (p)

5. layer (⟨mlp k, p ⟩) := max( k, layer (p)) 

14 Interpretable Algorithms by Decompiling Transformers 

In particular, each v ∈ V is a tuple consisting of x ∈ start and a path p.For each ⟨i, p ⟩ ∈ V , our D-RASP program will have a variable v⟨i,p ⟩. We will instantiate the variables so that, for each p,

d(v⟨tok,p ⟩) = |Σ|

d(v⟨pos,p ⟩) = Nd(v⟨mlp k ,p ⟩) = d

Using the variables v⟨i,p ⟩, we define the following program: First, v⟨tok, ⟨⟩⟩ = token(en) and v⟨pos, ⟨⟩⟩ = pos are already given in the program. We need to define the other variables. We first define as auxiliary quantities: 

T (token ) = E ∈ Rd×| Σ|

T (position ) = P [: , : N ] ∈ Rd×N

T (mlp l) = I d×d ∈ Rd×d

Defining Selector Matrices Selectors are defined as follows. For layer l, head h and each r, s ∈ V , layer (r), layer (s) < l ,we add the line Line defining a selector 

αl,h,r,s = select (k= vr , q= vs, op= Al,h,r,s )Intuitively, we have a different term in the selection for each interaction of existing variables. Here, for the tuple 

⟨ik, p k⟩, ⟨iq , p q ⟩ ∈ V we define the selector matrices: 

Al,h, ⟨iq ,p q ⟩,⟨ik ,p k ⟩ := T (iq )T ·

 Y

> ⟨l′,h ′⟩∈ pq

Vl′,h ′



> T

· QTl,h · Kl,h ·

 Y

> ⟨l′,h ′⟩∈ pk

Vl′,h ′

 · T (ik) (11) where the product Q 

> ·∈ p

is computed in decreasing order along the entries of the path. For each i ∈ start, layer (p) < l , h

we add the line: Line performing aggregation 

v⟨i, ⟨⟨ l,h ⟩⟩⊕ p⟩ = aggregate (P 

> r,s

αl,h,r,s , v⟨i,p ⟩)where the sum runs over all r, s for which we above defined αl,h,r,s .

Running Example: One-Layer One-Head Model For a one-layer one-head model, we obtain the following set of variables indexed by tuples ⟨x, p ⟩ ∈ V :

v⟨pos, ⟨⟩⟩ Associated layer: 0

v⟨tok, ⟨⟩⟩ Associated layer: 0

v⟨pos, ⟨⟨ 1,1⟩⟩⟩ Associated layer: 1

v⟨tok, ⟨⟨ 1,1⟩⟩⟩ Associated layer: 1

v⟨mlp 1,⟨⟩⟩ Associated layer: 1

We first obtain the following selectors and aggregations – recall pos = v⟨pos, ⟨⟩⟩ and token = v⟨tok, ⟨⟩⟩ :15 Interpretable Algorithms by Decompiling Transformers 

Line of generated program 

1.α 1,1,v ⟨tok, ⟨⟩⟩ ,v ⟨tok, ⟨⟩⟩ = select(q= v⟨tok, ⟨⟩⟩ , k= v⟨tok, ⟨⟩⟩ ,op= A1,1,⟨tok, ⟨⟩⟩ ,⟨tok, ⟨⟩⟩ )

2.α 1,1,v ⟨tok, ⟨⟩⟩ ,v ⟨pos, ⟨⟩⟩ = select(q= v⟨tok, ⟨⟩⟩ , k= v⟨pos, ⟨⟩⟩ ,op= A1,1,⟨tok, ⟨⟩⟩ ,⟨pos, ⟨⟩⟩ )

3.α 1,1,v ⟨pos, ⟨⟩⟩ ,v ⟨tok, ⟨⟩⟩ = select(q= v⟨pos, ⟨⟩⟩ , k= v⟨tok, ⟨⟩⟩ ,op= A1,1,⟨pos, ⟨⟩⟩ ,⟨tok, ⟨⟩⟩ )

4.α 1,1,v ⟨pos, ⟨⟩⟩ ,v ⟨pos, ⟨⟩⟩ = select(q= v⟨pos, ⟨⟩⟩ , k= v⟨pos, ⟨⟩⟩ ,op= A1,1,⟨pos, ⟨⟩⟩ ,⟨pos, ⟨⟩⟩ )

5.v ⟨tok, ⟨⟨ 1,1⟩⟩⟩ = aggregate(s= α1,1,v ⟨tok, ⟨⟩⟩ ,v ⟨tok, ⟨⟩⟩ + α1,1,v ⟨tok, ⟨⟩⟩ ,v ⟨pos, ⟨⟩⟩ + α1,1,v ⟨pos, ⟨⟩⟩ ,v ⟨tok, ⟨⟩⟩ +

α1,1,v ⟨pos, ⟨⟩⟩ ,v ⟨pos, ⟨⟩⟩ , v= v⟨tok, ⟨⟩⟩ )6.v ⟨pos, ⟨⟨ 1,1⟩⟩⟩ = aggregate(s= α1,1,v ⟨tok, ⟨⟩⟩ ,v ⟨tok, ⟨⟩⟩ + α1,1,v ⟨tok, ⟨⟩⟩ ,v ⟨pos, ⟨⟩⟩ + α1,1,v ⟨pos, ⟨⟩⟩ ,v ⟨tok, ⟨⟩⟩ +

α1,1,v ⟨pos, ⟨⟩⟩ ,v ⟨pos, ⟨⟩⟩ , v= v⟨pos, ⟨⟩⟩ )

With simplified variable naming, as used in our decompiled programs, we get the following equivalent program (suppressing the op= arguments): Line of generated program 

1. s1 = select(q=token, k=token) 2. s2 = select(q=token, k=pos) 3. s3 = select(q=pos, k=token) 4. s4 = select(q=pos, k=pos) 5. a1 = aggregate(s=s1+s2+s3+s4, v=token) 6. a2 = aggregate(s=s1+s2+s3+s4, v=pos) 

Defining elementwise operations We set Line performing per-position operation 

v⟨mlp l,⟨⟩⟩ = element wise op (vs(i) : s ∈ V − { vmlp,l }, layer (s) ≤ l, func= f )

where f outputs the output vector of the original MLP fM LP,l of the l-th layer: 

f  v(s,p )(i) : ( s, p ) ∈ V , s ̸ = mlp l, layer (( s, p )) ≤ l := fM LP,l 

 X

(s,p )∈V ,s ̸=mlp l,layer (( s,p )) ≤l

s

 (12) 

Running Example: One-Layer One-Head Model In our running example, we have the line: Line of generated program 7. v⟨mlp 1,⟨⟩⟩ = element wise op (v⟨pos, ⟨⟩⟩ , v ⟨tok, ⟨⟩⟩ , v ⟨pos, ⟨⟨ 1,1⟩⟩⟩ , v ⟨tok, ⟨⟨ 1,1⟩⟩⟩ , func= fM LP, 1)

or in the simplified notation (we generally suppress the f argument): Line of generated program 

7. m = element_wise_op(token+pos+a1+a2) 

Treating Unembedding Matrices We define the unembedding projections as 

R⟨i,p ⟩ = U

 Y

⟨l′,h ′⟩∈ p

Vh′,l ′

 T (i) (13) for each ⟨i, p ⟩ with layer (p) ≤ L, and set 16 Interpretable Algorithms by Decompiling Transformers 

Line performing per-position operation 

p⟨i,p ⟩ = project (v⟨i,p ⟩) for each layer (⟨i, p ⟩) ≤ L, op= R⟨i,p ⟩)

The output is obtained as the softmax of all p⟨i,p ⟩ variables, without bias. 

Running Example: One-Layer One-Head Model In our running example: Lines of generated program (simplified naming) 8. p⟨tok, ⟨⟩⟩ = project( v⟨tok, ⟨⟩⟩ , op= U T (token ))

9. p⟨pos, ⟨⟩⟩ = project (v⟨pos, ⟨⟩⟩ , op= U T (pos ))

10. p⟨tok, ⟨⟨ 1,1⟩⟩⟩ = project( v⟨tok, ⟨⟨ 1,1⟩⟩⟩ , op= U T (token ))

11. p⟨pos, ⟨⟨ 1,1⟩⟩⟩ = project( v⟨pos, ⟨⟨ 1,1⟩⟩⟩ , op= U T (pos ))

12. p⟨mlp 1,⟨⟩⟩ = project( v⟨mlp 1,⟨⟩⟩ , op= U T (mlp 1))

13. prediction = softmax (p⟨tok, ⟨⟩⟩ + p⟨pos, ⟨⟩⟩ + p⟨tok, ⟨⟨ 1,1⟩⟩⟩ + p⟨pos, ⟨⟨ 1,1⟩⟩⟩ + p⟨mlp 1,⟨⟩⟩ )

or, with simplified variable naming (suppressing the op= arguments): Lines of generated program (simplified naming) 

8. logits1 = project(token) 9. logits2 = project(pos) 10. logits3 = project(a1) 11. logits4 = project(a2) 12. logits5 = project(m) 13. prediction = softmax(logits1+logits2+logits3+logits4+logits5) 

B.4. Proof of Theorem 3.2 Theorem B.2 (Restated from Theorem 3.2) . Consider a GPT-2-style transformer satisfying LLNA. There is a D-RASP program defining the same input-output map; it can be explicitly obtained from the transformer’s parameters. Indeed, the residual stream at each layer can be recovered linearly from a set of variables in the D-RASP program. Proof. We show that the D-RASP program described in Appendix B.3 satisfies the claims made in the theorem. The key idea is that, in each layer, we can linearly recover the residual stream from appropriate variables in the program. The key claim is the following linear decomposition of the residual stream into variables: 

y(l) 

> i

= X

> (x,p )∈V :layer (⟨x,p ⟩)≤l

 Y

> ⟨l′,h ′⟩∈ p

Vl′,h ′

 · T (x) · v⟨x,p ⟩(i) (14) where the product is performed in decreasing order according to the order of layers in the elements of the path p. Equation 14 shows that one can recover the original transformer’s computations linearly. Once this is established, we can recover the original transformer’s output by taking l = L and applying the unembedding matrix U on both sides, and applying the definition of R(i,p ) (13): 

U y L = X

> layer (⟨i,p ⟩)≤L

U

 Y

> ⟨l′,h ′⟩∈ p

Vl′,h ′

 T (i)v⟨i,p ⟩

= X

> layer (⟨i,p ⟩)≤L

R(i,p )v⟨i,p ⟩

= X

> layer (⟨i,p ⟩)≤L)

project (v⟨i,p ⟩, op= (R⟨i,p ⟩)

17 Interpretable Algorithms by Decompiling Transformers 

It remains to prove Equation 14. The proof proceeds by induction over the layers. We first consider the input: 

y(0) 

i = Exi + Pi = T (token ) · token + T (position ) · pos (15) where we use that the only variables at layer ≤ 0 are token and pos ; pos = v⟨pos, ⟨⟩⟩ ; token = v⟨tok, ⟨⟩⟩ .We next perform the inductive step, assuming the claim has been shown for all layers < l . We first consider the attention logits of each head h in layer l (Equation 8): 

a(l,h )

i,j = ( y(l−1) 

i )T QTl,h Kl,h y(l−1) 

j for 1 ≤ j ≤ i ≤ N (16) We now rewrite this as follows, using the inductive hypothesis: 

a(l,h ) 

> i,j

=( y(l−1)  

> i

)T QTl,h Kl,h y(l−1) 

> j

=

 X

> (x,p )∈V :layer (⟨x,p ⟩)≤l−1

 Y

> ⟨l′,h ′⟩∈ p

Vl′,h ′

 · T (x) · v⟨x,p ⟩(i)



> T

QTl,h Kl,h 

 X

> (y,q )∈V :layer (⟨y,q ⟩)≤l−1

 Y

> ⟨l′,h ′⟩∈ p

Vl′,h ′

 · T (y) · v⟨y,q ⟩(j)



=

X

> (x,p )∈V :layer (⟨x,p ⟩)≤l−1

v⟨x,p ⟩(i)T T (x)T

 Y

> ⟨l′,h ′⟩∈ p

Vl′,h ′

 

> T

 QTl,h Kl,h 

 X

> (y,q )∈V :layer (⟨y,q ⟩)≤l−1

 Y

> ⟨l′,h ′⟩∈ q

Vl′,h ′

 · T (y) · v⟨y,q ⟩(j)



= X

> (x,p )∈V :layer (⟨x,p ⟩)≤l−1

X

> (y,q )∈V :layer (⟨y,q ⟩)≤l−1

v⟨x,p ⟩(i)T T (x)T

 Y

> ⟨l′,h ′⟩∈ p

Vl′,h ′



> T

QTl,h Kl,h 

 Y

> ⟨l′,h ′⟩∈ q

Vl′,h ′

 · T (y) · v⟨y,q ⟩(j)= X

> (x,p )∈V :layer (⟨x,p ⟩)≤l−1

X

> (y,q )∈V :layer (⟨y,q ⟩)≤l−1

v⟨x,p ⟩(i)T ATl,h, (x,p ),(y,q )v⟨y,q ⟩(j)

The last term indeed is the sum of all selectors created for head (l, h ) under “Line defining a selector” in Appendix B.3 (compare (11)). Thus, the sum of the selectors associated to a head (l, h ) exactly recovers the head’s attention logit. As a consequence, for each ⟨x, p ⟩ with ⟨x, p ⟩ < l , we recover the output of the head as a linear combination of the variables associated to a path ending in (l, h ), i.e., variables of the form v⟨x, ⟨⟨ l,h ⟩⟩⊕ p⟩:

X

x,p 

 Y

⟨l′,h ′⟩∈⟨⟨ l,h ⟩⊕ p⟩

Vl′,h ′

 T (x)v⟨x, ⟨⟨ l,h ⟩⟩⊕ p⟩(i)=Vl,h · X

x,p 

 Y

⟨l′,h ′⟩∈ p

Vl′,h ′

 T (x)v⟨x, ⟨⟨ l,h ⟩⟩⊕ p⟩(i)=Vl,h · X

x,p 

 Y

⟨l′,h ′⟩∈ p

Vl′,h ′

 T (x) · aggregate X

r,s 

αl,h,r,s v⟨x,p ⟩

!

(i)=Vl,h · X

x,p 

 Y

⟨l′,h ′⟩∈ p

Vl′,h ′

 T (x)

iX

j=1 

softmax X

r,s 

αl,h,r,s 

!

i,j 

v⟨x,p ⟩(j)=Vl,h ·

iX

j=1 

softmax X

r,s 

αl,h,r,s 

!

i,j 

X

x,p 

 Y

⟨l′,h ′⟩∈ p

Vl′,h ′

 T (x)v⟨x,p ⟩(j)=

iX

j=1 

softmax X

r,s 

αl,h,r,s 

!

i,j 

Vl,h · y(l−1) 

j (i.e., the output of the head) where softmax( P

r,s αl,h,r,s )i,j denotes the attention weight with query position i and key position j. We thus recover 

Y (l)

i = X

(x,p ): x̸=mlp l,layer (( x,p )) ≤l

 Y

⟨l′,h ′⟩∈ p

Vl′,h ′

 · T (x) · v⟨x,p ⟩(i) (17) Taken together with the definition of v⟨mlp l,⟨⟩⟩ , (14) follows for layer l. This concludes the proof. 18 Interpretable Algorithms by Decompiling Transformers 

B.5. Example of D-RASP Translation 

In Figure 9, we show a full program provided by Theorem 3.2 on a one-layer one-head transformer. To give an idea of the extent of pruning, we use strike-through show which parts of the program remain (plain) or not (strike-through) in the program shown in Figure 1. 

Original: 

1. s1 = select(q=token, k=token) 2. s2 = select(q=token, k=pos) 3. s3 = select(q=pos, k=token) 4. s4 = select(q=pos, k=pos) 5. a1 = aggregate(s=s1+s2+s3+s4, v=token) 6. a2 = aggregate(s=s1+s2+s3+s4, v=pos) 7. m = elementwise_op(token+pos+a1+a2) 8. logits1 = project(token) 9. logits2 = project(pos) 10. logits3 = project(a1) 11. logits4 = project(a2) 12. logits5 = project(m) 13. prediction = softmax(logits1+logits2+logits3+logits4+logits5) 

After pruning: 

1. s1 = select(q=token, k=token) 2. s2 = select(q=token, k=pos) 3. s3 = select(q=pos, k=token) 4. s4 = select(q=pos, k=pos) 5. a1 = aggregate(s=s1+s2+s3+s4, v=token) 6. a2 = aggregate(s=s1+s2+s3+s4, v=pos) 7. m = elementwise_op(token+pos+a1+a2) 8. logits1 = project(token) 9. logits2 = project(pos) 10. logits3 = project(a1) 11. logits4 = project(a2) 12. logits5 = project(m) 13. prediction = softmax(logits1+logits2+logits3+logits4+logits5) 

Figure 9. See text. 

B.6. Size of D-RASP Translation 

The size of the D-RASP translation given by Theorem 3.2 is given by: 

def determine_total_line(num_layer, num_head, split_mlps): num_v = 2 num_line = 0for i in range(num_layer): num_line += (num_v ** 2 + num_v) * num_head num_v += num_v * num_head if split_mlps: num_line += num_v num_v += num_v else: num_line += 1num_v += 1num_line += num_v num_line += 1 # bias term 

19 Interpretable Algorithms by Decompiling Transformers 

num_line += 1 # prediction return num_line 

Here, split-mlps indicates whether the element-wise operations are split into single-input operations, which further increases the line count before pruning. 

C. Expressivity of D-RASP and C-RASP 

Theorem C.1 (Correspondence of D-RASP and C-RASP, repeated from Theorem 2.1) . Let D be the class of D-RASP programs not using pos where all tensor entries Aij , b i (for the parameters of select , project ) are in {−∞ , }∪{ log q :

q ∈ Q+}. Then the programs in D, using the rounded semantics, define exactly the same functions as C-RASP. Remark C.2 . For the rounded semantics, we assume that outputs are rounded to p-bit precision for some p ∈ N. We assume that numbers are rounded downwards (as a consequence, negative numbers are rounded to negative numbers). By “define the same functions” we mean that both classes define the same maps from strings x ∈ Σ∗ to next-token predictions ∈ Σ|x|, assigning each token a predicted next token. 3

We note that, in the absence of token , D-RASP programs can be in principle applied at unbounded input lengths, making rigorous comparison to C-RASP feasible. Regarding token , the treatment of positional encodings in extensions of C-RASP is nontrivial (Huang et al., 2025). Many of the operations involving token that we recover resemble the local (such as in Figure 3) or periodic (such as in Figure 18) functions assumed in Huang et al. (2025), supporting their treatment. 

Proof of Theorem 2.1. We refer to (Yang & Chiang, 2024; Huang et al., 2025; Yang et al., 2025) for expositions of C-RASP. We begin with a D-RASP program satisfying the described constraints. By the fixed precision assumption, there is some 

C ∈ Q+ such that all possible values in activation variables are in V := {0, 2−p, −2−p, 2 · 2−p, −2 · 2−p, . . . , C, −C}.4

In particular, each activation variable can only take a finite number of values, which will be hard-coded in the C-RASP program (similar to the argument translating between C-RASP and transformers in (Yang et al., 2025)). For each activation variable v, each z ∈ V, and each j = 1 , . . . , d (v), we want to define a C-RASP predicate Pv,z,j that is true at position i if and only if v(i)j = z. We show this by induction; it is clearly true at token . For the inductive step, it is also clearly true that element-wise transformations preserve this property, because they can be directly hard-coded. We need to consider aggregation. By assumption, each softmax entry is rational. Thus, determining the rounded value of the aggregation outcome boils down to checking linear inequalities with integer coefficients over the frequencies of different activation vectors over 

D, which can be performed in C-RASP. By the same argument, we can express in C-RASP which token receives largest softmax logits (or positive sigmoid logits) and is thus the predicted next token. For the other direction, consider a C-RASP program. For each Boolean predicate P , we will design a variable v such that v(i) ∈ { 0, 1} and v(i) = 1 iff P (i) holds. Rather than simulating individual count-valued variables from C-RASP in D-RASP, it will be advantageous to directly simulate Boolean-valued terms created as inequalities between linear combinations of counts: 

> A

X

> k=1

λk#[ j ≤ i]Pk(j) ≥ 0 (18) where A is some constant, λ1, . . . , λ A ∈ Z are constant coefficients, P1, . . . , P A are Boolean-valued predicates (already defined and translated to D-RASP variables by induction). We use an element-wise operation to prepare an integer-valued (one-dimensional) D-RASP variable v(i) = λ11P1(i) + · · · + λi1PA(i). We then use uniform aggregation (with a zero selector) to obtain an aggregate one-dimensional variable a(i) = round 

 PAk=1 λk #[ j≤i]Pk (j)

> i



. By checking if the rounded value is ≥ 0 or not, we can reconstruct the truth value of the linear inequality (18). Any Boolean-valued formula in C-RASP is a Boolean combination of such ienqualities; an elementwise operation can perform such combinations. Overall, we have shown that C-RASP programs can be translated to the described D-RASP fragment.    

> 3In the case of a sigmoid output as we use for formal languages, we can analogously formalize this as assigning each token a set of possible next tokens.
> 4By induction, the set of possible entries in the vectors is bounded, ensuring the existence of such a C.

20 Interpretable Algorithms by Decompiling Transformers 

D. Tasks and Model Training Details 

In this section we describe how we obtain the models before interpreting them. 

D.1. Task Definitions and Data generation 

Here, we define the tasks used in this paper formally. BOS, SEP, and EOS are special tokens that represent beginning of the sequence, separator, and end of sequence. All the tasks below except for those under “Formal Languages” referred to as algorithmic tasks. We use two different training objectives for algorithmic tasks and formal language tasks. For algorithmic tasks , we use language modeling as training objective, the language modeling loss only includes loss over the subsequence starting from SEP. In other words, models are trained to predict the next tokens over all tokens after SEP. For single-token answer, namely Majority, Binary Majority, and Parity, Special tokens only include BOS and SEP. For other algorithmic tasks, BOS, SEP and EOS always occur in each sequence. The models performance is measured by Task Accuracy , which is the proportion of the inputs on which the model makes correct prediction on all token positions that are included in the training loss. For formal language tasks , a different paradigm is used. Because formal language tasks are essentially recognition tasks, which means we need to find negative examples that do not belong to the language. However, generating such negative examples can be quite difficult empirically if they are generated purely at random, most of the time they can be quite trivial, the model usually end up with learning shortcuts because “hard” examples are not enough. In other words negative examples should cover all failure mode with enough probability such that model can learn the real rule of the formal language. Butoi et al. (2025) introduced perturbation-based method to generate non-trivial negative samples. In our preliminary experiments, we found that their method, while effectively mitigating the problem, still cannot completely solve the problem. Models learn shortcut on, for example, Dn. Therefore, like previous work (Bhattamishra et al., 2020; Sarrof et al., 2024; Huang et al., 2025), we use predictive modeling as the training objective. Specifically, the inputs are only valid examples, but on each of the token in the input, the model needs to predict all the possible valid next tokens, including the EOS token. For example, given the language (aa )∗. The input BOS a a a a will have the target         

> (a, EOS) (a,) (a, EOS) (a,) (a, EOS)

where “(a, EOS)” means the next token can be either “a” or “EOS”. At the same time we replace the softmax after the last linear layer (so-called language modeling head) in the transformer with a sigmoid function, so that the model can predict whether each token in the vocabulary can be the valid next token. And we use binary cross entropy (BCE) loss as the training objective. Correspondingly the Task Accuracy is defined as the proportion of inputs on which the model can make correct prediction of all valid and invalid next tokens on all positions. Similarity, the definition of Match Accuracy is also modified in this way. Specifically, it becomes the proportion of inputs on which the pruned model’s predictions about the validity of all tokens in vocabulary are all the same as the original model’s predictions, on all positions. We now define the tasks individually. The definitions closely follow Huang et al. (2025). 

Binary Majority. This problem is to find the most frequent bit in a sequence of random bits. An example is        

> BOS 10... 1SEP 1

The part between BOS and SEP is the sequence of random bits. We constrain the se-quences such that the number of 0s and 1s are always not equal. The bit after SEP is the label, i.e., the most frequent token. 

Binary Majority Interleave. The inputs in this problem are created by interleaving multiple binary majority (see above) inputs while avoiding repeating special tokens (e.g., BOS). We use 3 binary majority sequence to compose one sequence in this task. Formally speaking, given 3 binary sequences of the same length, x11, · · · x1

> n

, x21, · · · x2

> n

, and x31, · · · x3

> n

, and their corresponding labels y1, y2, y3, the interleaved input is BOS x11, x 21, x 31, x 12, x 22, x 32, · · · , x 1

> n

, x 2

> n

, x 3

> n

. SEP y1, y2, y3 EOS .An example is BOS 1 0 1 1 0 0 ... SEP 1 0 0 EOS The task is from Huang et al. (2025). 

Most Frequent. 5 This is similar to the binary majority problem, except the vocabulary is larger. An example is        

> BOS cbabSEP b

, where the part between BOS and SEP is a sequence of random tokens, each of which is sampled independently from an alphabet of 26 symbols. We constrain the sequences such that there is always a unique answer. 

> 5This task is called Majority in Huang et al. (2025). We feel that “Most Frequent” is more accurate.

21 Interpretable Algorithms by Decompiling Transformers 

Sort. In sort problem, the model outputs a sorted version of the given sequence. An example is            

> BOS 14 23 69SEP 6914 23 EOS

where the part between BOS and SEP is a sequence of unique numbers, and the part between SEP and EOS is the sorted version of it. The total vocabulary size of tokens except for special tokens is equal to the maximum testing length, i.e., 150. The task is from (Zhou et al., 2024; Huang et al., 2025). 

Count. In this problem, the model outputs the same sequence as the given sequence, which consists of unique tokens. An example is BOS 14 19 SEP 14 15 16 17 18 19 EOS where the two numbers between BOS and SEP are the start and end of counting, such counting generates numbers in the part between SEP and EOS. The total vocabulary size of tokens except for special tokens is equal to the maximum testing length, i.e., 150. Note that this is not trivial since the model needs to predict where to start counting and when to end counting. The task is from (Zhou et al., 2024). 

Unique Copy In this problem, the model outputs the same sequence as the given sequence. Tokens in the given sequence occur at most once (thus unique). An example is BOS 14 23 6 9 SEP 14 23 6 9 EOS where the part between BOS and SEP is a sequence of unique numbers, and the part between SEP and EOS is a copy of it. The total vocabulary size of tokens except for special tokens is equal to the maximum testing length, i.e., 150. The task is from (Zhou et al., 2024; Huang et al., 2025). 

Unique Bigram Copy In this problem, the model outputs the same sequence as the given sequence, which consists of unique bigrams. An example is BOS 14 23 6 14 SEP 14 23 6 14 EOS where the part between BOS and SEP is a sequence of unique bigrams (i.e. (14, 23), (23, 6), (56, 14)), we guarantee that each bigram only occurs once in a sequence. The part between SEP and EOS is a copy of it. The total vocabulary size of tokens except for special tokens is 16, since we allow for repetition of the same tokens in one sequence. 

Unique Reverse Copy. In this problem, the model copies the given sequence in reverse order, the given sequence consists of unique tokens. An example is BOS 14 23 6 9 SEP 9 6 23 14 EOS where the part between BOS and SEP is a sequence of unique numbers, and the part between SEP and EOS is a copy of it in reverse order. The total vocabulary size of tokens except for special tokens is equal to the maximum testing length, i.e., 150. The task is from (Jobanputra et al., 2025). 

Repeat Copy. The model outputs the same sequence as the given sequence. The given sequence can contain repeated tokens. An example is BOS a b a b SEP a b a b EOS where the part between BOS and SEP is a sequence of random symbols. We use an alphabet of only 2 symbols, so that we avoid n-gram being always unique for certain small n. Each symbols is sampled independently and uniformly. The task is from (Zhou et al., 2024; Huang et al., 2025). 

Parity. In the parity problem, the model recognizes whether the given sequence contains even number of 1s. An example is BOS 1 0 0 1 0 SEP 0 The bits between BOS and SEP is a random sequence of bits. The bit after SEP is the number of ones modulo 2, i.e., 0 when number of 1s is even, and 1 otherwise. The bits are randomly sampled in a way such that the number of 1s is distributed uniformly given a fixed sequence length. 

Addition. In this problem, the model need to do binary addition. An example is             

> BOS 101+10SEP 111EOS

The bits between BOS and SEP are separated by “+”, producing two operands. The bits between SEP and EOS are the answer of the addition. The two operands are sampled randomly. Note that we do not pad zeros in the front of operands to make them of equal length. 

Formal Languages. We also include 17 finite-state formal languages that are also used in (Bhattamishra et al., 2020; Sarrof et al., 2024; Huang et al., 2025). • Tomita Grammars. Alphabet Σ = {0, 1}

– Tomita 1: 1∗

– Tomita 2: (10) ∗

– Tomita 3: strings without odd-length strings of ones followed by odd-length strings of zeros (i.e., no 01 2n+1 02m+1 1

substrings) 

– Tomita 4: strings without any 000’s substrings 22 Interpretable Algorithms by Decompiling Transformers 

– Tomita 5: strings of even length with an even number of 1’s 

– Tomita 6: strings where number of 0’s - number of 1’s is divisible by 3 

– Tomita 7: 0∗1∗0∗1∗

• Dn. Alphabet Σ = {a, b }. The general definition Dn = ( aD n−1b)∗. In other words, Dyck-1 language with depth bounded by n. 

– D2

– D3

– D4

– D12 

• Others. Alphabet is all the letters occurring in the expression. 

– (aa )∗

– (aaaa )∗

– (abab )∗

– aa ∗bb ∗cc ∗dd ∗ee ∗

– {ab }∗d{b, c }∗

– {0, 1, 2}∗02 ∗

For most of the tasks, we keep the lengths of the inputs uniformly distributed in the desired range, that is, ≤ 50 during training (the minimum length depends on the task, e.g., minimum number of random bits in Binary Majority is 1 and in the interleaved version is 3) and ≤ 150 during pruning. The exception includes Binary Majority Interleave, Dn, Tomita 2, Tomita 5, Tomita 6, (abab )∗ (aa )∗ (aaaa )∗, because some lengths are not possible. 

D.2. Model Training 

We are interested in testing whether models can length-generalize, and see how it relates to interpretability of the model. Therefore, like Huang et al. (2025) (and similar to Zhou et al. (2024)), at train time, we add random offsets to position indices so that all position embeddings are trained. For example, for the input BOS c b a ... , the position indices are 0 1 2 3 ... , after adding an offset of 7, new position indices are 7 8 9 10 ... . We use the same training and testing lengths as Huang et al. (2025), i.e., train models on inputs of length ≤ 50 and test them on ≤ 50 ,

[51 , 100] and [101 , 150] . The offsets are sampled uniformly at random in the range of 0 to number of position embeddings subtracted by input length. Like Zhou et al. (2024), we sample independent training batches on the fly instead of using a finite-size training set. There are 3 test sets for each length range, each test set contains 2000 samples that are sampled at the beginning of each experiment, given the same random seed. We do not exclude the test samples in the test set of ≤ 50 

during training. We train decoder-only transformers from scratch, using implementations from Hugging Face Transformers 6. Same as (Huang et al., 2025), we train models for maximum 30k steps with a batch size of 64. We stop training early once the model’s accuracy reaches 100% on the in-distribution test set (length ≤ 50 ). We use AdamW, with a weight decay rate of 0.01. We train various models with hyperparameters in the following domain: number of layers: {1, 2, 4 }, number of heads {1, 2, 4}, model dimension: {16, 64, 256 }, and learning rate: {0.001, 0.0001 }, dropout rate: {0.0, 0.1 } (same rate for attention dropout, residual stream dropout, and embedding layer dropout). 

Representative models for each task We keep at least one model for each task, use this as a representative model for the task. Similar to (Huang et al., 2025), we train models on all combination of hyperparameters above, and select the one that length-generalizes best. As we mentioned in main paper, length-generalizing models tend to be decompilable. So for each task we try to get an interpretable model. 

> 6https://huggingface.co/docs/transformers/en/model_doc/gpt2#transformers.GPT2LMHeadModel

23 Interpretable Algorithms by Decompiling Transformers           

> (a) Binary Majority Interleave (b) Count (c) Unique Reverse (d) Unique Copy (e) Unique Bigram Copy
> Figure 10. Each subplot shows pareto frontiers for multiple models trained on the same task. They are trained with different hyperparame-ters and architectures, resulting in different length-generalization and decompilability. Results are from stage 1 of causal pruning, where layer norm is linearized. In the tasks shown here, we see in LLNA does not hold on models that do not generalize, so the decompilability correlates strongly with length-generalization, and much stronger than with model architectures (compare models of the same layers and different layers in the figure). Moreover, when LLNA holds, there are a large proportion of edges can be pruned.

Checkpoints at different training steps We also interested in seeing how model algorithms evolve throughout the training process. We save checkpoints for a few representative tasks: Copy (unique), Copy (unique bigram). The timing for saving is determined as follows: we evaluate model performance every 100 steps. In each time if we observe an increment in accuracy that is greater than 0.25 on any of the 3 test bins (i.e., input of lengths ≤ 50 , [51 , 100] , [101 , 150] ), compared to the last saved checkpoint (compare to 0 when no previous saved checkpoint), we save the checkpoint. 

Different model architectures For many tasks, we also keep additional models of different the architectures (e.g., number of layer and head), such that we have (1) both length-generalizing and non-length-generalizing models for the same task; (2) models of very different architectures but length-generalizing on the same task. This allows us to make interesting comparison to support our claims. 

Model naming scheme In the paper and our web application, the models we are interpreting are named as [task name]-[number of layer]l[number of head]h[model dimension]d[ log 0.1(learning rate)]lr[dropout rate * 10]drop . For example, unique reverse-2l1h64d3lr01drop is a model with 2 layers, 1 head per layer and model dimension of 64, trained on Reverse Copy (unique) with learning rate of 1 × 10 −3

and dropout rate of 0.1. For intermediate checkpoint models, their names also include the trained steps at the end. 

E. Complete Results on Generalization and Decompilability 

As we mentioned in main paper, for each task, we keep the model that length-generalize the best among all models trained with different hyperparameters. So if a task has only one model, that’s the model with the best length-generalizing result. We show the complete results in Table 1 and Table 2. 

Does different architecture affect decompilability? We show pareto frontiers when layer norm is linearized in Figure 10. For each task, we show 2-3 models different architectures. We can see that very different architectures can all be decompilable (e.g., two green lines in Figure 10(a) correspond to 4 layer vs. 2 layer models), while similar architectures can exhibit different decompilability (e.g., two 2-layer models in Figure 10(a)). 

How does decompilability evolve throughout training? We save checkpoints during training for two models: the length-generalizing one for Unique Copy (2l1h64d3lr01drop), and for Unique Bigram Copy (2l4h256d3lr01drop). We would like to see how the algorithm evolves during training. But we find that most checkpoints before the final one cannot be decompiled, as shown in Figure 11. We can only conclude that for these two models, layer norm plays an non-trivial role until the model finally reaches perfect performance on training length ( ≤ 50 ) (recall that we evaluate every 3k step and stop training once accuracy is 100% on training length). 

Can we get sparse graphs if original layer norm is allowed? In Figure 4 we see decompilable models can usually be pruned to a sparse form, resulting simple program. One might wonder if this is true in general for all models trained on these synthetic tasks. Thus in Figure 12 we allow models to use original layer norm and do pruning over component-level circuit. We can see that for those models where LLNA does not hold, their inner computational graph is usually relatively dense. 24 Interpretable Algorithms by Decompiling Transformers 

Figure 11. The checkpoints saved when training model for Unique Copy task (Left) and Unique Bigram Copy task (right). We show both task accuracy on 3 input ranges and decompilability. LLNA holds if the model can achieve > 90% match accuracy after linearizing Layer Norm. Green dot shows the minimum proportion of edges needed to achieve > 90% match accuracy. 

Figure 12. We show the pareto frontiers in stage 0 of causal pruning (i.e., component-level circuits with original layer norm enabled, see F.4 ) for all models where LLNA does not hold. The green lines correspond to the 6 models that length-generalize but requires original layer norm (see Table 2). We can see that most models cannot be pruned to be very sparse (consider how many edges they need in order to make match accuracy ≥ 90 .) 

Figure 13. Pareto frontiers in stage 1 of causal pruning (i.e., component-level circuits with linearized layer norm.) We find all models that achieves nearly perfect task accuracy in training length (specifically, all are ≥ 99 .4), and very low accuracy in test length of [101 − 150] 

(only one is 50.8, others are 0.0). For each model we show the frontier when pruning is done and match accuracy is measured on ≤ 150 

and ≤ 50 . The purpose is to see whether we can decompile the models by restricting output matching only on inputs where model’s performance is perfect. We can see positive results on bottom row, but still negative results on the top row. 

25 Interpretable Algorithms by Decompiling Transformers Task Model Task Acc in < 50 Task Acc in [51-100] (OOD) Task Acc in [101-150] (OOD) LLNA holds? Can split MLPs? 

Most Frequent 1l4h256d3lr01drop 100.0 99.7 98.4 ✓ ✓

4l4h256d3lr00drop 100.0 100.0 99.7 ✓ ✓

Binary Majority 1l1h16d3lr01drop 100.0 100.0 100.0 ✓ ✓

Binary Majority Interleave 2l2h16d3lr00drop 100.0 99.6 91.7 ✓ ✓

2l2h64d4lr00drop 100.0 74.0 19.5 × -4l1h64d3lr01drop 100.0 98.5 97.9 ✓ ✓

Sort 1l1h256d3lr01drop 97.8 100.0 100.0 ✓ ✓

Count 1l4h256d4lr01drop 100.0 100.0 94.8 ✓ ✓

1l4h256d3lr01drop 100.0 24.0 0.0 × -2l4h256d4lr01drop 100.0 100.0 99.3 ✓ ✓

Unique Copy 2l1h64d3lr01drop 100.0 100.0 99.1 ✓ ✓

4l4h256d4lr00drop 100.0 8.3 0.0 × -2l1h64d3lr01drop-1100 26.3 0.0 0.0 × -2l1h64d3lr01drop-1200 92.5 44.2 0.4 × -2l1h64d3lr01drop-1300 99.4 93.7 50.8 × -2l1h64d3lr01drop-1400 99.9 99.2 81.0 × -Unique Reverse 2l1h64d3lr01drop 100.0 100.0 99.2 ✓ ✓

2l4h256d4lr01drop 100.0 40.7 0.0 × -4l1h256d4lr01drop 100.0 100.0 99.6 ✓ ✓

Unique Bigram Copy 2l4h256d3lr01drop 100.0 100.0 99.7 ✓ ×

4l2h64d3lr00drop 100.0 46.7 0.0 × -2l4h256d3lr01drop-500 34.3 0.2 0.0 × -2l4h256d3lr01drop-700 61.0 5.8 0.0 × -2l4h256d3lr01drop-1400 87.0 27.4 0.8 × -2l4h256d3lr01drop-1900 91.4 53.3 7.8 × -2l4h256d3lr01drop-2100 96.9 85.3 44.3 × -2l4h256d3lr01drop-2200 99.2 95.2 74.2 × -2l4h256d3lr01drop-3300 100.0 99.9 99.8 ✓ ✓

Repeat Copy 4l4h256d3lr00drop 100.0 26.6 0.0 × -Parity 4l4h256d4lr00drop 93.9 67.5 63.2 × -Addition 4l2h64d3lr00drop 50.0 1.1 0.0 × -

Table 1. Length-generalization Performance and Decompilability (whether LLNA holds) of all models trained on algorithmic tasks. The model names are based on their architecture (see the naming scheme in Appendix D.2). We highlight the task accuracy in the longest length range with green, if ≥ 90 , and red otherwise. We say LLNA holds if there exists a model with linearized layer norm achieving match accuracy of ≥ 90 (stage 1, Appendix F.1). Our decompilation pipeline works if LLNA holds, so the LLNA column shows decompilability. Across all the models here, we see that the green and red color in two highlighted columns exactly match. In other words, in all models in this table, our method work on models that length-generalize. 

What if we do decompilation on training length? As mentioned before, we run decompilation using all input lengths, i.e., ≤ 150 , in order to match models’ behavior on both in- and out-of-distribution data. One might wonder what would happen if we remove the all the settings about length-generalization, so models are both trained and decompiled on the same length (e.g, ≤ 50 )? The reason that LLNA does not hold and circuits are not sparse might be because the pruned models are matching the original models’ messy and unpredictable behavior on OOD data, they might be decompilable if we only care about behavior on training data. To investigate this, we select all models whose performance is nearly perfect on training length, but bad on OOD testing length. There are 6 this kind of models. The lowest task accuracy on training length is 99.4%, and on test length of [101 − 150] , 5 models’ accuracy is 0.0% accuracy and one model’s is 50.8%. From the result we can see that although for 3 of them, focusing on lengths where models perform perfectly would enable decompilation, but for the other 3, it does not help at all. Linearizing layer norm would still result in nearly 0 match accuracy. For these models, their circuits that produce perfect in-distribution prediction are still complex. Since we can decompile some of them by restricting length to ≤ 50 , one might wonder if we can get interpretable programs and thus know why they fail on longer length. The answer is no, the layer norm starts to play an important role at longer length. We can probably get interpretable programs but the output of the programs do not match the models’ output on longer length, thus cannot be used to explain model behavior in that scenario. 26 Interpretable Algorithms by Decompiling Transformers Task Model Task Acc in 

< 50 Task Acc in 

[51 − 100] (OOD) Task Acc in 

[101 − 150] (OOD) LLNA holds? Can split MLPs? 

Tomita1 1l1h16d3lr00drop 100.0 100.0 100.0 ✓ ✓

Tomita2 1l1h16d3lr00drop 100.0 100.0 100.0 ✓ ✓

Tomita3 4l4h64d3lr00drop 100.0 97.6 88.3 × -Tomita4 2l2h16d3lr01drop 100.0 100.0 100.0 × -Tomita5 2l1h64d3lr00drop 98.9 70.5 21.6 × -Tomita6 2l2h256d4lr00drop 68.1 9.5 2.3 × -Tomita7 2l1h16d3lr01drop 100.0 100.0 100.0 ✓ ✓

D2 1l1h16d3lr00drop 100.0 100.0 100.0 ✓ ×*

D3 1l1h16d4lr00drop 100.0 100.0 100.0 × -

D4 1l2h256d4lr00drop 100.0 100.0 98.3 ✓ ✓

D12 4l2h256d3lr00drop 100.0 100.0 97.4 ×* ✓

(aa )∗ 1l2h16d3lr01drop 100.0 100.0 100.0 ✓ ×

(aaaa )∗ 2l1h64d4lr01drop 100.0 100.0 100.0 × ✓

(abab )∗ 1l1h64d3lr00drop 100.0 100.0 100.0 × -

aa ∗bb ∗cc ∗dd ∗ee ∗ 1l1h16d3lr00drop 100.0 100.0 100.0 ✓ ✓

{a, b }∗d{b, c }∗ 1l1h16d3lr01drop 100.0 100.0 100.0 ✓ ✓

{0, 1, 2}∗02 ∗ 1l1h64d4lr00drop 100.0 100.0 100.0 × -

Table 2. Length-generalization Performance and Decompilability (whether LLNA holds) of all models trained on formal language tasks. Same as Table 1, the model names are based on their architecture (see the naming scheme in Appendix D.2). We highlight the task accuracy in the longest length range with green, if ≥ 90 , and red otherwise. We say LLNA holds if there exists a model with linearized layer norm achieving match accuracy of ≥ 90 (stage 1, Appendix F.1). Our decompilation pipeline works if LLNA holds, so the LLNA column shows decompilability. There are 2 special cases: the ×* in LLNA column of D12 means that although the highest match accuracy is (only slightly) below 90 in stage 1, it becomes higher than 90 in later pruning stage, thus the resulting program still match the original model and we treat this result as a success of decompilation; the ×* in last column for D2 means that although match accuracy is below 90 for stage 2, it becomes higher than 90 in stage 3, thus we can still split MLPs. We can see that there are 5 cases where non-linear layer norm plays an important role in length-generalizing models: Tomita4, D3, (aaaa )∗, (abab )∗, and {0, 1, 2}∗02 ∗. Nonetheless, for models trained on formal languages, the overall trend still exists, the majority of length-generalizing models are decompilable. 

Remark on FlipFlop We remark that {0, 1, 2}∗02 ∗, equivalent to FlipFlop and challenging for transformers to generalize perfectly on (Liu et al., 2023; Jobanputra et al., 2025), shows strong behavior in Table 2, potentially because we did not specifically design a hard test set testing out-of-distribution generalization (beyond length generalization) as done by Liu et al. (2023). Nonetheless, the model cannot be decompiled, aligning with theoretical predictions about the inability of C-RASP to represent the language (Huang et al., 2025). 

F. Details of Causal Pruning & Linearizing Layer Norm 

Here, we describe the step 2.1 in Sec. 3 in detail. Conceptually, it corresponds to pruning variables and lines from the D-RASP program – equivalently, to computation graph edges between transformer components. To make it scalable, we draw on ideas from the circuit discovery literature. For the convenience of readers familiar with that literature, and to enable comparison with it, we describe the pruning not on the level of D-RASP, but rather in the language of computation graphs representing transformer calculations. As we mentioned in main paper, because the size of D-RASP program grows exponentially with the original transformer’s size, we do not unfold the transformer into full program and then do pruning. Instead, we perform multi-stage pruning, with different definition of computational graph in each stage. Simply speaking, an edge in early stage correspond to many edges in later stage, so in early stage we prune the model in a coarse-grained but efficient way, resulting in much smaller graphs for later stage. Therefore, multi-stage pruning avoids memory explosion and improves efficiency. 

F.1. Stage 1: Pruning Components Component-level Circuit In this stage, we define the same computational graph as circuit discovering research (Conmy et al., 2023; Li & Janson, 2024; Bhaskar et al., 2024). As we described in Sec. B.1, due to residual connection, the output of each transformer layer is accumulated across layers. When a component (e.g. an attention head) takes the residual stream 

y(l) 

> i

as input, all previous components whose output adds to the residual stream are connected to this component, as this component depends on their outputs. Because this dependency is directed, we refer the component that receives other 27 Interpretable Algorithms by Decompiling Transformers 

components’ output as receiving vertex and refer the other side of the connection as sending vertex. Importantly, we treat the query, key and value of an attention head as three separate receiving vertices, while we treat the attention head as one sending node. MLP layer are both receiving vertices and sending vertices; token and position embedding are only sending nodes, final unembedding layer is only a receiving vertex. If the connection between attention head (li, h j ) and (lk, h l)’s value is pruned, it means all ⟨i, p ⟩ ∈ V where p includes 

⟨... (li, h j ), (lk, h l)... ⟩ are pruned together. In this way, we first perform coarse-grained but efficient pruning. 

Pruning Algorithm We apply Uniform Gradient Sampling (UGS) and Optimal Ablation from Li & Janson (2024). We describe the overall idea of the algorithm and emphasize the parts in our implementation that differ from the original paper. For details of the method, please refer to Li & Janson (2024). The method learns masks for edges. The masks are trained for an objective balancing two pressures: on one hand, (i) the masks are encouraged to be zero to completely mask out connections by replacing them with learnable vectors, known as Optimal Ablation; on the other hand, (ii) the KL divergence between the output distribution of pruned model and of original model is minimized during pruning, thus encouraging important connections to be kept. The method optimizes all masks simultaneously using gradient signal, and is thus very scalable. Specifically, if an edge is pruned, the sending vertex (component A)’s output A(x) ∈ RN ×d (x is the input) is replaced by a learned vector a∗ ∈ Rd when computing the input of the receiving vertex. In other words, the replacement is specific to the receiving vertex; other receiving nodes can still receive the original A(x). Because we replace with a constant, the sending vertex’s output does not depend on the input, thus carrying no information. As in Li & Janson (2024), the constant 

a∗ is specific to each vertex A, instead of each edge — this means that, if multiple out-edges from A are pruned, the same constant is used across different receiving vertices. Importantly, unlike Li & Janson (2024), we do not learn different a∗

for different sequence positions, we learn a single a∗ that is applied to the whole sequence uniformly, so that each row of A(x) ∈ RN ×d is replaced by the same a∗. The loss is defined as the mean Kullback-Leibler (KL) divergence between the output distribution of the original model and that of the pruned model. Therefore, these learned optimal ablations are optimized to help the pruned model faithfully preserve the behavior of the original model. Masks are learned simultaneously with the ablation vectors. For each edge, we learn a parameter ˜θ, such that θ = sigmoid (˜θ)

indicates the probability of the edge remaining in the computation graph (as opposed to being replaced with the learned ablation vector). The gradient of θ comes from two terms in the loss function. One is (A) the number of edges in the circuit, represented by its continuous relaxation P θ, where the sum runs over all edges 7. This term is differentiable. The other term is (B) the KL divergence compared to the original model, as used for learning optimal ablations. The gradient of this term with respect to θ is estimated as follows: the ablation coefficient α is sampled from uniform distribution Unif (0 , 1) ; this coefficient is used to mix the original A(x) and the learned replacement a∗ as a convex combination, which is fed into the receiving vertex. The gradient with respect to α is thus differentiable. The average gradient with respect to α is used as the estimated gradient for θ. Importantly, we do not always do sampling for α. When not sampling from uniform distribution, α

is either 0 or 1. Whether or not to do sampling is controlled by a sampling frequency, which is low when θ is close to 0 or 1 (high confidence). Optimal ablation is learned at the same time, but only on examples where α = 0 (not sampled). In summary, the training objective we used is as follows: 

min  

> a∗,˜θ

h

EX∼D ,α∼P ˜θ DKL (M(X)||M a∗,α(X)) + λR( ˜θ)

i

(19) where a∗ represents the collection of a∗ for all edges, ˜θ denotes the collection of ˜θ for all edges, and similarly α denotes all 

α. P ˜θ denotes the distribution of ablation coefficients specified by ˜θ. X and D denote input data and its distribution. M

and Ma∗,α represent the original model’s and the pruned model’s output distribution respectively. R( ˜θ) is the sparsity regularizer representing the expected number of edges. λ is a hyperparameter used to balance between the two terms in the training objective. 

Notes on Layer Normalization Our translation assumes LLNA; we thus replace Layer Normalization (LN) with a linear operation. Since the gradient signal is available during pruning, we also learn scalar constants to replace the variance term in LN in the same way as we learn optimal ablation. For each receiving vertex, a scalar constant is learned. 

> 7Unlike (Li & Janson, 2024), we do not include a pressure towards vertex sparsity because, in some preliminary experiments, we do not find it important.

28 Interpretable Algorithms by Decompiling Transformers 

Hyperparameters We use batch size of 120 with only 10 unique inputs (each copied 12 times), which is same as (Li & Janson, 2024). We use learning rate of 0.002, 0.1 and 0.1 for optimal ablation a∗, masking parameter ˜θ, and scalar constants in linearized LN respectively. We clip the gradient for ˜θ to maximum norm of 5. We stop training once no ˜θ is between -1 and 1 (no ambivalent masks) for 1000 steps, or the maximum step of 5000 is reached. 

F.2. Stage 2: Pruning Paths Path-level Circuit We first convert the resulting graph from step 1 to a new graph where the sending vertices are paths (multiple edges from previous stage connected). Suppose the Value of attention head (l, h ) receives inputs from remaining vertices p1, ..., p n, and the same head (l, h ) is sending to downstream vertices c1, ..., c m. For each downstream vertices, we replace its parent, (l, h ), with multiple new parents p1 ⊕ (l, h ), .., p n ⊕ (l, h ). We do this from earlier layer to later layers. The process implements the definition B.1. The new graph is then used as the initial base graph for further pruning in this step. For example, suppose the following graph is remained after stage 1. There is a list for each vertex, showing the upstream vertices it connects to (it receives inputs from them). 

layer 0: head 0: q: [pos] k: [pos] v: [token] mlp: [token, head 00 ]layer 1: head 0: q: [ mlp 0]k: [ mlp 0]v: [ head 00 , mlp 0]mlp: [] unembedding: [head 10 ]

At the beginning of stage 2, it is converted to the following new graph: 

layer 0: head 0: q: [pos] k: [pos] v: [token] mlp: [token, head 00 -token] layer 1: head 0: q: [ mlp 0]k: [ mlp 0]v: [ head 00 -token, mlp 0]mlp: [] unembedding: [head 10 -head 00 -token, head 10 -mlp 0]

Pruning Details In this step, new optimal ablations are learned for each sending vertex. We need to capture the output of the new vertices, which are “paths”. In one forward pass of the model, we run forward pass of each attention head multiple times, iterating over each of the inputs received by its Value . Because of LLNA, running an attention head with its Value 

29 Interpretable Algorithms by Decompiling Transformers 

taking as input the sum of multiple terms is equivalent to the sum of individual outputs when running it with its Value taking each of one these terms as input (if bias terms are not considered). 

AttnHead (key = X

> i

pi, query = X

> j

p′ 

> j

, value = X

> k

p′′  

> k

) = = X

> k

AttnHead (key = X

> i

pi, query = X

> j

p′ 

> j

, value = p′′  

> k

)

We simply capture each of these individual output of the attention head, which is the output of each path. 

Learned Bias Term for Receiving Vertices Previously, we mentioned that an optimal ablation vector is learned for each sending vertex. We also learn constants (bias term) for each receiving vertex. We note that this should not be viewed as an ablation because these constants are not used to replace real activations. For each receiving vertex, its bias term is always added to its input, so it represents the sum of all previous constant terms. There are many constant terms excluded from the graph, e.g., the bias term of the linear layer computing values in attention heads, it is set to zero when running attention heads over single input source because we do not want to add the bias term repeatedly. Therefore, each receiving vertex learns a bias term to account for all bias terms in previous layer normalization ( β), in linear layers computing value, in linear layers computing the attention output, and most importantly, optimal ablation vectors of vertices pruned in step 1. 

MLP Splitting As mentioned in Sec. 3, we try to split each MLP layer into a sum of new MLPs, so that each new MLP has only a single input. Equivalently, in terms of D-RASP programs, we try to replace multi-input per-position operations with multiple single-input operations. If MLP splitting is enabled, when converting the resulting graph from step 1, we do the same for MLP layers as we do for attention head values. More specifically, suppose an MLP layer mlp k receives values from vertices p1, ..., p n, and it sends values to vertices c1, ..., c m. For each ci, we replace its connection to mlp k, with multiple new connections 

p1 ⊕ mlp k, .., p n ⊕ mlp k. We do this from earlier layer to later layers, together with attention heads. When replacing with new MLP layers, we keep the output dimension of the MLPs the same as the original model dimension. For example, suppose the following graph is remained after stage 1 (Same as the previous one). 

layer 0: head 0: q: [pos] k: [pos] v: [token] mlp: [token, head 00 ]layer 1: head 0: q: [ mlp 0]k: [ mlp 0]v: [ head 00 , mlp 0]mlp: [] unembedding: [head 10 ]

At the beginning of stage 2, if MLP splitting is enabled, it is converted to the following new graph: 

layer 0: head 0: q: [pos] k: [pos] v: [token] mlp: [token, head 00 -token] 

30 Interpretable Algorithms by Decompiling Transformers 

layer 1: head 0: q: [ mlp 0-token, mlp 0-head 00 -token] k: [ mlp 0-token, mlp 0-head 00 -token] v: [ head 00 -token, mlp 0-head 00 -token] mlp: [] unembedding: [head 10 -head 00 -token, head 10 -mlp 0-head 00 -token] 

Empirically we found MLP splitting is usually possible with little effect on match accuracy. 

Hyperparameters We use the same hyperparameters as the previous stage, except for the following: we stop training once no ˜θ is between -1 and 1 (no ambivalent masks) for 500 steps, or the maximum step of 5000 is reached. 

F.3. Stage 3: Pruning QK products Product-level Attention Computation This stage focuses on the Query and Key vertices of attention heads – equivalently, on the select operations. We first convert the resulting graph from stage 2 into a new graph. For each head, we take the Cartesian product of vertices sending to its Query and its Key. The results are pairs of paths, which sending to a new vertex of the head, denoted as QK vertex . In previous stages, the attention logits had been represented as the product of sums, for example: 

AttnLogit = ( Etoken + P pos )⊤ Q⊤K (Etoken + P pos ) , (20) while in this stage attention logits are rewritten as sum of products: 

AttnLogit = token T ET Q⊤KEtoken +token T ET Q⊤KP pos +pos T P T Q⊤KEtoken +pos T P T Q⊤KP pos 

(21) Each pair of paths represents a product. The Query vertex and its edges are thus deleted, while the Key vertex remains. The incoming edges of Key vertex represent the unary (key-only) select operations; this accounts for the fact that a constant term in Query might play a role for attention computation, i.e., some keys are always attended more regardless of the query. This conversion process follows the Equation 2. Use our running example from last stage, and suppose MLP splitting is enabled. Suppose after the last stage’s pruning, the following graph is remained: 

layer 0: head 0: q: [pos] k: [pos] v: [token] mlp: [token, head 00 -token] layer 1: head 0: q: [ mlp 0-token, mlp 0-head 00 -token] k: [ mlp 0-token, mlp 0-head 00 -token] v: [ mlp 0-head 00 -token] mlp: [] unembedding: [head 10 -mlp 0-head 00 -token] 

the beginning of this stage, the graph is converted to the following: 

layer 0: 

31 Interpretable Algorithms by Decompiling Transformers 

head 0: qk: [(pos, pos)] k: [pos] v: [token] mlp: [token, head 00 -token] layer 1: head 0: qk: [( mlp 0-token, mlp 0-token), (mlp 0-token, mlp 0-head 00 -token), (mlp 0-head 00 -token, mlp 0-token), ( mlp 0-head 00 -token, mlp 0-head 00 -token)] k: [ mlp 0-token, mlp 0-head 00 -token] v: [ mlp 0-head 00 -token] mlp: [] unembedding: [head 10 -mlp 0-head 00 -token] 

Pruning in this stage is only applied to terms in QK vertices and K vertices, others remain unchanged. In other words, we are pruning away unimportant select operations. 

Pruning Details In forward passes, we compute individual products of each pair, which are then multiplied with their corresponding mask (recall the θ in previous section F.1) before summed together. The same mask is broadcasted to all token position pairs, i.e., the mask varies for different edges, but stay the same for different token positions in the sequence, just like previous stages. However, unlike previous stages, the optimal ablation is zero here. Because ablating a product means an element of the pair is replaced by a constant. When this element is key, any constant is equivalent to zero because of the softmax later. When this element is query, this product becomes essentially a key-only term, which is the reason for why we introduce key-only terms. For each key-only term, a bias term is learned to act as the query. The product of the learned bias and the key is also multiplied with mask and added to the attention logits. 

AttnLogit( i, s ) = 

> t

X

> r=1

θr Qr (i)⊤Kr (s)

| {z }

> binary (key+query) terms

+

> w

X

> u=1

θu˜b⊤ 

> u

Kt+u(s)

| {z }

> unary (key-only) terms

+ mask (i, s ),

where we use K to represent the real activations (output of paths) to distinguish from the k used in 2, and likewise for Q, θ

is the mask, and ˜b is the learned bias term for each key-only term. Other symbols retain the same meaning as in equation 2. The logits are then fed into softmax function to compute the attention weights. Note that like learning optimal ablation, the bias terms are only trained on samples where the mask θ is 1. 

F.4. Automatic Pruning Coefficients Searching 

During pruning, a coefficient λ is used to balance between faithfulness and sparsity, as described in Eq. 19. In this section, we describe our solution for automatically adjusting the coefficients in different stages, in order to cover as many different sparsity levels as we can and obtain Pareto frontiers. Instead of using the same λ through out all stages, we allow different coefficients for each stages, we denote them as 

λ1, λ 2, λ 3. We begin by establishing a baseline accuracy, which is the accuracy of a model without any attention and MLP layers (a bigram model). For the stage 1, we partition the accuracy range between baseline and perfect performance into 10 evenly spaced intervals. The algorithm adaptively searches for λ1 values based on previous runs, which are (accuracy, coefficient) points. When both higher and lower accuracy points exist for a target interval, we select the geometric mean of their coefficients; when data exists on only one side, we explore new range by scaling the known coefficient by a factor of 5. This process continues until each accuracy interval contains at least one data point or a maximum number of experiments is reached (almost always latter is the case). For each experiments, we stop after stage 1 is finished. Once this initial curve is established, we identify the Pareto-efficient runs (i.e., no other run has both fewer edges and higher match accuracy). From this frontier, we select a handful of representative runs that are evenly spaced along the curve. If 32 Interpretable Algorithms by Decompiling Transformers 

even the highest accuracy achieved in stage 1 runs is below 0.7 (even with negative λ1), we stop and do not continue to later stages. The stage 2 searching is then built directly upon these selected runs. We resume the pruning of the selected stage 1 runs. For each stage 1 run, we run stage 2 with different λ2 to explore the accuracy range near its original accuracy obtained in stage 1. Then similarly we select efficient and representative runs whose resulting (accuracy, number of edges) points are on Pareto frontier. We then resume these runs with different λ3 to do stage 3 searching, in a similar way as before. Hence, the most promising (Pareto-optimal) models from stage 1 are refined in the next, systematically tracing out the full trade-off between sparsity and faithfulness across coefficient configurations. In addition, to see the Pareto frontiers when original layer normalization is enabled, we also run similar coefficient searching before stage 1, which is referred as stage 0. In stage 0 and stage 1, the computational graphs are defined in the same way, the only difference is whether to linearize layer normalization. Importantly, results from stage 0 are not used as the base checkpoints to continue pruning. The coefficient selection in stage 1 is also not informed by stage 0. In other words, stage 0 is separate from other stages, and it is also not a necessary part of the pipeline; its purpose is just to evaluate the Pareto frontier in the presence of full layer normalization. 

G. Details of Explaining Elementwise Operations 

This section describes the details of Step 2.2 in Sec. 3. After the 3-stage pruning, we obtain a sparse (if possible) computational graph for the model we are interested in, corresponding to a pruned D-RASP program. We then try to automatically replace the components in the graph (equivalently, the elementwise operations and the A, b matrices in the D-RASP program) with pre-defined primitives from a library, allowing us to understand their functions and generate informative programs. 

G.1. Tracing activation variables through paths 

This section supplements Sec. 3 in the main paper. We list real activations, activation variables, etc starting from bottom layer. Initial residual stream is: 

Etoken + P pos (22) 

token as a graph vertex (see above examples of graph) sends real activation value Etoken ∈ Rd×N to downstream vertices, where token ∈ R|Σ|× N is the activation variable. Similarly for pos. In head 0 at layer 0, head output is 

V0,0(Etoken + P pos )aT 

> 00

(23) 

=V0,0Etoken aT 

> 00

+ V0,0P pos aT 

> 00

(24) where a00 ∈ RN ×N is the attention weights of this head (recall ai,s defined in Sec. 2, a is softmax over the sum of selectors α). In general, it is query sequence length by key sequence length. Recall that V includes linearized layer norm, Value matrix, and Output matrix of an attention head. Again, head 00 -token as a graph vertex sends real activation 

V0,0Etoken aT

> 00

, and token aT 

> 00

is the activation variable v⟨tok, ⟨1,h ⟩⟩ (defined in Sec. 3 as the output of aggregate ). Therefore, similarly head 10 -head 00 -token as a graph vertex sends real activation V1,0V0,0Etoken aT

> 00

aT

> 10

, and 

token aT

> 00

aT 

> 10

is the activation variable v⟨tok, ⟨0,0⟩,⟨1,0⟩⟩ 

For MLP layers, as mentioned in Step 2.2 in Sec. 3, we replace MLP with primitives. If the primitive is found, we can trace activation variables through the MLP. For example, mlp 0-head 00 -token as a vertex sends the following activation: 

mlp (V00 Etoken aT

> 00

) (25) 

=Cmlp −head 00 −token element wise op (token aT

> 00

, func= f ) (26) where f is a known primitive, and element wise op (token aT

> 00

, func= f ) is the activation variable v⟨v⟨tok, ⟨0,0⟩⟩ ⟩

One more example: if a pair ( head 10 -head 00 -token, mlp 0-head 00 -token ) is remained as input for the QK vertex of head 0 at layer 2, it means the following selector: 33 Interpretable Algorithms by Decompiling Transformers 

(Q2,0V1,0V0,0Etoken aT

> 00

aT

> 10

)T (K2,0Cmlp −head 00 −token element wise op (token aT

> 00

, func= f )) (27) 

= a10 a00 token T

| {z }

> q= v⟨tok, ⟨0,0⟩,⟨1,0⟩⟩

ET V T

> 0,0

V T

> 1,0

QT

> 2,0

K2,0Cmlp −head 00 −token 

| {z }

> op= A

element wise op (token aT

> 00

, func= f )) 

| {z }

> k= v⟨v⟨tok, ⟨0,0⟩⟩ ⟩

(28) In summary, activation variables can be captured by tracing from initial token or pos , and iteratively consider each unit in the path from bottom to top. If it is attention head, then multiply the attention weights. If it is MLP layer, evaluate the element wise op with the known primitive. If the MLP does not have a matched primitive, we lose track of the activation variables and use backup approach of inspecting downstream effect, which will be described later. 

G.2. Primitive Matching for Single-input MLP 

We first describe the primitive matching for per-position operations in the D-RASP program – equivalently, for MLPs in the pruned transformer. In most cases, MLPs can be split into single-input MLPs (see Appendix F.2) with little effect on match accuracy. In these cases, for each single-input MLP remaining in the pruned graph, we search among a list of predefined single-input primitives to replace it, starting from the first layer. For each MLP, the replacement is done as follows: 1. Collect output activations v ∈ Rd×N (see method in Appendix F.2) of the target MLP (which is also activation variables before finding a primitive), and activation variables x ∈ Rd(x)×N (see method in Appendix G.1) of its inputs. Columns of v is the result of per-column operation of x, i.e., v(i) only depends on x(i). We collect N = 20000 such pairs after filtering out v(i) whose gradient is zero. Note that capturing x requires successful primitive matching of previous MLP in the path, if the previous MLP does not have a matched primitive, we stop the primitive matching process for the current MLP as we do not know the input activation variables. So the current MLP does not have a matched primitive either. 2. Test primitives. For each primitive fj in the single input primitive list, do the following: (a) Obtain the transformed activation variables w ∈ Rd(w)×N by applying fj to x (i.e., w(i) =

element wise op (x(i), func= fs)). d(w) depends on the output dimension of fj .(b) Obtain a linear mapping C by solving the problem min C ∥v − Cw|| 2 

> F

, with solution C = vw +, where w+ is the pseudo-inverse of w.(c) Evaluate the replaced MLP with match accuracy. In each forward pass, replace MLP output v with 

Celement wise op (x, func= fj ) and compute match accuracy. 3. Select the best primitive. Importantly, to simplify the generated program, we favor replacing MLP with linear operation, i.e., fno −op . We first try fno −op , if match accuracy is above a threshold then we match fno −op with the MLP and skip all other primitives. In our experiments we use 0.92 as the threshold. Otherwise, we test all primitives and match the one with the highest accuracy 8 with the MLP. Meanwhile, save the C corresponding to the selected primitive. Afterwards for downstream layers, w computed from the selected primitive will be the activation variable for the MLP, and C will be used to compose the op= matrix in select and project 

4. However, if all primitives fail to match original model’s output — that is, the match accuracy of all primitives is lower than a threshold (we use 0.9) — we conclude that no primitive in our library matches the MLP. In this case, we store the variables and visualize them so that one can still interpret the MLP manually (Section G.4). One may also add new primitive once they get insights from manual inspection, so that the MLP is matched to a primitive. In our experiments, we search among the library below: 

Library of Single-input primitives 

1. fno −op (x) := x  

> 8we favor fno −op again by increasing its accuracy by 0.01 when comparing with others

34 Interpretable Algorithms by Decompiling Transformers 

2. fsharpen (x)i := xniP 

> ixni

, where hyperparameter n ∈ { 2, 3, 5}.3. fharden (x) := earg max j x, i.e., n → ∞ version of fsharpen 

4. f01 −balanced := max( x1 − x0, 0) n, max( x0 − x1, 0) n, 1 − max( x1 − x0, 0) n − max( x0 − x1, 0) n, where hyperparameter n ∈ { 0.5, 0.05 , 0.01 }. Only test this when input activation variable is token and the alphabet Σ

only contains “0” “1” besides special tokens. 5. fis −pure := I(x1 > τ ), · · · , I(xd(x) > τ ), 1 − Pdi=1 I(xi > τ ), hyperparameter τ ∈{0.95 , 0.9, 0.85 , 0.8, 0.75 , 0.7}.where we abuse notation a bit, use x as a vector in this list (so is the x(i) outside of the list), xi is an entry of it, xa

denotes the entry correspond to token “a”, and similarly xb, x 0, x i.

G.3. Primitive Matching for Multi-input MLP 

For some models, we observe that if we split MLPs, the resulting model often do not make the same prediction as the original model, no matter how many edges are enabled. Therefore, we do not split MLP for these models. Our idea of replacing MLP with primitives can still work in this case, though the search space of possible primitives becomes much larger. We do the same as we do for single-input MLPs, with the following small changes: 1. There are multiple input activation variables, x1, . . . , x s, we collect them all. 2. The tested fi operates over multiple inputs w = element wise op (x1, . . . , x s, func= fi), and is from a different list (see below). 

Library of Multiple-input primitives 

1. fkeep −i(x(1) , . . . , x (i), . . . , x (s)) := x(i), where hyperparameter i ∈ { 1, . . . , s }, x(i) is a vector corresponding to the i-th operand. 2. fcartesian (x(1) , . . . , x (s))j1,...,j s := min {x(1)       

> j1,...,x (s)
> js }
> P
> j1,...,js min {x(1)
> j1,...,x (s)
> js }

, where output fcartesian (x(1) , . . . , x (s)) ∈

Rd(x(1) )×···× d(x(s)), and j1, . . . , j s are indices for each variable. The output is then flatten to a vector 

∈ Rd(x(1) )...d (x(s)).

G.4. Backup Approach (No Matching Primitive): Inspecting Downstream Effect 

In case no primitive is found for the MLP, one can still interpret it manually by observing the downstream effects caused by the MLP receiving different inputs. There are two types of downstream effects: one is how MLPs affect the QK product and thus attention weights (or information flow between residual streams); the other is, given fixed attention weights, how MLPs affect model output. The former corresponds to MLPs in the paths ends with QK product in the final pruned graph (i.e., a 

select operator), the latter corresponds to MLPs in the paths ends with unembedding (i.e., a project operator). We discuss these two cases separately. 

Paths ending with unembedding (i.e., project operator) Our method for collecting output effect is as follows: 1. Given a path containing unexplained MLP (i.e., MLP not replaced with a primitive from the library) and ending with unembedding in the pruned graph, we first identify the first unexplained MLP in the path (in earliest layer). Collect its input activation variable samples vinp and corresponding output of this MLP vout . We aim to make vout interpretable by tracing downstream effect until output vocabulary space. 2. We absorb all matrices acting on the MLP output throughout the downstream path into the MLP output. Formally, we iterate over downstream components afterwards on the path. For each attention head in the path, we update it 35 Interpretable Algorithms by Decompiling Transformers 

by multiplying with OV matrix vout ← V vout . If it is an MLP layer, we update it by applying the MLP layer at a black-box function vout ← mlp (vout ). If it is the final logit, we update it by multiplying with unembedding matrix 

vout ← U vout . So finally vout is projected to vocabulary space. Therefore, we obtain pairs of interpretable vectors, which are columns of (vinp , v out ).3. In terms of the D-RASP program, we add a per-position operation mapping vinp to an output of dimension d(vout ) = 

|Σ|. We replace the original project operation by project (vout , I |Σ|×| Σ|), that is, the project operation uses the identity matrix. 4. To aid interpretation, we subtract each column of vout with its second largest value, since this is equivalent under softmax. We emphasize the token promoted most. When using BCE loss, we do not do this. 

Example For example, in the pruned model, if an unexplained MLP is in a path of the form: 

unembedding : head 10 -mlp -head 01 -token then we inspect the following pairs: instances of its input activation variable v⟨tok, ⟨(0 ,1) ⟩⟩ , and their corresponding MLP output vmlp after multiplied with V1,0 and unembedding matrix U . Each pair is two vectors ∈ R|Σ|, one shows an aggregation of input tokens from the context, another shows which tokens would be promoted or suppressed if the MLP’s output for such aggregation is attended or routed to the prediction by downstream heads. For another example, the path can be: 

unembedding : head 20 -mlp ′-head 10 -mlp -head 01 -token Assuming mlp can’t find a matched primitive, we cannot apply primitive matching to mlp ′ as well because we cannot get the input activation variable. Our backup approach explain the two MLPs together, by treating the part mlp ′-head 10 -mlp as a black-box and inspecting its input and output. However, if there are attention layers between the first unexplained MLP to the last MLP on the path, and any of them produces non-one-hot attention weights, then the final transformed vout is not correct. Because when tracing through MLPs, we actually assume each representation vector is fully attended, instead of being mixed with others. In the last example, the final vout represents what output token would be promoted/suppressed when the output of mlp is fully routed to the prediction. Because the MLP output on a weighted sum of representations is not equal to the weighted sum of MLP output on the same representations, so we do not know the true effect of each individual mlp output vector when they are mixed with others by the head 10 .Therefore, we rigorously identify the condition when the backup approach faithfully represents the downstream effect and when it does not. We raise a warning in our pipeline if such condition is not satisfied. 

Paths ending with QK product Similar to the previous approach, we do the following: 1. Given a QK product whose query or key contains at least one unexplained MLP in the pruned graph, we first identify the first unexplained MLP in the path (in earliest layer) for both query and key. Collect its input activation variable samples v(q) 

> inp

and corresponding output of this MLP v(q)

> out

. Similarly for v(k) 

> inp

and v(k)

> out

.2. Similar to previous method, we iterate over downstream components afterwards on the path to get final v(q) 

> out

and v(k)

> out

.When only one of query and key contains unexplained MLP, we apply the method in Appendix G.1 for the other to get these two variables easily. 3. We do (v(q)

> out

)T · QTl,h · Kl,h · v(k)

> out

. Each entry in this matrix describes how each pair of columns in v(k) 

> inp

and v(q)

> inp

is associated. So We make vout interpretable by tracing downstream effect on attention. In other words, we inspect what are the input variable pairs that will be associated in the QK product. The unexplained MLP plays a role in this association, and it is understood together with other components. 4. In terms of the D-RASP program, we add a per-position operation mapping to this output, and replace the select 

operation by one where the A matrix is the identity matrix. 36 Interpretable Algorithms by Decompiling Transformers 

5. To aid interpretation, we center the products of the same query, since this is equivalent under softmax. Moreover, we apply K-means clustering to query’s activation variables, and show what are the keys associated with each “type” (cluster) of query. 

G.5. Remark on Logit Lens 

Our backup approach uses an idea somewhat similar to logit lens (nostalgebraist, 2020) or Direct Logit Attribution, but our method allows us to determine the correct way to transform the MLP output. For example, when explaining the MLP in this path: unembedding : head 10 -mlp -head 01 -token, the activation should first be multiplied by OV matrix of head 0 at layer 1 before multiplying by the unembedding matrix, because this is how it contributes to the model prediction according to the pruned graph. This is a very important distinction from naively applying the unmbedding matrix, which assumes that a components always contribute mainly via a direct connection to the output. Thus, for components which contribute to the output mainly by indirect connection (there are other components along the path between it and the final logit), such naive projecting to vocabulary space is reading the “side-effect” in the direct path and causing illusionary interpretation. Unfortunately, many studies have done (and are still doing) this with justification only based on plausibility of the results. Moreover, many components contribute by forming the correct QK circuits (i.e., which tokens should be attended) instead of via OV circuits (what’s the effect on output if attended). Our backup approach successfully account for this (i.e., paths ending with QK product). 

H. Replacing attention and unembedding matrices with primitives 

We try to replace each parameter matrix in select and project with a subset of primitives from Table 3. We use only a subset of primitives for each type of parameter matrices, since some of the primitives are not applicable to some of the types of parameter matrices. Specifically, “Diagonal”, “ kth Diagonal” and “Every kth” primitives are designed for replacing matrices and are not applicable to replacing vectors. Apart from that, unembedding matrices always project the activation to the vocabulary, which makes “Decreasing”, “Increasing”, “ kth Diagonal” and “Every kth” primitives not interpretable when applied to unembedding projections, since vocabulary tokens do not have intrinsic ordering. We then notice that in many cases the dimension of the matrix that needs to be replaced corresponds to vocabulary size (the variable which is being multiplied by the primitive corresponds to a path ⟨tok, x 1 . . . x k⟩). In these cases, it is beneficial to separate the entries in the primitives corresponding to special tokens and non-special tokens. We then separate the primitive matrix into two pieces: when query is a special token and when it is a non-special token, and select a separate primitive for each. In this case, we use separate arguments op= and special op= for those two primitives. 

Selecting Primitives When selecting the primitives, we replace primitives one by one, starting from the lowest layers. When replacing the primitive, we try all possible combinations of tuples of primitives (attend-to-special, attend-to-non-special) in the order: • For attention bias (vector): “Zeros”, “to BOS”, “to EOS”, “to SEP”, “Decreasing”, “Increasing”. • For attention matrix: “Zeros”, “to BOS”, “to SEP”, “Diagonal”, “Second Diagonal”, “Third Diagonal”, “Every Second”, “Every Third”, “Decreasing”, “Increasing”. • Fro logits bias (vector): “Zeros”, “to EOS”. • For logit matrix: “Zeros”, “to EOS”, “Diagonal”. When the accuracy of replacement is above the threshold ( 0.95 × match accuracy of the model after pruning), we accept it and do not try other primitives. 

Simplifying Remaining Matrices We then simplify the parameter matrices that were not replaced with predefined primitives. The goal here is especially to remove information that is not causally relevant to the model’s predictions, to aid in interpretation. We initialize each of the parameter matrices not replaced with predefined primitives Ai as the original parameter matrices, and optimize them all together in two steps. We denote all the optimized parameters as 

A = [ A1, A 2, . . . ]. First, we incentivize the parameters to be close to zero (penalty p := ∥A∥1), and then then we incentivize them to be integers (penalty p := ∥A − round (A)∥1). We train the matrix for 2,000 steps with the loss l + λp ,37 Interpretable Algorithms by Decompiling Transformers 

Primitive Name Matrix Definition (A = {aij }) Matrix example Vector Definition (a = {ai}) Vector example 

Diagonal 

> op=(k==q) op=(inp==out)

aij = δi=j — —

kth Diagonal  

> op=(k==q-k)

aij = δi=j+k — —Every kth  

> op=(k% k==q% k==0)

aij = δi mod k=0 & j mod k=0 — —to BOS/SEP/EOS 

> op=(k==BOS/SEP/EOS) op=(out==BOS/SEP/EOS)

aij = δj is BOS/SEP/EOS ai = δi is BOS/SEP/EOS 

Decreasing    

> op=(k is first)

aij = jm , (A ∈ Rn×m) ai = jn , (a ∈ Rn)

Increasing    

> op=(k is last)

aij = m−j+1  

> m

, (A ∈ Rn×m) n−j+1  

> n

, (a ∈ Rn)

Zeros   

> op=(uniform selection)

aij = 0 ai = 0 

Table 3. Our library of primitives for A, b. We scale all matrices with a large number (we take 10 , 000 ). This allows us to simulate near-hard outputs after softmax. This is both in line with the learned solutions, which often hold large magnitudes (cf. Figure 15) and with theoretical arguments showing that large attention logits are needed to produce discrete outputs (e.g. Chiang & Cholak, 2022; Zhou et al., 2024). In the case of attention, we note that rows denote the query dimensions and columns denote key dimensions. In the case of prediction logits, we note that rows denote the input dimensions and columns denote output dimensions. In the case of selector matrices 

A, rows denote queries; columns denote keys. In the case of project matrices A, rows denote inputs; columns denote outputs. When row and column counts differ, matrices are not square; in this case, we start from the top left corner and truncate the matrix accordingly. 

where p is the penalty and l is the original task loss. We stop training if average match accuracy with the original model drops below the threshold (defined as 0.91 * match accuracy of the model after pruning) over the interval of 10 steps. If this happens, we revert to the checkpoint when the match accuracy is still above the threshold. We use batch size 120, lr 10 −4,and sweep λ over 10 −1, 10 −2, 10 −4.We then forcibly round each primitive matrix if the match accuracy of the model stays above the threshold, to account for potential imperfections in rounding after the optimization procedure . 

I. Example of Primitive Replacement 

Here, we show examples of the matrices in Unique Copy (Figure 3), after replacement with primitives (Figure 14) and for comparison when performing regularization towards zero and inetegrs, but no primitive replacement (Figure 15). Comparison shows that the basic patterns are similar (e.g., diagonal and off-diagonal matrices, with special behavior for special tokens), but the primitives bring the behavior out more clearly. 38 Interpretable Algorithms by Decompiling Transformers 0 50 100 150 200 250 300 

0

50 

100 

150 

200 

250 

300 

0

2000 

4000 

6000 

8000 

10000 

(a) Line 1: op=(k==q-1) 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0

2000 

4000 

6000 

8000 

10000 

(b) Line 3: op=(k==q), spe-cial op=(k==BOS) 0

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> 25
> 26
> 27
> 28
> 29
> 30
> 31
> 32
> 33
> 34
> 35
> 36
> 37
> 38
> 39
> 40
> 41
> 42
> 43
> 44
> 45
> 46
> 47
> 48
> 49
> 50
> 51
> 52
> 53
> 54
> 55
> 56
> 57
> 58
> 59
> 60
> 61
> 62
> 63
> 64
> 65
> 66
> 67
> 68
> 69
> 70
> 71
> 72
> 73
> 74
> 75
> 76
> 77
> 78
> 79
> 80
> 81
> 82
> 83
> 84
> 85
> 86
> 87
> 88
> 89
> 90
> 91
> 92
> 93
> 94
> 95
> 96
> 97
> 98
> 99
> 100
> 101
> 102
> 103
> 104
> 105
> 106
> 107
> 108
> 109
> 110
> 111
> 112
> 113
> 114
> 115
> 116
> 117
> 118
> 119
> 120
> 121
> 122
> 123
> 124
> 125
> 126
> 127
> 128
> 129
> 130
> 131
> 132
> 133
> 134
> 135
> 136
> 137
> 138
> 139
> 140
> 141
> 142
> 143
> 144
> 145
> 146
> 147
> 148
> 149
> <bos>
> <sep>
> <eos>
> <pad>

2.5 

0.0 

2.5 

(c) Line 6: op= c in logits2 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0

2000 

4000 

6000 

8000 

10000 

(d) Line 5: op=(inp==out), spe-cial op=(uniform selection) 

Figure 14. Heatmaps supporting the program for unique copy model. This is discussed in Main paper, Figure 3. 

39 Interpretable Algorithms by Decompiling Transformers 0 50 100 150 200 250 300 

> 0
> 50
> 100
> 150
> 200
> 250
> 300

300 

200 

100 

0

100 

200 

(a) Line 1: op=(k==q-1) 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

75 

50 

25 

0

25 

50 

75 

100 

(b) Line 3: op=(k==q), spe-cial op=(k==BOS) 0

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> 25
> 26
> 27
> 28
> 29
> 30
> 31
> 32
> 33
> 34
> 35
> 36
> 37
> 38
> 39
> 40
> 41
> 42
> 43
> 44
> 45
> 46
> 47
> 48
> 49
> 50
> 51
> 52
> 53
> 54
> 55
> 56
> 57
> 58
> 59
> 60
> 61
> 62
> 63
> 64
> 65
> 66
> 67
> 68
> 69
> 70
> 71
> 72
> 73
> 74
> 75
> 76
> 77
> 78
> 79
> 80
> 81
> 82
> 83
> 84
> 85
> 86
> 87
> 88
> 89
> 90
> 91
> 92
> 93
> 94
> 95
> 96
> 97
> 98
> 99
> 100
> 101
> 102
> 103
> 104
> 105
> 106
> 107
> 108
> 109
> 110
> 111
> 112
> 113
> 114
> 115
> 116
> 117
> 118
> 119
> 120
> 121
> 122
> 123
> 124
> 125
> 126
> 127
> 128
> 129
> 130
> 131
> 132
> 133
> 134
> 135
> 136
> 137
> 138
> 139
> 140
> 141
> 142
> 143
> 144
> 145
> 146
> 147
> 148
> 149
> <bos>
> <sep>
> <eos>
> <pad>

2.5 

0.0 

2.5 

(c) Line 6: op= c in logits2 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> 25
> 26
> 27
> 28
> 29
> 30
> 31
> 32
> 33
> 34
> 35
> 36
> 37
> 38
> 39
> 40
> 41
> 42
> 43
> 44
> 45
> 46
> 47
> 48
> 49
> 50
> 51
> 52
> 53
> 54
> 55
> 56
> 57
> 58
> 59
> 60
> 61
> 62
> 63
> 64
> 65
> 66
> 67
> 68
> 69
> 70
> 71
> 72
> 73
> 74
> 75
> 76
> 77
> 78
> 79
> 80
> 81
> 82
> 83
> 84
> 85
> 86
> 87
> 88
> 89
> 90
> 91
> 92
> 93
> 94
> 95
> 96
> 97
> 98
> 99
> 100
> 101
> 102
> 103
> 104
> 105
> 106
> 107
> 108
> 109
> 110
> 111
> 112
> 113
> 114
> 115
> 116
> 117
> 118
> 119
> 120
> 121
> 122
> 123
> 124
> 125
> 126
> 127
> 128
> 129
> 130
> 131
> 132
> 133
> 134
> 135
> 136
> 137
> 138
> 139
> 140
> 141
> 142
> 143
> 144
> 145
> 146
> 147
> 148
> 149
> <bos>
> <sep>
> <eos>
> <pad>

15 

10 

5

0

5

10 

15 

20 

(d) Line 5: op=(inp==out), spe-cial op=(uniform selection) 

Figure 15. Heatmaps supporting the program for unique copy model. Heatmaps are only rounded, not replaced with primitives. 

J. Decompiled Programs on Algorithmic Tasks 

How to read this section Each subsection below shows a D-RASP decompilation of a trained transformer. We report the target task, description of the model’s architecture, and two performance metrics: task accuracy and match accuracy with the original model. We report accuracies after reparametrization and simplification (pruning) steps, but before replacement of attention, unembedding projections, or MLPs with primitives, as well as accuracies after all steps of the pipeline. D-RASP code for each model consists of code and supporting heatmaps that specify operations. For ease of understanding, we also report visualizations of variables computed on example input. Throughout, for selector matrices A, rows denote query dimensions and columns denote key dimensions. For projection matrices A, rows denote input dimensions and columns denote output dimensions. For variables heatmaps , in the case of activation variables, the x-axis denotes the positions in a sample input string; the y-axis denotes the dimensions in the variable. In the case of selector variables, the rows denote query positions, the columns denote key positions. We show causal masking by graying out of cells. In the programs where MLP operations cannot be replaced with primitives, we show visualizations inspecting their downstream affect, computed as described in Section G.4. When interpreting the effect on unembedding projection, we 40 Interpretable Algorithms by Decompiling Transformers 

Task Program 

Binary Majority Sec. J.1 Binary Majority Interleave Sec. J.2 J.3 Count Sec. J.4 J.5 Most Frequent Sec. J.6 J.7 Sort Sec. J.8 Unique Bigram Copy Sec. J.9 J.10 Unique Copy Sec. J.11 Unique Reverse Sec. J.12 J.13 

D12 Sec. K.1 

D2 Sec. K.2 

D4 Sec. K.3 

(aa )∗ Sec. K.4 

aa ∗bb ∗cc ∗dd ∗ee ∗ Sec. K.5 

{a, b }∗d{b, c }∗ Sec. K.6 Tomita1 Sec. K.7 Tomita2 Sec. K.8 Tomita7 Sec. K.9 visualize inputs to MLP that give rise to high and low logits of a few representative tokens in separate plots. When interpreting the effect on attention logits, we visualize pairs of keys and queries together that give rise to high and low attention logits. We note that, when interpreting these operations, we use the terms “elementwise operation” (which is the formal term in D-RASP as in the original RASP, Weiss et al. (2021)), “per-position operation”, and “MLP” interchangeably. 

J.1. Majority Task Description: 

⟨bos ⟩ s ⟨sep ⟩ Maj (s) where s ∈ { 0, 1}∗

Architecture: Layers: 1 Heads: 1 Hidden Dim: 16 LR: 0.001 Dropout: 0.1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 1.00 → 1.00 ; Match Accuracy: 1.00 → 1.00 

Code 

1. a1 = aggregate(s=[], v=token) # layer 0 head 0 2. logits1 = project(inp=a1, op=(inp==out), special op=(uniform selection)) 3. prediction = softmax(logits1) 

Interpretation The program is essentially equivalent to the program shown for the non-binary version in Figure 1 of the main paper. a1 holds counts of the relative frequencies of 0, 1, BOS, SEP (Figure 17a). Line 2 then projects the counts of 0, 1 onto output logits, disregarding the special tokens (Figure 17b). As a result, at SEP (where the prediction relevant to solving the task is made), the more common symbol receives the higher logit. 41 Interpretable Algorithms by Decompiling Transformers 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

1

<bos> 

<sep> 

<eos> 

<pad> 0

> 2000
> 4000
> 6000
> 8000
> 10000

(a) Line 2: op=(inp==out), special op=(uniform selection) 

Figure 16. Heatmaps supporting the program for Majority model. (a) The identity matrix (temperature-scaled to create effectively hard attention) for normal tokens, uniform on special tokens. <bos>  

> 0
> 0
> 0
> 1
> 0
> <sep>
> 0
> 0
> 1
> <bos>
> <sep>
> <eos>
> <pad> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(a) Line 1: a1 <bos>   

> 0
> 0
> 0
> 1
> 0
> <sep>
> 0
> 0
> 1
> <bos>
> <sep>
> <eos>
> <pad> 0
> 2000
> 4000
> 6000

(b) Line 2: logits1 

Figure 17. Variables Heatmaps for Majority model on an example input. (a) Aggregation computes a histogram of symbols seen so far (x-axis is the input, y-axis are the dimensions of the activation). (b) Output logits are directly obtained from (a) in the case of normal tokens, and erased for special tokens, reflecting the matrix in Figure 16. 

J.2. Majority Interleave Task Description: 

⟨bos ⟩ s ⟨sep ⟩ Maj ({si|i%3 = 0 })Maj ({si|i%3 = 1 })Maj ({si|i%3 = 2 }) ⟨eos ⟩where s ∈ { 0, 1}∗

Architecture: Layers: 2 Heads: 2 Hidden Dim: 16 LR: 0.001 Dropout: 0 

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.89 → 0.77 ; Match Accuracy: 0.92 → 0.80 

Code 

1. s1 = select(q=token, k=token, op= a ) # layer 0 head 0 2. s2 = select(q=pos, k=pos, op= b ) # layer 0 head 0 3. a1 = aggregate(s=s1+s2, v=token) # layer 0 head 0 4. s3 = select(q=pos, k=pos, op= c ) # layer 0 head 1 5. a2 = aggregate(s=s3, v=token) # layer 0 head 1 6. s4 = select(q=pos, k=token, op= j ) # layer 1 head 0 7. s5 = select(q=pos, k=pos, op=(k==q-2)) # layer 1 head 0 8. a3 = aggregate(s=s4+s5, v=token) # layer 1 head 0 9. a4 = aggregate(s=s4+s5, v=pos) # layer 1 head 0 10. is_pure_token = is_pure(token) # layer 1 mlp 11. is_01_balance_a3 = is_01_balance(a3) # layer 1 mlp 12. logits1 = project(inp=a4, op= k ) 13. logits2 = project(inp=token, op= e ) 

42 Interpretable Algorithms by Decompiling Transformers 

14. logits3 = project(inp=pos, op= l ) 15. logits4 = project(inp=is_pure_token, op= f ) 16. logits5 = project(inp=a1, op= g ) 17. logits6 = project(inp=a2, op= h ) 18. logits7 = project(inp=is_01_balance_a3, op= i ) 19. logits8 = project(inp=pos, op= m ) 20. prediction = softmax(logits1+ logits2+ logits3+ logits4+ logits5+ logits6+ logits7+ logits8) 

Interpretation In this task, the model is tasked with solving three binary majority tasks in succession, with the three input strings presented in interleaved order. The main algorithm is similar to the one of the majority model (Figure 1): a1 

roughly holds counts of frequencies of tokens at every third position (Figure 17c), which then get projected with the diagonal projection matrix in Line 16 (Figure 18g). a2 also shows a variation of position-driven aggregation (Figure 18c), however, it aggregates the values in all positions, except for current one (Figure 17e). It then gets projected in Line 17 (Figure 18h) by a anti-diagonal matrix, with amplitude of weights twice as low as of the projection matrix on Line 16, which suggests that this might act as a kind of correction mechanism. Logit projections in Lines 13, 15, 18 implement special behavior to predict EOS. Lines 14 and 19 implement position-based logic that promotes generation of ⟨eos ⟩ and cancels each other’s effect in 0 or 1 tokens (Figure 19k,p). Interestingly, a simpler and more accurate program is extracted from a 4-layer 1-head model (App. J.3). 43 Interpretable Algorithms by Decompiling Transformers 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

1

<bos> 

<sep> 

<eos> 

<pad> 25 

0

25 

50 

75 (a) Line 1: op= a in s1 0 20 40 60 80 100 120 140 

> 0
> 20
> 40
> 60
> 80
> 100
> 120
> 140
> 1.0
> 0.5
> 0.0
> 0.5
> 1.0
> 1.5
> 2.0

(b) Line 2: op= b in s2 0 20 40 60 80 100 120 140 

> 0
> 20
> 40
> 60
> 80
> 100
> 120
> 140
> 10
> 8
> 6
> 4
> 2
> 0
> 2
> 4
> 6

(c) Line 4: op= c in s3 0 20 40 60 80 100 120 140 

> 0
> 20
> 40
> 60
> 80
> 100
> 120
> 140
> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(d) Line 7: op=(k==q-2) 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

1

<bos> 

<sep> 

<eos> 

<pad> 

2

0

2

4

6

(e) Line 13: op= e in logits2 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 1
> 2
> 3
> 4
> 5
> 6

40 

30 

20 

10 

0

(f) Line 15: op= f in logits4 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

1

<bos> 

<sep> 

<eos> 

<pad> 20 

10 

0

10 

20 

(g) Line 16: op= g in logits5 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

1

<bos> 

<sep> 

<eos> 

<pad> 

5

0

5

10 

(h) Line 17: op= h in logits6 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>
> 0.5
> 0.0
> 0.5
> 1.0
> 1.5
> 2.0
> 2.5

100 

75 

50 

25 

0

(i) Line 18: op= i in logits7 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

20 

40 

60 

80 

100 

120 

140 

200 

150 

100 

50 

0

50 

(j) Line 6: op= j in s4 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

20 

40 

60 

80 

100 

120 

140 

20 

10 

0

10 

20 

(k) Line 12: op= k in logits1 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

20 

40 

60 

80 

100 

120 

140 

0

5

10 

15 

20 

(l) Line 14: op= l in logits3 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

20 

40 

60 

80 

100 

120 

140 

15 

10 

5

0

5

10 

15 

(m) Line 19: op= m in logits8 

Figure 18. Heatmaps supporting the program for Majority Interleave model. 

44 Interpretable Algorithms by Decompiling Transformers <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

<bos> 

1

0

1

1

0

1

<sep> 

1

0

1

<eos> 

0

20 

40 

60 (a) Line 1: s1 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

<bos> 

1

0

1

1

0

1

<sep> 

1

0

1

<eos> 1.0 

0.5 

0.0 

0.5 

1.0 (b) Line 2: s2 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (c) Line 3: a1 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

<bos> 

1

0

1

1

0

1

<sep> 

1

0

1

<eos> 

7.5 

5.0 

2.5 

0.0 

2.5 (d) Line 4: s3 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (e) Line 5: a2 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

<bos> 

1

0

1

1

0

1

<sep> 

1

0

1

<eos> 

100 

50 

0

(f) Line 6: s4 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

<bos> 

1

0

1

1

0

1

<sep> 

1

0

1

<eos> 0

2000 

4000 

6000 

8000 

10000 (g) Line 7: s5 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (h) Line 8: a3 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 

10 

0

10 (i) Line 12: logits1 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 

2

0

2

4

6 (j) Line 13: logits2 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 

0

5

10 

(k) Line 14: logits3 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 

40 

30 

20 

10 

0 (l) Line 15: logits4 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 

5

0

5

(m) Line 16: log-its5 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 

5

0

5

(n) Line 17: log-its6 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 100 

75 

50 

25 

0 (o) Line 18: logits7 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 

10 

5

0

5

(p) Line 19: logits8 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

20 

40 

60 

80 

100 

120 

140 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (q) Line 9: a4 

Figure 19. Variables Heatmaps for Majority Interleave model on an example input. 

45 Interpretable Algorithms by Decompiling Transformers 

J.3. Majority Interleave : different arch with same length generalization performance Task Description: 

⟨bos ⟩ s ⟨sep ⟩ Maj ({si|i%3 = 0 })Maj ({si|i%3 = 1 })Maj ({si|i%3 = 2 }) ⟨eos ⟩where s ∈ { 0, 1}∗

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.97 → 0.97 ; Match Accuracy: 0.97 → 0.97 

Code 

1. s1 = select(q=token, k=token, op= a ) # layer 0 head 0 2. s2 = select(q=pos, k=pos, op=(k%3==q%3==0)) # layer 0 head 0 3. a1 = aggregate(s=s1+s2, v=token) # layer 0 head 0 4. new_a1 = element_wise_op(a1) # layer 0 mlp 5. logits1 = project(inp=new_a1, op=(inp==out)) 6. logits2 = project(op= c ) 7. prediction = softmax(logits1+ logits2) 

Interpretation In this task, the model is tasked with solving three binary majority tasks in succession, with the three input strinfs presented in interleaved order. Line 1 defines a selector ensuring that, at SEP, attention specifically goes to the non-special tokens (Figure 21a). Line 2 defines a selector putting attention on the basis of positions modulo 3 (Figure 21b). For instance, on SEP, this leads to attention to the positions 1, 4, 7, . . . of the input string – which consists to the first of the three interleaved inputs. Line 3 then aggregates across these positions. Similarly, after outputting the first and second result, attention will go onto the second and third of the three interleaved input string (Figure 21c) to obtain relative counts of 0, 1 throughout the relevant slice of the input. Line 4 then performs an element-wise operation. Inspecting it shows that it provides a high logit for 1 if 1 is more common than 0 (Figure 23), a high logit for 0 if 0 is more common (Figure 22), and EOS if SEP occupies a substantial amount of activation (Figure 24). The last case happens when all three results have been output, causing the model to output EOS and stop. Line 6 boosts EOS further. Overall, the prediction is first three times a 0/1 label, one for each of the three interleaved strings, and then EOS. 46 Interpretable Algorithms by Decompiling Transformers 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

1

<bos> 

<sep> 

<eos> 

<pad> 

40 

20 

0(a) Line 1: op= a in s1 0 20 40 60 80 100 120 140  

> 0
> 20
> 40
> 60
> 80
> 100
> 120
> 140
> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(b) Line 2: op=(k%3==q%3==0) 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

2

(c) Line 6: op= c in logits2 

Figure 20. Heatmaps supporting the program for Majority Interleave : different arch with same length generalization performance model. <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

<bos> 

1

0

1

1

0

1

<sep> 

1

0

1

<eos> 15 

10 

5

0

5(a) Line 1: s1 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

<bos> 

1

0

1

1

0

1

<sep> 

1

0

1

<eos> 0

2000 

4000 

6000 

8000 

10000 (b) Line 2: s2 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (c) Line 3: a1 <bos> 

> 1
> 0
> 1
> 1
> 0
> 1
> <sep>
> 1
> 0
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 50 

> 0
> 50

(d) Line 5: logits1 0 2 4 6 8 10 

0

1

<bos> 

<sep> 

<eos> 

<pad> 1.0  

> 0.5
> 0.0
> 0.5
> 1.0
> 1.5
> 2.0

(e) Line 6: logits2 

Figure 21. Variables Heatmaps for Majority Interleave : different arch with same length generalization performance model on an example input. 

47 Interpretable Algorithms by Decompiling Transformers 

MLP Input-Output Distributions Explaining per-position operation in Line 4 via its effect on Output Logits in Line 5 

Output Token: 0 0

> 1
> <bos>
> <sep>

0.89 0.00 0.10 0.00 

0.86 0.00 0.14 0.00 

0.90 0.01 0.09 0.00 

0.87 0.01 0.12 0.00 

0.82 0.00 0.17 0.00 

0.88 0.01 0.11 0.00 

0.81 0.02 0.17 0.00 

0.89 0.01 0.10 0.00 

0.86 0.02 0.12 0.00 

0.84 0.02 0.14 0.00 

0.58 0.01 0.41 0.00 

0.91 0.01 0.08 0.00 

0.91 0.00 0.09 0.00 

0.88 0.01 0.12 0.00 

0.90 0.03 0.07 0.00 

0.87 0.01 0.12 0.00 

0.75 0.13 0.12 0.00 

0.69 0.23 0.08 0.00 

0.72 0.15 0.12 0.00 

0.89 0.11 0.00 0.00 

0.65 0.27 0.07 0.00 

0.59 0.41 0.00 0.00 

0.54 0.44 0.03 0.00 

0.01 0.93 0.07 0.00 

0.29 0.70 0.00 0.00 

0.25 0.75 0.00 0.00 

0.07 0.93 0.00 0.00 

0.08 0.91 0.00 0.00 

0.09 0.91 0.00 0.00 

0.08 0.92 0.00 0.00 

0.05 0.95 0.00 0.00 

0.03 0.97 0.00 0.00 

0.05 0.86 0.00 0.09 

Input Features 

logit:0 

27.08 

27.07 

26.81 

26.80 

26.64 

26.59 

26.38 

26.38 

26.30 

26.27 

24.36 

24.32 

24.31 

24.30 

24.26 

21.65 

18.92 

16.23 

13.53 

10.83 

8.12 

5.42 

2.69 

-2.23 

-4.53 

-6.89 

-9.22 

-11.56 

-13.91 

-16.17 

-18.03 

-19.22 

-21.55 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

20 

10 

0

10 

20 

Figure 22. MLP Input-Output for token: 0 (sorted by logits) 

Output Token: 1 

48 Interpretable Algorithms by Decompiling Transformers 0

> 1
> <bos>
> <sep>

0.01 0.90 0.09 0.00 

0.01 0.89 0.10 0.00 

0.01 0.92 0.07 0.00 

0.01 0.88 0.11 0.00 

0.01 0.88 0.12 0.00 

0.01 0.86 0.13 0.00 

0.01 0.89 0.10 0.00 

0.01 0.83 0.16 0.00 

0.01 0.90 0.09 0.00 

0.02 0.85 0.13 0.00 

0.00 0.94 0.06 0.00 

0.04 0.90 0.06 0.00 

0.04 0.91 0.05 0.00 

0.01 0.92 0.08 0.00 

0.02 0.89 0.10 0.00 

0.05 0.85 0.10 0.00 

0.08 0.75 0.17 0.00 

0.23 0.67 0.10 0.00 

0.22 0.62 0.16 0.00 

0.34 0.65 0.01 0.00 

0.22 0.78 0.00 0.00 

0.36 0.49 0.15 0.00 

0.46 0.53 0.01 0.00 

0.86 0.01 0.13 0.00 

0.50 0.37 0.00 0.13 

0.74 0.26 0.00 0.00 

0.78 0.22 0.00 0.00 

0.88 0.11 0.00 0.00 

0.95 0.05 0.01 0.00 

0.68 0.21 0.00 0.11 

1.00 0.00 0.00 0.00 

0.99 0.01 0.00 0.00 

Input Features 

logit:1 

28.36 

28.12 

27.72 

27.64 

27.64 

27.59 

27.52 

27.50 

27.46 

27.42 

25.49 

25.43 

25.42 

25.40 

25.39 

22.69 

19.85 

17.01 

14.14 

11.34 

8.51 

5.66 

2.83 

-1.94 

-4.03 

-5.97 

-8.02 

-10.09 

-12.19 

-14.07 

-15.52 

-16.44 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

20 

10 

0

10 

20 

Figure 23. MLP Input-Output for token: 1 (sorted by logits) 

Output Token: EOS 

49 Interpretable Algorithms by Decompiling Transformers 0                                                                                    

> 1
> <bos>
> <sep>
> 0.27 0.00 0.01 0.72
> 0.01 0.42 0.00 0.57
> 0.12 0.26 0.00 0.62
> 0.00 0.34 0.00 0.65
> 0.47 0.01 0.00 0.52
> 0.30 0.10 0.00 0.60
> 0.31 0.11 0.00 0.58
> 0.03 0.39 0.01 0.57
> 0.24 0.12 0.00 0.64
> 0.40 0.06 0.00 0.53
> 0.51 0.01 0.00 0.48
> 0.39 0.09 0.00 0.51
> 0.34 0.19 0.00 0.46
> 0.23 0.36 0.00 0.41
> 0.53 0.16 0.00 0.31
> 0.15 0.57 0.00 0.28
> 0.35 0.42 0.00 0.22
> 0.47 0.34 0.00 0.19
> 0.14 0.73 0.00 0.12
> 0.13 0.78 0.09 0.00
> 0.78 0.16 0.06 0.00
> 0.28 0.64 0.08 0.00
> 0.39 0.35 0.26 0.00
> 0.39 0.00 0.60 0.00
> 0.31 0.00 0.69 0.00
> 0.01 0.35 0.65 0.00
> 0.20 0.26 0.53 0.00
> 0.00 0.28 0.72 0.00

Input Features 

logit:<eos> 

> 39.95
> 38.90
> 33.84
> 33.36
> 31.75
> 30.45
> 30.24
> 29.14
> 27.92
> 26.54
> 26.53
> 25.12
> 24.73
> 22.57
> 17.46
> 16.66
> 11.07
> 7.77
> 3.74
> -3.79
> -7.63
> -11.45
> -15.25
> -19.05
> -22.70
> -25.67
> -29.04
> -31.59

Output 

0.0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

30 

20 

10 

0

10 

20 

30 

Figure 24. MLP Input-Output for token: EOS (sorted by logits) 

J.4. Count Task Description: 

⟨bos ⟩ s0, s n ⟨sep ⟩ s0s1 . . . s n ⟨eos ⟩where s0, s n ∈ { 0, 1, . . . , 150 }, s n > s 0, s i+1 = si + 1 

Architecture: Layers: 1 Heads: 4 Hidden Dim: 256 LR: 0.0001 Dropout: 0.1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.92 → 0.92 ; Match Accuracy: 0.90 → 0.92 

Code 

1. s1 = select(q=token, k=token, op= a ) # layer 0 head 0 2. a1 = aggregate(s=s1, v=token) # layer 0 head 0 3. s2 = select(q=token, k=token, op= b ) # layer 0 head 1 4. s3 = select(k=token, op= c ) 5. a2 = aggregate(s=s2+s3, v=token) # layer 0 head 1 6. new_a2 = element_wise_op(a2) # layer 0 mlp 7. logits1 = project(inp=token, op= d ) 8. logits2 = project(inp=a1, op= e ) 9. logits3 = project(inp=new_a2, op=(inp==out)) 10. prediction = softmax(logits1+ logits2+ logits3) 

Interpretation Line 1 defines a selector favoring weight to numbers larger than the present one. So each number would attend to the previous largest number, which the terminating number. In contrast, we also notice this pattern is reversed 50 Interpretable Algorithms by Decompiling Transformers 

for SEP (please zoom in on the op matrix), so SEP always attends to the smaller number, which is the starting number. Line 2 moves the attended token, so at this point the model already get the needed information. Lines 3–5 perform a very similar operation. Interestingly, whereas Lines 1–2 used only a single A matrix, these lines 3–5 achieve this effect using both a A matrix and a b vector giving higher weight to large numbers. We can also see the patterns in A for both heads are not perfect, so they probably supplementing each other. a1 and a2 contain similar information, but feed into different downstream paths in the program: a1 feeds into logits2 (discussed below), whereas a2 does so only after going through a per-position operation (line 6). Inspecting the operation shows that it performs a hardening operation (strongly visible at 1 and 10, Figures 27, 28), but does so mostly only for smaller numbers (no clear effect at 100, Figure 29). We attribute this to the fact that the input is heavily skewed towards larger numbers due to the b vector. We are now ready to discuss how the output logits are generated. First, as also discussed in the main paper, line 7 directly maps token to the increment of the current number via an off-diagonal matrix (Figure 25d); this accounts for the bulk of the counting behavior, but is not sufficient for starting counting after SEP and ending it with EOS when reaching the upper limit. First, line 8 is responsible for starting counting after SEP, outputting the value retrieved from a1 at SEP (faintly visible in Figure 26g), but this effect is not as strong as line 7, so except at SEP, model would still predict the increment-by-one token. Line 9 forwards the output from line 6. At the end, no larger number is found in the context, triggering EOS. 51 Interpretable Algorithms by Decompiling Transformers 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

100 

50 

0

50 

100 

150 (a) Line 1: op= a in s1 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

100 

50 

0

50 

100 

150 (b) Line 3: op= b in s2 0

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> 25
> 26
> 27
> 28
> 29
> 30
> 31
> 32
> 33
> 34
> 35
> 36
> 37
> 38
> 39
> 40
> 41
> 42
> 43
> 44
> 45
> 46
> 47
> 48
> 49
> 50
> 51
> 52
> 53
> 54
> 55
> 56
> 57
> 58
> 59
> 60
> 61
> 62
> 63
> 64
> 65
> 66
> 67
> 68
> 69
> 70
> 71
> 72
> 73
> 74
> 75
> 76
> 77
> 78
> 79
> 80
> 81
> 82
> 83
> 84
> 85
> 86
> 87
> 88
> 89
> 90
> 91
> 92
> 93
> 94
> 95
> 96
> 97
> 98
> 99
> 100
> 101
> 102
> 103
> 104
> 105
> 106
> 107
> 108
> 109
> 110
> 111
> 112
> 113
> 114
> 115
> 116
> 117
> 118
> 119
> 120
> 121
> 122
> 123
> 124
> 125
> 126
> 127
> 128
> 129
> 130
> 131
> 132
> 133
> 134
> 135
> 136
> 137
> 138
> 139
> 140
> 141
> 142
> 143
> 144
> 145
> 146
> 147
> 148
> 149
> <bos>
> <sep>
> <eos>
> <pad>

50 

0

50 

(c) Line 4: op= c in s3 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

15 

10 

5

0

5

10 

15 

20 

25 

30 

(d) Line 7: op= d in logits1 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

15 

10 

5

0

5

10 

15 

(e) Line 8: op= e in logits2 

Figure 25. Heatmaps supporting the program for Count model. 

52 Interpretable Algorithms by Decompiling Transformers <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

<bos> 

72 

75 

<sep> 

72 

73 

74 

75 

<eos> 

50 

25 

0

25 

50 (a) Line 1: s1 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

<bos> 

72 

75 

<sep> 

72 

73 

74 

75 

<eos> 

50 

0

50 (b) Line 3: s2 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

0.50 

0.25 

0.00 

0.25 

0.50 

0

25 (c) Line 4: s3 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

(d) Line 2: a1 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (e) Line 5: a2 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

5

0

5

10 

15 

20 

25 (f) Line 7: logits1 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

10 

5

0

5

10 

15 (g) Line 8: logits2 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

10 

8

6

4

2

0

2

4

6 (h) Line 9: logits3 

Figure 26. Variables Heatmaps for Count model on an example input. 

MLP Input-Output Distributions Explaining per-position operation in Line 6 via its effect on Output Logits in Line 9 

Output Token: 1 

53 Interpretable Algorithms by Decompiling Transformers 1

> ...
> 43
> ...
> 80
> ...
> 86
> ...
> 88
> ...
> 103
> ...
> 115
> ...
> 128
> ...
> 131
> ...
> 135
> ...
> 139
> ...
> 147
> 148
> 149

Input Features 

logit:1 

4.73 

4.43 

4.33 

4.17 

3.92 

1.51 

1.43 

1.30 

1.17 

1.09 

1.06 

0.82 

0.69 

0.57 

0.55 

0.53 

0.29 

-0.96 

-1.92 

-2.89 

-3.85 

-4.81 

-5.77 

-6.74 

-7.69 

-8.64 

-9.22 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

5

0

5

Figure 27. MLP Input-Output for token: 1 (sorted by logits) 

Output Token: 10 10 

> ...
> 61
> ...
> 72
> ...
> 74
> ...
> 89
> 90
> 91
> ...
> 98
> 99
> 100
> ...
> 118
> ...
> 133
> ...
> 137
> ...
> 140
> ...
> 142
> ...
> 145
> ...
> 148
> 149
> <bos>

Input Features 

logit:10 

2.47 

2.35 

2.04 

1.53 

1.11 

1.05 

0.34 

0.24 

0.18 

0.01 

0.00 

-0.79 

-0.80 

-0.80 

-0.80 

-0.81 

-1.63 

-2.44 

-3.25 

-4.07 

-4.88 

-5.70 

-6.50 

-7.30 

-8.14 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

7.5 

5.0 

2.5 

0.0 

2.5 

5.0 

7.5 

Figure 28. MLP Input-Output for token: 10 (sorted by logits) 

Output Token: 100 57 

> ...
> 66
> ...
> 88
> ...
> 94
> ...
> 98
> ...
> 100
> ...
> 110
> ...
> 118
> 119
> 120
> ...
> 129
> ...
> 135
> ...
> 144
> 145

Input Features 

logit:100 

0.93 

0.89 

0.89 

0.84 

0.83 

0.76 

0.75 

0.74 

0.73 

0.71 

0.70 

0.68 

0.63 

0.60 

0.58 

0.55 

0.40 

0.32 

0.21 

0.00 

-2.37 

-4.74 

-7.11 

-9.48 

-11.84 

-14.22 

-16.38 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

15 

10 

5

0

5

10 

15 

Figure 29. MLP Input-Output for token: 100 (sorted by logits) 

54 Interpretable Algorithms by Decompiling Transformers 

Output Token: EOS 6

> ...
> 14
> ...
> 16
> ...
> 20
> ...
> 22
> ...
> 60
> 61
> ...
> 64
> ...
> 78
> ...
> 90
> 91
> ...
> 93
> ...
> 96
> ...
> 100
> ...
> 103
> ...
> 117
> 118
> ...
> 137
> ...
> 140
> ...
> 144
> ...
> <bos>
> <sep>
> Input Features
> logit:<eos>
> 2.52
> 1.53
> 1.12
> 1.01
> 0.55
> 0.47
> 0.44
> 0.24
> 0.23
> 0.17
> 0.11
> 0.05
> 0.04
> 0.00
> -2.84
> -2.87
> -5.80
> -8.74
> -11.67
> -14.57
> -17.49
> -20.27
> -22.68
> Output
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 20
> 10
> 0
> 10
> 20

Figure 30. MLP Input-Output for token: EOS (sorted by logits) 

J.5. Count : different arch with same length generalization performance Task Description: 

⟨bos ⟩ s0, s n ⟨sep ⟩ s0s1 . . . s n ⟨eos ⟩where s0, s n ∈ { 0, 1, . . . , 150 }, s n > s 0, s i+1 = si + 1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.94 → 0.86 ; Match Accuracy: 0.94 → 0.86 

Code 

1. s1 = select(q=token, k=token, op=(uniform selection), special op=(k is last)) # layer 0 head 2 2. a1 = aggregate(s=s1, v=token) # layer 0 head 2 3. new_a1 = element_wise_op(a1) # layer 0 mlp 4. s2 = select(q=token, k=token, op=(k==q), special op=(uniform selection)) # layer 1 head 3 5. s3 = select(q=token, k=new_a1, op=(q==k)) # layer 1 head 3 6. a2 = aggregate(s=s2+s3, v=new_a1) # layer 1 head 3 7. logits1 = project(inp=a2, op=(inp==out)) 8. logits2 = project(inp=token, op= c ) 9. logits3 = project(inp=new_a1, op=(inp==out)) 10. logits4 = project(inp=token, op= d ) 11. prediction = softmax(logits1+ logits2+ logits3+ logits4) 

Interpretation This program uses a similar strategy to the other version, but with a few differences. In line 1, SEP attends to the smallest number, which is the starting number. Line 1, 2, 3, and 9 forms the mechanism to predict starting number (e.g., Figure 37 shows the obtained number is predicted). Meanwhile, line 8 and 10 form the mechanism for incrementing the number by 1. This mechanism is activated on normal tokens but is not activated on special tokens (magnitude is nearly 0 on rows correspond to special tokens in A). The mechanism for predicting EOS token is a bit more complex. Figure 36 shows that line 3-7 is responsible for regulating when EOS is output, as logits1 specifically holds predictions for EOS. 55 Interpretable Algorithms by Decompiling Transformers 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0

2000 

4000 

6000 

8000 

10000 (a) Line 1: op=(uniform selec-tion), special op=(k is last) 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0

2000 

4000 

6000 

8000 

10000 

(b) Line 4: op=(k==q), spe-cial op=(uniform selection) 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

10 

5

0

5

10 

15 

20 

(c) Line 8: op= c in logits2 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

5

4

3

2

1

0

1

2

3

(d) Line 10: op= d in logits4 

Figure 31. Heatmaps supporting the program for Count : different arch with same length generalization performance model. 

56 Interpretable Algorithms by Decompiling Transformers <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

<bos> 

72 

75 

<sep> 

72 

73 

74 

75 

<eos> 0

2000 

4000 (a) Line 1: s1 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

<bos> 

72 

75 

<sep> 

72 

73 

74 

75 

<eos> 0

2000 

4000 

6000 

8000 

10000 (b) Line 4: s2 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

<bos> 

72 

75 

<sep> 

72 

73 

74 

75 

<eos> 

25 

0

25 

50 (c) Line 5: s3 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

(d) Line 2: a1 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

0

50 

100 

150 

200 

250 

2

1

0

1

2 (e) Line 6: a2 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0

2

4

6

8

(f) Line 7: log-its1 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

5.0 

2.5 

0.0 

2.5 

5.0 

7.5 

10.0 

12.5 

(g) Line 8: log-its2 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

6

4

2

0

2

4

6

(h) Line 9: log-its3 <bos> 

> 72
> 75
> <sep>
> 72
> 73
> 74
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

2.0 

1.5 

1.0 

0.5 

0.0 

0.5 

1.0 

(i) Line 10: log-its4 

Figure 32. Variables Heatmaps for Count : different arch with same length generalization performance model on an example input. 

57 Interpretable Algorithms by Decompiling Transformers 

MLP Input-Output Distributions Explaining per-position operation in Line 3 via its effect on Output Logits in Line 7 

Output Token: 1 3

> ...
> 20
> ...
> 32
> ...
> 38
> 39
> ...
> 45
> ...
> 60
> ...
> 73
> ...
> 86
> ...
> 94
> ...
> 105
> ...
> 113
> ...
> 121
> ...
> 135
> ...
> <bos>

Input Features 

> logit:1
> 0.26
> 0.26
> 0.26
> 0.23
> 0.18
> 0.16
> 0.13
> 0.13
> 0.10
> 0.09
> 0.08
> 0.08
> 0.08
> 0.07
> 0.06
> 0.05
> 0.02
> -0.55
> -1.09
> -1.63
> -2.17
> -2.74
> -3.24
> -3.83
> -4.31
> -4.59

Output 

> 0.0
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 4
> 2
> 0
> 2
> 4

Figure 33. MLP Input-Output for token: 1 (sorted by logits) 

Output Token: 10 14 

> ...
> 18
> ...
> 25
> ...
> 30

Input Features 

logit:10 

-0.00 

-0.01 

-0.01 

-0.02 

-0.04 

-0.05 

-0.06 

-0.06 

-0.06 

-0.06 

-0.99 

-0.99 

-0.99 

-0.99 

-1.00 

-1.99 

-2.92 

-3.72 

-4.24 

-5.11 

-9.26 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

7.5 

5.0 

2.5 

0.0 

2.5 

5.0 

7.5 

Figure 34. MLP Input-Output for token: 10 (sorted by logits) 

Output Token: 100 

58 Interpretable Algorithms by Decompiling Transformers 2

> 3
> ...
> 15
> ...
> 44

Input Features 

logit:100 

0.08 

0.06 

0.03 

0.01 

0.00 

-1.96 

-1.96 

-1.96 

-1.96 

-1.96 

-1.96 

-1.96 

-1.96 

-3.93 

-3.93 

-3.93 

-5.74 

-7.41 

-8.55 

-12.11 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

10 

5

0

5

10 

Figure 35. MLP Input-Output for token: 100 (sorted by logits) 

Output Token: EOS 2

> ...
> 6
> ...
> 13
> ...
> 18
> ...
> 20
> ...
> 24
> 25
> ...
> 31
> ...
> 40
> ...
> 49
> ...
> 56
> ...
> 60
> ...
> 77
> ...
> 85
> ...
> 89
> ...
> 103
> 104
> ...
> 110
> 111
> ...
> 113
> ...
> 116
> ...
> 121
> ...
> 132
> ...
> 143
> ...
> 149
> <bos>

Input Features 

logit:<eos> 

> 17.35
> 17.27
> 15.54
> 15.36
> 15.34
> 15.15
> 14.48
> 14.16
> 14.13
> 13.72
> 13.60
> 13.52
> 13.23
> 12.90
> 12.71
> 12.06
> 10.37
> 8.61
> 6.92
> 5.18
> 3.44
> 1.70
> -8.91
> -17.87
> -26.10
> -33.11
> -36.93
> -49.37
> -84.62

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

75 

50 

25 

0

25 

50 

75 

Figure 36. MLP Input-Output for token: EOS (sorted by logits) 

Explaining per-position operation in Line 3 via its effect on Output Logits in Line 9 

Output Token: 1 

59 Interpretable Algorithms by Decompiling Transformers 1

> ...
> 13
> ...
> 39
> ...
> 57
> ...
> 66
> ...
> 108
> ...
> 115
> ...
> 137
> ...
> 148
> ...
> <bos>
> <sep>

Input Features 

logit:1 

5.24 

4.68 

4.26 

4.23 

4.07 

2.67 

2.09 

2.03 

2.02 

1.95 

1.93 

1.70 

1.53 

1.00 

0.81 

0.66 

0.28 

-1.91 

-3.83 

-5.71 

-7.53 

-9.35 

-11.38 

-14.24 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

10 

5

0

5

10 

Figure 37. MLP Input-Output for token: 1 (sorted by logits) 

Output Token: 10 3

> ...
> 9
> 10
> ...
> 25
> ...
> 29
> ...
> 36
> 37
> 38
> ...
> 45
> ...
> 141
> ...
> 147
> ...
> <bos>
> <sep>

Input Features 

logit:10 

3.08 

2.16 

2.05 

1.80 

1.74 

0.69 

0.09 

0.00 

-0.03 

-0.05 

-0.08 

-0.17 

-2.05 

-2.05 

-2.05 

-2.05 

-4.10 

-6.07 

-8.07 

-9.81 

-11.85 

-15.47 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

15 

10 

5

0

5

10 

15 

Figure 38. MLP Input-Output for token: 10 (sorted by logits) 

Output Token: 100 

60 Interpretable Algorithms by Decompiling Transformers 4

> 5
> 6
> ...
> 17
> ...
> 36
> ...
> 58
> ...
> 61
> ...
> 66
> ...
> 75
> ...
> 100
> ...
> <bos>
> <sep>

Input Features 

logit:100 

> 2.82
> 0.00
> -0.02
> -0.03
> -0.04
> -1.90
> -1.90
> -1.90
> -1.90
> -1.90
> -1.90
> -1.91
> -1.91
> -3.80
> -3.80
> -3.81
> -5.65
> -7.40
> -9.11
> -10.57
> -13.25

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

10 

5

0

5

10 

Figure 39. MLP Input-Output for token: 100 (sorted by logits) 

Output Token: EOS 4

> ...
> 17
> 18
> ...
> 27
> ...
> 39
> ...
> 47
> ...
> 61
> ...
> 130
> ...
> <bos>
> <sep>

Input Features 

logit:<eos> 

6.54 

6.25 

6.23 

6.23 

6.20 

6.16 

6.15 

6.08 

6.02 

6.01 

5.88 

5.86 

5.86 

5.85 

5.83 

5.23 

4.58 

3.93 

3.27 

2.62 

1.96 

1.30 

0.65 

-1.17 

-2.43 

-3.68 

-4.93 

-6.09 

-7.39 

-8.39 

-9.46 

-10.31 

-11.28 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

10 

5

0

5

10 

Figure 40. MLP Input-Output for token: EOS (sorted by logits) 

Explaining per-position operation in Line 3 via its effect on Output Key in Line 5 See Figure 41. 61 Interpretable Algorithms by Decompiling Transformers 13 

> 14
> ...
> 26
> ...
> 44
> ...
> 46
> ...
> 51
> ...
> 101
> ...
> 115
> ...
> 131
> ...
> 137
> ...
> 142
> Input Features: Query
> 5
> ...
> 13
> 14
> ...
> 26
> ...
> 39
> ...
> 44
> ...
> 46
> ...
> 56
> 57
> ...
> 64
> ...
> 74
> ...
> 83
> ...
> 126
> ...
> <bos>
> Input Features: Key
> Logits
> 132.66
> 132.00
> 130.12
> 117.22
> 117.10
> 107.74
> 105.53
> 99.62
> 99.61
> 92.83
> 92.37
> 79.43
> 79.18
> 66.24
> 53.03
> 53.01
> 13.27
> -64.81
> -127.86
> Output
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 100
> 50
> 0
> 50
> 100

Figure 41. MLP Input-Output (sorted by logits) 

J.6. Most Frequent Task Description: 

⟨bos ⟩ s ⟨sep ⟩ Maj (s) where s ∈ { a, b, c, . . . , y, z }∗

Architecture: Layers: 1 Heads: 4 Hidden Dim: 256 LR: 0.001 Dropout: 0.1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.99 → 1.00 ; Match Accuracy: 0.98 → 0.99 

Code 

1. a1 = aggregate(s=[], v=token) # layer 0 head 1 2. logits1 = project(inp=a1, op=(inp==out), special op=(uniform selection)) 3. prediction = softmax(logits1) 

Interpretation This program is interpreted in Main Paper, Figure 1. a

> b
> c
> d
> e
> f
> g
> h
> i
> j
> k
> l
> m
> n
> o
> p
> q
> r
> s
> t
> u
> v
> w
> x
> y
> z
> <bos>
> <sep>
> <eos>
> <pad>

a

b

c

d

e

f

g

h

i

j

k

l

m

n

o

p

q

r

s

t

u

v

w

x

y

z

<bos> 

<sep> 

<eos> 

<pad> 

> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(a) Line 2: op=(inp==out), special op=(uniform selection) 

Figure 42. Heatmaps supporting the program for Most Frequent model. The identity matrix (temperature-scaled to create effectively hard attention) for normal tokens, uniform on special tokens. 

62 Interpretable Algorithms by Decompiling Transformers <bos> 

> o
> b
> r
> o
> <sep>
> o
> a
> b
> c
> d
> e
> f
> g
> h
> i
> j
> k
> l
> m
> n
> o
> p
> q
> r
> s
> t
> u
> v
> w
> x
> y
> z
> <bos>
> <sep>
> <eos>
> <pad>
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(a) Line 1: a1 <bos>  

> o
> b
> r
> o
> <sep>
> o
> a
> b
> c
> d
> e
> f
> g
> h
> i
> j
> k
> l
> m
> n
> o
> p
> q
> r
> s
> t
> u
> v
> w
> x
> y
> z
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 1000
> 2000
> 3000
> 4000
> 5000

(b) Line 2: logits1 

Figure 43. Variables Heatmaps for Most Frequent model on an example input. (a) Aggregation computes a histogram of symbols seen so far (x-axis is the input, y-axis are the dimensions of the activation). (b) Output logits are directly obtained from (a) in the case of normal tokens, and erased for special tokens, reflecting the matrix in Figure 42a. 

J.7. Most Frequent : different architecture Task Description: 

⟨bos ⟩ s ⟨sep ⟩ Maj (s) where s ∈ { a, b, c, . . . , y, z }∗

Architecture: Layers: 4 Heads: 4 Hidden Dim: 256 LR: 0.001 Dropout: 0 

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.98 → 1.00 ; Match Accuracy: 0.98 → 1.00 

Code 

1. a1 = aggregate(s=[], v=token) # layer 0 head 0 2. logits1 = project(inp=a1, op=(inp==out), special op=(uniform selection)) 3. prediction = softmax(logits1) 

Interpretation The program is essentially the same as in App. J.6. 63 Interpretable Algorithms by Decompiling Transformers a

> b
> c
> d
> e
> f
> g
> h
> i
> j
> k
> l
> m
> n
> o
> p
> q
> r
> s
> t
> u
> v
> w
> x
> y
> z
> <bos>
> <sep>
> <eos>
> <pad>
> a
> b
> c
> d
> e
> f
> g
> h
> i
> j
> k
> l
> m
> n
> o
> p
> q
> r
> s
> t
> u
> v
> w
> x
> y
> z
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(a) Line 2: op=(inp==out), special op=(uniform selection) 

Figure 44. Heatmaps supporting the program for Most Frequent : different architecture model. <bos> 

> o
> b
> r
> o
> <sep>
> o

a

b

c

d

e

f

g

h

i

j

k

l

m

n

o

p

q

r

s

t

u

v

w

x

y

z

<bos> 

<sep> 

<eos> 

<pad> 

> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(a) Line 1: a1 <bos> 

> o
> b
> r
> o
> <sep>
> o

a

b

c

d

e

f

g

h

i

j

k

l

m

n

o

p

q

r

s

t

u

v

w

x

y

z

<bos> 

<sep> 

<eos> 

<pad>  

> 0
> 1000
> 2000
> 3000
> 4000
> 5000

(b) Line 2: logits1 

Figure 45. Variables Heatmaps for Most Frequent : different architecture model on an example input. 

J.8. Sort Task Description: 

⟨bos ⟩ s ⟨sep ⟩ sσ(0) , s σ(0) . . . s σ(n) ⟨eos ⟩where s0 ∈ { 0, 1, . . . , 150 }, s i+1 = si + 1 , σsortss 

Architecture: Layers: 1 Heads: 1 Hidden Dim: 256 LR: 0.001 Dropout: 0.1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.95 → 0.90 ; Match Accuracy: 0.95 → 0.90 

Code 

1. s1 = select(q=token, k=token, op= a ) # layer 0 head 0 2. a1 = aggregate(s=s1, v=token) # layer 0 head 0 

64 Interpretable Algorithms by Decompiling Transformers 

3. new_a1 = element_wise_op(a1) # layer 0 mlp 4. logits1 = project(inp=new_a1, op=(inp==out)) 5. prediction = softmax(logits1) 

Interpretation This is discussed in Main paper, Figure 5. Line 1 assigns weight to input numbers that are a little larger than the current token. Line 2 then creates a histogram of larger numbers, with the biggest weight given to the smallest one. The operation in line 3 essentially performs a hardening operation and produces the output logits. 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

200 

100 

0

100 

200 

300 

400 

500 (a) Line 1: op= a in s1 

Figure 46. Heatmaps supporting the program for Sort model. Part of this matrix is shown in Figure 5. 

65 Interpretable Algorithms by Decompiling Transformers <bos> 

> 66
> 115
> 81
> 55
> 136
> 118
> <sep>
> 55
> 66
> 81
> 115
> 118
> 136
> <eos>

<bos> 

66 

115 

81 

55 

136 

118 

<sep> 

55 

66 

81 

115 

118 

136 

<eos> 

> 200
> 100
> 0
> 100
> 200

(a) Line 1: s1 <bos> 

> 66
> 115
> 81
> 55
> 136
> 118
> <sep>
> 55
> 66
> 81
> 115
> 118
> 136
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (b) Line 2: a1 <bos> 

> 66
> 115
> 81
> 55
> 136
> 118
> <sep>
> 55
> 66
> 81
> 115
> 118
> 136
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

10 

5

0

5

10 

15 

20 (c) Line 4: logits1 

Figure 47. Variables Heatmaps for Sort model on an example input. (a) The selector (rows index query positions, columns index key positions) favors numbers slightly larger than the current one. (b) The resulting weighted histogram. (c) Next-token predictions result from applying the elementwise operation on a1 and projecting via the identity matrix. At each generation step (after SEP), logit is highest on the next symbol; and on EOS at the end. 

MLP Input-Output Distributions Explaining per-position operation in Line 3 via its effect on Output Logits in Line 4 

Output Token: 0 

66 Interpretable Algorithms by Decompiling Transformers 0

> ...
> 2
> 3
> ...
> 5
> ...
> 10
> ...
> 12
> ...
> 15
> ...
> 32
> 33
> ...
> 41
> ...
> 58
> ...
> 90
> ...
> 116
> ...
> 118
> ...
> 122
> ...
> 141
> ...
> 144

Input Features 

logit:0 

> 11.07
> 10.60
> 10.35
> 7.00
> 0.00
> -0.39
> -0.41
> -0.42
> -0.46
> -0.52
> -0.61
> -3.89
> -3.91
> -3.93
> -3.94
> -3.95
> -7.91
> -11.87
> -15.82
> -19.78
> -23.73
> -27.66
> -31.59
> -35.48
> -39.57

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

20 

0

20 

Figure 48. MLP Input-Output for token: 0 (sorted by logits). The operation hardens the input by promoting the output dimension for “0” when the input has a high entry in this dimension; similarly for the other dimensions. 

Output Token: 10 10 

> ...
> 27
> ...
> 58
> ...
> 60
> ...
> 65
> ...
> 87
> ...
> 92
> ...
> 108
> ...
> 111
> 112
> ...
> 142
> ...
> <bos>

Input Features 

logit:10 

12.49 

12.45 

12.41 

12.40 

12.27 

12.24 

11.94 

11.80 

11.53 

11.52 

11.23 

10.93 

10.78 

10.62 

10.48 

9.95 

8.27 

-3.36 

-6.79 

-10.24 

-13.66 

-17.07 

-20.48 

-23.87 

-27.13 

-30.12 

-32.91 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

30 

20 

10 

0

10 

20 

30 

Figure 49. MLP Input-Output for token: 10 (sorted by logits) 

Output Token: 100 

67 Interpretable Algorithms by Decompiling Transformers 7

> ...
> 17
> ...
> 64
> ...
> 73
> ...
> 76
> ...
> 83
> ...
> 100
> 101
> ...
> 131
> ...
> 136
> ...
> 141
> ...
> 146

Input Features 

logit:100 

> 17.21
> 17.16
> 16.72
> 16.61
> 16.53
> 16.48
> 16.13
> 16.10
> 16.04
> 15.97
> 15.46
> 15.34
> 14.51
> 14.28
> 14.10
> 13.30
> 11.82
> 8.08
> -3.95
> -7.96
> -11.94
> -15.93
> -19.90
> -23.88
> -27.81
> -31.51
> -34.88
> -39.30

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

20 

0

20 

Figure 50. MLP Input-Output for token: 100 (sorted by logits) 

Output Token: EOS 4

> ...
> 6
> ...
> 14
> 15
> ...
> 22
> ...
> 26
> ...
> 30
> ...
> 59
> ...
> 66
> ...
> 109
> ...
> 111
> 112
> ...
> 115
> ...
> 123
> ...
> 129
> 130
> ...
> 147
> ...
> <bos>

Input Features 

> logit:<eos>
> 16.28
> 14.55
> 14.45
> 13.97
> 13.62
> 13.45
> 13.15
> 13.10
> 12.97
> 12.92
> 12.17
> 11.74
> 11.38
> 11.18
> 11.06
> 10.94
> 9.46
> 7.98
> 6.43
> 4.84
> 3.17
> 1.30
> -6.96
> -13.96
> -20.96
> -27.98
> -34.97
> -41.88
> -48.35
> -55.07
> -62.80
> -69.82

Output 

> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> 60
> 40
> 20
> 0
> 20
> 40
> 60

Figure 51. MLP Input-Output for token: EOS (sorted by logits). The operation generates a high logit for EOS when the input activation has a high entry for BOS. 

J.9. Unique Bigram Copy Task Description: 

⟨bos ⟩ s0, s 1, . . . , s n ⟨sep ⟩ s0, s 1, . . . , s n ⟨eos ⟩where si ∈ { 0, 1, . . . , 15 }, (si, s i+1 )̸ = ( sj , s j+1 ) iff i̸ = j

Architecture: Layers: 2 Heads: 4 Hidden Dim: 256 LR: 0.001 Dropout: 0.1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.95 → 0.92 ; Match Accuracy: 0.94 → 0.91 

Code 

1. s1 = select(q=pos, k=pos, op= a ) # layer 0 head 0 2. a1 = aggregate(s=s1, v=token) # layer 0 head 0 

68 Interpretable Algorithms by Decompiling Transformers 

3. s2 = select(q=pos, k=pos, op=(k==q-1)) # layer 0 head 2 4. a2 = aggregate(s=s2, v=token) # layer 0 head 2 5. token_x_a1_x_a2 = Cartesian_product(token, a1, a2) # layer 0 mlp 6. s3 = select(q=token_x_a1_x_a2, k=token_x_a1_x_a2, op= c ) # layer 1 head 27. a3 = aggregate(s=s3, v=token_x_a1_x_a2) # layer 1 head 2 8. logits1 = project(inp=a3, op= d ) 9. prediction = softmax(logits1) 

Interpretation This is an extension of the induction head program discussed in Main Paper Figure 3; it crucially involves a joint nonlinear representation of trigrams. Lines 1–4 retrieve the two preceding symbols; Line 5 creates a joint (nonlinear) representation of the trigram ending with the current symbol. In the original transformer, this had been provided by the MLP in the first layer. Each dimension in variable token x a1 x a2 corresponds to a combination of token t0-t−2-t−1.Line 6 then defines a selector where a query representing a trigram X-...-Y matches a key representing any trigram ...-Y-X (Figure 52c, e.g., (0-0/1/2-2, 0-2-0), (0-0/1/2-1, 0-1-0) have high values.). That is, matching is done based only on two tokens tq

0 tq

−1 of the query and tk

−1 tk

−2 of the key. Because bigrams are unique in the string, there will be a unique (if any) match. The resulting aggregate a3 now holds this key trigram. The current token of the key trigam tk

0 is forwarded to the output logit in line 8. 0 50 100 150 200 250 300 

> 0
> 50
> 100
> 150
> 200
> 250
> 300
> 4000
> 2000
> 0
> 2000
> 4000
> 6000

(a) Line 1: op= a in s1 0 50 100 150 200 250 300  

> 0
> 50
> 100
> 150
> 200
> 250
> 300
> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(b) Line 3: op=(k==q-1) 0-0-0 

> 0-0-1
> 0-0-2
> 0-0-3
> 0-0-4
> 0-0-5
> 0-0-6
> 0-0-7
> 0-0-8
> 0-0-9
> 0-0-10
> 0-0-11
> 0-0-12
> 0-0-13
> 0-0-14
> 0-0-15
> 0-0-<bos>
> 0-0-<sep>
> 0-0-<eos>
> 0-0-<pad>
> 0-1-0
> 0-1-1
> 0-1-2
> 0-1-3
> 0-1-4
> 0-1-5
> 0-1-6
> 0-1-7
> 0-1-8
> 0-1-9
> 0-1-10
> 0-1-11
> 0-1-12
> 0-1-13
> 0-1-14
> 0-1-15
> 0-1-<bos>
> 0-1-<sep>
> 0-1-<eos>
> 0-1-<pad>
> 0-2-0
> 0-2-1
> 0-2-2
> 0-2-3
> 0-2-4
> 0-2-5
> 0-2-6
> 0-2-7
> 0-2-8
> 0-2-9
> ...
> <pad>-<pad>-<pad>
> 0-0-0
> 0-0-1
> 0-0-2
> 0-0-3
> 0-0-4
> 0-0-5
> 0-0-6
> 0-0-7
> 0-0-8
> 0-0-9
> 0-0-10
> 0-0-11
> 0-0-12
> 0-0-13
> 0-0-14
> 0-0-15
> 0-0-<bos>
> 0-0-<sep>
> 0-0-<eos>
> 0-0-<pad>
> 0-1-0
> 0-1-1
> 0-1-2
> 0-1-3
> 0-1-4
> 0-1-5
> 0-1-6
> 0-1-7
> 0-1-8
> 0-1-9
> 0-1-10
> 0-1-11
> 0-1-12
> 0-1-13
> 0-1-14
> 0-1-15
> 0-1-<bos>
> 0-1-<sep>
> 0-1-<eos>
> 0-1-<pad>
> 0-2-0
> 0-2-1
> 0-2-2
> 0-2-3
> 0-2-4
> 0-2-5
> 0-2-6
> 0-2-7
> 0-2-8
> 0-2-9
> ...
> <pad>-<pad>-<pad>
> 100
> 50
> 0
> 50
> 100
> 150

(c) Line 6: op= c in s3 0

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>

0-0-0 

0-0-1 

0-0-2 

0-0-3 

0-0-4 

0-0-5 

0-0-6 

0-0-7 

0-0-8 

0-0-9 

0-0-10 

0-0-11 

0-0-12 

0-0-13 

0-0-14 

0-0-15 

0-0-<bos> 

0-0-<sep> 

0-0-<eos> 

0-0-<pad> 

0-1-0 

0-1-1 

0-1-2 

0-1-3 

0-1-4 

0-1-5 

0-1-6 

0-1-7 

0-1-8 

0-1-9 

... 

<pad>-<pad>-<pad> 

5.0 

2.5 

0.0 

2.5 

5.0 

7.5 

10.0 

12.5 (d) Line 8: op= d in logits1 

Figure 52. Heatmaps supporting the program for Unique Bigram Copy model. In (c) and (d), the input to the operation is a result of Cartesian product of three variables, all of which have dimension |Σ|, resulting in a total input size of |Σ| × | Σ| × | Σ|. We visualize only the top left part of the full heatmap for presentation. 

69 Interpretable Algorithms by Decompiling Transformers <bos> 

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>

<bos> 

6

0

9

12 

7

<sep> 

6

0

9

12 

7

<eos> 

> 2000
> 0
> 2000
> 4000

(a) Line 1: s1 <bos>  

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(b) Line 2: a1 <bos> 

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>

<bos> 

6

0

9

12 

7

<sep> 

6

0

9

12 

7

<eos>  

> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(c) Line 3: s2 <bos>  

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(d) Line 4: a2 <bos> 

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>

<bos> 

6

0

9

12 

7

<sep> 

6

0

9

12 

7

<eos> 

> 0
> 100

(e) Line 6: s3 <bos>  

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>
> ...
> 0-<bos>-6
> ...
> 0-<sep>-6
> ...
> 6-7-<sep>
> ...
> 6-<bos>-<bos>
> ...
> 7-9-12
> ...
> 9-6-0
> ...
> 12-0-9
> ...
> <bos>-<bos>-<bos>
> ...
> <sep>-12-7
> ...
> <eos>-12-7
> ...
> <pad>-<pad>-<pad>
> ...
> <pad>-<pad>-<pad>
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(f) Line 7: a3 <bos>  

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 2.5
> 0.0
> 2.5
> 5.0
> 7.5
> 10.0

(g) Line 8: logits1 

Figure 53. Variables Heatmaps for Unique Bigram Copy model on an example input. In (f), similarly to Figure 52, as a result of Cartesian product, the variables have dimension |Σ| × | Σ| × | Σ|. We hide the rows which have only zero values. 

J.10. Unique Bigram Copy : checkpoint at step 3300 Task Description: 

⟨bos ⟩ s0, s 1, . . . , s n ⟨sep ⟩ s0, s 1, . . . , s n ⟨eos ⟩where si ∈ { 0, 1, . . . , 15 }, (si, s i+1 )̸ = ( sj , s j+1 ) iff i̸ = j

Architecture: Layers: 2 Heads: 4 Hidden Dim: 256 LR: 0.001 Dropout: 0.1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.99 → 0.99 ; Match Accuracy: 0.99 → 0.99 

Code 

1. s1 = select(q=pos, k=pos, op= a ) # layer 0 head 1 2. a1 = aggregate(s=s1, v=token) # layer 0 head 1 3. s2 = select(q=pos, k=pos, op=(k==q-1)) # layer 0 head 2 4. a2 = aggregate(s=s2, v=token) # layer 0 head 2 5. s3 = select(q=token, k=a2, op=(k==q), special op=(k==BOS)) # layer 1 head 1 6. s4 = select(q=a2, k=a1, op= d ) # layer 1 head 1 7. a3 = aggregate(s=s3+s4, v=token) # layer 1 head 1 8. logits1 = project(inp=a3, op= e ) 9. prediction = softmax(logits1) 

Interpretation This is a different extension of the induction head program discussed in Main Paper, Figure 3; it relies only on select and aggregate operations. Similar to the previous Unique Bigram program, the model takes the previous previous token t−2 (s1 and a1 ), and the previous token t−1 (s2 and a2 ). But unlike previous preogram, they are not combined together with t0 (token ). They are used to form two separate selectors s3 (match tq

0 and tk

−1) and s4 (match 70 Interpretable Algorithms by Decompiling Transformers 

tq

−1 and tk

−2)). These two selectors are added together in line 7 such that the intersection would stand out (i.e., performing bigram matching). Only token is aggregated (which simpler than the previous program), so the model obtains the correct continuation of the bigram tk

0 , which is a3 . Line 8 outputs a3 , with special behavior to correctly predict EOS. In sum, we see a simpler program performed by this earlier checkpoint, compared to the previous Unique Bigram Copy model. This model has separate matching mechanisms and results are simply added. So we can see that in this case, continuous training after this checkpoint makes the inner mechanism more nonlinear and strengthens the role played by MLP layers. Even though the performance stay the same, the inner mechanism does not stop evolving. 0 50 100 150 200 250 300 

> 0
> 50
> 100
> 150
> 200
> 250
> 300
> 20000
> 10000
> 0
> 10000
> 20000

(a) Line 1: op= a in s1 0 50 100 150 200 250 300  

> 0
> 50
> 100
> 150
> 200
> 250
> 300
> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(b) Line 3: op=(k==q-1) 0

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(c) Line 5: op=(k==q), special op=(k==BOS) 0

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 500
> 400
> 300
> 200
> 100
> 0

(d) Line 6: op= d in s4 0

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 5
> 10
> 15

(e) Line 8: op= e in logits1 

Figure 54. Heatmaps supporting the program for Unique Bigram Copy : different checkpoints model. We note that replacement with a primitive was successful in (b,c), and not in (a,d,e). 

71 Interpretable Algorithms by Decompiling Transformers <bos> 

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>

<bos> 

6

0

9

12 

7

<sep> 

6

0

9

12 

7

<eos> 

> 10000
> 0
> 10000

(a) Line 1: s1 <bos>  

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(b) Line 2: a1 <bos> 

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>

<bos> 

6

0

9

12 

7

<sep> 

6

0

9

12 

7

<eos>  

> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(c) Line 3: s2 <bos>  

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(d) Line 4: a2 <bos> 

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>

<bos> 

6

0

9

12 

7

<sep> 

6

0

9

12 

7

<eos> 

> 0
> 2000
> 4000
> 6000
> 8000
> 10000

(e) Line 5: s3 <bos> 

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>

<bos> 

6

0

9

12 

7

<sep> 

6

0

9

12 

7

<eos>  

> 400
> 200
> 0

(f) Line 6: s4 <bos>  

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(g) Line 7: a3 <bos>  

> 6
> 0
> 9
> 12
> 7
> <sep>
> 6
> 0
> 9
> 12
> 7
> <eos>
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 5
> 10
> 15

(h) Line 8: logits1 

Figure 55. Variables Heatmaps for Unique Bigram Copy : different checkpoints model on an example input. 

J.11. Unique Copy Task Description: 

⟨bos ⟩ s0, s 1, . . . , s n ⟨sep ⟩ s0, s 1, . . . , s n ⟨eos ⟩where si ∈ { 0, 1, . . . , 150 }, s i̸ = sj iff i̸ = j

Architecture: Layers: 2 Heads: 1 Hidden Dim: 64 LR: 0.001 Dropout: 0.1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.93 → 1.00 ; Match Accuracy: 0.92 → 0.94 

Code 

1. s1 = select(q=pos, k=pos, op=(k==q-1)) # layer 0 head 0 2. a1 = aggregate(s=s1, v=token) # layer 0 head 0 3. s2 = select(q=token, k=a1, op=(k==q), special op=(k==BOS)) # layer 1 head 0 4. a2 = aggregate(s=s2, v=token) # layer 1 head 0 5. logits1 = project(inp=a2, op=(inp==out), special op=(uniform selection)) 6. logits2 = project(op= c ) 7. prediction = softmax(logits1+ logits2) 

Interpretation This is discussed in Main paper, Figure 3. 72 Interpretable Algorithms by Decompiling Transformers 0 50 100 150 200 250 300 

0

50 

100 

150 

200 

250 

300 

0

2000 

4000 

6000 

8000 

10000 (a) Line 1: op=(k==q-1) 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0

2000 

4000 

6000 

8000 

10000 

(b) Line 3: op=(k==q), spe-cial op=(k==BOS) 0

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> 25
> 26
> 27
> 28
> 29
> 30
> 31
> 32
> 33
> 34
> 35
> 36
> 37
> 38
> 39
> 40
> 41
> 42
> 43
> 44
> 45
> 46
> 47
> 48
> 49
> 50
> 51
> 52
> 53
> 54
> 55
> 56
> 57
> 58
> 59
> 60
> 61
> 62
> 63
> 64
> 65
> 66
> 67
> 68
> 69
> 70
> 71
> 72
> 73
> 74
> 75
> 76
> 77
> 78
> 79
> 80
> 81
> 82
> 83
> 84
> 85
> 86
> 87
> 88
> 89
> 90
> 91
> 92
> 93
> 94
> 95
> 96
> 97
> 98
> 99
> 100
> 101
> 102
> 103
> 104
> 105
> 106
> 107
> 108
> 109
> 110
> 111
> 112
> 113
> 114
> 115
> 116
> 117
> 118
> 119
> 120
> 121
> 122
> 123
> 124
> 125
> 126
> 127
> 128
> 129
> 130
> 131
> 132
> 133
> 134
> 135
> 136
> 137
> 138
> 139
> 140
> 141
> 142
> 143
> 144
> 145
> 146
> 147
> 148
> 149
> <bos>
> <sep>
> <eos>
> <pad>

2.5 

0.0 

2.5 

(c) Line 6: op= c in logits2 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0

2000 

4000 

6000 

8000 

10000 

(d) Line 5: op=(inp==out), special op=(uniform selection) 

Figure 56. Heatmaps supporting the program for Unique Copy model. 

73 Interpretable Algorithms by Decompiling Transformers <bos> 

> 1
> 3
> 4
> 2
> <sep>
> 1
> 3
> 4
> 2
> <eos>

<bos> 

1

3

4

2

<sep> 

1

3

4

2

<eos> 0

2000 

4000 

6000 

8000 

10000 (a) Line 1: s1 <bos> 

> 1
> 3
> 4
> 2
> <sep>
> 1
> 3
> 4
> 2
> <eos>

<bos> 

1

3

4

2

<sep> 

1

3

4

2

<eos> 0

2000 

4000 

6000 

8000 

10000 (b) Line 3: s2 <bos> 

> 1
> 3
> 4
> 2
> <sep>
> 1
> 3
> 4
> 2
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

(c) Line 2: a1 <bos> 

> 1
> 3
> 4
> 2
> <sep>
> 1
> 3
> 4
> 2
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (d) Line 4: a2 <bos> 1342

> <sep> 1342
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0

2000 

4000 

6000 

8000 

10000 (e) Line 5: logits1 0.0 2.5 5.0 7.5 10.0 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

3

2

1

0

1

2

3

4 (f) Line 6: logits2 

Figure 57. Variables Heatmaps for Unique Copy model on an example input. 

74 Interpretable Algorithms by Decompiling Transformers 

J.12. Unique Reverse Task Description: 

⟨bos ⟩ s0, s 1, . . . , s n ⟨sep ⟩ sn, s n−1, . . . , s 0 ⟨eos ⟩where si ∈ { 0, 1, . . . , 150 }, s i̸ = sj iff i̸ = j

Architecture: Layers: 2 Heads: 1 Hidden Dim: 64 LR: 0.001 Dropout: 0.1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.98 → 0.97 ; Match Accuracy: 0.96 → 0.95 

Code 

1. s1 = select(q=token, k=token, op= a ) # layer 0 head 0 2. s2 = select(q=pos, k=pos, op= b ) # layer 0 head 0 3. s3 = select(k=token, op= d ) 4. s4 = select(k=pos, op= e ) 5. a1 = aggregate(s=s1+s2+s3+s4, v=token) # layer 0 head 0 6. s5 = select(q=token, k=token, op=(k==q), special op=(k==SEP)) # layer 1 head 0 7. a2 = aggregate(s=s5, v=a1) # layer 1 head 0 8. logits1 = project(inp=a2, op= f ) 9. prediction = softmax(logits1) 

Interpretation Line 1 defines a selector that assigns increased weight to SEP, with strength varying with the query. (Figure 58a). Line 2 defines a selector assigning increased weight to the immediately preceding position (Figure 58b). Line 3 defines a selector assigning high weight to SEP. Line 4 defines a selector that tends to assign increased weight to later positions. These four selectors jointly come together in line 5, producing a1 . Inspecting a1 (Figure 59f) shows that (i) up until SEP, it collects the immediately preceding symbol; (ii) after SEP, it just holds SEP. Line 6 defines a selector assigning weight to occurrences of the current token, or (in the case of a special token), SEP. The resulting variable a2 holds the token that immediately precedes prior (pre-SEP) occurrences of the current token. Here, we understand why a1 just holds SEP in the post-SEP tokens: because the strings before and after SEP have opposite orders, this is needed to find the predecessor of the pre-SEP occurrence of the current token. This is additional complexity introduced by the fact that the model here copies 

backwards . This resulting token is then forwarded to output logits. By and large, the output logits are a “noisy” version of 

a2 (compare Figures 59 g and h), with one exception: at the last token, 75, a2 has retrieved BOS and SEP, which indicates that generation should stop, i.e., the next token should actually be EOS. The project operation converts the BOS/SEP entries into EOS entries. We note that the matrices in Figure 58 are similar to various primitives (e.g., (b) is similar to the identity matrix), but only (e) was successfully replaced. One possible reason is that the program has relatively specific behavior on special tokens, which was not captured by any of the primitives. 75 Interpretable Algorithms by Decompiling Transformers 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

200 

150 

100 

50 

0

50 (a) Line 1: op= a in s1 0 50 100 150 200 250 300 

0

50 

100 

150 

200 

250 

300 

100 

75 

50 

25 

0

25 

50 

75 

100 (b) Line 2: op= b in s2 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0

2000 

4000 

6000 

8000 

10000 (c) Line 6: op=(k==q), spe-cial op=(k==SEP) 0

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> 25
> 26
> 27
> 28
> 29
> 30
> 31
> 32
> 33
> 34
> 35
> 36
> 37
> 38
> 39
> 40
> 41
> 42
> 43
> 44
> 45
> 46
> 47
> 48
> 49
> 50
> 51
> 52
> 53
> 54
> 55
> 56
> 57
> 58
> 59
> 60
> 61
> 62
> 63
> 64
> 65
> 66
> 67
> 68
> 69
> 70
> 71
> 72
> 73
> 74
> 75
> 76
> 77
> 78
> 79
> 80
> 81
> 82
> 83
> 84
> 85
> 86
> 87
> 88
> 89
> 90
> 91
> 92
> 93
> 94
> 95
> 96
> 97
> 98
> 99
> 100
> 101
> 102
> 103
> 104
> 105
> 106
> 107
> 108
> 109
> 110
> 111
> 112
> 113
> 114
> 115
> 116
> 117
> 118
> 119
> 120
> 121
> 122
> 123
> 124
> 125
> 126
> 127
> 128
> 129
> 130
> 131
> 132
> 133
> 134
> 135
> 136
> 137
> 138
> 139
> 140
> 141
> 142
> 143
> 144
> 145
> 146
> 147
> 148
> 149
> <bos>
> <sep>
> <eos>
> <pad>

0

200 

(d) Line 3: op= d in s3 0 50 100 150 200 250 300 

50 

0 (e) Line 4: op= e in s4 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

15 

10 

5

0

5

10 

15 

20 

(f) Line 8: op= f in logits1 

Figure 58. Heatmaps supporting the program for Unique Reverse model. 

76 Interpretable Algorithms by Decompiling Transformers <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>

<bos> 

75 

73 

105 

135 

<sep> 

135 

105 

73 

75 

<eos> 

200 

100 

0(a) Line 1: s1 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>

<bos> 

75 

73 

105 

135 

<sep> 

135 

105 

73 

75 

<eos> 

50 

0

50 (b) Line 2: s2 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>
> 0.4
> 0.2
> 0.0
> 0.2
> 0.4

0

200 (c) Line 3: s3 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>
> 0.4
> 0.2
> 0.0
> 0.2
> 0.4

80 

60 (d) Line 4: s4 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>

<bos> 

75 

73 

105 

135 

<sep> 

135 

105 

73 

75 

<eos> 0

2000 

4000 

6000 

8000 

10000 

(e) Line 6: s5 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (f) Line 5: a1 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (g) Line 7: a2 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

10 

5

0

5

10 

15 (h) Line 8: logits1 

Figure 59. Variables Heatmaps for Unique Reverse model on an example input. 

77 Interpretable Algorithms by Decompiling Transformers 

J.13. Unique Reverse : different architecture with same length generalization performance Task Description: 

⟨bos ⟩ s0, s 1, . . . , s n ⟨sep ⟩ sn, s n−1, . . . , s 0 ⟨eos ⟩where si ∈ { 0, 1, . . . , 150 }, s i̸ = sj iff i̸ = j

Architecture: Layers: 4 Heads: 1 Hidden Dim: 256 LR: 0.0001 Dropout: 0.1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.98 → 0.99 ; Match Accuracy: 0.97 → 0.98 

Code 

1. s1 = select(q=pos, k=pos, op= a ) # layer 0 head 0 2. s2 = select(k=token, op= d ) 3. a1 = aggregate(s=s1+s2, v=token) # layer 0 head 0 4. s3 = select(q=token, k=token, op=(k==q), special op=(uniform selection)) # layer 2 head 0 5. s4 = select(q=token, k=a1, op= c ) # layer 2 head 0 6. a2 = aggregate(s=s3+s4, v=a1) # layer 2 head 0 7. new_a2 = element_wise_op(a2) # layer 3 mlp 8. logits1 = project(inp=new_a2, op=(inp==out)) 9. prediction = softmax(logits1) 

Interpretation Comparing the activations in Figure 61 with those of the other program on the same task (Figure 59) shows some commonalities, and indeed the algorithm is similar. s1 defines a selector that looks at the previous token in the input string, and s2 puts increased attention on SEP (Figure 60a,d). Aggregating information based on both these selectors, 

a1 then for each token holds the identity of the previous token, and for the tokens after SEP it also holds SEP. a2 copies the information from a1 into the residual stream of the same token later in the input, essentially retrieving the output. MLP in Line 7 sharpens a2 (Figures 62, 63, 64), and forces the model to generate EOS instead of BOS (Figure 65). 0 50 100 150 200 250 300 

> 0
> 50
> 100
> 150
> 200
> 250
> 300

40 

20 

0

20 

40 

60 

80 (a) Line 1: op= a in s1 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0

2000 

4000 

6000 

8000 

10000 (b) Line 4: op=(k==q), spe-cial op=(uniform selection) 012345678910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 <bos> <sep> <eos> <pad> 

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

1500 

1000 

500 

0

500 (c) Line 5: op= c in s4 0

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> 25
> 26
> 27
> 28
> 29
> 30
> 31
> 32
> 33
> 34
> 35
> 36
> 37
> 38
> 39
> 40
> 41
> 42
> 43
> 44
> 45
> 46
> 47
> 48
> 49
> 50
> 51
> 52
> 53
> 54
> 55
> 56
> 57
> 58
> 59
> 60
> 61
> 62
> 63
> 64
> 65
> 66
> 67
> 68
> 69
> 70
> 71
> 72
> 73
> 74
> 75
> 76
> 77
> 78
> 79
> 80
> 81
> 82
> 83
> 84
> 85
> 86
> 87
> 88
> 89
> 90
> 91
> 92
> 93
> 94
> 95
> 96
> 97
> 98
> 99
> 100
> 101
> 102
> 103
> 104
> 105
> 106
> 107
> 108
> 109
> 110
> 111
> 112
> 113
> 114
> 115
> 116
> 117
> 118
> 119
> 120
> 121
> 122
> 123
> 124
> 125
> 126
> 127
> 128
> 129
> 130
> 131
> 132
> 133
> 134
> 135
> 136
> 137
> 138
> 139
> 140
> 141
> 142
> 143
> 144
> 145
> 146
> 147
> 148
> 149
> <bos>
> <sep>
> <eos>
> <pad>

0

50 

(d) Line 2: op= d in s2 

Figure 60. Heatmaps supporting the program for Unique Reverse : different architecture with same length generalization performance model. 

78 Interpretable Algorithms by Decompiling Transformers <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>

<bos> 

75 

73 

105 

135 

<sep> 

135 

105 

73 

75 

<eos> 

20 

0

20 

40 (a) Line 1: s1 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>
> 0.4
> 0.2
> 0.0
> 0.2
> 0.4

0

50 (b) Line 2: s2 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>

<bos> 

75 

73 

105 

135 

<sep> 

135 

105 

73 

75 

<eos> 0

2000 

4000 

6000 

8000 

10000 (c) Line 4: s3 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>

<bos> 

75 

73 

105 

135 

<sep> 

135 

105 

73 

75 

<eos> 

1000 

500 

0

500 (d) Line 5: s4 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

(e) Line 3: a1 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (f) Line 6: a2 <bos> 

> 75
> 73
> 105
> 135
> <sep>
> 135
> 105
> 73
> 75
> <eos>

0

1

2

3

4

5

6

7

8

9

10 

11 

12 

13 

14 

15 

16 

17 

18 

19 

20 

21 

22 

23 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59 

60 

61 

62 

63 

64 

65 

66 

67 

68 

69 

70 

71 

72 

73 

74 

75 

76 

77 

78 

79 

80 

81 

82 

83 

84 

85 

86 

87 

88 

89 

90 

91 

92 

93 

94 

95 

96 

97 

98 

99 

100 

101 

102 

103 

104 

105 

106 

107 

108 

109 

110 

111 

112 

113 

114 

115 

116 

117 

118 

119 

120 

121 

122 

123 

124 

125 

126 

127 

128 

129 

130 

131 

132 

133 

134 

135 

136 

137 

138 

139 

140 

141 

142 

143 

144 

145 

146 

147 

148 

149 

<bos> 

<sep> 

<eos> 

<pad> 

10 

0

10 

20 

30 (g) Line 8: logits1 

Figure 61. Variables Heatmaps for Unique Reverse : different architecture with same length generalization performance model on an example input. 

MLP Input-Output Distributions Explaining per-position operation in Line 7 via its effect on Output Logits in Line 8 79 Interpretable Algorithms by Decompiling Transformers 

Output Token: 0 0

> ...
> 43
> ...
> 78
> 79
> ...
> 81
> ...
> <bos>
> <sep>

Input Features 

logit:0 

> 9.53
> 8.62
> 8.61
> 8.52
> 8.37
> 8.36
> 8.33
> 8.28
> 8.24
> 8.21
> 8.09
> 8.01
> 7.53
> 7.40
> 7.35
> 7.16
> 6.36
> 5.45
> -2.30
> -4.60
> -6.89
> -9.12
> -11.43
> -13.35
> -14.01
> -17.49

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

15 

10 

5

0

5

10 

15 

Figure 62. MLP Input-Output for token: 0 (sorted by logits) 

Output Token: 10 

80 Interpretable Algorithms by Decompiling Transformers 10 

> ...
> 47
> ...
> 64
> ...
> 104
> ...
> 107
> ...
> <bos>
> <sep>

Input Features 

logit:10 

> 7.53
> 7.04
> 6.69
> 6.58
> 6.51
> 6.32
> 6.30
> 6.29
> 6.18
> 6.12
> 6.11
> 6.00
> 5.97
> 5.95
> 5.91
> 5.83
> 5.20
> 3.80
> -2.38
> -4.76
> -7.14
> -9.51
> -11.89
> -14.24
> -16.56
> -18.09

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

15 

10 

5

0

5

10 

15 

Figure 63. MLP Input-Output for token: 10 (sorted by logits) 

Output Token: 100 

81 Interpretable Algorithms by Decompiling Transformers 15 

> ...
> 49
> ...
> 66
> ...
> 69
> ...
> 97
> ...
> 100
> ...
> 123
> ...
> <bos>

Input Features 

logit:100 

7.58 

7.37 

7.36 

7.29 

7.08 

7.07 

7.03 

6.93 

6.93 

6.91 

6.78 

6.69 

6.64 

6.59 

6.58 

6.04 

5.29 

4.53 

0.00 

-2.32 

-4.64 

-6.96 

-9.27 

-11.52 

-13.90 

-16.23 

-18.53 

-20.74 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

20 

10 

0

10 

20 

Figure 64. MLP Input-Output for token: 100 (sorted by logits) 

Output Token: EOS 13 

> ...
> 32
> ...
> 45
> ...
> 48
> ...
> 58
> ...
> 70
> ...
> 99
> ...
> 118
> ...
> 146
> ...
> <bos>

Input Features 

logit:<eos> 

> 39.97
> 39.79
> 39.47
> 39.00
> 38.79
> 38.75
> 38.69
> 38.69
> 38.63
> 38.53
> 35.93
> 35.87
> 35.82
> 35.80
> 35.74
> 31.95
> 20.74
> 0.00
> -1.31
> -2.63
> -3.94
> -5.26
> -6.57
> -7.88
> -9.17
> -10.49
> -11.57

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

20 

0

20 

Figure 65. MLP Input-Output for token: EOS (sorted by logits) 

82 Interpretable Algorithms by Decompiling Transformers 

K. Decompiled Programs on Formal Languages 

K.1. Dyck-12 Task Description: 

⟨bos ⟩ (a . . . (a(a

| {z }

> 12 times

b)∗b)∗ . . . b )∗

| {z }

> 12 times

⟨eos ⟩

Architecture: Layers: 4 Heads: 2 Hidden Dim: 256 LR: 0.001 Dropout: 0 

Performance (w/Pruning → w/Primitives): Task Accuracy: 0.92 → 1.00 ; Match Accuracy: 0.92 → 0.99 

Code 

1. a1 = aggregate(s=[], v=token) # layer 0 head 0 2. a2 = aggregate(s=[], v=pos) # layer 0 head 0 3. s1 = select(q=token, k=token, op= a ) # layer 0 head 1 4. s2 = select(q=pos, k=token, op= c ) # layer 0 head 1 5. s3 = select(k=token, op= b ) 6. a3 = aggregate(s=s1+s2+s3, v=token) # layer 0 head 1 7. a4 = aggregate(s=s1+s2+s3, v=pos) # layer 0 head 1 8. new_a1 = element_wise_op(a1) # layer 0 mlp 9. new_a3 = element_wise_op(a3) # layer 0 mlp 10. logits1 = project(inp=new_a1, op=(inp==out)) 11. logits2 = project(inp=a2, op= d ) 12. logits3 = project(inp=new_a3, op=(inp==out)) 13. logits4 = project(inp=a4, op= e ) 14. prediction = sigmoid(logits1+ logits2+ logits3+ logits4) 

Interpretation This program is a more complex version of the D4 program discussed in the main paper Figure 8. Similar to Figure 8, a1 aggregates all the tokens in the input string, and then feeds into the MLP in Line 8, which roughly checks the balance between ’ a’ and ’ b’ symbols. It promotes the ’ b’ only when the string is imbalanced (Figure 69), and EOS only when it is balanced (Figure 70). Some “imperfections” in this MLP are counteracted by the MLP in Line 9. For example, when on the input ’ ⟨bos ⟩a’ the MLP on Line 8 downweights the logit of ’ a’ (Figure 68), while the MLP on Line 9 gives it a much higher logit (Figure 71), mitigating the effect of MLP on Line 8. s1 , s2 , s3 act together to put attention on BOS token, while also aggregating information about ’ a”s and ’ b”s (Figure 66a,b,c). The MLP on Line 9 then encourages the model to output EOS if the value of BOS token after aggregation is low (Figure 73), which happens when the string is long. It additionally checks the balance of ’ a”s and ’ b”s, similarly to the MLP on Line 8. a4 and a2 perform aggregation of positional information, which then gets projected in Lines 11 and 13, acting similar to a bias term, prohibiting the model from promoting SEP, PAD or BOS (Figure 66d,e). 83 Interpretable Algorithms by Decompiling Transformers a

> b
> <bos>
> <sep>
> <eos>
> <pad>

a

b

<bos> 

<sep> 

<eos> 

<pad> 

> 2
> 0
> 2
> 4

(a) Line 3: op= a in s1 a

> b
> <bos>
> <sep>
> <eos>
> <pad>

0

2

(b) Line 5: op= b in s3 a

> b
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 20
> 40
> 60
> 80
> 100
> 120
> 140
> 0.5
> 0.0
> 0.5
> 1.0
> 1.5

(c) Line 4: op= c in s2 a

> b
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 20
> 40
> 60
> 80
> 100
> 120
> 140
> 100
> 80
> 60
> 40
> 20
> 0
> 20
> 40
> 60

(d) Line 11: op= d in logits2 a

> b
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 20
> 40
> 60
> 80
> 100
> 120
> 140
> 60
> 40
> 20
> 0
> 20
> 40

(e) Line 13: op= e in logits4 

Figure 66. Heatmaps supporting the program for Dyck-12 model. 

84 Interpretable Algorithms by Decompiling Transformers <bos> 

> a
> a
> a
> a
> a
> b
> b
> b
> b
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad> 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (a) Line 1: a1 <bos> 

> a
> a
> a
> a
> a
> b
> b
> b
> b
> b
> <eos>

<bos> 

a

a

a

a

a

b

b

b

b

b

<eos> 

2

0

2

4 (b) Line 3: s1 <bos> 

> a
> a
> a
> a
> a
> b
> b
> b
> b
> b
> <eos>

<bos> 

a

a

a

a

a

b

b

b

b

b

<eos> 

0.0 

0.5 

1.0 (c) Line 4: s2 <bos> 

> a
> a
> a
> a
> a
> b
> b
> b
> b
> b
> <eos>
> 0.4
> 0.2
> 0.0
> 0.2
> 0.4

1

0

1 (d) Line 5: s3 <bos> 

> a
> a
> a
> a
> a
> b
> b
> b
> b
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad> 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (e) Line 6: a3 <bos> 

> a
> a
> a
> a
> a
> b
> b
> b
> b
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad> 

200 

0

200 

(f) Line 10: logits1 <bos> 

> a
> a
> a
> a
> a
> b
> b
> b
> b
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad> 100 

50 

0

50 (g) Line 11: logits2 <bos> 

> a
> a
> a
> a
> a
> b
> b
> b
> b
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad> 

100 

0

100 

200 

(h) Line 12: log-its3 <bos> 

> a
> a
> a
> a
> a
> b
> b
> b
> b
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad> 30 

20 

10 

0

10 (i) Line 13: logits4 <bos> 

> a
> a
> a
> a
> a
> b
> b
> b
> b
> b
> <eos>

0

20 

40 

60 

80 

100 

120 

140 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

(j) Line 2: a2 <bos> 

> a
> a
> a
> a
> a
> b
> b
> b
> b
> b
> <eos>

0

20 

40 

60 

80 

100 

120 

140 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 (k) Line 7: a4 

Figure 67. Variables Heatmaps for Dyck-12 model on an example input. 

85 Interpretable Algorithms by Decompiling Transformers 

MLP Input-Output Distributions Explaining per-position operation in Line 8 via its effect on Output Logits in Line 10 

Output Token: a a

> b
> <bos>

0.92 0.00 0.08 

0.92 0.00 0.08 

0.91 0.00 0.09 

0.90 0.00 0.10 

0.87 0.07 0.07 

0.87 0.07 0.07 

0.89 0.00 0.11 

0.86 0.07 0.07 

0.86 0.07 0.07 

0.88 0.00 0.12 

0.85 0.08 0.08 

0.83 0.08 0.08 

0.81 0.12 0.06 

0.82 0.09 0.09 

0.80 0.13 0.07 

0.76 0.18 0.06 

0.58 0.41 0.02 

0.53 0.46 0.01 

0.52 0.47 0.01 

0.51 0.48 0.01 

0.50 0.48 0.02 

0.49 0.49 0.02 

0.50 0.00 0.50 

Input Features 

logit:a 

6.04 

5.90 

5.74 

5.54 

5.35 

5.35 

5.31 

5.19 

5.19 

5.02 

5.01 

4.80 

4.69 

4.57 

4.52 

4.20 

3.62 

3.00 

2.40 

1.80 

1.12 

0.44 

-26.99 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

20 

10 

0

10 

20 

Figure 68. MLP Input-Output for token: a (sorted by logits) 

Output Token: b a

> b
> <bos>

0.92 0.00 0.08 

0.92 0.00 0.08 

0.91 0.00 0.09 

0.87 0.07 0.07 

0.90 0.00 0.10 

0.86 0.07 0.07 

0.89 0.00 0.11 

0.85 0.08 0.08 

0.82 0.12 0.06 

0.82 0.12 0.06 

0.81 0.12 0.06 

0.88 0.00 0.12 

0.82 0.09 0.09 

0.76 0.19 0.05 

0.79 0.14 0.07 

0.76 0.18 0.06 

0.68 0.28 0.04 

0.64 0.32 0.04 

0.54 0.45 0.01 

0.50 0.48 0.02 

0.44 0.44 0.11 

Input Features 

logit:b 

391.73 

378.38 

362.60 

347.11 

343.67 

332.48 

320.53 

315.61 

313.01 

313.01 

298.10 

291.61 

272.69 

264.41 

261.91 

254.95 

192.37 

153.42 

78.22 

37.45 

-14.98 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

300 

200 

100 

0

100 

200 

300 

Figure 69. MLP Input-Output for token: b (sorted by logits) 

Output Token: EOS 

86 Interpretable Algorithms by Decompiling Transformers a

> b
> <bos>

0.00 0.00 1.00 

0.33 0.33 0.33 

0.50 0.00 0.50 

0.40 0.40 0.20 

0.40 0.40 0.20 

0.50 0.25 0.25 

0.43 0.43 0.14 

0.44 0.44 0.11 

0.50 0.33 0.17 

0.50 0.33 0.17 

0.45 0.45 0.09 

0.47 0.47 0.07 

0.50 0.48 0.02 

0.51 0.46 0.03 

0.50 0.48 0.02 

0.50 0.49 0.01 

0.58 0.41 0.02 

0.63 0.34 0.03 

0.70 0.26 0.04 

0.79 0.16 0.05 

0.87 0.07 0.07 

Input Features 

logit:<eos> 

277.58 

86.89 

73.78 

47.58 

47.58 

30.31 

29.96 

20.34 

16.14 

16.14 

14.85 

11.12 

-51.83 

-53.15 

-53.49 

-54.24 

-160.94 

-214.19 

-302.94 

-396.26 

-480.73 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

400 

200 

0

200 

400 

Figure 70. MLP Input-Output for token: EOS (sorted by logits) 

Explaining per-position operation in Line 9 via its effect on Output Logits in Line 12 

Output Token: a a

> b
> <bos>

0.00 0.00 1.00 

0.30 0.00 0.70 

0.30 0.00 0.70 

0.30 0.00 0.70 

0.30 0.00 0.70 

0.31 0.00 0.69 

0.31 0.00 0.69 

0.31 0.00 0.69 

0.31 0.00 0.69 

0.31 0.00 0.69 

0.31 0.00 0.69 

0.21 0.24 0.55 

0.22 0.24 0.54 

0.22 0.24 0.54 

0.22 0.24 0.54 

0.46 0.00 0.54 

0.57 0.00 0.43 

0.69 0.00 0.31 

0.75 0.00 0.25 

0.54 0.33 0.14 

0.48 0.50 0.02 

0.50 0.46 0.03 

0.61 0.31 0.08 

0.51 0.47 0.02 

0.51 0.47 0.02 

0.62 0.32 0.07 

0.52 0.46 0.02 

0.60 0.35 0.05 

0.69 0.22 0.09 

0.84 0.00 0.16 

Input Features 

logit:a 

265.84 

172.95 

172.65 

172.36 

172.10 

171.92 

171.79 

171.49 

171.12 

170.95 

170.61 

143.09 

141.34 

140.74 

140.25 

132.28 

106.32 

79.67 

53.17 

26.57 

-5.80 

-11.65 

-17.58 

-23.40 

-29.07 

-35.12 

-40.59 

-45.35 

-48.07 

-53.46 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

200 

100 

0

100 

200 

Figure 71. MLP Input-Output for token: a (sorted by logits) 

87 Interpretable Algorithms by Decompiling Transformers 

Output Token: b a

> b
> <bos>

0.32 0.00 0.68 

0.32 0.00 0.68 

0.32 0.00 0.68 

0.32 0.00 0.68 

0.32 0.00 0.68 

0.32 0.00 0.68 

0.33 0.00 0.67 

0.31 0.00 0.69 

0.31 0.00 0.69 

0.31 0.00 0.69 

0.46 0.00 0.54 

0.47 0.00 0.53 

0.47 0.00 0.53 

0.47 0.00 0.53 

0.47 0.00 0.53 

0.00 0.00 1.00 

0.57 0.00 0.43 

0.46 0.17 0.37 

0.69 0.00 0.31 

0.61 0.13 0.26 

0.60 0.19 0.21 

0.66 0.22 0.12 

0.51 0.39 0.10 

0.53 0.43 0.03 

0.50 0.46 0.04 

0.48 0.47 0.05 

0.52 0.46 0.02 

0.51 0.48 0.02 

0.50 0.48 0.02 

0.49 0.49 0.01 

0.47 0.50 0.03 

0.47 0.51 0.02 

0.47 0.52 0.02 

Input Features 

logit:b 

136.75 

136.75 

136.74 

136.74 

136.72 

136.70 

136.70 

136.69 

136.67 

136.66 

116.60 

116.05 

115.48 

115.03 

114.51 

100.82 

92.19 

77.40 

66.66 

53.84 

40.35 

26.70 

13.56 

-6.29 

-12.68 

-19.07 

-25.43 

-31.75 

-38.15 

-44.49 

-50.87 

-57.17 

-63.60 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

100 

50 

0

50 

100 

Figure 72. MLP Input-Output for token: b (sorted by logits) 

Output Token: EOS 

88 Interpretable Algorithms by Decompiling Transformers a                                                                  

> b
> <bos>
> 0.47 0.52 0.02
> 0.47 0.51 0.01
> 0.47 0.51 0.01
> 0.47 0.51 0.02
> 0.47 0.51 0.02
> 0.47 0.51 0.02
> 0.47 0.51 0.02
> 0.47 0.51 0.02
> 0.47 0.51 0.02
> 0.48 0.51 0.01
> 0.47 0.51 0.02
> 0.47 0.51 0.02
> 0.48 0.51 0.02
> 0.47 0.51 0.02
> 0.47 0.51 0.02
> 0.47 0.50 0.02
> 0.48 0.50 0.03
> 0.49 0.49 0.03
> 0.51 0.47 0.02
> 0.51 0.47 0.02
> 0.50 0.46 0.03
> 0.45 0.46 0.09
> 0.54 0.43 0.04
> 0.51 0.39 0.10
> 0.57 0.28 0.16
> 0.58 0.21 0.21
> 0.80 0.00 0.20
> 0.69 0.00 0.31
> 0.64 0.00 0.36
> 0.57 0.00 0.43
> 0.00 0.00 1.00
> 0.46 0.00 0.54
> 0.32 0.00 0.68

Input Features 

logit:<eos> 

> 87.37
> 86.98
> 86.65
> 85.21
> 84.97
> 84.76
> 84.40
> 84.06
> 84.00
> 83.14
> 78.63
> 78.47
> 77.99
> 77.86
> 77.79
> 69.89
> 61.16
> 52.40
> 43.63
> 34.84
> 26.17
> 17.46
> 8.73
> -18.52
> -37.40
> -55.66
> -74.19
> -90.71
> -106.37
> -126.66
> -142.61
> -161.17
> -189.88

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

150 

100 

50 

0

50 

100 

150 

Figure 73. MLP Input-Output for token: EOS (sorted by logits) 

K.2. Dyck-2 Task Description: 

⟨bos ⟩ (a(ab )∗b)∗ ⟨eos ⟩

Architecture: Layers: 1 Heads: 1 Hidden Dim: 16 LR: 0.001 Dropout: 0 

Performance (w/Pruning → w/Primitives): Task Accuracy: 1.00 → 1.00 ; Match Accuracy: 1.00 → 1.00 

Code 

1. s1 = select(q=token, k=token, op= a ) # layer 0 head 0 2. a1 = aggregate(s=s1, v=token) # layer 0 head 0 3. new_a1 = element_wise_op(a1) # layer 0 mlp 4. logits1 = project(inp=token, op= b ) 5. logits2 = project(inp=a1, op= c ) 6. logits3 = project(inp=token, op= d ) 7. logits4 = project(inp=new_a1, op=(inp==out)) 8. logits5 = project(op= e ) 9. prediction = sigmoid(logits1+ logits2+ logits3+ 

89 Interpretable Algorithms by Decompiling Transformers 

logits4+ logits5) 

Interpretation This program is a more complex version of the D4 program discussed in Figure 8 the main paper. Of note, the selector in Line 1 assigns unequal weights to “a” and “b” (Figures 74a and 75a). As a result, the output a1 does not simply record the relative counts of “a”, “b”, and “BOS’, but rather records differently weighted counts. The element-wise operation in line 3 performs a computation similar to that in D4, as shown by the samples below. The output logits are then composed by complementing the output of the elementwise operation (line 7) with information from the current token (lines 4 and 6), the weighted relative counts (line 5), and a bias favoring “a” over “b”/“EOS” (line 8). We note that the “unbalanced” weighting of “a” and “b” closely relates to findings from Wen et al. (2023), who found that transformers can recognize bounded-depth Dyck languages with “unbalanced” attention patterns, though such unbalanced-ness may eventually hurt length generalization on unboundedly long inputs. That said, our results here show that, at lengths 

≤ 150 , the program manages to deal with the unbalanced counts at perfect accuracy. a

> b
> <bos>
> <sep>
> <eos>
> <pad>

a

b

<bos> 

<sep> 

<eos> 

<pad> 

> 0.2
> 0.0
> 0.2

(a) Line 1: op= a in s1 a

> b
> <bos>
> <sep>
> <eos>
> <pad>

a

b

<bos> 

<sep> 

<eos> 

<pad> 100  

> 0
> 100

(b) Line 4: op= b in logits1 a

> b
> <bos>
> <sep>
> <eos>
> <pad>

a

b

<bos> 

<sep> 

<eos> 

<pad>  

> 100
> 0
> 100
> 200

(c) Line 5: op= c in logits2 a

> b
> <bos>
> <sep>
> <eos>
> <pad>

a

b

<bos> 

<sep> 

<eos> 

<pad> 

> 100
> 0
> 100

(d) Line 6: op= d in logits3 a

> b
> <bos>
> <sep>
> <eos>
> <pad>
> 5
> 0
> 5

(e) Line 8: op= e in logits5 

Figure 74. Heatmaps supporting the program for Dyck-2 model. 

90 Interpretable Algorithms by Decompiling Transformers <bos> 

> a
> a
> b
> a
> b
> b
> a
> b
> a
> b
> <eos>

<bos> 

a

a

b

a

b

b

a

b

a

b

<eos> 

> 0.2
> 0.0
> 0.2

(a) Line 1: s1 <bos> 

> a
> a
> b
> a
> b
> b
> a
> b
> a
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad> 0.0  

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(b) Line 2: a1 <bos> 

> a
> a
> b
> a
> b
> b
> a
> b
> a
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad> 100  

> 0
> 100

(c) Line 4: logits1 <bos> 

> a
> a
> b
> a
> b
> b
> a
> b
> a
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad> 

> 100
> 0
> 100

(d) Line 5: logits2 <bos> 

> a
> a
> b
> a
> b
> b
> a
> b
> a
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad>  

> 100
> 0
> 100

(e) Line 6: logits3 <bos> 

> a
> a
> b
> a
> b
> b
> a
> b
> a
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad>  

> 200
> 0
> 200

(f) Line 7: logits4 0 2 4 6 8 10 

a

b

<bos> 

<sep> 

<eos> 

<pad> 

> 5.0
> 2.5
> 0.0
> 2.5
> 5.0
> 7.5

(g) Line 8: logits5 

Figure 75. Variables Heatmaps for Dyck-2 model on an example input. 

MLP Input-Output Distributions Explaining per-position operation in Line 3 via its effect on Output Logits in Line 7 

Output Token: a 

91 Interpretable Algorithms by Decompiling Transformers a

> b
> <bos>

0.00 0.00 1.00 

0.34 0.33 0.32 

0.41 0.40 0.19 

0.41 0.40 0.19 

0.44 0.43 0.14 

0.44 0.43 0.14 

0.45 0.44 0.11 

0.45 0.44 0.11 

0.46 0.45 0.09 

0.46 0.45 0.09 

0.46 0.45 0.09 

0.49 0.48 0.03 

0.49 0.48 0.03 

0.52 0.45 0.03 

0.52 0.45 0.03 

0.53 0.40 0.06 

0.57 0.29 0.14 

Input Features 

logit:a 

61.23 

24.21 

-5.92 

-5.92 

-22.28 

-22.28 

-32.19 

-32.19 

-38.78 

-38.78 

-38.78 

-58.26 

-59.05 

-77.40 

-77.40 

-89.72 

-119.74 

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

100 

50 

0

50 

100 

Figure 76. MLP Input-Output for token: a (sorted by logits) 

Output Token: b a

> b
> <bos>

0.67 0.00 0.33 

0.60 0.20 0.19 

0.60 0.20 0.19 

0.57 0.29 0.14 

0.57 0.29 0.14 

0.56 0.34 0.11 

0.56 0.34 0.11 

0.54 0.37 0.09 

0.54 0.37 0.09 

0.54 0.37 0.09 

0.54 0.37 0.09 

0.54 0.39 0.07 

0.52 0.44 0.04 

0.52 0.44 0.04 

0.50 0.48 0.02 

0.50 0.48 0.02 

0.41 0.40 0.19 

Input Features 

logit:b 

260.06 

186.33 

186.33 

155.10 

155.10 

137.92 

137.92 

127.06 

127.06 

127.06 

127.06 

119.59 

101.86 

100.03 

77.73 

77.73 

10.09 

Output 

0.0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

200 

100 

0

100 

200 

Figure 77. MLP Input-Output for token: b (sorted by logits) 

Output Token: EOS 

92 Interpretable Algorithms by Decompiling Transformers a                                

> b
> <bos>
> 0.00 0.00 1.00
> 0.34 0.33 0.32
> 0.41 0.40 0.19
> 0.41 0.40 0.19
> 0.44 0.43 0.14
> 0.44 0.43 0.14
> 0.44 0.43 0.14
> 0.45 0.44 0.11
> 0.45 0.44 0.11
> 0.46 0.45 0.09
> 0.46 0.45 0.09
> 0.49 0.48 0.02
> 0.49 0.48 0.02
> 0.52 0.44 0.04
> 0.52 0.44 0.04
> 0.54 0.37 0.09

Input Features 

> logit:<eos>
> 132.34
> 42.07
> -15.70
> -15.70
> -46.24
> -46.24
> -46.24
> -64.60
> -64.60
> -76.76
> -76.76
> -119.23
> -119.99
> -155.12
> -157.94
> -196.80

Output 

> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> 150
> 100
> 50
> 0
> 50
> 100
> 150

Figure 78. MLP Input-Output for token: EOS (sorted by logits) 

K.3. Dyck-4 Task Description: 

⟨bos ⟩ (a(a(a(ab )∗b)∗b)∗b)∗ ⟨eos ⟩

Architecture: Layers: 1 Heads: 2 Hidden Dim: 256 LR: 0.0001 Dropout: 0 

Performance (w/Pruning → w/Primitives): Task Accuracy: 1.00 → 1.00 ; Match Accuracy: 1.00 → 0.99 

Code 

1. a1 = aggregate(s=[], v=token) # layer 0 head 0 2. new_a1 = element_wise_op(a1) # layer 0 mlp 3. logits1 = project(inp=new_a1, op=(inp==out)) 4. prediction = sigmoid(logits1) 

Interpretation This program is discussed in the main paper, Figure 8. We provide further examples of the elementwise operation below. <bos> 

> a
> a
> a
> a
> b
> b
> b
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad> 0.0 

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(a) Line 1: a1 <bos> 

> a
> a
> a
> a
> b
> b
> b
> b
> <eos>

a

b

<bos> 

<sep> 

<eos> 

<pad> 20  

> 0
> 20
> 40
> 60

(b) Line 3: logits1 

Figure 79. Variables Heatmaps for Dyck-4 model on an example input. (b) shows the logits1 generated by the elementwise operation: “a” is allowed as long as the depth doesn’t exceed 4; “EOS” is allowed at the beginning and end (as the string is balanced there); “b” is allowed except at the beginning and end (it is only possible when the string is unbalanced). 

MLP Input-Output Distributions Explaining per-position operation in Line 2 via its effect on Output Logits in Line 3 

Output Token: a 

93 Interpretable Algorithms by Decompiling Transformers a                                          

> b
> <bos>
> 0.00 0.00 1.00
> 0.33 0.33 0.33
> 0.50 0.00 0.50
> 0.67 0.00 0.33
> 0.40 0.40 0.20
> 0.50 0.33 0.17
> 0.50 0.33 0.17
> 0.57 0.29 0.14
> 0.57 0.29 0.14
> 0.50 0.38 0.12
> 0.50 0.38 0.12
> 0.51 0.48 0.01
> 0.51 0.48 0.01
> 0.51 0.48 0.01
> 0.51 0.48 0.01
> 0.51 0.48 0.01
> 0.52 0.47 0.01
> 0.52 0.46 0.01
> 0.53 0.45 0.02
> 0.57 0.38 0.05
> 0.64 0.27 0.09

Input Features 

logit:a 

> 270.38
> 83.41
> 52.32
> 51.20
> 46.00
> 26.93
> 26.93
> 25.79
> 25.79
> 23.75
> 23.75
> -0.70
> -0.74
> -0.79
> -0.90
> -0.95
> -1.85
> -2.83
> -3.74
> -5.56
> -6.28

Output 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

200 

100 

0

100 

200 

Figure 80. MLP Input-Output for token: a (sorted by logits). A condensed version (with just the output signs, and all three output dimensions together) is shown in Figure 8. We see that “a” receives a positive output logit if and only if #a - #b < 4 · #BOS, noting that #BOS = 1 by definition. The operation thus correctly enforces the bounded-depth constraint of D4. 

Output Token: b a                                        

> b
> <bos>
> 0.50 0.00 0.50
> 0.50 0.25 0.25
> 0.50 0.25 0.25
> 0.51 0.49 0.01
> 0.51 0.49 0.01
> 0.51 0.49 0.01
> 0.51 0.49 0.01
> 0.51 0.49 0.01
> 0.51 0.49 0.01
> 0.51 0.49 0.01
> 0.50 0.33 0.17
> 0.60 0.20 0.20
> 0.50 0.49 0.01
> 0.55 0.42 0.03
> 0.50 0.49 0.01
> 0.50 0.49 0.01
> 0.50 0.49 0.01
> 0.50 0.50 0.01
> 0.46 0.46 0.08
> 0.33 0.33 0.33

Input Features 

logit:b 

> 17.94
> 10.34
> 10.34
> 8.19
> 8.19
> 8.18
> 8.18
> 8.17
> 8.17
> 8.17
> 8.16
> 7.17
> 7.16
> 7.15
> 5.35
> 5.35
> 3.33
> -4.39
> -7.93
> -32.74

Output 

0.0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

30 

20 

10 

0

10 

20 

30 

Figure 81. MLP Input-Output for token: b (sorted by logits). A condensed version (with just the output signs, and all three output dimensions together) is shown in Figure 8. We see that “b” receives a positive output logit if and only if #a > #b. 

Output Token: EOS 

94 Interpretable Algorithms by Decompiling Transformers a                                        

> b
> <bos>
> 0.00 0.00 1.00
> 0.33 0.33 0.33
> 0.40 0.40 0.20
> 0.43 0.43 0.14
> 0.43 0.43 0.14
> 0.44 0.44 0.11
> 0.44 0.44 0.11
> 0.44 0.44 0.11
> 0.45 0.45 0.09
> 0.45 0.45 0.09
> 0.45 0.45 0.09
> 0.46 0.46 0.08
> 0.50 0.50 0.01
> 0.50 0.50 0.01
> 0.50 0.50 0.01
> 0.50 0.50 0.01
> 0.50 0.49 0.01
> 0.50 0.49 0.01
> 0.51 0.48 0.01
> 0.51 0.49 0.01

Input Features 

> logit:<eos>
> 246.57
> 79.60
> 45.75
> 31.41
> 31.41
> 23.48
> 23.48
> 23.48
> 18.36
> 18.36
> 18.36
> 14.93
> 6.44
> 6.33
> 6.26
> 6.19
> -3.98
> -5.95
> -10.39
> -10.92

Output 

> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> 200
> 100
> 0
> 100
> 200

Figure 82. MLP Input-Output for token: EOS (sorted by logits). A condensed version (with just the output signs, and all three output dimensions together) is shown in Figure 8. We see that “EOS” receives a positive output logit if and only if #a = #b, i.e., the operation enforces that any string in the language must be balanced. 

K.4. aastar Task Description: 

⟨bos ⟩ (aa )∗ ⟨eos ⟩

Architecture: Layers: 1 Heads: 2 Hidden Dim: 16 LR: 0.001 Dropout: 0.1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 1.00 → 1.00 ; Match Accuracy: 1.00 → 1.00 

Code 

1. s1 = select(k=token, op=(k==BOS)) 2. a1 = aggregate(s=s1, v=pos) # layer 0 head 1 3. m1 = element_wise_op(pos+a1) # layer 0 mlp 4. logits1 = project(inp=m1, op=(inp==out)) 5. prediction = sigmoid(logits1) 

Interpretation Lines 1–2 retrieve the position of BOS (note that, while the position is 0 in this example, this does not generally hold due to the use of random offsets in positional encoding during training). Line 3 now jointly processes the current position with the position of BOS in an elementwise operation. In this case, the elementwise operation was not replaced with a primitive. It directly feeds into the next-token prediction logits1 ; inspecting its values on a sample input (Figure 84b) show that “a” always receives a positive logit, whereas the logit for EOS alternates – which is correct behavior as the number of a’s in the input string in the language has to be even. This shows that the elementwise operation in Line 3 effectively checks the parity between the current position of BOS, and outputs a logit for EOS on this basis. 95 Interpretable Algorithms by Decompiling Transformers a

> <bos>
> <sep>
> <eos>
> <pad>

0

10000 

(a) Line 1: op=(k==BOS) 

Figure 83. Heatmaps supporting the program for aastar model. <bos>  

> a
> a
> a
> a
> <eos>
> 0.50
> 0.25
> 0.00
> 0.25
> 0.50 0
> 5000
> 10000

(a) Line 1: s1 <bos> 

> a
> a
> a
> a
> <eos>

a

<bos> 

<sep> 

<eos> 

<pad> 10  

> 5
> 0
> 5

(b) Line 4: logits1 <bos>  

> a
> a
> a
> a
> <eos>
> 0
> 20
> 40
> 60
> 80
> 100
> 120
> 140
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(c) Line 2: a1 

Figure 84. Variables Heatmaps for aastar model on an example input. 

96 Interpretable Algorithms by Decompiling Transformers 

K.5. abcde Task Description: 

⟨bos ⟩ aa ∗bb ∗cc ∗dd ∗ee ∗ ⟨eos ⟩

Architecture: Layers: 1 Heads: 1 Hidden Dim: 16 LR: 0.001 Dropout: 0 

Performance (w/Pruning → w/Primitives): Task Accuracy: 1.00 → 1.00 ; Match Accuracy: 1.00 → 1.00 

Code 

1. logits1 = project(inp=token, op= a ) 2. prediction = sigmoid(logits1) 

Interpretation This program is discussed in the main paper. a

> b
> c
> d
> e
> <bos>
> <sep>
> <eos>
> <pad>

a

b

c

d

e

<bos> 

<sep> 

<eos> 

<pad> 

> 10
> 5
> 0
> 5

(a) Line 1: op= a in logits1 

Figure 85. Heatmaps supporting the program for abcde model. <bos> 

> a
> b
> c
> d
> d
> d
> d
> e
> <eos>
> a
> b
> c
> d
> e
> <bos>
> <sep>
> <eos>
> <pad>
> 10
> 5
> 0
> 5

(a) Line 1: logits1 

Figure 86. Variables Heatmaps for abcde model on an example input. 

K.6. ab d bc Task Description: 

⟨bos ⟩ { a, b }∗d{b, c }∗ ⟨eos ⟩

Architecture: Layers: 1 Heads: 1 Hidden Dim: 16 LR: 0.001 Dropout: 0.1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 1.00 → 1.00 ; Match Accuracy: 1.00 → 1.00 

Code 

1. s1 = select(k=token, op= b ) 2. a1 = aggregate(s=s1, v=token) # layer 0 head 0 

97 Interpretable Algorithms by Decompiling Transformers 

3. logits1 = project(inp=a1, op= a ) 4. logits2 = project(op= c ) 5. prediction = sigmoid(logits1+ logits2) 

Interpretation Essentially, the model needs to check if “d” has occurred so far: if it hasn’t, {a, b, d } are valid; if it has, 

{b, c, EOS } are valid. Indeed, lines 1–2 encode this information by assigning weight to “d” if it has occurred. Line 3 translates this information into the next-word expectations described above: e.g., the row for BOS assigns positive weight to a, b, d and negative weight to c, EOS; whereas the row for “d” assigns positive weight to b, c, EOS. Line 4 complements this by assigning negative weight to BOS, SEP, PAD. a

> b
> c
> d
> <bos>
> <sep>
> <eos>
> <pad>

a

b

c

d

<bos> 

<sep> 

<eos> 

<pad> 10 

> 5
> 0
> 5
> 10

(a) Line 3: op= a in logits1 a

> b
> c
> d
> <bos>
> <sep>
> <eos>
> <pad>
> 0
> 50

(b) Line 1: op= b in s1 a 

> b
> c
> d
> <bos>
> <sep>
> <eos>
> <pad>
> 2.5
> 0.0
> 2.5

(c) Line 4: op= c in logits2 

Figure 87. Heatmaps supporting the program for ab d bc model. 

98 Interpretable Algorithms by Decompiling Transformers <bos> 

> a
> a
> b
> a
> d
> b
> c
> <eos>
> 0.50
> 0.25
> 0.00
> 0.25
> 0.50
> 0
> 50

(a) Line 1: s1 <bos> 

> a
> a
> b
> a
> d
> b
> c
> <eos>

a

b

c

d

<bos> 

<sep> 

<eos> 

<pad> 0.0  

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(b) Line 2: a1 <bos> 

> a
> a
> b
> a
> d
> b
> c
> <eos>

a

b

c

d

<bos> 

<sep> 

<eos> 

<pad>  

> 5
> 0
> 5
> 10

(c) Line 3: logits1 0 2 4 6 8 

> a
> b
> c
> d
> <bos>
> <sep>
> <eos>
> <pad> 3
> 2
> 1
> 0
> 1
> 2
> 3

(d) Line 4: logits2 

Figure 88. Variables Heatmaps for ab d bc model on an example input. 

K.7. tomita1 Task Description: 

⟨bos ⟩ 1∗ ⟨eos ⟩

Architecture: Layers: 1 Heads: 1 Hidden Dim: 16 LR: 0.001 Dropout: 0 

Performance (w/Pruning → w/Primitives): Task Accuracy: 1.0 → 1.0; Match Accuracy: 1.0 → 1.0

Code 

1. prediction = sigmoid(bias) 

K.8. tomita2 Task Description: 

⟨bos ⟩ (10) ∗ ⟨eos ⟩

Architecture: Layers: 1 Heads: 1 Hidden Dim: 16 LR: 0.001 Dropout: 0 

Performance (w/Pruning → w/Primitives): Task Accuracy: 1.00 → 1.00 ; Match Accuracy: 1.00 → 1.00 

Code 

1. logits1 = project(inp=token, op= a ) 2. prediction = sigmoid(logits1) 

Interpretation This program straightforwardly describes next-token expectations in terms of the current token: BOS is followed by 1 or EOS; 1 is followed by 0; 0 is followed by 1 or EOS. 99 Interpretable Algorithms by Decompiling Transformers 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

1

<bos> 

<sep> 

<eos> 

<pad> 

> 5
> 0
> 5

(a) Line 1: op= a in logits1 

Figure 89. Heatmaps supporting the program for tomita2 model. <bos> 

> 1
> 0
> 1
> 0
> <eos>
> 0
> 1
> <bos>
> <sep>
> <eos>
> <pad>
> 5
> 0
> 5

(a) Line 1: logits1 

Figure 90. Variables Heatmaps for tomita2 model on an example input. 

K.9. tomita7 Task Description: 

⟨bos ⟩ 0∗1∗0∗1∗ ⟨eos ⟩

Architecture: Layers: 2 Heads: 1 Hidden Dim: 16 LR: 0.001 Dropout: 0.1 

Performance (w/Pruning → w/Primitives): Task Accuracy: 1.00 → 0.98 ; Match Accuracy: 1.00 → 0.98 

Code 

1. s1 = select(q=token, k=token, op= a ) # layer 0 head 0 2. a1 = aggregate(s=s1, v=token) # layer 0 head 0 3. s2 = select(q=token, k=a1, op=(k==q), special op=(uniform selection)) # layer 1 head 0 4. a2 = aggregate(s=s2, v=a1) # layer 1 head 0 5. logits1 = project(inp=a2, op= c ) 6. logits2 = project(op= d ) 7. prediction = sigmoid(logits1+ logits2) 

Interpretation The model essentially prohibits generating a 0 if a 1∗0∗1∗ subsequence is detected in Lines 1-4. In s1 

(Figure 91a) q=0 pays attention to 1’s and q=1 to 0’s, and thus a1 (Figure 92b) holds the count of 0’s (for q=1) and 1’s (for q=0) before the current token. Then, in Line 4 a1 is aggregated to form a2 (Figure 92d), which for q=0 holds the count of 0’s preceding 1’s preceding current token. Essentially, it detects whether the current symbol is one of the trailing 0’s in a substring 0∗1∗0∗. Similarly, for q=1, a2 detects the substring 1∗0∗1∗. In Line 5, the model severely downweights logit 1 if 

1∗0∗1∗ substring was detected (Figure 91c). By default, the model allows output of all the tokens among 0, 1 and ⟨eos ⟩, as shown in high logits for bias and projection matrix in line 5 (Figure 91c,d). 100 Interpretable Algorithms by Decompiling Transformers 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

1

<bos> 

<sep> 

<eos> 

<pad> 

> 50
> 0
> 50

(a) Line 1: op= a in s1 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

1

<bos> 

<sep> 

<eos> 

<pad> 0 

> 2000
> 4000
> 6000
> 8000
> 10000

(b) Line 3: op=(k==q), spe-cial op=(uniform selection) 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

0

1

<bos> 

<sep> 

<eos> 

<pad>  

> 4
> 2
> 0
> 2
> 4

(c) Line 5: op= c in logits1 0

> 1
> <bos>
> <sep>
> <eos>
> <pad>

5

0

5

(d) Line 6: op= d in logits2 

Figure 91. Heatmaps supporting the program for tomita7 model. <bos> 

> 0
> 0
> 1
> 0
> 0
> 0
> 1
> 1
> <eos>

<bos> 

0

0

1

0

0

0

1

1

<eos> 

> 50
> 0
> 50

(a) Line 1: s1 <bos> 

> 0
> 0
> 1
> 0
> 0
> 0
> 1
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 0.0  

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(b) Line 2: a1 <bos> 

> 0
> 0
> 1
> 0
> 0
> 0
> 1
> 1
> <eos>

<bos> 

0

0

1

0

0

0

1

1

<eos> 0 

> 2000
> 4000
> 6000
> 8000
> 10000

(c) Line 3: s2 <bos> 

> 0
> 0
> 1
> 0
> 0
> 0
> 1
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad> 0.0 

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

(d) Line 4: a2 <bos> 

> 0
> 0
> 1
> 0
> 0
> 0
> 1
> 1
> <eos>

0

1

<bos> 

<sep> 

<eos> 

<pad>  

> 4
> 2
> 0
> 2
> 4

(e) Line 5: logits1 0 2 4 6 8

0

1

<bos> 

<sep> 

<eos> 

<pad>  

> 5
> 0
> 5

(f) Line 6: logits2 

Figure 92. Variables Heatmaps for tomita7 model on an example input. 

101