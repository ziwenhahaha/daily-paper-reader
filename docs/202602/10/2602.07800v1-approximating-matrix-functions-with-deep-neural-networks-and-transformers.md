---
title: Approximating Matrix Functions with Deep Neural Networks and Transformers
title_zh: 利用深度神经网络和 Transformer 逼近矩阵函数
authors: "Rahul Padmanabhan, Simone Brugiapaglia"
date: 2026-02-08
pdf: "https://arxiv.org/pdf/2602.07800v1"
tags: ["keyword:SR", "query:SR"]
score: 6.0
evidence: 用于数值计算和科学计算的 Transformer
tldr: "本研究探讨了利用深度神经网络（特别是Transformer）近似矩阵函数（如矩阵指数和矩阵符号函数）的能力。研究首先在理论上证明了ReLU网络近似矩阵指数所需的宽度和深度界限；随后通过实验展示了采用合适数值编码的Transformer编解码器能以较高概率实现5%以内的相对误差。研究强调了编码方案对性能的关键影响，为数值计算领域的深度学习应用提供了新见解。"
motivation: 尽管Transformer在自然语言处理领域取得巨大成功，但其在处理科学计算中常见的矩阵函数近似方面的潜力尚未得到充分探索。
method: 结合理论分析证明ReLU网络复杂度界限，并实验研究了采用不同数值编码方案的Transformer编解码器架构的近似能力。
result: "实验表明Transformer能以5%的相对误差近似特定矩阵函数，且其性能表现高度依赖于所选的数值编码方案。"
conclusion: 深度神经网络特别是Transformer在数值矩阵计算中具有显著潜力，但针对不同函数选择匹配的编码方案是提升性能的关键。
---

## 摘要
Transformer 彻底改变了自然语言处理领域，但其在数值计算中的应用受到的关注较少。我们研究了使用包括 Transformer 在内的神经网络来逼近矩阵函数（将标量函数映射到矩阵）。我们专注于将方阵映射到同维方阵的函数。这类矩阵函数在科学计算中广泛存在，例如连续时间马尔可夫链中的矩阵指数，以及动力系统稳定性分析中的矩阵符号函数。在本文中，我们做出了两项贡献。首先，我们证明了以任意精度逼近矩阵指数所需的 ReLU 网络宽度和深度的界限。其次，我们通过实验表明，采用合适数值编码的 Transformer 编码器-解码器能够以高概率在 5% 的相对误差内逼近某些矩阵函数。我们的研究揭示了编码方案对性能有显著影响，且不同的方案在处理不同函数时表现各异。

## Abstract
Transformers have revolutionized natural language processing, but their use for numerical computation has received less attention. We study the approximation of matrix functions, which map scalar functions to matrices, using neural networks including transformers. We focus on functions mapping square matrices to square matrices of the same dimension. These types of matrix functions appear throughout scientific computing, e.g., the matrix exponential in continuous-time Markov chains and the matrix sign function in stability analysis of dynamical systems. In this paper, we make two contributions. First, we prove bounds on the width and depth of ReLU networks needed to approximate the matrix exponential to an arbitrary precision. Second, we show experimentally that a transformer encoder-decoder with suitable numerical encodings can approximate certain matrix functions at a relative error of 5% with high probability. Our study reveals that the encoding scheme strongly affects performance, with different schemes working better for different functions.