Title: Approximating Matrix Functions with Deep Neural Networks and Transformers

URL Source: https://arxiv.org/pdf/2602.07800v1

Published Time: Tue, 10 Feb 2026 02:01:41 GMT

Number of Pages: 10

Markdown Content:
# Approximating Matrix Functions with Deep Neural Networks and Transformers 

Rahul Padmanabhan 1[0009 ‚àí0005 ‚àí8660 ‚àí6234] and Simone Brugiapaglia 1[0000 ‚àí0003 ‚àí1927 ‚àí8232] 

Department of Mathematics and Statistics, Concordia University, 1455 Blvd. De Maisonneuve Ouest, Montreal, Quebec H3G 1M8, Canada 

{rahul.padmanabhan,simone.brugiapaglia}@concordia.ca 

Abstract. Transformers have revolutionized natural language process-ing, but their use for numerical computation has received less attention. We study the approximation of matrix functions, which map scalar func-tions to matrices, using neural networks including transformers. We focus on functions mapping square matrices to square matrices of the same dimension. These types of matrix functions appear throughout scien-tific computing, e.g., the matrix exponential in continuous-time Markov chains and the matrix sign function in stability analysis of dynamical sys-tems. In this paper, we make two contributions. First, we prove bounds on the width and depth of ReLU networks needed to approximate the matrix exponential to an arbitrary precision. Second, we show exper-imentally that a transformer encoder-decoder with suitable numerical encodings can approximate certain matrix functions at a relative error of 5% with high probability. Our study reveals that the encoding scheme strongly affects performance, with different schemes working better for different functions. 

Keywords: Transformers ¬∑ Matrix Functions ¬∑ Deep Neural Networks ¬∑

Function Approximation ¬∑ Matrix Exponential ¬∑ Matrix Sign Function. 

## 1 Introduction 

Matrix functions appear throughout scientific computing. The matrix exponen-tial gives transition probabilities in continuous-time Markov chains [15], the ma-trix sign function solves algebraic Riccati equations in control theory [14], and the matrix sine and cosine arise in second-order differential equations [3]. Given 

A ‚àà Cn√ón, a matrix function f (A) produces another n √ó n matrix. These func-tions do not correspond to element-wise operations, but they respect the spectral structure of A. Standard algorithms for computing these functions can be expen-sive, (especially for large matrices), which motivates the search for fast surrogate models. Neural networks are natural candidates for such surrogates. Deep learning has proven effective in vision [20] and language [12, 21], and recent work has begun applying neural networks to mathematical tasks. Transformers [19], which under-lie models like Llama [18], ChatGPT [1], Claude [6], Grok [22], and DeepSeek [8],  

> arXiv:2602.07800v1 [cs.LG] 8 Feb 2026 2R. Padmanabhan and S. Brugiapaglia

have shown promise for symbolic integration [11], linear algebra [7], and finding Lyapunov functions [5]. But matrix functions present a different challenge: the output depends on the eigenstructure of the input, and the mapping from en-tries to entries is highly nonlinear. Can neural networks learn these mappings accurately? We study this question for five matrix functions (exponential, logarithm, sign, sine, and cosine) using both feedforward networks and transformers. Our experiments show that standard deep networks struggle beyond 1 √ó 1 matrices, but transformer encoder-decoders with appropriate numerical encodings can ap-proximate certain functions reasonably well. 

Summary of Contributions. The contribution of this paper is twofold. From a theoretical perspective we prove that a ReLU feedforward network can ap-proximate eA to precision œµ, with width exponential in nM and depth linear in 

nM (Theorem 1). Moreover, numerically, we show that a transformer encoder-decoder with numerical encodings can approximate the matrix sign function with 88.41% accuracy for 3 √ó 3 matrices at 1% tolerance. 

Organization. We start with Section 2, where we provide some preliminaries on matrix functions, transformers, and the encoding scheme used in the experi-ments. In Section 3, we prove the theoretical result on the width and depth of a ReLU feedforward network to approximate the matrix exponential. In Section 4, we present the numerical results on the transformer encoder-decoder with nu-merical encodings to approximate the matrix sign function. Finally, we conclude in Section 5 with a discussion of the results and future directions. 

## 2 Preliminaries 

This section provides some preliminaries on matrix functions, transformers, and the encoding scheme used in the experiments. 

Matrix Functions. Matrix functions extend scalar functions to matrices while preserving spectral properties. Higham [9] is a comprehensive reference for ma-trix functions. We use the Jordan canonical form definition as, while theoretical, it is a simple and intuitive definition that can be used to understand the behavior of matrix functions. 

Definition 1 (Matrix Function via Jordan Canonical Form). Let f be de-fined on the spectrum of A ‚àà Cn√ón and let A = ZJZ ‚àí1 where J = diag( J1, . . . , J p)

is the Jordan form. Then f (A) := Z diag( f (J1), . . . , f (Jp)) Z‚àí1, where, for a Jordan block Jk with eigenvalue Œªk of size mk, we define 

f (Jk) := 

Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞

f (Œªk) f ‚Ä≤(Œªk) ¬∑ ¬∑ ¬∑ f (mk ‚àí1) (Œªk )(mk ‚àí1)! 

f (Œªk) . . . .... . . f ‚Ä≤(Œªk)

f (Œªk)

Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª

.Matrix Functions with DNNs and Transformers 3

Encoding 3.14 ‚àí6.02 √ó 10 23 Tokens/coef. Vocab size 

P10 [+ , 3, 1, 4, E-2 ] [ ‚àí, 6, 0, 2, E21 ] 5 210 P1000 [+ , 314 , E-2 ] [‚àí, 602 , E21 ] 3 1100 B1999 [314 , E-2 ] [‚àí602 , E21 ] 2 2000 FP15 [FP314/-2 ] [FP-602/21 ] 1 30000 

Table 1: Encoding schemes for floating-point numbers [7]. One of the assumptions made in Definition 1 is that the function f is mk ‚àí 1

times differentiable at the eigenvalue Œªk. We will now briefly describe the five matrix functions used in the experiments: 

‚Äì Matrix Exponential. For A ‚àà Cn√ón, we have eA = P‚àû  

> k=0 Ak
> k!

. This solves 

Àôx = Ax with x(t) = eAt x(0) .

‚Äì Matrix Logarithm. For A ‚àà Cn√ón with no eigenvalues on R‚àí, the principal logarithm satisfies elog( A) = A. When ‚à•A ‚àí I‚à• < 1, it can be computed via 

log( A) = P‚àû

> k=1

(‚àí1) k+1 ( A‚àíI)k 

> k

. This is used in robotics in the context of interpolation of motion [4]. 

‚Äì Matrix Sign. For A ‚àà Cn√ón with no purely imaginary eigenvalues, write 

A = Z diag( J1, J 2)Z‚àí1 where eigenvalues of J1 have negative real parts and those of J2 positive real parts. Then sign( A) = Z diag( ‚àíI, I )Z‚àí1. This is a key function used in solving algebraic Riccati equations [14] and Lyapunov equations [9]. 

‚Äì Matrix Sine and Cosine. For A ‚àà Cn√ón, we have sin( A) = P‚àû

> k=0

(‚àí1) k A2k+1 

> (2 k+1)!

and cos( A) = P‚àû

> k=0

(‚àí1) k A 2k 

> (2 k)!

. These are used in second-order differential equations [3]. 

Transformer Architecture. Transformers [19] process sequences via self-attention. The key idea is that each position in the sequence can attend to all other posi-tions, with learned weights that capture which inputs are relevant for producing each output. For input X ‚àà Rn√ód, Attention( Q, K, V ) = softmax 

 QK T

> ‚àödk



V ,where Q = XW Q, K = XW K , V = XW V are linear projections of the in-put. Here Q, K, V are the queries, keys, and values: the query at each posi-tion is compared against all keys to determine how much weight to give each value. Multi-head attention runs h parallel operations: MultiHead( Q, K, V ) = Concat( head 1, . . . , head h)WO . The matrices WQ, W K , W V , W O are learned pa-rameters. 

Encoding Schemes for Numerical Data. Following Charton [7], we represent floating-point numbers in the sign, mantissa, and exponent form as x ‚âà s¬∑m¬∑10 e

where s ‚àà {‚àí 1, 1}, m ‚àà { 100 , . . . , 999 }, e ‚àà Z. Table 1 shows four schemes. 4 R. Padmanabhan and S. Brugiapaglia 

## 3 Theoretical Results: DNN Bounds for Matrix Exponential 

We bound the architecture of ReLU Deep Neural Networks (DNNs) for approx-imating the matrix exponential. The proof uses the Taylor expansion and the following lemma from Adcock et al. [2] (based on arguments from Schwab et al. [16]). 

Lemma 1 (Approximate Multiplication by ReLU DNNs [2, Lemma 7.1]). Let 0 < Œ¥ < 1, l ‚àà N, and M = Qli=1 Mi ‚â• 1. There exists a ReLU DNN 

œá(l) 

> Œ¥

with 

sup 

> |xi|‚â§ Mi
> l

Y

> i=1

xi ‚àí œá(l) 

> Œ¥

(x) ‚â§ Œ¥, 

width ‚â§ c1 ¬∑ l, and depth ‚â§ c2(1 + log( l)[log( lŒ¥ ‚àí1) + log( M )]) , where c1, c 2 > 0

are universal constants. 

Lemma 2 follows from repeated application of the definition of matrix multi-plication: each entry (Ak)ij is a sum over all index paths from i to j of length k.This can be proved by a standard induction argument, see Padmanabhan [13]. 

Lemma 2 (Matrix Power Representation). For A ‚àà Cn√ón and k ‚àà N,the entries of Ak satisfy: (Ak)ij = nP

> ‚Ñì1=1

¬∑ ¬∑ ¬∑ nP

> ‚Ñìk‚àí1=1
> k

Q

> q=1

a‚Ñìq‚àí1‚Ñìq , where ‚Ñì0 = i and 

‚Ñìk = j.

We now state our main theoretical result, which gives explicit bounds on the width and depth of a ReLU network that approximates the matrix exponential to arbitrary precision œµ. The width grows like K ¬∑ nK ‚âà M n ¬∑ nnM , exponential in nM . The depth grows as K ln( K) times logarithmic factors, roughly linear in 

M n . The complete proof can be found in Padmanabhan [13]. 

Theorem 1 (DNN Architecture for Matrix Exponential). Let n ‚àà N and 

M ‚â• 1. For any œµ > 0, there exists a ReLU network fœµ with 

sup  

> A‚àà[‚àíM,M ]n√ón

‚à•fœµ(A) ‚àí eA‚à•F ‚â§ œµ, 

and bounds 

width( fœµ) ‚â§ C1 ¬∑ K ¬∑ nK ,

depth( fœµ) ‚â§ C2

1 + ln( K)  ln( K) + ln   2eœµ

 + K(ln( n) + ln( M ))  ,

where K =



max 



enM, nM +ln( ‚àö2‚àöœÄœµ )ln(2) ‚àí 1

 

, ‚à• ¬∑ ‚à• F denotes the Frobenius norm, and C1, C 2 > 0 are universal constants. Matrix Functions with DNNs and Transformers 5

Proof (Sketch). Here we provide a sketch of the proof. 

Step 1: Taylor truncation. Write eA = KP 

> k=0
> Ak
> k!

+ RK (A) where 

‚à•RK (A)‚à•F ‚â§ 1

‚àö2œÄ

 12

K+1 

enM 

for K ‚â• 2enM ‚àí 1.

Step 2: Remainder bound. Requiring ‚à•RK (A)‚à•F ‚â§ œµ/ 2 gives the condition 

K ‚â• max 

(

enM, nM +ln   

> 1‚àö2œÄœµ/ 2
> 
> ln(2)

‚àí 1

)

.

Step 3: Network construction. By Lemma 2, each (Ak)ij is a sum of nk‚àí1

products of k terms. Using Lemma 1, we build networks P(k) approximating 

Ak. The network Œ¶(A) = KP 

> j=0 1
> j!

P(j)(A) satisfies ‚à•Œ¶(A) ‚àí KP 

> j=0
> Aj
> j!

‚à•F ‚â§ Œ¥e n with 

Œ¥ = œµ/ (2 en). ‚ñ°

## 4 Numerical Experiments 

We now run experiments with the objective of approximating the five matrix functions as referred to in Section 2. We test four architectures: (1) shallow neural network with 3 hidden layers, (2) deep neural network with 7 hidden layers, (3) transformer encoder with Fourier features based on the work of Tancik et al. [17], and (4) transformer encoder-decoder with numerical encodings based on the work of Charton [7]. The next section details the experimental setup for the baseline methods and the transformer encoder-decoder with numerical encodings. Our experiments can be reproduced using the code available at www. github.com/rahul3/LAWT. 

4.1 Experimental Setup 

The baseline methods were chosen to compare the performance of more general neural network architectures and an encoder only transformer architecture with the transformer encoder-decoder with numerical encodings. Across all methods we use a tolerance-based accuracy metric to evaluate the performance of the models which is defined as follows: 

Definition 2 (Tolerance-Based Accuracy Metric). For tolerance œÑ , let 

Accuracy( œÑ ) = 1

Neval 

> Neval

X

> i=1

1

" P  

> j,k

|( ÀÜYi)jk ‚àí (Yi)jk |

P 

> j,k

|(Yi)jk | + œµ < œÑ 

#

(1) 

where Neval is the number of evaluation samples, Yi is the true output, ÀÜYi is the predicted output, œµ is some small positive constant for numerical stability. 

In our experiments, we use œµ = 10 ‚àí7 and œÑ ‚àà { 0.05 , 0.02 , 0.01 , 0.005 }.6 R. Padmanabhan and S. Brugiapaglia 

Baseline Methods. We used the following as the baseline methods: 

‚Äì The shallow neural network has hidden layers of size 128, 256, and 128 with ReLU activation function. 

‚Äì The deep neural network has hidden layers of size 128, 256, 512, 1024, 512, 256, and 128 with ReLU activation function and dropout 0.2. 

‚Äì The transformer encoder with Fourier features has 2, 4, 8, and 16 layers with 

d2 attention heads where d is the dimension of the matrix. The training data for all cases above is sampled from a Gaussian distribution in [‚àí5, 5] and Adam [10] is used as the optimizer with a learning rate of 10 ‚àí3.The amount of training samples was varied from 25 to 218 in all the baseline methods. The shallow and deep neural networks are trained for 100 epochs with a batch size of 128. The transformer encoder with Fourier features is trained for 600 epochs with a batch size of 64 to give it enough time to learn the underlying structure of the data. In the case of the transformer encoder with Fourier fea-tures, we follow the approach of Tancik et al. [17] where the inputs are mapped via Œ≥(x) = [cos(2 œÄBx ), sin(2 œÄBx )] , where Bij ‚àº N (0 , œÉ 2) are independent. We test 2, 4, 8, and 16 layers with up to d2 heads where d is the dimension of the matrix. 

Baseline Method Loss Functions. The loss functions are selected based on the function being approximated. For the exponential operation, due to the dynamic range of the output, we use the ‚Ñì1 relative error loss Lrel = E[‚à• ÀÜY ‚àíY ‚à•1/(‚à•Y ‚à•1+œµ)] 

where œµ = 10 ‚àí7, to account for the large dynamic range of exponential outputs. For logarithm, sign, sine, and cosine operations, we employ the Frobenius norm loss LFrob = ‚à•Y ‚àí ÀÜY ‚à•F for matrices ( d > 1) and mean squared error for scalars (d = 1 ). The relative error formulation for exponentials ensures scale-invariant learning, while the Frobenius norm naturally extends Euclidean distance to ma-trix spaces. 

Transformer Encoder-Decoder. The main architecture has 8 encoder layers, 1 decoder layer, 8 attention heads, embedding dimension 512. We train for 100 epochs, using 300K samples/epoch, batch size 64, Adam with warmup (10,000 steps), peak learning rate of 10 ‚àí4. Matrix entries for the random matrices con-sidered are sampled from a normal distribution, then clipped to the interval 

[‚àí5, 5] .

4.2 Baseline Results 

The baseline results are shown in Figures 1a and 1b for the 3 layer shallow neu-ral network and the 7 layer deep neural network respectively. In the case of the Fourier-based transformer encoder, we start with the evaluation of dimension 3 and find that the results are unsatisfactory with approximately 0% accuracy across all functions. The highest tolerance-based accuracy is 0.256% for the ma-trix sign function at dimension 3 at a tolerance of 0.05 when the Fourier-based transformer encoder has 4 layers. This motivates the use of the encoder-decoder with numerical encodings, which is the main focus of the next section. Matrix Functions with DNNs and Transformers 7 

> (a) 3-layer shallow network (b) 7-layer deep network

Fig. 1: Maximum tolerance-based accuracy decreases as matrix dimension in-creases. From dimension 3 onwards, accuracy is less than 3% for all functions. Experiments were run up to dimension 8. 

4.3 Transformer Encoder-Decoder Results 

Our main numerical result is that the transformer encoder-decoder with the FP15 encoding approximates the matrix sign function at 88.41% accuracy at a tolerance of 0.01 for 3 √ó 3 matrices. Tables 2 and 3 report accuracy at several tolerance levels for 3 √ó 3 and 5 √ó 5 matrices respectively. We find that the choice of encoding matters significantly. The FP15 encoding performs well for the matrix sign function and the P10 encoding performs well at an accuracy of 74.96% for the matrix logarithm function at a tolerance of 0.01 for 3√ó3 matrices. The FP15 has the highest vocabulary size (30,000 words) and the P10 has the lowest vocabulary size (210 words) in our experiments. Every encoding fails to approximate the matrix sine and cosine functions. As we have used [‚àí5, 5] as the range for the matrix entries, the matrix exponential has an extreme dynamic range. Our baseline methods of the 3 and 7 layer neural networks were unable to approximate the matrix exponential function for 3 √ó 3 matrices at a tolerance of 0.05, higher than the tolerance of 0.01 which we use as a reasonable benchmark. However, we were able to approximate the matrix exponential function for 3 √ó 3

and 5 √ó 5 matrices at a tolerance of 0.05 with the transformer encoder-decoder with the B1999 encoding at a tolerance-based accuracy of 99.58% and 93.86% respectively which is a significant improvement over the baseline methods. 

## 5 Conclusions 

We presented two results on neural networks for matrix function approximation. Theorem 1 shows ReLU DNNs can approximate the matrix exponential with width exponential in nM and depth linear up to log factors. Despite the positive result in Theorem 1, numerical experiments with ReLU DNNs were not satis-factory. This gap is not surprising given that the architecture from the theorem was not implemented in the numerical experiments. Moreover, the theorem is an 8 R. Padmanabhan and S. Brugiapaglia                                                                                                          

> Operation Encoding Tolerance-Based Accuracy tol=0.05 tol=0.02 tol=0.01 tol=0.005
> Exponential P10 85.6% 24.18% 2.48% 0.13% Logarithm P10 93.09% 86.56% 74.96% 46.92% Sign P10 95.84% 87.58% 61.36% 27.26% Sine P10 2.92% 1.39% 0.23% 0.02% Cosine P10 5.16% 1.17% 0.11% 0.00% Exponential P1000 88.92% 37.84% 9.9% 1.21% Logarithm P1000 92.52% 85.13% 72.09% 46.29% Sign P1000 96.26% 91.28% 76.53% 42.61% Sine P1000 5.13% 2.3% 0.19% 0.02% Cosine P1000 10.43% 5.28% 0.64% 0.00% Exponential FP15 0.00% 0.00% 0.00% 0.00% Logarithm FP15 0.00% 0.00% 0.00% 0.00% Sign FP15 97.0% 94.5% 88.41% 67.52%
> Sine FP15 0.00% 0.00% 0.00% 0.00% Cosine FP15 0.00% 0.00% 0.00% 0.00% Exponential B1999 99.58% 74.54% 24.79% 2.46% Logarithm B1999 93.02% 79.41% 55.68% 19.34% Sign B1999 44.85% 28.0% 21.26% 20.69% Sine B1999 0.00% 0.00% 0.00% 0.00% Cosine B1999 0.04% 0.00% 0.00% 0.00%

Table 2: Accuracy results for different matrix functions and encodings on 3 √ó 3

matrices across various error tolerances. existence result and does not involve training. The main numerical result from Table 2 shows that transformer encoder-decoders with numerical encodings can approximate the matrix sign function well for 3 √ó 3 matrices at a tolerance of 0.01. This is an improvement over the baseline methods, which were unable to approximate the matrix functions used in the experiments for 3 √ó 3 matrices. Our results on approximating the matrix sign function connect to recent work on transformers for Lyapunov functions by Alfarano et al. [5]. While that work targets symbolic solutions for nonlinear systems, our approach addresses numer-ical computation for linear systems. Both directions point to the potential of transformers in stability analysis and control theory. Finally, the strong depen-dence on encoding scheme suggests that numerical representations designed for specific problems could help. 

Future Work. Several questions remain open. First, extending these results to larger matrices (dimension 10 or higher) is important for practical applications but requires new ideas, given the sharp accuracy drop from dimension 3 to 5. Second, the complete failure on matrix sine and cosine is not yet understood; the oscillatory nature of these functions may pose a distinct challenge. Third, while we provide approximation bounds for ReLU networks, theoretical analysis of transformer architectures for matrix functions is lacking. Fourth, the strong Matrix Functions with DNNs and Transformers 9

Operation Encoding Tolerance-Based Accuracy tol=0.05 tol=0.02 tol=0.01 tol=0.005 

Exponential P10 0.00% 0.00% 0.00% 0.00% Logarithm P10 0.00% 0.00% 0.00% 0.00% Sign P10 0.00% 0.00% 0.00% 0.00% Sine P10 0.00% 0.00% 0.00% 0.00% Cosine P10 0.00% 0.00% 0.00% 0.00% Exponential P1000 68.04% 11.24% 0.64% 0.00% Logarithm P1000 69.08% 31.89% 2.62% 0.00% Sign P1000 1.89% 1.89% 1.89% 1.89% Sine P1000 0.00% 0.00% 0.00% 0.00% Cosine P1000 0.00% 0.00% 0.00% 0.00% Exponential FP15 0.00% 0.00% 0.00% 0.00% Logarithm FP15 0.00% 0.00% 0.00% 0.00% Sign FP15 1.19% 1.19% 1.19% 1.19% Sine FP15 0.00% 0.00% 0.00% 0.00% Cosine FP15 0.00% 0.00% 0.00% 0.00% Exponential B1999 93.86% 32.17% 1.75% 0.00% Logarithm B1999 84.31% 57.03% 13.06% 0.03% Sign B1999 1.01% 1.01% 1.01% 1.01% Sine B1999 0.00% 0.00% 0.00% 0.00% Cosine B1999 0.00% 0.00% 0.00% 0.00% 

Table 3: Accuracy results for different matrix functions and encodings on 5 √ó 5

matrices across various error tolerances. dependence on encoding scheme is purely empirical. Developing principled guide-lines for selecting or designing encodings based on the target function would be valuable. 

Acknowledgments. This work utilized the computational resources provided by the Digital Research Alliance of Canada (DRAC). SB acknowledges the support of NSERC through grant RGPIN-2020-06766. 

Disclosure of Interests. The authors have no competing interests to declare that are relevant to the content of this article. 

## References 

1. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023) 2. Adcock, B., Brugiapaglia, S., Dexter, N., Moraga, S.: Near-optimal learning of Banach-valued, high-dimensional functions via deep neural networks. Neural Net-works (2025) 3. Al-Mohy, A.H., Higham, N.J., Relton, S.D.: New algorithms for computing the matrix sine and cosine separately or simultaneously. SIAM Journal on Scientific Computing 37 (1), A456‚ÄìA487 (2015) 10 R. Padmanabhan and S. Brugiapaglia 4. Alexa, M.: Linear combination of transformations. ACM Transactions on Graphics (TOG) 21 (3), 380‚Äì387 (2002) 5. Alfarano, A., Charton, F., Hayat, A.: Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers. Advances in Neural Information Processing Systems 37 , 93643‚Äì93670 (2024) 6. Anthropic: The Claude 3 model family: Opus, Sonnet, Haiku. https: //www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf (2024) 7. Charton, F.: Linear algebra with transformers. Transactions on Machine Learning Research (2022) 8. DeepSeek-AI: DeepSeek-V3 technical report. arXiv preprint arXiv:2412.19437 (2024) 9. Higham, N.J.: Functions of Matrices: Theory and Computation. SIAM (2008) 10. Kingma, D.P.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 11. Lample, G., Charton, F.: Deep learning for symbolic mathematics. In: International Conference on Learning Representations (2019) 12. Otter, D.W., Medina, J.R., Kalita, J.K.: A survey of the usages of deep learning for natural language processing. IEEE Transactions on Neural Networks and Learning Systems 32 (2), 604‚Äì624 (2020) 13. Padmanabhan, R.: Deep Learning Approximation of Matrix Functions: From Feed-forward Neural Networks to Transformers. Master‚Äôs thesis, Concordia University (2025) 14. Roberts, J.D.: Linear model reduction and solution of the algebraic Riccati equa-tion by use of the sign function. International Journal of Control 32 (4), 677‚Äì687 (1980) 15. Ross, S.M.: Introduction to Probability Models. Academic press (2014) 16. Schwab, C., Zech, J.: Deep learning in high dimension: Neural network expression rates for generalized polynomial chaos expansions in uq. Analysis and Applications 

17 (01), 19‚Äì55 (2019) 17. Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Sing-hal, U., Ramamoorthi, R., Barron, J., Ng, R.: Fourier features let networks learn high frequency functions in low dimensional domains. In: Advances in Neural In-formation Processing Systems. vol. 33, pp. 7537‚Äì7547 (2020) 18. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozi√®re, B., Goyal, N., Hambro, E., Azhar, F., et al.: LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023) 19. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, ≈Å., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information Processing Systems. vol. 30, pp. 5998‚Äì6008 (2017) 20. Voulodimos, A., Doulamis, N., Doulamis, A., Protopapadakis, E.: Deep learning for computer vision: A brief review. Computational intelligence and neuroscience 

2018 (1), 7068349 (2018) 21. Wu, S., Roberts, K., Datta, S., Du, J., Ji, Z., Si, Y., Soni, S., Wang, Q., Wei, Q., Xiang, Y., et al.: Deep learning in clinical natural language processing: a methodical review. Journal of the American Medical Informatics Association 27 (3), 457‚Äì470 (2020) 22. xAI: Grok-1: A 314b mixture-of-experts model. https://github.com/xai-org/grok-1 (2024), gitHub repository