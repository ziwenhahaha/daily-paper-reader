---
title: "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding"
title_zh: PolySAE：通过多项式解码在稀疏自编码器中建模特征交互
authors: "Panagiotis Koromilas, Andreas D. Demou, James Oldfield, Yannis Panagakis, Mihalis Nicolaou"
date: 2026-02-01
pdf: "https://arxiv.org/pdf/2602.01322v1"
tags: ["keyword:SR", "query:SR"]
score: 6.0
evidence: 自动编码器中特征交互的多项式解码
tldr: 针对稀疏自编码器（SAE）在线性重构假设下难以捕捉特征组合结构（如无法区分特征共现与语义合成）的问题，本文提出了 PolySAE。该方法在保持线性编码器的同时，通过低阶张量分解在解码器中引入高阶多项式项来建模特征间的交互。实验证明，PolySAE 在仅增加极小参数开销的情况下，显著提升了特征的探测性能和类别区分度，能更有效地揭示语言模型中的组合语义结构。
motivation: 传统的线性 SAE 假设特征间仅存在加性关系，导致其无法区分特征的简单共现与深层的语义组合，从而难以分解复合概念。
method: 在解码器中引入基于低阶张量分解的高阶多项式项以建模特征交互，同时保留线性编码器以确保特征提取的可解释性。
result: "PolySAE 在多个模型上将探测 F1 值平均提升了 8%，且学习到的交互权重与特征共现频率几乎无关，显示其捕捉到了更本质的组合结构。"
conclusion: 通过引入多项式解码，PolySAE 证明了建模特征交互是提升稀疏自编码器解释能力和捕捉复杂语义组合的关键方向。
---

## 摘要
稀疏自编码器 (SAEs) 已成为一种极具前景的神经网络表示解释方法，它通过将激活值分解为字典原子的稀疏组合来实现。然而，SAEs 假设特征通过线性重构进行加性组合，这一假设无法捕捉组合结构：线性模型无法区分“星巴克”是由“星星”和“咖啡”特征组合而成，还是仅仅因为它们共同出现。这迫使 SAEs 为复合概念分配整体特征，而不是将其分解为可解释的组成部分。我们提出了 PolySAE，它通过高阶项扩展了 SAE 解码器以建模特征交互，同时保留了对可解释性至关重要的线性编码器。通过在共享投影子空间上进行低秩张量分解，PolySAE 以较小的参数开销（在 GPT2 上为 3%）捕捉了成对和三重的特征交互。在四种语言模型和三种 SAE 变体中，PolySAE 在保持相当的重构误差的同时，探测 F1 分数平均提高了约 8%，并且在类条件特征分布之间产生了 2-10 倍更大的 Wasserstein 距离。关键在于，学习到的交互权重与共现频率的相关性极低（r = 0.06，而 SAE 特征协方差为 r = 0.82），这表明多项式项捕捉到了组合结构（如形态绑定和短语组合），且在很大程度上独立于表面统计数据。

## Abstract
Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether "Starbucks" arises from the composition of "star" and "coffee" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.