Title: OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference

URL Source: https://arxiv.org/pdf/2602.01493v1

Published Time: Tue, 03 Feb 2026 02:44:34 GMT

Number of Pages: 38

Markdown Content:
# OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

Zhuoyuan Wang 1 Hanjiang Hu 1 Xiyu Deng 1 Saviz Mowlavi 2 Yorie Nakahira 1

# Abstract 

Solving diverse partial differential equations (PDEs) is fundamental in science and engineer-ing. Large language models (LLMs) have demon-strated strong capabilities in code generation, sym-bolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains chal-lenging. Prior work on LLM-based code gener-ation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execu-tion success rate and numerical accuracy arises, particularly when generalization to unseen param-eters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM paramet-ric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including un-seen parameters and configurations, and provides seamless integration with LLMs for natural lan-guage specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator in-ference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving. 

# 1. Introduction 

Solving diverse partial differential equations (PDEs) is fun-damental in science and engineering, underpinning applica-tions ranging from fluid dynamics and heat transfer to elec-tromagnetics, materials science, and climate modeling (Cai et al., 2021; Oommen et al., 2024; Nohra & Dufour, 2024; Pathak et al., 2022). In practice, modern PDE workflows must accommodate heterogeneous governing equations, pa-rameter regimes, and boundary and initial conditions, often 

> 1

Carnegie Mellon University 2Mitsubishi Electric Re-search Laboratories. Correspondence to: Zhuoyuan Wang 

<zhuoyuaw@andrew.cmu.edu >.

Preprint. February 3, 2026. Prediction Error    

> Success Rate (%)
> CodePDE
> LLM
> MOL -LLM
> OpInf -LLM (ours)

Figure 1. Trade-offs in prediction error and success rate. 

under strict accuracy and robustness requirements. As a result, there is growing interest in automated and general-izable PDE solvers that can flexibly adapt to new problem instances without extensive manual intervention or retrain-ing (Karniadakis et al., 2021; Li et al., 2020; Lu et al., 2019; Sun et al., 2025; Ye et al., 2024; Liu et al., 2024a). Large language models (LLMs) have recently demonstrated strong capabilities in code generation (Jiang et al., 2024; Huynh & Lin, 2025), symbolic reasoning (Xu et al., 2024; Shojaee et al., 2024), and tool use (Xu et al., 2025b), mo-tivating a new line of work on language-driven scientific computing (Nejjar et al., 2025). In the context of PDEs, prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances, including automatic solver synthesis and data-driven surrogate modeling (Li et al., 2025; Gaonkar et al., 2025; Wu et al., 2025; Negrini et al., 2025). However, reli-ably solving PDEs across heterogeneous settings remains challenging for LLM-based approaches. In particular, a persistent trade-off between execution success rate and nu-merical accuracy arises, especially when generalization to unseen parameters, boundary conditions, and domain config-urations is required (Fig. 1). Code-generation-based meth-ods often suffer from brittle execution and solver instability, while purely data-driven neural surrogates can exhibit poor extrapolation and high training costs. These limitations highlight a fundamental gap between ex-pressive, flexible language-driven interfaces and robust, gen-eralizable numerical solvers. On the one hand, LLMs pro-1

> arXiv:2602.01493v1 [cs.LG] 2 Feb 2026

OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference Offline Proper Orthogonal Decomposition (POD)                               

> Least -Squares Optimization
> Dataset
> PDE Parameter
> Î¾
> Trajectory ð· OpInf
> Online
> Shared Basis Î¦
> Reduced -Order
> Operators ð´ ,ð» ,ðµ ,ð‘
> Natural Language
> Instruction
> â€œI want to solve the
> viscous Burgersâ€™
> equation with nu = 0.03
> ...â€
> Agentic Tool Calling
> Operator Regression &ODE Integration
> function polynomial _regression
> (params_data,coefficient_data ,
> xi) ...
> return A, H, B, c
> function ode_integration (...)
> PDE Parameters
> and Operators Data {ðœ‰ ,ð´ ,ð» ,ðµ ,ð‘ }
> LLM
> Parse Equation and
> Parameters of Interest
> Test -Time Solution
> Reconstruction
> PDE Prediction on Unseen
> Parameters and Varying BC
> Heat Equation ðœˆ âˆˆ[0,3]
> Burgersâ€™ Equation ðœˆ âˆˆ[0,1]
> Lid -Driven Cavity Re âˆˆ[60 ,150 ]

Figure 2. Overall diagram of the OpInf-LLM framework. 

vide a powerful abstraction layer for specifying PDEs and solver configurations via natural language (Li et al., 2024a). On the other hand, classical reduced-order modeling tech-niques, such as operator inference, offer principled mech-anisms for constructing compact, interpretable, and gen-eralizable dynamical system representations from limited data, but cannot handle diverse PDEs at once with language specifications (Peherstorfer & Willcox, 2016; Kramer et al., 2024). Bridging these two paradigms offers a promising pathway toward reliable, language-driven PDE solving. In this work, we propose OpInf-LLM, an LLM paramet-ric PDE solving framework via operator inference (Fig. 2). We first leverage operator inference to learn reduced-order models for diverse parametric PDE instances from a small amount of solution data over a finite set of parameter values and configuration settings, yielding a shared reduced basis and parameter-dependent reduced operators. We then inte-grate large language models to infer and solve new reduced-order models via agentic tool calling, enabling the prediction of PDE solutions for diverse instances, including previously unseen parameters and varying boundary conditions. By construction, the reduced-order model effectively reduces the complexity of PDE generalization to polynomial fit-ting over learned operators and time integration of a low-dimensional ODE system, resulting in low computational demands at test time and a high execution success rate across heterogeneous PDE settings. Moreover, the use of LLMs naturally admits natural language instructions for specifying PDEs parameters and configurations, providing a flexible and unified interface for diverse PDE-solving tasks. The key advantages of OpInf-LLM are: â€¢ Accurate zero-shot prediction of PDE solutions under unseen parameters and configuration settings. â€¢ Lightweight training with high execution success rates across heterogeneous PDEs and settings. â€¢ Seamless natural language specification and solution of diverse PDEs within a single unified framework. 

# 2. Related Work 

Operator Inference. Reduced-order modeling (ROM) (Benner et al., 2015; Brunton & Kutz, 2022) is a class of model reduction techniques that aims to reduce the compu-tational cost of simulating large-scale dynamical systems governed by PDEs by leveraging a low-dimensional latent space. Among model reduction techniques, operator infer-ence (OpInf) (Kramer et al., 2024; Peherstorfer & Willcox, 2016) is a data-driven ROM method that learns structured reduced dynamics consistent with PDE nonlinearities, allow-ing accurate prediction with improved generalization and lower data requirements compared to black-box approaches. OpInf has been applied to parametric settings (McQuarrie et al., 2023), used to simulate complex physics such as com-bustion chamber dynamics (McQuarrie et al., 2021; Swis-chuk et al., 2020), and enhancements have been proposed to account for a wider class of PDE nonlinearities (Qian et al., 2020; 2022) or enhance robustness of the predic-tions (Sawant et al., 2023). To the best of our knowledge, ROMs have not been explored for language-model-driven PDE solving or for extending beyond single parametric PDE families. We address this gap by bridging OpInf with ex-pressive LLM interfaces and tool-use capabilities to enable reliable multi-PDE-family solving across diverse instances. 

PDE Foundation Models. Recent advances in AI/ML have led to the development of a broad class of data-driven PDE solvers, including physics-informed neural networks (PINNs) (Raissi et al., 2019; Han et al., 2018; Cuomo et al., 2022; Cho et al., 2024; Huang et al., 2022), neural opera-tors (Kovachki et al., 2023; Li et al., 2020; Lu et al., 2019; 2021; 2022; Li et al., 2024b; Goswami et al., 2023), and neural PDE solvers (Brandstetter et al., 2022; Takamoto 2OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

et al., 2023; Hsieh et al., 2019). More recently, PDE founda-tion models that learn generalizable representations across diverse equation families have been proposed (Herde et al., 2024; Liu et al., 2024b; Ye et al., 2024; Sun et al., 2025; Shen et al., 2024; Rautela et al., 2025; Zhu et al., 2025; Zhou et al., 2025; Hao et al., 2024; Wiesner et al., 2025). These models aim to solve multiple PDEs with a single pre-trained architecture, reducing the need for equation-specific train-ing. Building on this direction, multimodal approaches have emerged that incorporate language inputs to enhance flex-ibility and interpretability (Negrini et al., 2025; Bao et al., 2025). In parallel, diffusion-based neural PDE solvers have been developed to support text-conditioned simulation using captions generated by multimodal LLMs (Zhou et al., 2024). Additionally, LLMs have been explored as auxiliary modal-ities to improve surrogate model performance (Lorsung & Farimani, 2024). Despite this progress, robust generaliza-tion to unseen parameters and initial and boundary condition settings remains a significant challenge. 

LLMs for Scientific Machine Learning. Large language models have shown promising capabilities in scientific ma-chine learning. Prior works focus on using LLMs to directly generate executable PDE solvers from natural language de-scriptions (Li et al., 2025; Wu et al., 2025), as well as lever-aging LLMs to write code for broader scientific machine learning problems (Gaonkar et al., 2025; Jiang et al., 2025; He et al., 2025). Beyond code generation, Yang et al. (2025) fine-tunes LLMs for in-context operator learning, while Xu et al. (2025a) explores PDE discovery by training generative models with textbook information. LLM-based agents have also adopted agentic workflows for PDE solving (Jiang & Karniadakis, 2025; Liu et al., 2025). In parallel, many ef-forts have explored LLM-driven autoformalization of PDEs, translating language descriptions into formal mathematical representations (Soroco et al., 2025; Mensfelt et al., 2025), building on broader autoformalization research that maps in-formal mathematical content into formal, machine-verifiable languages (Wu et al., 2022; Weng et al., 2025). Equation discovery from data has also been explored in Du et al. (2024); Grayeli et al. (2024). Despite these advances, solver reliability and execution robustness remain key challenges. 

# 3. Proposed Method 

In this section, we present the proposed OpInf-LLM method. The overall diagram is shown in Fig. 2. The method con-sists of two stages. In the offline stage, operator inference (OpInf) is used to construct a parametric reduced-order model (ROM) by learning a projection basis and reduced modal dynamics from trajectory data of PDE systems with finite parameter coverage. In the online stage, a large lan-guage model (LLM) is integrated at test time to enable the prediction of solutions for diverse PDE instances based on natural-language instructions, while generalizing to previ-ously unseen parameter configurations. 

3.1. Operator Inference 

We start by introducing OpInf, which is a data-driven model reduction method for PDEs (Kramer et al., 2024; Peherstor-fer & Willcox, 2016) and is the key component in the offline stage of the proposed method. Consider a general class of nonlinear parametric PDEs: 

âˆ‚y âˆ‚t = F( âˆ‚y âˆ‚x , âˆ‚2yâˆ‚x 2 , Â· Â· Â· , s, Î¾ ), x âˆˆ Î©, t âˆˆ [0 , T ], (1) with initial and boundary conditions 

I[y]( x, 0) = g(x), x âˆˆ Î©,

B[y]( x, t ) = u(t), x âˆˆ âˆ‚Î©, (2) where Î© âŠ‚ Rd is the spatial domain, s is a known source term, Î¾ âˆˆ R is the PDE parameter, and F is the generic nonlinear PDE operator. OpInf learns a ROM approximating the PDE dynamics within a finite r-dimensional subspace spanned by a set of basis functions Ï•1(x), . . . , Ï• r (x). Specifically, the PDE state is decomposed as 

y(x, t, Î¾ ) = 

> r

X

> i=1

ai(t, Î¾ )Ï•i(x), (3) where ai(t, Î¾ ) are time-dependent modal coefficients. In classical model reduction, the dynamics for these modal co-efficients can be derived from the PDE itself using Galerkin projection (Holmes et al., 2012). For PDE operators F with at most quadratic nonlinearities, which encompass a broad class of PDEs commonly arising in physics (e.g., the diffu-sion equation modeling heat transfer or the Navier-Stokes equations modeling fluid dynamics), the modal coefficients inherit reduced dynamics of the following form given Î¾:

da dt = A(Î¾)a + H(Î¾)( a âŠ— a) + B(Î¾)u + c(Î¾), (4) where A âˆˆ RrÃ—r is a linear operator, H âˆˆ RrÃ—r2

is a quadratic operator, B âˆˆ RrÃ—d is a control input matrix, and 

c âˆˆ Rr is an offset vector, with âŠ— denoting the Kronecker product. This procedure naturally applies to PDE operators 

F with higher-order or non-polynomial nonlinearities, lead-ing to modal dynamics with structures that differ from (4) ,for example through the inclusion of cubic terms (Qian et al., 2022). In this paper, we stick to quadratically nonlinear PDEs with the formulation in (4) for clarity. For a fixed Î¾, OpInf composes the following two steps: 1) identify an appropriate basis Ï•1(x), . . . , Ï• r (x); 2) iden-tify the corresponding reduced-order operators A, H, B, c.3OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

These procedures yield a reduced order model defined by (3) and (4) for solving PDEs with varying boundary conditions. For the rest of this subsection, we consider a single fixed Î¾

and we omit Î¾ dependency in the notation for conciseness. 

Basis identification. We collect a dataset D, obtained either from physical experiments or numerical simulations of the PDE (1) . The dataset consists of time series of the solution and its temporal derivatives over a discretization x of the full spatial domain 

D = {y (x, t 1) , . . . , y (x, t K )}âˆª

 âˆ‚y âˆ‚t (x, t 1) , . . . , âˆ‚y âˆ‚t (x, t K )



. (5) For notational simplicity, (5) is written for a single trajectory. In practice, the dataset may include multiple trajectories corresponding to different initial conditions and boundary conditions. Then, for a given reduced-order basis dimen-sion r, OpInf computes the basis functions by conduct-ing a proper orthogonal decomposition (POD) (Chatterjee, 2000) of the concatenated solution values y contained in the dataset D, which yields an optimal basis that maximizes the captured energy. Specifically, Î¦ := [ Ï•1(x), . . . , Ï• r (x)] are the first r left-singular vectors of the trajectory data matrix 

{y (x, t 1) , . . . , y (x, t K )}, corresponding to the r largest singular values. 1 The number of basis functions r can be tuned based on the desired level of captured energy (Pe-herstorfer & Willcox, 2016). Ablations on the effects of the basis function number r can be found in Section 4.4. 

Operator identification. Once the POD basis has been obtained, we project the dataset D onto the basis functions to form a reduced-order dataset A of modal coefficients and their time derivatives. Specifically, 

A = {a (t1) , . . . , a (tK )} âˆª { Ë™a (t1) , . . . , Ë™a (tK )} , (6) where ai (tj ) = âŸ¨y (x, t j ) , Ï• i(x)âŸ© ,

Ë™ai (tj ) = 

 âˆ‚y âˆ‚t (x, t j ) , Ï• i(x)



, (7) for i = 1 , . . . , r and j = 1 , . . . , K . Here âŸ¨Â· , Â·âŸ© denotes the L2 inner product. Then, OpInf calculates the operators 

A, H , B and c such that the reduced dynamics (4) best fits the data in the least-squares sense, which can be formulated as the least-squares problem 

min 

> A,H,B,c K

X

> k=1

âˆ¥[Aa + H (a âŠ— a) + Bu + c âˆ’ Ë™a] ( tk)âˆ¥2

> F

+ Î»  âˆ¥Aâˆ¥2 

> F

+ âˆ¥Hâˆ¥2 

> F

+ âˆ¥Bâˆ¥2 

> F

+ âˆ¥câˆ¥22

 ,

(8)     

> 1For PDEs with d > 1spatial dimensions, solution snapshots
> y(x, t )in the trajectory data matrix are vectorized (flattened) be-fore performing the POD.

where || Â· âˆ¥ F denotes the Frobenius norm, and Î» is a Tikhonov regularization coefficient that prevents model over-fitting and is often selected through a line search (Swischuk et al., 2020). Ablations on the effects of the regularization intensity Î» can be found in Section 4.4. During online inference, given any initial condition and boundary conditions of interest, the ROM approximates the PDE solution by first calculating the initial modal coeffi-cients corresponding to the initial condition, then propa-gating the modal coefficients over time according to the reduced dynamics (4) , and reconstructing the full-order so-lution via (3). 

3.2. Parametric OpInf and LLM Integration 

In the previous section, we discussed how OpInf can learn ROMs for PDEs with a single fixed parameter Î¾. In this section, we will discuss how OpInf can be extended to parametric PDEs, and how we integrate it with LLMs to solve diverse equations with natural language instructions. The key insight behind parametric OpInf is as follows (Mc-Quarrie et al., 2023). First, the ROM can share a common basis Î¦ within a parametric PDE family. Second, once re-duced dynamics operators A, H, B, c have been separately obtained for a set of training parameters Î¾1, Î¾ 2, . . . , reduced dynamics operators for any unseen Î¾ can be obtained via polynomial regression among the existing operators. Based on those insights, the proposed OpInf-LLM performs the following procedures to solve diverse parametric PDEs. 

Basis identification. We collect a trajectory dataset D in the form of time series of solutions and time derivatives similar to (5), but for multiple parameters Î¾

D = {y (x, t 1, Î¾ 1) , . . . , y (x, t K , Î¾ 1)} âˆª { y (x, t 1, Î¾ 2) , . . . }Â· Â· Â· âˆª 

 âˆ‚y âˆ‚t (x, t 1, Î¾ 1) , . . . 



âˆª

 âˆ‚y âˆ‚t (x, t 1, Î¾ 2) , . . . 



Â· Â· Â· .

(9) Here (9) shows a single trajectory for each Î¾i for notation conciseness, but multiple trajectories can be used. Then, for a given number of basis functions r, the shared basis Î¦ := [Ï•1(x), . . . , Ï• r (x)] can be obtained via proper orthogonal decomposition (POD) of all solution data y in (9). 

Operator identification. Once the POD basis is determined, the reduced-order dataset A of modal coefficients and their time derivatives can be obtained by projecting (7) on (9) 

A = {a (t1, Î¾ 1) , . . . , a (tK , Î¾ 1)} âˆª { a (t1, Î¾ 2) , . . . } Â· Â· Â· âˆª { Ë™a (t1, Î¾ 1) , . . . , Ë™a (tK , Î¾ 1)} âˆª { Ë™a (t1, Î¾ 2) , . . . } Â· Â· Â· .

(10) Then for each Î¾i in the dataset A, we solve the least-squares problem (8) to obtain the reduced operators A(Î¾i), H(Î¾i),

B(Î¾i), c(Î¾i).4OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

OpInf-LLM. Once the above procedures are executed, we integrate large language models (LLMs) to admit natural language instructions specifying the target PDE and its con-figurations, and leverage LLM tool-calling capabilities to infer the reduced operators and perform time integration, enabling parametric PDE solving across diverse settings. Specifically, given a natural language description of a PDE-solving task (e.g., â€œsolve the viscous Burgersâ€™ equation with viscosity Î½ = 0 .03 ...â€), the LLM first parses the governing equation and the specified parameters. Based on this infor-mation, the LLM calls existing tools to perform polynomial regression to infer the parameter-dependent reduced oper-ators A(Î¾), H(Î¾), B(Î¾), and c(Î¾), and then integrates the reduced-order ODE system in (4) using the parsed initial and boundary condition specifications. The key features of this integration include: (i). Natural language descriptions of diverse PDEs as instructions; (ii). Low execution failure rate enabled by moderate polynomial fitting and reduced-order ODE integration demands; (iii). Zero-shot generalization to unseen PDE parameters and ICBC configurations from unified ROM structure. 

# 4. Experiments 

In this section, we evaluate the performance of the proposed OpInf-LLM framework and compare it against representa-tive baseline methods. While many widely used PDE bench-marks focus on fixed or periodic boundary conditions (Li et al., 2020; Raissi et al., 2019; Takamoto et al., 2022; Gupta & Brandstetter, 2022), we consider parametric PDEs with varying non-homogeneous boundary conditions to highlight the generalization capabilities of the proposed approach. Full experimental details are provided in Appendix A. 

4.1. PDEs of Interest Heat Equation. We consider the one-dimensional heat equation 

yt(x, t ) = Î½ y xx (x, t ), x âˆˆ [0 , 1] , (11) subject to Dirichlet boundary conditions 

y(0 , t ) = y(1 , t ) = u(t), (12) and a fixed smooth initial condition y(x, 0) = g(x). The boundary input u(t) is chosen as a multi-sine signal 

u(t) = b +

> 3

X

> k=1

Ak sin(2 Ï€f kt + Ï•k) , (13) where the amplitudes Ak, frequencies fk, phases Ï•k, and bias b are independently randomized for each trajectory. This construction provides persistent excitation across a range of temporal modes. Training data are generated over a time horizon T = 1 .0

using diffusivity values Î½ âˆˆ { 0.1, 0.5, 2.0} with 3 trajecto-ries per parameter. Number of POD modes is set to r = 6 

for OpInf and regularization Î» = 10 âˆ’6. Evaluation is per-formed on 20 trajectories each corresponding to diffusivity values Î½ âˆˆ { 0.5, 1.0, 3.0}.

Burgersâ€™ Equation. We consider the forced viscous Burg-ersâ€™ equation 

yt(x, t ) + y y x(x, t ) = Î½ y xx (x, t ) + s(x, t ), x âˆˆ [0 , 1] ,

(14) with Dirichlet boundary conditions 

y(0 , t ) = u1(t), y(1 , t ) = u2(t). (15) The boundary inputs u1(t) and u2(t) represent time-dependent boundary conditions, while s(x, t ) denotes an internal source term. Both the boundary inputs and the source term are constructed using multi-sine signals, with a fixed spatial profile used to localize the source term. The initial condition is chosen to be smooth and consistent with the boundary values at t = 0 .Training data are generated over a time horizon T = 2 .0 us-ing viscosity parameters Î½ âˆˆ { 0.01 , 0.02 , 0.05 , 0.1} with 100 trajectories per parameter. Number of POD modes is set to r = 10 for OpInf and regularization Î» = 0 .5. Evalua-tion is performed on 20 trajectories each corresponding to previously unseen viscosity values Î½ âˆˆ { 0.03 , 0.07 }, while maintaining the same boundary and forcing input structure. 

2D Lid-Driven Cavity Flow. We consider two-dimensional incompressible flow in a square cavity using the vorticity-stream function formulation 

Ï‰t + v1 Ï‰p1 + v2 Ï‰p2 = 1Re (Ï‰p1p1 + Ï‰p2p2 ) , (16) 

âˆ†Ïˆ = âˆ’Ï‰, (17) where Ï‰ is the vorticity, x = ( p1, p 2) âˆˆ [0 , 1] 2 denotes the two-dimensional spatial coordinate, subscripts p1 and p2

indicate corresponding partial derivatives, and the velocity 

v = ( v1, v 2) is related to the stream function Ïˆ as 

v1 = Ïˆp2 , v2 = âˆ’Ïˆp1 . (18) No-slip boundary conditions are enforced on all cavity walls. The top lid is driven by a spatially varying and time-dependent horizontal velocity 

v1(x, t ) = h(p1) u(t), v2 = 0 , (19) where h(p1) is a fixed, non-symmetric spatial profile, and 

u(t) is a randomized sinusoidal input. Training data are generated over a time horizon T = 2 .0

using Reynolds numbers Re âˆˆ { 50 , 75 , 100 , 125 , 150 }

5OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference Ground Truth       

> CodePDE
> MOL -LLM
> LLM
> OpInf -LLM
> Figure 3. Heat equation predictions for Î½= 1 .0and Î½= 3 .0. All plots share the same color scale. Results are shown for CodePDE, LLM and OpInf-LLM with GPT-4.1.

with 8 trajectories per parameter. The number of POD modes is set to r = 20 for OpInf and the regularization strength is fixed to Î» = 3 . Model evaluation is carried out on 2 trajectories for each unseen Reynolds numbers 

Re âˆˆ { 60 , 80 , 90 , 110 , 120 , 140 }.

4.2. Baselines CodePDE (Li et al., 2025). CodePDE represents a class of LLM-based approaches that directly generate executable PDE solvers in a programming language from a textual de-scription of the governing equations, initial conditions, and boundary conditions. It employs a self-debugging mecha-nism that iteratively refines generated code using execution feedback. We apply test-time scaling with 5 independent solver generation rounds, each allowing 5 debugging trials, and report the best-performing solver. 

MOL-LLM (Negrini et al., 2025). MOL-LLM represents multimodal PDE foundation models with language integra-tion, combining a transformer-based PDE foundation model with language inputs. The model takes as input the initial state of the discretized solution, the PDE parameter vector, and boundary condition parameters, together with textual tokens describing the governing equation. Given these in-puts, MOL-LLM outputs the full solution trajectory over a fixed time grid. The model is trained for 25000 steps as in the original implementation. 2      

> 2https://github.com/enegrini/MOL-LLM Ground Truth
> CodePDE
> MOL -LLM
> LLM
> OpInf -LLM
> Figure 4. Burgersâ€™ equation predictions for Î½= 0 .03 and Î½=0.07 . All plots share the same color scale. Results are shown for CodePDE, LLM and OpInf-LLM with GPT-4.1.

MOL-LLM (large dataset). Following the original MOL-LLM work, which is trained on large-scale datasets, we also evaluate a large-data variant of this baseline. In this setting, the same MOL-LLM model and training procedure are used, but the training set is expanded to 5000 trajectories per equation sampled from the same underlying distribution. 

LLMs. We directly prompt large language models with the governing PDE, full initial and boundary conditions, external inputs, and a coarse output grid for tractability (heat/Burgers: 16 spatial points Ã— 41 time steps; cavity: 

6 Ã— 6 spatial points Ã— 21 time steps), and ask it to return the full spatiotemporal state array in JSON format (3-decimal precision) for a single parameter setting and trajectory. The LLM is treated as a black-box simulator, and its predicted field is compared against downsampled ground truth data using the relative L2 error. For each parameter instance and equation, the LLM is queried 10 times, and the averaged error over all successful cases is reported. Example prompts are provided in Appendix B.3. 

4.3. Results 

We report the results of the proposed OpInf-LLM frame-work and compare them with baseline methods. Table 1 summarizes the number of training samples, training time, code success rate, and the average relative L2 error over 

[0 , T ] for the heat, Burgersâ€™, and lid-driven cavity flow equa-tions. Visualizations of prediction results across equation 6OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference                                                                                        

> Table 1. Results summary. Average relative L2error is reported for each equation. Bold denotes the best result, and underline denotes the second best. â†‘higher is better, â†“lower is better. Method Data Train time (s) Success rate â†‘Heat â†“Burgers â†“Cavity â†“
> CodePDE (GPT-4.1) â€“â€“49.9% 1.59e âˆ’21.50e âˆ’11.41e0
> CodePDE (GPT-4o) â€“â€“39.9% 1.60e âˆ’11.23e0 2.89e0
> CodePDE (Gemini-2.0-flash) â€“â€“47.6% 1.74e âˆ’29.56e âˆ’11.55e0
> MOL-LLM 449 14306 100.0% 1.69e0 1.44e0 7.24e âˆ’1
> MOL-LLM (large dataset) 15000 35915 100.0% 4.87e0 1.58e0 6.44e âˆ’1
> LLM (GPT-4.1) â€“â€“100.0% 7.79e âˆ’19.82e âˆ’11.00e0
> LLM (GPT-4o) â€“â€“85.0% 9.24e âˆ’11.30e0 1.00e0
> LLM (Gemini-2.0-flash) â€“â€“33.3% 9.02e âˆ’1failed failed
> OpInf-LLM (GPT-4.1) 449 30 99 .2% 1.29e âˆ’24.91e âˆ’14.63e âˆ’2
> OpInf-LLM (GPT-4o) 449 30 99 .2% 1.29e âˆ’24.91e âˆ’14.63e âˆ’2
> OpInf-LLM (Gemini-2.0-flash) 449 30 99 .2% 1.29e âˆ’24.91e âˆ’14.63e âˆ’2

parameters and methods are shown in Fig. 3-5. It can be seen that CodePDE does not consistently produce high-quality solvers, often resulting in either code failures or inaccurate numerical solutions, which limits its reliability in practice. MOL-LLM achieves a high execution success rate since it is primarily a transformer-based method, but its prediction error becomes large when generalizing to dif-ferent parameters and boundary conditions is required, and its training cost is substantially higher than that of the other methods. Direct LLM prediction of PDE solutions remains challenging, as existing LLMs tend to infer boundary condi-tions correctly but zero out or purely interpolate the interior values, and their success rate is further reduced by shape or format mismatches. In contrast, OpInf-LLM yields ac-curate predictions across all equations while maintaining a high code execution success rate. It requires only a small amount of training data, accepts natural language instruc-tions, and accurately predicts diverse PDE solutions with varying boundary conditions across different instances and previously unseen parameter configurations. 

4.4. Ablations Extrapolation in time. We examine the capability of OpInf-LLM to extrapolatively predict solutions beyond the time horizons observed during training. Specifically, we eval-uate its performance on the interval [T, 2T ] for the three equations considered, following the settings in Section 4.1, and report the relative L2 error in Table 2. Although the framework has never seen any data on [T, 2T ], the predic-tion error remains consistently low and comparable to that on [0 , T ], demonstrating strong temporal generalization of the proposed method. Additional visualizations on extended horizons are provided in Fig. 6-9 in the Appendix. 

Number of POD modes. We further examine the effect of the number of POD basis modes in the proposed OpInf-LLM                      

> Table 2. Prediction error of OpInf-LLM with extrapolation in time. LLM model Equation Error [0 , T ]Error [T, 2T]
> GPT-4o Heat 1.29e âˆ’23.60e âˆ’2
> Burgers 4.91e âˆ’16.36e âˆ’1
> Cavity 4.63e âˆ’24.86e âˆ’2
> Gemini-2.0 flash Heat 1.29e âˆ’23.60e âˆ’2
> Burgers 4.91e âˆ’16.36e âˆ’1
> Cavity 4.63e âˆ’24.86e âˆ’2

method. Specifically, we follow the settings in Section 4.1 and vary the number of POD modes r from 4 to 10 for each equation instance. We report the total energy captured by the POD basis on the training data, along with the predic-tion errors on test parameters and configurations for both the original and extended time horizons. Table 3 summarizes the results for Burgersâ€™ equation with full results available in Table 7 in the Appendix. As expected, increasing the number of POD modes captures more energy in the training data and generally improves prediction accuracy, provided that the additional modes are not overfitting. In practice, capturing around 99% of the energy serves as a useful guide-line for selecting the number of POD modes (Kramer et al., 2024). Overly large POD bases can lead to solver instability, as observed for the heat equation with a large number of modes. Nevertheless, the OpInf-LLM framework remains executable across these settings with 100% code success rate, indicating robustness with respect to the choice of POD dimension. 

Regularization intensity. We next examine the effect of the regularization strength Î» in (8) on the proposed OpInf-LLM scheme. Following the settings in Section 4.1, we vary Î» logarithmically from 0 to 10 for each equa-tion instance, and report the averaged operator norm pâˆ¥Aâˆ¥2 

> F

+ âˆ¥Hâˆ¥2 

> F

+ âˆ¥Bâˆ¥2 

> F

+ âˆ¥câˆ¥2 

> F

on the training set, to-7OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference Ground Truth                                                

> CodePDE
> MOL -LLM
> LLM
> OpInf -LLM
> Figure 5. Lid-driven cavity predictions for Re = 80 and Re = 120
> at t= 1 and t= 2 . All plots share the same color scale. Results are shown for CodePDE, LLM and OpInf-LLM with GPT-4.1.
> Table 3. POD basis ablation for OpInf-LLM. Equation POD Energy (%) Error [0 , T ]Error [T, 2T]
> Burgers 488.01 3.93e âˆ’14.99e âˆ’1
> Burgers 592.26 3.44e âˆ’14.48e âˆ’1
> Burgers 695.63 4.13e âˆ’19.63e âˆ’1
> Burgers 797.26 3.52e âˆ’15.41e âˆ’1
> Burgers 898.31 3.64e âˆ’14.90e âˆ’1
> Burgers 998.85 4.70e âˆ’16.46e âˆ’1
> Burgers 10 99.20 4.91e âˆ’16.36e âˆ’1

gether with the prediction errors on test configurations for both the original and extended time horizons. Table 4 shows the results for Burgersâ€™ equation and full results are avail-able in Table 8 in the Appendix. The averaged operator norms decrease as the regularization strength increases, and the best Î» for test performance can be identified, which is typically within a range spanning one order of magni-tude or more. However, certain regularization values (e.g., 

Î» = 0 and 10 âˆ’6 for the cavity flow) can lead to failed least-squares optimization due to numerical singularity. In practice, ablations over a range of regularization values are typically performed on a validation set, and the operator norm is jointly considered to determine an appropriate Î»,with low validation error and a moderate operator norm being desired (Golub et al., 1979; McQuarrie et al., 2021).                                               

> Table 4. Regularization intensity ( Î») ablation for OpInf-LLM. Equation Î»Norm Error [0 , T ]Error [T, 2T]
> Burgers 01730.53 2.25e1 2.92e1
> Burgers 10 âˆ’61730.53 2.25e1 2.92e1
> Burgers 10 âˆ’51645.36 2.25e1 2.92e1
> Burgers 10 âˆ’41645.28 2.25e1 2.92e1
> Burgers 10 âˆ’31637.70 2.26e1 2.94e1
> Burgers 10 âˆ’21377.88 2.96e1 3.90e1
> Burgers 10 âˆ’1346.36 1.81e1 5.42e1
> Burgers 176.14 3.31e âˆ’15.53e âˆ’1
> Burgers 10 33.22 3.34e âˆ’14.53e âˆ’1

# 5. Discussion 

In this section, we discuss limitations and potential exten-sions of the proposed OpInf-LLM method. For a given PDE family, the same reduced basis can be reused across dif-ferent parameter values and configurations, but new bases are usually constructed when introducing new governing equations, although such cost is incurred upfront offline rather than introducing additional computation at inference time. Notably, the OpInf framework does not require ex-plicit knowledge of the full PDE form and only assumes the type of nonlinearity in the reduced dynamics, such as linear or quadratic structure, without needing the specific PDE instance or parameter values, which makes it well suited for settings in which the governing equations are partially unknown or difficult to identify. In addition, although we employ the standard OpInf in our framework (Peherstorfer & Willcox, 2016), extensions of the OpInf framework that improve its robustness and accuracy (Sawant et al., 2023; Geng et al., 2024) can be potentially used to further enhance the performance of OpInf-LLM. 

# 6. Conclusions 

In this work, we provide a new perspective on reduced or-der modeling for LLM-based PDE solving, and propose OpInf-LLM, an LLM parametric PDE solving framework via operator inference. The proposed framework leverages shared reduced-order bases across each PDE family to sup-port accurate, generalizable solutions under varying bound-ary conditions from limited data. By reformulating para-metric PDE solving as polynomial regression in a reduced-order operator space followed by ODE integration, OpInf-LLM is able to solve unseen PDE instances from natural language instructions while achieving improved trade-offs between accuracy and execution success rate. Ablations further show robustness over extended test horizons and different OpInf configurations. Future directions include in-tegrating more advanced operator inference techniques and developing richer languageâ€“solver interfaces for scalable scientific computing. 8OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

# Impact Statement 

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. 

# References 

Bao, J., Boull Â´e, N., Liu, T. J., Sarfati, R., and Earls, C. J. Text-trained llms can zero-shot extrapolate pde dynamics. 

arXiv preprint arXiv:2509.06322 , 2025. Benner, P., Gugercin, S., and Willcox, K. A survey of projection-based model reduction methods for parametric dynamical systems. SIAM review , 57(4):483â€“531, 2015. Brandstetter, J., Worrall, D., and Welling, M. Message pass-ing neural pde solvers. arXiv preprint arXiv:2202.03376 ,2022. Brunton, S. L. and Kutz, J. N. Data-driven science and engineering: Machine learning, dynamical systems, and control . Cambridge University Press, 2022. Cai, S., Wang, Z., Wang, S., Perdikaris, P., and Karniadakis, G. E. Physics-informed neural networks for heat transfer problems. Journal of Heat Transfer , 143(6), 2021. Chatterjee, A. An introduction to the proper orthogonal decomposition. Current science , pp. 808â€“817, 2000. Cho, W., Jo, M., Lim, H., Lee, K., Lee, D., Hong, S., and Park, N. Parameterized physics-informed neural networks for parameterized pdes. arXiv preprint arXiv:2408.09446 ,2024. Cuomo, S., Di Cola, V. S., Giampaolo, F., Rozza, G., Raissi, M., and Piccialli, F. Scientific machine learning through physicsâ€“informed neural networks: Where we are and whatâ€™s next. Journal of Scientific Computing , 92(3):88, 2022. Du, M., Chen, Y., Wang, Z., Nie, L., and Zhang, D. Llm4ed: Large language models for automatic equation discovery. 

arXiv preprint arXiv:2405.07761 , 2024. Gaonkar, S., Zheng, X., Xi, H., Tiwari, R., Keutzer, K., Morozov, D., Mahoney, M. W., and Gholami, A. Sciml agents: Write the solver, not the solution. arXiv preprint arXiv:2509.09936 , 2025. Geng, Y., Singh, J., Ju, L., Kramer, B., and Wang, Z. Gradi-ent preserving operator inference: Data-driven reduced-order models for equations with gradient structure. Com-puter Methods in Applied Mechanics and Engineering ,427:117033, 2024. Golub, G. H., Heath, M., and Wahba, G. Generalized cross-validation as a method for choosing a good ridge parame-ter. Technometrics , 21(2):215â€“223, 1979. Goswami, S., Bora, A., Yu, Y., and Karniadakis, G. E. Physics-informed deep neural operator networks. In Ma-chine learning in modeling and simulation: methods and applications , pp. 219â€“254. Springer, 2023. Grayeli, A., Sehgal, A., Costilla Reyes, O., Cranmer, M., and Chaudhuri, S. Symbolic regression with a learned concept library. Advances in Neural Information Process-ing Systems , 37:44678â€“44709, 2024. Gupta, J. K. and Brandstetter, J. Towards multi-spatiotemporal-scale generalized pde modeling. arXiv preprint arXiv:2209.15616 , 2022. Han, J., Jentzen, A., and E, W. Solving high-dimensional partial differential equations using deep learning. Pro-ceedings of the National Academy of Sciences , 115(34): 8505â€“8510, 2018. Hao, Z., Su, C., Liu, S., Berner, J., Ying, C., Su, H., Anand-kumar, A., Song, J., and Zhu, J. Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training. arXiv preprint arXiv:2403.03542 , 2024. He, X., You, L., Tian, H., Han, B., Tsang, I., and Ong, Y.-S. Lang-pinn: From language to physics-informed neural networks via a multi-agent framework. arXiv preprint arXiv:2510.05158 , 2025. Herde, M., Raonic, B., Rohner, T., K Â¨appeli, R., Molinaro, R., de B Â´ezenac, E., and Mishra, S. Poseidon: Efficient foundation models for pdes. Advances in Neural Infor-mation Processing Systems , 37:72525â€“72624, 2024. Holmes, P., Lumley, J. L., Berkooz, G., and Rowley, C. W. Turbulence, Coherent Structures, Dynamical Sys-tems and Symmetry . Cambridge University Press, 2 edition, 2012. ISBN 9781107008250. doi: 10.1017/ CBO9780511919701. Hsieh, J.-T., Zhao, S., Eismann, S., Mirabella, L., and Er-mon, S. Learning neural pde solvers with convergence guarantees. arXiv preprint arXiv:1906.01200 , 2019. Huang, X., Ye, Z., Liu, H., Ji, S., Wang, Z., Yang, K., Li, Y., Wang, M., Chu, H., Yu, F., et al. Meta-auto-decoder for solving parametric partial differential equations. Ad-vances in Neural Information Processing Systems , 35: 23426â€“23438, 2022. Huynh, N. and Lin, B. Large language models for code generation: A comprehensive survey of challenges, tech-niques, evaluation, and applications. arXiv preprint arXiv:2503.01245 , 2025. 9OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

Jiang, J., Wang, F., Shen, J., Kim, S., and Kim, S. A survey on large language models for code generation. arXiv preprint arXiv:2406.00515 , 2024. Jiang, Q. and Karniadakis, G. Agenticsciml: Collabora-tive multi-agent systems for emergent discovery in scien-tific machine learning. arXiv preprint arXiv:2511.07262 ,2025. Jiang, Q., Gao, Z., and Karniadakis, G. E. Deepseek vs. chatgpt vs. claude: A comparative study for scientific computing and scientific machine learning tasks. The-oretical and Applied Mechanics Letters , 15(3):100583, 2025. Karniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris, P., Wang, S., and Yang, L. Physics-informed machine learning. Nature Reviews Physics , 3(6):422â€“440, 2021. Kovachki, N., Li, Z., Liu, B., Azizzadenesheli, K., Bhat-tacharya, K., Stuart, A., and Anandkumar, A. Neural operator: Learning maps between function spaces with applications to pdes. Journal of Machine Learning Re-search , 24(89):1â€“97, 2023. Kramer, B., Peherstorfer, B., and Willcox, K. E. Learning nonlinear reduced models from data with operator infer-ence. Annual Review of Fluid Mechanics , 56(1):521â€“548, 2024. Li, S., Marwah, T., Shen, J., Sun, W., Risteski, A., Yang, Y., and Talwalkar, A. Codepde: An inference frame-work for llm-driven pde solver generation. arXiv preprint arXiv:2505.08783 , 2025. Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhat-tacharya, K., Stuart, A., and Anandkumar, A. Fourier neural operator for parametric partial differential equa-tions. arXiv preprint arXiv:2010.08895 , 2020. Li, Z., Wu, Y., Li, Z., Wei, X., Zhang, X., Yang, F., and Ma, X. Autoformalize mathematical statements by symbolic equivalence and semantic consistency. Advances in Neu-ral Information Processing Systems , 37:53598â€“53625, 2024a. Li, Z., Zheng, H., Kovachki, N., Jin, D., Chen, H., Liu, B., Azizzadenesheli, K., and Anandkumar, A. Physics-informed neural operator for learning partial differential equations. ACM/JMS Journal of Data Science , 1(3):1â€“27, 2024b. Liu, J., Zhu, R., Xu, J., Ding, K., Zhang, X.-Y., Meng, G., and Liu, C.-L. Pde-agent: A toolchain-augmented multi-agent framework for pde solving. arXiv preprint arXiv:2512.16214 , 2025. Liu, X.-Y., Zhu, M., Lu, L., Sun, H., and Wang, J.-X. Multi-resolution partial differential equations preserved learning framework for spatiotemporal dynamics. Communica-tions Physics , 7(1):31, 2024a. Liu, Y., Sun, J., He, X., Pinney, G., Zhang, Z., and Schaeffer, H. Prose-fd: A multimodal pde foundation model for learning multiple operators for forecasting fluid dynamics. 

arXiv preprint arXiv:2409.09811 , 2024b. Lorsung, C. and Farimani, A. B. Explain like iâ€™m five: Using llms to improve pde surrogate models with text. 

arXiv preprint arXiv:2410.01137 , 2024. Lu, L., Jin, P., and Karniadakis, G. E. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of opera-tors. arXiv preprint arXiv:1910.03193 , 2019. Lu, L., Jin, P., Pang, G., Zhang, Z., and Karniadakis, G. E. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature machine intelligence , 3(3):218â€“229, 2021. Lu, L., Meng, X., Cai, S., Mao, Z., Goswami, S., Zhang, Z., and Karniadakis, G. E. A comprehensive and fair comparison of two neural operators (with practical exten-sions) based on fair data. Computer Methods in Applied Mechanics and Engineering , 393:114778, 2022. McQuarrie, S. A., Huang, C., and Willcox, K. E. Data-driven reduced-order models via regularised operator in-ference for a single-injector combustion process. Journal of the Royal Society of New Zealand , 51(2):194â€“211, 2021. McQuarrie, S. A., Khodabakhshi, P., and Willcox, K. E. Nonintrusive reduced-order models for parametric partial differential equations via data-driven operator inference. 

SIAM Journal on Scientific Computing , 45(4):A1917â€“ A1946, 2023. Mensfelt, A., Cucala, D. T., Franco, S., Koutsoukou-Argyraki, A., Trencsenyi, V., and Stathis, K. Towards a common framework for autoformalization. arXiv preprint arXiv:2509.09810 , 2025. Negrini, E., Liu, Y., Yang, L., Osher, S. J., and Schaef-fer, H. A multimodal pde foundation model for pre-diction and scientific text descriptions. arXiv preprint arXiv:2502.06026 , 2025. Nejjar, M., Zacharias, L., Stiehle, F., and Weber, I. Llms for science: Usage for code generation and data analysis. 

Journal of Software: Evolution and Process , 37(1):e2723, 2025. 10 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

Nohra, M. and Dufour, S. Physics-informed neural net-works for the numerical modeling of steady-state and transient electromagnetic problems with discontinuous media. arXiv preprint arXiv:2406.04380 , 2024. Oommen, V., Shukla, K., Desai, S., Dingreville, R., and Karniadakis, G. E. Rethinking materials simulations: Blending direct numerical simulations with neural opera-tors. npj Computational Materials , 10(1):145, 2024. Pathak, J., Subramanian, S., Harrington, P., Raja, S., Chattopadhyay, A., Mardani, M., Kurth, T., Hall, D., Li, Z., Azizzadenesheli, K., et al. Fourcastnet: Aglobal data-driven high-resolution weather model us-ing adaptive fourier neural operators. arXiv preprint arXiv:2202.11214 , 2022. Peherstorfer, B. and Willcox, K. Data-driven operator infer-ence for nonintrusive projection-based model reduction. 

Computer Methods in Applied Mechanics and Engineer-ing , 306:196â€“215, 2016. Qian, E., Kramer, B., Peherstorfer, B., and Willcox, K. Lift & learn: Physics-informed machine learning for large-scale nonlinear dynamical systems. Physica D: Nonlinear Phenomena , 406:132401, 2020. Qian, E., Farcas, I.-G., and Willcox, K. Reduced operator in-ference for nonlinear partial differential equations. SIAM Journal on Scientific Computing , 44(4):A1934â€“A1959, 2022. Raissi, M., Perdikaris, P., and Karniadakis, G. E. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics , 378:686â€“707, 2019. Rautela, M. S., Most, A., Mansingh, S., Love, B. C., Biswas, A., Oyen, D., and Lawrence, E. Morph: Pde founda-tion models with arbitrary data modality. arXiv preprint arXiv:2509.21670 , 2025. Sawant, N., Kramer, B., and Peherstorfer, B. Physics-informed regularization and structure preservation for learning stable reduced models from data with operator inference. Computer Methods in Applied Mechanics and Engineering , 404:115836, 2023. Shen, J., Marwah, T., and Talwalkar, A. Ups: Efficiently building foundation models for pde solving via cross-modal adaptation. arXiv preprint arXiv:2403.07187 ,2024. Shojaee, P., Meidani, K., Gupta, S., Farimani, A. B., and Reddy, C. K. Llm-sr: Scientific equation discovery via programming with large language models. arXiv preprint arXiv:2404.18400 , 2024. Soroco, M., Song, J., Xia, M., Emond, K., Sun, W., and Chen, W. Pde-controller: Llms for autoformalization and reasoning of pdes. arXiv preprint arXiv:2502.00963 ,2025. Sun, J., Liu, Y., Zhang, Z., and Schaeffer, H. Towards a foundation model for partial differential equations: Mul-tioperator learning and extrapolation. Physical Review E ,111(3):035304, 2025. Swischuk, R., Kramer, B., Huang, C., and Willcox, K. Learning physics-based reduced-order models for a single-injector combustion process. AIAA Journal , 58(6): 2658â€“2672, 2020. Takamoto, M., Praditia, T., Leiteritz, R., MacKinlay, D., Alesiani, F., Pfl Â¨uger, D., and Niepert, M. Pdebench: An extensive benchmark for scientific machine learning. 

Advances in Neural Information Processing Systems , 35: 1596â€“1611, 2022. Takamoto, M., Alesiani, F., and Niepert, M. Learning neu-ral pde solvers with parameter-guided channel attention. In International Conference on Machine Learning , pp. 33448â€“33467. PMLR, 2023. Weng, K., Du, L., Li, S., Lu, W., Sun, H., Liu, H., and Zhang, T. Autoformalization in the era of large language models: A survey. arXiv preprint arXiv:2505.23486 ,2025. Wiesner, F., Wessling, M., and Baek, S. Towards a physics foundation model. arXiv preprint arXiv:2509.13805 ,2025. Wu, H., Zhang, X., and Zhu, L. Automated code develop-ment for pde solvers using large language models. arXiv preprint arXiv:2509.25194 , 2025. Wu, Y., Jiang, A. Q., Li, W., Rabe, M., Staats, C., Jam-nik, M., and Szegedy, C. Autoformalization with large language models. Advances in neural information pro-cessing systems , 35:32353â€“32368, 2022. Xu, H., Chen, Y., Cao, R., Tang, T., Du, M., Li, J., Callaghan, A. H., and Zhang, D. Generative discovery of partial differential equations by learning from math hand-books. Nature Communications , 16(1):10255, 2025a. Xu, J., Fei, H., Pan, L., Liu, Q., Lee, M.-L., and Hsu, W. Faithful logical reasoning via symbolic chain-of-thought. 

arXiv preprint arXiv:2405.18357 , 2024. Xu, W., Huang, C., Gao, S., and Shang, S. Llm-based agents for tool learning: A survey: W. xu et al. Data Science and Engineering , pp. 1â€“31, 2025b. 11 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

Yang, L., Liu, S., and Osher, S. J. Fine-tune language mod-els as multi-modal differential equation solvers. Neural Networks , pp. 107455, 2025. Ye, Z., Huang, X., Chen, L., Liu, H., Wang, Z., and Dong, B. Pdeformer: Towards a foundation model for one-dimensional partial differential equations. arXiv preprint arXiv:2402.12652 , 2024. Zhou, A., Li, Z., Schneier, M., Buchanan Jr, J. R., and Farimani, A. B. Text2pde: Latent diffusion mod-els for accessible physics simulation. arXiv preprint arXiv:2410.01153 , 2024. Zhou, H., Ma, Y., Wu, H., Wang, H., and Long, M. Uni-solver: Pde-conditional transformers are universal neural pde solvers. In ICLR 2025 Workshop on Foundation Models in the Wild , 2025. Zhu, M., Sun, J., Zhang, Z., Schaeffer, H., and Lu, L. Pi-mfm: Physics-informed multimodal foundation model for solving partial differential equations. arXiv preprint arXiv:2512.23056 , 2025. 12 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

# A. Experiment Details 

Below we provide additional details on input signal generation, numerical discretization, dataset construction, and simulation settings for the PDE systems described in Section 4.1. 

A.1. Heat Equation 

The initial condition for the heat equation is fixed across all trajectories and given by 

g(x) = exp  Î±(x âˆ’ 1)  + exp( âˆ’Î±x ) âˆ’ exp( âˆ’Î±), Î± = 100 . (20) The Dirichlet boundary input u(t) applied at both ends of the domain is constructed as a three-component multi-sine signal 

u(t) = b +

> 3

X

> k=1

Ak sin(2 Ï€f kt + Ï•k) , (21) where, for each trajectory, the amplitudes Ak âˆ¼ U (0 .1, 0.5) , frequencies fk âˆ¼ U (0 .2, 5.0) , phases Ï•k âˆ¼ U (0 , 2Ï€), and bias 

b âˆ¼ U (0 .8, 1.2) are sampled independently. Independent realizations of the boundary input are used for training and test datasets. The spatial domain x âˆˆ [0 , 1] is discretized using 1023 uniformly spaced interior points, and the second-order central finite difference scheme is used to generate full-order PDE solution. Training trajectories are sampled at 1001 uniformly spaced time instances over the horizon T = 1 .0, with BDF integrator. For evaluation, trajectories are generated over the extended horizon Ttest = 2 .0 using 2001 time steps. 

A.2. Burgersâ€™ Equation 

The Burgersâ€™ equation includes an internal source term s(x, t ) with a fixed spatial profile and a time-dependent amplitude. The spatial component of the source is defined as 

s0(x) = cosh 

 x âˆ’ 0.50.05 

âˆ’1

, (22) and the full source term is given by 

s(x, t ) = s0(x) Ïƒ(t), (23) where the temporal modulation Ïƒ(t) is generated as a single-component sinusoidal signal 

Ïƒ(t) = bs + As sin(2 Ï€f st + Ï•s) . (24) The parameters are sampled independently for each trajectory according to As âˆ¼ U (0 .1, 3.0) , fs âˆ¼ U (0 .2, 1.0) , Ï•s âˆ¼U(0 , 2Ï€), and bs âˆ¼ U (âˆ’1.0, 1.0) .The system is subject to time-dependent Dirichlet boundary conditions 

y(0 , t ) = u1(t), y(1 , t ) = u2(t), (25) where the boundary inputs u1(t) and u2(t) are generated independently as three-component multi-sine signals of the form 

ui(t) = bi +

> 3

X

> k=1

Ai,k sin(2 Ï€f i,k t + Ï•i,k ), i âˆˆ { 1, 2}. (26) For each trajectory, the amplitudes Ai,k âˆ¼ U (0 .1, 1.0) , frequencies fi,k âˆ¼ U (0 .2, 5.0) , phases Ï•i,k âˆ¼ U (0 , 2Ï€), and biases 

bi âˆ¼ U (âˆ’0.5, 0.5) are sampled independently. The initial condition is constructed to be smooth and consistent with the boundary values at t = 0 :

y(x, 0) = u2(0) + 12

 u1(0) âˆ’ u2(0)  

1 âˆ’ tanh 

 x âˆ’ 0.30.1

 

. (27) 13 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

The spatial discretization uses Chebyshev collocation with N = 64 interior points. For evaluation and visualization, the solution is interpolated onto a uniform grid with 1001 points. Training trajectories span the time horizon T = 2 .0 with 

1001 time steps, while test trajectories span Ttest = 4 .0 with 2001 time steps. All source and boundary signals are sampled independently across trajectories. RK45 integrator is used for full-order solution generation. 

A.3. 2D Lid-Driven Cavity Flow 

The no-slip boundary conditions enforce zero velocity on all walls except the top lid. The horizontal velocity on the top boundary is prescribed as 

v1(x, t ) = h(p1) u(t), v2(x, t ) = 0 , (28) where the fixed spatial profile is 

h(p1) = 1 + 0 .3 sin(2 Ï€p 1) + 0 .2 p1. (29) The temporal modulation u(t) is generated as a single-sine signal 

u(t) = b + A sin(2 Ï€f t + Ï•), (30) where b âˆ¼ U (0 .7, 1.3) , A âˆ¼ U (0 .1, 0.4) , f âˆ¼ U (0 .5, 2.0) , and Ï• âˆ¼ U (0 , 2Ï€) are sampled independently for each trajectory. The cavity domain is defined on x = ( p1, p 2) âˆˆ [0 , 1] 2 and discretized using a uniform grid with 32 interior points in each spatial direction, resulting in a (34 Ã— 34) grid including boundary points. The full-order solver uses a fixed time step 

âˆ†t = 10 âˆ’3, with second-order finite difference scheme in space and forward Euler temporal integration. Training data are collected over the horizon T = 2 .0, with solution snapshots recorded every 0.02 time units, yielding 101 snapshots per trajectory. For evaluation, trajectories are simulated over Ttest = 4 .0 using the same solver and snapshot spacing. 

A.4. Dataset Construction 

For all PDE systems, training and test datasets are generated using independent random seeds for boundary and forcing inputs. Each dataset consists of multiple trajectories corresponding to different realizations of input signals and, where applicable, different physical parameter values. All reported results are obtained using identical datasets across methods to ensure fair comparison. Numerical solvers, discretization schemes, and time-stepping parameters are fixed across training and testing, and no test-time information is used during model training. 

# B. Implementation Details 

B.1. CodePDE 

We evaluate CodePDE (Li et al., 2025) using 5 rounds of solver generation for each PDE, with 5 debug trials for each round. The test performance is measured using the relative L2 error over the full time horizon. The best solver for each equation is reported for prediction errors. Success rate is calculated based on trial-level failures. The prompts and solver templates for all equations are shown below. 

Prompts:                                                                         

> heat_description =â€™â€™â€™ The PDE is the 1D heat equation with time-varying Dirichlet boundary conditions, given by \\[ \\begin{{cases}} \\partial_t u(t, x) =\\nu \\partial_{{xx}} u(t, x), &x\\in (0,1), \; t\\in (0,T] \\\\ u(0, x) =u_0(x), &x\\in (0,1) \\\\ u(t, 0) =u_{{bc}}(t), &t\\in (0,T] \\\\ u(t, 1) =u_{{bc}}(t), &t\\in (0,T] \\end{{cases}} \\] where $\\nu$ is the thermal diffusivity coefficient. **Note that the boundary conditions are symmetric** (same function at both boundaries) and time-varying.

14 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

**Initial Condition (Fixed)**: The initial condition is a fixed exponential profile given by \\[ u_0(x) = eË†{{100(x-1)}} + eË†{{-100x}} - eË†{{-100}} \\] This creates steep gradients near the boundaries with approximate zero values at both ends. **Boundary Conditions (Time-varying)**: The boundary value $u_{{bc}}(t)$ is provided as an input array of shape [batch_size, T+1] containing the boundary values at each time step. The same values apply to both $x=0$ and $x=1$ (symmetric boundaries). Given the discretization of the boundary condition values $u_{{bc}}(t)$ of shape [batch_size, T+1], you need to implement a solver that: 1. Computes the fixed initial condition for the given spatial grid 2. Solves the heat equation forward in time while enforcing the time-varying boundary conditions 3. Returns the solution of shape [batch_size, T+1, N] where N=1023 (interior spatial points) In particular, your code should be tailored to the case where $\\nu={opinf_heat_nu}$, i.e., optimizing it particularly for this use case. **Important implementation notes**: - Use **1023 interior spatial points** (not including boundaries at x=0 and x=1) - The domain is $x \\in [0,1]$ with $dx \\approx 9.766 \\times 10Ë†{{-4}}$ - Time horizon is $T=1.0$ with 1001 time points ($dt = 10Ë†{{-3}}$) - The boundary conditions must be enforced at every time step - Consider using implicit methods (e.g., BDF, Crank-Nicolson) for stability - Use finite difference for spatial discretization (2nd order central differences recommended) â€™â€™â€™ burgers_description = â€™â€™â€™ The PDE is the 1D Burgers equation with asymmetric time-varying Dirichlet boundary conditions and a localized source term, given by \\[ \\begin{{cases}} \\partial_t u(t, x) + u(t, x) \\partial_x u(t, x) = \\nu \\partial_{{xx}} u(t, x) + s(x) \\cdot w_3(t), & x \\in (0,1), \; t \\in (0,T] \\\\ u(0, x) = u_0(x), & x \\in (0,1) \\\\ u(t, 0) = w_1(t), & t \\in (0,T] \\\\ u(t, 1) = w_2(t), & t \\in (0,T] \\end{{cases}} \\] where $\\nu$ is the viscosity coefficient, and we have **three time-varying input functions** provided as arrays: - $w_1(t)$: Left boundary condition (shape [batch_size, T+1]) - $w_2(t)$: Right boundary condition (shape [batch_size, T+1]) - $w_3(t)$: Source term temporal modulation (shape [batch_size, T+1]) **Initial Condition (Variable)**: The initial condition depends on the boundary values at $t=0$ and must be computed: \\[ u_0(x) = w_2(0) + 0.5(w_1(0) - w_2(0))(1 - \\tanh((x - 0.3)/0.1)) \\] This creates a smooth transition from the left BC to the right BC, centered at $x=0.3$ with transition width $0.1$. **Source Term (Localized)**: The spatial source profile is a narrow bell-shaped function centered at $x=0.5$: 

15 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

\\[ s(x) = \\text{{sech}}Ë†2\\left(\\frac{{x - 0.5}}{{0.05}}\\right) =\\frac{{1}}{{\\coshË†2\\left(\\frac{{x - 0.5}}{{0.05}}\\right)}} \\] The full source term is $s(x) \\cdot w_3(t)$, where $w_3(t)$ modulates the source strength in time (element-wise multiplication for each spatial point). **Boundary Conditions (Asymmetric, Time-varying)**: The left and right boundaries have **independent** time-varying values $w_1(t) \\neq w_2(t)$. Given the discretization of the boundary functions $w_1(t)$, $w_2(t)$, and source modulation $w_3(t)$ (each of shape [batch_size, T+1]), you need to implement a solver that: 1. Computes the initial condition from $w_1(0)$ and $w_2(0)$ 2. Evaluates the spatial source profile $s(x)$ on the grid 3. Solves the Burgers equation forward in time with boundary conditions and source term 4. Returns the solution of shape [batch_size, T+1, N] where N is the number of spatial grid points In particular, your code should be tailored to the case where $\\nu={opinf_burgers_nu}$, i.e., optimizing it particularly for this use case. **Important implementation notes**: - Spatial discretization: 1001 points uniformly spaced in [0,1] (including boundaries) or use spectral methods - Time horizon is $T=2.0$ with 1001 time points ($dt = 2 \\times 10Ë†{{-3}}$) - The source term is localized at the center ($x=0.5$) with very narrow width (0.05) - Boundary conditions are **asymmetric**: $w_1(t) \\neq w_2(t)$ - The nonlinear convection term $u \\partial_x u$ can cause shocks/steep gradients at low viscosity - Consider using explicit methods (e.g., RK45) or spectral methods for accuracy - Pay special attention to handling the localized source term accurately â€™â€™â€™ cavity_description = â€™â€™â€™ The PDE is the 2D lid-driven cavity flow in vorticity-streamfunction form: \\[ \\omega_t + u \\omega_x + v \\omega_y = \\frac{{1}}{{Re}}(\\omega_{{xx}} +\\omega_{{yy}}), \\quad (x, y) \\in (0,1)Ë†2 \\] \\[ \\Delta \\psi = -\\omega, \\quad u = \\psi_y, \\quad v = -\\psi_x \\] Boundary conditions are no-slip on all walls. The top lid has a time-varying velocity: \\[ u_{{lid}}(x, t) = a(x) f(t), \\quad v=0 \\] where the fixed spatial profile is: \\[ a(x) = 1 + 0.3\\sin(2\\pi x) + 0.2x \\] and the input is a scalar function f(t). Given the discretization of f(t) as an input array of shape [batch_size, T+1], you need to implement a solver that returns the vorticity field \\omega(t, x, y) for all times. The solution should be of shape [batch_size, T+1, N, N] where N=34 (32 interior points plus boundaries). In particular, your code should be tailored to the case where Re={opinf_cavity_re}. Important implementation notes: - Initial condition: \\omega(x, y, 0) = 0 and \\psi(x, y, 0) = 0

16 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

- Spatial grid: 34x34 uniform grid on [0, 1]Ë†2 - Time step in the full-order model is dt = 0.001 with snapshots every 0.02 - Use finite differences for spatial derivatives and a Poisson solver for \\psi - Consider small internal time steps for stability â€™â€™â€™ 

Solver templates:  

> 12

import numpy as np  

> 34

def solver(u_bc, t_coordinate, nu):  

> 5

"""Solves the 1D heat equation with time-varying Dirichlet boundary conditions.  

> 67

Args:  

> 8

u_bc (np.ndarray): Boundary condition values at both x=0 and x=1  

> 9

Shape: [batch_size, T+1] where T+1 is the number of time points.  

> 10

The same boundary value applies to both boundaries (symmetric).  

> 11

t_coordinate (np.ndarray): Time coordinates of shape [T+1].  

> 12

It begins with t_0=0 and follows the time steps t_1, ..., t_T.  

> 13

nu (float): Thermal diffusivity coefficient.  

> 14 15

Returns:  

> 16

solutions (np.ndarray): Shape [batch_size, T+1, N] where N=1023 (interior points).  

> 17

solutions[:, 0, :] contains the initial conditions (fixed exponential profile) , 

> 18

solutions[:, i, :] contains the solutions at time t_coordinate[i].  

> 19

"""  

> 20

# TODO: Implement the solver for the heat equation with time-varying BCs  

> 21

# 

> 22

# Key implementation steps:  

> 23

# 1. Create spatial grid: N=1023 interior points in [0, 1]  

> 24

# x = linspace(0, 1, 1025)[1:-1] # Interior points only  

> 25

# dx = 1/1024  

> 26

# 

> 27

# 2. Compute fixed initial condition:  

> 28

# u0 = exp(100*(x-1)) + exp(-100*x) - exp(-100)  

> 29

# 

> 30

# 3. Set up finite difference matrix for dË†2/dxË†2 (2nd order central)  

> 31

# 

> 32

# 4. Time integration loop:  

> 33

# - Enforce boundary conditions at each time step  

> 34

# - Solve: du/dt = nu * dË†2 u/dxË†2  

> 35

# - Consider using implicit methods (BDF, Crank-Nicolson) for stability  

> 36

# 

> 37

# Hints:  

> 38

# - Use scipy.integrate.solve_ivp with method=â€™BDFâ€™ or implement implicit scheme  

> 39

# - Consider using PyTorch or JAX for GPU acceleration  

> 40

# - Remember that u_bc has shape [batch_size, T+1] with same values for both boundaries  

> 41 42

return solutions  

> 43 44 45 46

def solver(w1_bc, w2_bc, source, t_coordinate, nu):  

> 47

"""Solves the 1D Burgers equation with asymmetric time-varying Dirichlet BCs and source term.  

> 48 49

Args:  

> 50

w1_bc (np.ndarray): Left boundary condition values at x=0  

> 51

Shape: [batch_size, T+1] where T+1 is the number of time points.  

> 52

w2_bc (np.ndarray): Right boundary condition values at x=1  

> 53

Shape: [batch_size, T+1]. Note: w1 \neq w2 (asymmetric boundaries).  

> 54

source (np.ndarray): Source term temporal modulation w3(t) 

17 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference  

> 55

Shape: [batch_size, T+1]. This modulates the spatial source profile s(x).  

> 56

t_coordinate (np.ndarray): Time coordinates of shape [T+1].  

> 57

It begins with t_0=0 and follows the time steps t_1, ..., t_T.  

> 58

nu (float): Viscosity coefficient.  

> 59 60

Returns:  

> 61

solutions (np.ndarray): Shape [batch_size, T+1, N] where N is spatial grid points.  

> 62

solutions[:, 0, :] contains the initial conditions (computed from w1(0), w2(0) ),  

> 63

solutions[:, i, :] contains the solutions at time t_coordinate[i].  

> 64

"""  

> 65

# TODO: Implement the solver for Burgers equation with BCs and source term  

> 66

# 

> 67

# Key implementation steps:  

> 68

# 1. Create spatial grid: N=1001 points (including boundaries) or use spectral  

> 69

# x = linspace(0, 1, 1001) for uniform grid  

> 70

# dx = 1/1000  

> 71

# 

> 72

# 2. Compute spatial source profile s(x):  

> 73

# s_x = 1 / coshË†2((x - 0.5) / 0.05)  

> 74

# This is a narrow bell-shaped function centered at x=0.5  

> 75

# 

> 76

# 3. Compute initial condition for each batch item:  

> 77

# u0 = w2_bc[:, 0] + 0.5 * (w1_bc[:, 0] - w2_bc[:, 0]) * (1 - tanh((x - 0.3) / 0.1) ) 

> 78

# Note: u0 depends on boundary values at t=0  

> 79

# 

> 80

# 4. Time integration loop:  

> 81

# - Enforce boundary conditions at each time step:  

> 82

# u[:, 0] = w1_bc[:, t_idx] (left)  

> 83

# u[:, -1] = w2_bc[:, t_idx] (right)  

> 84

# - Solve: du/dt + u*du/dx = nu*dË†2 u/dxË†2 + s(x)*source[:, t_idx]  

> 85

# - Consider using RK45 or spectral methods for accuracy  

> 86

# 

> 87

# Hints:  

> 88

# - The source term is localized (narrow width 0.05) - handle carefully  

> 89

# - Boundaries are asymmetric: w1(t) \neq w2(t)  

> 90

# - Consider using scipy.integrate.solve_ivp with method=â€™RK45â€™  

> 91

# - For spectral methods, use Chebyshev collocation  

> 92

# - Nonlinear term u*du/dx can cause shocks at low viscosity  

> 93 94

return solutions  

> 95 96 97

def solver(lid_bc, t_coordinate, re):  

> 98

"""Solves 2D lid-driven cavity flow in vorticity-streamfunction form.  

> 99 100

Args:  

> 101

lid_bc (np.ndarray): Scalar lid velocity modulation f(t).  

> 102

Shape: [batch_size, T+1]. The top-wall velocity is a(x) * f(t),  

> 103

where a(x) is a fixed spatial profile defined in the spec.  

> 104

t_coordinate (np.ndarray): Time coordinates of shape [T+1].  

> 105

It begins with t_0=0 and follows the time steps t_1, ..., t_T.  

> 106

re (float): Reynolds number.  

> 107 108

Returns:  

> 109

solutions (np.ndarray): Shape [batch_size, T+1, N, N] where N=34 (includes boundaries).  

> 110

solutions[:, 0, :, :] contains the initial vorticity (zeros),  

> 111

solutions[:, i, :, :] contains the vorticity at time t_coordinate[i].  

> 112

"""  

> 113

# TODO: Implement the cavity flow solver in vorticity-streamfunction form.  

> 114

# 

> 115

# Key implementation steps:  

> 116

# 1. Create spatial grid: N=34 (32 interior + 2 boundaries) in [0, 1] x [0, 1] 

18 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference  

> 117

# 2. Initialize vorticity omega = 0 and streamfunction psi = 0 

> 118

# 3. Enforce no-slip boundaries; top lid velocity: u_lid(x, t) = a(x) * f(t)  

> 119

# with a(x) = 1 + 0.3*sin(2*pi*x) + 0.2*x  

> 120

# 4. Time integration loop:  

> 121

# - Solve Poisson: Laplacian(psi) = -omega  

> 122

# - Compute u = psi_y, v = -psi_x  

> 123

# - Advance omega with advection-diffusion: omega_t + u*omega_x + v*omega_y = (1/re ) * Laplacian(omega)  

> 124

# 5. Return vorticity snapshots over time  

> 125

# 

> 126

# Hints:  

> 127

# - Use finite differences for spatial derivatives (2nd order central)  

> 128

# - Consider an explicit time step (dt <= 1e-3) for stability  

> 129

# - Use a fast Poisson solver (e.g., FFT or sparse solve) for psi  

> 130 131

return solutions 

B.2. MOL-LLM 

We implement MOL-LLM (Negrini et al., 2025) based on the original implementation, with extensions to support time-varying boundary conditions and 2D PDEs. The model adopts a GPT-2â€“style transformer backbone that processes a concatenation of natural-language PDE descriptions and a parallel numeric token stream encoding the initial state, PDE coefficients, and boundary condition (BC) parameters. In addition to the standard coefficient embedding (e.g., viscosity Î½

or Reynolds number Re ), we add the 34-dimensional BC parameter vector that encodes multi-sine boundary or forcing signals in (21) , (26) , (30) (zero-filled when inactive). This vector is embedded by a dedicated MLP into the same token space and treated as an additional metadata token so the transformer can explicitly attend to BC information. Training is performed over a fixed horizon [0 , T ] with tlen = 128 time points, and solution values are produced by a time-query head conditioned on the initial state and metadata tokens. For the 2D lid-driven cavity equation, the vorticity field is flattened to a one-dimensional vector and downsampled to 128 spatial points to match the modelâ€™s maximum numeric dimension. Predictions are interpolated back to the original 34 Ã— 34 grid using inverse-distance weighting for full-order model comparison and visualization. The combined transformer input consists of GPT-2 tokenized text (augmented with number and padding tokens) concatenated with numeric tokens corresponding to the initial state, coefficient vector, and the BC token, enabling unified attention over symbolic PDE descriptions and structured numeric conditioning. The text descriptions for each equation used for training is below. 

Text Descriptions: 

{"type": 60, "text": [["The parametric heat equation is ut = nu * uxx with time-varying boundary conditions and varying viscosity. We have initial condition ="], ["and viscosity coefficient nu ="], ["This equation models diffusion with time-varying boundary forcing."]]} {"type": 60, "text": [["The parametric heat equation is ut = nu * uxx with time-varying boundary conditions and varying viscosity. We have initial condition ="], ["and viscosity coefficient nu ="], ["The viscosity parameter controls the rate of heat diffusion in the domain."]]} {"type": 60, "text": [["The parametric heat equation is ut = nu * uxx with time-varying boundary conditions and varying viscosity. We have initial condition ="], ["and viscosity coefficient nu ="], ["This formulation captures 1D heat transport with parametric viscosity."]]} {"type": 60, "text": [["The parametric heat equation is ut = nu * uxx with time-varying boundary conditions and varying viscosity. We have initial condition ="], ["and viscosity coefficient nu ="], ["The solution evolves under diffusive dynamics driven by boundary inputs."]]} {"type": 60, "text": [["The parametric heat equation is ut = nu * uxx with time-varying boundary conditions and varying viscosity. We have initial condition ="], ["and viscosity coefficient nu ="], ["This case studies diffusion with parametric viscosity and time-varying BCs."]]} {"type": 61, "text": [["The parametric Burgers equation is ut + u*ux = nu*uxx +s(x)*w3(t) with time-varying boundary conditions. We have initial condition ="], ["and viscosity coefficient nu ="], ["This equation models nonlinear advection-diffusion with a time-dependent forcing."]]} 

19 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

{"type": 61, "text": [["The parametric Burgers equation is ut + u*ux = nu*uxx +s(x)*w3(t) with time-varying boundary conditions. We have initial condition ="], ["and viscosity coefficient nu ="], ["The viscosity controls shock smoothing in the nonlinear transport dynamics."]]} {"type": 61, "text": [["The parametric Burgers equation is ut + u*ux = nu*uxx +s(x)*w3(t) with time-varying boundary conditions. We have initial condition ="], ["and viscosity coefficient nu ="], ["This setup couples boundary driving and interior forcing in 1D flow."]]} {"type": 61, "text": [["The parametric Burgers equation is ut + u*ux = nu*uxx +s(x)*w3(t) with time-varying boundary conditions. We have initial condition ="], ["and viscosity coefficient nu ="], ["Nonlinear advection competes with diffusion and time-varying inputs."]]} {"type": 61, "text": [["The parametric Burgers equation is ut + u*ux = nu*uxx +s(x)*w3(t) with time-varying boundary conditions. We have initial condition ="], ["and viscosity coefficient nu ="], ["This equation demonstrates driven viscous Burgers dynamics."]]} {"type": 62, "text": [["The 2D lid-driven cavity flow in vorticity-streamfunction formulation omega_t + u*omega_x + v*omega_y = (1/Re)*(omega_xx + omega_yy) models incompressible viscous flow in a square cavity with time-varying spatially-varying lid velocity. We have initial condition ="], ["and Reynolds number Re ="], ["The Reynolds number controls the relative importance of inertial versus viscous effects in the flow."]]} {"type": 62, "text": [["The 2D lid-driven cavity flow in vorticity-streamfunction formulation omega_t + u*omega_x + v*omega_y = (1/Re)*(omega_xx + omega_yy) models incompressible viscous flow in a square cavity with time-varying spatially-varying lid velocity. We have initial condition ="], ["and Reynolds number Re ="], ["This equation captures cavity circulation driven by a moving lid."]]} {"type": 62, "text": [["The 2D lid-driven cavity flow in vorticity-streamfunction formulation omega_t + u*omega_x + v*omega_y = (1/Re)*(omega_xx + omega_yy) models incompressible viscous flow in a square cavity with time-varying spatially-varying lid velocity. We have initial condition ="], ["and Reynolds number Re ="], ["Higher Reynolds number increases inertial effects in the cavity flow."]]} {"type": 62, "text": [["The 2D lid-driven cavity flow in vorticity-streamfunction formulation omega_t + u*omega_x + v*omega_y = (1/Re)*(omega_xx + omega_yy) models incompressible viscous flow in a square cavity with time-varying spatially-varying lid velocity. We have initial condition ="], ["and Reynolds number Re ="], ["This setup models 2D vortical dynamics with time-varying lid motion."]]} {"type": 62, "text": [["The 2D lid-driven cavity flow in vorticity-streamfunction formulation omega_t + u*omega_x + v*omega_y = (1/Re)*(omega_xx + omega_yy) models incompressible viscous flow in a square cavity with time-varying spatially-varying lid velocity. We have initial condition ="], ["and Reynolds number Re ="], ["The vorticity-streamfunction form enforces incompressibility in the cavity."]]} 

B.3. LLMs 

We directly prompt large language models with the governing PDE, full initial and boundary conditions, external inputs, and a coarse output grid for tractability (heat/Burgers: 16 spatial points Ã— 41 time steps; cavity: 6 Ã— 6 spatial points Ã— 21 time steps), and ask it to return the full spatiotemporal state array in JSON format (3-decimal precision) for a single parameter setting and trajectory. The LLM is treated as a black-box simulator, and its predicted field is compared against downsampled ground truth data using the relative L2 error. For each parameter instance and equation, the LLM is queried 10 times, and the averaged error over all successful cases is reported. Example prompts are provided below. 

Prompts: 

Heat: You are given a PDE, full IC/BC specs, parameter values, and input signals. Return ONLY a valid JSON object with numeric arrays at 3-decimal precision. No extra text, no code fences. Task: Heat equation u_t = nu * u_xx on x in [0,1].\nBoundary conditions: u(0,t)=u(1,t)= u_bc(t).\nParameter: nu = 0.5.\nOutput grid: 16 spatial points, 41 time steps.\ nReturn JSON with fields: equation, param, grid_shape, t_steps, x, t, u0, u.\nAll 

20 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

numeric values must be rounded to 3 decimals.\nx = [0.001,0.067,0.134,0.200,0.268, 0.334,0.400,0.467,0.533,0.600,0.666,0.732,0.800,0.866,0.933,0.999]\nt = [0.000,0.050 ,0.100,0.150,0.200,0.250,0.300,0.350,0.400,0.450,0.500,0.550,0.600,0.650,0.700,0.750 ,0.800,0.850,0.900,0.950,1.000,1.050,1.100,1.150,1.200,1.250,1.300,1.350,1.400,1.450 ,1.500,1.550,1.600,1.650,1.700,1.750,1.800,1.850,1.900,1.950,2.000]\nu0(x) = [0.581, 0.001,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.001, 0.581]\nu_bc(t) = [0.581,0.762,0.783,0.686,0.648,0.818,1.184,1.564,1.759,1.705,1.527 ,1.418,1.473,1.605,1.625,1.409,1.019,0.655,0.498,0.565,0.705,0.736,0.609,0.452,0.456 ,0.709,1.109,1.442,1.558,1.483,1.391,1.440,1.632,1.804,1.773,1.491,1.099,0.810,0.737 ,0.812,0.848]\nJSON schema:\n{"equation":"heat","param":{"nu":...},"grid_shape":[16] ,"t_steps":41,"x":[...],"t":[...],"u0":[...],"u":[[...]]}\n Burgers: You are given a PDE, full IC/BC specs, parameter values, and input signals. Return ONLY a valid JSON object with numeric arrays at 3-decimal precision. No extra text, no code fences. Task: Burgers equation u_t + u*u_x = nu*u_xx + s(x)*w3(t), x in [0,1].\nBoundary: u(0,t) =w1(t), u(1,t)=w2(t).\nForcing shape: s(x)=cosh((x-0.5)/0.05)Ë†(-1).\nParameter: nu = 0.03.\nOutput grid: 16 spatial points, 41 time steps.\nReturn JSON with fields: equation, param, grid_shape, t_steps, x, t, u0, w1, w2, w3, u.\nAll numeric values must be rounded to 3 decimals.\nx = [0.000,0.067,0.133,0.200,0.267,0.333,0.400,0.467 ,0.533,0.600,0.667,0.733,0.800,0.867,0.933,1.000]\nt = [0.000,0.100,0.200,0.300, 0.400,0.500,0.600,0.700,0.800,0.900,1.000,1.100,1.200,1.300,1.400,1.500,1.600,1.700, 1.800,1.900,2.000,2.100,2.200,2.300,2.400,2.500,2.600,2.700,2.800,2.900,3.000,3.100, 3.200,3.300,3.400,3.500,3.600,3.700,3.800,3.900,4.000]\nu0(x) = [0.084,0.082,0.077, 0.060,0.015,-0.050,-0.095,-0.112,-0.117,-0.119,-0.119,-0.119,-0.119,-0.119,-0.119, -0.119]\nw1(t) = [0.084,1.099,-1.020,-0.635,-0.467,-1.006,1.104,-0.197,1.551,0.562, 0.421,0.978,-0.573,1.219,-0.322,0.766,0.327,-0.576,0.454,-1.390,0.491,-0.253,0.836, 1.754,0.708,2.081,-0.317,0.278,-0.945,-1.321,0.040,-0.923,1.531,0.528,1.461,1.244, -0.077,1.020,-0.974,0.595,-0.267]\nw2(t) = [-0.119,-0.546,0.115,1.417,0.645,0.414, -0.283,-0.293,0.474,1.305,0.912,-0.492,-0.064,0.006,0.848,1.094,0.622,-0.517,-0.451, 0.901,0.789,0.948,0.092,-0.363,-0.286,1.031,1.299,0.143,0.010,-0.400,0.336,0.927, 1.299,-0.045,-0.603,0.244,0.519,1.156,0.654,0.035,-0.790]\nw3(t) = [-2.453,-1.108, 0.170,1.133,1.618,1.573,1.077,0.313,-0.470,-1.018,-1.133,-0.719,0.192,1.447,2.806, 3.992,4.757,4.940,4.499,3.528,2.232,0.885,-0.234,-0.903,-1.007,-0.566,0.271,1.259, 2.112,2.570,2.457,1.724,0.465,-1.107,-2.703,-4.019,-4.806,-4.923,-4.371,-3.288, -1.923]\nJSON schema:\n{"equation":"burgers","param":{"nu":...},"grid_shape":[16]," t_steps":41,"x":[...],"t":[...],"u0":[...],"w1":[...],"w2":[...],"w3":[...],"u ":[[...]]}\n Cavity: You are given a PDE, full IC/BC specs, parameter values, and input signals. Return ONLY a valid JSON object with numeric arrays at 3-decimal precision. No extra text, no code fences. Task: 2D lid-driven cavity (vorticity/streamfunction).\nEquations: omega_t + u*omega_x + v*omega_y = (1/Re)*(omega_xx+omega_yy), Laplacian(psi) = -omega, u=psi_y, v=-psi_x .\nNo-slip walls. Lid velocity u_lid(x,t)=a(x)*f(t), v=0.\na(x)=1+0.3*sin(2*pi*x) +0.2*x (fixed).\nInitial condition: omega=0, psi=0.\nParameter: Re = 120.\nOutput grid: 6x6, 21 time steps.\nReturn JSON with fields: equation, param, grid_shape, t_steps, x, y, t, u_lid, omega, psi.\nAll numeric values must be rounded to 3decimals.\nx = [0.000,0.212,0.394,0.606,0.788,1.000]\ny = [0.000,0.212,0.394,0.606, 0.788,1.000]\nt = [0.000,0.200,0.400,0.600,0.800,1.000,1.200,1.400,1.600,1.800,2.000 ,2.180,2.380,2.580,2.780,2.980,3.180,3.380,3.580,3.780,3.980]\nu_lid(t) = [0.658, 0.728,0.873,0.606,0.826,0.793,0.618,0.890,0.693,0.689,0.891,0.639,0.754,0.856,0.602, 0.847,0.767,0.632,0.897,0.670,0.714]\nJSON schema:\n{"equation":"cavity","param":{" Re":...},"grid_shape":[6,6],"t_steps":21,"x":[...],"y":[...],"t":[...],"u_lid":[...] ,"omega":[[[...]]],"psi":[[[...]]]}\n 

21 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

B.4. OpInf-LLM 

We collect data and produce reduced-order operators following the settings in Section 4.1, and prompt an LLM to predict reduced-order operators at new parameter values by interpolating or regressing from a set of training parameters with known operators. Specifically, through tool calling, the LLM reasons about whether a query corresponds to interpolation or extrapolation in parameter space, invokes a local interpolation or regression tool to compute the reduced-order operators at the queried parameter value, and optionally validates them via lightweight heuristics. The predicted operators are then used to simulate the reduced-order model (ROM). The key procedures are summarized in Alg. 1. Relative L2 errors are reported over both the training time horizon and an extended time horizon to assess long-term generalization. System prompts and the provided tools used by OpInf-LLM are listed below. 

Algorithm 1 OpInf-LLM 

Given: basis number r, regularization intensity Î»

Offline:  

> 1:

Collect PDE solution data D as in (9) of different parameters Î¾, from multiple trajectories with different ICBCs.  

> 2:

Identify basis Î¦ := [ Ï•1(x), . . . , Ï• r (x)] through POD. Specifically, Y = UÎ£V âŠ¤ with 

Y = [ y (x, t 1, Î¾ 1) , . . . , y (x, t K , Î¾ 1) , y (x, t 1, Î¾ 2) , . . . , y (x, t K , Î¾ 2) , . . . ]

and U = [ Ï•1, Â· Â· Â· , Ï• r , Â· Â· Â· ]. 

> 3:

Identify reduced-order operators A, H, B, c for all parameters Î¾ in the dataset via (8). 

Online:  

> 4:

Parse natural language PDE solving instruction to obtain parameter Î¾ and ICBC of interest.  

> 5:

Call agentic tools to perform polynomial regression to identify reduced operators A, H, B, c for test parameter Î¾. 

> 6:

Call agentic tools to integrate ODE to solve reduced dynamics (4).  

> 7:

Reconstruct full-order solution via (3). 

Prompts:                                                                 

> Parsing: schema ={"equations": ["heat", "burgers", "cavity"], "provider": "openai|gemini|deepseek|anthropic|qwen", "model_name": "string or null", "output_dir": "string or null", "save_raw": "true/false", "reuse_operators": "true/false", "heat_nus": "list of numbers or null", "burgers_nus": "list of numbers or null", "cavity_res": "list of numbers or null", }system = ( "You are astrict parser. Return JSON only. Do not include code fences." )user =("Parse the prompt into the following JSON keys. Use lowercase for equations.\n" f"Schema: {json.dumps(schema)}\n" f"Prompt: {prompt}" )messages = [{"role": "system", "content": system}, {"role": "user", "content": user}, ]Solving:

22 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

**Task**: Predict OpInf operators for a {equation_type} equation at parameter \nu = {query_nu} **Available Data** (already loaded in the system): - Training parameters: \nu = {nu_train} - Operators at each \nu: {operator_names} **Available Tools** (USE THESE IN ORDER): 1. â€˜analyze_parameter_range(nu_train, nu_query)â€˜: Check if \nu={query_nu} is interpolation or extrapolation {step_2} 3. â€˜validate_operators(operators, equation_type)â€˜: Check physical constraints **Instructions**: 1. First, call analyze_parameter_range({nu_train}, {query_nu}) to understand the problem {instruction_2} 3. Finally, call validate_operators with the returned operators and equation_type="{ equation_type}" Please solve this step-by-step using the tools. if method == "regression": step_2 = f"2. â€˜simple_linear_regress(nu_query)â€˜: Get regressed operators (just pass nu_query={query_nu})" instruction_2 = f"2. Then, call simple_linear_regress({query_nu}) to get the operators" else: step_2 = f"2. â€˜simple_interpolate(nu_query, method)â€˜: Get interpolated operators ( EASIEST - just pass nu_query={query_nu})" instruction_2 = f"2. Then, call simple_interpolate({query_nu}, \"linear\") to get the operators" initial_prompt = f"""You are an expert in reduced-order modeling and operator inference. """ 

Provided tools:  

> 12

# ============================================================================  

> 3

# Tool Functions  

> 4

# ============================================================================  

> 56

def interpolate_operators(  

> 7

nu_train: List[float],  

> 8

operators_train: Dict[str, List[List[float]]],  

> 9

nu_query: float,  

> 10

method: Literal["linear", "quadratic", "cubic"] = "linear"  

> 11

) -> Dict[str, Any]:  

> 12

"""  

> 13

Interpolate OpInf operators to a new parameter value.  

> 14 15

Args:  

> 16

nu_train: Training parameter values (e.g., [0.1, 0.5, 2.0])  

> 17

operators_train: Dict of operator matrices at each nu_train  

> 18

Format: {"A": [matrix_at_nu1, matrix_at_nu2, ...],  

> 19

"B": [...], "C": [...]}  

> 20

nu_query: Target parameter value  

> 21

method: Interpolation method (linear, quadratic, cubic)  

> 22 23

Returns:  

> 24

Dict with interpolated operators and metadata  

> 25

"""  

> 26

nu_array = np.array(nu_train)  

> 27

operator_names = list(operators_train.keys()) 

> 28

23 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference  

> 29

# Validate method against number of points  

> 30

if method == "quadratic" and len(nu_train) < 3:  

> 31

method = "linear"  

> 32

if method == "cubic" and len(nu_train) < 4:  

> 33

method = "quadratic" if len(nu_train) >= 3 else "linear"  

> 34 35

interpolated = {}  

> 36 37

for op_name in operator_names:  

> 38

# Stack operator matrices  

> 39

op_stack = np.array(operators_train[op_name]) # (n_params, ...)  

> 40 41

# Interpolate element-wise  

> 42

interp_func = interp1d(  

> 43

nu_array, op_stack, axis=0,  

> 44

kind=method,  

> 45

fill_value=â€™extrapolateâ€™  

> 46

) 

> 47 48

op_interp = interp_func(nu_query)  

> 49 50

interpolated[op_name] = { 

> 51

"values": op_interp.tolist(),  

> 52

"shape": list(op_interp.shape),  

> 53

"norm": float(np.linalg.norm(op_interp)),  

> 54

"mean": float(np.mean(op_interp)),  

> 55

"std": float(np.std(op_interp)),  

> 56

"min": float(np.min(op_interp)),  

> 57

"max": float(np.max(op_interp))  

> 58

} 

> 59 60

return { 

> 61

"operators": interpolated,  

> 62

"method": method,  

> 63

"nu_query": nu_query,  

> 64

"success": True  

> 65

} 

> 66 67 68

def linear_regress_operators(  

> 69

nu_train: List[float],  

> 70

operators_train: Dict[str, List[List[float]]],  

> 71

nu_query: float,  

> 72

) -> Dict[str, Any]:  

> 73

"""  

> 74

Linear regression of operators vs normalized nu.  

> 75

Fits y = a*z + b per entry, where z = (nu - mean) / std.  

> 76

"""  

> 77

nu_array = np.array(nu_train, dtype=float)  

> 78

nu_mean = float(nu_array.mean())  

> 79

nu_std = float(nu_array.std()) if float(nu_array.std()) > 1e-12 else 1.0  

> 80

z = (nu_array - nu_mean) / nu_std  

> 81

z_q = (float(nu_query) - nu_mean) / nu_std  

> 82 83

operator_names = list(operators_train.keys())  

> 84

outputs = {}  

> 85

for op_name in operator_names:  

> 86

op_stack = np.array(operators_train[op_name]) # (n_params, ...)  

> 87

flat = op_stack.reshape(len(nu_array), -1)  

> 88

X = np.vstack([z, np.ones_like(z)]).T  

> 89

coeffs, _, _, _ = np.linalg.lstsq(X, flat, rcond=None)  

> 90

a = coeffs[0]  

> 91

b = coeffs[1]  

> 92

pred_flat = a * z_q + b 

> 93

pred = pred_flat.reshape(op_stack.shape[1:]) 

24 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference  

> 94

outputs[op_name] = { 

> 95

"values": pred.tolist(),  

> 96

"shape": list(pred.shape),  

> 97

"norm": float(np.linalg.norm(pred)),  

> 98

"mean": float(np.mean(pred)),  

> 99

"std": float(np.std(pred)),  

> 100

"min": float(np.min(pred)),  

> 101

"max": float(np.max(pred)),  

> 102

} 

> 103 104

return { 

> 105

"operators": outputs,  

> 106

"method": "regression",  

> 107

"nu_query": nu_query,  

> 108

"success": True,  

> 109

} 

> 110 111 112

def linear_regress_operators_batch(  

> 113

nu_train: List[float],  

> 114

operators_train: Dict[str, List[List[float]]],  

> 115

nu_queries: List[float],  

> 116

) -> Dict[str, Any]:  

> 117

"""Batch linear regression for multiple nu values."""  

> 118

outputs = {}  

> 119

for nu_query in nu_queries:  

> 120

outputs[str(nu_query)] = linear_regress_operators(nu_train, operators_train, nu_query)  

> 121

return { 

> 122

"nu_queries": nu_queries,  

> 123

"method": "regression",  

> 124

"predictions": outputs,  

> 125

"success": True,  

> 126

} 

> 127 128 129

def analyze_parameter_range(  

> 130

nu_train: List[float],  

> 131

nu_query: float  

> 132

) -> Dict[str, Any]:  

> 133

"""  

> 134

Analyze whether query is interpolation or extrapolation.  

> 135 136

Args:  

> 137

nu_train: Training parameter values  

> 138

nu_query: Query parameter value  

> 139 140

Returns:  

> 141

Analysis of parameter range  

> 142

"""  

> 143

nu_min = min(nu_train)  

> 144

nu_max = max(nu_train)  

> 145 146

is_interpolation = nu_min <= nu_query <= nu_max  

> 147 148

# Calculate relative position  

> 149

if is_interpolation:  

> 150

if nu_max != nu_min:  

> 151

relative_position = (nu_query - nu_min) / (nu_max - nu_min)  

> 152

else:  

> 153

relative_position = 0.5  

> 154

else:  

> 155

relative_position = None  

> 156 157

# Determine extrapolation distance 

25 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference  

> 158

if nu_query < nu_min:  

> 159

extrapolation_distance = abs(nu_query - nu_min)  

> 160

extrapolation_direction = "below"  

> 161

elif nu_query > nu_max:  

> 162

extrapolation_distance = abs(nu_query - nu_max)  

> 163

extrapolation_direction = "above"  

> 164

else:  

> 165

extrapolation_distance = 0 

> 166

extrapolation_direction = None  

> 167 168

return { 

> 169

"nu_train": nu_train,  

> 170

"nu_query": nu_query,  

> 171

"nu_range": [nu_min, nu_max],  

> 172

"is_interpolation": is_interpolation,  

> 173

"relative_position": relative_position,  

> 174

"extrapolation_distance": extrapolation_distance,  

> 175

"extrapolation_direction": extrapolation_direction,  

> 176

"confidence": "high" if is_interpolation else "low",  

> 177

"recommendation": "Use linear interpolation" if is_interpolation else  

> 178

"Extrapolation detected - results may be less accurate"  

> 179

} 

> 180 181 182

def validate_operators(  

> 183

operators: Dict[str, Dict],  

> 184

equation_type: str = "heat"  

> 185

) -> Dict[str, Any]:  

> 186

"""  

> 187

Validate physical constraints on operators.  

> 188 189

Args:  

> 190

operators: Interpolated operators with â€™valuesâ€™ field  

> 191

equation_type: Type of equation (heat, burgers, etc.)  

> 192 193

Returns:  

> 194

Validation results  

> 195

"""  

> 196

validations = {}  

> 197 198

for op_name, op_data in operators.items():  

> 199

op_array = np.array(op_data["values"])  

> 200 201

checks = { 

> 202

"has_nan": bool(np.any(np.isnan(op_array))),  

> 203

"has_inf": bool(np.any(np.isinf(op_array))),  

> 204

"is_finite": bool(np.all(np.isfinite(op_array))),  

> 205

"max_abs_value": float(np.max(np.abs(op_array)))  

> 206

} 

> 207 208

# Equation-specific checks  

> 209

if equation_type == "heat":  

> 210

# For heat equation, A should have negative eigenvalues (stable)  

> 211

if op_name == "A" and len(op_array.shape) == 2 and op_array.shape[0] == op_array.shape[1]:  

> 212

try:  

> 213

eigvals = np.linalg.eigvals(op_array)  

> 214

checks["max_real_eigenvalue"] = float(np.max(np.real(eigvals)))  

> 215

checks["is_stable"] = checks["max_real_eigenvalue"] < 0 

> 216

except:  

> 217

checks["eigenvalue_check"] = "failed"  

> 218 219

elif equation_type == "burgers":  

> 220

# For Burgers equation, check H tensor shape  

> 221

if op_name == "H": 

26 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

Table 5. Results for OpInf-LLM with code generation. Method Success rate (%) â†‘ Heat â†“ Burgers â†“ Cavity â†“

OpInf-LLM (code generation w/ interpolation) 52.2 2.89e âˆ’2 3.38e âˆ’1 5.03e âˆ’2

OpInf-LLM (code generation w/ regression) 44.2 1.33e âˆ’2 1.87e âˆ’1 1.36e âˆ’1

OpInf-LLM (tool calling w/ interpolation) 99.2 1.23e âˆ’2 4.57e âˆ’1 3.85e âˆ’2

OpInf-LLM (tool calling w/ regression) 99.2 1.29e âˆ’2 4.91e âˆ’1 4.63e âˆ’2 

> 222

checks["shape_info"] = f"H tensor: {op_array.shape}"  

> 223 224

validations[op_name] = checks  

> 225 226

# Overall validation  

> 227

all_valid = all(  

> 228

not v["has_nan"] and not v["has_inf"] and v["is_finite"]  

> 229

for v in validations.values()  

> 230

) 

> 231 232

return { 

> 233

"is_valid": all_valid,  

> 234

"operator_checks": validations,  

> 235

"equation_type": equation_type  

> 236

}

# C. Additional Results 

C.1. Code Generation 

While the main text focuses on tool calling for reduced operator prediction at test time, here we explore the feasibility of LLM-based code generation without access to external tools. Specifically, we prompt the LLM to directly generate code for polynomial regression or interpolation to predict reduced operators, as well as an ODE integrator to solve the resulting ROM. Quantitative results using GPT-4o are summarized in Table 5. We observe that, under code generation, the prediction error is comparable to that achieved with tool calling, but the execution success rate is lower. Upon inspecting the generated code, we find that failures primarily arise from data loading and output shape mismatches, as well as formatting issues in data saving, whereas the polynomial regression/interpolation and ODE integration components themselves generally function correctly. Improving interface design and code I/O handling may therefore further increase the success rate in future work. Prompts used for code generation are shown below. 

Prompts: 

Heat/Burgersâ€™: COEFFICIENT JSON STRUCTURE (IMPORTANT): - JSON file at: {coeff_path} - Top-level keys: equation, n_modes, pod_basis_shape, parameters, pod_basis - parameters is a list; each item has: * nu (float) * operators: {{A,B,C}} for heat or {{H,A,B,C}} for burgers * operators.<name>.values holds the array - pod_basis.values is the POD basis (space x r) to use as phi When use_json=True: - Load operators from the JSON, NOT from per_nu_models in the PKL. - Use phi from coeff JSON (pod_basis.values). - Use the PKL model ONLY to read x_grid/x_fine for dx. """ return f"""You are an expert Python programmer. 

27 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

Generate Python code to: 1) Load model from: {model_path} 2) Load case data from: {data_path} 3) Compute operators for the query parameter using method={method} 4) Integrate the ROM and save output to: {output_path} USE_JSON={str(use_json)} {coeff_block} MODEL STRUCTURE (IMPORTANT): - pickle with keys: per_nu_models (list of dicts), phi, x_grid or x_fine, t_eval - Each per_nu_models item has keys: "nu", "A", "B", "C" and for Burgers also "H" - There is NO nested "operators" key. Operators are top-level per entry. - Operator shapes (must match exactly): {op_shapes} - phi shape: {phi_shape} (space x r) CASE DATA (.npz) contains: - Y_ref (may be saved as time x space); if Y_ref.shape[0] != phi.shape[0], transpose. - U_ref (may be time x n_inputs); ensure U_ref is (n_inputs x time). - t_eval (time grid) - nu (float) REQUIREMENTS: - Always read inputs via: case_data = np.load(data_path) - Always save outputs via this exact call (do not hard-code filenames): np.savez(output_path, Y_ref=Y_ref, Y_rom=Y_rom, t_eval=t_eval, nu=nu_query) - Do NOT hard-code any filenames or paths. - OUTPUT PATH RULE: The runner sets output_path per case/trajectory; you must use output_path directly. Example only (do NOT hard-code): output_path = "codegen/gpt-4o/burgers/regression/llm_codegen_burgers_nu0.07 _traj12_raw.npz" - If nu matches a training nu exactly, use that operator without regression. - For method=regression: per-entry linear regression y = a*nu + b across training nus. Use numpy only (np.linalg.lstsq or closed form). Do NOT use sklearn. - Regression implementation (stable): flatten operator to shape (n_train, n_flat), build X = [nu_train, ones], solve X @ coeffs = op_flat using lstsq. Then op_query_flat = coeffs[0]*nu_query + coeffs[1], reshape to op_shape. - For method=interpolation: linear interpolation per entry. Use flatten-interp-reshape for arrays (np.interp expects 1D values). - Heat ROM: aâ€™ = C + A a + B*u (B is (r,), u is scalar) - Burgers ROM: aâ€™ = C + A a + H(a,a) + B @ u (B is (r x 3), u is 3-vector) - For H(a,a), use: quad = np.einsum(â€™ijk,j,k->iâ€™, H, a, a) - Use solve_ivp(method=â€™BDFâ€™, vectorized=False), fallback to RK45; for burgers, if needed fallback to LSODA. - Always use t_eval from CASE DATA (do not use model t_eval). - Projection: a0 = phi.T @ (Y_ref[:,0] * dx), after ensuring Y_ref is (space x time). - Ensure a is a 1D vector of length r (e.g., a = np.asarray(a).ravel()) and rom_rhs returns shape (r,). - In rom_rhs: start with a = np.asarray(a).ravel(); if a.size != r, set a = a[:r]; never reshape to (r,1) or (r,r). - After computing a0, if a0.size != r, set a0 = a0[:r]. - Enforce r = phi.shape[1]; assert A.shape[0] == r and A.shape[1] == r before integration. - Do NOT reference â€˜modelâ€˜ inside helper functions. If a helper needs x_grid/dx/phi, pass it as an argument. - After solve_ivp, set a_rom = sol.y; if a_rom.shape[0] != r, set a_rom = a_rom.T. - Lift: Y_rom = phi @ a_rom. - Save output with EXACT statement: np.savez(output_path, Y_ref=Y_ref, Y_rom=Y_rom, t_eval=t_eval, nu=nu_query) - Do not print large arrays. - Heat-specific shapes: B is (r,), do NOT reshape it to (r,r). - Burgers-specific shapes: B is (r,3). u = [w1(t), w2(t), w3(t)]. - If Y_ref time length != len(t_eval), resample each spatial row to t_eval using np. interp. 

28 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

- U_ref handling: * If U_ref.ndim == 1, reshape to (1, -1). * If U_ref.shape[0] == len(t_eval) and U_ref.shape[1] != len(t_eval), transpose. * After reshape, enforce U_ref.shape[1] == len(t_eval). If not, resample U_ref to t_eval using np.interp with a linearly spaced time grid over [t_eval[0], t_eval[-1]]. * For heat, use u = float(np.interp(t, t_eval, U_ref[0])). * For burgers, build u vector by interp each row; ensure u has shape (3,). SANITY CHECKS (must pass before saving): - assert Y_ref.shape[1] == len(t_eval) - assert Y_rom.shape == Y_ref.shape - In rom_rhs, return a 1D array (shape (r,)); do NOT return (r,1) or (r,r). CODE TEMPLATE (use this structure, adjust details): 1) Load model, extract nu_train and operator arrays per entry. 2) For each operator tensor: flat = arr.reshape(-1); stack across nu; interp/regress each entry; reshape to op_shapes. 3) Load Y_ref/U_ref; if Y_ref.shape[0] != phi.shape[0] or Y_ref.shape[0] == len(t_eval), transpose. Ensure U_ref is (n_inputs x time). 4) Integrate ROM; save output. Return only the complete Python code (no explanations). """ Cavity: Generate Python code to: 1) Load cavity model from: {model_path} 2) Load case data from: {data_path} 3) Compute operators for the query Re using method={method} 4) Integrate ROM with RK4 and save output to: {output_path} MODEL STRUCTURE (IMPORTANT): - pickle with keys: per_Re_models (list of dicts), phi, x, y, t_eval - Each per_Re_models item has keys: "Re", "H", "A", "B", "C" - There is NO nested "operators" key. - Operator shapes (must match exactly): {op_shapes} - phi shape: {phi_shape} (state x r) CASE DATA (.npz) contains: - Y_omega (n x time) - Y_psi (n x time) - U_lid (time) - t_eval (time grid) - Re (float) REQUIREMENTS: - Always read inputs via: case_data = np.load(data_path) - Always save outputs via this exact call (do not hard-code filenames): np.savez(output_path, Y_omega_fom=Y_omega, Y_psi_fom=Y_psi, Y_omega_rom=Y_omega_rom, Y_psi_rom=Y_psi_rom, U_lid=U_lid, x=x, y=y, t_eval=t_eval, Re=Re_query) - Do NOT hard-code any filenames or paths. - OUTPUT PATH RULE: The runner sets output_path per case/trajectory; you must use output_path directly. Example only (do NOT hard-code): output_path = "codegen/gpt-4o/cavity/regression/llm_codegen_cavity_Re120.0_traj2_raw .npz" - If Re matches a training Re exactly, use that operator without regression. - For method=regression: per-entry linear regression y = a*Re + b. Use numpy only (np.linalg.lstsq or closed form). Do NOT use sklearn. 

29 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

- Regression implementation (stable): flatten operator to shape (n_train, n_flat), build X = [Re_train, ones], solve X @ coeffs = op_flat using lstsq. Then op_query_flat = coeffs[0]*Re_query + coeffs[1], reshape to op_shape. - For method=interpolation: linear interpolation per entry (flatten-interp-reshape). - Modal state dimension is r = phi.shape[1]. Integrate a(t) in RË†r. - Use operators H, A, B, C with quadratic term H(a,a) via einsum: quad = np.einsum(â€™ijk,j,k->iâ€™, H, a, a) - Build Y_fom by stacking omega and psi. - Project with phi.T @ (Y_fom * dA) where dA = dx*dx. - Integrate with fixed-step RK4 using dt from t_eval. - Ensure Y_omega, Y_psi are (n x time) and time length == len(t_eval); do NOT downsample .- If Y_omega/Y_psi time length != len(t_eval), resample each spatial row to t_eval using np.interp. - Save Y_omega_rom/Y_psi_rom with the same shape as Y_omega/Y_psi. - Save output with EXACT statement: np.savez(output_path, Y_omega_fom=Y_omega, Y_psi_fom=Y_psi, Y_omega_rom=Y_omega_rom, Y_psi_rom=Y_psi_rom, U_lid=U_lid, x=x, y=y, t_eval=t_eval, Re=Re_query) - Do not print large arrays. - Use op_shapes exactly; do not reshape to other sizes. - U_lid handling: if U_lid is not length len(t_eval), resample to t_eval using np.interp on a linear time grid over [t_eval[0], t_eval[-1]]. - Ensure a is 1D (r,) throughout; do not use column vectors. SANITY CHECKS (must pass before saving): - assert Y_omega_rom.shape == Y_omega.shape - assert Y_psi_rom.shape == Y_psi.shape CODE TEMPLATE (use this structure, adjust details): 1) Load model, extract Re_train and operator arrays per entry. 2) For each operator tensor: flat = arr.reshape(-1); stack across Re; interp/regress each entry; reshape to op_shapes. 3) Load Y_omega/Y_psi; build Y_fom; project with phi to get a0 (r x 1). 4) Integrate a(t) with RK4 in r-dim; then Y_rom = phi @ a_traj. 5) Split Y_rom into omega/psi using n = Y_omega.shape[0]. 6) Save outputs. Return only the complete Python code (no explanations). """ 

C.2. Full Tables and Visualizations 

In this section, we provide full experiment and ablation results. While we consider polynomial regression for predicting reduced operators in the main draft, we additionally consider interpolation methods. Performance for both methods are comparable and the quantitative results are shown in Table 6, with errors in extended time horizon reported. Full ablation results on the number of POD basis and regularization intensity are shown in Table 7 and Table 8, respectively. Additional visualizations are shown in Fig. 6-9. 

Table 6. OpInf-LLM full results. LLM model Equation Method Parameter Error [0 , T ] Error [T, 2T ] Success rate (%) GPT-4.1 heat interpolation 0.5 8.70e âˆ’3 5.97e âˆ’3 100 GPT-4.1 heat interpolation 1.0 6.07e âˆ’3 5.24e âˆ’3 100 GPT-4.1 heat interpolation 3.0 2.22e âˆ’2 1.17e âˆ’1 100 GPT-4.1 heat regression 0.5 8.70e âˆ’3 5.97e âˆ’3 100 GPT-4.1 heat regression 1.0 1.02e âˆ’2 3.08e âˆ’2 100 GPT-4.1 heat regression 3.0 1.99e âˆ’2 7.12e âˆ’2 100 

Continued on next page 

30 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

LLM model Equation Method Parameter Error [0 , T ] Error [T, 2T ] Success rate (%) GPT-4.1 burgers interpolation 0.03 4.81e âˆ’1 5.96e âˆ’1 95 GPT-4.1 burgers interpolation 0.07 4.33e âˆ’1 4.97e âˆ’1 100 GPT-4.1 burgers regression 0.03 4.50e âˆ’1 6.56e âˆ’1 95 GPT-4.1 burgers regression 0.07 5.32e âˆ’1 6.15e âˆ’1 100 GPT-4.1 cavity interpolation 60 5.74e âˆ’2 5.94e âˆ’2 100 GPT-4.1 cavity interpolation 80 3.95e âˆ’2 3.54e âˆ’2 100 GPT-4.1 cavity interpolation 90 4.76e âˆ’2 4.15e âˆ’2 100 GPT-4.1 cavity interpolation 110 2.51e âˆ’2 2.64e âˆ’2 100 GPT-4.1 cavity interpolation 120 4.23e âˆ’2 4.34e âˆ’2 100 GPT-4.1 cavity interpolation 140 2.92e âˆ’2 2.83e âˆ’2 100 GPT-4.1 cavity regression 60 5.54e âˆ’2 5.57e âˆ’2 100 GPT-4.1 cavity regression 80 4.87e âˆ’2 5.13e âˆ’2 100 GPT-4.1 cavity regression 90 5.83e âˆ’2 5.94e âˆ’2 100 GPT-4.1 cavity regression 110 3.57e âˆ’2 4.14e âˆ’2 100 GPT-4.1 cavity regression 120 4.44e âˆ’2 4.60e âˆ’2 100 GPT-4.1 cavity regression 140 3.50e âˆ’2 3.78e âˆ’2 100 GPT-4o heat interpolation 0.5 8.70e âˆ’3 5.97e âˆ’3 100 GPT-4o heat interpolation 1.0 6.07e âˆ’3 5.24e âˆ’3 100 GPT-4o heat interpolation 3.0 2.22e âˆ’2 1.17e âˆ’1 100 GPT-4o heat regression 0.5 8.70e âˆ’3 5.97e âˆ’3 100 GPT-4o heat regression 1.0 1.02e âˆ’2 3.08e âˆ’2 100 GPT-4o heat regression 3.0 1.99e âˆ’2 7.12e âˆ’2 100 GPT-4o burgers interpolation 0.03 4.81e âˆ’1 5.96e âˆ’1 95 GPT-4o burgers interpolation 0.07 4.33e âˆ’1 4.97e âˆ’1 100 GPT-4o burgers regression 0.03 4.50e âˆ’1 6.56e âˆ’1 95 GPT-4o burgers regression 0.07 5.32e âˆ’1 6.15e âˆ’1 100 GPT-4o cavity interpolation 60 5.74e âˆ’2 5.94e âˆ’2 100 GPT-4o cavity interpolation 80 3.95e âˆ’2 3.54e âˆ’2 100 GPT-4o cavity interpolation 90 4.76e âˆ’2 4.15e âˆ’2 100 GPT-4o cavity interpolation 110 2.51e âˆ’2 2.64e âˆ’2 100 GPT-4o cavity interpolation 120 4.23e âˆ’2 4.34e âˆ’2 100 GPT-4o cavity interpolation 140 2.92e âˆ’2 2.83e âˆ’2 100 GPT-4o cavity regression 60 5.54e âˆ’2 5.57e âˆ’2 100 GPT-4o cavity regression 80 4.87e âˆ’2 5.13e âˆ’2 100 GPT-4o cavity regression 90 5.83e âˆ’2 5.94e âˆ’2 100 GPT-4o cavity regression 110 3.57e âˆ’2 4.14e âˆ’2 100 GPT-4o cavity regression 120 4.44e âˆ’2 4.60e âˆ’2 100 GPT-4o cavity regression 140 3.50e âˆ’2 3.78e âˆ’2 100 Gemini-2.0-flash heat interpolation 0.5 8.70e âˆ’3 5.97e âˆ’3 100 Gemini-2.0-flash heat interpolation 1.0 6.07e âˆ’3 5.24e âˆ’3 100 Gemini-2.0-flash heat interpolation 3.0 2.22e âˆ’2 1.17e âˆ’1 100 Gemini-2.0-flash heat regression 0.5 8.70e âˆ’3 5.97e âˆ’3 100 Gemini-2.0-flash heat regression 1.0 1.02e âˆ’2 3.08e âˆ’2 100 Gemini-2.0-flash heat regression 3.0 1.99e âˆ’2 7.12e âˆ’2 100 Gemini-2.0-flash burgers interpolation 0.03 4.81e âˆ’1 5.96e âˆ’1 95 Gemini-2.0-flash burgers interpolation 0.07 4.33e âˆ’1 4.97e âˆ’1 100 Gemini-2.0-flash burgers regression 0.03 4.50e âˆ’1 6.56e âˆ’1 95 Gemini-2.0-flash burgers regression 0.07 5.32e âˆ’1 6.15e âˆ’1 100 Gemini-2.0-flash cavity interpolation 60 5.74e âˆ’2 5.94e âˆ’2 100 Gemini-2.0-flash cavity interpolation 80 3.95e âˆ’2 3.54e âˆ’2 100 Gemini-2.0-flash cavity interpolation 90 4.76e âˆ’2 4.15e âˆ’2 100 Gemini-2.0-flash cavity interpolation 110 2.51e âˆ’2 2.64e âˆ’2 100 Gemini-2.0-flash cavity interpolation 120 4.23e âˆ’2 4.34e âˆ’2 100 

Continued on next page 

31 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

LLM model Equation Method Parameter Error [0 , T ] Error [T, 2T ] Success rate (%) Gemini-2.0-flash cavity interpolation 140 2.92e âˆ’2 2.83e âˆ’2 100 Gemini-2.0-flash cavity regression 60 5.54e âˆ’2 5.57e âˆ’2 100 Gemini-2.0-flash cavity regression 80 4.87e âˆ’2 5.13e âˆ’2 100 Gemini-2.0-flash cavity regression 90 5.83e âˆ’2 5.94e âˆ’2 100 Gemini-2.0-flash cavity regression 110 3.57e âˆ’2 4.14e âˆ’2 100 Gemini-2.0-flash cavity regression 120 4.44e âˆ’2 4.60e âˆ’2 100 Gemini-2.0-flash cavity regression 140 3.50e âˆ’2 3.78e âˆ’2 100 Qwen Plus heat interpolation 0.5 8.70e âˆ’3 5.97e âˆ’3 100 Qwen Plus heat interpolation 1.0 6.07e âˆ’3 5.24e âˆ’3 100 Qwen Plus heat interpolation 3.0 2.22e âˆ’2 1.17e âˆ’1 100 Qwen Plus heat regression 0.5 8.70e âˆ’3 5.97e âˆ’3 100 Qwen Plus heat regression 1.0 1.02e âˆ’2 3.08e âˆ’2 100 Qwen Plus heat regression 3.0 1.99e âˆ’2 7.12e âˆ’2 100 Qwen Plus burgers interpolation 0.03 4.81e âˆ’1 5.96e âˆ’1 95 Qwen Plus burgers interpolation 0.07 4.33e âˆ’1 4.97e âˆ’1 100 Qwen Plus burgers regression 0.03 4.50e âˆ’1 6.56e âˆ’1 95 Qwen Plus burgers regression 0.07 5.32e âˆ’1 6.15e âˆ’1 100 Qwen Plus cavity interpolation 60 6.35e âˆ’2 6.93e âˆ’2 100 Qwen Plus cavity interpolation 80 3.90e âˆ’2 3.52e âˆ’2 100 Qwen Plus cavity interpolation 90 4.68e âˆ’2 4.11e âˆ’2 100 Qwen Plus cavity interpolation 110 2.45e âˆ’2 2.61e âˆ’2 100 Qwen Plus cavity interpolation 120 4.21e âˆ’2 4.33e âˆ’2 100 Qwen Plus cavity interpolation 140 2.98e âˆ’2 3.08e âˆ’2 100 Qwen Plus cavity regression 60 5.54e âˆ’2 5.57e âˆ’2 100 Qwen Plus cavity regression 80 4.87e âˆ’2 5.13e âˆ’2 100 Qwen Plus cavity regression 90 5.83e âˆ’2 5.94e âˆ’2 100 Qwen Plus cavity regression 110 3.57e âˆ’2 4.14e âˆ’2 100 Qwen Plus cavity regression 120 4.44e âˆ’2 4.60e âˆ’2 100 Qwen Plus cavity regression 140 3.50e âˆ’2 3.78e âˆ’2 100 Claude Sonnet 4 heat interpolation 0.5 8.70e âˆ’3 5.97e âˆ’3 100 Claude Sonnet 4 heat interpolation 1.0 6.07e âˆ’3 5.24e âˆ’3 100 Claude Sonnet 4 heat interpolation 3.0 2.22e âˆ’2 1.17e âˆ’1 100 Claude Sonnet 4 heat regression 0.5 8.70e âˆ’3 5.97e âˆ’3 100 Claude Sonnet 4 heat regression 1.0 1.02e âˆ’2 3.08e âˆ’2 100 Claude Sonnet 4 heat regression 3.0 1.99e âˆ’2 7.12e âˆ’2 100 Claude Sonnet 4 burgers interpolation 0.03 4.81e âˆ’1 5.96e âˆ’1 95 Claude Sonnet 4 burgers interpolation 0.07 4.33e âˆ’1 4.97e âˆ’1 100 Claude Sonnet 4 burgers regression 0.03 4.50e âˆ’1 6.56e âˆ’1 95 Claude Sonnet 4 burgers regression 0.07 5.32e âˆ’1 6.15e âˆ’1 100 Claude Sonnet 4 cavity interpolation 60 6.35e âˆ’2 6.93e âˆ’2 100 Claude Sonnet 4 cavity interpolation 80 3.90e âˆ’2 3.52e âˆ’2 100 Claude Sonnet 4 cavity interpolation 90 4.68e âˆ’2 4.11e âˆ’2 100 Claude Sonnet 4 cavity interpolation 110 2.45e âˆ’2 2.61e âˆ’2 100 Claude Sonnet 4 cavity interpolation 120 4.21e âˆ’2 4.33e âˆ’2 100 Claude Sonnet 4 cavity interpolation 140 2.98e âˆ’2 3.08e âˆ’2 100 Claude Sonnet 4 cavity regression 60 5.54e âˆ’2 5.57e âˆ’2 100 Claude Sonnet 4 cavity regression 80 4.87e âˆ’2 5.13e âˆ’2 100 Claude Sonnet 4 cavity regression 90 5.83e âˆ’2 5.94e âˆ’2 100 Claude Sonnet 4 cavity regression 110 3.57e âˆ’2 4.14e âˆ’2 100 Claude Sonnet 4 cavity regression 120 4.44e âˆ’2 4.60e âˆ’2 100 Claude Sonnet 4 cavity regression 140 3.50e âˆ’2 3.78e âˆ’2 100 

32 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

Table 7. POD modes ablation for OpInf-LLM. Equation POD Method Error [0 , T ] Error [T, 2T ] Error [0 , 2T ]

Heat 4 interpolation 3.35e âˆ’2 5.81e âˆ’2 4.78e âˆ’2

Heat 4 regression 1.96e âˆ’2 2.39e âˆ’2 2.21e âˆ’2

Heat 5 interpolation 1.77e âˆ’2 3.72e âˆ’2 2.98e âˆ’2

Heat 5 regression 3.32e âˆ’2 7.87e âˆ’2 6.03e âˆ’2

Heat 6 interpolation 1.23e âˆ’2 4.27e âˆ’2 3.16e âˆ’2

Heat 6 regression 1.29e âˆ’2 3.60e âˆ’2 2.72e âˆ’2

Heat 7 interpolation 7.18e âˆ’3 1.94e âˆ’2 1.47e âˆ’2

Heat 7 regression 2.03e âˆ’2 6.00e âˆ’2 4.44e âˆ’2

Heat 8 interpolation 1.07e âˆ’3 9.44e âˆ’4 1.02e âˆ’3

Heat 8 regression unstable unstable unstable 

Heat 9 interpolation 8.72e âˆ’4 4.52e âˆ’4 6.74e âˆ’4

Heat 9 regression unstable unstable unstable 

Heat 10 interpolation 7.83e âˆ’4 2.40e âˆ’4 5.54e âˆ’4

Heat 10 regression unstable unstable unstable 

Burgers 4 interpolation 4.09e âˆ’1 5.31e âˆ’1 4.58e âˆ’1

Burgers 4 regression 3.93e âˆ’1 4.99e âˆ’1 4.36e âˆ’1

Burgers 5 interpolation 3.72e âˆ’1 4.83e âˆ’1 4.15e âˆ’1

Burgers 5 regression 3.44e âˆ’1 4.48e âˆ’1 3.86e âˆ’1

Burgers 6 interpolation 4.15e âˆ’1 7.89e âˆ’1 5.95e âˆ’1

Burgers 6 regression 4.13e âˆ’1 9.63e âˆ’1 6.94e âˆ’1

Burgers 7 interpolation 3.27e âˆ’1 4.81e âˆ’1 3.94e âˆ’1

Burgers 7 regression 3.52e âˆ’1 5.41e âˆ’1 4.38e âˆ’1

Burgers 8 interpolation 3.63e âˆ’1 4.54e âˆ’1 4.08e âˆ’1

Burgers 8 regression 3.64e âˆ’1 4.90e âˆ’1 4.24e âˆ’1

Burgers 9 interpolation 3.95e âˆ’1 5.02e âˆ’1 4.44e âˆ’1

Burgers 9 regression 4.70e âˆ’1 6.46e âˆ’1 5.50e âˆ’1

Burgers 10 interpolation 4.57e âˆ’1 5.47e âˆ’1 5.04e âˆ’1

Burgers 10 regression 4.91e âˆ’1 6.36e âˆ’1 5.67e âˆ’1

Cavity 4 interpolation 8.24e âˆ’2 5.73e âˆ’2 7.16e âˆ’2

Cavity 4 regression 8.64e âˆ’2 6.39e âˆ’2 7.66e âˆ’2

Cavity 5 interpolation 7.40e âˆ’2 5.29e âˆ’2 6.49e âˆ’2

Cavity 5 regression 8.00e âˆ’2 6.22e âˆ’2 7.21e âˆ’2

Cavity 6 interpolation 7.27e âˆ’2 5.29e âˆ’2 6.40e âˆ’2

Cavity 6 regression 7.79e âˆ’2 6.17e âˆ’2 7.07e âˆ’2

Cavity 7 interpolation 7.12e âˆ’2 4.92e âˆ’2 6.18e âˆ’2

Cavity 7 regression 7.58e âˆ’2 5.66e âˆ’2 6.73e âˆ’2

Cavity 8 interpolation 7.08e âˆ’2 4.83e âˆ’2 6.12e âˆ’2

Cavity 8 regression 7.62e âˆ’2 5.53e âˆ’2 6.72e âˆ’2

Cavity 9 interpolation 7.08e âˆ’2 4.61e âˆ’2 6.05e âˆ’2

Cavity 9 regression 7.68e âˆ’2 5.53e âˆ’2 6.77e âˆ’2

Cavity 10 interpolation 7.35e âˆ’2 4.93e âˆ’2 6.34e âˆ’2

Cavity 10 regression 7.74e âˆ’2 5.77e âˆ’2 6.89e âˆ’2

33 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

Table 8. Regularization intensity ( Î») ablation for OpInf-LLM. Equation Î» Method Error [0 , T ] Error [T, 2T ] Error [0 , 2T ]

Heat 0 interpolation 1.23e âˆ’2 4.27e âˆ’2 3.16e âˆ’2

Heat 0 regression 1.29e âˆ’2 3.60e âˆ’2 2.72e âˆ’2

Heat 10 âˆ’6 interpolation 1.23e âˆ’2 4.27e âˆ’2 3.16e âˆ’2

Heat 10 âˆ’6 regression 1.29e âˆ’2 3.60e âˆ’2 2.72e âˆ’2

Heat 10 âˆ’5 interpolation 1.23e âˆ’2 4.27e âˆ’2 3.16e âˆ’2

Heat 10 âˆ’5 regression 1.29e âˆ’2 3.60e âˆ’2 2.72e âˆ’2

Heat 10 âˆ’4 interpolation 1.23e âˆ’2 4.27e âˆ’2 3.16e âˆ’2

Heat 10 âˆ’4 regression 1.29e âˆ’2 3.60e âˆ’2 2.72e âˆ’2

Heat 10 âˆ’3 interpolation 1.23e âˆ’2 4.27e âˆ’2 3.16e âˆ’2

Heat 10 âˆ’3 regression 1.29e âˆ’2 3.60e âˆ’2 2.72e âˆ’2

Heat 10 âˆ’2 interpolation 1.25e âˆ’2 4.4e âˆ’2 3.25e âˆ’2

Heat 10 âˆ’2 regression 1.22e âˆ’2 3.33e âˆ’2 2.54e âˆ’2

Heat 10 âˆ’1 interpolation 4.00e âˆ’2 4.43e âˆ’1 3.07e âˆ’1

Heat 10 âˆ’1 regression 3.44e âˆ’2 1.09e âˆ’1 7.97e âˆ’2

Heat 1 interpolation 4.37e3 4.43e9 3.01e9 

Heat 1 regression 5.67e âˆ’2 8.61e âˆ’2 7.3e âˆ’2

Heat 10 interpolation 3.73e âˆ’1 1.52e0 1.12e0 

Heat 10 regression 2.71e âˆ’1 3.16e âˆ’1 2.96e âˆ’1

Burgers 0 interpolation 1.34e0 9.17e âˆ’1 1.14e0 

Burgers 0 regression 2.25e1 2.92e1 2.5e1 

Burgers 10 âˆ’6 interpolation 1.34e0 9.17e âˆ’1 1.14e0 

Burgers 10 âˆ’6 regression 2.25e1 2.92e1 2.5e1 

Burgers 10 âˆ’5 interpolation 1.34e0 9.17e âˆ’1 1.14e0 

Burgers 10 âˆ’5 regression 2.25e1 2.92e1 2.5e1 

Burgers 10 âˆ’4 interpolation 1.34e0 9.17e âˆ’1 1.14e0 

Burgers 10 âˆ’4 regression 2.25e1 2.92e1 2.5e1 

Burgers 10 âˆ’3 interpolation 1.34e0 9.17e âˆ’1 1.14e0 

Burgers 10 âˆ’3 regression 2.26e1 2.94e1 2.51e1 

Burgers 10 âˆ’2 interpolation 1.31e0 8.91e âˆ’1 1.12e0 

Burgers 10 âˆ’2 regression 2.96e1 3.90e1 3.32e1 

Burgers 10 âˆ’1 interpolation 8.72e âˆ’1 6.82e âˆ’1 7.85e âˆ’1

Burgers 10 âˆ’1 regression 1.81e1 5.42e1 3.74e1 

Burgers 1 interpolation 3.67e âˆ’1 6.48e âˆ’1 5.02e âˆ’1

Burgers 1 regression 3.31e âˆ’1 5.53e âˆ’1 4.33e âˆ’1

Burgers 10 interpolation 3.52e âˆ’1 4.92e âˆ’1 4.12e âˆ’1

Burgers 10 regression 3.34e âˆ’1 4.53e âˆ’1 3.83e âˆ’1

Cavity 0 interpolation failed failed failed Cavity 0 regression failed failed failed Cavity 10 âˆ’6 interpolation failed failed failed Cavity 10 âˆ’6 regression failed failed failed Cavity 10 âˆ’5 interpolation 9.98e3 1.09e4 1.05e4 

Cavity 10 âˆ’5 regression 2.14e âˆ’1 2.83e âˆ’1 2.5e âˆ’1

Cavity 10 âˆ’4 interpolation 9.98e3 1.09e4 1.05e4 

Cavity 10 âˆ’4 regression 2.14e âˆ’1 2.83e âˆ’1 2.5e âˆ’1

Cavity 10 âˆ’3 interpolation 9.98e3 1.07e4 1.03e4 

Cavity 10 âˆ’3 regression 2.14e âˆ’1 2.82e âˆ’1 2.5e âˆ’1

Cavity 10 âˆ’2 interpolation 1.01e4 1.2e4 1.11e4 

Cavity 10 âˆ’2 regression 1.95e âˆ’1 2.56e âˆ’1 2.27e âˆ’1

Cavity 10 âˆ’1 interpolation 4.29e âˆ’2 3.90e âˆ’2 4.11e âˆ’2

Cavity 10 âˆ’1 regression 9.56e3 1.03e4 1.08e4 

Cavity 1 interpolation 4.01e âˆ’2 3.88e âˆ’2 3.95e âˆ’2

Continued on next page 

34 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference 

Equation Î» Method Error [0 , T ] Error [T, 2T ] Error [0 , 2T ]

Cavity 1 regression 8.35e âˆ’2 9.60e âˆ’2 9.00e âˆ’2

Cavity 10 interpolation 2.21e âˆ’1 1.51e âˆ’1 1.96e âˆ’1

Cavity 10 regression 2.37e âˆ’1 1.66e âˆ’1 2.08e âˆ’1Ground Truth Ground Truth OpInf -LLM Regression OpInf -LLM Regression OpInf -LLM Interpolation OpInf -LLM Interpolation  

> Trajectory 1 Trajectory 2

Figure 6. Heat equation predictions of different Î½.

35 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference Ground Truth Ground Truth OpInf -LLM Regression OpInf -LLM Regression OpInf -LLM Interpolation OpInf -LLM Interpolation 

Trajectory 1 Trajectory 2 

Figure 7. Burgersâ€™ equation predictions of different Î½.

36 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference Ground Truth  OpInf -LLM Regression  OpInf -LLM Interpolation 

> Trajectory
> 1
> Trajectory
> 1
> Trajectory
> 1
> Trajectory
> 2
> Trajectory
> 2
> Trajectory
> 2
> Figure 8.  Cavity results across Reynolds numbers.

37 OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference Ground Truth  OpInf -LLM Regression  OpInf -LLM Interpolation 

> Trajectory
> 1
> Trajectory
> 1
> Trajectory
> 1
> Trajectory
> 2
> Trajectory
> 2
> Trajectory
> 2
> Re = 60
> Re = 80
> Re = 90
> Figure 9.  Cavity results across Reynolds numbers.

38