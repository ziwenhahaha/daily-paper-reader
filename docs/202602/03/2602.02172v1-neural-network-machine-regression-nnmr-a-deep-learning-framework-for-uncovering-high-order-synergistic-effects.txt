Title: Neural Network Machine Regression (NNMR): A Deep Learning Framework for Uncovering High-order Synergistic Effects

URL Source: https://arxiv.org/pdf/2602.02172v1

Published Time: Tue, 03 Feb 2026 03:35:45 GMT

Number of Pages: 39

Markdown Content:
# Neural Network Machine Regression (NNMR): A Deep Learning Framework for Uncovering High-order Synergistic Effects 

## Jiuchen Zhang Department of Biostatistics, University of Michigan and Ling Zhou Center of Statistical Research and School of Statistics, Southwestern University of Finance and Economics and Peter Song Department of Biostatistics, University of Michigan February 3, 2026 

Abstract 

We propose a new neural network framework, termed Neural Network Machine Regression (NNMR), which integrates trainable input gating and adaptive depth regularization to jointly perform feature selection and function estimation in an end-to-end manner. By penalizing both gating parameters and redundant layers, NNMR yields sparse and interpretable architectures while capturing complex nonlinear relationships driven by high-order synergistic effects. We further develop a post-selection inference procedure based on split-sample, permutation-based hypothesis testing, enabling valid inference without restrictive parametric assumptions. Compared with existing methods, including Bayesian kernel machine regression and widely used post hoc attribution techniques, NNMR scales efficiently to high-dimensional feature spaces while rigorously controlling type I error. Simulation studies demonstrate its superior selection accuracy and inference reliability. Finally, an empirical application reveals sparse, biologically meaningful food group predictors associated with somatic growth among adolescents living in Mexico City. 

Keywords: post-selection inference, depth regularization, input gating 1

> arXiv:2602.02172v1 [stat.ME] 2 Feb 2026

# 1 Introduction 

In many scientific fields, such as biomedical research, genomics, epidemiology, and envi-ronmental science, researchers work with high-dimensional datasets where the number of potential explanatory variables far exceeds the number of observations. Identifying a relevant subset of variables is crucial for improving model interpretability, reducing overfitting, and enhancing predictive performance. Variable selection has a long history in linear modeling, where sparsity penalties such as the LASSO (Tibshirani 1996) and smoothly clipped absolute deviation (SCAD; Fan & Li 2001) are now routine. However, many scientific questions involve nonlinear, possibly high -order interactions that lie well beyond the scope of linear assumptions. To accommodate such complexity, research has shifted toward nonparametric frameworks that let the data reveal flexible functional forms while still isolating the influential predictors. Early efforts extended the linear model to additive structures in which each covariate enters through an unspecified univariate function (Hastie 2017). Although additive models inherit interpretability and can be equipped with component -wise selection rules, they may miss interaction effects unless higher -order additive terms or interaction kernels are incorporated, a step that greatly complicates both estimation and feature selection. Complementary strategies tackle the “curse of dimensionality” by screening rather than penalizing. Sure independence screening (SIS; Fan & Lv 2008) and its variants rank predictors through marginal utilities to select the feature space before more elaborate modeling; however, their reliance on marginal signals can overlook variables that act solely through interactions. Fully Bayesian machinery, typified by Bayesian kernel machine regression (BKMR; Bobb et al. 2015), integrates variable selection, nonlinear response surfaces, and uncertainty quantification in a single coherent framework, but at the expense of Markov chain Monte Carlo computation that scales poorly with thousands of samples or 2covariates. Collectively, these developments underscore a persistent tension in nonparametric regression: balancing modeling flexibility, statistical efficiency, computational feasibility, and interpretability. Deep neural networks pose a dual challenge for feature selection: the parameterization is massively over -complete, yet the correspondence between weights and inputs is highly entangled. Post hoc attribution tools such as SHAP (Lundberg & Lee 2017), Integrated Gradients (Sundararajan et al. 2017), and deepLIFT (Shrikumar et al. 2017) estimate ex post importance scores by propagating gradients or relevance values back to the input layer. These methods require only a trained model and are widely used in practice, but they do not impose sparsity during training; their scores can be unstable under collinearity and provide no formal guarantee that low -scoring variables are irrelevant—limitations that become acute in high-dimensional biomedical applications. To induce sparsity within neural networks, two embedded approaches have been proposed. Deep feature selection inserts a sparse linear layer at the network input and applies an ℓ1

penalty to its weights, thereby jointly learning both a representation and feature importance in an end-to-end fashion (Li et al. 2016). Building on this framework, nonlinear variable selection methods introduce a continuous ℓ0 relaxation to selection layer, yielding rigorous convergence guarantees and selection consistency under a generalized stable Hessian condition (Chen et al. 2021). Group Lasso regularization instead treats all outgoing connections of each input variable as a group, which prunes entire neurons or inputs to produce highly compact networks without extensive manual grouping (Scardapane et al. 2017). However, these methods still lack automated control over both network depth and width, offer limited support for valid post-selection inference. In this paper, we propose a neural network-based variable selection method tailored to effectively identify relevant features in high-dimensional settings. Our approach incorporates 3several innovations that enhance interpretability, scalability, and statistical rigor. First, we introduce a trainable feature-gating mechanism at the neural network’s input layer, where each feature is weighted by a learnable parameter. Coupled with an L1-penalty analogous to the LASSO (Tibshirani 1996), this mechanism directly induces sparsity, explicitly identifying a concise subset of predictors most strongly associated with the outcome. Second, our method includes an adaptive thresholding procedure during training, periodically removing features whose gating parameters remain persistently small. Additionally, we dynamically prune redundant hidden layers by replacing them with identity mappings if their contributions become negligible. These two procedures promote structured sparsity across both input features and network depth, resulting in a compact and interpretable model. Third, we establish theoretical guarantees for our approach by deriving an risk upper bound, achieving minimax optimality under mild conditions. Combined with our proposed data-splitting and permutation-based inference framework, our method rigorously controls type-I errors in high-dimensional analyses, effectively reducing false discoveries. Our neural network-based variable selection approach exhibits several advantages over existing methods, particularly in contexts involving large-scale biomedical and genetic data. Unlike Bayesian kernel machine regression (BKMR) (Bobb et al. 2015), which in-volves computationally expensive kernel inversions, or traditional screening approaches (Fan & Lv 2008), our framework scales efficiently to hundreds or even thousands of predic-tors. Moreover, in contrast to popular post hoc interpretation methods such as SHapley Additive exPlanations (SHAP) or Integrated Gradients—often unstable or misleading in high-dimensional settings—our method directly incorporates interpretability within the model training process through structured penalties. Overall, our unified framework simultaneously addresses feature selection, adaptive neural 4network optimization, and rigorous statistical inference, positioning our method as a powerful advancement for interpretable, scalable, and statistically rigorous feature selection in contemporary statistical modeling and deep learning applications. 

# 2 Methodology 

We consider variable selection within the framework of neural networks. Suppose we observe independent and identically distributed samples {(Xi, Y i)}ni=1 , where Yi ∈ R is a scalar response and Xi ∈ Rd is a high-dimensional vector of predictors. Our objective is to simultaneously estimate the unknown relationship g(·) between predictors and response, and identify a sparse subset of relevant features. To accomplish this, we embed the variable selection directly into the neural network training process. Specifically, we estimate g by minimizing the empirical squared-error loss: 

ˆg = arg min 

> g∈G

1

n

> n∑
> i=1

(Yi − g(Xi)) 2, (1) where G denotes the class of neural networks designed explicitly to perform variable selection. 

## 2.1 Neural Network 

We set G to be a function class consisting of multi-layers neural networks with a ReLU activation function to approximate the conditional expectation: 

G := N N (W, D, S, B),

where the input data is the predictor Xi, forming the first layer, and the output is the last layer of the network. This network G has width D, which includes D hidden layers and 

D + 2 total layers. Let Nl denote the width of layer l for l = 0 , . . . , D, D + 1 , which is the number of nodes in each layer. Specifically, N0 = d represents the input dimension of X,5and ND+1 = 1 represents the response Y . The width W is the maximum width among the hidden layers: 

W = max( N1, . . . , N D).

Without loss of generality, we consider the same width for all hidden layers in this paper. The size S is the total number of parameters in the network G, given by S = ∑D 

> l=0

Nl+1 × (Nl + 1) .In particular, a ReLU neural network with D hidden layers is a collection of mappings 

g : RN0 → RND+1 of the form g(x) = hD ◦σ◦hD− 1◦· · ·◦ σ◦h0(x), where h1◦h2(x) := h1(h2(x)) 

represents the composition of two functions h1 and h2; σ(x) := max (x, 0) is the ReLU function, which is applied elementwise; hl(x) := Wlx + cl is an affine transformation with 

Wl ∈ RNl+1 ×Nl and cl ∈ RNl . We assume that every function g ∈ G satisfies ∥g∥∞ ≤ B for some 0 < B < ∞, where ∥g∥∞ is the supnorm of the function g.

Figure 1: Simple Multi-layer Neural Network 62.2 Variable Selection 

In high-dimensional nonparametric regression problems, the response variable Y often depends on only a small subset of the available features, while the rest contribute little to predictive accuracy. This suggests that the data lies on a low-dimensional manifold embedded in the high-dimensional feature space. By leveraging this assumption, we aim to identify and retain only the most relevant features, reducing computational complexity while maintaining predictive performance. To exploit this sparsity we attach to the input layer a trainable gating vector α =(α1, . . . , α d)⊤ ∈ Rd and feed the re-weighted input 

> ˜

X = α ⊙ X, (α ⊙ X)j = αj Xj ,

into the network. This transformation allows the model to learn which features are important by setting some entries of α to zero, effectively removing those variables from consideration. The resulting feature selection mechanism is embedded within the neural network architecture, ensuring that only a subset of the input dimensions contributes to the final model. To formalize the selection process, we incorporate L1 regularization on α, which encourages sparsity by penalizing small coefficient values. If α has exactly d0 nonzero elements, then ˜X

can be viewed as a d0-dimensional vector, allowing the model to learn a function g : Rd0 → R.This leads to the following optimization problem: 

Ln(g) = 1

n

> n∑
> i=1
> {

Yi − g(Xi)}2

+ λ1∥α∥1,

Here, g ∈ G (α, θ) is a neural network that maps the selected features to Y , where G(α, θ) = 

{g : g = ϕ(α ⊙ X), ϕ ∈ G} , θ = ( Wl, c l, l = 0 , · · · , D), and ∥ · ∥ 1 serves as an L1 norm that ensures sparsity in α. This formulation not only selects the most informative features 7but also learns the nonlinear mapping between those features and the response variable. Unlike traditional feature selection methods that operate in a separate preprocessing step, our approach integrates feature selection into the model training process, allowing it to dynamically adapt the selected features based on the data. Compared to existing feature selection techniques such as Bayesian Kernel Machine Re-gression (BKMR) and SHAP-based methods, our approach offers significant advantages in terms of scalability and efficiency. BKMR, while powerful in modeling nonlinear in-teractions, suffers from high computational costs due to kernel matrix inversion, making it impractical for large-scale problems. Similarly, SHAP-based methods, which estimate feature importance post hoc, do not enforce sparsity and can be computationally expensive for high-dimensional data. In contrast, our method is fully compatible with GPU-accelerated deep learning frameworks, allowing it to scale efficiently. The integrated feature selection mechanism eliminates the need for separate preprocessing steps, reducing overhead. Moreover, by explicitly learning a sparse representation, our approach ensures that the model remains interpretable while retaining predictive power. The input -gating layer reduces the width of the effective feature space, but the depth of the network remains fixed. Retraining a new architecture each time the size of the active set changes is computationally infeasible. Following Tan et al. (2024), a hidden layer l is redundant when its linear map hl(x) = Wlx simply forwards its input, i.e. Wl = I and 

cl = 0 , where I is the identity matrix. Note that the bias term has been absorbed into the augmented input. This motivates the depth penalty, 

DP( θ) = 

> D∑
> l=1

(∥Wl − I∥1 + |cl|),

8which is zero precisely when every dispensable layer collapses to the identity. Combining input gating and depth regularisation, the final training problem becomes 

(̂ α,̂ θ) = arg min  

> g∈G (α,θ)
> {

1

n

> n∑
> i=1
> {

Yi − g(Xi)}2

+ λ1∥α∥1 + λ2

> D∑
> l=1

(∥Wl − I∥1 + |cl|)

> }

, (12) with λ1 controlling sparsity and λ2 regulating depth. During optimization each Wl is softly shrunk toward the identity matrix whose penalty terms converge to zero can be pruned after training, yielding a compact architecture that matches the complexity of the selected feature subset while retaining the expressive power of deep ReLU networks. 

## 2.3 Inference Procedure after Variable Selection 

Following variable selection, rigorous statistical inference is required to assess the significance of the retained predictors. We employ a data-splitting strategy (Cox 1975, Hurvich & Tsai 1990) combined with the model-free test of conditional independence (Cai et al. 2022). Specifically, we partition the sample into two parts—one for both variable selection and estimation of conditional mean functions, and the other solely for inference—thereby preserving maximum power. Specifically, we divide the original dataset into two subsets: one for variable selection and estimation (denoted D1), and another purely for inference (denoted D2). The first subset 

D1 is used both to identify important predictors through the penalized neural network described in Section 2.2 and to estimate conditional expectations for inference purposes. The inference step directly utilizes the entire second subset D2 without further splitting, enhancing statistical power. Let the selected variables identified from D1 be represented as XS . The inference procedure involves testing the null hypothesis: 

H0 : E(Y | X−j,S ) = E(Y | XS ) versus H1 : E(Y | X−j,S )̸ = E(Y | XS ),

9where Xj denotes potential variable of interest and X−j,S denotes the rest of variables in 

XS excluding Xj .Following the methodology of Cai et al. (2022) with our enhanced approach, we fit two nonparametric models using data from D1: one model incorporating both XS and Z,denoted by ˆg1(XS , and a null model that includes only X−j,S , denoted by ˆg0(X−j,S ). To assess the predictive performance of these models, we perform two-sample comparisons on the residuals from these models using the inference dataset D2. Specifically, we utilize the two-sample t-test (TS), defined as the average difference in squared residuals between the two models. The test statistic TT S is computed as: 

TT S = 1

n2

> ∑
> i∈D2
> [

{Yi − ˆg1(XS,i )}2 − { Yi − ˆg0(X−j,S,i )}2]

,

and we reject the null hypothesis H0 if TT S is significantly negative. This test evaluates the difference in mean squared prediction errors between the two models, under the assumption that second-order moments exist for the data. 10 Algorithm 1: Inference procedure with permutation testing 

Input: Training data D1, inference data D2, number of permutations B, selected variable XS obtained from D1

Output: Test statistic and p-value  

> 1

Estimate ˆg1(XS ) and ˆg0(X−j,S ) using D1; 

> 2

Evaluate both models on D2 and calculate residuals:  

> 3

Ui = Yi − ˆg1(XS,i ), 

> 4

Vi = Yi − ˆg0(X−j,S,i ), for all i ∈ D2; 

> 5

Define the pooled set S = {Ui} ∪ { Vi}; 

> 6

Compute observed test statistic T using {Ui}, {Vi}; 

> 7

for b = 1 to B do  

> 8

Randomly partition S into two equal-sized sets {U ∗ 

> i

}, {V ∗ 

> i

}; 

> 9

Compute test statistic T ∗ 

> T S,b

using {U ∗ 

> i

}, {V ∗ 

> i

}; 

> 10

Compute p-value: ˆpT S = 1 

> B
> ∑Bb=1

I{T > T ∗

> T S,b

};The significance of this incremental predictive power is evaluated by permutation tests, as described in Algorithm 1. Estimation of ˆg1(XS ) and ˆg0(X−j,S ) using D1 increases statistical efficiency and power by fully utilizing the inference dataset, ensuring robust inference without restrictive parametric assumptions. Additionally, our inference framework is flexible, allowing for individual testing of each selected variable separately, or focusing solely on specific variables of particular interest based on the research objectives. In addition, under suitable conditions, one can show that the asymptotic Gaussianity of the t statistic TT S has been established in Lei (2020). As a result, the last step of the p value calculation can be modified by first estimating the standard derivation of {T ∗

> T S,b

, b = 1 , ..., B },denoted as ˆσB , and then calculating the p value as Φ−1(T / ˆσB ), where Φ is the cumulative distribution function of standard normal distribution. This will provide a p value with 11 relatively high resolution. 

# 3 Theory 

In this section, we present theoretical results concerning the consistency and validity of our neural network-based variable selection approach. We begin by formally defining the theoretical framework underlying our methodology. Let X[j] be the j-th component of X. Then X[j] is defined as conditionally unimportant if and only if 

E(Y | X[−j] = x[−j]) = E(Y | X = x), (2) where X[−j] = ( X[1] , · · · , X [j−1] , X [j+1] , · · · , X [d]), and x−j is defined similarly. We assume X is supported on a bounded set, and for simplicity, we assume this bounded set to be [0 , 1] d. In the rest of the paper, the constant c denotes a positive constant that may vary across different contexts. We separate X into Xs ∈ [0 , 1] d0 and Xc ∈ [0 , 1] d−d0 , denoting the conditional important and unimportant variables, respectively. For any x ∈ [0 , 1] p,

E(Y | Xs = xs) = E(Y | X = x).Let g∗ be the minimizer of the population loss, that is, 

g∗ = arg min  

> g∈H β([0 ,1] p,B)

E (Yi − g(Xi)) 2 ,

where the minimizer is taken over the entire space and thus implies that g∗ does not necessarily belong to the FNN set G(α, θ). Clearly, based on the definition (2) g∗(X)

depends on Xs only. To simplify notation, we assume the first d components of X are important, and divide α into two parts α = ( αs, αc) with αs ∈ Rd and αc ∈ Rp−d

corresponding to the coefficients of Xs and Xc, respectively. 12 We establish the large sample property of ˆg in terms of its excess risk, which is defined as the difference between the risk of g and g∗:

R(g) − R (g∗) = E (Yi − g(Xi)) 2 − E (Yi − g∗(Xi)) 2 .

We construct an estimator of g∗ within G(α, θ) with network parameters θ and variable importance index parameters α. Thus, referring to the optimal network solution 

g∗G = arg min  

> g∈G (α,θ)

E (Yi − g(Xi)) 2 ,

we define a set of (α, θ) as 

Θ = {(α, θ) : R(gα,θ) = R(g∗G ), g α,θ ∈ G (α, θ)}.

With proper conditions on the data structure and network class, g∗G can approach g∗ close enough such that αc = 0 and αs̸ = 0 . That is, for any (α, θ) ∈ Θ, there exists a positive constant c > 0 such that min 1≤j≤d |αj | ≥ c. Further, there exists a solution αs̸ = 0 , αc = 0

such that (αs, 0, θ) ∈ Θ still holds. These properties of g∗G and Θ are given in Lemma 7.1 in the Supplementary Materials. In practice, the network parameters ( ˆα, ˆθ) obtained in (12) lies outside of Θ. To measure the distance between (α, θ) and Θ, we define metric d(( α, θ), Θ) := min ϑ∈Θ ∥(αθ

> )

− ϑ∥22.Clearly, d(( ˆα, ˆθ), Θ) → 0 is sufficient for ˆαs to be bounded away from zero. Thus, we can establish the selection results via establishing the convergence rate of the proposed penalized network ˆgG in terms of its parameters. We define G| x := {g(x1), g (x2), · · · , g (xn) : g ∈ G (α, θ)} for a given sequence x =(x1, · · · , xn) and denote N2n = sup x N2n(δ, ∥ · ∥ ∞, G| x) as the covering number of G| x

under the norm ∥ · ∥ ∞ with radius δ. Let A ⪯ B represent A ≤ cB for a positive constant c,

A ∧ B = min (A, B ), and Φ1 ◦ Φ2 := {ϕ1 ◦ ϕ2 : ϕ1 ∈ Φ1, ϕ 2 ∈ Φ2} represents the composition of two function classes Φ1 and Φ2.13 The next conditions are needed to establish the theoretical results: (C1) The dimension of conditional important variables d is fixed, and there exists a constant 

τd > 0 such that 

min 

> 1≤j≤d
> ∣∣∣∣∣

E(Y | X[−j] = x[−j]) − E(Y | X = x) 

> ∣∣∣∣∣

≥ cτ d,

for some positive constant c.(C2) Assume Y is a sub-Gaussian random variable. (C3) Function class for gα,θ and g∗: for any function gα,θ ∈ G and the true function g∗, we assume ∥gα,θ∥∞ < B and ∥g∗∥∞ < B.

Theorem 3.1 Suppose the conditions (C1)-(C3) hold. If λ21 ⪯ log 2 n log N2n 

> nd

+ (R(g∗G )−R (g∗))

> d

and λ22 ⪯ log 2 n log N2n 

> nW2D

+    

> (
> R(gˇα,ˇθ)−R (g∗)
> )
> W2D

, ( ˆ α, ˆθ) defined in (12) satisfies 

E[d(( ˆ α, ˆθ), Θ)] ⪯ log 2 n log N2n

n + log 2 n (

R(g∗G ) − R (g∗))

E∥ ˆαc∥22 ⪯ log 2 n log N2n

n + log 2 n (

R(g∗G ) − R (g∗))

,

where E is taken with respect to (Xi, Y i)ni=1 .

Now, we further explore how the error relies on the FNN structure and the function class to which g∗ belongs. We consider the Hölder class, which is broad enough to cover most applications. In particular, denote ⌈a⌉ and ⌊a⌋ to be the smallest integer no less than a and the largest integer strictly smaller than a, respectively. Let N+ be the set of positive integers and N0 be the set of nonnegative integers. Let β = s + r, r ∈ (0 , 1] and s = ⌊β⌋ ∈ N0. For a finite constant B0 > 0, the H ¨older class Hβ ([0 , 1] d, B 0) is defined as 

Hβ ([0 , 1] d, B 0) = {g : [0 , 1] d 7 → R, max  

> ∥α∥1<s

∥∂αg∥∞ ≤ B0,

max  

> ∥α∥1=s

sup 

> x̸=y

|∂αg(x) − ∂αg(y)|∥x − y∥r

> 2

≤ B0},

where ∂α = ∂α1 · · · ∂αd with α = ( α1, · · · , α d)⊤ ∈ Nd 

> 0

and ∥α∥1 = ∑di=1 |αi|.14 Based on Theorem 3.3 of Jiao et al. (2023) for the approximation error in terms of FNN structures and Theorem 3 and 7 of Bartlett et al. (2019) for the bounding covering number, we can conclude the following Corollary 3.1 from Theorem 3.1: 

Corollary 3.1 Given H ¨older smooth functions g∗ ∈ H β ([0 , 1] d, B 0), for any D ∈ N+,

W ∈ N+, under conditions of Theorem 3.1, conditions of Theorem 3.3 in Jiao et al. (2023) and Theorem 3 and 7 in Bartlett et al. (2019), if the FNN with a ReLU activation function has width W = c(⌊β⌋ + 1) 2d⌊β⌋+1 W ⌈log 2(8 W )⌉ and depth D = c(⌊β⌋ + 1) 2D ⌈log 2(8 D)⌉

and λ1 ⪯ n−1SD log (S)/d + ( W D )−4β/d /d and λ2 ⪯ n−1S log (S)/W2 + ( W D )−4β/d /(W2D),then 

E[d(( ˆ α, ˆθ), Θ)] ⪯ (log 2 n)n−1SD log( S) + ( W D )−4β/d (log 2 n).

E∥ ˆαc∥22 ⪯ (log 2 n)n−1SD log( S) + ( W D )−4β/d (log 2 n).

To facilitate reading, Theorem 3.3 of Jiao et al. (2023) and Theorems 3 and 7 of Bartlett et al. (2019) are also shown in Lemmas and in Appendix. In Corollary 3.1, the first term comes from the covering number of G, which is bounded by its VC dimension log N2n(n−1, ∥ · ∥∞, G| x) = O(SD log (S/n −1)) (Bartlett et al. 2019), where S and D are the total number of parameters and hidden layers, respectively. The second term follows from the approximation results from Jiao et al. (2023) that ∥∥∥g∗ − g∗G 

> ∥∥∥∞

≤ 18 B0(⌊β⌋ + 1) 2d⌊β⌋+max {β, 1}/2(W D )−2β/d 

and E(R(g∗G ) − R (g∗)) ≃ E|g∗G − g∗|2, where A ≃ B represents A ⪯ B and B ⪯ A.

Corollary 3.2 Suppose the conditions of Corollary 3.1 hold. If λ1 ⪯ n−1SD log (S)/d +(W D )−4β/d /d and λ2 ⪯ n−1S log (S)/W2 + ( W D )−4β/d /(W2D), for any j = 1 , · · · , d and 

k = d + 1 , · · · , p , it holds that Pr ({| ˆαj | ≥ τd} ∩ {| ˆαk| ≤ τd}) → 1,

where τd is defined in Condition (C1). 

15 4 Computation 

Optimising the composite objective 

J (g, α) = 1

n

> n∑
> i=1
> {

Yi − g(α ⊙ Xi)}2

+ λ1∥α∥1 + λ2

> D∑
> l=1

(∥Wl − I∥1 + |cl|) (11) is challenging because the L1 penalties on α and on each Wl − I are nondifferentiable at zero, yet we need exact zeros to identify both irrelevant inputs and redundant layers. We therefore employ a periodic hard-thresholding (truncation). Continuous shrinkage via these gradient descent steps ensures that small coefficients approach zero, while exact zeros are enforced by truncation every K iterations. In the truncation step, any αj satisfying |αj | ≤ τ1 is set to zero, and any layer l for which 

∥Wl − I∥1 ≤ τ2 is collapsed by resetting Wl = I. Threshold τ1 is selected based on validation performance, and we follow Scardapane et al. (2017) in fixing τ2 = 10 −2 for synthetic studies and 10 −3 for real data. By combining subgradient shrinkage with periodic truncation, the algorithm achieves stable convergence, exact sparsity in both α and network depth, and full compatibility with GPU-accelerated training pipelines. 16 Algorithm 2: Truncation procedure 

Input : gate vector α; weight matrices {Wl}; thresholds τ1, τ 2

Output: αtrunc , {W trunc  

> l

} 

> 1

αtrunc ← α 

> 2

for j = 1 to d do  

> 3

if |αj | ≤ τ1 then  

> 4

αj ← 0 

> 5

for l = 1 to D do  

> 6

if ∥Wl − I∥1 ≤ τ2 then  

> 7

Wl ← I 

> 8

return αtrunc , {W trunc  

> l

}

# 5 Simulations 

We conduct simulation studies to evaluate the empirical performance of the proposed neural network-based variable selection method. In particular, we assess its ability to identify the true set of active predictors and compare it against several widely used approaches: Bayesian Kernel Machine Regression (BKMR), SHapley Additive exPlanations (SHAP), and DeepLIFT. The evaluation focuses on variable selection accuracy under a complex nonlinear model with sparse signal. 

## 5.1 Consistency of Variable Selection 

Data are generated from the nonlinear model 

Y = X30 (X22 + X5) − | X7| cos( X8) + ε, 

where ε ∼ N (0 , 1) and each predictor Xj ∼ N (0 , 1) independently for j = 0 , . . . , 199 . Only five coordinates {0, 2, 5, 7, 8} enter the truth, while the remaining d − 5 = 195 variables 17 are noise. We take n = 1000 samples and repeat the entire simulation 100 times to assess variability. We compare our method against five established approaches. Bayesian kernel machine regression (BKMR) (Bobb et al. 2015) fits a Gaussian-process-type model and uses poste-rior inclusion probabilities for variable selection. SHapley additive explanations (SHAP) (Lundberg & Lee 2017) compute Shapley values post hoc on a pretrained neural network to rank feature importance. DeepLIFT (Shrikumar et al. 2017) is a backpropagation-based attribution method that compares activations to a reference and assigns contribution scores to each input. GLNN (Scardapane et al. 2017) applies group Lasso regularization to neural networks, penalizing groups of weights. Deep feature selection (DPS) (Li et al. 2016, Chen et al. 2021) incorporates sparsity constraints via selection layers embedded in deep architec-tures. For SHAP and DeepLIFT, we use the same network architecture as our method but without additional adaptive thresholding or depth adjustment, while for GLNN and DPS, we use the architectures specified in their original papers. For all competing methods except BKMR and the proposed method, the top five features are selected by weight magnitude. Selection accuracy is quantified by the following metrics: Precision = | ˆSn ∩ S⋆|| ˆSn| , Recall = | ˆSn ∩ S⋆||S⋆| , F1 = 2 Precision × Recall Precision + Recall .

Table 1 reports the mean and standard deviation of these metrics over 100 simulation replicates. 18 Table 1: Variable selection performance across competing methods. Method Precision Recall F1 Score Proposed 0.927 (0.150) 0.880 (0.183) 0.879 (0.135) BKMR 0.025 (0.000) 1.000 (0.000) 0.049 (0.000) SHAP 0.780 (0.060) 0.780 (0.060) 0.780 (0.060) DeepLIFT 0.720 (0.133) 0.720 (0.133) 0.720 (0.133) GLNN 0.460 (0.100) 0.460 (0.100) 0.460 (0.100) DPS 0.244 (0.083) 0.244 (0.083) 0.244 (0.083) As shown in Table 1, our proposed method achieves an F1 score of approximately 0.88, significantly outperforming competing approaches. BKMR, despite achieving perfect recall, exhibits extremely low precision and consequently poor overall selection performance (F1 ≈ 0.05 ), indicating severe over-selection. SHAP and DeepLIFT yield moderate selection accuracy but lag behind our proposed method, likely due to their post hoc attribution nature that ignores structured sparsity during training. GLNN and DPS show considerably weaker performance, reflecting limitations in selecting highly relevant nonlinear features under limited depth or overly restrictive sparsity constraints. These findings underscore the advantage of embedding adaptive sparsity constraints and automatic depth selection directly into the neural network training process, thereby substantially enhancing variable selection accuracy and interpretability in high-dimensional, nonlinear modeling scenarios. 

## 5.2 Post-selection Inference 

After demonstrating the consistency of our variable selection procedure, we examine its impact on post-selection inference. To this end, we test the null hypothesis 

H0 : E(Y | X1, Z ) = E(Y | Z),

19 where Z denotes the set of covariates conditioned upon. In the “no-selection” scenario, Z

comprises all predictors except X1; in the post-selection scenario, Z includes only those features retained by our variable-selection procedure (excluding X1). We then apply the permutation-based test outlined in Section 2.3 to the inference data D2 under both scenarios, and report the empirical Type I error rates in Table 2. Table 2: Empirical Type I error rates for testing the null hypothesis H0 : E(Y | X1, Z ) = 

E(Y | Z) with sample size n = 1000 and 500 simulation replicates. Results are shown for inference conducted with and without a preceding variable selection step. Inference setting Type I error No selection 0.13 Post-selection 0.01 Table 2 demonstrates that inference without prior selection inflates the Type I error to 13%, whereas conditioning on the data-driven subset restores nominal control at 1%. This confirms that our post-selection inference procedure maintains valid error rates even after feature selection. 

# 6 Real Data Application 

The real data analysis uses the Early Life Exposure to Environmental Toxicants (ELEMENT) Project. ELEMENT consists of three mother–child cohorts recruited in Mexico City from 1994 to 2005. More than 2,000 women and their children were enrolled and followed from pregnancy through adolescence. We apply these data to evaluate a nonparametric Neural Network Machine Regression (NNMR) method with statistical inference. Our main goal is to identify which prenatal exposures are most strongly associated with infant 20 growth. Infant growth is measured by WHO anthropometric z-scores considering age and sex. Measurements are collected at regular visits from birth to age five, as well as during cholesterol substudies and at the P20 follow up. The analysis includes a high dimensional panel of dietary patterns grouped by food categories for both mothers and children. We exclude records with missing or implausible z-scores and standardize all exposure variables before modeling. We evaluated the performance of our Neural Network Machine Regression (NNMR) method for variable selection in high dimensional dietary data, using infant growth z-scores as the outcome. We compared NNMR to the competing method mentioned in section 5. All methods were applied to a dataset containing child and mother dietary exposures, with performance evaluated by Akaike Information Criterion (AIC) calculated on separate training and test datasets. Table 3: Average AIC and computation time (seconds) for six competing methods applied to the ELEMENT dietary exposure data. Reported values are means with standard deviations in parentheses, calculated on test sets across 100 replicates. Method AIC Time (s) NNMR 2164.98 (23.66) 6.74 (1.20) BKMR 2230.27 (23.71) 78.21 (20.31) SHAP 2166.99 (25.33) 87.01 (8.86) DeepLIFT 2162.45 (23.33) 3.66 (0.56) GroupLASSO 2195.56 (41.16) 2.68 (0.43) DFS 2167.62 (23.36) 7.69 (1.59) Table 3 shows that NNMR achieves competitive AIC values and computation times compared with DeepLIFT, while outperforming SHAP, DFS, and Group LASSO, and substantially 21 improving upon BKMR. The poorer performance of BKMR reflects its tendency to select excessively large models, often including nearly all variables. DeepLIFT and SHAP yield prediction errors comparable to NNMR, since they share the same underlying model, but DeepLIFT tends to select the smallest variable set. Group LASSO exhibits unstable behavior, either selecting nearly all variables or only one or two. By contrast, NNMR demonstrates modest variability in AIC across splits, highlighting its stable generalization ability. 

Figure 2: Venn diagram of dietary exposures selected by NNMR, SHAP, and DeepLIFT in the ELEMENT study. Displayed are variables with selection frequency exceeding 20% across 100 replicates. Variables in bold indicate those that remained statistically significant after applying the post-selection inference procedure. Figure 2 summarizes variable selection via a Venn diagram, comparing NNMR, SHAP, and DeepLIFT. To improve interpretability, we display only variables selected with frequency exceeding 20% across 100 replicates. The complete results are provided in the Supplementary 22 Materials. Consistent with prior observations, BKMR selects nearly all exposures, while Group LASSO exhibits unstable behavior. DFS selects only milk_child , while SHAP emphasizes maternal food groups. In contrast, NNMR and DeepLIFT consistently highlight child dietary exposures with higher and more coherent selection frequencies. We then applied the inference procedure in Section 2.3 to the union of variables identified by NNMR, DFS, DeepLIFT, and SHAP, using an inference split not employed for selection. Variables highlighted in red in Figure 2 were found to be statistically significant predictors of growth z-scores, the majority of which were prioritized by NNMR. The five variables uniquely retained by NNMR after inference align closely with established findings in the nutritional epidemiology literature. Higher intake of beef, zinc, and choline during infancy has been linked to improved inhibitory control and attention at ages 3 to 5 years (Wilk et al. 2022). Chicken production and consumption in nutrition sensitive agricultural programs have been shown to benefit child growth in low income settings (Passarelli et al. 2020). Fish serves as a nutrient-dense protein source, rich in vitamins, minerals, and essential fatty acids (Yılmaz et al. 2018). Frequent dessert consumption is associated with increased risk of overweight, a major determinant of growth z-scores (Barroso et al. 2016). Finally, the otherveg category, which includes zucchini, cucumber, and green beans, reflects the importance of dietary diversity for child development (Arimond & Ruel 2004, Thorne-Lyman et al. 2019). Collectively, these results underscore the biological plausibility of the predictors identified by NNMR and their relevance to early life growth. 

# 7 Discussion 

We have introduced Neural Network Machine Regression (NNMR), an integrated neural net-work framework designed for simultaneous feature selection, nonlinear function estimation, and rigorous post-selection inference in high-dimensional data analysis. By embedding a 23 trainable gating layer coupled with an L1 regularization strategy and an adaptive threshold-ing mechanism, NNMR achieves direct sparsity enforcement in input features and hidden network layers, resulting in highly compact and interpretable models. Our theoretical analysis provides guarantees for consistent recovery of the relevant feature set, supported by a minimax-optimal upper bound on the risk under mild assumptions. Additionally, our integrated split-sample permutation testing approach ensures robust control over type I error rates, mitigating false-positive risks commonly encountered in high-dimensional inference. Empirical results from extensive simulations illustrate that NNMR outperforms established approaches such as BKMR, SHAP, DeepLIFT, GLNN, and DPS in terms of variable selection precision, recall, and overall accuracy (F 1 score). While BKMR achieves high recall at the cost of severe over-selection, and SHAP and DeepLIFT offer moderate accuracy, NNMR clearly demonstrates superior performance in identifying true predictors and controlling selection errors. GLNN and DPS exhibit considerably lower accuracy, underscoring the limitations of traditional sparsity approaches in capturing complex nonlinear relationships. Real-data applications further validate NNMR’s practical utility by demonstrating its capability to pinpoint meaningful, interpretable predictors in complex biomedical datasets. Unlike post hoc attribution methods, which are often unstable under feature collinearity, NNMR’s built-in sparsity ensures accurate identification of genuinely inactive features. The dynamic pruning of unnecessary layers also facilitates resource-efficient implementations suitable for GPU acceleration. In summary, NNMR bridges interpretability and predictive modeling power by embedding statistically rigorous variable selection, adaptive model optimization, and valid inference within a unified training framework. This integrative approach positions NNMR as a robust and versatile solution for modern high-dimensional statistical and deep learning applications. 24 References 

Anthony, M., Bartlett, P. L., Bartlett, P. L. et al. (1999), Neural network learning: Theoret-ical foundations , Vol. 9, cambridge university press Cambridge. Arimond, M. & Ruel, M. T. (2004), ‘Dietary diversity is associated with child nutritional status: evidence from 11 demographic and health surveys’, The Journal of nutrition 

134 (10), 2579–2585. Barroso, C. S., Roncancio, A., Moramarco, M. W., Hinojosa, M. B., Davila, Y. R., Mendias, E. & Reifsnider, E. (2016), ‘Food security, maternal feeding practices and child weight-for-length’, Applied Nursing Research 29 , 31–36. Bartlett, P. L., Harvey, N., Liaw, C. & Mehrabian, A. (2019), ‘Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks’, The Journal of Machine Learning Research 20 (1), 2285–2301. Bobb, J. F., Valeri, L., Claus Henn, B., Christiani, D. C., Wright, R. O., Mazumdar, M., Godleski, J. J. & Coull, B. A. (2015), ‘Bayesian kernel machine regression for estimating the health effects of multi-pollutant mixtures’, Biostatistics 16 (3), 493–508. Cai, Z., Lei, J. & Roeder, K. (2022), ‘Model-free prediction test with application to genomics data’, Proceedings of the National Academy of Sciences 119 (34), e2205518119. Chen, Y., Gao, Q., Liang, F. & Wang, X. (2021), ‘Nonlinear variable selection via deep neural networks’, Journal of Computational and Graphical Statistics 30 (2), 484–492. Cox, D. R. (1975), ‘A note on data-splitting for the evaluation of significance levels’, 

Biometrika pp. 441–444. Fan, J. & Li, R. (2001), ‘Variable selection via nonconcave penalized likelihood and its oracle properties’, Journal of the American statistical Association 96 (456), 1348–1360. 25 Fan, J. & Lv, J. (2008), ‘Sure independence screening for ultrahigh dimensional feature space’, 

Journal of the Royal Statistical Society Series B: Statistical Methodology 70 (5), 849–911. Hastie, T. J. (2017), ‘Generalized additive models’, Statistical models in S pp. 249–307. Hurvich, C. M. & Tsai, C. (1990), ‘The impact of model selection on inference in linear regression’, The American Statistician 44 (3), 214–217. Jiao, Y., Shen, G., Lin, Y. & Huang, J. (2023), ‘Deep nonparametric regression on approxi-mate manifolds: Nonasymptotic error bounds with polynomial prefactors’, The Annals of Statistics 51 (2), 691–716. Lei, J. (2020), ‘Cross-validation with confidence’, Journal of the American Statistical Association 115 (532), 1978–1997. Li, Y., Chen, C.-Y. & Wasserman, W. W. (2016), ‘Deep feature selection: theory and application to identify enhancers and promoters’, Journal of Computational Biology 

23 (5), 322–336. Lundberg, S. M. & Lee, S.-I. (2017), ‘A unified approach to interpreting model predictions’, 

Advances in neural information processing systems 30 .Passarelli, S., Ambikapathi, R., Gunaratna, N. S., Madzorera, I., Canavan, C. R., Noor, A. R., Worku, A., Berhane, Y., Abdelmenan, S., Sibanda, S. et al. (2020), ‘A chicken production intervention and additional nutrition behavior change component increased child growth in ethiopia: a cluster-randomized trial’, The Journal of Nutrition 150 (10), 2806–2817. Scardapane, S., Comminiello, D., Hussain, A. & Uncini, A. (2017), ‘Group sparse regular-ization for deep neural networks’, Neurocomputing 241 , 81–89. Shrikumar, A., Greenside, P. & Kundaje, A. (2017), Learning important features through 26 propagating activation differences, in ‘International conference on machine learning’, PMlR, pp. 3145–3153. Sundararajan, M., Taly, A. & Yan, Q. (2017), Axiomatic attribution for deep networks, in 

‘International conference on machine learning’, PMLR, pp. 3319–3328. Tan, Z., Zhou, L. & Lin, H. (2024), ‘Generative adversarial learning with optimal input dimension and its adaptive generator architecture’, arXiv preprint arXiv:2405.03723 .Thorne-Lyman, A. L., Shrestha, M., Fawzi, W. W., Pasqualino, M., Strand, T. A., Kvestad, I., Hysing, M., Joshi, N., Lohani, M. & Miller, L. C. (2019), ‘Dietary diversity and child development in the far west of nepal: a cohort study’, Nutrients 11 (8), 1799. Tibshirani, R. (1996), ‘Regression shrinkage and selection via the lasso’, Journal of the Royal Statistical Society Series B: Statistical Methodology 58 (1), 267–288. Wilk, V. C., McGuire, M. K. & Roe, A. J. (2022), ‘Early life beef consumption patterns are related to cognitive outcomes at 1–5 years of age: An exploratory study’, Nutrients 

14 (21), 4497. Yılmaz, E., Aydın, M., Yıldırım, A. & Şahin, P. (2018), ‘The importance of consumption of fish meat in early childhood period in terms of healthy development’, Süleyman Demirel Üniversitesi Eğirdir Su Ürünleri Fakültesi Dergisi 14 (4), 357–364. 

# Appendix 

## Full List of Variable Selection Results in ELEMENT Study 

Table 4 presents the complete list of variables along with their selection frequencies across repeated data splits for the proposed Neural Network Machine Regression (NNMR), SHAP, DeepLIFT, and DFS methods. For SHAP, only variables with selection frequencies greater 27 than 0.05 are displayed to improve readability. Since BKMR and GroupLASSO consistently selected nearly all variables with frequencies close to 1.0 and 0.45, their results are omitted from the table for brevity. This presentation highlights the variables that were most consistently identified as important by the methods that demonstrated more selective behavior. Table 4: Variables with nonzero selection frequencies across repeated splits for NNMR, SHAP (only ≥ 0.05 shown), DeepLIFT, and DFS. BKMR and GroupLASSO selected nearly all variables (frequencies ≈ 1.0 and 0.45) and are omitted for brevity. feature NNMR SHAP DeepLIFT DFS 

atole _child 0.38 – – –

avocado _child 0.15 – 0.25 –

beef _child 0.41 – 0.19 –

chicken _child 0.43 – 0.15 –

chili _child 0.09 – – –

chips _child 0.02 – – –

corn _tortilla _child 0.01 0.08 – –

corncob _child 0.13 – – –

corncob _mom – 0.06 – –

cruveg _child 0.01 – – –

dessert _child 0.31 0.06 – –

egg _child 0.16 – 0.30 –

f ish _child 0.23 – 0.03 –

f ish _mom – 0.06 – –Continued on next page 28 feature NNMR SHAP DeepLIFT DFS 

f ruit _child 0.38 – 0.74 –

f ruit _mom – 0.07 – –

hf _dairy _child 0.01 – 0.16 –

jam _child 0.01 – – –

leaf veg _child 0.39 – – –

legumes _child 0.03 – – –

milk _child 0.02 – 0.19 1.00 

natural _juice _mom – 0.48 – –

organmeat _child 0.16 – 0.03 –

otherveg _child 0.35 – – –

pork _child 0.04 – 0.09 –

potato _child 0.29 – 0.04 –

procmeat _child 0.02 0.06 0.23 –

ref grain _child 0.13 0.12 – –

ref grain _mom – 0.08 – –

soup _child 0.18 – – –

sugar _beverages _child 0.05 0.07 – –

sugar _beverages _mom – 0.13 – –

tomato _child 0.39 – 0.22 –

unsat _oil _child 0.43 – – –

wholegrain _child 0.26 – – –

yeveg _child 0.02 0.09 – –Continued on next page 29 feature NNMR SHAP DeepLIFT DFS 

yogurt _child 0.01 – 0.15 –

yogurt _mom – 0.32 – –

## Proofs of Theorems 

Denote Rn(gα,θ) = 1

> n
> ∑ni=1
> (

Yi − g(Xi))2

, for g ∈ G (α, θ). For any independent and identically distributed (i.i.d.) samples Dn = {Xi, Y i}ni=1 with sample size n. Define 

S(gα,θ, Xi) = (

Yi − g(Xi))2

− (

Yi − g∗(Xi))2

.

Let D′ 

> n

= {X′ 

> i

, Y ′ 

> i

} be another sample independent of Dn, and write 

L(gα,θ, X′ 

> i

) = ED′ 

> n

(S(gα,θ, X′ 

> i

)) − 2S(gα,θ, Xi).

Recall that gθ(α ⊙ · ) ∈ G (α, θ) and 

( ˆ α, ˆθ) ∈ arg min Ln(gα,θ) = arg min 

> (α,θ)
> [

Rn(gα,θ) + λ1∥α∥1 + λ2

> D∑
> l=1

∥Wl − I∥1

> ]

.

Covering number . Given a δ-uniform covering of G, we denote the centers of the balls by gq, q = 1 , · · · , N2n, where N2n = sup x N2n (δ, ∥ · ∥ ∞, G| x) is the uniform covering number with radius δ under the norm ∥ · ∥ ∞. By the definition of covering, there exists a q∗ such that ∥g ˆα, ˆθ − gq∗ ∥∞ ≤ δ on x ∈ (X1, · · · , Xn, X′

> 1

, · · · , X′

> n

). Let A ⪯ B represent A ≤ cB 

for a postive constant c.

Proof of Theorem 3.1 . Let ( ˇ α, ˇθ) ∈ Θ such that 

( ˇ α, ˇθ) ∈ arg min  

> (α,θ)∈Θ

E

> [

d

> ((

ˆα, ˆθ)

, Θ

> )]

.

30 Without loss of generality, we slightly abuse notation by writing ∑D

> l=1

∥Wl − I∥1 to denote 

∑D

> l=1

(∥Wl − I∥1 + |cl|), where the intercept term is absorbed into the weight matrix Wl of the neural network. Then, it follows that 

Rn(g ˆα, ˆθ) + λ1∥ ˆα∥1 + λ2

> D

∑

> l=1

∥ ˆWl − I∥1 ≤ R n(g ˇα, ˇθ) + λ1∥ ˇα∥1 + λ2

> D

∑

> l=1

∥ ˇWl − I∥1.

For the expected excess risk, we have the following decomposition, 

E

(

R(g ˆα, ˆθ) − R (g ˇα, ˇθ)

)

= E (

R(g ˆα, ˆθ) − R n(g ˆα, ˆθ))

+ E (

Rn(g ˆα, ˆθ) − R n(g ˇα, ˇθ))

(3) 

+E(

Rn(g ˇα, ˇθ) − R (g ˇα, ˇθ))

≤ I1 + EDn

[

λ1 (∥ ˇα∥1 − ∥ ˆα∥1) + λ2

> D

∑

> l=1

(

∥ ˇWl − I∥1 − ∥ ˆWl − I∥1

)]

. (4) Note that the first term has the following upper bound: 

I1 := E (

R(g ˆα, ˆθ) − R n(g ˆα, ˆθ))

= EDn

{

n−1

> n

∑

> i=1

[

ED′

> n

(

n−1

> n

∑

> i=1

(Y ′ 

> i

− gˆθ( ˆ α ⊙ X′ 

> i

)) 2

)

(5) 

− (Yi − gˆθ( ˆ α ⊙ Xi)) 2]} 

= EDn

{

n−1

> n

∑

> i=1

[

ED′

> n

(

n−1

> n

∑

> i=1

S(g ˆα, ˆθ, X′ 

> i

)

)

− 2S(g ˆα, ˆθ, X′ 

> i

)

]} 

+EDn

{

n−1

> n

∑

> i=1

[(

Yi − gˆθ( ˆ α ⊙ Xi))2

− (

Yi − gˇθ( ˇ α ⊙ Xi))2]}

+EDn

{

n−1

> n

∑

> i=1

S(gˇθ, ˇα, X′ 

> i

)

}

≤ I11 + EDn

[

λ1 (∥ ˇα∥1 − ∥ ˆα∥1) + λ2

> D

∑

> l=1

(

∥ ˇWl − I∥1 − ∥ ˆWl − I∥1

)]

+ (

R(g ˇα, ˇθ) − R (g∗))

. (6) Next, we will give an upper bound of I11 and handle it with truncation and classical chaining 31 technique of empirical processes. According to the definition of S(gα,θ, Xi), we have 

∣∣∣S(g ˆα, ˆθ, X′ 

> i

) − S(gq∗ , X′ 

> i

)∣∣∣

= ∣∣∣2Yi

(

gˆθ( ˆ α ⊙ Xi) − gq∗ (Xi))

+ (

g2ˆθ( ˆ α ⊙ Xi) − g2 

> q∗

(Xi))∣ ∣∣

≤ (2 |Yi| + 2 B)δ, 

and 

∣∣∣L(g ˆα, ˆθ, X′ 

> i

) − L(gq∗ , X′ 

> i

)∣∣∣

≤ ED′

> n

(∣ ∣∣S(g ˆα, ˆθ, X′ 

> i

) − S(gq∗ , X′ 

> i

)∣∣∣)

+ 2 ∣∣∣S(g ˆα, ˆθ, X′ 

> i

) − S(gq∗ , X′ 

> i

)∣∣∣

≤ 3( E|Yi| + B + |Yi|)δ. 

Then, it follows that 

EDn

(

n−1

> n

∑

> i=1

∣∣∣L(g ˆα, ˆθ, X′ 

> i

) − L(gq∗ , X′ 

> i

)∣∣∣)

≤ 6( E|Yi| + B)δ, 

which leads to 

EDn

(

n−1

> n

∑

> i=1

L(g ˆα, ˆθ, X′ 

> i

)

)

≤ EDn

(

n−1

> n

∑

> i=1

L(gq∗ , X′ 

> i

)

)

+ 6( E|Yi| + B)δ. (7) Let 0 < β n be a positive number who may depend on the sample size n. Denote Tβn Y = Y

if Y ≤ βn and Tβn Y = βn otherwise. Define the function g∗ 

> βn

by 

g∗ 

> βn

(x) = arg min  

> g:∥g∥∞<B

E

[(

Tβn Yi − g(Xi))2

| Xi = x

]

.

For any g ∈ G (α, θ), let Sβn (gα,θ, X′ 

> i

) = (

Tβn Yi − gθ(α ⊙ Xi))2

− (

Tβn Yi − g∗ 

> βn

(Xi))2

. Then, 32 we have 

E(

S(gα, θ, X′ 

> i

))

= E(

Sβn (gα, θ, X′ 

> i

))

+ E

[(

Yi − gθ(α ⊙ Xi))2

− (

Tβn Yi − gθ(α ⊙ Xi))2]

−E

[(

Yi − g∗(Xi))2

− (

Tβn Yi − g∗(Xi))2]

−E

[(

Tβn Yi − g∗(Xi))2

− (

Tβn Yi − g∗ 

> βn

(Xi))2]

≤ E(

Sβn (gα, θ, X′ 

> i

))

+ E(

Y 2 

> i

− (Tβn Yi)2 − 2( Yi − Tβn Yi)gθ(α ⊙ Xi))

−E(

Y 2 

> i

− (Tβn Yi)2 − 2( Yi − Tβn Yi)g∗(Xi))

≤ E(

Sβn (gα, θ, X′ 

> i

))

+ 4 E(|Yi|I(Yi > β n)) B,

and 

E(

Sβn (gα, θ, X′ 

> i

))

= E(

S(gα, θ, X′ 

> i

))

− E

[(

Yi − gθ(α ⊙ Xi))2

− (

Tβn Yi − gθ(α ⊙ Xi))2]

−E

[(

Tβn Yi − g∗ 

> βn

(Xi))2

− (

Yi − g∗ 

> βn

(Xi))2]

−E

[(

Yi − g∗ 

> βn

(Xi))2

− (

Yi − g∗(Xi))2]

≤ E(

S(gα, θ, X′ 

> i

))

− E(

Y 2 

> i

− (Tβn Yi)2 − 2( Yi − Tβn Yi)gθ(α ⊙ Xi))

+E(

Y 2 

> i

− (Tβn Yi)2 − 2( Yi − Tβn Yi)g∗ 

> βn

(Xi))

≤ E(

S(gα, θ, X′ 

> i

))

+ 4 E(|Yi|I(Yi > β n)) B,

which leads to 

∣∣∣∣∣E(

S(gα, θ, X′ 

> i

) − Sβn (gα, θ, X′ 

> i

))∣∣∣∣∣ ≤ 4E(|Yi|I(Yi > β n)) B.

Then, 

∣∣∣∣∣EDn

[ 1

n

> n

∑

> i=1

(

L(gq∗ , X′ 

> i

) − Lβn (gq∗ , X′ 

> i

))] ∣ ∣∣∣∣

≤

∣∣∣ED′

> n

(

S(gq∗ , X′ 

> i

) − Sβn (gq∗ , X′ 

> i

))∣ ∣∣ + 2 ∣∣∣EDn

(

S(gq∗ , X′ 

> i

) − Sβn (gq∗ , X′ 

> i

))∣ ∣∣

≤ 12 E(|Yi|I(Yi > β n)) B. (8) 33 On the other hand, for any g ∈ G (α, θ), we have 

|Sβn (g, X′ 

> i

)| ≤ 5( βn + B)2,σ2 

> S

(g) := Var (Sβn (g, X′ 

> i

)) ≤ E{S2 

> βn

(g, X′ 

> i

)} ≤ 5( βn + B)2E(Sβn (g, X′ 

> i

)) .

Following the Bernstein inequality, for any t > 0, let u = t/ 2 + σ2 

> S

(g)/(10( βn + B)2), we have 

P

{

n−1

> n

∑

> i=1

Lβn (gq, X′ 

> i

) > t 

}

= P

{

ED′ 

> n

(Sβn (gq, X′ 

> i

)) − n−12

> n

∑

> i=1

Sβn (gq, X′ 

> i

) > t 

}

= P

{

ED′ 

> n

{Sβn (gq, X′ 

> i

)} − 1

n

> n

∑

> i=1

Sβn (gq, X′ 

> i

) > t

2 + 12ED′ 

> n

{Sβn (gq, X′ 

> i

)}

}

≤ P

{

ED′ 

> n

{Sβn (gq, X′ 

> i

)} − 1

n

> n

∑

> i=1

Sβn (gq, X′ 

> i

) > t

2 + 12

σ2 

> S

(g)5( βn + B)2

}

≤ exp( − nu 2

2σ2 

> S

(g) + 20 u(βn + B)2/3)

≤ exp( − nu 2

20 u(βn + B)2 + 20 u(βn + B)2/3)

≤ exp( − 120 + 20 /3

nu 

(βn + B)2 )

≤ exp( − 140 + 40 /3

nt 

(βn + B)2 )= exp 

(

− Cnt 

(βn + B)2

)

.

This leads to a tail probability bound of n−1 ∑ni=1 Lβn (gq∗ , X′ 

> i

), that is, 

P

{

n−1

> n

∑

> i=1

Lβn (gq∗ , X′ 

> i

) > t 

}

≤ 2N2n exp 

(

− Cnt 

(βn + B)2

)

.

Then for an > 0,

EDn

[ 1

n

> n

∑

> i=1

Lβn (gq∗ , X′ 

> i

)

]

≤ an +

∫ ∞

> an

P

{ 1

n

> n

∑

> i=1

Lβn (gq∗ , X′ 

> i

) > t 

}

dt 

≤ an +

∫ ∞

> an

2N2n exp 

(

− Cnt 

(βn + B)2

)

dt 

≤ an + 2 N2n exp 

(

−an

Cn 

(βn + B)2

) (βn + B)2

Cn .

Choosing an = log 2 N2n (βn+B)2 

> Cn

, the above inequality leads to 

EDn

[ 1

n

> n

∑

> i=1

Lβn (gq∗ , X′ 

> i

)

]

≤ C(βn + B)2(log 2 N2n + 1) 

n . (9) 34 Combining inequalities (7), (8), and (9), we have 

I11 = EDn

(

n−1

> n

∑

> i=1

L(g ˆα, ˆθ, X′ 

> i

)

)

≤ EDn

(

n−1

> n

∑

> i=1

L(gq∗ , X′ 

> i

)

)

+ 6 ( E|Yi| + B) δ

≤ EDn

(

n−1

> n

∑

> i=1

Lβn (gq∗ , X′ 

> i

)

)

+ 6 ( E|Yi| + B) δ + 12 E(|Yi|I(Yi > β n)) B≤ C(βn + B)2(log 2 N2n + 1) 

n + 6 ( E|Yi| + B) δ + 12 E(|Yi|I(Yi > β n)) B.

Let βn = log n and δ = n−1. Under conditions (C2) and (C3), using the above inequalities, we obtain that 

I11 ⪯ C log 2 n log N2n

n . (10) Then, combining inequalities (3) , (5) , and (10) , using the condition that E

[

d

(

(α, θ), Θ

)] 

≤

cE

(

R(gα,θ) − R (g ˇα, ˇθ)

)

, we can obtain 

E[

d(

( ˆ α, ˆθ), Θ)] 

= E

(∥∥∥( ˆ α, ˆθ) − ( ˇ α, ˇθ)∥∥∥22

)

≤ c E{

R(

g ˆα, ˆθ

)

− R (

g ˇα, ˇθ

)} 

≤ c I 1 + c E

[

λ1

(

∥ ˇα∥1 − ∥ ˆα∥1

)

+ λ2

> D

∑

> l=1

(

∥ ˇWl − I∥1 − ∥ ˆWl − I∥1

)]

≤ c I 11 + c λ 1 E[∥ ˇα − ˆα∥1] + c λ 2

> D

∑

> l=1

E[

∥ ˇWl − ˆWl∥1

]

≤ c C log 2 n log N2n(n−1, ∥ · ∥ ∞, G| x)

n + c λ 1

√d E[∥ ˇα − ˆα∥2]+ c λ 2

> D

∑

> l=1

W E[

∥ ˇWl − ˆWl∥F

]

,

(11) To let the right side of the above inequality to be minimum, we have that 

λ21 ≤ c log 2 n log N2n

nd + c(

R(g ˇα, ˇθ) − R (g∗))

d ,λ22 ≤ c log 2 n log N2n

nW2D + c(

R(g ˇα, ˇθ) − R (g∗))

W2D ,

(12) Note that using Young’s inequality, we have 35 E[

cλ 1

√d ∥ ˇα − ˆα∥2

]

≤ 12 E[

∥ ˇα − ˆα∥22

]

+ c2λ21d

2 .

E[

cλ 2 W ∥ ˇWl − ˆWl∥F

]

≤ 12 E[

∥ ˇWl − ˆWl∥2

> F

]

+ c2λ22W2

2 .

(13) Combining inequalities (11), (12) and (13), we can obtain that 

E[

d(

( ˆ α, ˆθ), Θ)] 

≤ c

{ log 2 n log N2n

n + (

R(g ˇα, ˇθ) − R (g∗))

+ λ21d + λ22W2D

}

.

≤ c

{ log 2 n log N2n

n + (

R(g ˇα, ˇθ) − R (g∗))}

.

Let (α∗ 

> s

, θ∗ 

> s

) ∈ arg min αs,θs R(gαs,θs ) = arg min αs,θs E (Y − gθs (αs ⊙ Xs)) 2. Define 

Θs = {(αs, θs) : R(gαs,θs ) = R(gα∗  

> s,θ∗
> s

)}. Define d(( αs, θs), Θs) = min (αs,θs)∈Θs ∥( ˆαs, ˆθs) −

(αs, θs)∥22, and write ( ˇαs, ˇθs) ∈ arg min (αs,θs)∈Θs ∥( ˆαs, ˆθs) − (αs, θs)∥22, where 

ˇθs = {( ˇWs,l , ˇcs,l ), l = 0 , · · · , L }. Denote ˜θ ∈ arg min θ R(g ˜α,θ), where ˜α = ( ˇαs, 0),Then, it follows from Lemma 7.1 that ( ˇ αs, 0, ˜θ) ∈ Θ.It follows from 

Rn(g ˆα, ˆθ) + λ1∥ ˆα∥1 + λ2

> D

∑

> l=1

∥ ˆWl − I∥1 ≤ R n(g ˇαs,0, ˇθ) + λ1∥ ˇαs∥1 + λ2

> D

∑

> l=1

∥ ˇWl − I∥1,

that 

λ1∥ ˆαc∥1 ≤ R n(g ˇαs,0, ˇθ) − R n(g ˆα, ˆθ) + λ1

(

∥ ˇαs∥1 − ∥ ˆαs∥1

)

+ λ2

> D

∑

> l=1

(∥∥∥ ˇWl − I∥∥∥1 − ∥∥∥ ˆWl − I∥∥∥1

)

.

(14) Insert and subtract population risks gives: 

λ1∥ ˆαc∥1 ≤

[

R(g ˇαs,0, ˇθ) − R (g ˆα, ˆθ)

]

+

[

R(g ˆα, ˆθ) − R n(g ˆα, ˆθ)

]

+ λ1

(

∥ ˇαs∥1 − ∥ ˆαs∥1

)

+ λ2

> D

∑

> l=1

(∥∥∥ ˇWl − I∥∥∥1 − ∥∥∥ ˆWl − I∥∥∥1

)

. (15) 36 Because ( ˇαs, 0, ˇθ) ∈ Θ we have R(g ˇαs,0, ˇθ) = R(g∗G ) ≤ R (g ˆα, ˆθ), hence the first bracket in (15) is nonpositive and can be dropped: 

λ1∥ ˆαc∥1 ≤

[

R(g ˆα, ˆθ) − R n(g ˆα, ˆθ)

]

+ λ1

(

∥ ˇαs∥1 − ∥ ˆαs∥1

)

+ λ2

> D

∑

> l=1

(∥∥∥ ˇWl − I∥∥∥1 − ∥∥∥ ˆWl − I∥∥∥1

)

. (16) For the gate term, ∥ ˇαs∥1 − ∥ ˆαs∥1 ≤ ∥ ˇαs − ˆαs∥1 ≤ √d ∥ ˇαs − ˆαs∥2. For the depth term, by triangle inequality and ℓ1–ℓ2,

> D

∑

> l=1

∣∣∣∣∥∥∥ ˇWl − I∥∥∥1 − ∥∥∥ ˆWl − I∥∥∥1

∣∣∣∣ ≤

> D

∑

> l=1

∥∥∥ ˇWl − ˆWl

∥∥∥1 ≤ √S ∥ ˇθ − ˆθ∥2,

where S = W2D is the total number of scalar parameters in θ. Therefore (16) yields 

λ1∥ ˆαc∥1 ≤

[

R(g ˆα, ˆθ) − R n(g ˆα, ˆθ)

]

+ λ1

√d ∥ ˇαs − ˆαs∥2 + λ2

√S ∥ ˇθ − ˆθ∥2. (17) With Young’s inequality, we have: 

λ1∥ ˆαc∥1 ≤

[

R(g ˆα, ˆθ) − R n(g ˆα, ˆθ)

]

+ 12 ∥ ˇαs − ˆαs∥22 + 12 λ21d + 12 ∥ ˇθ − ˆθ∥22 + 12 λ22S. (18) Taking expectations: 

E[λ1∥ ˆαc∥1] ≤ E[

R(g ˆα, ˆθ) − R n(g ˆα, ˆθ)]

+ 12 E[

∥( ˆ αs, ˆθ) − ( ˇ αs, ˇθ)∥22

]

+ 12 λ21d + 12 λ22S. (19) which leads to 

E(∥ ˆαc∥22) ≤ E(∥ ˆαc∥21) ≤ c log 2 n log N2n

n + c (

R(g ˇαs,0, ˇθ) − R (g∗))

.

## Lemmas 

Lemma 7.1 Under condition (C1), if p = O(log cn) for some positive constant c, for 

(α∗, θ∗) ∈ arg min g∈G (α,θ) E(Yi − g(Xi)) 2, then it follows that (i) |α∗ 

> j

| ≥ τd, for ∀j = 1 , · · · , d and some positive constant c;

37 (ii) there exists a solution (α∗, θ∗), such that α∗ 

> c

= 0 .

Proof of Lemma 7.1 .(i) Suppose that there exists a (α∗, θ∗) such that |α∗ 

> j

| < cτ d for at least one 

j ∈ { 1, · · · , d }. Then, for any random vector X ∈ [0 , 1] p, we construct the vector X[j](0) = (X[1] , · · · , X [j−1] , 0, X [j+1] , · · · , X [p]), clearly, |gθ∗

(

α∗⊙X[j](0) )

−gθ∗

(

α∗⊙X)| ≤ cτ d for some positive constant c. Based on the definition that g∗G = arg min g∈G (α,θ) E(Yi − gθ(α ⊙ Xi)) 2,that 

gθ∗

(

α∗ ⊙ X[j](0) )

= E(Y | X[−j] = x[−j]), and gθ∗ (α∗ ⊙ X) = E(Y | X = x).

It contradicts condition (C2). Thus, for any j = 1 , · · · , d , |α∗ 

> j

| ≥ τd for some positive constant c.(ii) Let (α∗ 

> s

, θ∗ 

> s

) ∈ arg min αs,θs R(gαs,θs ) = arg min αs,θs E (Y − gθs (αs ⊙ Xs)) 2. Define 

Θs = {(αs, θs) : R(gαs,θs ) = R(gα∗  

> s,θ∗
> s

)}. Define d(( αs, θs), Θs) = min (αs,θs)∈Θs ∥( ˆαs, ˆθs) −

(αs, θs)∥22, and write ( ˇαs, ˇθs) ∈ arg min (αs,θs)∈Θs ∥( ˆαs, ˆθs) − (αs, θs)∥22, where 

ˇθs = {( ˇAs,l , ˇcs,l ), l = 0, · · · , L }. Denote ˜θ ∈ arg min θ R(g ˜α,θ), where ˜α = ( ˇαs, 0).Let ˇθ = {( ˜Al, ˜cl), l = 0 , · · · , L } with ˜cl = ˇcs,l for l = 0 , · · · , L , ˜Al = ˇAs,l for l = 1 , · · · , L ,and ˜A0 = ( ˇAs, 0, A c, 0). Then, based on the definition (2), it is easy to show that 

R(g ˜α, ˜θ) ≤ R (g ˜α, ˇθ) = R(g ˇαs, ˇθs ) = R(gα∗,θ∗ ).

Clearly, ( ˜ α, ˜θ) is the solution. 

Lemma 7.2 (Approximation error, (Theorem 3.3 in Jiao et al. 2023)) Given H ¨older smooth functions g∗ ∈ H β ([0 , 1] d, B 0), for any D ∈ N+ and W ∈ N+,there exists a function g∗G implemented by a ReLU feedforward neural network with width 

W = 38( ⌊β⌋ + 1) 2d⌊β⌋+1 W ⌈log 2(8 W )⌉ and depth D = 21( ⌊β⌋ + 1) 2D ⌈log 2(8 D)⌉ such that 

∣∣∣g∗ − g∗G

∣∣∣ ≤ 18 B0(⌊β⌋ + 1) 2d⌊β⌋+max {β, 1}/2(W D )−2β/d ,

38 for all x ∈ [0 , 1] d \ Ω([0 , 1] d , K, δ ) where 

Ω([0 , 1] d , K, δ ) = 

> d⋃
> i=1

{x = [ x1, · · · , x d]T : xi ∈

> K−1⋃
> k=1

(k/K − δ, k/K )},

with K = ⌊W D ⌋ and δ is an arbitrary number in (0 , 1/(3 K)] .

Lemma 7.3 (Bounding the covering number, (Theorem 12.2 in Anthony et al. 1999) and (Theorems 3 and 7 in Bartlett et al. 2019)) Let ReLU feedforward neural network G be a set of real functions from a domain X to the bounded interval [0 , B]. There exists a universal constant C such that the following holds. Given any D, S with S > C D > C 2, there exists network class G with ≤ D layers and ≤ S 

parameters with V C -dimension ≥ SD log( S/D)/C and given δ > 0sup 

> x

log N2n(δ, ∥ · ∥ ∞, G| x) = O(

SD log( S/δ ))

.

39