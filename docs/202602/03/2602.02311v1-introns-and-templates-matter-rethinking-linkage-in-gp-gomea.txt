Title: Introns and Templates Matter: Rethinking Linkage in GP-GOMEA

URL Source: https://arxiv.org/pdf/2602.02311v1

Published Time: Tue, 03 Feb 2026 04:04:36 GMT

Number of Pages: 16

Markdown Content:
# Introns and Templates Matter: Rethinking Linkage in GP -GOMEA 

## Johannes Koch 

Centrum Wiskunde & Informatica Amsterdam, The Netherlands Delft University of Technology Delft, The Netherlands johannes.koch@cwi.nl 

## Tanja Alderliesten 

Leiden University Medical Center Leiden, The Netherlands T.Alderliesten@lumc.nl 

## Peter A.N. Bosman 

Centrum Wiskunde & Informatica Amsterdam, The Netherlands Delft University of Technology Delft, The Netherlands peter.bosman@cwi.nl 

Abstract 

GP-GOMEA is among the state-of-the-art for symbolic regression, especially when it comes to finding small and potentially inter-pretable solutions. A key mechanism employed in any GOMEA variant is the exploitation of linkage, the dependencies between variables, to ensure efficient evolution. In GP-GOMEA, mutual in-formation between node positions in GP trees has so far been used to learn linkage. For this, a fixed expression template is used. This however leads to introns for expressions smaller than the full tem-plate. As introns have no impact on fitness, their occurrences are not directly linked to selection. Consequently, introns can adversely affect the extent to which mutual information captures dependen-cies between tree nodes. To overcome this, we propose two new measures for linkage learning, one that explicitly considers introns in mutual information estimates, and one that revisits linkage learn-ing in GP-GOMEA from a grey-box perspective, yielding a measure that needs not to be learned from the population but is derived directly from the template. Across five standard symbolic regres-sion problems, GP-GOMEA achieves substantial improvements using both measures. We also find that the newly learned linkage structure closely reflects the template linkage structure, and that explicitly using the template structure yields the best performance overall. 

Keywords 

Genetic programming, symbolic regression, linkage learning, GP-GOMEA 

1 Introduction 

Especially in domains where decisions can have severe conse-quences on lives and livelihoods, such as healthcare, accountability and responsibility are paramount. In recognition of this, the fields of eXplainable AI (XAI) and interpretable machine learning aim to either explain model outputs or learn models that are inherently transparent [ 39 , 54 ]. Symbolic regression (SR), the task of finding mathematical expressions that accurately model relationships in data, is one form of such inherently interpretable models [ 28 ]. Small mathematical expressions describing one target variable consisting of atomic functions (e.g., {+ , âˆ’, Ã—, Ã·, sin }), numerical parameters, and input features potentially are not only readable for human experts, but also have the potential to be interpretable and can help uncover yet unknown knowledge. 

> This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.

GP-GOMEA, a Genetic Programming (GP) version of the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA), is one of the leading algorithms when it comes to finding small, yet accurate symbolic expressions [ 3, 29 ]. GP algorithms typically start with an initially random population of solutions that is refined through repeated selection and variation. One key aspect to GP-GOMEA is the explicit modeling and exploitation of key dependencies between decision variables, also called linkage [ 52 ]. Linkage is well-known to be a crucial factor in how effectively an algorithm can tackle problems in general, where problems can be exponentially harder to solve without the correct linkage structure [ 45 ]. If no linkage information is known a priori, it can be detected and learned during optimization. Oftentimes, this happens based on statistics taken from the current (and potentially previous) population(s). Specifi-cally, a measure is needed that expresses how strongly variables are related. For this, it was previously proposed to use mutual in-formation (MI) between decision variables in GP-GOMEA, with a modification to avoid the initial detection of false linkage due to non-uniform initialization methods [52]. Similarly to other approaches that explicitly maintain distribu-tions over the expressions in the search space [ 22 , 34 , 41 ], a fixed template is used to ensure all solutions have the same number of variables and that variable indices correspond to identical expres-sion positions across solutions. Oftentimes, without further knowl-edge of the type of expression being searched for, the template corresponds to a full binary tree with a fixed depth. For expressions that do not use the entire template, some decision variables are conditionally inactive, often called introns [30]. However, from the perspective of linkage learning, introns can be problematic, as the absense of selection pressure on introns causes them to act as a source of noise to the statistics used to learn linkage. Inactive variables even have been recognized as a source of difficulty beyond GP [19, 20, 35, 36]. The goal of this paper is to study whether explicitly taking the presence of introns into account during linkage learning can im-prove the detection of important dependencies and consequently lead to improved performance by GP-GOMEA. Moreover, we wish to study closer what dependencies are discovered once the noise caused by the presence of introns is removed. The remainder of this paper is organized as follows: Related work is covered in Section 2. GP-GOMEA and new ways of detecting linkage are discussed in Section 3. Experiments and results thereof are covered in Section 4. Finally, we discuss our results in Section 5 and conclude in Section 6. 

> 1
> arXiv:2602.02311v1 [cs.NE] 2 Feb 2026 Koch et al.

2 Related Work 

The importance of linkage in evolutionary optimization is well-known [ 13 ] and it has been shown that the computational effort needed to solve optimization problems can exponentially increase without the right linkage model [ 45 ]. There are various ways to handle linkage to more effectively propagate building blocks with less disruption: The solution representation can be optimized to-wards highly linked low-order schemata [ 24 ], variables can be re-ordered during evolution to improve linkage [ 17 , 30 ], domain specific crossovers can be used [ 38 ], distribution-based algorithms can encode linkage in probabilistic models [ 8, 33 ], or linkage can be modeled and learned explicitly. GOMEAs fall into the last category of model-based evolutionary algorithms, where linkage information is either known or learned [4, 15, 35]. In the field of GP, too, decision variables are typically arranged such that related variables are grouped and varied together, for example by representing solutions as trees in standard GP. All the above-mentioned approaches arguably have been explored to various extents, albeit not necessarily with an explicit intent to-wards linkage in GP: Tree-based GP keeps related variables close together [ 28 ], linear and cartesian GP allow re-ordering subrou-tines by decoupling semantic from spatial adjacency [ 57 ], various context aware crossover operators [ 31 ] and distribution based GP algorithms [ 34 , 41 , 44 ] have been proposed. How linkage between variables is modeled and exploited is generally highly specific to the solution representation and algorithm used. When it comes to explicitly modeling and learning linkage as done in GOMEA, various topics have been explored. From dif-ferent ways of modeling and learning linkage, to similarity mea-sures, to filtering extraneous linkage, to conditional variation op-erators that consider the context of already varied linked vari-ables [ 4, 6, 7 , 9 , 15 , 35 , 37 ]. In particular, fitness-based linkage learning, i.e., performing targeted fitness evaluations to uncover definitive linkage instead of using potentially untruthful statistical measures, is gaining ground [ 4 , 35 , 37 ]. Due to the use of a fixed template in GP-GOMEA and the knowledge that it is possible for any two variables to interact, this form of linkage learning does not apply to symbolic regression (SR). The linkage measure used typically heavily depends on the domain [5, 10, 40]. To the best of our knowledge, additional information available in the GP setting such as the user-defined template structure or knowledge about inactive variables has not yet been considered when it comes to linkage. Nonetheless, an adjustment to the esti-mated linkage to prevent the detection of spurious linkage after initialization and a binning approach to handle numerical constants were previously proposed [ 52 ]. This adjusted linkage measure is re-visited and compared to in this work. 

3 Method 

In this section, GP-GOMEA is introduced, before the proposed changes to how linkage is learned, are presented. 

3.1 GP-GOMEA 

As in other population-based algorithms, GP-GOMEA works by iteratively improving an initial, often random, set of candidate so-lutions through variation and selection. Compared to most other algorithms, a linkage learning step is performed every generation in GP-GOMEA, followed by variation using the the gene-pool op-timal mixing (GOM) operator. GOM combines both variation and selection on a per-solution basis as shown in Algorithm 1. Each solution consists of a fixed-length string of decision vari-ables, where a common tree-based template structure is used to map decision variables to tree nodes, as shown in Figure 1. Typi-cally, this template is a single ð‘› -ary tree, where ð‘› is the maximum arity in the function set used [ 52 ]. However, the use of multiple output trees or modular subfunctions is also possible [ 21 , 43 ]. By using a fixed structural template compared to a representation with a dynamic length, the same variable position corresponds to the same tree node across all individuals. Hence, the linkage between variables corresponds to the linkage of the respective tree nodes. However, this inherently introduces conditionally inactive variables or introns , as for smaller expressions not all decision variables are used in the corresponding semantic expression. Template Structure                      

> +
> sin âˆš
> x0x2x1x3
> 0
> 12
> 3456
> 0123456
> Internal Representation
> x2x3+sin âˆšx0x1
> sin( x0) + âˆšx1
> Expression Semantics

Figure 1: A fixed-length string of decision variables is mapped to a fixed tree template, which defines the corresponding semantic expression. Shaded variables are introns, and do not affect the semantic meaning of the expression. 

In each generation, first a linkage model is learned, aimed at capturing the dependencies between variables. To represent this linkage information, a set containing multiple subsets of all de-cision variables, called a Family of Subsets (FOS, denoted F ), is commonly used. Each subset of variables in the FOS corresponds to a set of linked variables that are jointly optimized by using the subset as a crossover mask during variation. In GP-GOMEA, this FOS is learned during optimization based on a pairwise similarity measure between decision variables, such as MI. After estimating the similarity measure, hierarchical clustering with UPGMA [ 18 ] is performed to repeatedly merge subsets of variables starting from all single variable subsets. The last merge, i.e., the subset containing all variables, is discarded as the corresponding crossover mask would perform replacement instead of variation. This hierarchical FOS structure is commonly referred to as a Linkage Tree (LT) [15, 46]. Variation is performed separately for each solution, each of which is first cloned into an offspring solution. Next, for each offspring solution, each subset in the FOS is considered, and the offspring solution inherits all variables in the subset from a donor solution chosen uniformly randomly from the population. Specific to the GP domain, an evaluation is only necessary if this leads to a modification of any actively used part of the expression. If this is the case, the fitness of the resulting solution is compared with a backup and regressions in fitness are reverted. To avoid determining an appropriate population size for a prob-lem, (GP-)GOMEA is often paired with the interleaved multistart scheme (IMS), in which populations of exponentially increasing 

> 2Introns and Templates Matter: Rethinking Linkage in GP-GOMEA

Algorithm 1: GP-GOMEA                                                            

> 1P â†âˆ’ InitializeAndEvaluatePopulation()
> 2while Â¬TerminationCriteriaSatisfied do
> // 1. Linkage learning
> 3S â†âˆ’ EstimateSimilarity( P)
> 4F â†âˆ’ BuildLinkageTree( S)
> // 2. GOM
> 5O â†âˆ’ Clone ( P )
> 6for ð‘– âˆˆ { 1, . . . , | O | } do
> 7for ð‘  âˆˆShuffle( F)do
> 8ð‘‘ð‘œð‘›ð‘œð‘Ÿ â†âˆ’ Random( {1, . . . , | P | } \ { ð‘– })
> 9ð‘ð‘Žð‘ð‘˜ð‘¢ð‘ â†âˆ’ Clone ( O ð‘– )
> 10 Oð‘– [ð‘  ] â†âˆ’ P ð‘‘ð‘œð‘›ð‘œð‘Ÿ [ð‘  ]
> 11 if Oð‘– [ð‘Žð‘ð‘¡ð‘–ð‘£ð‘’ ]â‰ ð‘ð‘Žð‘ð‘˜ð‘¢ð‘ [ð‘Žð‘ð‘¡ð‘–ð‘£ð‘’ ]then
> 12 Evaluate( Oð‘– )
> 13 if ð‘“ ( O ð‘– )worse than ð‘“ (ð‘ð‘Žð‘ð‘˜ð‘¢ð‘ )then
> 14 Oð‘– â†âˆ’ ð‘ð‘Žð‘ð‘˜ð‘¢ð‘
> 15 P â†âˆ’ O

sizes perform generations in a scheduled manner, with larger pop-ulations performing generations less frequently than smaller popu-lations, and under-performing populations are terminated when converged or once larger populations perform better [52]. 

3.2 Linkage Measures in GP-GOMEA 

The FOS that dictates how variation in GP-GOMEA adheres to the learned variable dependencies, is constructed using a pairwise similarity measure. This measure is computed by interpreting the population as a dataset without regard for the mapping to the tree template. In this work, two new approaches are proposed to compute more fitting similarity measures by exploiting so far unused domain knowledge: the knowledge about inactive variables and the fixed template structure. 

3.2.1 Existing measures. For GP, the previously explored measures are MI, an adjusted version thereof, and random noise [ 51 , 52 ]. For the random measure, pairwise similarity is sampled from U ( 0, 1).For two random variables X and Y, the MI is defined as: 

ð‘€ð¼ (ð‘‹, ð‘Œ ) = ð» (ð‘‹ ) + ð» (ð‘Œ ) âˆ’ ð» (ð‘‹, ð‘Œ ) (1) where ð» (ð‘‹ ) = Ãð‘¥ âˆ’ð‘ƒ (ð‘‹ = ð‘¥ ) log 2 (ð‘ƒ (ð‘‹ = ð‘¥ )) defines entropy. In GP, expressions are typically initialized with domain-specific methods such as Half-and-Half [ 52 ] instead of uniformly randomly. While this often achieves more desirable diversity in some aspects, such as the initial expression depths, biases can appear in the ini-tial MI estimates [ 52 ]. To avoid this, normalization factors were proposed to adjust the MI such that no linkage is detected directly after initialization, leading to increased performance [52]. 

3.2.2 Exploiting knowledge about introns. The use of MI in GOMEA can often lead to successfully detecting variable dependencies. How-ever, inactive variables in the form of introns are inherent to GP-GOMEA. The impact that this may have on dependency detection, has so far not been considered. For each solution, it is known which variables are currently active or inactive. The values of inactive vari-ables do not influence the semantics and thus are freely interchange-able and not subject to selection pressure. From the perspective of 0 1 2                                

> sin x0x1
> x0x1x0
> sin x1x0
> +x0x1
> sin x0
> x0
> sin x1
> +x0x1
> Solutions
> 012
> 012
> 1.5 22
> 0.5 11
> 0.5 11
> MI
> 012
> 012
> 1.5 21.5
> 11.5 2
> 0.81 0.31 0.81
> MI masked
> 0
> 12
> Template
> {0} {1} {2}
> {1, 2}
> {0, 1, 2} â†“
> {0} {1} {2}
> {0, 1}
> {0, 1, 2} â†“

Figure 2: Top left: population of 4 solutions. Bottom left: tree template. Matrices in blue show entropy (above the black line) and MI (below the black line). The corresponding linkage tree for both the normal and proposed masked MI are shown as well. The masked version treats all introns (shaded) as the same separate "masked" symbol. 

entropy estimation, the values of inactive variables can therefore be considered to be noise, hiding the true signal of active variables. To avoid this undesirable effect, we propose to mask the inactive variables using a special intron label during the entropy calculation, i.e., the alphabet of any variable now includes the "masked" label. An example is shown in Figure 2, where masking introns leads to a different linkage tree compared to using normal MI. Masking makes the estimated MI more accurate, by ensuring that the estimate is fully based on active variables under selection pressure. Note that this approach is not compatible with the adjustments for biased initialization methods proposed in [ 52 ]. This is because initially, variables that are inactive in the entire population would lead to divisions by zero. 

3.2.3 Exploiting the known template. In discrete and real-valued evolutionary optimization literature, it is common to model depen-dencies with a variable interaction graph (VIG) [ 4 , 50 , 56 ]. In such a graph, each variable is a vertex, and an edge exists between two vertices if there exists a dependency between the two variables. Commonly, a VIG is associated with grey-box optimization sce-narios in which it is assumed that the problem being optimized is sufficiently known. By studying the problem formulation, depen-dencies between variables may then be directly gauged by iden-tifying which variables are jointly part of a subfunction. As an extension, or more informed, variant of the VIG, the edges in the graph can be weighted. The resulting wVIG then expresses not only whether or not two variables are jointly dependent, but also how strong that dependence is, relative to other dependencies. In GOMEA, such a wVIG can be used directly to build a linkage tree (or other types of linkage structures such as conditional linkage sets [ 4] by creating a similarity matrix ð‘º that contains the weight of an edge between vertices ð‘– and ð‘— at ð‘º ð‘– ð‘— and ð‘º ð‘—ð‘– .The function represented by a GP tree template in GP-GOMEA, is a nested function. It can be defined using two functions ð‘“ and ð‘” 

in which the first argument of ð‘“ determines the operation repre-sented in the corresponding node of the GP tree, which may include returning the value of a constant or input variable and ignoring the other arguments. Function ð‘” is used only in leaf nodes of the GP 

> 3Koch et al.

tree template, returning the value of a constant or an input variable. Given a data vector ð’… , the function ðºð‘ƒ (ð’™ ) that a tree template rep-resents, can be written as a nested application of function ð‘“ with function ð‘” at the deepest level. I.e., for depth 2, we have: 

ðºð‘ƒ (ð’™ ,ð’… ) = ð‘“ (ð‘¥ 0,ð‘“ (ð‘¥ 1,ð‘” (ð‘¥ 3,ð’… ),ð‘” (ð‘¥ 4,ð’… ),ð’… ),ð‘“ (ð‘¥ 2,ð‘” (ð‘¥ 5,ð’… ),ð‘” (ð‘¥ 6,ð’… ),ð’… )) (2) This function is the core of what imposes dependencies between the variables that we optimize over in GP-GOMEA. Consider the SSE optimization function for symbolic regression over all data vectors ð’… ð‘– and targets ð‘¡ ð‘– :

min 

> ð’™

(âˆ‘ï¸ 

> ð‘–

 ðºð‘ƒ (ð’™ , ð’… ð‘– ) âˆ’ ð‘¡ ð‘– 2

)

(3) Essentially this is a large sum of independent terms. Focusing on a single term, we find ðºð‘ƒ (ð’™ , ð’… ð‘– )2 âˆ’ 2ðºð‘ƒ (ð’™ , ð’… ð‘– )ð‘¡ ð‘– + ( ð‘¡ ð‘– )2. Since the variables in ðºð‘ƒ are already fully coupled through nonlinear in-teractions due to the nested function composition, squaring the function does not alter the dependency structure, but amplifies existing interactions. Hence, we can construct the wVIG based on the definition of ðºð‘ƒ (ð’™ , ð’… ).A straightforward and common way of doing so is counting how many times a variable is contained in the same subfunction as another variable. For the depth 2 case, we get the similarity matrix defined in the middle of Figure 3. However, for multiple reasons, the dependence between the arguments of a parent node in the template tree is likely almost always smaller than the dependence between an argument and its parent node. One of these reasons is that some functions may impose no dependencies between its arguments. Another reason is that generally, the higher a node is in the template tree (and therefore the less deeply nested in the functional description), the larger its impact on changing the output of the tree and thus on the regression error. Moreover, internal nodes may be terminals. Thus, the second and third arguments of ð‘“ may not always be used. In that case, it does not matter what values the variables take in the deeper nested functions (as they are then introns), and thus, in those cases, those variables are independent. We can convey this information in a systematic manner by defining a distance function between any two nodes in the GP template that is the smallest distance between these nodes to travel along the template. This makes the distance between two arguments of the same parent node 1 larger than the distance between each of the arguments and the parent node (i.e., 2 and 1, respectively). This distance measure can be turned into a normalized similarity measure as follows: 

ð‘º ð‘–,ð‘— =

(1 âˆ’ ð‘‘ (ð‘–,ð‘— )     

> 1+max ð‘˜,ð‘™ âˆˆI {ð‘‘ (ð‘˜,ð‘™ ) }

if ð‘– and ð‘— are connected 

0 otherwise (4) where the distance ð‘‘ (ð‘–, ð‘— ) is the total number of edges between nodes ð‘– and ð‘— and their closest ancestor in the template, and I is the set of all node indices. As shown on the right of Figure 3, while nodes are still equally similar by definition using this measure of structural relatedness, there is more granularity than in the direct subfunction encapsula-tion approach. During the hierarchical clustering, ties are broken randomly if there are multiple merges with the same simiarlity. By Template                                                                                       

> 0
> 12
> 3456
> 0123456
> #Common Subfunctions
> 0123456
> 111111
> 112211
> 111122
> 121211
> 121211
> 112112
> 112112
> 0123456
> Node Proximity
> 0123456
> 0.8 0.8 0.6 0.6 0.6 0.6
> 0.8 0.6 0.8 0.8 0.4 0.4
> 0.8 0.6 0.4 0.4 0.8 0.8
> 0.6 0.8 0.4 0.6 0.2 0.2
> 0.6 0.8 0.4 0.6 0.2 0.2
> 0.6 0.4 0.8 0.2 0.2 0.6
> 0.6 0.4 0.8 0.2 0.2 0.6

Figure 3: An example template with highlighted subfunc-tions and the corresponding node proximity and subfunction based similarity measures. 

rebuilding the linkage tree every generation, different LT FOS struc-tures representing the similarity matrix are created. Note that com-pared to MI estimates, this form of pre-defined similarity disregards semantic relations due to variable values entirely. However, there is no need for population sizes sufficiently large to get statistically reliable estimates and literature has shown that appropriate offline similarity measures can lead to much better results [11, 12, 49]. 

4 Experiments and Results 

In this section, two experiments are performed to evaluate the proposed measures. First, the proposed measures are compared with previously used measures. The second experiment aims to uncover what linkage structures are learned. 

4.1 Experimental Setup 

To isolate the effect of linkage learning from other algorithm set-tings such as the fixed structural template and whether linear scal-ing [ 26 ] (hereafter LS) is enabled, all combinations are tested. LS is a much - used post -processing technique that provides scale and translation invariance by performing a linear regression on the output of the GP expression. The other settings are detailed in Ta-ble 2. The operator set was chosen based on [ 32 ]. The coefficient of determination ( ð‘… 2 Score) is used as objective: 

ð‘… 2 (ð‘¦ ð‘ð‘Ÿð‘’ð‘‘ , ð‘¦ ð‘¡ð‘Žð‘Ÿð‘”ð‘’ð‘¡ ) = 1 âˆ’ ð‘€ð‘†ð¸ (ð‘¦ ð‘ð‘Ÿð‘’ð‘‘ , ð‘¦ ð‘¡ð‘Žð‘Ÿð‘”ð‘’ð‘¡ )

ð‘£ð‘Žð‘Ÿ (ð‘¦ ð‘¡ð‘Žð‘Ÿð‘”ð‘’ð‘¡ ) (5) 

ð‘€ð‘†ð¸ (ð‘¦ ð‘ð‘Ÿð‘’ð‘‘ , ð‘¦ ð‘¡ð‘Žð‘Ÿð‘”ð‘’ð‘¡ ) = 1

ð‘ 

> ð‘

âˆ‘ï¸ 

> ð‘– =1

(ð‘¦ ð‘ð‘Ÿð‘’ð‘‘ ð‘– âˆ’ ð‘¦ ð‘¡ð‘Žð‘Ÿð‘”ð‘’ð‘¡ ð‘– )2 (6) where ð‘¦ ð‘ð‘Ÿð‘’ð‘‘ and ð‘¦ ð‘¡ð‘Žð‘Ÿð‘”ð‘’ð‘¡ are the predicted output (as in Equation (2)) and target variable respectively, and ð‘ is the dataset size. Note that with LS enabled, the predicted values are scaled and transformed before computing the prediction error. The problems we used in our experiments (listed in Table 1) were selected to contain varying numbers of input features. For each problem, 25% of the instances form a held - out test set. The remain-ing data is split into 5 cross - validation folds, using 80% for training and 20% for validation. Each fold is repeated 6 times with differ-ent seeds, yielding 30 independent runs. Seeds are shared across algorithms so that all algorithms have identical initial populations. Numerical constants in the form of ephemeral random constants (ERCs) are used for which random values are sampled during initial-ization that remain constant throughout evolution. During linkage learning, ERCs are handled as in [ 42 ], i.e., for the MI-based mea-sures, constant values are split into 25 discrete bins (note: measures not considering population statistics are unaffected). 

> 4Introns and Templates Matter: Rethinking Linkage in GP-GOMEA

Table 1: The problems used in the experiments. 

Dataset #Instances #Features Airfoil [48] 1503 5Bike Sharing (Daily) [16] 731 12 Concrete Compressive Strength [25] 1030 8Dow Chemical [1] 1066 57 Tower [55] 4999 25 

In the first experiment, a computational budget of 10 7 evaluations was used. As in [ 42 ], the IMS starting population size is set to 64 with a subgeneration factor of 10 generations before larger populations are started. All experiments follow the setup described in Table 2 and were run on a separate physical core of a machine with two Intel Xeon E5-2699v4 processors, ensuring that runtimes are comparable. In the second experiment, instead of IMS, a single population with a fixed size of 1024 and a generational budget of 20, a template height of 5 (31 Nodes), and LS enabled, is used. This population size aligns with earlier work on GP-GOMEA [27, 52, 53]. We consider the following linkage measures: random linkage, MI, bias adjusted MI as in [ 52 ], the proposed approach of masking inactive variables when computing the MI, and the similarity based on node proximity in the structural template with and without repeated building of the linkage tree every generation, hereafter abbreviated as Random, MI, ð‘€ð¼ ð‘Žð‘‘ ð‘—ð‘¢ð‘ ð‘¡ð‘’ð‘‘ , ð‘€ð¼ ð‘šð‘Žð‘ ð‘˜ð‘’ð‘‘ , Node, and Node (static) respectively. Both building of the linkage tree only once and rebuilding it every generation are considered for the node proximity measure to assess the effect of the aforementioned random tie-breaking during hierarchical clustering. Results for another baseline assuming full variable independence and the subfunction based measure are presented and discussed in the supplementary material, where the results confirm that variables are not independent and the Node measure outperforms the subfunction based measure. To analyze the results, we follow the recommendations and im-plementation provided by [ 2], where interval estimates via stratified bootstrap confidence intervals are preferred over point estimates and statistical significance tests to take uncertainty and the effect size into account. We mainly consider training performance, as that is directly optimized and we are primarily interested in the effectiveness of the linkage models instead of the generalization per-formance. Statistical analysis is also done as recommended in [ 14 ]with the implementation provided by [23]. 

Table 2: Experiment Setup Setting Experiment I Experiment II 

Objective ð‘… 2 Score Initialization Half-and-Half [52] ERC Initialization U ( min (ð‘¦ ð‘¡ð‘Ÿð‘Žð‘–ð‘› ), max (ð‘¦ ð‘¡ð‘Ÿð‘Žð‘–ð‘› )) 

Operator set {+ , âˆ’, Ã—, Ã·, sin }

Termination 10 7 evaluations 20 generations Linear scaling Yes / No Yes Template Height 5 / 7 (31 / 127 Nodes) 5 (31 Nodes) 

4.2 Experiment 1: Effectiveness of Various Linkage Measures 

To assess the effectiveness of the various linkage measures, we consider both the final accuracies achieved (Figure 5) and inter-mediate results throughout optimization in Figures 4 and 6. The supplementary material provides per problem results (Section C), and a repeat of the first experiment showing that the conclusions made extend to varying operator set sizes (Section E). The expected final ð‘… 2 scores on both the training and test splits are shown in Figure 5 with two different aggregate metrics, split up into the various combinations of template height and linear scaling that we considered. Two different aggregates, based on the mean performance without outliers, and the median per problem performance are provided, as both measures are differently affected by asymmetric accuracy distributions. Across all settings, the Node measure performs best, followed by ð‘€ð¼ ð‘šð‘Žð‘ ð‘˜ð‘’ð‘‘ , indicating that the proposed methods provide more effective linkage information. Interestingly, the static version of the Node measure, where the initial linkage tree is re-used within the same population for all subsequent generations, clearly performs worse than the randomized version. This aligns with [ 47 ], where it was found that there is not necessarily one best static linkage structure and randomization can be beneficial. This could be due to the static version introducing bias by breaking ties only one way, while the repeated building of linkage trees with ties being broken randomly, leads to an unbiased FOS representation of the estimated linkage structure over multiple generations. Another effect is that the randomization possibly promotes propagation of building blocks between overlapping subsets of subsequent FOS structures better than the static version. In general, increasing the template height and thus the possible expression sizes, leads to better accuracies, and the same holds for enabling LS. While the template size and LS have a larger impact on accuracy, clearly, linkage learning too has a noticeable impact. 5e5 1e6 2e6 5e6 1e7          

> Evaluations
> 0.725
> 0.750
> 0.775
> 0.800
> 0.825
> IQM  R2 Train
> 1min 5min 10min 20min 30min
> Approximate Runtime
> 0.750
> 0.775
> 0.800
> 0.825
> IQM  R2 Train
> Random MI MI adjusted MI masked Node Node (static)

Figure 4: The interquartile mean (IQM) training ð‘… 2 and boot-strapped 95% confidence intervals (as per [ 2 ]) over evalua-tions and approximate runtime, across all runs performed. For the runtime, the last recorded value before each point in time was used, and dashed horizontal lines show the final values reached for the existing measures. The vertical line in-dicates the computational budget (not to scale w.r.t runtime) where the final values reached are marked with crosses. 

> 5Koch et al. H = 5
> H = 5
> LS
> H = 7
> H = 7
> LS
> R2 Train

Interquartile Mean Mean of Medians                

> 0.65 0.70 0.75 0.80 0.85 0.90
> H = 5
> H = 5
> LS
> H = 7
> H = 7
> LS
> R2 Test
> 0.65 0.70 0.75 0.80 0.85 0.90
> Random MI MI adjusted MI masked Node Node (static)

Figure 5: Aggregate ð‘… 2 scores (higher is better) on the problems for each combination of template height and LS considered. The interquartile mean corresponds to the mean after discarding the bottom and top 25% of runs for each problem, and the mean of medians corresponds to the mean of the median performances on each problem. The colored bar corresponds to the 95% confidence interval estimated using a percentile bootstrap with stratified sampling as per [ 2 ] with the expected value in black. 0.0 0.5 1.0          

> 1e7
> 0.6
> 0.7
> R2 Train
> H=7
> Airfoil
> 0.0 0.5 1.0
> 1e7
> 0.80
> 0.85
> 0.90 Bike Sharing
> 0.0 0.5 1.0
> 1e7
> 0.7
> 0.8
> Concrete Compressive Strength
> 0.0 0.5 1.0
> 1e7
> 0.4
> 0.6
> 0.8
> Dow Chemical
> 0.0 0.5 1.0
> 1e7
> 0.7
> 0.8
> Tower

Evaluations             

> 02000 4000
> 0.60
> 0.65
> 0.70
> 0.75
> R2 Train
> H=7
> Airfoil
> 0500 1000 1500
> 0.800
> 0.825
> 0.850
> 0.875
> 0.900 Bike Sharing
> 01000 2000
> 0.70
> 0.75
> 0.80
> 0.85
> Concrete Compressive Strength
> 01000 2000
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> Dow Chemical
> 05000 10000
> 0.70
> 0.75
> 0.80
> 0.85
> Tower

Approximate Runtime [s]      

> Random MI MI adjusted MI masked Node Node (static)

Figure 6: Median training ð‘… 2 score (higher is better) over evaluations and approximate runtime across problems with template height 7 and without linear scaling. The filled area corresponds to the interquartile range, and the runtime where the first run finished is marked with a cross, after which the final value obtained is re-used for completed runs until all runs have finished. 

However, considering only the already existing methods, the ran-dom baseline is not clearly outperformed for all settings, suggesting that previously proposed linkage measures are suboptimal for GP. When it comes to the relative performance of the linkage mea-sures, increasing the number of decision variables (template height) widens the gap between the various methods. LS, as arguably could be expected, has the opposite effect and decreases the differences. Moreover, of course, there is a generalization gap when the training and test performance are compared, however, the relative perfor-mance of the various linkage measures mostly remains unaffected. Overall, MI tends to outperform the bias-adjusted version, while the setting using a template height of 5 and LS as used in [ 52 ] where statistical tests showed the opposite. Compared to the experiments performed in [ 52 ], the budget used here is an order of magnitude                                       

> 6Introns and Templates Matter: Rethinking Linkage in GP-GOMEA 0510 15 20 25 30
> Generation 0
> MI Generation 5
> MI Generation 10
> MI Generation 20
> MI Node / Node (static)
> 0510 15 20 25 30
> MI adjusted MI adjusted MI adjusted MI adjusted Node / Node (static)
> 0510 15 20 25 30
> 0510 15 20 25 30
> MI masked
> 0510 15 20 25 30
> MI masked
> 0510 15 20 25 30
> MI masked
> 0510 15 20 25 30
> MI masked
> 0510 15 20 25 30
> Node / Node (static)
> 0.2
> 0.4
> 0.2
> 0.4
> 0.6
> 0.8
> âˆ’0.05
> 0.00
> 0.05
> 0.10
> 0.2
> 0.4
> 0.6
> 0.8
> 0.25
> 0.50
> 0.75
> 1.00
> 0.2
> 0.4
> 0.6
> 0.8

Figure 7: The different linkage measures over generations, averaged over 30 runs on the Bike Sharing dataset with a template height of 5 (31 Nodes), LS and a fixed population size of 1024. Note that different scales are used for each measure, as the focus lies on differences between variable pairs within measures instead of differences between linkage measures. The Node measure is shown only once per row as it does not change throughout evolution to ease comparisons. 

larger, and thus any beneficial effects of mitigating initial biases are likely less visible in the final results achieved after exhausting the computational budget. Figure 4, where the convergence aggre-gated over all combinations and problems considered is shown, also shows an initial advantage of ð‘€ð¼ ð‘Žð‘‘ ð‘—ð‘¢ð‘ ð‘¡ð‘’ð‘‘ over MI, aligning with the results obtained in [ 52 ]. This suggests that the bias adjustment it-self does work, however, as optimization progresses, the initial bias becomes less and continued mitigation can be harmful as the MI measure outperforms its adjusted counterpart in various settings. Intermediate results and the runtime performance across all runs are shown in Figure 4, and Figure 6 provides convergence results per problem for template height 7 without LS. Results for the other combinations are in Section C of the supplementary material. In both figures, the Node and ð‘€ð¼ ð‘šð‘Žð‘ ð‘˜ð‘’ð‘‘ measures outperform the other measures throughout evolution with respect to both evaluations performed and runtime. In terms of evaluations, use of the proposed measures outperforms use of the existing measures at only half the computational budget, and the same holds for comparing the IQM achieved using the Node measure after 5ð‘’ 5 evaluations, and the existing methods after 1ð‘’ 6 evaluations. Considering runtime, the varying number of instances between the problems (see Table 1) leads to varying runtimes until the full budget is exhausted, ranging from several minutes to hours. Nonetheless, the Node and ð‘€ð¼ ð‘šð‘Žð‘ ð‘˜ð‘’ð‘‘ 

measures exceed the final accuracy reached by the existing meth-ods after 10 and 20 minutes, respectively, showing a substantial speed-up in terms of runtime and highlighting the importance of exploiting linkage. The observed efficiency improvement possibly is proportional to the number of nodes in the template, as the ad-vantage in Figure 6 is larger compared to Figure 4 where all settings are aggregated. Statistical differences are shown in Figure 8 across all settings considered, where a critical difference diagram is used to show the average measure ranks. It can be seen that all measures are significantly different from each other, except the normal and bias adjusted variants of mutual information. Clearly, the proposed linkage measures exploit the problem structure more effectively compared to the existing measures, with the template informed Node measure performing best followed by ð‘€ð¼ ð‘šð‘Žð‘ ð‘˜ð‘’ð‘‘ .123456 

> Random
> MI
> MI adjusted Node (static)
> MI masked
> Node
> CD

Figure 8: Statistical differences across all problem, template height and linear scaling combinations considered obtained by performing a Friedman test followed by the pairwise Ne-menyi test using [ 23 ]. Lower ranks indicate better perfor-mance, and measures not connected by a bar exceed the crit-ical difference (CD) and are significant (at ð‘ = 0.05 ). 

> 7Koch et al.

4.3 Experiment 2: What linkage structures are learned? 

The learned pairwise similarities pertaining to the various mea-sures averaged over 30 runs on the Bike Sharing problem are shown in Figure 7. Results for the other problems are shown in Section D of the supplementary material. Note that both Node versions use the same similarity measure that does not change throughout evolution and Random is not shown as it is not informative. The similari-ties shown at generation 0 are recorded before any variation is performed, i.e. directly after initialization. In the first row of Figure 7, the bias in the mutual information estimate due to non-uniform initialization (as addressed in [ 52 ]) is clearly visible in the first generation, and notably absent for 

ð‘€ð¼ ð‘Žð‘‘ ð‘—ð‘¢ð‘ ð‘¡ð‘’ð‘‘ . As the number of generations performed increases, the bias in MI (and the bias adjustment of ð‘€ð¼ ð‘Žð‘‘ ð‘—ð‘¢ð‘ ð‘¡ð‘’ð‘‘ ) becomes less visible and both measures learn similar patterns of linked variables. Compared to the existing measures, ð‘€ð¼ ð‘šð‘Žð‘ ð‘˜ð‘’ð‘‘ does not seem to suffer from the same bias as MI, suggesting that not accounting for inactive variables is the cause of the bias. The linkage structure learned by ð‘€ð¼ ð‘šð‘Žð‘ ð‘˜ð‘’ð‘‘ is highly similar to the node proximity measure introduced. However, with increasing generations, parts of the template structure learned by ð‘€ð¼ ð‘šð‘Žð‘ ð‘˜ð‘’ð‘‘ degrade over time. The patterns learned in later generations by MI and ð‘€ð¼ ð‘Žð‘‘ ð‘—ð‘¢ð‘ ð‘¡ð‘’ð‘‘ , as well as the linkage structure learned by ð‘€ð¼ ð‘šð‘Žð‘ ð‘˜ð‘’ð‘‘ resemble the Node measure, suggesting that indeed the distance of variables in the fixed template is highly informative. Clearly, once introns are taken into account, this structure can be learned from population statistics. Section D of the supplementary material shows that this general trend of mutual information based linkage learning approximating the template structure holds across all problems considered. 

5 Discussion 

Learning and exploitation of linkage information can be crucial for effective optimization. We re-visited linkage learning in GP-GOMEA with the aim to more fully exploit the available problem knowledge, in particular knowledge about inactive variables and the user-defined structural template shared across all solutions. The experiments clearly show that the proposed measures outperform existing measures. This holds across all combinations of template heights and linear scaling considered, with clear speed-ups in terms of evaluations and runtime performance. The proposed methods include no new algorithm settings, and either are a simple modification to how introns are handled during the calculation of the mutual information between variable pairs or require no runtime cost in the case of the node proximity measure. Notably, masking introns to improve the signal-to-noise ratio of variables not under selection pressure is not specific to GP, and can be applied to other settings where it is known which variables are inactive, as in [19, 35, 36]. For the first time in GP-GOMEA, we also explored what linkage structures are learned by the various measures. Interestingly, all mutual information based variants seem to approximate the struc-tural template, suggesting that at least for the problems considered, the solution representation is more important than variable val-ues when it comes to the dependency strengths between variables. However, there could be problems where this does not hold. Clearly, the best solutions differ between problems, suggesting that the importance of different parts in the template can vary as well. This could potentially still be captured by runtime measures, especially the masked measure that we proposed that takes into account the conditionally inactive variables that appear due to the use of a tree template. It would be interesting to see in future work if such hybrid linkage measures could improve performance further. Another aspect to consider is the modeling of the learned linkage, i.e. why randomization of the node proximity performs better, and how much randomization of the FOS structure is best. Currently, the tie breaking in UPGMA only resorts to randomness in the face of equally strongly linked subsets of variables, which might not truly lead to FOS structures that over multiple generations form an unbiased representation of the linkage information. Beyond improving the learning, modeling and exploitation of linkage, other factors potentially interacting with linkage learning in GP were not considered, but could be equally important. This includes, for example, representing numerical constants differently as done in [ 27 ] and the analysis of possible effects on interpretability from an explainability viewpoint, which is not considered here. Nonetheless, the fixed template size used in GP-GOMEA typically restricts the search space to small, and thus more likely humanly interpretable expressions. 

6 Conclusion 

Learning and exploiting linkage is a key mechanism of GP-GOMEA, an algorithm among the current state-of-the-art when it comes to SR. With this in mind, we revisited this core aspect of the algo-rithm, leading to two new linkage measures that exploit more of the available problem knowledge. Our experiments, through which we evaluated the use of various linkage measures in the context of varying template sizes and the use of linear scaling, showed that by exploiting known problem information, clear accuracy and runtime performance improve-ments are possible across all combinations considered. Moreover, the linkage structures that are learned, align with the user-defined structural template available a priori, with the best performing link-age measure being the one that encapsulates exactly the structure of the template, requiring no additional on-line linkage learning from the population statistics. This establishes a clear bridge be-tween GP-GOMEA and other linkage learning EAs, particularly GOMEA variants, highlighting the advantage of explicitly modeling compositional problem structure, a principle widely recognized in discrete and real-valued optimization. 

References         

> [1] 2012. Symbolic Regression Competition. https://web.archive.org/web/ 20120628140646/http%3A//casnew.iti.upv.es/index.php/evocompetitions/105-symregcompetition. [2] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare. [n. d.]. Deep Reinforcement Learning at the Edge of the Statistical Precipice. ([n. d.]). [3] Guilherme S. Imai Aldeia, Hengzhe Zhang, Geoffrey Bomarito, Miles Cranmer, Alcides Fonseca, Bogdan Burlacu, William G. La Cava, and FabrÃ­cio Olivetti de FranÃ§a. 2025. Call for Action: Towards the next Generation of Symbolic Regres-sion Benchmark. In Proceedings of the Genetic and Evolutionary Computation Conference Companion . 2529â€“2538. arXiv:2505.03977 [cs] doi:10.1145/3712255. 3734309 [4] Georgios Andreadis, Tanja Alderliesten, and Peter A. N. Bosman. 2024. Fitness-Based Linkage Learning and Maximum-Clique Conditional Linkage Modelling 8

Introns and Templates Matter: Rethinking Linkage in GP-GOMEA 

for Gray-box Optimization with RV-GOMEA. arXiv:2402.10757 [cs] doi:10.48550/ arXiv.2402.10757 [5] Peter A.N. Bosman, Ngoc Hoang Luong, and Dirk Thierens. 2016. Expanding from Discrete Cartesian to Permutation Gene-pool Optimal Mixing Evolutionary Algorithms. In Proceedings of the Genetic and Evolutionary Computation Confer-ence 2016 (GECCO â€™16) . Association for Computing Machinery, New York, NY, USA, 637â€“644. doi:10.1145/2908812.2908917 [6] Peter A.N. Bosman and Dirk Thierens. 2012. Linkage Neighbors, Optimal Mixing and Forced Improvements in Genetic Algorithms. In Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation (GECCO â€™12) . Association for Computing Machinery, New York, NY, USA, 585â€“592. doi:10.1145/2330163.2330247 [7] Peter A.N. Bosman and Dirk Thierens. 2013. More Concise and Robust Linkage Learning by Filtering and Combining Linkage Hierarchies. In Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation (GECCO â€™13) .Association for Computing Machinery, New York, NY, USA, 359â€“366. doi:10. 1145/2463372.2463420 [8] Peter A. N. Bosman and Dirk Thierens. 1999. Linkage Information Processing in Distribution Estimation Algorithms. In Proceedings of the 1st Annual Confer-ence on Genetic and Evolutionary Computation - Volume 1 (GECCOâ€™99) . Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 60â€“67. [9] Peter A. N. Bosman and Dirk Thierens. 2012. On Measures to Build Linkage Trees in LTGA. In Parallel Problem Solving from Nature - PPSN XII , David Hutchison, Takeo Kanade, Josef Kittler, Jon M. Kleinberg, Friedemann Mattern, John C. Mitchell, Moni Naor, Oscar Nierstrasz, C. Pandu Rangan, Bernhard Stef-fen, Madhu Sudan, Demetri Terzopoulos, Doug Tygar, Moshe Y. Vardi, Gerhard Weikum, Carlos A. Coello Coello, Vincenzo Cutello, Kalyanmoy Deb, Stephanie Forrest, Giuseppe Nicosia, and Mario Pavone (Eds.). Vol. 7491. Springer Berlin Heidelberg, Berlin, Heidelberg, 276â€“285. doi:10.1007/978-3-642-32937-1_28 [10] Anton Bouter, Tanja Alderliesten, Cees Witteveen, and Peter A. N. Bosman. 2017. Exploiting Linkage Information in Real-Valued Optimization with the Real-Valued Gene-Pool Optimal Mixing Evolutionary Algorithm. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO â€™17) . Association for Computing Machinery, New York, NY, USA, 705â€“712. doi:10.1145/3071178. 3071272 [11] Anton Bouter, Stefanus C. Maree, Tanja Alderliesten, and Peter A. N. Bosman. 2020. Leveraging Conditional Linkage Models in Gray-Box Optimization with the Real-Valued Gene-Pool Optimal Mixing Evolutionary Algorithm. In Proceedings of the 2020 Genetic and Evolutionary Computation Conference . ACM, CancÃºn Mexico, 603â€“611. doi:10.1145/3377930.3390225 [12] Anton Bouter, Dirk Thierens, and Peter A. N. Bosman. 2025. The Pitfalls and Potentials of Adding Gene-invariance to Optimal Mixing. In Proceedings of the Genetic and Evolutionary Computation Conference Companion (GECCO â€™25 Com-panion) . Association for Computing Machinery, New York, NY, USA, 1918â€“1926. doi:10.1145/3712255.3734293 [13] Ying-ping Chen, Tian-Li Yu, K. Sastry, and D. Goldberg. 2007. A Survey of Linkage Learning Techniques in Genetic and Evolutionary Algorithms. [14] Janez DemÅ¡ar. 2006. Statistical Comparisons of Classifiers over Multiple Data Sets. J. Mach. Learn. Res. 7 (Dec. 2006), 1â€“30. [15] Arkadiy Dushatskiy, Marco Virgolin, Anton Bouter, Dirk Thierens, and Peter A. N. Bosman. 2021. Parameterless Gene-pool Optimal Mixing Evolutionary Algorithms. arXiv:2109.05259 [cs] doi:10.48550/arXiv.2109.05259 [16] Hadi Fanaee-T. 2013. Bike Sharing. doi:10.24432/C5W894 [17] David E. Goldberg, Bradley Korb, and Kalyanmoy Deb. [n. d.]. Messy Genetic Algorithms: Motivation, Analysis, and First Results. Complex Systems 3, 5 ([n. d.]). [18] Ilan Gronau and Shlomo Moran. 2007. Optimal Implementations of UPGMA and Other Common Clustering Algorithms. Inform. Process. Lett. 104, 6 (Dec. 2007), 205â€“210. doi:10.1016/j.ipl.2007.07.002 [19] Arthur Guijt, Dirk Thierens, Tanja Alderliesten, and Peter A. N. Bosman. 2024. Exploring the Search Space of Neural Network Combinations Obtained with Efficient Model Stitching. In Proceedings of the Genetic and Evolutionary Computa-tion Conference Companion (GECCO â€™24 Companion) . Association for Computing Machinery, New York, NY, USA, 1914â€“1923. doi:10.1145/3638530.3664131 [20] Arthur Guijt, Dirk Thierens, Tanja Alderliesten, and Peter A. N. Bosman. 2024. Stitching for Neuroevolution: Recombining Deep Neural Networks without Breaking Them. In Proceedings of the Genetic and Evolutionary Computation Conference Companion . 1914â€“1923. arXiv:2403.14224 [cs] doi:10.1145/3638530. 3664131 [21] Joe Harrison, Peter A. N. Bosman, and Tanja Alderliesten. 2025. Thinking Outside the Template with Modular GP-GOMEA. arXiv:2505.01262 [cs] doi:10.48550/ arXiv.2505.01262 [22] Erik Hemberg, Kalyan Veeramachaneni, James McDermott, Constantin Berzan, and Una-May Oâ€™Reilly. 2012. An Investigation of Local Patterns for Estimation of Distribution Genetic Programming. In Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation (GECCO â€™12) . Association for Comput-ing Machinery, New York, NY, USA, 767â€“774. doi:10.1145/2330163.2330270 [23] Steffen Herbold. 2020. Autorank: A Python Package for Automated Ranking of Classifiers. (2020). [24] John H. Holland. 1992. Adaptation in Natural and Artificial Systems: An Introduc-tory Analysis with Applications to Biology, Control, and Artificial Intelligence . The MIT Press. doi:10.7551/mitpress/1090.001.0001 [25] I-Cheng Yeh. 1998. Concrete Compressive Strength. doi:10.24432/C5PK67 [26] Maarten Keijzer. 2004. Scaled Symbolic Regression. Genetic Programming and Evolvable Machines 5, 3 (Sept. 2004), 259â€“269. doi:10.1023/B:GENP.0000030195. 77571.f9 [27] Johannes Koch, Tanja Alderliesten, and Peter A. N. Bosman. 2024. Simultaneous Model-Based Evolution of Constants and Expression Structure in GP-GOMEA for Symbolic Regression. In Parallel Problem Solving from Nature â€“ PPSN XVIII ,Michael Affenzeller, Stephan M. Winkler, Anna V. Kononova, Heike Trautmann, Tea TuÅ¡ar, Penousal Machado, and Thomas BÃ¤ck (Eds.). Vol. 15148. Springer Nature Switzerland, Cham, 238â€“255. doi:10.1007/978-3-031-70055-2_15 [28] John R. Koza. 1994. Genetic Programming as a Means for Programming Com-puters by Natural Selection. Statistics and Computing 4, 2 (June 1994), 87â€“112. doi:10.1007/BF00175355 [29] William La Cava, Bogdan Burlacu, Marco Virgolin, Michael Kommenda, Patryk Orzechowski, FabrÃ­cio Olivetti de FranÃ§a, Ying Jin, and Jason H. Moore. 2021. Contemporary Symbolic Regression Methods and Their Relative Performance. 

Advances in neural information processing systems 2021, DB1 (Dec. 2021), 1â€“16. [30] Fernando G Lobo, Kalyanmoy Deb, David E Goldberg, Georges R Harik, and Liwei Wang. [n. d.]. Compressed Introns in a Linkage Learning Genetic Algorithm. ([n. d.]). [31] Hammad Majeed and Conor Ryan. 2006. A Less Destructive, Context-Aware Crossover Operator for GP. In Genetic Programming , Pierre Collet, Marco Tomassini, Marc Ebner, Steven Gustafson, and AnikÃ³ EkÃ¡rt (Eds.). Springer, Berlin, Heidelberg, 36â€“48. doi:10.1007/11729976_4 [32] Miguel Nicolau and Alexandros Agapitos. 2021. Choosing Function Sets with Better Generalisation Performance for Symbolic Regression Models. Genetic Programming and Evolvable Machines 22, 1 (March 2021), 73â€“100. doi:10.1007/ s10710-020-09391-4 [33] Martin Pelikan, David E. Goldberg, and Erick CantÃº-Paz. 1999. BOA: The Bayesian Optimization Algorithm. In Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation - Volume 1 (GECCOâ€™99) . Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 525â€“532. [34] Riccardo Poli and Nicholas Freitag McPhee. 2008. A Linear Estimation-of-Distribution GP System. In Genetic Programming , Michael Oâ€™Neill, Leonardo Vanneschi, Steven Gustafson, Anna Isabel Esparcia AlcÃ¡zar, Ivanoe De Falco, Antonio Della Cioppa, and Ernesto Tarantino (Eds.). Springer, Berlin, Heidelberg, 206â€“217. doi:10.1007/978-3-540-78671-9_18 [35] Michal W. Przewozniczek, Peter A.N. Bosman, Anton Bouter, Arthur Guijt, Marcin M. Komarnicki, and Dirk Thierens. 2025. Conditional Direct Empir-ical Linkage Discovery for Solving Multi-Structured Problems. In Proceedings of the 18th ACM/SIGEVO Conference on Foundations of Genetic Algorithms (FOGA â€™25) . Association for Computing Machinery, New York, NY, USA, 238â€“249. doi:10.1145/3729878.3746624 [36] Michal W. Przewozniczek, Bartosz Frej, and Marcin M. Komarnicki. 2024. The Hop-like Problem Nature â€“ Unveiling and Modelling New Features of Real-World Problems. arXiv:2406.01215 [cs] doi:10.48550/arXiv.2406.01215 [37] Michal W. Przewozniczek, Marcin M. Komarnicki, and Bartosz Frej. 2021. Direct Linkage Discovery with Empirical Linkage Learning. In Proceedings of the Genetic and Evolutionary Computation Conference . ACM, Lille France, 609â€“617. doi:10. 1145/3449639.3459333 [38] Ozeas Quevedo De Carvalho and Darrell Whitley. 2025. Dramatically Faster Partition Crossover for the Traveling Salesman Problem. In Proceedings of the Ge-netic and Evolutionary Computation Conference . ACM, NH Malaga Hotel Malaga Spain, 818â€“826. doi:10.1145/3712256.3726465 [39] Cynthia Rudin. 2019. Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead. Nature Machine Intelligence 1, 5 (May 2019), 206â€“215. doi:10.1038/s42256-019-0048-x [40] Krzysztof L. Sadowski, Peter A.N. Bosman, and Dirk Thierens. 2016. Learning and Exploiting Mixed Variable Dependencies with a Model-Based EA. In 2016 IEEE Congress on Evolutionary Computation (CEC) . 4382â€“4389. doi:10.1109/CEC. 2016.7744347 [41] RafaÅ‚ SaÅ‚ustowicz and JÃ¼rgen Schmidhuber. 1997. Probabilistic Incremental Pro-gram Evolution: Stochastic Search through Program Space. In Machine Learning: ECML-97 , Maarten van Someren and Gerhard Widmer (Eds.). Springer, Berlin, Heidelberg, 213â€“220. doi:10.1007/3-540-62858-4_86 [42] Thalea Schlender, Mafalda Malafaia, Tanja Alderliesten, and Peter Bosman. 2024. Improving the Efficiency of GP-GOMEA for Higher-Arity Operators. In Proceed-ings of the Genetic and Evolutionary Computation Conference . ACM, Melbourne VIC Australia, 971â€“979. doi:10.1145/3638529.3654118 [43] E. M. C. Sijben, T. Alderliesten, and P. A. N. Bosman. 2022. Multi-Modal Multi-Objective Model-Based Genetic Programming to Find Multiple Diverse High-Quality Models. In Proceedings of the Genetic and Evolutionary Computation Conference . ACM, Boston Massachusetts, 440â€“448. doi:10.1145/3512290.3528850 [44] Jinglu Song, Qiang Lu, Bozhou Tian, Jingwen Zhang, Jake Luo, and Zhiguang Wang. 2024. Symbol Graph Genetic Programming for Symbolic Regression. In 9Koch et al. 

Parallel Problem Solving from Nature â€“ PPSN XVIII: 18th International Conference, PPSN 2024, Hagenberg, Austria, September 14â€“18, 2024, Proceedings, Part I . Springer-Verlag, Berlin, Heidelberg, 221â€“237. doi:10.1007/978-3-031-70055-2_14 [45] Dirk Thierens. 1999. Scalability Problems of Simple Genetic Algorithms. Evolu-tionary Computation 7, 4 (Dec. 1999), 331â€“352. doi:10.1162/evco.1999.7.4.331 [46] Dirk Thierens. 2010. The Linkage Tree Genetic Algorithm. In Parallel Problem Solving from Nature, PPSN XI , Robert Schaefer, Carlos Cotta, Joanna KoÅ‚odziej, and GÃ¼nter Rudolph (Eds.). Springer, Berlin, Heidelberg, 264â€“273. doi:10.1007/978-3-642-15844-5_27 [47] Dirk Thierens and Peter Bosman. 2012. Predetermined versus Learned Linkage Models. In Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation (GECCO â€™12) . Association for Computing Machinery, New York, NY, USA, 289â€“296. doi:10.1145/2330163.2330205 [48] D. Pope Thomas Brooks. 1989. Airfoil Self-Noise. doi:10.24432/C5VW2C [49] Renato TinÃ³s, Michal Przewozniczek, Darrell Whitley, and Francisco Chicano. 2023. Genetic Algorithm with Linkage Learning. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO â€™23) . Association for Computing Machinery, New York, NY, USA, 981â€“989. doi:10.1145/3583131.3590349 [50] Renato TinÃ³s, Michal W. Przewozniczek, Darrell Whitley, and Francisco Chicano. 2024. Iterated Local Search with Linkage Learning. ACM Trans. Evol. Learn. Optim. 4, 2 (June 2024), 7:1â€“7:29. doi:10.1145/3651165 [51] Marco Virgolin, Tanja Alderliesten, Cees Witteveen, and Peter A. N. Bosman. 2017. Scalable Genetic Programming by Gene-Pool Optimal Mixing and Input-Space Entropy-Based Building-Block Learning. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO â€™17) . Association for Computing Machinery, New York, NY, USA, 1041â€“1048. doi:10.1145/3071178.3071287 [52] Marco Virgolin, Tanja Alderliesten, Cees Witteveen, and Peter A. N. Bosman. 2021. Improving Model-based Genetic Programming for Symbolic Regression of Small Expressions. Evolutionary Computation 29, 2 (June 2021), 211â€“237. arXiv:1904.02050 [cs] doi:10.1162/evco_a_00278 [53] Marco Virgolin and Peter A. N. Bosman. 2022. Coefficient Mutation in the Gene-Pool Optimal Mixing Evolutionary Algorithm for Symbolic Regression. In 

Proceedings of the Genetic and Evolutionary Computation Conference Companion .ACM, Boston Massachusetts, 2289â€“2297. doi:10.1145/3520304.3534036 [54] Marco Virgolin, Eric Medvet, Tanja Alderliesten, and Peter A. N. Bosman. 2022. Less Is More: A Call to Focus on Simpler Models in Genetic Programming for Interpretable Machine Learning. arXiv:2204.02046 [cs] doi:10.48550/arXiv.2204. 02046 [55] Ekaterina J. Vladislavleva, Guido F. Smits, and Dick den Hertog. 2009. Order of Nonlinearity as a Complexity Measure for Models Generated by Symbolic Regression via Pareto Genetic Programming. IEEE Transactions on Evolutionary Computation 13, 2 (April 2009), 333â€“349. doi:10.1109/TEVC.2008.926486 [56] L. Darrell Whitley, Francisco Chicano, and Brian W. Goldman. 2016. Gray Box Optimization for Mk Landscapes (NK Landscapes and MAX-kSAT). Evolutionary Computation 24, 3 (2016), 491â€“519. doi:10.1162/EVCO_a_00184 [57] Garnett Wilson and Wolfgang Banzhaf. 2008. A Comparison of Cartesian Ge-netic Programming and Linear Genetic Programming. In Genetic Programming ,Michael Oâ€™Neill, Leonardo Vanneschi, Steven Gustafson, Anna Isabel Espar-cia AlcÃ¡zar, Ivanoe De Falco, Antonio Della Cioppa, and Ernesto Tarantino (Eds.). Springer, Berlin, Heidelberg, 182â€“193. doi:10.1007/978-3-540-78671-9_16 

A Additional Measures 

In Figure 9, and Sections B to E, results are provided for all mea-sures in the main work, and two additional measures. First, the subfunction based common subfunction count (as #CS, with re-peated LT rebuilding) and the univariate FOS (as Univariate), which assumes no variable interaction and consists of all single variable subsets. The univariate FOS is often used as baseline, and it can be the case that this FOS performs well even if there are variable inter-actions [ 11 ]. However, thus far this FOS has not been considered in GP-GOMEA due to the fact that GP clearly is not a separable problem with independent decision variables. All results in the supplementary material include both additional measures, with the exeption for Section D, where the univariate model is not shown as it assumes that there is no pairwise similarity. The results indicate that the univariate model clearly performs worse than all other methods indicating that linkage matters for GP. Furthermore, as expected, the Node measure which does not assume subfunction independence in the problem formulation generally outperforms the subfunction based measure. 12345678

Univariate 

Random 

MI 

MI adjusted Node (static) 

#CS 

MI masked 

Node 

CD 

Figure 9: Statistical differences across all problem, template height and linear scaling combinations considered obtained by performing a Friedman test followed by the pairwise Ne-menyi test using [ 23 ]. Lower ranks indicate better perfor-mance, and measures not connected by a bar exceed the crit-ical difference (CD) and are significant (at ð‘ = 0.05 ). 

B Probability of Improvement 

In addition to the null-hypothesis based used in the main work, the pairwise probabilities of one measure performing better than another measure across all runs is shown in Figure 10, computed using a stratified bootstrap using [ 2]. Probabilities with confidence intervals overlapping 50% are clearly not significant and the results align with Figure 8. In expectation, all measures outperform the uni-variate model clearly, as well as the random measure showcasing that linkage can be exploited in GP. Moreover, ð‘€ð¼ and ð‘€ð¼ ð‘Žð‘‘ ð‘—ð‘¢ð‘ ð‘¡ð‘’ð‘‘ 

perform comparably with an edge towards ð‘€ð¼ . The static version of Node likely outperforms the existing measures, but not the other proposed methods. The subfunction based measure likely outper-forms the existing measures, however, not the other measures pro-posed in this work except for the static version of the node prox-imity measure. ð‘€ð¼ ð‘šð‘Žð‘ ð‘˜ð‘’ð‘‘ is preferable to all measures except Node. Note that while Node is likely to perform best, there is overlap between the score distributions of all measures. For example, there is a roughly 15% chance of Random outperforming Node after the full budget is exhausted. 

C Per Problem Results 

Figure 11 shows the training accuracy throughout evolution and Fig-ure 12 shows the distribution of the final training ð‘… 2 scores achieved per problem. Notably, the proposed methods perform better in all settings except the Dow Chemical problem with a template height of 5. This problem has the largest number of input features, however, this likely it not the cause of the proposed methods underperform-ing as this is not the case with larger templates. Interestingly, while the Node measure typically reaches higher accuracies faster, the runs do not exhaust the evaluation faster. This possibly indicates that more GOM steps only modify inactive variables and thus no evaluation is performed. This effect is particularly evident for the Univariate model on the Dow Chemical problem without LS and template height 5, where some runs have a vastly longer runtime. 

D Per Problem Similarity Measures 

To assess whether the linkage structure learned corresponds to the structural template across the problems considered, Figure 13 dis-plays the averaged linkage across runs with linear scaling, and Fig-ure 14 shows the results from an additional repeat of the experiment 

10 Introns and Templates Matter: Rethinking Linkage in GP-GOMEA Univariate 

> Random
> MI adjusted
> MI
> Node (static)
> #CS
> MI masked

B

> Random
> MI adjusted
> MI
> Node (static)
> #CS
> MI masked
> Node
> A
> 0.92
> [0.90, 0.94]
> 0.90
> [0.88, 0.91]
> 0.57
> [0.54, 0.60]
> 0.90
> [0.88, 0.91]
> 0.59
> [0.56, 0.62]
> 0.51
> [0.48, 0.54]
> 0.91
> [0.89, 0.92]
> 0.63
> [0.60, 0.66]
> 0.57
> [0.54, 0.60]
> 0.57
> [0.54, 0.60]
> 0.93
> [0.92, 0.95]
> 0.69
> [0.66, 0.72]
> 0.59
> [0.56, 0.62]
> 0.59
> [0.56, 0.62]
> 0.54
> [0.51, 0.57]
> 0.95
> [0.94, 0.96]
> 0.73
> [0.71, 0.76]
> 0.66
> [0.63, 0.69]
> 0.66
> [0.63, 0.69]
> 0.58
> [0.54, 0.61]
> 0.54
> [0.51, 0.57]
> 0.97
> [0.96, 0.98]
> 0.85
> [0.82, 0.87]
> 0.78
> [0.75, 0.80]
> 0.79
> [0.76, 0.81]
> 0.73
> [0.71, 0.76]
> 0.68
> [0.66, 0.71]
> 0.66
> [0.63, 0.69]
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> P(A > B)

Figure 10: The pairwise probabilities of measure A achieving a higher training ð‘… 2 score than measure B across all runs per-formed ( ð‘ƒ (ðµ â‰¥ ð´ ) = 1 âˆ’ ð‘ƒ (ð´ > ðµ )). The interval corresponds to the 95% bootstrapped confidence interval as per [2]. 

without linear scaling. Without linear scaling, the averaged struc-tures still generally align with the node proximity and the initial bias of MI is clearly visible. However, the emerging structure tends to be noisier compared to with LS enabled, in particular for the Dow Chemical dataset. In addition to the pairwise similarity shown when averaged across all 30 runs, single run results with LS enabled are shown in Figure 15 to give some indication of the variance between runs. Again, the masked MI variant approximates the template structure and MI shows an initial bias. 

E Effect of Operator Set Size 

The first experiment in the paper considers the combinations of tem-plate height and linear scaling with a single operator set. However, the operator set itself is another core parameter that potentially affects linkage and thus the first experiment was repeated with a larger operator set containing {+ , âˆ’, Ã—, Ã·, sin , cos , exp , log , âˆšÂ·, Â·2 }.The interval estimate comparisons factorized by setting for this larger experiment are shown in Figure 16 and Figure 17 shows per problem results. The additional operators tend to perform worse compared to the smaller operator set, while the relative perfor-mance between the various linkage measures remains largely un-affected. Notably, the mutual information based measures tend to perform worse compared to the random measure when the larger operator set is used. 

11 Koch et al. 0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.50 

0.55 

0.60 

0.65 

> R2 Train
> H=5

Airfoil 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.725 

0.750 

0.775 

0.800 

0.825 

Bike Sharing 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.6 

0.7 

0.8 

Concrete Compressive Strength 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.3 

0.4 

0.5 

0.6 

Dow Chemical 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.6 

0.7 

0.8 

Tower 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.65 

0.70 

0.75 

> R2 Train
> H=5 LS

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.82 

0.84 

0.86 

0.88 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.78 

0.80 

0.82 

0.84 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.75 

0.80 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.85 

0.86 

0.87 

0.88 

0.89 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.6 

0.7 

> R2 Train
> H=7

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.75 

0.80 

0.85 

0.90 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.6 

0.7 

0.8 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.4 

0.6 

0.8 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.65 

0.70 

0.75 

0.80 

0.85 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.70 

0.75 

0.80 

0.85 

> R2 Train
> H=7 LS

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.84 

0.86 

0.88 

0.90 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.82 

0.84 

0.86 

0.88 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.75 

0.80 

0.85 

0.0 0.2 0.4 0.6 0.8 1.0 

1e7 

0.86 

0.88 

0.90 

0.92 

Evaluations 

0 250 500 750 1000 

0.50 

0.55 

0.60 

0.65 

> R2 Train
> H=5

Airfoil 

0 200 400 

0.725 

0.750 

0.775 

0.800 

0.825 

Bike Sharing 

0 200 400 600 800 

0.60 

0.65 

0.70 

0.75 

0.80 

Concrete Compressive Strength 

0 2000 4000 

0.3 

0.4 

0.5 

0.6 

Dow Chemical 

0 1000 2000 3000 

0.60 

0.65 

0.70 

0.75 

0.80 

Tower 

0 250 500 750 1000 

0.650 

0.675 

0.700 

0.725 

0.750 

> R2 Train
> H=5 LS

0 200 400 600 800 

0.82 

0.84 

0.86 

0.88 

0 200 400 600 

0.80 

0.82 

0.84 

0 200 400 600 

0.725 

0.750 

0.775 

0.800 

0.825 

0 1000 2000 3000 

0.85 

0.86 

0.87 

0.88 

0.89 

0 1000 2000 3000 4000 

0.55 

0.60 

0.65 

0.70 

0.75 

> R2 Train
> H=7

0 500 1000 1500 

0.75 

0.80 

0.85 

0.90 

0 500 1000 1500 2000 

0.65 

0.70 

0.75 

0.80 

0.85 

0 1000 2000 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0 5000 10000 15000 

0.70 

0.75 

0.80 

0.85 

0 1000 2000 3000 

0.70 

0.75 

0.80 

0.85 

> R2 Train
> H=7 LS

0 500 1000 1500 

0.86 

0.88 

0.90 

0 1000 2000 

0.82 

0.84 

0.86 

0.88 

0 1000 2000 

0.75 

0.80 

0.85 

0 5000 10000 15000 

0.88 

0.90 

0.92 

Approximate Runtime [s] 

Random MI MI adjusted MI masked Node Node (static) #CS Univariate 

Figure 11: Median training ð‘… 2 score (higher is better) over evaluations and approximate runtime across problem, template and linear scaling combinations. The filled area corresponds to the interquartile range, and the runtime where the first run finished is marked with a cross, after which the final value obtained is re-used for completed runs until all runs have finished. 

12 Introns and Templates Matter: Rethinking Linkage in GP-GOMEA 0.6 0.8 

H = 7 

LS 

H = 7 

H = 5 

LS 

H = 5 

> R2 Train

Airfoil 

0.8 0.9 

Bike Sharing 

0.6 0.8 

Concrete Compressive Strength 

0.4 0.6 0.8 

Dow Chemical 

0.6 0.7 0.8 0.9 

Tower 

Random MI MI adjusted MI masked Node Node (static) #CS Univariate 

Figure 12: ð‘… 2 score (higher is better) across problem, template and linear scaling combination considered. The boxes show the quartiles and the whiskers extend to points that lie within 1.5 inter-quartile ranges of the lower and upper quartile. Observations outside this range are displayed independently. 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions       

> 0510 15 20 25 30
> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

Node / Node (static) 

> 0.0
> 0.2
> 0.4
> 0.2
> 0.4
> 0.6
> 0.8
> 0.00
> 0.05
> 0.10
> 1
> 2
> 3
> 4
> 0.25
> 0.50
> 0.75
> 1.00
> 0.2
> 0.4
> 0.6
> 0.8

(a) Airfoil 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions       

> 0510 15 20 25 30
> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

Node / Node (static)  

> 0.2
> 0.4
> 0.2
> 0.4
> 0.6
> 0.8
> âˆ’0.05
> 0.00
> 0.05
> 0.10
> 1
> 2
> 3
> 4
> 0.25
> 0.50
> 0.75
> 1.00
> 0.2
> 0.4
> 0.6
> 0.8

(b) Concrete Compressive Strength 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions       

> 0510 15 20 25 30
> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

Node / Node (static) 

> 0.5
> 1.0
> 1.5
> 2.0
> 0.4
> 0.6
> 0.8
> âˆ’0.3
> âˆ’0.2
> âˆ’0.1
> 0.0
> 1
> 2
> 3
> 4
> 0.5
> 1.0
> 1.5
> 0.2
> 0.4
> 0.6
> 0.8

(c) Dow Chemical 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions       

> 0510 15 20 25 30
> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

MI masked       

> 0510 15 20 25 30

Node / Node (static)  

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> 0.2
> 0.4
> 0.6
> 0.8
> âˆ’0.1
> 0.0
> 1
> 2
> 3
> 4
> 0.5
> 1.0
> 1.5
> 0.2
> 0.4
> 0.6
> 0.8

(d) Tower 

Figure 13: The different linkage measures over generations, averaged over 30 runs per problem with a template height of 5 (31 Nodes), LS and a fixed population size of 1024. Note that different scales are used for each measure as the focus lies on differences between variable pairs within measures instead of differences between linkage measures. 

13 Koch et al. 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions 

0 5 10 15 20 25 30 

> 0510 15 20 25 30

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

Node / Node (static) 

0.1 

0.2 

0.3 

0.4 

0.4 

0.6 

0.8 

âˆ’0.05 

0.00 

0.05 

0.10 

1

2

3

4

0.25 

0.50 

0.75 

1.00 

0.2 

0.4 

0.6 

0.8 

(a) Airfoil 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions 

0 5 10 15 20 25 30 

> 0510 15 20 25 30

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

Node / Node (static) 

0.2 

0.4 

0.2 

0.4 

0.6 

0.8 

âˆ’0.05 

0.00 

0.05 

1

2

3

4

0.25 

0.50 

0.75 

1.00 

1.25 

0.2 

0.4 

0.6 

0.8 

(b) Bike Sharing (Daily) 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions 

0 5 10 15 20 25 30 

> 0510 15 20 25 30

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

Node / Node (static) 

0.1 

0.2 

0.3 

0.4 

0.4 

0.6 

0.8 

âˆ’0.05 

0.00 

0.05 

0.10 

0.15 

1

2

3

4

0.00 

0.25 

0.50 

0.75 

1.00 

0.2 

0.4 

0.6 

0.8 (c) Concrete Compressive Strength 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions 

0 5 10 15 20 25 30 

> 0510 15 20 25 30

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

Node / Node (static) 

0.5 

1.0 

1.5 

2.0 

0.2 

0.4 

0.6 

0.8 

âˆ’0.2 

âˆ’0.1 

0.0 

1

2

3

4

0.0 

0.5 

1.0 

1.5 

0.2 

0.4 

0.6 

0.8 

(d) Dow Chemical 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions 

0 5 10 15 20 25 30 

> 0510 15 20 25 30

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

Node / Node (static) 

0.25 

0.50 

0.75 

1.00 

0.2 

0.4 

0.6 

0.8 

âˆ’0.1 

0.0 

1

2

3

4

0.5 

1.0 

1.5 

0.2 

0.4 

0.6 

0.8 (e) Tower 

Figure 14: The different linkage measures over generations, averaged over 30 runs per problem with a template height of 5 (31 Nodes), without LS and a fixed population size of 1024. Note that different scales are used for each measure as the focus lies on differences between variable pairs within measures instead of differences between linkage measures. 

14 Introns and Templates Matter: Rethinking Linkage in GP-GOMEA 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions 

0 5 10 15 20 25 30 

> 0510 15 20 25 30

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

Node / Node (static) 

0.00 

0.25 

0.50 

0.75 

1.00 

0.2 

0.4 

0.6 

0.8 

0.0 

0.1 

0.2 

0.3 

1

2

3

4

0.25 

0.50 

0.75 

1.00 

0.2 

0.4 

0.6 

0.8 

(a) Airfoil 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions 

0 5 10 15 20 25 30 

> 0510 15 20 25 30

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

Node / Node (static) 

0.2 

0.4 

0.6 

0.2 

0.4 

0.6 

0.8 

0.0 

0.1 

0.2 

1

2

3

4

0.0 

0.5 

1.0 

0.2 

0.4 

0.6 

0.8 

(b) Bike Sharing (Daily) 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions 

0 5 10 15 20 25 30 

> 0510 15 20 25 30

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

Node / Node (static) 

0.0 

0.5 

1.0 

0.2 

0.4 

0.6 

0.8 

0.0 

0.1 

0.2 

0.3 

1

2

3

4

0.25 

0.50 

0.75 

1.00 

0.2 

0.4 

0.6 

0.8 (c) Concrete Compressive Strength 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions 

0 5 10 15 20 25 30 

> 0510 15 20 25 30

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked MI masked 

0 5 10 15 20 25 30 

Node / Node (static) 

0.5 

1.0 

1.5 

2.0 

0.2 

0.4 

0.6 

0.8 

âˆ’0.3 

âˆ’0.2 

âˆ’0.1 

0.0 

0.1 

1

2

3

4

0.5 

1.0 

1.5 

2.0 

0.2 

0.4 

0.6 

0.8 

(d) Dow Chemical 0510 15 20 25 30 

Generation 0 

MI Generation 5 

MI Generation 10 

MI Generation 20 

MI Random 

> 0510 15 20 25 30

MI adjusted MI adjusted MI adjusted MI adjusted #Common Subfunctions 

0 5 10 15 20 25 30 

> 0510 15 20 25 30

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

MI masked 

0 5 10 15 20 25 30 

Node / Node (static) 

0.25 

0.50 

0.75 

1.00 

0.2 

0.4 

0.6 

0.8 

âˆ’0.1 

0.0 

0.1 

0.2 

1

2

3

4

0.5 

1.0 

1.5 

0.2 

0.4 

0.6 

0.8 (e) Tower 

Figure 15: The different linkage measures over generations, for the first of 30 runs per problem with a template height of 5 (31 Nodes), LS and a fixed population size of 1024. Note that different scales are used for each measure, as the focus lies on differences between variable pairs within measures instead of differences between linkage measures. Empty matrices indicate that the run finished before reaching the indicated generation. 

15 Koch et al. 0.60 0.65 0.70 0.75 0.80 0.85 0.90 

> H = 5
> #O = 5
> H = 5
> #O = 10
> H = 5
> LS
> #O = 5
> H = 5
> LS
> #O = 10
> H = 7
> #O = 5
> H = 7
> #O = 10
> H = 7
> LS
> #O = 5
> H = 7
> LS
> #O = 10
> R2 Train

Interquartile Mean 

0.60 0.65 0.70 0.75 0.80 0.85 0.90 

Mean of Medians 

Random MI MI adjusted MI masked Node Node (static) #CS Univariate 

Figure 16: Aggregate ð‘… 2 scores (higher is better) on the problems for each combination of template height, LS and operator set size considered in the extended experiment. The interquartile mean corresponds to the mean after discarding the bottom and top 25% of runs for each problem, and the mean of medians corresponds to the mean of the median performances on each problem. The colored bar corresponds to the 95% confidence interval estimated using a percentile bootstrap with stratified sampling as per [2] with the expected value in black. 0.6 0.8 

> H = 7
> LS
> #O = 10
> H = 7
> LS
> #O = 5
> H = 7
> #O = 10
> H = 7
> #O = 5
> H = 5
> LS
> #O = 10
> H = 5
> LS
> #O = 5
> H = 5
> #O = 10
> H = 5
> #O = 5
> R2 Train

Airfoil  

> 0.6 0.8

Bike Sharing  

> 0.6 0.8

Concrete Compressive Strength   

> 0.25 0.50 0.75

Dow Chemical  

> 0.6 0.8

Tower        

> Random MI MI adjusted MI masked Node Node (static) #CS Univariate

Figure 17: ð‘… 2 score (higher is better) across problem, template, linear scaling and operator set combination considered. The boxes show the quartiles and the whiskers extend to points that lie within 1.5 inter-quartile ranges of the lower and upper quartile. Observations outside this range are displayed independently. 

16