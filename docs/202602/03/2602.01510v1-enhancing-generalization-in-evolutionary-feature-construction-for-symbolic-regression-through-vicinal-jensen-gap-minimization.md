---
title: Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization
title_zh: 通过邻域 Jensen 间隙最小化增强符号回归中演化特征构建的泛化能力
authors: "Hengzhe Zhang, Qi Chen, Bing Xue, Wolfgang Banzhaf, Mengjie Zhang"
date: 2026-02-02
pdf: "https://arxiv.org/pdf/2602.01510v1"
tags: ["keyword:SR", "query:SR"]
score: 10.0
evidence: 直接针对符号回归中的泛化和过拟合问题，采用了演化特征构建方法。
tldr: 针对符号回归中遗传编程特征构造的过拟合问题，本文提出一种基于邻域Jensen间隙最小化的进化框架。研究证明了邻域风险可分解为经验风险与正则项之和，并据此设计了动态噪声估计和流形侵入检测机制。实验表明，该方法在58个数据集上优于15种主流算法，显著提升了模型的泛化能力。
motivation: 遗传编程在特征构造中容易出现过拟合，限制了其在自动化机器学习中的广泛应用。
method: 提出一种联合优化经验风险与邻域Jensen间隙的进化框架，并引入噪声估计和流形侵入检测机制。
result: 在58个数据集上的实验证明，该方法在泛化性能上优于其他复杂性度量指标及15种主流机器学习算法。
conclusion: 通过最小化邻域Jensen间隙并结合动态正则化策略，可以有效控制符号回归特征构造中的过拟合，提升模型泛化性。
---

## 摘要
近年来，基于遗传编程的特征构建作为一种增强学习性能的自动机器学习技术取得了显著成功。然而，过拟合仍然是限制其更广泛应用的一个挑战。为了提高泛化能力，我们证明了通过噪声扰动或基于 mixup 的数据增强所估计的邻域风险，其上界由经验风险与正则化项（有限差分或邻域 Jensen 间隙）之和构成。利用这一分解，我们提出了一种演化特征构建框架，通过联合优化经验风险和邻域 Jensen 间隙来控制过拟合。由于不同数据集的噪声水平可能有所不同，我们开发了一种噪声估计策略来动态调整正则化强度。此外，为了减轻流形侵入（即数据增强可能生成超出数据流形的不真实样本）的影响，我们提出了一种流形侵入检测机制。在 58 个数据集上的实验结果表明，与其它复杂度度量相比，Jensen 间隙最小化具有显著的有效性。与 15 种机器学习算法的对比进一步表明，采用所提过拟合控制策略的遗传编程实现了优越的性能。

## Abstract
Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.

---

## 论文详细总结（自动生成）

这是一份关于论文《Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization》的深度结构化总结：

### 1. 核心问题与整体含义（研究动机和背景）
论文关注的核心问题是**遗传编程（GP）在演化特征构建（EFC）过程中的过拟合问题**。
*   **背景**：GP 能够自动构建具有解释性的符号特征，但在处理回归任务时，往往会在训练集上表现优异，却难以泛化到未见数据。
*   **动机**：传统的复杂度控制方法（如限制模型大小、Rademacher 复杂度等）往往无法准确衡量现代特征构建技术的泛化性能。作者受到深度学习中“邻域风险最小化（VRM）”和“Mixup 数据增强”的启发，试图将其引入 GP 框架，通过数学证明将邻域风险分解为可优化的正则项，从而在保持拟合能力的同时提升平滑度和泛化性。

### 2. 方法论：核心思想与关键技术
论文提出了一种名为 **VJM-GP**（基于邻域 Jensen 间隙最小化的遗传编程）的框架：
*   **理论分解**：论文通过数学证明（定理 1 和定理 2）指出，邻域风险的上界可以分解为**经验风险**（训练误差）与**正则化项**之和。
    *   若使用噪声扰动，正则项为**有限差分（Finite Difference）**。
    *   若使用 Mixup（线性插值），正则项为**邻域 Jensen 间隙（Vicinal Jensen Gap）**。
*   **核心算法流程**：
    1.  **邻域数据合成**：在演化开始前，利用 Mixup 技术在训练样本及其邻域间生成合成样本。
    2.  **多目标优化**：利用 NSGA-II 算法同时优化两个目标：交叉验证损失（准确性）和邻域 Jensen 间隙（平滑性/复杂度）。
    3.  **噪声估计策略**：利用极端随机树（Extra Trees）预估数据集的噪声水平（$R^2$ 分数），动态调整正则化权重 $\tau$。
    4.  **流形侵入检测**：引入检测机制，剔除那些偏离原始数据流形的“虚假”合成样本，防止过度惩罚有效的非线性模型。

### 3. 实验设计
*   **数据集**：选自 PMLB（宾夕法尼亚机器学习基准库）的 **58 个真实世界回归数据集**。
*   **对比方法**：
    *   **内部对比（复杂度度量）**：标准 GP、Parsimony Pressure (PP)、Tikhonov 正则化 (TK)、Rademacher 复杂度 (RC)、P-VRM（悲观邻域风险最小化）等 8 种方法。
    *   **外部对比（主流算法）**：与 15 种算法竞争，包括 XGBoost、LightGBM、Random Forest、MLP、SVR 以及符号回归专用算法如 Operon、FFX 等。
*   **评估协议**：每个算法在每个数据集上独立运行 30 次，采用 $R^2$ 分数作为主要评价指标，并使用 Wilcoxon 符号秩检验进行统计显著性分析。

### 4. 资源与算力
*   **算力说明**：论文未明确提及具体的 GPU 或 CPU 型号及数量。
*   **训练时长**：论文提供了训练时间的对比数据。VJM-GP 由于需要对合成数据进行多次特征评估，计算开销较大。在 58 个数据集上，VJM-GP 的平均训练时间约为 **1041 秒**（约 17 分钟），而引入“早期停止（Early Stopping）”加速策略后，时间可缩短至 **381 秒**。总体而言，单次实验在 2 小时内可完成。

### 5. 实验数量与充分性
*   **实验规模**：实验涵盖了 58 个不同领域的真实数据集，每种方法运行 30 次，总计进行了数千次独立实验。
*   **充分性**：实验设计非常充分。除了主实验外，还包含了：
    *   **消融实验**：验证了流形侵入检测和噪声估计策略的有效性。
    *   **参数敏感性分析**：分析了 Mixup 中的 $\alpha$ 参数、采样范围 $\gamma$ 等对结果的影响。
    *   **标签噪声实验**：验证了在标签含有噪声的极端情况下算法的鲁棒性。
*   **公平性**：所有基准算法均在相同的 GP 框架下实现，并进行了超参数网格搜索优化，确保了对比的客观性。

### 6. 主要结论与发现
*   **VJG 优于有限差分**：在控制过拟合方面，最小化 Jensen 间隙比简单的有限差分（噪声扰动）更有效。
*   **泛化性能卓越**：VJM-GP 在 58 个数据集中的 36 个上显著优于标准 GP，且在与 XGBoost 等强力机器学习算法的对比中表现出更强的竞争力，尤其是在小样本场景下。
*   **复杂度与规模的关系**：实验发现，降低“功能复杂度”（通过 VJG）比单纯减小“模型大小”（通过节点数限制）更能提升泛化能力。
*   **可解释性**：可视化结果表明，VJM-GP 构建的特征更符合领域常识（如正相关关系），而标准 GP 容易捕捉到无意义的震荡噪声。

### 7. 优点
*   **理论支撑强**：将深度学习中的 VRM 理论成功引入演化计算，并给出了严谨的数学分解证明。
*   **动态自适应**：提出的噪声估计策略解决了不同数据集对正则化强度需求不同的痛点。
*   **鲁棒性高**：流形侵入检测机制有效解决了数据增强带来的潜在负面影响。

### 8. 不足与局限
*   **计算成本**：尽管有加速策略，但 VJM-GP 的计算开销仍显著高于传统的 parsimony pressure 方法，在大规模数据集上可能面临扩展性挑战。
*   **超参数依赖**：虽然对部分参数不敏感，但噪声估计中的阈值和流形检测中的边际参数 $\mu$ 仍可能影响特定任务的表现。
*   **应用范围**：目前主要针对回归任务，在分类任务或其他复杂演化任务（如强化学习策略演化）中的表现尚待验证。

（完）
