Title: Artificial Intelligence and Symmetries: Learning, Encoding, and Discovering Structure in Physical Data

URL Source: https://arxiv.org/pdf/2602.02351v1

Published Time: Tue, 03 Feb 2026 04:08:29 GMT

Number of Pages: 27

Markdown Content:
Prepared for submission to JHEP 

Artificial Intelligence and Symmetries: Learning, Encoding, and Discovering Structure in Physical Data 

Veronica Sanz 1

Instituto de Física Corpuscular (IFIC), Universitat de València–CSIC, E-46980 València, Spain 

Abstract: Symmetries play a central role in physics, organizing dynamics, constrain-ing interactions, and determining the effective number of physical degrees of freedom. In parallel, modern artificial intelligence methods have demonstrated a remarkable ability to extract low-dimensional structure from high-dimensional data through representation learn-ing. This review examines the interplay between these two perspectives, focusing on the extent to which symmetry-induced constraints can be identified, encoded, or diagnosed using machine learning techniques. Rather than emphasizing architectures that enforce known symmetries by construction, we concentrate on data-driven approaches and latent representation learning, with partic-ular attention to variational autoencoders. We discuss how symmetries and conservation laws reduce the intrinsic dimensionality of physical datasets, and how this reduction may manifest itself through self-organization of latent spaces in generative models trained to balance reconstruction and compression. We review recent results, including case studies from simple geometric systems and particle physics processes, and analyze the theoretical and practical limitations of inferring symmetry structure without explicit inductive bias. 

This manuscript is an invited review at the International Journal of Modern Physics A. 

Keywords: Symmetries, Machine Learning, Representation Learning, Variational Au-toencoders, Physics-informed AI 

> 1email: veronica.sanz@uv.es
> arXiv:2602.02351v1 [hep-ph] 2 Feb 2026

Contents 

1 Introduction 12 Symmetries as Information and Constraints 33 Symmetry in Machine Learning: Three Paradigms 4

3.1 Architectural Symmetry: Invariance and Equivariance 53.2 Implicit Symmetry via Data Augmentation and Self-Supervision 63.3 Emergent Structure in Latent Representations 6

4 Variational Autoencoders as Probes of Symmetry 7

4.1 Compression, Reconstruction, and Latent Variables 74.2 Why VAEs Are Different from Other Dimensionality Reduction Methods 84.3 Relation to Disentanglement and Its Limitations 84.4 Symmetry as Latent-Space Self-Organization 9

5 Latent-Space Self-Organization and Relevance Measures 9

5.1 Self-Organization in Overparameterized Latent Spaces 10 5.2 Relevance as a Quantitative Diagnostic 10 5.3 Effective Dimensionality and Hierarchies 11 5.4 Toy Example: Exact Constraint and Dimensional Reduction 11 5.5 Correlations with Physical Observables 11 5.6 Robustness and Limitations 12 

6 Case Studies from Physics 12 

6.1 Geometric Constraints and Exact Continuous Symmetries 12 6.2 Lepton Collisions and Momentum Conservation 14 6.3 Hadron Collisions and Approximate Symmetries 15 6.4 Lessons from the Case Studies 16 

7 Theoretical Perspectives on Latent–Symmetry Alignment 16 

7.1 Symmetry, Redundancy, and Information Compression 17 7.2 Linear Limits and the Connection to PCA 17 7.3 Nonlinear Manifolds and Local Coordinate Choices 18 7.4 Relation to the Information Bottleneck Principle 18 7.5 Limits of Generality and Absence of Guarantees 18 

8 Comparison with Alternative Approaches 19 

8.1 Linear Dimensionality Reduction: PCA and Related Methods 19 8.2 Deterministic Autoencoders 19 8.3 Disentanglement-Oriented Variational Models 20 8.4 Equivariant and Group-Structured Models 20 – i – 8.5 Normalizing Flows and Diffusion Models 20 8.6 Summary of Comparative Features 21 

9 Outlook and Open Problems 21 

9.1 Approximate, Broken, and Emergent Symmetries 21 9.2 Unknown Symmetries and Exploratory Diagnostics 22 9.3 Connections to Effective Field Theory and Model Building 22 9.4 Architectural Biases and Hybrid Approaches 22 9.5 From Diagnostics to Practice 22 

10 Conclusions 23 

1 Introduction 

Symmetries play a central role in modern theoretical physics. They organize physical laws, constrain dynamics, and determine which degrees of freedom are physically relevant. From Noether’s theorem, which links continuous symmetries to conservation laws, to gauge in-variance and spacetime symmetries in quantum field theory, much of our understanding of fundamental interactions can be traced back to symmetry principles [1–3]. In this sense, symmetries are not merely aesthetic features of physical theories, but constitute a form of information compression: they encode redundancy in the description of physical systems and identify equivalence classes of configurations that are physically indistinguishable. Traditionally, symmetry considerations enter physics through analytic reasoning and model building. One postulates a symmetry group, constrains the allowed operators or dynamics accordingly, and confronts the resulting theory with data. This approach has been extraordinarily successful, yet it relies on a crucial assumption: that the relevant symmetries are known a priori . In complex systems, high-dimensional datasets, or situations where symmetries are only approximate, emergent, or partially broken, this assumption may no longer be justified. This raises a natural question: can symmetries be inferred directly from data, without being imposed by hand? 

Recent advances in artificial intelligence, and in particular in representation learning, have brought renewed attention to this question. Modern machine learning models are capable of extracting low-dimensional structure from extremely high-dimensional data, of-ten discovering latent representations that are more compact, structured, and interpretable than the original input space [4, 5]. In scientific contexts, this ability has been exploited for pattern recognition, acceleration of simulations, and surrogate modeling. More sub-tly, it has also opened the possibility that learning algorithms may act as diagnostic tools ,revealing hidden constraints, redundancies, or organizing principles in data. The relationship between machine learning and symmetry has so far developed along three partially distinct directions. First, known symmetries can be explicitly enforced at the architectural level. Convolutional neural networks exploit translational invariance through – 1 – weight sharing [6], while more general group-equivariant and geometric deep learning frame-works encode rotational, permutation, Lorentz, or gauge symmetries directly into network layers [7, 8]. When the symmetry group and its action on the data are known, this approach yields clear benefits in sample efficiency, generalization, and interpretability. Second, symmetries can be encouraged indirectly through data augmentation or self-supervised objectives. By exposing a model to symmetry-related versions of the same data point—rotations, boosts, or other transformations—one can bias the learned representation toward invariance or equivariance, even if this is not enforced exactly by the architecture [9, 10]. While powerful, this strategy still presupposes some knowledge of the relevant transformations. A third, more exploratory direction asks whether symmetry-related structure can emerge spontaneously in the internal representations of learning algorithms, even when no symme-try is imposed or labeled. Early evidence for this phenomenon appeared in studies of disentangled representation learning, where unsupervised generative models were shown to align latent variables with human-interpretable factors of variation such as position, ori-entation, or scale [11, 12]. Subsequent work, however, clarified that purely unsupervised disentanglement is not identifiable in general, and cannot be guaranteed without inductive biases or supervision [13]. This result underscores the need for caution: emergent structure in latent space should be treated as a diagnostic, not as a proof of underlying physical factors. From the perspective of physics, this diagnostic viewpoint is nonetheless extremely ap-pealing. Physical symmetries imply constraints and redundancies that reduce the effective dimensionality of observable data. A learning algorithm trained to compress and recon-struct data efficiently should therefore be incentivized to allocate information preferentially along symmetry-independent directions, while suppressing redundant ones. If this intuition is correct, the organization of latent representations may provide a data-driven window into the symmetry structure of the underlying system, even when that structure is only approximate or partially obscured. The purpose of this review is to critically examine this idea. Rather than focus-ing on architectures that enforce symmetry by construction, we concentrate on standard representation-learning models and ask under which conditions symmetry-induced struc-ture manifests itself in their latent spaces. Particular emphasis is placed on variational autoencoders (VAEs), which combine nonlinear dimensional reduction with a well-defined probabilistic notion of compression [14]. We review how symmetries, constraints, and con-servation laws can lead to latent-space self-organization, and how this organization can be quantified and interpreted. This article is based on a sequence of recent works [15–17] and invited talks, but its scope is deliberately broader. Our goal is not to advocate a specific algorithm as a universal symmetry detector, but to clarify what can—and cannot—be inferred from data-driven latent representations. Throughout, we emphasize limitations as much as successes, and we frame machine learning as a complementary tool to traditional theoretical reasoning, rather than as a replacement for it. – 2 – Data space 

> Redundant

coordinates 

> Constraint manifold
> Symmetry

orbits  

> z1
> z2z3
> Latent space
> Effective

degrees of freedom 

> Figure 1 . Schematic view of the role of symmetries in data-driven representation learning. Phys-ical symmetries induce constraints and redundancies in the data manifold, reducing the effective number of independent degrees of freedom. Representation-learning models trained to balance re-construction and compression may reflect this structure through self-organization of their latent spaces.

2 Symmetries as Information and Constraints 

In physics, symmetries are not merely transformations under which equations remain invari-ant; they encode fundamental information about what distinguishes physically inequivalent states from redundant descriptions. Two configurations related by a symmetry transforma-tion correspond to the same physical situation, even though they may differ at the level of coordinates or fields. In this sense, symmetries identify equivalence classes in the space of descriptions and thereby reduce the effective number of independent degrees of freedom. This observation can be formalized in a variety of familiar contexts. In classical me-chanics, invariance under spatial translations implies conservation of momentum, while ro-tational invariance implies conservation of angular momentum, as formalized by Noether’s theorem [1]. In quantum field theory, internal symmetries constrain the allowed interactions and operator content of a theory, while spacetime symmetries govern dispersion relations, selection rules, and the structure of scattering amplitudes [2, 3]. In all these cases, symme-try reduces arbitrariness: it forbids certain variations while identifying others as physically irrelevant. From an information-theoretic perspective, this role of symmetry can be viewed as a form of redundancy removal. If a dataset is generated by a physical process with an exact symmetry, then multiple data points related by that symmetry carry no additional inde-pendent information. The true information content of the dataset resides in variables that parametrize inequivalent symmetry orbits, rather than in the raw coordinates themselves. The dimension of this reduced space is often referred to as the intrinsic dimensionality of the system. A simple geometric example illustrates this point. Consider data points distributed on a circle embedded in a two-dimensional space. Although each data point is specified by two coordinates, the physical configuration is fully described by a single angular variable. The continuous rotational symmetry identifies all points related by a global rotation as equivalent up to that angle. The constraint defining the circle, together with the symmetry, – 3 – reduces the effective degrees of freedom from two to one. Similar reasoning applies to more complex manifolds, group orbits, and constrained dynamical systems. In many physical systems, symmetries manifest themselves not only through invariance but also through explicit constraints. Conservation laws impose relations among observ-ables that must hold event by event, reducing the dimensionality of the space of allowed configurations. In particle collisions, for instance, energy–momentum conservation relates the momenta of final-state particles, while on-shell conditions impose additional constraints [18]. These relations define a lower-dimensional manifold within the space of all kinemati-cally allowed variables. Importantly, not all symmetries encountered in practice are exact. Approximate sym-metries, emergent symmetries, and explicitly broken symmetries are ubiquitous in physics. Chiral symmetry in QCD, Lorentz symmetry in effective theories with cutoffs, or approxi-mate conservation laws arising from scale separation all provide examples where symmetry holds only within a limited regime of validity [19]. In such cases, symmetry-induced con-straints still shape the structure of the data, but in a softened or distorted manner. The effective dimensionality may be reduced only approximately, and deviations from exact constraints carry physically meaningful information. This distinction is crucial when considering data-driven analyses. A learning algorithm trained on data generated by a symmetric system is not exposed to the abstract symmetry group itself, but only to its concrete consequences: correlations, constraints, and redundan-cies among observables. From this viewpoint, detecting symmetry amounts to detecting a structured reduction of information. Exact symmetries correspond to sharp constraints and clear dimensional reduction, while approximate symmetries manifest as hierarchies, soft modes, or preferred directions in the space of variations. The central theme of this review is that these information-theoretic signatures of symmetry can, under suitable conditions, be reflected in learned representations of data. Representation-learning models trained to balance fidelity and compression are naturally sensitive to redundancy. If a dataset contains symmetry-related degeneracies, an efficient representation should suppress redundant directions and allocate capacity to variables that parametrize physically distinct configurations. The extent to which this happens, and how reliably it can be diagnosed, is the subject of the sections that follow. 

3 Symmetry in Machine Learning: Three Paradigms 

The interaction between symmetry and machine learning has developed along several con-ceptually distinct lines. While these approaches are often grouped together under the broad label of “symmetry-aware learning”, they differ substantially in their assumptions, objec-tives, and epistemic status. In this section, we organize the existing literature into three paradigms, ordered by the degree to which symmetry is specified a priori . This classification will be useful for clarifying what can reasonably be expected from data-driven symmetry diagnostics in later sections. – 4 – Figure 2 . Illustration of symmetry as information reduction. A high-dimensional space of de-scriptions (top) contains redundancy due to symmetry transformations, which identify equivalence classes of physically identical configurations. Constraints and conservation laws restrict the data to a lower-dimensional manifold, while symmetry further quotients this manifold into physically inequivalent degrees of freedom (bottom). Figure generated with AI. 

3.1 Architectural Symmetry: Invariance and Equivariance 

The most direct way to incorporate symmetry into a learning model is to encode it explicitly at the architectural level. This approach mirrors the traditional theoretical strategy in physics: one identifies the relevant symmetry group and restricts the space of allowed functions accordingly. In machine learning, this idea appears as invariance or equivariance under group actions. The canonical example is the convolutional neural network (CNN), where translational invariance is implemented through weight sharing and local connectivity [6]. This principle has since been generalized to arbitrary groups, leading to group-equivariant convolutional networks and, more broadly, to the field of geometric deep learning [7, 8]. In particle physics, these ideas have been extended to permutation-invariant architectures for sets of particles, Lorentz- equivariant networks, and models respecting gauge or spacetime symmetries [20, 21]. When the symmetry group and its action on the data are known, architectural symme-try provides strong guarantees. The hypothesis space is restricted to symmetry-consistent functions, reducing sample complexity and improving generalization. From a physics per-spective, this approach is conceptually clean: the symmetry principle is imposed before learning, much like in effective field theory or model building. However, this strength is also its main limitation. Architectural symmetry requires 

prior knowledge of the relevant group, its representation, and how it acts on the data. It is therefore ill-suited to situations where symmetries are unknown, approximate, emergent, or explicitly broken, or where the correct variables on which the symmetry acts are themselves – 5 – unclear. In such cases, enforcing an incorrect symmetry can bias the model and obscure physically relevant structure. 

3.2 Implicit Symmetry via Data Augmentation and Self-Supervision 

A second paradigm incorporates symmetry indirectly, without enforcing it exactly at the level of the architecture. Instead, symmetry is introduced through the training procedure, most commonly via data augmentation or self-supervised objectives. The model is exposed to multiple transformed versions of the same data point and is encouraged to produce similar representations for all of them. This strategy has been particularly influential in representation learning for images and signals, where contrastive and self-supervised methods use augmentations such as transla-tions, rotations, or color transformations to learn invariant features [9, 10]. In physics-inspired applications, analogous ideas appear in the use of Lorentz boosts, rotations, or permutations as augmentation strategies [22, 23]. Implicit symmetry has two notable advantages. First, it is flexible: approximate or domain-specific symmetries can be incorporated without requiring an exact group-theoretic formulation. Second, it allows the same architecture to be reused across different symme-try assumptions. However, this flexibility comes at the cost of reduced guarantees. The learned invariance is only as good as the augmentation scheme, and different choices of transformations can lead to qualitatively different representations. From a conceptual standpoint, this paradigm still relies on prior knowledge of symme-try. The transformations used for augmentation are chosen by the practitioner, and the model is guided toward invariance by construction. As such, implicit symmetry learning does not constitute symmetry discovery, but rather symmetry enforcement through training .

3.3 Emergent Structure in Latent Representations 

The third paradigm is more exploratory and is the primary focus of this review. Here, symmetry is neither imposed architecturally nor encoded explicitly through data augmen-tation. Instead, one asks whether symmetry-related structure can emerge spontaneously in the internal representations of learning algorithms trained on raw data. This question arose prominently in the context of unsupervised and weakly supervised representation learning. Early work on disentangled representations suggested that latent variables in generative models could align with interpretable factors of variation, such as position, orientation, or scale, without explicit supervision [11, 12]. Subsequent analyses, however, demonstrated that such alignment is not identifiable in general and cannot be guaranteed without inductive biases or supervision [13]. These results impose a fundamen-tal limitation: emergent structure in latent space must be interpreted diagnostically, not ontologically. Despite this limitation, emergent latent organization remains of significant interest for physics. Physical symmetries do not merely correspond to abstract transformations; they impose concrete constraints and redundancies in observable data. A learning algorithm trained to compress data efficiently is therefore exposed to the consequences of symmetry, even if it is ignorant of the symmetry itself. If the data contain exact or approximate – 6 – Figure 3 . Three paradigms for incorporating symmetry in machine learning. Left : Architectural symmetry, where invariance or equivariance is imposed by construction. Center : Implicit symmetry, introduced through data augmentation or self-supervised objectives. Right : Emergent structure, where symmetry-related organization may arise in latent representations without explicit enforce-ment, and is interpreted diagnostically. Figure generated with AI. 

degeneracies, an efficient representation may suppress redundant directions and allocate representational capacity to symmetry-independent degrees of freedom. In this paradigm, the goal is not to recover a symmetry group or its generators explicitly, but to detect structured reductions of dimensionality, hierarchies of relevance, or preferred directions in representation space. These features can serve as indicators of underlying constraints or approximate symmetries, provided their limitations are clearly understood. The remainder of this review is devoted to examining this diagnostic approach in detail, with particular emphasis on variational autoencoders as a concrete and interpretable framework. 

4 Variational Autoencoders as Probes of Symmetry 

The previous sections emphasized that physical symmetries manifest themselves as redun-dancies, constraints, and reductions in effective dimensionality of observable data. If sym-metry is to be diagnosed directly from data, rather than imposed by construction, the learning algorithm must satisfy two basic requirements. First, it must learn a compressed representation that reflects the intrinsic structure of the data, rather than simply mem-orizing it. Second, it must provide a principled way of quantifying which directions in representation space are meaningfully used and which are redundant. In this section we argue that variational autoencoders (VAEs) provide a natural framework for addressing these requirements. 

4.1 Compression, Reconstruction, and Latent Variables 

A variational autoencoder is a generative model that learns a probabilistic mapping between observed data x and a set of latent variables z, trained by maximizing the evidence lower bound (ELBO) [14]. The objective balances two competing goals: accurate reconstruction of the data and compression of the latent representation toward a simple prior distribution. Schematically, 

LVAE = Eq(z|x)[log p(x|z)] − β D KL (q(z|x) ∥ p(z)) , (4.1) where q(z|x) is the encoder, p(x|z) the decoder, and p(z) a chosen prior, typically a factor-ized Gaussian. – 7 – From the perspective of symmetry diagnostics, the crucial feature of this objective is not its generative nature per se, but the explicit tension between reconstruction fidelity and information compression. Directions in latent space that carry little information about the data are penalized by the Kullback–Leibler term and driven toward the prior. Conversely, latent directions that encode genuine variation across the dataset are retained. This mecha-nism naturally mirrors the effect of symmetry-induced redundancy: if multiple data points are related by constraints or symmetries, efficiently encoding all of them requires fewer independent latent variables. 

4.2 Why VAEs Are Different from Other Dimensionality Reduction Methods 

At first sight, this logic resembles classical dimensionality reduction techniques such as principal component analysis (PCA). Indeed, in linear settings, autoencoders and PCA are closely related [24]. However, several features distinguish VAEs as probes of symmetry-induced structure. First, VAEs operate in a nonlinear regime. Physical constraints and symmetry orbits often define curved, non-Euclidean manifolds embedded in high-dimensional spaces. Linear methods may detect reduced rank locally, but they generally fail to capture global structure. VAEs can, in principle, learn nonlinear coordinate systems adapted to such manifolds. Second, VAEs provide an explicit probabilistic latent representation. Each latent vari-able is characterized not only by a mean value, but also by an event-wise uncertainty. This distinction between variation across the dataset and uncertainty within a single data point plays a central role in assessing which latent directions are genuinely informative. Deter-ministic autoencoders lack a canonical notion of latent scale or relevance, making post-hoc interpretation ambiguous. Third, the prior imposed on the latent variables provides a reference against which information content can be measured. Latent directions that are unused collapse to the prior in a controlled way, rather than remaining arbitrarily scaled or entangled. This property is essential for defining meaningful diagnostics of effective dimensionality and latent-space organization. 

4.3 Relation to Disentanglement and Its Limitations 

Variational autoencoders have often been discussed in the context of disentangled represen-tation learning. Modifications of the standard objective, such as the β-VAE, FactorVAE, or 

β-TCVAE, explicitly encourage statistical independence among latent variables [11, 25, 26]. While these approaches can promote alignment between latent variables and independent factors of variation, it is now well established that purely unsupervised disentanglement is not identifiable without inductive bias or supervision [13]. For the purposes of symmetry diagnostics, this result has an important implication. Alignment of latent variables with symmetry directions should not be interpreted as the recovery of a unique or physically privileged coordinate system. Instead, it should be under-stood as evidence that the data admit a reduced and structured representation, consistent with symmetry-induced redundancy. The goal is therefore not disentanglement in the strong sense, but the identification of robust hierarchies or reductions in latent space usage. – 8 – Figure 4 . Conceptual view of a variational autoencoder as a symmetry probe. Symmetry-induced constraints restrict the data to a lower-dimensional manifold. During training, the VAE balances reconstruction accuracy against compression toward a simple latent prior, leading to suppression of redundant latent directions and preferential use of symmetry- independent degrees of freedom. 

4.4 Symmetry as Latent-Space Self-Organization 

When a VAE is trained on data generated by a physical system with exact or approximate symmetries, it is exposed only to the observable consequences of those symmetries: reduced degrees of freedom, correlations among variables, and constraints that hold across events. If the model successfully balances reconstruction and compression, these features can manifest themselves as a self-organization of the latent space. In particular, one may observe that only a subset of latent variables carries significant variation across the dataset, while the remaining directions are suppressed toward the prior. The number of such relevant latent directions provides a data-driven estimate of the effective dimensionality of the system. Moreover, correlations between latent variables and specific combinations of input features can reveal how the model internally encodes constraints such as conservation laws. Crucially, this behavior is not guaranteed. It depends on the strength and clarity of the symmetry-induced redundancy, the choice of architecture and hyperparameters, and the degree to which the symmetry is exact or approximate. For this reason, VAEs should be viewed as probes of symmetry rather than symmetry detectors. They provide a way to test whether the data themselves support a reduced, structured representation, without imposing that structure by hand. The remainder of this review is devoted to making this diagnostic viewpoint concrete. In the next section, we introduce quantitative measures of latent relevance and illustrate how latent-space self-organization emerges in simple toy models and in realistic examples from particle physics. 

5 Latent-Space Self-Organization and Relevance Measures 

A recurring theme in the preceding sections is that physical symmetries manifest them-selves as redundancies and constraints in observable data. When a representation- learning model is trained to compress such data efficiently, these redundancies may translate into a nontrivial organization of the latent space. In this section, we make this statement precise – 9 – by introducing quantitative diagnostics that allow one to assess how many latent directions are effectively used, and how this usage reflects symmetry- induced structure. 

5.1 Self-Organization in Overparameterized Latent Spaces 

Throughout this review, we focus on deliberately overparameterized latent spaces. That is, the dimensionality of the latent space is chosen to be equal to or larger than the expected intrinsic dimensionality of the data. This choice is essential: if the latent dimension were fixed to the minimal value from the outset, any apparent dimensional reduction would be imposed rather than learned. Let z = ( z1, . . . , z dz ) denote the latent variables of a variational autoencoder trained on a dataset {x(i)}Ni=1 . For each data point, the encoder defines a posterior distribution 

q(z|x(i)) = 

> dz

Y

> j=1

N



zj | μ(i) 

> j

, σ (i)2 

> j



, (5.1) where μ(i) 

> j

and σ(i) 

> j

are the mean and standard deviation of the j-th latent variable for event 

i.If the data contain symmetry-induced redundancies, efficient compression suggests that only a subset of latent directions should encode significant variation across the dataset. The remaining directions should be suppressed toward the prior distribution, carrying little or no information about x. This separation between informative and redundant latent variables is what we refer to as latent-space self-organization .

5.2 Relevance as a Quantitative Diagnostic 

To quantify this effect, one requires a measure that distinguishes variation across events from uncertainty within a single event. A natural diagnostic exploits the probabilistic structure of the VAE latent space and compares the spread of latent means across the dataset to the typical posterior uncertainty. For each latent variable zj , we define a relevance measure 

ρj =std i



μ(i)

> j

D

σ(i)

> j

E

> i

, (5.2) where std i denotes the standard deviation over the dataset and ⟨·⟩ i denotes the average over events. This ratio has a simple interpretation. The numerator measures how strongly the latent coordinate zj varies across different data points, while the denominator measures the typical uncertainty associated with that coordinate for a single data point. A large value of ρj indicates that the latent variable captures structured, event-to-event variation that exceeds its intrinsic noise, and is therefore relevant for representing the dataset. Conversely, values ρj ≲ O(1) indicate latent variables whose variation is comparable to or smaller than their uncertainty, suggesting that they carry little meaningful information. – 10 – Importantly, this definition does not assume statistical independence of latent variables and does not enforce disentanglement. It is therefore well suited to a diagnostic setting, where the goal is to assess effective dimensionality rather than to recover a unique set of generative factors. 

5.3 Effective Dimensionality and Hierarchies 

Ordering the latent variables by decreasing relevance, 

ρ1 ≥ ρ2 ≥ · · · ≥ ρdz , (5.3) often reveals a pronounced hierarchy. In datasets without strong constraints, several latent variables may exhibit comparable relevance, reflecting multiple independent degrees of free-dom. In contrast, datasets subject to symmetry constraints frequently show a sharp drop in ρj after a small number of dominant directions. This hierarchy provides a data-driven estimate of the effective dimensionality of the system. While the precise numerical value of ρj depends on architecture and hyperparam-eters, the ordering and separation between relevant and irrelevant directions are typically robust across training runs. As such, the relevance spectrum serves as an order parameter for symmetry-induced dimensional reduction. 

5.4 Toy Example: Exact Constraint and Dimensional Reduction 

The logic above can be illustrated with a simple example. Consider a dataset of points embedded in R2, subject either to no constraint or to an exact constraint x21 + x22 = R2.In the unconstrained case, the data have two independent degrees of freedom, and a VAE trained with dz ≥ 2 typically exhibits two latent variables with comparable relevance. In the constrained case, the data lie on a one-dimensional manifold, and only a single latent direction remains strongly relevant, while the others are suppressed. Beyond the relevance spectrum, latent-space organization can be visualized by pro-jecting the mean latent activations ⟨zj ⟩ onto the input space. In symmetry- constrained datasets, the most relevant latent variable often parametrizes the symmetry orbit itself, providing a smooth coordinate along the data manifold. 

5.5 Correlations with Physical Observables 

A further layer of interpretation is obtained by studying correlations between relevant latent variables and physically meaningful combinations of input features. If symmetry constraints enforce relations such as conservation laws, the dominant latent directions often correlate with the independent combinations left unconstrained. For example, in particle collision data, momentum conservation relates the momenta of final-state particles, reducing the number of independent kinematic variables. When a VAE is trained on such data, the most relevant latent variables frequently correlate with differences or invariant combinations of momenta, while directions corresponding to redundant information are suppressed. – 11 – These correlations should not be interpreted as a unique or canonical identification of physical coordinates. Rather, they provide evidence that the latent representation has internalized the constraint structure of the data in a nontrivial way. 

5.6 Robustness and Limitations 

The relevance-based diagnostics described above are empirical and subject to limitations. They depend on successful training, sufficient data coverage of the underlying manifold, and a clear separation between symmetry-induced redundancy and noise. Approximate or softly broken symmetries may lead to less pronounced hierarchies, while finite-sample effects can blur the relevance spectrum. Nevertheless, when applied judiciously, these diagnostics offer a practical and inter-pretable way to assess whether high-dimensional datasets admit a reduced latent description consistent with underlying symmetries. In the following sections, we apply this framework to progressively more realistic physical systems, illustrating how exact and approximate symmetries manifest themselves in latent-space organization. 

6 Case Studies from Physics 

In this section we illustrate how the diagnostic framework introduced above manifests itself in concrete physical systems, following closely the discussion in Ref. [15]. The goal is not to demonstrate optimal performance on specific datasets, but to examine how exact and approximate symmetries shape the organization of latent representations when a variational autoencoder is trained on physically motivated data. We proceed from simple geometric examples to realistic scattering processes, increasing the level of complexity and realism at each step. 

6.1 Geometric Constraints and Exact Continuous Symmetries 

We begin with simple geometric datasets that provide a controlled environment in which the role of symmetry is transparent. Consider data points embedded in a two-dimensional space, generated either without constraints or subject to the exact constraint x21 + x22 = R2.In the unconstrained case, the data possess two independent degrees of freedom. In the constrained case, the data lie on a one-dimensional manifold, invariant under continuous rotations. When a variational autoencoder is trained on these datasets with a latent dimension 

dz ≥ 2, a clear distinction emerges. For the unconstrained dataset, two latent variables exhibit comparable relevance, reflecting the absence of redundancy. In contrast, for the constrained dataset, the relevance spectrum shows a pronounced hierarchy, with a single dominant latent direction and the remaining ones strongly suppressed. This is shown in the upper panel of Fig. 5. This behavior is stable across independent training runs and choices of initial conditions. Beyond the relevance hierarchy, the structure of the dominant latent variable provides additional insight. Projecting the mean latent activation onto the input space reveals that this variable parametrizes the angular coordinate along the circle, providing a smooth – 12 – Relevance plot (1D vs 2D case) 

> Relevance

<latexit sha1_base64="8RlkLIL/xEmGZVmuARtU0/6EM6U=">AAAB5HicbZDNTgIxFIXv4B/iH+rSTSMxcUVmjAGXRDcuMcpPAhPSKXegodOZtB0TMuENdGXUnU/kC/g2FpyFgmf19Z7T5J4bJIJr47pfTmFtfWNzq7hd2tnd2z8oHx61dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5mfudR1Sax/LBTBP0IzqSPOSMGju6lwNvUK64VXchsgpeDhXI1RyUP/vDmKURSsME1brnuYnxM6oMZwJnpX6qMaFsQkfYsyhphNrPFqvOyFkYK2LGSBbv39mMRlpPo8BmImrGetmbD//zeqkJr/yMyyQ1KJmNWC9MBTExmTcmQ66QGTG1QJnidkvCxlRRZuxdSra+t1x2FdoXVa9Wrd1dVhrX+SGKcAKncA4e1KEBt9CEFjAYwTO8wbsTOk/Oi/P6Ey04+Z9j+CPn4xvxx4s2</latexit> 

n1 <latexit sha1_base64="PBLC5YjpWLaYHHgzAAzgP+isAt4=">AAAB5HicbZDNTgIxFIXv4B/iH+rSTSMxcUVmiEGXRDcuMcpPAhPSKXegodOZtB0TMuENdGXUnU/kC/g2FpyFgmf19Z7T5J4bJIJr47pfTmFtfWNzq7hd2tnd2z8oHx61dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5mfudR1Sax/LBTBP0IzqSPOSMGju6l4PaoFxxq+5CZBW8HCqQqzkof/aHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ1HSCLWfLVadkbMwVsSMkSzev7MZjbSeRoHNRNSM9bI3H/7n9VITXvkZl0lqUDIbsV6YCmJiMm9MhlwhM2JqgTLF7ZaEjamizNi7lGx9b7nsKrRrVa9erd9dVBrX+SGKcAKncA4eXEIDbqEJLWAwgmd4g3cndJ6cF+f1J1pw8j/H8EfOxzfzRYs3</latexit> 

n2 <latexit sha1_base64="Uw1qFuEkSap3MbKTkQvyGGxmiOs=">AAAB5HicbZDNTgIxFIXv4B/iH+rSTSMxcUVm1KBLohuXGOUngQnplDvQ0OlM2o4JmfAGujLqzifyBXwbC85CwbP6es9pcs8NEsG1cd0vp7Cyura+UdwsbW3v7O6V9w9aOk4VwyaLRaw6AdUouMSm4UZgJ1FIo0BgOxjfzPz2IyrNY/lgJgn6ER1KHnJGjR3dy/55v1xxq+5cZBm8HCqQq9Evf/YGMUsjlIYJqnXXcxPjZ1QZzgROS71UY0LZmA6xa1HSCLWfzVedkpMwVsSMkMzfv7MZjbSeRIHNRNSM9KI3G/7ndVMTXvkZl0lqUDIbsV6YCmJiMmtMBlwhM2JigTLF7ZaEjaiizNi7lGx9b7HsMrTOql6tWru7qNSv80MU4QiO4RQ8uIQ63EIDmsBgCM/wBu9O6Dw5L87rT7Tg5H8O4Y+cj2/0w4s4</latexit> 

n3 <latexit sha1_base64="LVaVGu7RF7qtfzDb612uHzbm2Cs=">AAAB5XicbZDNTsJAFIVv8Q/xD3XpZiIxcUVaQ8Al0Y1LTOQngYZMh1s6YTptZqYmpOERdGXUnS/kC/g2ttiFgmf1zT1nknuuFwuujW1/WaWNza3tnfJuZW//4PCoenzS01GiGHZZJCI18KhGwSV2DTcCB7FCGnoC+97sNvf7j6g0j+SDmcfohnQquc8ZNflIjhuVcbVm1+2lyDo4BdSgUGdc/RxNIpaEKA0TVOuhY8fGTakynAlcVEaJxpiyGZ3iMENJQ9Ruutx1QS78SBETIFm+f2dTGmo9D70sE1IT6FUvH/7nDRPjX7spl3FiULIsknl+IoiJSF6ZTLhCZsQ8A8oUz7YkLKCKMpMdJq/vrJZdh95V3WnWm/eNWvumOEQZzuAcLsGBFrThDjrQBQYBPMMbvFtT68l6sV5/oiWr+HMKf2R9fAMrH4tN</latexit> 

n4<latexit sha1_base64="CxthGmbB2m0jnNoC6ApIHf6qhTI=">AAAB5XicbZDNTsJAFIVv8Q/xD3XpZiIxcUVaY9Al0Y1LTARJoCHT4ZZOmP5k5tZICI+gK6PufCFfwLexxS4UPKtv7jmT3HO9RElDtv1llVZW19Y3ypuVre2d3b3q/kHHxKkW2BaxinXX4waVjLBNkhR2E4089BTee+Pr3L9/QG1kHN3RJEE35KNI+lJwykePA6cyqNbsuj0XWwangBoUag2qn/1hLNIQIxKKG9Nz7ITcKdckhcJZpZ8aTLgY8xH2Mox4iMadznedsRM/1owCZPP37+yUh8ZMQi/LhJwCs+jlw/+8Xkr+pTuVUZISRiKLZJ6fKkYxyyuzodQoSE0y4ELLbEsmAq65oOwweX1nsewydM7qTqPeuD2vNa+KQ5ThCI7hFBy4gCbcQAvaICCAZ3iDd2tkPVkv1utPtGQVfw7hj6yPbzWsi1Q=</latexit> 

x1

<latexit sha1_base64="WHPMd7JF4dlm7ImHUUGXNOmpu6Q=">AAAB5XicbZDNTsJAFIVv8Q/xD3XpZiIxcUVaYtAl0Y1LTOQngYZMh1s6YaZtZqZG0vAIujLqzhfyBXwbW+xCwbP65p4zyT3XiwXXxra/rNLa+sbmVnm7srO7t39QPTzq6ihRDDssEpHqe1Sj4CF2DDcC+7FCKj2BPW96k/u9B1SaR+G9mcXoSjoJuc8ZNfnocdSojKo1u24vRFbBKaAGhdqj6udwHLFEYmiYoFoPHDs2bkqV4UzgvDJMNMaUTekEBxmGVKJ208Wuc3LmR4qYAMni/TubUqn1THpZRlIT6GUvH/7nDRLjX7kpD+PEYMiySOb5iSAmInllMuYKmRGzDChTPNuSsIAqykx2mLy+s1x2FbqNutOsN+8uaq3r4hBlOIFTOAcHLqEFt9CGDjAI4Bne4N2aWE/Wi/X6Ey1ZxZ9j+CPr4xs3K4tV</latexit> 

x2

> <latexit  sha1_base64="sgHuGHbrC9iMDerEkEBXitmwCMY=">AAACCnicbZC7TsNAEEXXPEN4BShpVkQIqshGKFBG0FAGiTyk2LLWm3Gyyvqh3TEiWPkD+BmoENBR8gP8DbZJAQlTnZ17V5p7vVgKjab5ZSwsLi2vrJbWyusbm1vblZ3dto4SxaHFIxmprsc0SBFCCwVK6MYKWOBJ6Hijy1zv3ILSIgpvcByDE7BBKHzBGWYrt3JkI9yhCtI2kwnQyKcTaksWDiTQe9eitiq47FaqZs0shs6DNYUqmU7TrXza/YgnAYTIJdO6Z5kxOilTKLiESdlONMSMj9gAehmGLADtpEWgCT30I0VxCLR4//amLNB6HHiZJ2A41LNavvxP6yXonzupCOMEIeSZJdP8RFKMaN4L7QsFHOU4A8aVyK6kfMgU45i1l8e3ZsPOQ/ukZtVr9evTauNiWkSJ7JMDckwsckYa5Io0SYtw8kieyRt5Nx6MJ+PFeP2xLhjTP3vkzxgf31b/mfM=</latexit>
> Value of  hz1i

<latexit sha1_base64="CxthGmbB2m0jnNoC6ApIHf6qhTI=">AAAB5XicbZDNTsJAFIVv8Q/xD3XpZiIxcUVaY9Al0Y1LTARJoCHT4ZZOmP5k5tZICI+gK6PufCFfwLexxS4UPKtv7jmT3HO9RElDtv1llVZW19Y3ypuVre2d3b3q/kHHxKkW2BaxinXX4waVjLBNkhR2E4089BTee+Pr3L9/QG1kHN3RJEE35KNI+lJwykePA6cyqNbsuj0XWwangBoUag2qn/1hLNIQIxKKG9Nz7ITcKdckhcJZpZ8aTLgY8xH2Mox4iMadznedsRM/1owCZPP37+yUh8ZMQi/LhJwCs+jlw/+8Xkr+pTuVUZISRiKLZJ6fKkYxyyuzodQoSE0y4ELLbEsmAq65oOwweX1nsewydM7qTqPeuD2vNa+KQ5ThCI7hFBy4gCbcQAvaICCAZ3iDd2tkPVkv1utPtGQVfw7hj6yPbzWsi1Q=</latexit> 

x1

<latexit sha1_base64="WHPMd7JF4dlm7ImHUUGXNOmpu6Q=">AAAB5XicbZDNTsJAFIVv8Q/xD3XpZiIxcUVaYtAl0Y1LTOQngYZMh1s6YaZtZqZG0vAIujLqzhfyBXwbW+xCwbP65p4zyT3XiwXXxra/rNLa+sbmVnm7srO7t39QPTzq6ihRDDssEpHqe1Sj4CF2DDcC+7FCKj2BPW96k/u9B1SaR+G9mcXoSjoJuc8ZNfnocdSojKo1u24vRFbBKaAGhdqj6udwHLFEYmiYoFoPHDs2bkqV4UzgvDJMNMaUTekEBxmGVKJ208Wuc3LmR4qYAMni/TubUqn1THpZRlIT6GUvH/7nDRLjX7kpD+PEYMiySOb5iSAmInllMuYKmRGzDChTPNuSsIAqykx2mLy+s1x2FbqNutOsN+8uaq3r4hBlOIFTOAcHLqEFt9CGDjAI4Bne4N2aWE/Wi/X6Ey1ZxZ9j+CPr4xs3K4tV</latexit> 

x2

> <latexit  sha1_base64="sgHuGHbrC9iMDerEkEBXitmwCMY=">AAACCnicbZC7TsNAEEXXPEN4BShpVkQIqshGKFBG0FAGiTyk2LLWm3Gyyvqh3TEiWPkD+BmoENBR8gP8DbZJAQlTnZ17V5p7vVgKjab5ZSwsLi2vrJbWyusbm1vblZ3dto4SxaHFIxmprsc0SBFCCwVK6MYKWOBJ6Hijy1zv3ILSIgpvcByDE7BBKHzBGWYrt3JkI9yhCtI2kwnQyKcTaksWDiTQe9eitiq47FaqZs0shs6DNYUqmU7TrXza/YgnAYTIJdO6Z5kxOilTKLiESdlONMSMj9gAehmGLADtpEWgCT30I0VxCLR4//amLNB6HHiZJ2A41LNavvxP6yXonzupCOMEIeSZJdP8RFKMaN4L7QsFHOU4A8aVyK6kfMgU45i1l8e3ZsPOQ/ukZtVr9evTauNiWkSJ7JMDckwsckYa5Io0SYtw8kieyRt5Nx6MJ+PFeP2xLhjTP3vkzxgf31b/mfM=</latexit>
> Value of  hz1i

Figure 5 . Top plot: Distribution of relevance (as defined in Eq.5.2) in the latent variables. In orange, the truly two-dimensional dataset D2D and in blue the dataset constrained to a circle D1D .The latent variables are ordered by decreasing relevance. Bottom plot: Illustration of latent-space organization for data in 2D (left) and constrained to a circle (right). The dominant latent variable (denoted by z1) provides a smooth coordinate along the symmetry orbit, while remaining latent directions are suppressed. Figures from Ref. [15]. 

ordering of points along the symmetry orbit. Although this coordinate is not unique—any monotonic reparametrization would serve equally well—it demonstrates that the latent representation has internalized the effective dimensionality imposed by the symmetry. This behaviour is shown in the lower panel of Fig.5. All figures are obtained from Ref.[15]. This example serves as a benchmark: when symmetry is exact, global, and cleanly represented in the data, latent-space self-organization provides a sharp and unambiguous diagnostic of reduced dimensionality. – 13 – 6.2 Lepton Collisions and Momentum Conservation 

We now turn to a physically richer example: electron–positron annihilation into a muon– antimuon pair, see Fig.6. At fixed center-of-mass energy, the final state is fully described by the three-momenta of the two outgoing leptons. While this yields six observable com-ponents, momentum conservation imposes three exact constraints, ⃗p μ+ +⃗p μ− = 0 , (6.1) reducing the number of independent degrees of freedom to three. When a variational 

e−(l1)

e+(l2)

μ−(p1)

μ+(p2)

γ(k)

Figure 6 . Feynman diagram for the process e+e− → μ+μ− in QED. Relevance plot-lepton collisions 

> Relevance

<latexit sha1_base64="8RlkLIL/xEmGZVmuARtU0/6EM6U=">AAAB5HicbZDNTgIxFIXv4B/iH+rSTSMxcUVmjAGXRDcuMcpPAhPSKXegodOZtB0TMuENdGXUnU/kC/g2FpyFgmf19Z7T5J4bJIJr47pfTmFtfWNzq7hd2tnd2z8oHx61dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5mfudR1Sax/LBTBP0IzqSPOSMGju6lwNvUK64VXchsgpeDhXI1RyUP/vDmKURSsME1brnuYnxM6oMZwJnpX6qMaFsQkfYsyhphNrPFqvOyFkYK2LGSBbv39mMRlpPo8BmImrGetmbD//zeqkJr/yMyyQ1KJmNWC9MBTExmTcmQ66QGTG1QJnidkvCxlRRZuxdSra+t1x2FdoXVa9Wrd1dVhrX+SGKcAKncA4e1KEBt9CEFjAYwTO8wbsTOk/Oi/P6Ey04+Z9j+CPn4xvxx4s2</latexit> 

n1 <latexit sha1_base64="PBLC5YjpWLaYHHgzAAzgP+isAt4=">AAAB5HicbZDNTgIxFIXv4B/iH+rSTSMxcUVmiEGXRDcuMcpPAhPSKXegodOZtB0TMuENdGXUnU/kC/g2FpyFgmf19Z7T5J4bJIJr47pfTmFtfWNzq7hd2tnd2z8oHx61dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5mfudR1Sax/LBTBP0IzqSPOSMGju6l4PaoFxxq+5CZBW8HCqQqzkof/aHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ1HSCLWfLVadkbMwVsSMkSzev7MZjbSeRoHNRNSM9bI3H/7n9VITXvkZl0lqUDIbsV6YCmJiMm9MhlwhM2JqgTLF7ZaEjamizNi7lGx9b7nsKrRrVa9erd9dVBrX+SGKcAKncA4eXEIDbqEJLWAwgmd4g3cndJ6cF+f1J1pw8j/H8EfOxzfzRYs3</latexit> 

n2 <latexit sha1_base64="Uw1qFuEkSap3MbKTkQvyGGxmiOs=">AAAB5HicbZDNTgIxFIXv4B/iH+rSTSMxcUVm1KBLohuXGOUngQnplDvQ0OlM2o4JmfAGujLqzifyBXwbC85CwbP6es9pcs8NEsG1cd0vp7Cyura+UdwsbW3v7O6V9w9aOk4VwyaLRaw6AdUouMSm4UZgJ1FIo0BgOxjfzPz2IyrNY/lgJgn6ER1KHnJGjR3dy/55v1xxq+5cZBm8HCqQq9Evf/YGMUsjlIYJqnXXcxPjZ1QZzgROS71UY0LZmA6xa1HSCLWfzVedkpMwVsSMkMzfv7MZjbSeRIHNRNSM9KI3G/7ndVMTXvkZl0lqUDIbsV6YCmJiMmtMBlwhM2JigTLF7ZaEjaiizNi7lGx9b7HsMrTOql6tWru7qNSv80MU4QiO4RQ8uIQ63EIDmsBgCM/wBu9O6Dw5L87rT7Tg5H8O4Y+cj2/0w4s4</latexit> 

n3 <latexit sha1_base64="LVaVGu7RF7qtfzDb612uHzbm2Cs=">AAAB5XicbZDNTsJAFIVv8Q/xD3XpZiIxcUVaQ8Al0Y1LTOQngYZMh1s6YTptZqYmpOERdGXUnS/kC/g2ttiFgmf1zT1nknuuFwuujW1/WaWNza3tnfJuZW//4PCoenzS01GiGHZZJCI18KhGwSV2DTcCB7FCGnoC+97sNvf7j6g0j+SDmcfohnQquc8ZNflIjhuVcbVm1+2lyDo4BdSgUGdc/RxNIpaEKA0TVOuhY8fGTakynAlcVEaJxpiyGZ3iMENJQ9Ruutx1QS78SBETIFm+f2dTGmo9D70sE1IT6FUvH/7nDRPjX7spl3FiULIsknl+IoiJSF6ZTLhCZsQ8A8oUz7YkLKCKMpMdJq/vrJZdh95V3WnWm/eNWvumOEQZzuAcLsGBFrThDjrQBQYBPMMbvFtT68l6sV5/oiWr+HMKf2R9fAMrH4tN</latexit> 

n4 <latexit sha1_base64="Sj20O5xjMgfX0CQLzNx6Iu/Xq1g=">AAAB5XicbZDNTsJAFIVv8Q/xD3XpZiIxcUVaI+iS6MYlJvKTQEOmwy2dMJ02M1MT0vAIujLqzhfyBXwbW+xCwbP65p4zyT3XiwXXxra/rNLa+sbmVnm7srO7t39QPTzq6ihRDDssEpHqe1Sj4BI7hhuB/VghDT2BPW96m/u9R1SaR/LBzGJ0QzqR3OeMmnwkR43KqFqz6/ZCZBWcAmpQqD2qfg7HEUtClIYJqvXAsWPjplQZzgTOK8NEY0zZlE5wkKGkIWo3Xew6J2d+pIgJkCzev7MpDbWehV6WCakJ9LKXD//zBonxr92UyzgxKFkWyTw/EcREJK9MxlwhM2KWAWWKZ1sSFlBFmckOk9d3lsuuQvei7jTrzfvLWuumOEQZTuAUzsGBK2jBHbShAwwCeIY3eLcm1pP1Yr3+REtW8ecY/sj6+AYsnotO</latexit> 

n5 <latexit sha1_base64="okH3eXqhTXK1AT78kWSTAWlyByk=">AAAB5XicbZDNSsNAFIVv6l+tf1WXbgaL4KokItFl0Y3LCvYH2lAm05tm6GQSZiZCCX0EXYm684V8Ad/GpGahrWf1zT1n4J7rJ4JrY9tfVmVtfWNzq7pd29nd2z+oHx51dZwqhh0Wi1j1fapRcIkdw43AfqKQRr7Anj+9LfzeIyrNY/lgZgl6EZ1IHnBGTTGSI7c2qjfspr0QWQWnhAaUao/qn8NxzNIIpWGCaj1w7MR4GVWGM4Hz2jDVmFA2pRMc5ChphNrLFrvOyVkQK2JCJIv372xGI61nkZ9nImpCvewVw/+8QWqCay/jMkkNSpZHci9IBTExKSqTMVfIjJjlQJni+ZaEhVRRZvLDFPWd5bKr0L1oOm7Tvb9stG7KQ1ThBE7hHBy4ghbcQRs6wCCEZ3iDd2tiPVkv1utPtGKVf47hj6yPby4di08=</latexit> 

n6

Figure 7 . Relevance spectrum and illustrative correlations for lepton collision data. Three dom-inant latent directions reflect the effective dimensionality imposed by momentum conservation. Figure from Ref. [15]. 

autoencoder is trained on such data with a latent space dimension dz ≥ 6, the relevance spectrum again reveals a clear hierarchy. As shown in Fig.7, only three latent variables exhibit significant relevance, while the remaining directions collapse toward the prior. This behavior directly mirrors the dimensional reduction implied by momentum conservation. Correlations between the relevant latent variables and the input features provide further interpretation. As shown in Figure 8, the dominant latent directions typically correlate with independent combinations of momenta, such as differences between the muon and antimuon – 14 – <latexit sha1_base64="u7Y3VmeIl/CklFIZxOBc1r4+9pw=">AAAB6HicbZC9TsMwFIVv+C3lr8DIYlEhMVUJQoWxgoWxSPRHakPluDetqZ1EtoOoor4DTAjYeB5egLfBLRmg5Uyf7zmW7rlBIrg2rvvlLC2vrK6tFzaKm1vbO7ulvf2mjlPFsMFiEat2QDUKHmHDcCOwnSikMhDYCkZXU7/1gErzOLo14wR9SQcRDzmjxo5aSe/xrivTXqnsVtyZyCJ4OZQhV71X+uz2Y5ZKjAwTVOuO5ybGz6gynAmcFLupxoSyER1gx2JEJWo/m607IcdhrIgZIpm9f2czKrUey8BmJDVDPe9Nh/95ndSEF37GoyQ1GDEbsV6YCmJiMm1N+lwhM2JsgTLF7ZaEDamizNjbFG19b77sIjRPK161Ur05K9cu80MU4BCO4AQ8OIcaXEMdGsBgBM/wBu/OvfPkvDivP9ElJ/9zAH/kfHwDYKyNQw==</latexit> 

pμ

x

<latexit sha1_base64="fdKaUphECXEqL0GNswwcatViZhI=">AAAB6HicbZDNTgIxFIXv4B/iH+rSTSMxcUVmjEGXRDcuMZGfBEbSKXeg0s5M2o7JZMI76MqoO5/HF/BtLMhCwbP6es9pcs8NEsG1cd0vp7Cyura+UdwsbW3v7O6V9w9aOk4VwyaLRaw6AdUoeIRNw43ATqKQykBgOxhfT/32IyrN4+jOZAn6kg4jHnJGjR21k35235Npv1xxq+5MZBm8OVRgrka//NkbxCyVGBkmqNZdz02Mn1NlOBM4KfVSjQllYzrErsWIStR+Plt3Qk7CWBEzQjJ7/87mVGqdycBmJDUjvehNh/953dSEl37OoyQ1GDEbsV6YCmJiMm1NBlwhMyKzQJnidkvCRlRRZuxtSra+t1h2GVpnVa9Wrd2eV+pX80MU4QiO4RQ8uIA63EADmsBgDM/wBu/Og/PkvDivP9GCM/9zCH/kfHwDYi6NRA==</latexit> 

pμ

y

<latexit sha1_base64="2KQa0egE4zgTfngfeK5ZUuYnOWc=">AAAB6HicbZC9TsMwFIVv+C3lr8DIYlEhMVUJQoWxgoWxSPRHakPluDetqZ1EtoNUor4DTAjYeB5egLfBLRmg5Uyf7zmW7rlBIrg2rvvlLC2vrK6tFzaKm1vbO7ulvf2mjlPFsMFiEat2QDUKHmHDcCOwnSikMhDYCkZXU7/1gErzOLo14wR9SQcRDzmjxo5aSe/xrivTXqnsVtyZyCJ4OZQhV71X+uz2Y5ZKjAwTVOuO5ybGz6gynAmcFLupxoSyER1gx2JEJWo/m607IcdhrIgZIpm9f2czKrUey8BmJDVDPe9Nh/95ndSEF37GoyQ1GDEbsV6YCmJiMm1N+lwhM2JsgTLF7ZaEDamizNjbFG19b77sIjRPK161Ur05K9cu80MU4BCO4AQ8OIcaXEMdGsBgBM/wBu/OvfPkvDivP9ElJ/9zAH/kfHwDY7CNRQ==</latexit> 

pμ

z

<latexit sha1_base64="dvLgP6M4Uq+cMKnZIyYHOLkMjSw=">AAAB73icbZDLTsJAFIZP8YZ4Q126mUhMXJHWGHRJdOMSE7kktJDpcAoTphdnpibY8By6MurOd/EFfBsH7ELBf/XN+f9Jzn/8RHClbfvLKqysrq1vFDdLW9s7u3vl/YOWilPJsMliEcuOTxUKHmFTcy2wk0ikoS+w7Y+vZ377AaXicXSnJwl6IR1GPOCMajPqJf3HXub6VBI3TKf9csWu2nORZXByqECuRr/86Q5iloYYaSaoUl3HTrSXUak5EzgtuanChLIxHWLXYERDVF4233pKToJYEj1CMn//zmY0VGoS+iYTUj1Si95s+J/XTXVw6WU8SlKNETMR4wWpIDoms/JkwCUyLSYGKJPcbEnYiErKtDlRydR3FssuQ+us6tSqtdvzSv0qP0QRjuAYTsGBC6jDDTSgCQwkPMMbvFv31pP1Yr3+RAtW/ucQ/sj6+AZ6aZA0</latexit> 

p¯μ

z

<latexit sha1_base64="N4Ce42mylIL6TpBxzmzC7SNMDK0=">AAAB73icbZDLTsMwEEUnPEt5FViysaiQWFUJQoVlBRuWRaIPqUkrx520Vp0HtoMURf0OWCFgx7/wA/wNbskCWu7qeO61NHf8RHClbfvLWlldW9/YLG2Vt3d29/YrB4dtFaeSYYvFIpZdnyoUPMKW5lpgN5FIQ19gx5/czPzOI0rF4+heZwl6IR1FPOCMajPqJ4Osn7s+lcQN0+mgUrVr9lxkGZwCqlCoOah8usOYpSFGmgmqVM+xE+3lVGrOBE7LbqowoWxCR9gzGNEQlZfPt56S0yCWRI+RzN+/szkNlcpC32RCqsdq0ZsN//N6qQ6uvJxHSaoxYiZivCAVRMdkVp4MuUSmRWaAMsnNloSNqaRMmxOVTX1nsewytM9rTr1Wv7uoNq6LQ5TgGE7gDBy4hAbcQhNawEDCM7zBu/VgPVkv1utPdMUq/hzBH1kf33jgkDM=</latexit> 

p¯μ

y

<latexit sha1_base64="pIfb8NDQEjULhYtIk4KDBCyEkEU=">AAAB73icbZDLTsJAFIZP8YZ4Q126mUhMXJHWGHRJdOMSE7kktJDpcAoTphdnpkbS8By6MurOd/EFfBsH7ELBf/XN+f9Jzn/8RHClbfvLKqysrq1vFDdLW9s7u3vl/YOWilPJsMliEcuOTxUKHmFTcy2wk0ikoS+w7Y+vZ377AaXicXSnJwl6IR1GPOCMajPqJf3HXub6VBI3TKf9csWu2nORZXByqECuRr/86Q5iloYYaSaoUl3HTrSXUak5EzgtuanChLIxHWLXYERDVF4233pKToJYEj1CMn//zmY0VGoS+iYTUj1Si95s+J/XTXVw6WU8SlKNETMR4wWpIDoms/JkwCUyLSYGKJPcbEnYiErKtDlRydR3FssuQ+us6tSqtdvzSv0qP0QRjuAYTsGBC6jDDTSgCQwkPMMbvFv31pP1Yr3+RAtW/ucQ/sj6+AZ3V5Ay</latexit> 

p¯μ

x

Activation vs features 

<latexit sha1_base64="8RlkLIL/xEmGZVmuARtU0/6EM6U=">AAAB5HicbZDNTgIxFIXv4B/iH+rSTSMxcUVmjAGXRDcuMcpPAhPSKXegodOZtB0TMuENdGXUnU/kC/g2FpyFgmf19Z7T5J4bJIJr47pfTmFtfWNzq7hd2tnd2z8oHx61dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5mfudR1Sax/LBTBP0IzqSPOSMGju6lwNvUK64VXchsgpeDhXI1RyUP/vDmKURSsME1brnuYnxM6oMZwJnpX6qMaFsQkfYsyhphNrPFqvOyFkYK2LGSBbv39mMRlpPo8BmImrGetmbD//zeqkJr/yMyyQ1KJmNWC9MBTExmTcmQ66QGTG1QJnidkvCxlRRZuxdSra+t1x2FdoXVa9Wrd1dVhrX+SGKcAKncA4e1KEBt9CEFjAYwTO8wbsTOk/Oi/P6Ey04+Z9j+CPn4xvxx4s2</latexit> 

n1

<latexit sha1_base64="PBLC5YjpWLaYHHgzAAzgP+isAt4=">AAAB5HicbZDNTgIxFIXv4B/iH+rSTSMxcUVmiEGXRDcuMcpPAhPSKXegodOZtB0TMuENdGXUnU/kC/g2FpyFgmf19Z7T5J4bJIJr47pfTmFtfWNzq7hd2tnd2z8oHx61dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5mfudR1Sax/LBTBP0IzqSPOSMGju6l4PaoFxxq+5CZBW8HCqQqzkof/aHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ1HSCLWfLVadkbMwVsSMkSzev7MZjbSeRoHNRNSM9bI3H/7n9VITXvkZl0lqUDIbsV6YCmJiMm9MhlwhM2JqgTLF7ZaEjamizNi7lGx9b7nsKrRrVa9erd9dVBrX+SGKcAKncA4eXEIDbqEJLWAwgmd4g3cndJ6cF+f1J1pw8j/H8EfOxzfzRYs3</latexit> 

n2

<latexit sha1_base64="Uw1qFuEkSap3MbKTkQvyGGxmiOs=">AAAB5HicbZDNTgIxFIXv4B/iH+rSTSMxcUVm1KBLohuXGOUngQnplDvQ0OlM2o4JmfAGujLqzifyBXwbC85CwbP6es9pcs8NEsG1cd0vp7Cyura+UdwsbW3v7O6V9w9aOk4VwyaLRaw6AdUouMSm4UZgJ1FIo0BgOxjfzPz2IyrNY/lgJgn6ER1KHnJGjR3dy/55v1xxq+5cZBm8HCqQq9Evf/YGMUsjlIYJqnXXcxPjZ1QZzgROS71UY0LZmA6xa1HSCLWfzVedkpMwVsSMkMzfv7MZjbSeRIHNRNSM9KI3G/7ndVMTXvkZl0lqUDIbsV6YCmJiMmtMBlwhM2JigTLF7ZaEjaiizNi7lGx9b7HsMrTOql6tWru7qNSv80MU4QiO4RQ8uIQ63EIDmsBgCM/wBu9O6Dw5L87rT7Tg5H8O4Y+cj2/0w4s4</latexit> 

n3

<latexit sha1_base64="LVaVGu7RF7qtfzDb612uHzbm2Cs=">AAAB5XicbZDNTsJAFIVv8Q/xD3XpZiIxcUVaQ8Al0Y1LTOQngYZMh1s6YTptZqYmpOERdGXUnS/kC/g2ttiFgmf1zT1nknuuFwuujW1/WaWNza3tnfJuZW//4PCoenzS01GiGHZZJCI18KhGwSV2DTcCB7FCGnoC+97sNvf7j6g0j+SDmcfohnQquc8ZNflIjhuVcbVm1+2lyDo4BdSgUGdc/RxNIpaEKA0TVOuhY8fGTakynAlcVEaJxpiyGZ3iMENJQ9Ruutx1QS78SBETIFm+f2dTGmo9D70sE1IT6FUvH/7nDRPjX7spl3FiULIsknl+IoiJSF6ZTLhCZsQ8A8oUz7YkLKCKMpMdJq/vrJZdh95V3WnWm/eNWvumOEQZzuAcLsGBFrThDjrQBQYBPMMbvFtT68l6sV5/oiWr+HMKf2R9fAMrH4tN</latexit> 

n4

<latexit sha1_base64="Sj20O5xjMgfX0CQLzNx6Iu/Xq1g=">AAAB5XicbZDNTsJAFIVv8Q/xD3XpZiIxcUVaI+iS6MYlJvKTQEOmwy2dMJ02M1MT0vAIujLqzhfyBXwbW+xCwbP65p4zyT3XiwXXxra/rNLa+sbmVnm7srO7t39QPTzq6ihRDDssEpHqe1Sj4BI7hhuB/VghDT2BPW96m/u9R1SaR/LBzGJ0QzqR3OeMmnwkR43KqFqz6/ZCZBWcAmpQqD2qfg7HEUtClIYJqvXAsWPjplQZzgTOK8NEY0zZlE5wkKGkIWo3Xew6J2d+pIgJkCzev7MpDbWehV6WCakJ9LKXD//zBonxr92UyzgxKFkWyTw/EcREJK9MxlwhM2KWAWWKZ1sSFlBFmckOk9d3lsuuQvei7jTrzfvLWuumOEQZTuAUzsGBK2jBHbShAwwCeIY3eLcm1pP1Yr3+REtW8ecY/sj6+AYsnotO</latexit> 

n5

<latexit sha1_base64="okH3eXqhTXK1AT78kWSTAWlyByk=">AAAB5XicbZDNSsNAFIVv6l+tf1WXbgaL4KokItFl0Y3LCvYH2lAm05tm6GQSZiZCCX0EXYm684V8Ad/GpGahrWf1zT1n4J7rJ4JrY9tfVmVtfWNzq7pd29nd2z+oHx51dZwqhh0Wi1j1fapRcIkdw43AfqKQRr7Anj+9LfzeIyrNY/lgZgl6EZ1IHnBGTTGSI7c2qjfspr0QWQWnhAaUao/qn8NxzNIIpWGCaj1w7MR4GVWGM4Hz2jDVmFA2pRMc5ChphNrLFrvOyVkQK2JCJIv372xGI61nkZ9nImpCvewVw/+8QWqCay/jMkkNSpZHci9IBTExKSqTMVfIjJjlQJni+ZaEhVRRZvLDFPWd5bKr0L1oOm7Tvb9stG7KQ1ThBE7hHBy4ghbcQRs6wCCEZ3iDd2tiPVkv1utPtGKVf47hj6yPby4di08=</latexit> 

n6Figure 8 . Scatter plots with mean latent activations as a function of the input kinematic features, for the Drell-Yan dataset. The first three latent variables are strongly correlated with the conserved variable combinations pμy − p¯μy , pμx − p¯μx , and pμz − p¯μz , respectively. Figure from Ref. [15]. 

momentum components. Redundant combinations—those fixed by conservation laws—are not encoded in relevant latent directions. This case study demonstrates that latent-space self-organization is not limited to toy models or abstract manifolds. It persists in realistic kinematic datasets, where symmetry appears as an exact conservation law acting event by event. 

6.3 Hadron Collisions and Approximate Symmetries 

The final example considers Drell–Yan production of muon pairs in proton–proton collisions. Compared to lepton collisions, the initial state is no longer fully known: the longitudinal momenta of the incoming partons fluctuate event by event, and the center-of-mass energy of the hard process is not fixed. As a result, only a subset of the symmetry constraints present in the electron–positron case survives. Transverse momentum conservation remains an excellent approximation, while longi-tudinal momentum conservation is broken by the unknown parton momentum fractions. Additional constraints arise from on-shell conditions for the final-state particles and, in se-lected kinematic regions, from resonant production through an intermediate vector boson. When a variational autoencoder is trained on this dataset, the relevance spectrum exhibits a hierarchy that is less sharp than in the previous cases but remains clearly struc-tured. As shown in Figure 9, a small number of latent variables dominate, reflecting the approximate reduction in effective dimensionality induced by the surviving constraints. The – 15 – Relevance plot-hadron collisions 

> Relevance

<latexit sha1_base64="8RlkLIL/xEmGZVmuARtU0/6EM6U=">AAAB5HicbZDNTgIxFIXv4B/iH+rSTSMxcUVmjAGXRDcuMcpPAhPSKXegodOZtB0TMuENdGXUnU/kC/g2FpyFgmf19Z7T5J4bJIJr47pfTmFtfWNzq7hd2tnd2z8oHx61dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5mfudR1Sax/LBTBP0IzqSPOSMGju6lwNvUK64VXchsgpeDhXI1RyUP/vDmKURSsME1brnuYnxM6oMZwJnpX6qMaFsQkfYsyhphNrPFqvOyFkYK2LGSBbv39mMRlpPo8BmImrGetmbD//zeqkJr/yMyyQ1KJmNWC9MBTExmTcmQ66QGTG1QJnidkvCxlRRZuxdSra+t1x2FdoXVa9Wrd1dVhrX+SGKcAKncA4e1KEBt9CEFjAYwTO8wbsTOk/Oi/P6Ey04+Z9j+CPn4xvxx4s2</latexit> 

n1 <latexit sha1_base64="PBLC5YjpWLaYHHgzAAzgP+isAt4=">AAAB5HicbZDNTgIxFIXv4B/iH+rSTSMxcUVmiEGXRDcuMcpPAhPSKXegodOZtB0TMuENdGXUnU/kC/g2FpyFgmf19Z7T5J4bJIJr47pfTmFtfWNzq7hd2tnd2z8oHx61dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5mfudR1Sax/LBTBP0IzqSPOSMGju6l4PaoFxxq+5CZBW8HCqQqzkof/aHMUsjlIYJqnXPcxPjZ1QZzgTOSv1UY0LZhI6wZ1HSCLWfLVadkbMwVsSMkSzev7MZjbSeRoHNRNSM9bI3H/7n9VITXvkZl0lqUDIbsV6YCmJiMm9MhlwhM2JqgTLF7ZaEjamizNi7lGx9b7nsKrRrVa9erd9dVBrX+SGKcAKncA4eXEIDbqEJLWAwgmd4g3cndJ6cF+f1J1pw8j/H8EfOxzfzRYs3</latexit> 

n2 <latexit sha1_base64="Uw1qFuEkSap3MbKTkQvyGGxmiOs=">AAAB5HicbZDNTgIxFIXv4B/iH+rSTSMxcUVm1KBLohuXGOUngQnplDvQ0OlM2o4JmfAGujLqzifyBXwbC85CwbP6es9pcs8NEsG1cd0vp7Cyura+UdwsbW3v7O6V9w9aOk4VwyaLRaw6AdUouMSm4UZgJ1FIo0BgOxjfzPz2IyrNY/lgJgn6ER1KHnJGjR3dy/55v1xxq+5cZBm8HCqQq9Evf/YGMUsjlIYJqnXXcxPjZ1QZzgROS71UY0LZmA6xa1HSCLWfzVedkpMwVsSMkMzfv7MZjbSeRIHNRNSM9KI3G/7ndVMTXvkZl0lqUDIbsV6YCmJiMmtMBlwhM2JigTLF7ZaEjaiizNi7lGx9b7HsMrTOql6tWru7qNSv80MU4QiO4RQ8uIQ63EIDmsBgCM/wBu9O6Dw5L87rT7Tg5H8O4Y+cj2/0w4s4</latexit> 

n3 <latexit sha1_base64="LVaVGu7RF7qtfzDb612uHzbm2Cs=">AAAB5XicbZDNTsJAFIVv8Q/xD3XpZiIxcUVaQ8Al0Y1LTOQngYZMh1s6YTptZqYmpOERdGXUnS/kC/g2ttiFgmf1zT1nknuuFwuujW1/WaWNza3tnfJuZW//4PCoenzS01GiGHZZJCI18KhGwSV2DTcCB7FCGnoC+97sNvf7j6g0j+SDmcfohnQquc8ZNflIjhuVcbVm1+2lyDo4BdSgUGdc/RxNIpaEKA0TVOuhY8fGTakynAlcVEaJxpiyGZ3iMENJQ9Ruutx1QS78SBETIFm+f2dTGmo9D70sE1IT6FUvH/7nDRPjX7spl3FiULIsknl+IoiJSF6ZTLhCZsQ8A8oUz7YkLKCKMpMdJq/vrJZdh95V3WnWm/eNWvumOEQZzuAcLsGBFrThDjrQBQYBPMMbvFtT68l6sV5/oiWr+HMKf2R9fAMrH4tN</latexit> 

n4 <latexit sha1_base64="Sj20O5xjMgfX0CQLzNx6Iu/Xq1g=">AAAB5XicbZDNTsJAFIVv8Q/xD3XpZiIxcUVaI+iS6MYlJvKTQEOmwy2dMJ02M1MT0vAIujLqzhfyBXwbW+xCwbP65p4zyT3XiwXXxra/rNLa+sbmVnm7srO7t39QPTzq6ihRDDssEpHqe1Sj4BI7hhuB/VghDT2BPW96m/u9R1SaR/LBzGJ0QzqR3OeMmnwkR43KqFqz6/ZCZBWcAmpQqD2qfg7HEUtClIYJqvXAsWPjplQZzgTOK8NEY0zZlE5wkKGkIWo3Xew6J2d+pIgJkCzev7MpDbWehV6WCakJ9LKXD//zBonxr92UyzgxKFkWyTw/EcREJK9MxlwhM2KWAWWKZ1sSFlBFmckOk9d3lsuuQvei7jTrzfvLWuumOEQZTuAUzsGBK2jBHbShAwwCeIY3eLcm1pP1Yr3+REtW8ecY/sj6+AYsnotO</latexit> 

n5 <latexit sha1_base64="okH3eXqhTXK1AT78kWSTAWlyByk=">AAAB5XicbZDNSsNAFIVv6l+tf1WXbgaL4KokItFl0Y3LCvYH2lAm05tm6GQSZiZCCX0EXYm684V8Ad/GpGahrWf1zT1n4J7rJ4JrY9tfVmVtfWNzq7pd29nd2z+oHx51dZwqhh0Wi1j1fapRcIkdw43AfqKQRr7Anj+9LfzeIyrNY/lgZgl6EZ1IHnBGTTGSI7c2qjfspr0QWQWnhAaUao/qn8NxzNIIpWGCaj1w7MR4GVWGM4Hz2jDVmFA2pRMc5ChphNrLFrvOyVkQK2JCJIv372xGI61nkZ9nImpCvewVw/+8QWqCay/jMkkNSpZHci9IBTExKSqTMVfIjJjlQJni+ZaEhVRRZvLDFPWd5bKr0L1oOm7Tvb9stG7KQ1ThBE7hHBy4ghbcQRs6wCCEZ3iDd2tiPVkv1utPtGKVf47hj6yPby4di08=</latexit> 

n6 <latexit sha1_base64="x2gRklis/bPiRkF8KSg+p0ByzZE=">AAAB5XicbZDNTsJAFIVv8Q/xB9Slm4nExBVpjQGXRDcuMZGfBBoyHW7phOm0mZmakIZH0JVRd76QL+Db2GIXCp7VN/ecSe65Xiy4Nrb9ZZU2Nre2d8q7lb39g8Nq7ei4p6NEMeyySERq4FGNgkvsGm4EDmKFNPQE9r3Zbe73H1FpHskHM4/RDelUcp8zavKRHLcq41rdbthLkXVwCqhDoc649jmaRCwJURomqNZDx46Nm1JlOBO4qIwSjTFlMzrFYYaShqjddLnrgpz7kSImQLJ8/86mNNR6HnpZJqQm0KtePvzPGybGv3ZTLuPEoGRZJPP8RBATkbwymXCFzIh5BpQpnm1JWEAVZSY7TF7fWS27Dr3LhtNsNO+v6u2b4hBlOIUzuAAHWtCGO+hAFxgE8Axv8G5NrSfrxXr9iZas4s8J/JH18Q0vnItQ</latexit> 

n7 <latexit sha1_base64="kJIMefUqYLh8EJk4kVoEdMvSGQ8=">AAAB5XicbZDNTsJAFIVv8Q/xD3XpZiIxcUVaY4Al0Y1LTOQngYZMh1s6YTptZqYmpOERdGXUnS/kC/g2ttiFgmf1zT1nknuuFwuujW1/WaWNza3tnfJuZW//4PCoenzS01GiGHZZJCI18KhGwSV2DTcCB7FCGnoC+97sNvf7j6g0j+SDmcfohnQquc8ZNflIjluVcbVm1+2lyDo4BdSgUGdc/RxNIpaEKA0TVOuhY8fGTakynAlcVEaJxpiyGZ3iMENJQ9Ruutx1QS78SBETIFm+f2dTGmo9D70sE1IT6FUvH/7nDRPjt9yUyzgxKFkWyTw/EcREJK9MJlwhM2KeAWWKZ1sSFlBFmckOk9d3VsuuQ++q7jTqjfvrWvumOEQZzuAcLsGBJrThDjrQBQYBPMMbvFtT68l6sV5/oiWr+HMKf2R9fAMxG4tR</latexit> 

n8Figure 9 . Latent relevance spectrum for hadron collision data. The hierarchy is softer than in cases with exact symmetry, reflecting approximate and kinematically dependent constraints. Figure from Ref. [15]. 

remaining latent directions show suppressed but non-negligible relevance, encoding residual variability associated with broken or softened symmetries. This example highlights a crucial point: approximate symmetries do not lead to an abrupt collapse of latent dimensionality, but rather to graded hierarchies. From a diag-nostic perspective, this behavior is particularly interesting, as it suggests that latent-space organization can carry information about the degree to which a symmetry is realized in the data. 

6.4 Lessons from the Case Studies 

Taken together, these examples illustrate a consistent pattern. Exact symmetries lead to sharp reductions in effective dimensionality and clear latent hierarchies. Approximate or context-dependent symmetries result in softer but still interpretable structures. Through-out, the latent representations do not encode symmetry generators explicitly. Instead, they reflect the informational consequences of symmetry: redundancy, constraint, and reduced variability. These observations reinforce the central message of this review. Variational autoen-coders, when used judiciously, can serve as probes of symmetry-induced structure in physical data. They do not replace analytic reasoning or symmetry principles, but they can provide a useful diagnostic layer—particularly in complex or high-dimensional settings where the relevant constraints may not be obvious from first principles. 

7 Theoretical Perspectives on Latent–Symmetry Alignment 

The case studies presented in the previous section suggest a recurring empirical pattern: when data are generated by systems with exact or approximate symmetries, variational – 16 – autoencoders trained to balance reconstruction and compression often organize their latent spaces along symmetry-independent directions. In this section, we examine to what extent this behavior can be understood from general theoretical considerations, and where its limitations lie. 

7.1 Symmetry, Redundancy, and Information Compression 

At a conceptual level, the alignment between latent representations and symmetry-induced structure can be traced back to redundancy. A symmetry identifies multiple configurations of the data as physically equivalent. From an information-theoretic perspective, such config-urations do not constitute independent messages. Any representation that aims to encode the data efficiently should therefore avoid allocating independent capacity to symmetry-related variations. In a variational autoencoder, this pressure toward compression is made explicit by the Kullback–Leibler term in the evidence lower bound. Latent directions that do not contribute significantly to reducing reconstruction error are penalized and driven toward the prior. If a dataset contains exact constraints—such as conservation laws—then variations along redundant directions do not improve reconstruction, and these directions are naturally suppressed. In this sense, symmetry acts indirectly, by shaping the loss landscape rather than by constraining the model explicitly. This argument is qualitative but robust. It does not depend on the detailed form of the symmetry group, nor on the interpretability of the latent variables. It relies only on the existence of redundancy and on the presence of a compression objective that penalizes unused degrees of freedom. 

7.2 Linear Limits and the Connection to PCA 

Additional insight can be gained by considering simplified settings in which analytic results are available. In the linear regime, autoencoders are closely related to principal component analysis. For a dataset with covariance matrix Σ, PCA identifies orthogonal directions that maximize variance. If the data distribution is invariant under a continuous symmetry, the covariance matrix often reflects this invariance, leading to degeneracies or reduced rank. For example, data uniformly distributed on a circle embedded in R2 have an isotropic covariance matrix proportional to the identity. Any single projection captures the same amount of variance, but projecting onto a one-dimensional subspace already achieves opti-mal compression. From the PCA perspective, the symmetry manifests itself as a reduction in effective dimensionality rather than as a preferred direction. A linear variational autoencoder reproduces this behavior. The reconstruction term encourages projection onto a low-dimensional subspace, while the KL term penalizes un-necessary latent dimensions. In this simplified setting, the suppression of redundant latent variables follows directly from the structure of the data covariance and the form of the loss. Although nonlinear VAEs operate far beyond this regime, the linear case provides a useful intuition: symmetry-aligned compression is the generic outcome of variance-based optimization in the presence of redundancy. – 17 – 7.3 Nonlinear Manifolds and Local Coordinate Choices 

In realistic physical datasets, symmetry orbits and constraint surfaces define nonlinear manifolds embedded in high-dimensional spaces. In such cases, there is no unique global coordinate system adapted to the symmetry. Any smooth reparametrization of the intrinsic degrees of freedom is equally valid. Consequently, one should not expect a variational autoencoder to recover symmetry generators or group parameters in a unique or canonical form. Instead, what can be expected is local alignment. In regions of the data manifold where the geometry is approximately flat, the encoder can learn coordinates that parametrize variations along the manifold while suppressing transverse directions. Globally, these coor-dinates may be distorted or nonlinearly related to physically familiar variables. From the diagnostic point of view adopted in this review, such distortions are immaterial. The key signal is the existence of a reduced set of latent directions that carry most of the variation across the dataset. This perspective also clarifies why different training runs or architectures may yield different latent parametrizations while still agreeing on the effective dimensionality. The alignment is not unique, but the redundancy structure it reflects is. 

7.4 Relation to the Information Bottleneck Principle 

The behavior described above is closely related to the information bottleneck principle, which frames representation learning as a trade-off between compression of the input and retention of information relevant for a given task. In the unsupervised setting of a VAE, the task is reconstruction, and the bottleneck is enforced through the latent prior and the KL penalty. From this viewpoint, symmetry-induced redundancy reduces the mutual information between the data and certain directions of variation. These directions can be discarded with minimal impact on reconstruction quality. Latent-space self-organization can therefore be seen as an instance of information bottleneck behavior, where the model preferentially retains variables that parameterize inequivalent configurations and discards redundant ones. It is important to stress, however, that the information bottleneck perspective does not predict which coordinates will be retained, only how many . This again underscores the diagnostic nature of latent-space alignment: it provides information about dimensionality and redundancy, not about a uniquely defined symmetry representation. 

7.5 Limits of Generality and Absence of Guarantees 

Despite the intuitive arguments above, there is no general theorem guaranteeing that a variational autoencoder trained on symmetric data will align its latent space with sym-metry directions. Several factors can obstruct this behavior. Finite data coverage, noise, optimization issues, and architectural choices can all prevent clear latent hierarchies from emerging. Approximate or softly broken symmetries may lead to weak or ambiguous sig-nals. In extreme cases, the model may encode redundancy inefficiently, using multiple latent variables to represent a single effective degree of freedom. – 18 – Moreover, the impossibility results for unsupervised disentanglement imply that any claim of symmetry recovery must be interpreted with care. Without explicit inductive bias, different latent parametrizations can represent the same data distribution equally well. The alignment observed in practice is therefore contingent, not fundamental. For these reasons, latent-space organization should be viewed as evidence for the pres-ence of symmetry-induced structure, not as proof of a specific symmetry or group action. When combined with physical insight and analytic reasoning, however, it provides a valuable complementary perspective—particularly in complex systems where the relevant constraints are not obvious from first principles. In the final section, we synthesize these insights and discuss how data-driven symmetry diagnostics may complement traditional approaches in theoretical physics, as well as the open challenges that remain. 

8 Comparison with Alternative Approaches 

The interpretation of latent-space self-organization as a diagnostic of symmetry-induced structure must be assessed in relation to other methods commonly used for dimensionality reduction, representation learning, and symmetry handling. In this section we contrast the relevance-based VAE approach discussed in this review with several widely used alternatives, emphasizing differences in assumptions, guarantees, and interpretability. 

8.1 Linear Dimensionality Reduction: PCA and Related Methods 

Principal Component Analysis (PCA) remains the standard baseline for identifying reduced dimensional structure in data [24, 27]. By construction, PCA identifies orthogonal directions of maximal variance and provides a transparent estimate of effective dimensionality when the data lie near a linear subspace. In systems with approximate linear constraints, PCA can indeed reveal rank reduction associated with symmetries or conservation laws. However, PCA is fundamentally limited to second-order statistics and linear embed-dings [28, 29]. When physical constraints define nonlinear manifolds—such as circular, spherical, or group-orbit structures—PCA can at best provide a local approximation. While it may signal reduced rank, it does not yield coordinates adapted to the intrinsic geometry of the data. Moreover, PCA lacks a probabilistic interpretation that would allow one to distinguish between genuine structure and noise in a principled way. In contrast, variational autoencoders generalize this idea to the nonlinear regime and introduce an explicit notion of latent uncertainty, which is essential for defining relevance measures that go beyond variance alone. 

8.2 Deterministic Autoencoders 

Deterministic autoencoders extend PCA by allowing nonlinear encoders and decoders. They can learn compact representations of data lying on nonlinear manifolds and have been widely used for visualization and compression. From a purely representational standpoint, they are capable of capturing symmetry-induced dimensional reduction[30, 31] . – 19 – Their main limitation in the present context is interpretability[4] . Without a latent prior or probabilistic structure, the scale, orientation, and usage of latent variables are not uniquely defined. Redundant directions can persist without penalty, and different training runs may yield representations that are difficult to compare. As a result, deterministic autoencoders provide no natural diagnostic for assessing which latent directions are mean-ingfully used and which are incidental. 

8.3 Disentanglement-Oriented Variational Models 

A large body of work has focused on encouraging disentangled latent representations[11, 13, 25, 26], in which each latent variable captures an independent factor of variation. Ap-proaches such as β-VAE, FactorVAE, and β-TCVAE modify the standard VAE objective by penalizing statistical dependence among latent variables. While these methods can produce more factorized representations in practice, their limitations are well understood. In the absence of inductive bias or supervision, disentan-glement is not identifiable, and different latent parametrizations can represent the same data distribution equally well. Moreover, independent factors of variation do not necessar-ily correspond to symmetry generators or conserved quantities. A symmetry may relate variables in a way that is inherently collective rather than separable. The relevance-based approach adopted here does not aim to produce disentanglement. Instead, it asks a weaker but more robust question: how many latent directions are effec-tively used, and does this number reflect symmetry-induced redundancy in the data? 

8.4 Equivariant and Group-Structured Models 

When the relevant symmetry group is known, equivariant neural networks and group-structured generative models offer strong guarantees [7, 8] . By construction, their latent variables transform in prescribed ways under group actions, ensuring alignment with sym-metry directions. Such models are particularly powerful in domains where symmetry prin-ciples are firmly established, such as Lorentz invariance in high-energy physics [20, 21, 32] or permutation invariance in set-based data. The limitation of this approach is its reliance on prior knowledge. The symmetry group, its representation, and its action on the data must be specified in advance. This makes equivariant models ill-suited to exploratory settings where symmetries are unknown, approximate, or emergent. In these cases, imposing an incorrect symmetry can bias the analysis and obscure physically relevant effects. The diagnostic approach discussed in this review is complementary. It does not compete with equivariant modeling, but rather provides a way to assess whether symmetry-aware architectures are warranted in the first place. 

8.5 Normalizing Flows and Diffusion Models 

Modern generative models based on normalizing flows or diffusion processes achieve state-of-the-art performance in density estimation and sample generation [33–37]. Their latent spaces, however, are typically optimized for likelihood rather than interpretability [38]. – 20 – Method Nonlinear Probabilistic Symmetry Required Diagnostic Use PCA No No No Limited Deterministic AE Yes No No Limited Disentangled VAE Yes Yes Implicit Partial Equivariant Models Yes Optional Yes Enforced VAE (this work) Yes Yes No Diagnostic 

> Table 1 . Comparison of common representation-learning approaches from the perspective of sym-metry diagnostics.

While these models can faithfully reproduce complex data distributions, the internal rep-resentation of symmetry-related structure is often implicit and difficult to extract. In principle, symmetry diagnostics could be applied post hoc to these models, but the absence of a simple probabilistic notion of latent relevance complicates the analysis. For the purpose of probing effective dimensionality and redundancy, VAEs occupy a useful middle ground between expressive power and interpretability. 

8.6 Summary of Comparative Features 

To summarize the discussion, Table 1 contrasts the main approaches along dimensions relevant for symmetry diagnostics [8, 13, 14]. Overall, no single method provides a universal solution. Equivariant models are optimal when symmetries are known; disentanglement-oriented models are useful when independent factors are expected; and linear methods remain valuable baselines. The relevance-based VAE framework reviewed here occupies a distinct niche: it offers a principled and inter-pretable diagnostic for detecting symmetry-induced structure in data, without requiring prior knowledge of the symmetry itself. 

9 Outlook and Open Problems 

The case studies and theoretical considerations reviewed in this article suggest that latent-space self-organization in generative models can provide useful diagnostics of symmetry-induced structure in physical data. At the same time, they highlight clear limitations and open questions that must be addressed before such methods can be systematically integrated into the theoretical physics toolkit. In this section, we outline several directions where further progress is both necessary and promising. 

9.1 Approximate, Broken, and Emergent Symmetries 

Most symmetries encountered in realistic physical systems are not exact. They may be explicitly broken by interactions, softly violated by external conditions, or emerge only in restricted regimes. Understanding how such situations are reflected in latent-space organiza-tion remains an open challenge. While exact symmetries tend to produce sharp hierarchies in latent relevance, approximate symmetries lead to graded spectra whose interpretation is less straightforward. – 21 – A systematic study of how symmetry-breaking scales, noise levels, and finite-sample effects map onto latent relevance hierarchies would be valuable. Such studies could help distinguish genuine physical symmetry breaking from artifacts of limited data or model capacity, and may provide a quantitative way to assess the degree to which a symmetry is realized in experimental data. 

9.2 Unknown Symmetries and Exploratory Diagnostics 

One of the main motivations for a diagnostic, rather than prescriptive, approach to sym-metry is the possibility of encountering unknown or unexpected structure. In complex sys-tems—ranging from many-body dynamics to astrophysical or cosmological observations—it is often unclear which symmetries, if any, should be imposed at the outset. In such settings, latent-space diagnostics may serve as a preliminary exploratory tool. Evidence for reduced effective dimensionality or structured redundancy could motivate the introduction of symmetry-aware models, effective theories, or analytic ansätze at a later stage. Conversely, the absence of such signals may indicate that symmetry-based modeling is not appropriate, or that relevant constraints lie outside the observed feature space. 

9.3 Connections to Effective Field Theory and Model Building 

The logic underlying effective field theory is deeply connected to symmetry and dimensional reduction: irrelevant degrees of freedom are integrated out, while the remaining dynamics are organized by symmetry principles. From this perspective, data- driven diagnostics of effective dimensionality may provide a complementary angle on EFT construction, partic-ularly in regimes where the appropriate degrees of freedom are not obvious. While current methods do not identify operators or symmetry generators explicitly, they may help delineate the boundary between relevant and redundant structures in data. Future work combining latent-space diagnostics with symbolic regression, sparse modeling, or operator inference may help bridge the gap between purely data-driven representations and analytic theoretical frameworks. 

9.4 Architectural Biases and Hybrid Approaches 

Another open direction concerns the controlled introduction of inductive bias. While this review has focused on standard variational autoencoders precisely because they do not enforce symmetry by construction, hybrid approaches are worth exploring. One could imagine models that remain agnostic about the precise symmetry group, but are biased toward low-dimensional, structured latent representations through weak regularization or hierarchical priors. Such approaches may strike a balance between flexibility and interpretability, allowing symmetry-related structure to emerge when supported by the data, while avoiding the risks associated with enforcing incorrect symmetries. 

9.5 From Diagnostics to Practice 

Finally, translating latent-space diagnostics into practical tools for data analysis requires careful validation. Sensitivity to architecture, hyperparameters, and training procedures – 22 – must be quantified, and diagnostic criteria must be calibrated against known benchmarks. Without such validation, there is a risk of over-interpreting latent structure as physically meaningful when it is not. Addressing these challenges will require close interaction between machine learning, theoretical modeling, and domain expertise. The potential payoff, however, is significant: a set of data-driven probes that can reveal hidden structure before committing to specific theoretical assumptions. 

10 Conclusions 

Symmetry has long served as a guiding principle in theoretical physics, shaping our un-derstanding of fundamental interactions and constraining the space of viable models. At the same time, modern machine learning has introduced powerful new tools for extracting structure from complex, high-dimensional data. This review has examined the intersection of these two perspectives, focusing on the extent to which symmetry-induced constraints and redundancies can be diagnosed directly from data through representation learning. Rather than emphasizing architectures that enforce symmetry by construction, we have adopted a diagnostic viewpoint. Using variational autoencoders as a concrete example, we have shown how the presence of exact or approximate symmetries can manifest itself through self-organization of latent spaces, leading to hierarchies of relevance and effective dimensional reduction. Case studies ranging from simple geometric systems to realistic particle physics processes illustrate how conservation laws and kinematic constraints are reflected in learned representations. We have also stressed the limitations of this approach. Latent-space alignment with symmetry directions is neither guaranteed nor unique, and cannot be interpreted as the recovery of symmetry generators in a strict sense. Impossibility results for unsupervised disentanglement, finite data effects, and model dependence all impose fundamental con-straints on what can be inferred. For these reasons, data-driven symmetry diagnostics should be viewed as complementary to, rather than a replacement for, traditional analytic reasoning. Within these limits, however, the diagnostic framework reviewed here offers a useful new layer of analysis. By probing effective dimensionality and redundancy directly from data, it can help identify when symmetry-based descriptions are appropriate, guide the choice of modeling assumptions, and highlight regimes where unexpected structure may be present. As data volumes and complexity continue to grow across many areas of physics, such tools may become increasingly valuable in navigating the interface between empirical evidence and theoretical interpretation. In this sense, artificial intelligence does not challenge the central role of symmetry in physics. Instead, it provides new ways of interrogating how symmetry leaves its imprint on data—and of discovering when that imprint is present, subtle, or absent altogether. – 23 – Acknowledgments 

This work is supported by the grants PID2023-148162NB-C21 and CEX2023-001292-S from the Ministerio de Ciencia, Innovacion y Universidades. 

References 

[1] E. Noether, Invariante variationsprobleme , Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen (1918) 235–257. [2] S. Weinberg, The Quantum Theory of Fields, Volume I . Cambridge University Press, 1995. [3] S. Weinberg, The Quantum Theory of Fields, Volume II . Cambridge University Press, 1996. [4] Y. Bengio, A. Courville, and P. Vincent, Representation learning: A review and new perspectives , IEEE Transactions on Pattern Analysis and Machine Intelligence 35 (2013) 1798–1828. [5] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning . MIT Press, 2016. [6] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, Gradient-based learning applied to document recognition , Proceedings of the IEEE 86 (1998) 2278–2324. [7] T. Cohen and M. Welling, Group equivariant convolutional networks , in Proceedings of the 33rd International Conference on Machine Learning , 2016. [8] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veličković, Geometric deep learning: Grids, groups, graphs, geodesics, and gauges , arXiv:2104.13478 (2021). [9] T. Chen and et al., A simple framework for contrastive learning of visual representations ,

ICML (2020). [10] J.-B. Grill and et al., Bootstrap your own latent: A new approach to self-supervised learning ,

NeurIPS (2020). [11] I. Higgins and et al., β-vae: Learning basic visual concepts with a constrained variational framework , ICLR (2017). [12] X. Chen and et al., Infogan: Interpretable representation learning by information maximizing generative adversarial nets , NeurIPS (2016). [13] F. Locatello and et al., Challenging common assumptions in the unsupervised learning of disentangled representations , ICML (2019). [14] D. P. Kingma and M. Welling, Auto-encoding variational bayes , ICLR (2014). [15] V. Sanz, Learning symmetries in datasets , arXiv:2504.05174 .[16] G. Barenboim, J. Hirn, and V. Sanz, Symmetry meets AI , SciPost Phys. 11 (2021) 014, [arXiv:2103.06115 ]. [17] G. Barenboim, L. D. Debbio, J. Hirn, and V. Sanz, Exploring how a generative ai interprets music , Neural Computing and Applications 36 (2024), no. 27 17007–17022. [18] M. E. Peskin and D. V. Schroeder, An Introduction to Quantum Field Theory . Westview Press, 1995. [19] H. Georgi, Weak Interactions and Modern Particle Theory . Benjamin/Cummings, 1984. 

– 24 – [20] A. Bogatskiy, T. Hoffman, D. W. Miller, and J. T. Offermann, Pelican: Permutation equivariant and lorentz invariant or covariant aggregator network for particle physics ,

arXiv:2211.00454 (2022). [21] Z. Hao, R. Kansal, J. Duarte, and N. Chernyavskaya, Lorentz group equivariant autoencoders , Eur. Phys. J. C 83 (2023) 485. [22] B. M. Dillon, G. Kasieczka, H. Olischläger, T. Plehn, P. Sorrenson, and L. Vogel, 

Symmetries, safety, and self-supervision , SciPost Physics 12 (2022) 188. [23] B. M. Dillon, T. Plehn, C. Sauer, and P. Sorrenson, Better latent spaces for better autoencoders , SciPost Physics 11 (2021) 061. [24] P. Baldi and K. Hornik, Neural networks and principal component analysis: Learning from examples without local minima , Neural Networks 2 (1989) 53–58. [25] H. Kim and A. Mnih, Disentangling by factorising , ICML (2018). [26] R. T. Chen and et al., Isolating sources of disentanglement in variational autoencoders ,

NeurIPS (2018). [27] I. T. Jolliffe, Principal Component Analysis . Springer, 2 ed., 2002. [28] S. T. Roweis and L. K. Saul, Nonlinear dimensionality reduction by locally linear embedding ,

Science 290 (2000) 2323–2326. [29] J. B. Tenenbaum, V. de Silva, and J. C. Langford, A global geometric framework for nonlinear dimensionality reduction , Science 290 (2000) 2319–2323. [30] G. E. Hinton and R. Salakhutdinov, Reducing the dimensionality of data with neural networks , Science 313 (2006) 504–507. [31] P. Vincent and et al., Extracting and composing robust features with denoising autoencoders ,

ICML (2008). [32] R. Kondor and S. Trivedi, On the generalization of equivariance and convolution in neural networks to the action of compact groups , ICML (2018). [33] D. J. Rezende and S. Mohamed, Variational inference with normalizing flows , ICML (2015). [34] L. Dinh, J. Sohl-Dickstein, and S. Bengio, Density estimation using real nvp , ICLR (2017). [35] D. P. Kingma and P. Dhariwal, Glow: Generative flow with invertible 1x1 convolutions ,

NeurIPS (2018). [36] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models , NeurIPS (2020). [37] Y. Song and et al., Score-based generative modeling through stochastic differential equations ,

ICLR (2021). [38] J. Brehmer, G. Louppe, J. Pavez, and K. Cranmer, Mining gold from implicit models to improve likelihood-free inference , PNAS 117 (2020) 5242–5249. 

– 25 –