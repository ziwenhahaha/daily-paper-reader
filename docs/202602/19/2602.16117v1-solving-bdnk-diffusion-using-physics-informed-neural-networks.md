---
title: Solving BDNK diffusion using physics-informed neural networks
title_zh: 使用物理信息神经网络求解 BDNK 扩散方程
authors: "Vicente Chomalí-Castro, Nick Clarisse, Nicki Mullins, Jorge Noronha"
date: 2026-02-18
pdf: "https://arxiv.org/pdf/2602.16117v1"
tags: ["query:sr"]
score: 6.0
evidence: 用于求解相对论扩散方程的物理信息神经网络
tldr: 本研究针对相对论BDNK扩散方程，将其重构为通量保守形式，并提出SA-PINN-ACTO框架进行求解。该框架结合了自适应PINN技术与代数变换方法，实现了初始和边界条件的精确约束，使网络能专注于最小化偏微分方程残差。通过与二阶有限体积法对比，验证了该方法在平滑及不连续初始数据、动态背景下的有效性，并探讨了PINN在处理尖锐梯度时的局限性。
motivation: 旨在探索利用物理信息神经网络（PINN）求解复杂的相对论BDNK扩散方程，并提高其在处理边界条件时的准确性。
method: 将方程重构为通量保守形式，并引入结合自适应权重与代数变换约束条件的SA-PINN-ACTO框架进行数值求解。
result: 在平滑剖面下，SA-PINN-ACTO方法与高阶有限体积法解高度吻合，但在处理具有尖锐梯度的不连续剖面时误差有所增加。
conclusion: SA-PINN-ACTO框架能有效解决平滑的BDNK扩散问题，但PINN在处理强间断或高梯度物理场时仍面临挑战。
---

## 摘要
在这项工作中，我们将相对论性 BDNK（Bemfica-Disconzi-Noronha-Kovtun）扩散方程重新表述为通量守恒形式，并使用二阶 Kurganov-Tadmor 有限体积格式和物理信息神经网络（PINNs）求解了所得的 (1+1) 维方程。特别地，我们引入了 SA-PINN-ACTO 框架，该框架将自适应 PINN 技术与通过网络原始输出的代数变换来精确施加初始和周期性边界条件的方法相结合，使网络能够专注于最小化偏微分方程（PDE）的残差。我们在光滑和不连续的初始数据上测试了这两种方法，涵盖了平庸和动态演化的速度及温度 BDNK 背景，以及两种特征速度。对于光滑剖面，SA-PINN-ACTO 方法与收敛的 Kurganov-Tadmor 解相吻合；而对于不连续剖面，误差有所增加，这反映了 PINNs 在剧烈梯度附近预期的局限性。

## Abstract
In this work, we reformulate the relativistic BDNK (Bemfica-Disconzi-Noronha-Kovtun) diffusion equation in flux-conservative form, and solve the resulting equations in $(1+1)$D using both a second-order Kurganov-Tadmor finite volume scheme and physics-informed neural networks (PINNs). In particular, we introduce the SA-PINN-ACTO framework, which combines the self-adaptive PINN technique with an exact enforcement of initial and periodic boundary conditions through an algebraic transform of the network's raw output, allowing the network to focus solely on minimizing the PDE residual. We test both approaches on smooth and discontinuous initial data, for both trivial and dynamically evolving velocity and temperature BDNK backgrounds, and for two characteristic speeds. The SA-PINN-ACTO method matches the converged Kurganov-Tadmor solutions for smooth profiles, while for discontinuous profiles the errors increase, reflecting an expected limitation of PINNs near sharp gradients.