Title: A Graph Meta-Network for Learning on Kolmogorov-Arnold Networks

URL Source: https://arxiv.org/pdf/2602.16316v1

Published Time: Thu, 19 Feb 2026 01:37:06 GMT

Number of Pages: 25

Markdown Content:
Published as a conference paper at ICLR 2026 

# A GRAPH META -N ETWORK FOR LEARNING ON 

# KOLMOGOROV -A RNOLD NETWORKS 

Guy Bar-Shalom ‚àó

Technion, Meta 

Ami Tavory 

Meta 

Itay Evron 

Meta 

Maya Bechler-Speicher 

Meta 

Ido Guy 

Meta, Ben-Gurion University of the Negev 

Haggai Maron 

Technion 

## ABSTRACT 

Weight-space models learn directly from the parameters of neural networks, en-abling tasks such as predicting their accuracy on new datasets. Naive methods ‚Äì like applying MLPs to flattened parameters ‚Äì perform poorly, making the design of better weight-space architectures a central challenge. While prior work leveraged permutation symmetries in standard networks to guide such designs, no analo-gous analysis or tailored architecture yet exists for Kolmogorov‚ÄìArnold Networks (KANs). In this work, we show that KANs share the same permutation symme-tries as MLPs, and propose the KAN-graph , a graph representation of their compu-tation. Building on this, we develop WS-KAN, the first weight-space architecture that learns on KANs, which naturally accounts for their symmetry. We analyze WS-KAN‚Äôs expressive power, showing it can replicate an input KAN‚Äôs forward pass - a standard approach for assessing expressiveness in weight-space architec-tures. We construct a comprehensive ‚Äúzoo‚Äù of trained KANs spanning diverse tasks, which we use as benchmarks to empirically evaluate WS-KAN. Across all tasks, WS-KAN consistently outperforms structure-agnostic baselines, often by a substantial margin. Our code is available at https://github.com/BarSGuy/KAN-Graph-Metanetwork. 

## 1 INTRODUCTION 

Deep neural networks are now powerful tools for prediction, generation, and beyond. A recent perspective (Navon et al., 2023; Zhang et al., 2023) views their parameters not merely as weights, but as data ‚Äì enabling the design of weight-space models , which are networks that operate directly on the parameters of other networks. This shift makes it possible to predict test accuracy (Eilertsen et al., 2020; Unterthiner et al., 2020), generate new sets of weights (Erkoc ¬∏ et al., 2023), and classify or synthesize Implicit Neural Representations (INRs; Mescheder et al. 2019; Sitzmann et al. 2020), all through a single forward pass. A straightforward way to design weight-space (WS) models is to flatten all weights and biases into a single feature vector for prediction. However, this overlooks neuron permutation symmetries 

(Hecht-Nielsen, 1990; Brea et al., 2019), i.e., parameter transformations that keep the underlying function computed by the neural network unchanged. Thus, such naive models may produce differ-ent predictions for equivalent reorderings. While pioneering approaches used generic architectures to process model parameters (Sch¬® urholt et al., 2022c;b;e), more recent developments incorporate those networks‚Äô symmetries, either through weight sharing in linear layers (Navon et al., 2023; Zhou et al., 2023) (via geometric deep learning principles; Bronstein et al. 2021), or by treating networks as graphs (Lim et al., 2024; Kalogeropoulos et al., 2024; Zhang et al., 2023; Kofinas et al., 2024) and applying Graph Neural Networks (GNNs; Scarselli et al. 2008; Kipf 2016; Gilmer et al. 2017), which naturally respect these symmetries. Previous work on WS learning has only begun to explore the potential of characterizing and ap-plying WS models across different architectures. Initially, research has concentrated on MLPs with 

> ‚àó

This work was completed during an internship at Meta. 

1

> arXiv:2602.16316v1 [cs.LG] 18 Feb 2026

Published as a conference paper at ICLR 2026 ‚ãÆ

> ‚ãÆ

# ‚ãÆ                         

> Layer ùëô ‚àí1Layer ùëô Layer ùëô +1
> ùë£ ùëù
> ùëô
> ùë£ 1
> ùëô ‚àí1
> ùë£ ùëë ùëô ‚àí1
> ùëô ‚àí1
> ùúô ùëù ,1
> ùëô
> ùúô ùëù ,ùëë ùëô ‚àí1
> ùëô ‚ãÆ
> We construct the KAN -graph by setting its edge features as:
> ùëí ùëù ,ùëû
> ùëô ‚Üê‡∑®ùúô ùëù ,ùëû
> ùëô
> where ‡∑®ùúô ùëù ,ùëû
> ùëô is a vector representation of ùúô ùëù ,ùëû
> ùëô .
> ùë• ùëù
> ùëô =‡∑ç
> ùëû =1
> ùëë ùëô ‚àí1
> ùúô ùëù ,ùëû
> ùëô ùë• ùëû
> ùëô ‚àí1
> In a KAN, neuron ùëù of layer ùëô is computed as
> for some parametric function ùúô ùëù ,ùëû
> ùëô .
> ‡∑©
> ‡∑©

Figure 1: Constructing the KAN-graph for a given Kolmogorov-Arnold Network (KAN). straightforward extensions to CNNs (Navon et al., 2023; Zhou et al., 2023). Recent developments have expanded to transformer architectures (Tran et al., 2024) and architectures incorporating tensor symmetry structures (Zhou et al., 2024). However, designing WS models for diverse architectural paradigms remains an important open challenge that warrants further investigation. In this work, we introduce the first WS model specifically designed to process an emerging class of neural networks: Kolmogorov‚ÄìArnold Networks (KANs; Liu et al. 2025). Why design a weight-space model that takes KANs as input? KANs represent a fundamentally different neu-ral paradigm‚Äîrather than employing scalar weight matrices with fixed nonlinear activations, they construct networks from matrices of learnable univariate functions, while preserving the universal approximation properties of conventional neural networks (Kolmogorov, 1954; Braun & Griebel, 2009). This architectural shift yields compelling advantages over standard MLPs, including supe-rior parameter efficiency (Koenig et al., 2025), accelerated neural scaling (i.e., performance scales faster than expected as model size increases) (Liu et al., 2025), and notably, enhanced interpretability (BaraÀá sin et al., 2024). The learnable functions that replace scalar weights can be directly visualized and analyzed, offering unprecedented insight into the network‚Äôs decision-making process. As KANs gain traction within the deep learning community, we anticipate a proliferation of trained KAN models across diverse applications. WS learning will become increasingly valuable for help-ing practitioners understand, compare, and leverage these models effectively. However, developing weight-space models for KANs presents several challenges. The network architecture differs fun-damentally from those previously studied in the weight-space literature, as its learnable components are functions rather than simple scalar parameters. Additionally, the structural understanding from symmetry perspectives that have been developed for traditional neural networks remains largely unexplored for KANs. 

Our contributions. We begin by showing that KANs also exhibit permutation symmetries ‚Äì in fact, the same as conventional neural networks. Building on this insight, we introduce the KAN-graph ,a novel attributed graph with edge features that compactly encodes the structure of a given KAN; see Figure 1. On top of this representation, we develop WS-KAN, a GNN-based architecture capa-ble of learning directly over the KAN-graph . We show that WS-KAN applied to a KAN-graph can simulate the forward pass of the corresponding KAN, validating our approach from a theoretical per-spective, and laying the ground for stronger results such as functional approximation theorems. To validate our approach experimentally, we construct the first ‚Äúmodel zoo‚Äù of pre-trained KANs across diverse tasks, together with their corresponding KAN-graph representations serving as a benchmark. We show that WS-KAN consistently outperforms both generic baselines (such as MLPs over flat-tened parameters) and more sophisticated architectures, which effectively act as ablation studies and further validate our architectural design from the perspective of the symmetry analysis. Furthermore, because WS-KAN operates directly on graph representations through a GNN-based architecture, it can be seamlessly applied to KAN architectures of varying sizes, including those not encountered during training. We empirically demonstrate promising results in these settings. 

## 2 BACKGROUND AND RELATED WORK 

Equivariance and invariance in deep learning. Many learning tasks involve functions that ei-ther remain unchanged (invariant) or transform in a predictable manner (equivariant) under specific symmetries of the input. A classic example is the translation invariance of Convolutional Neural Networks (CNN; Krizhevsky et al. 2012), where shifting an image does not alter its label. While 2Published as a conference paper at ICLR 2026 simple MLPs can, in principle, learn such symmetries from data, this approach is often inefficient. By explicitly incorporating the symmetry into the model architecture Cohen et al. (2018); Maron et al. (2020); Zaheer et al. (2017); Ravanbakhsh et al. (2017), the property becomes an inherent fea-ture of the model rather than something that must be inferred during training. This typically leads to better generalization and data efficiency (Cohen & Welling, 2016; Brehmer et al., 2024). Building on these principles, a new class of models has recently emerged, referred to as weight-space models .

weight-space models. The weight-space of a neural network refers to the collection of parameters that fully define its architecture. For a multilayer perceptron (MLP), this is the set of weights and biases across all layers, Œ∏ = ( W1, b1, . . . , WL, bL). weight-space models are approaches that op-erate directly on this parameter space. While pioneering approaches proposed standard architectures for learning in WS (Sch¬® urholt et al., 2021; 2022d;a), more recent works (Navon et al., 2023; Zhou et al., 2023) have focused on the symmetries inherent in neural networks (Hecht-Nielsen, 1990; Brea et al., 2019), leading to tailored architectures that explicitly respect these symmetries. Several strategies have emerged: for instance, Navon et al. (2023); Zhou et al. (2023) enforce weight sharing in linear layers, while Graph Meta-Networks (Kalogeropoulos et al., 2024; Kofinas et al., 2024; Lim et al., 2024) leverage graph neural networks to operate directly on a model‚Äôs computational graph. In particular, Lim et al. (2024) has shown that those neural network symmetries correspond to graph automorphisms of their computational graphs. 

Kolmogorov‚ÄìArnold Networks (KANs). KANs are a recently introduced class of neural networks in which edges of the computational graph, rather than carrying simple numerical weights, abstractly represent univariate functions (Liu et al., 2025). Formally, given an input x ‚àà Rd, an L-layer KAN defines a function f as follows: 

f (x) = xL, where xlp =

> dl‚àí1

X

> q=1

œï lp,q 

 xl‚àí1

> q

, x0 = x, (1) where each œïl is a matrix of univariate functions of size dl √ó dl‚àí1. That is, every entry is a function 

œïlp,q : R ‚Üí R. Conveniently, and analogous to MLPs, the composition of such layers is defined: 

f (x) = ( œïL ‚ó¶ ¬∑ ¬∑ ¬∑ ‚ó¶ œï1)x, (2) where the operator ‚ó¶ denotes the application of a layer to its input. To clarify the notation, for a given layer l, we define  œïl(xl‚àí1) 

> p

:= Pdl‚àí1 

> q=1

œïlp,q (xl‚àí1 

> q

).Several parameterizations of these 1D functions are possible (Bozorgasl & Chen, 2024; Zhang et al., 2025). In this work, consistent with the original KAN paper, we adopt a B-spline -based parametriza-tion (Schoenberg, 1946), denoted B(x), which represents smooth piecewise polynomial functions over a domain. For a more detailed review of B-splines, see Section B. Specifically, we define the 1D function œà(¬∑) composing KANs as follows, 

œà(x) = wb b(x) + ws B(x); B(x) = ‚ü®c, B(x)‚ü© = X

> i

ciBi(x), (3) where wb, w s are learnable parameters, b(x) = silu( x) = x 

> 1+ e‚àíx

, and Bi(x) are pre-defined B-spline basis functions with ci as the learnable coefficients. 

## 3 LEARNING ON KAN PARAMETER SPACES 

Overview. We begin by analyzing the structure of KANs‚Äô parameters and demonstrate that per-muting hidden neurons does not alter the underlying function, mirroring the behavior observed in MLPs. Inspired by prior work on weight-space models for MLPs that introduced graph representa-tions (Lim et al., 2023; Kofinas et al., 2024), our approach can be presented as follows: we repre-sent the input KAN as a graph, where nodes correspond to individual neurons and edges represent the connections between them. The learned one-dimensional functions of the KAN are used to define the edge features (details follow). We refer to this construction as the KAN-graph ; see Fig-ure 1. Importantly, the permutation symmetries‚Äîthose that leave the underlying KAN function unchanged‚Äîcorrespond to permutations of hidden neurons within the KAN-graph , which likewise leave the graph itself unchanged. Thus, we employ a graph neural network-based technique to pro-cess the KAN-graph , leveraging their inherent equivariance to node permutations. 3Published as a conference paper at ICLR 2026 In what follows, we (1) demonstrate that the permutation symmetries present in MLPs also hold in KANs; (2) present a method for converting KANs into KAN-graphs ; and (3) introduce WS-KAN a GNN-based architecture for processing KAN-graphs and analyze its expressive power. 3.1 PERMUTATION SYMMETRIES IN KAN S

In a seminal work, Hecht-Nielsen (1990) observed that MLPs exhibit permutation symmetries: re-ordering the neurons within any hidden layer leaves the represented function unchanged. In this subsection, we make the observation that the same symmetry also holds for KANs. A KAN is fully specified by the collection of one-dimensional functions assigned to each in-put‚Äìoutput pair in every layer. For convenience, we denote the functions in an L-layer KAN by 

[œïl]l‚àà[L] (where [L] := {1, 2, . . . , L }). Given permutation matrices P1 and P2, we define their action on a matrix of univariate functions œï as, 

(P1œïP 2)p,q = œïœÉ‚àí11 (p),œÉ 2(q), (4) where œÉ1 and œÉ2 are the permutations associated with P1 and P2, respectively. Intuitively, this corresponds to reordering the rows and columns of œï according to the given permutations. Below, we formally state the permutation symmetries of KANs. 

Proposition 3.1 (KAN symmetries) . Let Œ∏ = ( œïL, . . . , œï1) denote the collection of parametric one-dimensional functions composing an L-layer KAN. Consider the group, G := Sd1 √ó Sd2 √ó¬∑ ¬∑ ¬∑ √ó SdL‚àí1 , the direct product of symmetric groups corresponding to the intermediate dimensions 

d1, . . . , d L‚àí1. Let g = ( P1, . . . , PL‚àí1) ‚àà G, where each Pl is the permutation matrix of œÉl ‚àà Sdl .Define the group action g ¬∑ Œ∏ = Œ∏‚Ä≤ with Œ∏‚Ä≤ = ( œï‚Ä≤L, . . . , œï‚Ä≤1) given by, 

œï‚Ä≤1 = P ‚ä§ 

> 1

œï1 , œï‚Ä≤l = P ‚ä§ 

> L

œïlPl‚àí1, ‚àÄl = 2 , . . . , L ‚àí 1 , œï‚Ä≤L = œïLPL‚àí1 .

Then, fŒ∏ (x) = fŒ∏‚Ä≤ (x) for all x.

A short proof is given in Section E for the case of a single hidden-layer KAN, and the extension to multiple hidden layers follows naturally. ùúô 1,1    

> 1ùúô 1,2
> 1
> ùúô 2,1
> 1ùúô 2,2
> 1
> ùúô 3,1
> 1ùúô 3,2
> 1
> ‚àòùë• 1
> ùë• 2

ùúô 2 ‚àò ùúô 1 ùíô      

> ùúô 1,1
> 2ùúô 1,2
> 2ùúô 1,3
> 2
> ùúô 2,1
> 2ùúô 2,2
> 2ùúô 2,3
> 2
> ùúô 3,1
> 1
> ùúô 3,2
> 1
> ùúô 1,1
> 1
> ùúô 1,2
> 1
> ùúô 1,2
> 2
> ùúô 1,3
> 2ùúô 2,2
> 2
> ùúô 2,3
> 2
> ùúô 1,1
> 2
> ùúô 2,1
> 2
> ùúô 2,1
> 1
> ùúô 2,2
> 1

Figure 2: Hidden neuron permutation symmetries in KANs. Intuitively, such permutations corre-spond to reordering nodes within hid-den layers. As an example, Fig-ure 2 illustrates a single‚Äìhidden-layer KAN, where the permutation matrix 

P corresponds to (1 , 3) . In other words, the first and third nodes are mapped to one another, while the sec-ond remains unchanged. Importantly, these permutation symmetries hold independently of the chosen parametrization of the 1D functions. In the next subsection, we introduce the KAN-graph , a graph representation of KANs that (i) com-pactly encodes their structure, and (ii) remains invariant under neuron permutations that do not alter the function computed by the KAN. 3.2 KAN-GRAPH 

The main idea behind the definition of the KAN-graph is relatively natural: as illustrated in Figure 1, nodes represent the KAN‚Äôs neurons and edges should represent the univariate functions. In this subsection, we formalize the construction by explicitly defining the nodes, edges, and edge features derived from those univariate functions. ùúô 1,1    

> 1ùúô 2,1
> 1ùúô 3,1
> 1
> ùúô 1,2
> 1ùúô 2,2
> 1ùúô 3,2
> 1

# ùüé 

# ùüé ùüé 

# ùüé 

# ùüé 

# ùüé 

# ùüé      

> ùëë 0ùëë 1ùëë 2
> ùëë 0
> ùëë 1
> ùëë 2
> ùúô 1,1
> 2ùúô 1,2
> 2
> ùúô 2,1
> 2ùúô 2,2
> 2
> ùúô 3,1
> 2ùúô 3,2
> 2

We define the KAN-graph as a directed graph G = (V, E ) where 

V represents the set of neurons, E represents the set of edges, and 

v ‚àà Rn√ódV , e ‚àà Rm√ódE are their corresponding features, respectively. Here, n, m denote the total number of nodes and edges in the graph, re-spectively. To clarify, in Figure 2(left), we have n = 7 nodes, m = 12 

edges, and the corresponding adjacency matrix is visualized inset. 1

> 1

The adjacency matrix of a given KAN-graph is extremely sparse: nonzeros appear only in the first super-diagonal blocks, specifically between blocks l and l + 1 for each l ‚àà [L].

4Published as a conference paper at ICLR 2026 

Univariate functions as edge features in the KAN-graph . To fully define the KAN-graph , we must specify how to incorporate the explicit parametrization of [œïl]l‚àà[L] characterizing the KAN into the KAN-graph. In this work, we focus on the parametrization introduced in the original KAN paper (Liu et al., 2025), wherein the learnable univariate functions are based on B-splines; as per Eq. (3). In this case, the learnable parameters composing those univariate functions can be conveniently ‚Äòcollected‚Äô into a vector Àúœïlp,q := [ wlb;p,q , w ls;p,q , clp,q ]. Thus, we define, for a layer l, an input node (or neuron) p, and an output node q, the following edge feature, 

elp,q = Àúœïlp,q := [ wlb;p,q , w ls;p,q , clp,q ], (5) which elegantly collects the learnable parameters of the one-dimensional function œïlp,q .3.3 LEARNING ON THE KAN-graph 

To learn on the KAN-graph , we adopt a general message-passing framework motivated by Gilmer et al. (2017), as follows (the letters a‚Äìd denote the order of execution), 

a) vF 

> i

‚Üê MLP (2; F)

> v



vi, X 

> j:e(i,j )‚ààE

MLP (1; F) 

> v

(vj , e(i,j ))



; b) vB 

> i

‚Üê MLP (2; B)

> v



vi, X 

> j:e(i,j )‚ààET

MLP (1; B) 

> v

(vj , e(i,j ))



;

c) e(i,j ) ‚Üê MLP e(vi, vj , e(i,j )); d) vi ‚Üê MLP (3)  

> v

(vi, vF 

> i

, vB 

> i

) .

Intuitively, node features are updated by aggregating information from both their outgoing and in-coming neighbors. Edge features, in turn, are refined based on the states of their endpoints as well as their own current representation. Each node‚Äôs representation is then updated by combining its in-trinsic features with the forward- and backward-aggregated information. We note that although the computational graph of KANs is inherently directed, with edges pointing from one layer to the next, we explicitly perform bidirectional message passing ‚Äì propagating information not only forward but also in reverse ‚Äì as this dual flow was found to enhance performance. 0

> 1
> 0
> 2
> 2
> 2
> 3
> 4
> 0
> 1
> 4
> 4

Figure 3: PE. 

Positional encodings (PE) as additional edge and node features. 

We follow prior work (Kofinas et al., 2024; Lim et al., 2024) and augment each node and edge of the KAN-graph with positional em-beddings that indicate their position in the computation flow. This breaks potential artificial symmetries that may arise in the KAN-graph. Specifically, all nodes within the same intermediate layer share a common positional embedding. By contrast, input and output nodes are assigned distinct embeddings, since permutations of these nodes generally alter the network‚Äôs function. For edges, we assign a unique identifier to each one, associated with its input and output nodes. Figure 3 depicts a possible assignment of positional encoding (via simple integers) to both nodes and edges for the running example of Figure 2(left). Intuitively, permuting neurons within a hidden layer leaves the feature-augmented graph (i.e., its adjacency matrix) unchanged. In contrast, permuting a node from the first layer with one from the last does alter the graph. 

## 4 EXPRESSIVE POWER OF WS-KAN 

While there are multiple ways to design a WS architecture for processing KANs, we adopted one 

specific approach, which stems from equivariance principles, and models the KAN as a graph. How-ever, a question arises: is this the right design choice? 

In general, imposing group-equivariance constraints might reduce a model‚Äôs expressive power (Maron et al., 2019; Xu et al., 2019; Morris et al., 2019). In the weight-space literature, this ex-pressive power is often analyzed by demonstrating that the architecture can simulate (i.e., approxi-mate) the forward pass of a given input model. For example, Lim et al. (2024); Navon et al. (2023) establish such results in the context of processing standard neural networks. Such approximation results typically serve as an intermediate step toward proving stronger results, such as functional approximation theorems (Navon et al., 2023). 5Published as a conference paper at ICLR 2026 In this section, we show that WS-KAN can simulate a forward pass for a given input KAN. We begin by showing that there exists a set of weights for a single-hidden-layer MLP that can approximate any univariate function based on B-splines (under mild assumptions), as per Eq. (3), to arbitrary precision. We then use this result to prove our main result: WS-KAN can simulate the forward pass of the original KAN. All proofs are provided in Section E. 

Lemma 4.1 (MLP as an approximation of the univariate functions composing the KAN) . Let B

denote the family of cardinal B-splines of degree k, defined on a fixed grid G over the domain 

[a, b ], and parameterized by coefficients c = [ c0, . . . , c G+k‚àí1] ‚àà C ‚äÇ RG+k, where the set of admissible coefficients lies in a compact domain C. Consider the function œà : R ‚Üí R defined as 

œà(x) = wb b(x) + ws B(x), where wb, w s ‚àà W ‚äÇ R for some compact set W. b(x) denotes the silo function, and B(x) denotes the B-spline. Then, for any Œµ > 0, and for any compact domain X

there exists a set of weights for a multilayer perceptron MLP : RG+k+3 ‚Üí R such that, 

sup x‚ààX MLP (x, w s, w b, c) ‚àí œà(x) < Œµ. 

Theorem 4.1 essentially states that, under the assumptions above, there exists an MLP that effectively computes the univariate functions composing our input KANs, over any chosen compact domain X .Consequently, we show that under mild conditions, WS-KAN can approximate a forward pass of the input KAN model. 

Proposition 4.2 (WS-KAN can simulate the forward pass of KANs) . Let fŒ∏ be a given KAN ar-chitecture, defined over an input domain [a, b ]n, where each univariate function is represented by a B-spline from the family B. Let G be its KAN-graph , where the nodes in the first layer are enhanced with the input x ‚àà [a, b ]n. For every Œµ > 0, there exists a WS-KAN such that, 

sup x‚àà[a,b ]n WS-KAN (G) ‚àí fŒ∏ (x) < Œµ. 

Importantly, Theorem 4.2 holds for any choice of parameters defining the KANs‚Äô univariate func-tions (Eq. (3)). Below we present the proof idea, while the full proof can be found in Section E. 

Proof idea. A KAN computation can be viewed as a composition of L continuous functions (layers). Using Theorem 4.1, we show that WS-KAN can approximate each individual layer to arbitrary precision. Then, by applying standard techniques (e.g., ideas from Lemma 6 of Lim et al. 2022) and the message passing mechanism, we construct approximations layer by layer, ensuring that the overall WS-KAN output remains arbitrarily close to that of the original KAN. 

## 5 EXPERIMENTS 

While various ‚Äúmodel zoos‚Äù (e.g., Sch¬® urholt et al., 2022e) exist for benchmarking weight-space models in conventional neural networks, no comparable resource has yet been developed for KANs. Nor are there established baselines against which WS-KAN can be evaluated. To address this gap, we take inspiration from tasks and baselines explored in the weight-space literature for con-ventional neural networks and construct several families of model zoos of trained KANs. These zoos are designed to capture both invariant and equivariant tasks and are built from five datasets: MNIST (LeCun et al., 1998), Fashion-MNIST (F-MNIST; Xiao et al. 2017), Kuzushiji-MNIST (K-MNIST; Clanuwat et al. 2018), CIFAR10 (Krizhevsky et al., 2009), and a synthetic dataset that we designed inspired by the one in Navon et al. (2023). We focus on two invariant problems‚ÄîINR classification (Section 5.1) and accuracy prediction (Sec-tion 5.2)‚Äîand one equivariant task‚Äîpruning, where the goal is to predict a pruning mask directly from the KAN‚Äôs weights (Section 5.3). We also evaluate WS-KAN‚Äôs ability to generalize to KAN architectures unseen during training (Section 5.1.1). For each model in the zoo, we also construct the corresponding KAN-graph. We benchmark WS-KAN against the following baselines. 

Standard baselines: (1) MLP : A simple multilayer perceptron applied to a vectorized representation of the KAN‚Äôs parameters. (2) MLP + Aug. : The same MLP as above, but trained with permuta-tion augmentation, i.e., randomly permuting the input KAN in ways that preserve its underlying function. (3) MLP + Align. : Inspired by alignment techniques for MLPs (Ainsworth et al., 2022), where parameters are reordered to maximize model similarity, we extend this idea to KANs. The 6Published as a conference paper at ICLR 2026 main challenge is that KAN parameters are functions rather than scalars, and therefore alignment re-quires defining distances between functions. Full details are in Section D. We consider as a baseline an MLP applied to aligned KANs. (4) DMC (Eilertsen et al., 2020): a convolutional layer applied to the (vectorized) model parameters. Ablation baselines: We introduce two additional baselines to ablate our architectural design choice. (5) DS (DeepSets) : A DeepSets (Zaheer et al., 2017) ar-chitecture applied to the graph‚Äôs edge features. Importantly, while this baseline is invariant to KAN permutation symmetries, it is also invariant to many more permutations that do not correspond to these symmetries, and completely neglects the graph topology. (6) SetTrans : Similar to (5), but employing a transformer architecture (Vaswani et al., 2017) over the set of edges. We note that this baseline is only feasible for relatively small input KANs, since the attention matrix scales quadrat-ically with the number of neurons. 2 Thus, SetTrans is reported only when feasible. Additionally, we ablate key design choices in WS-KAN, including positional encoding and bidirectional message passing; see Section C.4 for details. In the following sections, we provide additional details on the tasks considered, describe the con-struction of the KAN training datasets, and present our results. For each experiment, we report mean performance over three seeds, with error bars for standard deviation. All test results were obtained by optimizing for validation performance. Additional experimental details, including the hyperparameter grid, implementation notes, and extended results, are available in Section C 3.5.1 INR CLASSIFICATION 

First, we evaluate WS-KAN on INR (Mescheder et al. 2019; Sitzmann et al. 2020) classification. 

What is an INR? For a given image, an INR learns a mapping from any input coordinate to the corre-sponding grayscale (or RGB) value of that coordi-nate in the image. See inset (top). At the bottom inset, we show example reconstructions produced by KAN-based INRs over CIFAR10, F-MNIST, MNIST ‚Äì left corresponds to ground truth image, and right corresponds to the reconstructed one. Weight -space           

> architecture
> (e.g., our WS -KAN )
> ùë• =
> ùë• =
> KANs trained on different images trained to predict original class
> ,ùë¶ =5
> ,ùë¶ =8
> INR training
> INR training
> INR
> classifier

Setup and dataset construction. The setup is il-lustrated inset. For this task, we convert the follow-ing datasets to KAN-based INRs: Sine waves (a syn-thetic dataset), MNIST, F-MNIST, and CIFAR10. Taking MNIST as an example, the key idea is that for each image in the dataset, we train an independent KAN-based INR to ‚Äòreconstruct‚Äô it. Once this zoo of INRs is constructed, we train the weight-space model under study (e.g., WS-KAN) to classify the digit, using as input the parameters of the KAN-based INR (or the KAN-graph when WS-KAN is tested) rather than the raw pixel data. See Section C.1 for dataset split and details. Table 1: INR classification accuracy. 

Method MNIST F-MNIST CIFAR-10 

MLP 34.1 ¬±0.1 41.3 ¬±0.3 16.8 ¬±0.1 

MLP + Aug. 62.7 ¬±1.1 63.0 ¬±0.3 28.2 ¬±0.7 

MLP + Align. 81.0 ¬±0.1 73.6 ¬±0.2 30.0 ¬±0.2 

DMC 73.4 ¬±3.0 73.1 ¬±1.0 33.0 ¬±0.7 

DS (Ours) 59.1 ¬±3.3 65.9 ¬±0.8 23.2 ¬±3.9 

SetTrans (Ours) 87.5 ¬±0.8 80.2 ¬±0.1 34.3 ¬±0.7 

WS-KAN (Ours) 94.3 ¬±0.5 84.6 ¬±0.6 42.2 ¬±0.8 

Results. Table 1 reports the accuracy of pre-dicting the class (e.g., the digit in MNIST) from the weights of the KAN-based INR. WS-KAN outperforms all baselines by a large mar-gin. SetTrans ranks second, suggesting that ex-plicitly accounting for symmetries is advanta-geous, albeit suboptimal, as the symmetries it captures are broader than the KAN permutation symmetries. Finally, we observe that ‚Äú MLP + Align. > MLP + Aug. > MLP ‚Äù, aligning with intuition and validating the effectiveness of our alignment technique. Results over the synthetic dataset are provided in Section C.1.3. 

> 2

In practice, we found this approach computationally expensive‚Äîtraining a single epoch could take up to one hour in some experiments‚Äîmaking it significantly slower than WS-KAN and other baselines. 

> 3

Our code, including model zoo construction, is available at https://github.com/BarSGuy/KAN-Graph-Metanetwork. 

7Published as a conference paper at ICLR 2026 5.1.1 OUT -OF -DISTRIBUTION GENERALIZATION TO WIDER KAN S

Since WS-KAN operates on KAN-graphs via a GNN-based architecture, it can naturally be ap-plied to KAN-graphs of varying topologies, e.g., differing in hidden-layer width or number of lay-ers. Hence, here we evaluate WS-KAN‚Äôs ability to generalize to KAN architectures larger than those seen during training, focusing on the INR classification task over the MNIST and F-MNIST datasets. Our KAN-based INRs from Section 5.1 share the topology [2 ‚Üí h ‚Üí h ‚Üí 10] , where 

h denotes the hidden-layer width, with h = 32 . Here, we take those WS-KAN‚Äôs trained on the 

h = 32 KANs and evaluate it on progressively wider KAN architectures with h ‚àà { 48 , 64 , 80 , 96 },none of which were seen during training. 

Results. Table 2 reports the out-of-distribution (OOD) accuracy of WS-KAN. We observe promis-ing OOD generalization, which, as expected, degrades as h increases and the distribution shift from the training setting ( h = 32 ) grows. Table 2: OOD generalization for INR classification. Test accuracies of WS-KAN on the INR classification task when trained on KANs with hidden width h = 32 and evaluated on wider, previously unseen architectures ( h ‚àà {48 , 64 , 80 , 96 }). All architectures follow the topology [2 ‚Üí h ‚Üí h ‚Üí 10] . The blue column denotes the in-distribution setting , while red columns denote out-of-distribution setting . 

Dataset h = 32 h = 48 h = 64 h = 80 h = 96 

MNIST 94.3 ¬± 0.5 91.4 ¬± 0.5 81.0 ¬± 3.2 67.0 ¬± 4.3 57.1 ¬± 6.1 F-MNIST 84.6 ¬± 0.6 84.6 ¬± 0.6 84.3 ¬± 0.7 83.3 ¬± 0.8 82.2 ¬± 0.7 5.2 ACCURACY PREDICTION 

Here, we consider the task of predicting the accuracy of a given KAN, based on its parameters. 

Setup and dataset construction. We experiment on MNIST, F-MNIST, and K-MNIST. 0 20 40 60 80 100 

> Noisy labels (%)
> 0
> 20
> 40
> 60
> 80
> 100
> Test accuracy (%)

Over each dataset, we train 4000 KAN models with a3000/500/500 train/validation/test split. We observed that dif-ferent KANs trained on these datasets yield similar accuracies. Thus, to make predicting accuracy from parameters more chal-lenging, we introduce label noise: for each KAN, we randomly sample portions of the training data and shuffle their labels. As shown inset, this results in a diverse set of trained KANs with varying test accuracies (results for other datasets provided in Figure 9 in Section C). The training pipeline is illustrated in Figure 10 in Section C.2. 

Results. To test how well WS-KAN predicts the test accuracies, we follow Lim et al. (2024) and report two metrics: Mean Squared Error (MSE) and R-squared ( R2). The results, summarized in Table 3, exhibit the same trend as in INR classification: WS-KAN consistently achieves the best performance across all datasets. The ordering of baselines remains unchanged, with our ablation baseline (DS) ranking second, followed by MLP + Alignment as the next most effective approach. Table 3: Accuracy prediction . Comparison of MSE and R2 across datasets. 

MSE (lower is better; ‚Üì) [ √ó10 3] R2 (higher is better; ‚Üë) [ √ó10 2]

Method MNIST F-MNIST K-MNIST MNIST F-MNIST K-MNIST MLP 14.58 ¬±0.02 11.55 ¬±0.12 6.68 ¬±0.16 76.99 ¬±0.04 69.59 ¬±0.33 80.13 ¬±0.49 

MLP + Aug. 10.41 ¬±0.24 8.86 ¬±1.47 5.33 ¬±0.19 83.56 ¬±0.38 76.66 ¬±3.88 84.15 ¬±0.55 

MLP + Align. 5.26 ¬±0.09 6.32 ¬±0.15 3.33 ¬±0.10 91.70 ¬±0.15 83.37 ¬±0.41 90.11 ¬±0.31 

DMC 7.00 ¬±0.06 6.46 ¬±0.32 3.27 ¬±0.05 88.95 ¬±0.09 82.99 ¬±0.85 90.29 ¬±0.14 

DS (Ours) 3.29 ¬±0.12 3.90 ¬±0.04 2.00 ¬±0.11 94.81 ¬±0.18 89.73 ¬±0.11 94.07 ¬±0.32 

WS-KAN (Ours) 3.29 ¬±0.17 2.94 ¬±0.13 1.45 ¬±0.08 94.81 ¬±0.27 92.27 ¬±0.35 95.69 ¬±0.24 

8Published as a conference paper at ICLR 2026 5.3 PRUNING MASK PREDICTION 

In this section, we tackle the challenging equivariant task of network pruning for KANs, aiming to discard a subset of weights without significantly degrading performance. 

Motivation. Most pruning methods are data-driven, requiring large amounts of data to determine which parameters to remove. For instance, activation-based approaches rely on recorded activation values, while gradient-based methods depend on training loss gradients. In contrast, pruning a model using only a simple forward pass, as enabled by WS-KAN (demonstrated below), is especially valuable, as it avoids repeated, data-intensive passes. 

Setup. Here, we use the same datasets as in Section 5.2. Supervision is obtained by applying a data-driven edge-based pruning algorithm for KANs 4, which we denote as Oracle-prune . The core idea behind this algorithm is straightforward: it removes edges whose average activation values, computed from training data, fall below a predefined threshold (set to 0.01 in all experiments). We refer to this pruning method as Oracle-pruning , as it is our supervision. The oracle pruning algorithm outputs a binary mask, where edges marked with 0 are pruned and those marked with 1 are retained. The task of interest is to predict this mask, see Figure 11 in Section C.3 for the training pipeline. Crucially, this task is equivariant: prediction is made for each individual edge of the KAN, rather than as a single prediction for the entire network. We are interested in evaluating two aspects: 

(i) how accurately WS-KAN predicts the pruning mask, and (ii) whether using the mask generated by WS-KAN leads to effective downstream pruning performance. Table 4: Pruning mask prediction.                                            

> Accuracy ( ‚Üë, %) ROC-AUC ( ‚Üë, %) Method MNIST F-MNIST K-MNIST MNIST F-MNIST K-MNIST MLP 93.10 ¬±<0.01 96.65 ¬±<0.01 91.39 ¬±<0.01 87.12 ¬±0.04 84.92 ¬±0.38 75.32 ¬±0.02
> MLP + Aug. 93.29 ¬±<0.01 96.65 ¬±<0.01 91.39 ¬±<0.01 91.36 ¬±0.12 86.21 ¬±0.09 74.89 ¬±0.01
> MLP + Align. 93.57 ¬±0.01 96.64 ¬±<0.01 91.52 ¬±0.01 93.00 ¬±0.03 91.66 ¬±0.10 82.76 ¬±0.05
> DMC 93.07 ¬±<0.01 96.59 ¬±<0.01 91.39 ¬±<0.01 84.27 ¬±0.06 84.39 ¬±0.01 75.06 ¬±0.02
> DS (Ours) 94.34 ¬±0.02 96.90 ¬±0.09 94.34 ¬±0.02 95.45 ¬±0.05 95.81 ¬±0.04 95.45 ¬±0.05
> WS-KAN (Ours) 97.93 ¬±0.19 98.93 ¬±0.05 97.72 ¬±0.14 99.54 ¬±0.01 99.72 ¬±0.02 99.46 ¬±0.09

Results (i). We evaluate mask prediction as a binary classification task using the metrics ROC-AUC and Accuracy. Results are provided in Table 4. WS-KAN consistently outperforms all baselines across all datasets and evaluation metrics. Again, the DS approach emerges as the second best over-all. Notably, the hierarchy among the MLP variants mirrors our earlier findings, further supporting the effectiveness of our alignment strategy. 

Results for downstream pruning performance (ii). To properly evaluate downstream pruning, it is not enough to only report the downstream accuracy achieved by applying a mask. A trivial mask that leaves all weights untouched would naturally yield high accuracy, but would provide no practical benefit. A meaningful mask must therefore strike a balance: it should maintain strong downstream accuracy while also inducing sparsity in the model. We now evaluate how well WS-KAN achieves this trade-off, and present two complementary plots. In Figure 5a, we report the downstream accuracy (y-axis) achieved by pruned models across different noise levels in the training labels (x-axis, binned in 20% non-overlapping intervals). In Figure 5b, we show the corresponding fraction of weights retained by each method. Together, these plots illustrate both the accuracy‚Äìsparsity trade-off and how each method compares against the oracle-pruning baseline. It is clear that WS-KAN most closely follows the accuracy‚Äìsparsity trade-off of the oracle-pruning technique. Moreover, DS consistently ranks second best, whereas all other approaches perform poorly. The only partial exception is MLP + Alignment, which shows some utility but still falls significantly behind both DS and WS-KAN. Additional downstream pruning results are available in Section C. Importantly, we observe that WS-KAN offers a significant timing advantage over Oracle Prune , being up to five orders of magnitude faster (Figure 5c).  

> 4This (off-the-shelf) pruning algorithm is found in https://github.com/KindXiaoming/pykan

9Published as a conference paper at ICLR 2026 0% 20% 40% 60% 80% 100% 

> Noisy labels (%)
> 0
> 20
> 40
> 60
> 80
> Test accuracy (%)
> Random Classifier
> WS-KAN (Ours)
> MLP
> MLP + Aug.
> MLP + Align.
> DeepSets (Ours)
> DMC
> Original KAN
> Oracle prune

(a) Downstream prune accuracy. 0% 20% 40% 60% 80% 100%  

> Noisy labels (%)
> 0
> 2
> 4
> 6
> 8
> Weights (%) Kept
> WS-KAN (Ours)
> MLP
> MLP + Aug.
> MLP + Align.
> DeepSets (Ours)
> DMC
> Oracle prune

(b) Downstream prune kept weight %. Oracle prune      

> WS-KAN
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> 10 1
> Time (seconds, log scale)
> 7.20 s
> 1.6e-04 s

(c) Prune timing. 

Figure 5: Downstream pruning performance across methods over KANs trained on MNIST. 

We report: (i) Test accuracy : the downstream accuracy of pruned networks, averaged over non-overlapping bins of 20%, to highlight the relative effectiveness of pruning strategies under varying noise levels ‚Äì Figure 5a; (ii) Kept weights : the percentage of weights retained after pruning, aver-aged over the same bins as in (i) ‚Äì Figure 5b; and (iii) Pruning time (‚Üì): the computational cost comparison (log scale) in seconds, between WS-KAN and Oracle prune ‚Äì Figure 5c, low is better. 

Discussion. WS-KAN consistently provides the most effective WS technique for learning over KANs. Importantly, our ablation baselines (DS/SetTrans) are generally second-best. This under-scores the value of incorporating the structural properties of KANs, since even a suboptimal inte-gration (as per DS/SetTrans) still proves beneficial to some extent. Although our alignment method (see Section D) does not outperform structure-aware approaches, it remains a strong alternative. Across nearly all tasks/datasets/metrics combinations, we consistently observe the ordering: ‚Äú MLP + Align. > MLP + Aug. > MLP ‚Äù. Finally, we note that WS-KAN is computationally efficient, with complexity scaling linearly in the number of edges of the KAN-graph; see Section C.5 for a detailed complexity and runtime analysis. 

## 6 CONCLUSIONS 

We addressed the development of weight-space models for KANs by performing a symmetry anal-ysis that guided the design of our WS-KAN architecture. We also built comprehensive model zoos to support future research and evaluation of weight-space models for KANs. Several promising future directions remain open. While we have shown that WS-KAN can gen-eralize to wider KAN architectures unseen during training (Section 5.1.1), extending this to deeper architectures and broader topology variations remains an exciting avenue. Beyond this, the availabil-ity of weight-space models for both KANs and MLPs naturally invites the study of transformations between the two, enabling us to leverage their complementary strengths. Many analytical tools are well established for MLPs; by converting a KAN into an equivalent MLP (while preserving its func-tion), these tools can be used to study the original KAN. Conversely, converting an MLP into a KAN allows us to exploit the interpretability of KANs (BaraÀá sin et al., 2024), offering new insights into the MLP. Finally, evaluating WS-KAN‚Äôs generalization to other KANs ‚Äì e.g., CNN-KANs ‚Äì would be an interesting direction. 

## REPRODUCIBILITY STATEMENT 

Our code is available at https://github.com/BarSGuy/KAN-Graph-Metanetwork. All implementa-tion details required to replicate the results and evaluations are provided in Section C. 10 Published as a conference paper at ICLR 2026 

## REFERENCES 

Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. arXiv preprint arXiv:2209.04836 , 2022. Irina BaraÀá sin, BlaÀá z BertalaniÀá c, Mihael MohorÀá ciÀá c, and Carolina Fortuna. Exploring kolmogorov-arnold networks for interpretable time series classification. arXiv preprint arXiv:2411.14904 ,2024. Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www. wandb.com/ . Software available from wandb.com. Zavareh Bozorgasl and Hao Chen. Wav-kan: Wavelet kolmogorov-arnold networks. arXiv preprint arXiv:2405.12832 , 2024. URL https://arxiv.org/abs/2405.12832 .J¬® urgen Braun and Michael Griebel. On a constructive proof of kolmogorov‚Äôs superposition theorem. 

Constructive approximation , 30(3):653‚Äì675, 2009. Johanni Brea, Berfin Simsek, Bernd Illing, and Wulfram Gerstner. Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss land-scape. arXiv preprint arXiv:1907.02911 , 2019. Johann Brehmer, S¬® onke Behrends, Pim De Haan, and Taco Cohen. Does equivariance matter at scale? arXiv preprint arXiv:2410.23179 , 2024. Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar VeliÀá ckovi¬¥ c. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478 , 2021. Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep learning for classical japanese literature, 2018. Taco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-ence on machine learning , pp. 2990‚Äì2999. PMLR, 2016. Taco S. Cohen, Mario Geiger, Jonas K¬® ohler, and Max Welling. Spherical cnns. In 6th International Conference on Learning Representations, ICLR , 2018. George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals, and systems , 2(4):303‚Äì314, 1989. Carl De Boor. A practical guide to splines , volume 27. springer New York, 1978. Gabriel Eilertsen, Daniel J¬® onsson, Timo Ropinski, Jonas Unger, and Anders Ynnerman. Classify-ing the classifier: dissecting the weight space of neural networks. In European Conference on Artificial Intelligence (ECAI 2020) , volume 325, pp. 1119‚Äì1926, 2020. Ziya Erkoc ¬∏, Fangchang Ma, Qi Shan, Matthias Nie√üner, and Angela Dai. Hyperdiffusion: Generat-ing implicit neural fields with weight-space diffusion. In Proceedings of the IEEE/CVF interna-tional conference on computer vision , pp. 14300‚Äì14310, 2023. Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with PyTorch Geometric. In 

ICLR Workshop on Representation Learning on Graphs and Manifolds , 2019. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning , pp. 1263‚Äì1272. Pmlr, 2017. Robert Hecht-Nielsen. On the algebraic structure of feedforward network weight spaces. In Ad-vanced Neural Computers , pp. 129‚Äì135. Elsevier, 1990. Ioannis Kalogeropoulos, Giorgos Bouritsas, and Yannis Panagakis. Scale equivariant graph metanet-works. Advances in neural information processing systems , 37:106800‚Äì106840, 2024. TN Kipf. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 , 2016. 11 Published as a conference paper at ICLR 2026 Benjamin C Koenig, Suyong Kim, and Sili Deng. Leankan: A parameter-lean kolmogorov-arnold network layer with improved memory efficiency and convergence behavior. arXiv preprint arXiv:2502.17844 , 2025. Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J Burghouts, Efstratios Gavves, Cees GM Snoek, and David W Zhang. Graph neural networks for learning equivariant represen-tations of neural networks. arXiv preprint arXiv:2403.12143 , 2024. Andrej Nikolajewitsch Kolmogorov. On the conservation of conditionally periodic motions for a small change in hamilton‚Äôs function [en rus]. In Dokl. Akad. Nauk SSSR , volume 98, pp. 525‚Äì 530, 1954. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-lutional neural networks. Advances in neural information processing systems , 25, 2012. Yann LeCun, L¬¥ eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE , 86(11):2278‚Äì2324, 1998. Derek Lim, Joshua Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, and Stefanie Jegelka. Sign and basis invariant networks for spectral graph representation learning. arXiv preprint arXiv:2202.13013 , 2022. Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, and James Lucas. Graph metanetworks for processing diverse neural architectures, 2023. URL https://arxiv.org/abs/2312. 04501 .Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, and James Lucas. Graph metanetworks for processing diverse neural architectures. In The Twelfth International Conference on Learning Representations , 2024. Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljacic, Thomas Y. Hou, and Max Tegmark. KAN: Kolmogorov‚Äìarnold networks. In The Thirteenth International Conference on Learning Representations , 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR , 2019. Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant networks. In International conference on machine learning , pp. 4363‚Äì4371. PMLR, 2019. Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements. In International Conference on Machine Learning , pp. 6734‚Äì6744. PMLR, 2020. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Oc-cupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 4460‚Äì4470, 2019. Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In Proceedings of the AAAI conference on artificial intelligence , volume 33, pp. 4602‚Äì4609, 2019. Aviv Navon, Aviv Shamsian, Idan Achituve, Ethan Fetaya, Gal Chechik, and Haggai Maron. Equiv-ariant architectures for learning in deep weight spaces. In International Conference on Machine Learning , pp. 25790‚Äì25816. PMLR, 2023. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems , 32, 2019. Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parameter-sharing. In International conference on machine learning , pp. 2892‚Äì2901. PMLR, 2017. 12 Published as a conference paper at ICLR 2026 Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks , 20(1):61‚Äì80, 2008. Isaac J Schoenberg. Contributions to the problem of approximation of equidistant data by analytic functions. part a. on the problem of smoothing or graduation. a first class of analytic approxima-tion formulae. Quarterly of applied mathematics , 4(1):45‚Äì99, 1946. Konstantin Sch¬® urholt, Dimche Kostadinov, and Damian Borth. Self-supervised representation learn-ing on neural network weights for model characteristic prediction. Advances in Neural Informa-tion Processing Systems , 34:16481‚Äì16493, 2021. Konstantin Sch¬® urholt, Boris Knyazev, Xavier Gir¬¥ o-i Nieto, and Damian Borth. Hyper-representations as generative models: Sampling unseen neural network weights. arXiv preprint arXiv:2209.14733 , 2022a. Konstantin Sch¬® urholt, Boris Knyazev, Xavier Gir¬¥ o i Nieto, and Damian Borth. Hyper-representations as generative models: Sampling unseen neural network weights, 2022b. Konstantin Sch¬® urholt, Dimche Kostadinov, and Damian Borth. Hyper-representations: Self-supervised representation learning on neural network weights for model characteristic prediction, 2022c. URL https://arxiv.org/abs/2110.15288 .Konstantin Sch¬® urholt, Diyar Taskiran, Boris Knyazev, Xavier Gir¬¥ o-i Nieto, and Damian Borth. Model zoos: A dataset of diverse populations of neural network models. arXiv preprint arXiv:2209.14764 , 2022d. Konstantin Sch¬® urholt, Diyar Taskiran, Boris Knyazev, Xavier Gir¬¥ o i Nieto, and Damian Borth. Model zoos: A dataset of diverse populations of neural network models, 2022e. URL https: //arxiv.org/abs/2209.14764 .Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-plicit neural representations with periodic activation functions. Advances in Neural Information Processing Systems , 33:7462‚Äì7473, 2020. Viet-Hoang Tran, Thieu N Vo, An Nguyen The, Tho Tran Huu, Minh-Khoi Nguyen-Nhat, Thanh Tran, Duy-Tung Pham, and Tan Minh Nguyen. Equivariant neural functional networks for trans-formers. arXiv preprint arXiv:2410.04209 , 2024. Thomas Unterthiner, Daniel Keysers, Sylvain Gelly, Olivier Bousquet, and Ilya Tolstikhin. Predict-ing neural network accuracy from weights. arXiv preprint arXiv:2002.11448 , 2020. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-tion processing systems , 30, 2017. Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In 7th International Conference on Learning Representations, ICLR , 2019. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. Advances in neural information processing systems , 30, 2017. David W Zhang, Miltiadis Kofinas, Yan Zhang, Yunlu Chen, Gertjan J Burghouts, and Cees GM Snoek. Neural networks are graphs! graph neural networks for equivariant processing of neural networks. 2023. Jusheng Zhang, Yijia Fan, Kaitong Cai, and Keze Wang. Kolmogorov-arnold fourier networks. 

arXiv preprint arXiv:2502.06018 , 2025. Allan Zhou, Kaien Yang, Kaylee Burns, Adriano Cardace, Yiding Jiang, Samuel Sokota, J Zico Kolter, and Chelsea Finn. Permutation equivariant neural functionals. Advances in neural infor-mation processing systems , 36:24966‚Äì24992, 2023. Allan Zhou, Chelsea Finn, and James Harrison. Universal neural functionals. Advances in neural information processing systems , 37:104754‚Äì104775, 2024. 13 Published as a conference paper at ICLR 2026 

## A APPENDIX ROADMAP 

This appendix gathers the mathematical background, full experimental details, the alignment proce-dure, and proofs that support the main text. See guide below. ‚Ä¢ Section B (B-splines). Reviews uniform B-splines and clarifies how they parameterize the univariate functions in KANs. ‚Ä¢ Section C (Extended Experimental Section). Provides all details to create the model zoos for each task and reproduce all results: 

‚Äì Section C.1 ‚Äì INR classification. 

‚Äì Section C.2 ‚Äì Accuracy prediction. 

‚Äì Section C.3 ‚Äì Pruning mask prediction. 

‚Äì Section C.4 ‚Äì Ablation: positional encoding and bidirectional message passing. 

‚Äì Section C.5 ‚Äì Complexity and runtime analysis. ‚Ä¢ Section D (Aligning Kolmogorov-Arnold Networks). Here we extend the alignment ideas presented in Ainsworth et al. (2022) to KANs. 

‚Äì Section D.1 (Aligning a model zoo or a dataset). Here we describe how our align-ment procedure discussed in Section D can be applied to an entire dataset of trained KANs. ‚Ä¢ Section E (Proofs). Collects all formal propositions and proofs. ‚Ä¢ Section F (Large Language Model (LLM) Usage). Here we provide details on how we used LLMs. 

## B B-SPLINES 

B-splines are smooth piecewise polynomial functions over a domain [a, b ], defined using a knot vector T. Specifically, a degree-k B-spline with G grid points is expressed as: 

B(x) = ‚ü®c, B(x)‚ü© =

> G+k‚àí1

X

> i=0

ciBi(x), (6) where Bi(x) are basis functions and ci are the learnable parameters; see the inset 5 illustra-tion for the basis functions corresponding to G = 5 and k = 3. The knot vector T =[t‚àík, . . . , t 0, t 1, . . . , t G, . . . , t G+k] is larger than the domain itself, and determines where and how the basis functions are defined. The basis functions are defined recursively using the Cox‚Äìde Boor formula (De Boor, 1978); the explicit expression is provided below. For uniform B-splines, of which we focus on in this paper, the knots are equally spaced with t0 = a, tG = b, and spacing 

‚àÜt = ti ‚àí ti‚àí1.

Cox‚Äìde Boor Formula The Cox‚Äìde Boor recursion De Boor (1978) defines the B-spline basis functions as follows: 

Ni,k ‚Ä≤ (x) = 

1, if ti ‚â§ x < t i+1 ,

0, otherwise , for k‚Ä≤ = 0 ,Ni,k ‚Ä≤ (x) = x ‚àí ti

ti+k‚Ä≤ ‚àí ti

Ni,k ‚Ä≤‚àí1(x) + ti+k‚Ä≤+1 ‚àí xti+k‚Ä≤+1 ‚àí ti+1 

Ni+1 ,k ‚Ä≤‚àí1(x), for k‚Ä≤ > 0.

We denote 

Bi(x) := Ni‚àík,k (x).

In the uniform case, the denominators simplify to k‚Ä≤‚àÜt.

> 5The figure is taken from Liu et al. (2025).

14 Published as a conference paper at ICLR 2026 

## C EXTENDED EXPERIMENTAL SECTION 

The implementation of WS-KAN was carried out using PyTorch (Paszke et al., 2019) and Py-Torch Geometric (Fey & Lenssen, 2019), which are distributed under the BSD and MIT licenses, respectively. Hyperparameter optimization was conducted with the Weight and Biases frame-work (Biewald, 2020). Below we provide the additional details necessary to reproduce our ex-periments. These include instructions on how we constructed the model zoos for each task, how we trained both WS-KAN and the baseline models, as well as the hyperparameter grids and other rel-evant configurations. Importantly, for all WS models considered we employed the exact same data split and hyperparameter grid search. For the specific procedure used to align a dataset of trained KANs, please refer to Section D.1. 

Optimizer and schedulers. For all considered datasets and tasks for WS models considered, we use the AdamW optimizer Loshchilov & Hutter (2019) in combination with a linear learning rate scheduler, incorporating a warm-up phase over the first 100 of training steps. All KANs we have trained for constructing the various model zoos use the fit function from the PyKAN library 6.C.1 INR CLASSIFICATION : E XTENDED SECTION 

C.1.1 MODEL ZOO ‚Äì INR CLASSIFICATION 

Constructing the synthetic 2D sine wave INR dataset. We start by sampling a frequency vector uniformly at random from the range w ‚àà [0 .5, 10] 2. Each sample defines a 2D sine wave of the form, 

g(x) = sin( w ¬∑ x),

where x is the input and g(x) is the target output. To learn this mapping, we train a KAN-based implicit neural representation (INR) with architecture depth/width configuration of [2 , 32 , 32 , 1] . We train 1,000 independent models, using a split of 800/100/100 for training, validation, and testing, respectively. Training is performed with a batch size of 128, and the learning rate is fixed at 0.01 ,for 1, 000 epochs. The univariate functions in the KAN is parameterized via B-splines with G = 30 

and k = 3 . See example of a KAN-based INR for a samples sine function in Figure 6. 

Constructing INRs for MNIST, F-MNIST, CIFAR10. For each dataset, we train a KAN-based implicit neural representation (INR) model that maps pixel coordinates to their corresponding intensity values‚Äîgrayscale for MNIST and F-MNIST, and RGB for CIFAR10. The network ar-chitecture is configured as [2 , 32 , 32 , 1] for MNIST and F-MNIST, and [2 , 32 , 32 , 3] for CIFAR10 to account for its three color channels. For all datasets, we adopt a batch size of 128 and train for 1,000 epochs using a fixed learning rate of 0.01 . The univariate functions in the KAN is parame-terized via B-splines with G = 10 and k = 3 . An illustration of a KAN-based INR is provided in Figure 7. 

> 6https://github.com/KindXiaoming/pykan

Figure 6: Example of a KAN-based INR applied to the synthetic 2D sine wave dataset. The left panel shows the ground truth, and the right panel shows the reconstructed result. 15 Published as a conference paper at ICLR 2026 

Figure 7: Reconstructions from INR on CIFAR-10, Fashion-MNIST, and MNIST. All PSNRs are more than 40. C.1.2 INR CLASSIFICATION - ADDITIONAL DETAILS 

Hyperparameters. For all datasets considered (except the sine wave dataset), we trained both WS-KAN and the baseline models on a randomly sampled subset of 10,000 trained KANs, with an additional 5,000 reserved for validation. The KANs used for testing are those that were trained on the original dataset‚Äôs test images. For each considerd WS model, we used 4 layers, a hidden dimension of 128, batch size of 128, weight decay of 0.01, dropout of 0.2, and 300 epochs. For the learning rate, we performed a search over the set {0.001 , 0.0001 }.For the sine wave dataset, we made the following adjustments: training was conducted for 200 epochs with a fixed learning rate of 0.001. Additionally, we experimented with varying training set sizes, using subsets of {100 , 200 , 500 , 800 } from the available 800 samples. C.1.3 RESULTS ON SYNTHETIC DATASET 

Synthetic dataset: 2D Sine waves. We constructed a dataset of KAN-based INRs for 2D sine waves ‚Äì see Section C.1.1 for more details on this dataset construction. The task is to predict the frequency 

w of a given test KAN-based INR, via its parameters. To evaluate the generalization capabilities of the architectures on this synthetic dataset, we repeat the experiment while varying the number of training examples (INRs). As shown in Figure 8, WS-KAN consistently outperforms all baseline methods, even when trained with only a small number of examples. C.2 ACCURACY PREDICTION ‚Äì EXTENDED SECTION 

The weight-space training pipeline for the task of accuracy prediction is illustrated in Figure 10. C.2.1 MODEL ZOO ‚Äì ACCURACY PREDICITON 

To construct the model zoos for the task of accuracy prediction, we used MNIST, F-MNIST, and K-MNIST. For each dataset, we trained 4,000 KAN models with a 3000/500/500 split for training, validation, and testing. Each KAN model we trained (to classify the images in the dataset at hand) 240 320 400 480 560 640 720 800  

> Number of train samples
> 10 1
> 10 0
> 10 1
> MSE (log scale)
> DMC
> DeepSets (Ours)
> MLP
> MLP + Align.
> MLP + Aug.
> SetTrans (Ours)
> WS-KAN (Ours)

Figure 8: Results on the synthetic 2D sine wave INR dataset. 16 Published as a conference paper at ICLR 2026 0 20 40 60 80 100 

> Noisy labels (%)
> 0
> 20
> 40
> 60
> 80
> 100
> Test accuracy (%)

(a) F-MNIST 0 20 40 60 80 100  

> Noisy labels (%)
> 0
> 20
> 40
> 60
> 80
> 100
> Test accuracy (%)

(b) K-MNIST 0 20 40 60 80 100  

> Noisy labels (%)
> 0
> 20
> 40
> 60
> 80
> 100
> Test accuracy (%)

(c) MNIST 

Figure 9: KANs test accuracies. Scatter plots of KAN accuracies on the test set (Y-axis) as a function of the number of noisy labels (X-axis). Weight -space 

architecture 

(e.g., our WS -KAN )     

> ,ùë¶ =93 %ùë• =
> ùë• =

trained KANs  trained to predict the KAN ‚Äôs accuracy    

> ,ùë¶ =78 %
> accuracy
> predictor

Figure 10: Weight-space training for accuracy prediction . We train KANs under varying noise levels, then fit a WS model (e.g., WS-KAN) to predict their accuracy from weights. used 3 layers. For MNIST and F-MNIST, the layer dimensions were [784 , 32 , 32 , 10] , where 

784 = 28 √ó 28 corresponds to the number of grayscale input pixels. For CIFAR10, the dimensions were [3072 , 32 , 32 , 10] , with 3072 = 32 √ó 32 √ó 3 accounting for the RGB input pixels. Note that 10 

denotes the number of target classes across all datasets. We observed that KANs trained on these datasets achieved similar accuracy levels. To increase the difficulty of predicting accuracy from parameters, we introduced label noise. Specifically, for each KAN, we randomly selected portions of the training data and permuted their labels. In Figure 9, we report the test accuracies over a random sample of 100 test instances. We trained those KANs for 100 epochs, with a fixed learning rate of 0.01 and a batch size of 128. We again used a grid size of G = 5 and k = 3 for the B-splines that define the univariate functions in the trained KANs. C.2.2 ACCURACY PREDICTION ‚Äì ADDITIONAL DETAILS 

Hyperparameters. The hyperparameters used in all experiments, across tasks, datasets, and base-lines, are summarized in Table 5. 

Parameter Values 

Embedding size {128 , 32 }

Number of epochs 100 

Learning rate {0.001 , 0.005 , 0.0001 }

Dropout 0.2

Number of layers {4, 1}

Batch size 32 

Table 5: Hyperparameter configuration used in the experiments for accuracy prediction. 17 Published as a conference paper at ICLR 2026 C.3 PRUNING MASK PREDICTION ‚Äì EXTENDED SECTION 

The pipeline for training a WS model for the task of predicting the pruning mask is provided in Figure 11. Weight -space 

## architecture 

(e.g., our WS -KAN )

, ùë¶ =ùë• =

, ùë¶ =ùë• =

## trained KANs  trained to predict oracle‚Äôs pruned mask 

mask 

predictor 

Figure 11: Pipeline for training a WS model for the task of pruning mask prediction. 

C.3.1 MODEL ZOO ‚Äì P RUNING 

The model zoos used in these experiments build upon those described in Section C.2.1. More specifi-cally, starting from the KANs obtained in Section C.2.1, we applied the Oracle prune algorithm‚Äîan edge-pruning method provided by the PyKAN library 7. Specifically, all edges whose average activa-tion on the training dataset fell below a fixed threshold were removed. Throughout all experiments, we used a threshold of 0.01. C.3.2 PRUNING MASK PREDICTION ‚Äì ADDITIONAL DETAILS AND RESULTS 

We note that for the test metrics ROC-AUC and Accuracy reported in Table 4, we used the full set of 500 test samples. Since Accuracy is a threshold-dependent metric, we selected the threshold by maximizing validation accuracy and applied the same threshold during testing. For the pruning plots for each dataset, as shown in Figures 5a, 5b, 12 and 13, we used a random subset of 100 KANs from the test set, the same as the one for Figure 9.This decision stems from the high computational cost of generating such plots, which requires full inference on the entire dataset (e.g., MNIST) for each pruned KAN produced by every WS method. 

Hyperparameter. For all datasets, we adopted a consistent training setup, closely aligned with the configuration described in Section C.1. Specifically, we used a 4-layer architecture with a hidden dimension of 128, a batch size of 32, weight decay of 0.01, and dropout of 0.2. Training was performed for 15 epochs. For the learning rate, we conducted a grid search over {0.001 , 0.0001 }.

Additional results. In Figures 12 and 13, we present the downstream pruning results on F-MNIST and K-MNIST, respectively. As shown in Figure 12, WS-KAN achieves the best per-formance, striking the most favorable trade-off between accuracy and parameter sparsity, closely approaching the oracle prune that serves as our supervision. Meanwhile, the other baselines per-form markedly worse. In contrast, Figure 13 shows that DS is also highly effective, performing on par with WS-KAN. C.4 ABLATION : POSITIONAL ENCODING AND BIDIRECTIONAL MESSAGE PASSING 

Here we present our ablation study to evaluate the importance of both the positional encoding and the bidirectional message passing. Specifically, we report all results in the paper without the positional encoding and the bidirectional message passing. 

> 7https://github.com/KindXiaoming/pykan .

18 Published as a conference paper at ICLR 2026 0% 20% 40% 60% 80% 100% 

Noisy labels (%) 

0

10 

20 

30 

40 

50 

60 

70 

80 

> Test accuracy (%)
> Random Classifier
> WS-KAN (Ours)
> MLP
> MLP + Aug.
> MLP + Align.
> DeepSets (Ours)
> DMC
> Original KAN
> Oracle prune

(a) Downstream prune accuracy. 0% 20% 40% 60% 80% 100% 

Noisy labels (%) 

0

2

4 

> Weights (%) Kept
> WS-KAN (Ours)
> MLP
> MLP + Aug.
> MLP + Align.
> DeepSets (Ours)
> DMC
> Oracle prune

(b) Downstream prune kept weight %. 

Figure 12: Downstream pruning performance across methods over F-MNIST. We report: (i) 

Test accuracy : the downstream accuracy of pruned networks, averaged over non-overlapping bins of 20%, to highlight the relative effectiveness of pruning strategies under varying noise levels ‚Äì Figure 12a; (ii) Kept weights : the percentage of weights retained after pruning, averaged over the same bins as in (i) ‚Äì Figure 12b. 0% 20% 40% 60% 80% 100% 

Noisy labels (%) 

0

10 

20 

30 

40 

50 

60 

70 

> Test accuracy (%)
> Random Classifier
> WS-KAN (Ours)
> MLP
> MLP + Aug.
> MLP + Align.
> DeepSets (Ours)
> DMC
> Original KAN
> Oracle prune

(a) Downstream prune accuracy. 0% 20% 40% 60% 80% 100% 

Noisy labels (%) 

0

2

4

6

8

10  

> Weights (%) Kept
> WS-KAN (Ours)
> MLP
> MLP + Aug.
> MLP + Align.
> DeepSets (Ours)
> DMC
> Oracle prune

(b) Downstream prune kept weight %. 

Figure 13: Downstream pruning performance across methods over K-MNIST. We report: (i) 

Test accuracy : the downstream accuracy of pruned networks, averaged over non-overlapping bins of 20%, to highlight the relative effectiveness of pruning strategies under varying noise levels ‚Äì Figure 13a; (ii) Kept weights : the percentage of weights retained after pruning, averaged over the same bins as in (i) ‚Äì Figure 13b. 

INR Classification Accuracy. The results are provided in Table 6. Table 6: Ablation study ‚Äì INR classification accuracy. Models MNIST F-MNIST CIFAR10 

WS-KAN‚Äì no PE 94 .3 ¬± 0.4 84 .5 ¬± 0.1 46 .1 ¬± 1.2

WS-KAN‚Äì no BIDIR 93 .8 ¬± 0.2 83 .4 ¬± 0.1 10 .0 ¬± <0.1

WS-KAN 94 .3 ¬± 0.5 84 .6 ¬± 0.6 42 .2 ¬± 0.8

Accuracy Prediction. The results are provided in Tables 7 and 8. Table 7: Ablation study ‚Äì Accuracy prediction (MSE) [ √ó10 3]. Model MNIST (MSE) F-MNIST (MSE) K-MNIST (MSE) 

WS-KAN‚Äì no PE 2.53 ¬± 0.22 2.44 ¬± 0.50 1.10 ¬± 0.17 

WS-KAN‚Äì no BIDIR 4.03 ¬± 0.62 3.64 ¬± 0.12 1.50 ¬± 0.08 

WS-KAN 3.29 ¬± 0.17 2.94 ¬± 0.13 1.45 ¬± 0.08 

19 Published as a conference paper at ICLR 2026 Table 8: Ablation study ‚Äì Accuracy prediction ( R2). Model MNIST ( R2) F-MNIST ( R2) K-MNIST ( R2)

WS-KAN‚Äì no PE 96 .00 ¬± 0.35 93 .57 ¬± 1.32 96 .62 ¬± 0.51 

WS-KAN‚Äì no BIDIR 93 .64 ¬± 0.98 90 .40 ¬± 0.31 95 .54 ¬± 0.23 

WS-KAN 94 .81 ¬± 0.27 92 .27 ¬± 0.35 95 .69 ¬± 0.24 

Pruning Mask Prediction. The results are provided in Tables 9 and 10. Table 9: Ablation study ‚Äì Pruning mask prediction accuracy. Model MNIST (Accuracy) F-MNIST (Accuracy) K-MNIST (Accuracy) 

WS-KAN‚Äì no PE 97 .77 ¬± 0.28 98 .68 ¬± 0.04 96 .28 ¬± 0.45 

WS-KAN‚Äì no BIDIR 96 .74 ¬± 0.27 98 .27 ¬± 0.04 95 .39 ¬± 0.14 

WS-KAN 97 .93 ¬± 0.19 98 .93 ¬± 0.05 97 .72 ¬± 0.14 

Table 10: Ablation study ‚Äì Pruning mask prediction AUC. Model MNIST (AUC) F-MNIST (AUC) K-MNIST (AUC) 

WS-KAN‚Äì no PE 99 .51 ¬± 0.09 99 .59 ¬± 0.02 98 .85 ¬± 0.21 

WS-KAN‚Äì no BIDIR 98 .38 ¬± 0.20 98 .99 ¬± 0.01 97 .11 ¬± 0.37 

WS-KAN 99 .54 ¬± 0.01 99 .72 ¬± 0.02 99 .46 ¬± 0.09 

We observe that WS-KAN performs well even without positional encoding. However, on the more challenging equivariant task‚Äîpruning mask prediction‚ÄîWS-KAN with positional encoding out-performs the version without it in all 6/6 cases. This indicates that positional encoding plays a particularly important role in enhancing WS-KAN‚Äôs performance on equivariant tasks. We also note that bidirectional message passing is crucial: across all tasks and dataset combinations, WS-KAN without bidirectional message passing consistently underperforms the version that includes it. C.5 COMPLEXITY AND RUNTIME ANALYSIS 

We report the running times observed in the INR classification tasks. Table 11 summarizes the average training and testing time per epoch (in seconds), computed over 10 epochs and presented together with the corresponding standard deviation. The results indicate that our runtimes are con-sistent with those commonly observed for standard GNN models on typical graph benchmarks. 

Complexity analysis. We assume the input KAN contains nl nodes at layer l ‚àà { 1, . . . , L }, and that all nodes and edges share the same feature dimensionality, which we treat as a constant. The total number of nodes is therefore 

N =

> L

X

> l=1

nl,

and the total number of edges‚Äîdue to full connectivity between consecutive layers‚Äîis 

E =

> L‚àí1

X

> l=1

nl nl+1 .

Referring to our message-passing mechanism, as presented in Section 3.3, we obtain the following: (a) The function MLP (1; F ) 

> v

is applied once per edge, giving E operations. Aggregating over the incoming neighbors of each node contributes another E operations, since the graph is directed and each edge is counted once. Thus, step (a) costs O(E).(b) This step mirrors (a), except that the edge directions are transposed. Therefore, it also costs 

O(E).20 Published as a conference paper at ICLR 2026 (c) This step applies an MLP independently to each edge, contributing another O(E) opera-tions. (d) This step is a per-node operation, giving a cost of O(N ). Because E > N , the edge-wise computations dominate the total cost. Hence, the overall complexity is 

O(E).

Table 11: Running times measured (in seconds) for all experiments conducted on the INR classifi-cation task. 

Dataset MNIST F-MNIST CIFAR-10 

Train time per epoch (s) 10 .12 ¬± 0.06 10 .18 ¬± 0.05 10 .88 ¬± 0.10 

Test time per epoch (s) 2.84 ¬± 0.12 2.81 ¬± 0.06 3.05 ¬± 0.08 

## D ALIGNING KOLMOGOROV -A RNOLD NETWORKS 

A complementary strategy to designing permutation-equivariant architectures, is to instead keep the model class simple and canonize the input. In our context, the core idea would be to map each KAN into a fixed canonical form by permuting its neurons so that two networks differing only by permutations (and thus computing the same function) are aligned to the same representation. A particularly interesting case arises in the context of MLPs as discussed in Ainsworth et al. (2022). In that case, Œò‚Äôs are collection of scalers, and a simple choice of dist to be the Euclidean distance 

‚à• ¬∑ ‚à• 2, the alignment problem becomes 

argmin 

> œÄ

vec(Œò A) ‚àí vec  œÄ(Œò B ) 

> 2

, = argmax 

> œÄ

vec(Œò A) ¬∑ vec  œÄ(Œò B ). (7) We can re-express this in terms of the full weights, 

argmax 

> œÄ={Pi}

‚ü®W(A)1 , P1W(B)1 ‚ü©F + ‚ü®W(A)2 , P2W(B)2 P ‚ä§ 

> 1

‚ü©F + ¬∑ ¬∑ ¬∑ + ‚ü®W(A) 

> L

, W(B) 

> L

P ‚ä§

> L‚àí1

‚ü©F . (8) Turning to KANs, the parameters ŒòA and ŒòB correspond to collections of univariate functions ,rather than scalars. To align such models, we must therefore define a distance function appropriate for functions. Importantly, there is no single ‚Äúcorrect‚Äù choice here‚Äîdifferent definitions may lead to different alignment outcomes. In this paper, consistent with our formalism in Section 3.2, we associate each such entry in Œò, with a parameter vector of the form 

(Œò A)i = [ wb, w s, c],

where, for the sake of this example and with a slight abuse of notation, wb, ws, and c are the coef-ficients defining the corresponding i-th 1D function. For convenience, we refer to these coefficients as channels , and denote the c-th channel of this vector by (Œò cA)i = [ wb, w s, c]c.With this structure, we define the alignment objective as the sum of ‚Ñì2 distances across all channels: 

arg min 

> œÄ

X

> c

vec(Œò cA) ‚àí vec  œÄ(Œò cB ) 

> 2

= arg max 

> œÄ

X

> c

vec(Œò cA) ¬∑ vec  œÄ(Œò cB ), (9) which parallels Eq. (7), but now summed across channels. Expanding this channel-wise objective yields 

argmax 

> œÄ={Pi}

X

> c



‚ü®œï(c;A)1 , P1œï(c;B)1 ‚ü©F + ‚ü®œï(c;A)2 , P2œï(c;B)2 P ‚ä§ 

> 1

‚ü©F + ¬∑ ¬∑ ¬∑ + ‚ü®œï(c;A) 

> L

, œï (c;B) 

> L

P ‚ä§

> L‚àí1

‚ü©F



=



argmax 

> œÄ={Pi}

‚ü®œï(A)1 , P1 ‚àó œï(B)1 ‚ü©F + ‚ü®œï(A)2 , P2 ‚àó œï(B)2 ‚àó P ‚ä§ 

> 1

‚ü©F + ¬∑ ¬∑ ¬∑ + ‚ü®œï(A) 

> L

, œï (B) 

> L

‚àó P ‚ä§

> L‚àí1

‚ü©F



.

21 Published as a conference paper at ICLR 2026 In the second line, we move the sum over channels c inside each Frobenius inner product. As a result, the channel superscript c is omitted: the Frobenius inner product now implicitly sums over 

c, absorbing the channel dimension. Importantly, ‚àó denotes a contraction operator that acts only on the first two indices, leaving the channel dimension untouched. Concretely, for the middle term we have: 

(P2 ‚àó œï(B)2 ‚àó P ‚ä§ 

> 1

)p,q,c := X

> p‚Ä≤,q ‚Ä≤

(P2)p,p ‚Ä≤ (œï(B)2 )p‚Ä≤,q ‚Ä≤,c (P ‚ä§ 

> 1

)q,q ‚Ä≤ . (10) This formalism provides a straightforward extension of the alignment framework of Ainsworth et al. (2022) for MLPs to the KAN setting: their algorithm can be directly applied once the ‚àó operation is defined as above. D.1 ALIGNING A DATASET OF TRAINED KAN S

While the formalism in Section D provides a method for aligning a pair of KANs, our experiments require aligning an entire dataset of KANs. To achieve this, we integrate our alignment technique with the MergeMany algorithm introduced in Ainsworth et al. (2022). The central idea is to itera-tively run the alignment procedure between one model and the average of all other models. In practice, we observed that convergence times could be prohibitively long. To address this, we adopted the following strategy: we fixed the error tolerance at 0.001 , then randomly sampled 1000 models and applied the MergeMany algorithm to this subset, consistent with Navon et al. (2023). The outcome was a new model representing the average of the aligned subset. This model then served as the reference for aligning the remaining training models as well as the test models, ulti-mately yielding fully aligned training and test sets. Finally, we note that in our formulation, all channels are equally weighted. While this simplifies the alignment, it may be suboptimal in certain cases. Alternative weighting schemes could yield improved results. We leave a detailed exploration of such extensions to future work. 

## E PROOFS 

Lemma E.1 (MLP as an approximation of the univariate functions composing the KAN) . Let B

denote the family of cardinal B-splines of degree k, defined on a fixed grid G over the domain 

[a, b ], and parameterized by coefficients c = [ c0, . . . , c G+k‚àí1] ‚àà C ‚äÇ RG+k, where the set of admissible coefficients lies in a compact domain C. Consider the function œà : R ‚Üí R defined as 

œà(x) = wb b(x) + ws B(x), where wb, w s ‚àà W ‚äÇ R for some compact set W. b(x) denotes the silo function, and B(x) denotes the B-spline. Then, for any Œµ > 0, and for any compact domain X

there exists a set of weights for a multilayer perceptron MLP : RG+k+3 ‚Üí R such that, 

sup x‚ààX MLP (x, w s, w b, c) ‚àí œà(x) < Œµ. 

Proof. The claim follows directly from the universal approximation theorem Cybenko (1989). The input vector to the MLP lies in the compact domain 

(x, w s, w b, c) ‚àà X √ó W √ó W √ó C ,

which is compact since finite cartesian products of compact sets are compact. Moreover, the target function œà(¬∑) is continuous, as it is a weighted sum of continuous functions‚Äîboth the silo function 

b(¬∑) and the B-spline B(¬∑) are continuous. Therefore, by the universal approximation theorem, there exists an MLP that uniformly approximates œà on this compact domain to arbitrary accuracy Œµ > 0.This completes the proof. 

Proposition E.2 (WS-KAN can simulate the forward pass of KANs) . Let fŒ∏ be a given KAN architecture, defined over an input domain [a, b ]n, where each univariate function is represented by a B-spline from the family B. Let G be its KAN-graph , where the nodes in the first layer are enhanced with the input x ‚àà [a, b ]n. For every Œµ > 0, there exists a WS-KAN such that, 

sup x‚àà[a,b ]n WS-KAN (G) ‚àí fŒ∏ (x) < Œµ. 

22 Published as a conference paper at ICLR 2026 

Proof. We recall that fŒ∏ (x) = ( œïL ‚ó¶ . . . ‚ó¶ œï1)x is a composition of continuous functions. We denote by vl the nodes in the KAN-graph that correspond to the activations in layer l, defined 

al. We prove via induction, that l + 1 layers of WS-KAN, can uniformly approximate the output 

al+1 = ( œïl+1 ‚ó¶ . . . ‚ó¶ œï1)x over the nodes vl+1 to arbitrary precision. 

Induction proof. We begin by defining a compact space, S, which encompasses all possible values attained by any neuron within the KAN. Since the input to the KAN lies in [a, b ]n (which is compact) and the applied univariate functions are continuous, their outputs form compact sets. With a mild abuse of terminology, we define the compact output space to be a compact set that includes the outputs corresponding to all inputs in the unit ball B(x, 1) around each input. Each neuron then receives a finite sum of elements, each belonging to a compact set, which implies that the neuron‚Äôs value also lies within a compact set. Moreover, because the number of neurons is finite, the union of all such compact sets is itself contained in a larger compact domain. Thus, there exists a compact space, defined as S, that contains the values of all neurons. Additionally, we denote the maximal width of the network by Nmax .We now proceed with a proof by induction. 

Step 1. Initialization. We define the KAN‚Äôs input over the KAN-graph as follows, 

v0 = x, vl> 0 = 0, elp,q = Àúœïlp,q := [ wlb;p,q , w ls;p,q , clp,q ],

so that the node features at layer 0 encode the KAN‚Äôs input x, while edge features store the spline parameters. Thus, the base case is satisfied. 

Step 2. Induction hypothesis. We assume that after l layers of WS-KAN message passing we have 

‚à•vl ‚àí al‚à• < Œ¥, 

where al denotes the activations at layer l of the KAN, and Œ¥ > 0 is an arbitrarily small approxima-tion error that we can control . Given œµ > 0, we now show that we are able to obtain vl+1 such that 

‚à•vl+1 ‚àí al+1 ‚à• < œµ .

Step 3. Inductive step. 

We recall a given WS-KAN update equation: 

vF 

> i

‚Üê MLP (2; F)

> v



vi, X 

> j:e(i,j )‚ààE

MLP (1; F) 

> v

(vj , e(i,j ))



, vB 

> i

‚Üê MLP (2; B)

> v



vi, X 

> j:e(i,j )‚ààET

MLP (1; B) 

> v

(vj , e(i,j ))



,

e(i,j ) ‚Üê MLP e(vi, vj , e(i,j )), vi ‚Üê MLP (3)  

> v

(vi, vF 

> i

, vB 

> i

).

The KAN update rule at layer l + 1 is 

al+1  

> p

= X

> q

œïlp,q (alq ),

where œïlp,q denotes a univariate spline function parameterized by (wlb;p,q , w ls;p,q , clp,q ).To reproduce this update within WS-KAN, we recall that MLP univariate can be chosen to be an arbi-trarly close approximation of the univariate function composting the KAN, recall Theorem 4.1, thus we choose MLP univariate , such that 

‚à•MLP univariate (a, b, c, d ) ‚àí œà(a)‚à• < œµ/ 2Nmax ,

where œà stands for the 1D univariate functions with the parameters (b, c, d ).Since MLP univariate (a, b, c, d ) is continuous and defined on a compact set, it is uniformally continuous. Thus, there exists Œ¥‚Ä≤ > 0 such that, 

‚à•a ‚àí a‚Ä≤‚à• < Œ¥ ‚Ä≤ ‚áí ‚à• MLP univariate (a, b, c, d ) ‚àí MLP univariate (a‚Ä≤, b, c, d )‚à• < œµ/ 2Nmax .

Thus, we fix Œ¥ < min( Œ¥‚Ä≤, 1) , and from the induction hypothesis we get that there is a WS-KAN network that can guarantee ‚à•vl ‚àí al‚à• < Œ¥ .23 Published as a conference paper at ICLR 2026 It holds that for a single neruon, given that ‚à•a ‚àí a‚Ä≤‚à• < Œ¥ ,

MLP univariate (a‚Ä≤, b, c, d ) ‚àí œà(a) =

MLP univariate (a‚Ä≤, b, c, d ) ‚àí MLP univariate (a, b, c, d ) + MLP univariate (a, b, c, d ) ‚àí œà(a) ‚â§

MLP univariate (a‚Ä≤, b, c, d ) ‚àí MLP univariate (a, b, c, d ) + MLP univariate (a, b, c, d ) ‚àí œà(a) ‚â§

œµ/ 2Nmax + œµ/ 2Nmax = œµ/N max 

We set, 

MLP (1; F )

> v

 vj , e(i,j )

 = MLP univariate  vlq , w lb;p,q , w ls;p,q , clp,q 

 . (11) Thus, it holds that for ‚à•vlq ‚àí alq ‚à• < Œ¥ , which is our induction assumption, we have, 

‚à•MLP (1; F )

> v

 vj , e(i,j )

 ‚àí œïlp,q (alq )‚à• < œµ/N max ,

We can also assign weight to the following MLP, to output the following precisely, 

MLP (2; F ) 

> v

(a, b ) = b, (12) (13) The forward aggregator vforward  

> i

thus becomes 

|vforward  

> q

‚àí X

> q

œïlp,q (alq )| ‚â§ œµ/N max ,

which approximates the KAN update in this layer. The backward aggregator, edge update, and auxiliary terms are irrelevant for this construction, so their associated MLPs, namely are set to the zero function, by assigning zero to all weight: 

MLP (1; B) 

> v

= MLP (2; B) 

> v

= MLP e = 0 , (14) Finally, we set the weights of MLP (3)  

> v

such that MLP (3)  

> v

(a, b, c ) = b. this ensures that vl+1  

> q

= vforward  

> q

.Thus. 

‚à•vl+1 ‚àí al+1 ‚à•2 = X

> p

 vl+1  

> p

‚àí al+1 

> p

2

! 12

‚â§ (X

> p

(œµ/N max )2) 12 ‚â§ œµ/ pNmax < œµ 

Proposition E.3 (KAN symmetries) . Let Œ∏ = ( œïL, . . . , œï1) denote the collection of parametric one-dimensional functions composing an L-layer KAN. Consider the group, G := Sd1 √ó Sd2 √ó¬∑ ¬∑ ¬∑ √ó SdL‚àí1 , the direct product of symmetric groups corresponding to the intermediate dimensions 

d1, . . . , d L‚àí1. Let g = ( P1, . . . , PL‚àí1) ‚àà G, where each Pl is the permutation matrix of œÉl ‚àà Sdl .Define the group action g ¬∑ Œ∏ = Œ∏‚Ä≤ with Œ∏‚Ä≤ = ( œï‚Ä≤L, . . . , œï‚Ä≤1) given by, 

œï‚Ä≤1 = P ‚ä§ 

> 1

œï1 , œï‚Ä≤l = P ‚ä§ 

> L

œïlPl‚àí1, ‚àÄl = 2 , . . . , L ‚àí 1 , œï‚Ä≤L = œïLPL‚àí1 .

Then, fŒ∏ (x) = fŒ∏‚Ä≤ (x) for all x.Proof. We compute the p-th output component: 

fŒ∏ (x)p = ( œï2 ‚ó¶ œï1 ‚ó¶ x)p = X

> q,k

œï2 

> p,q

(œï1

> q,k

(xk)) = X

> q,k

œï2

> p,œÉ (q)

(œï1

> œÉ(q),k

(xk)) = X

> q,k

(œï2P )p,q (P ‚ä§œï1)q,k (xk) = fÀúŒ∏ (x)p

The third equality holds by reindexing the summation using a permutation œÉ corresponding to 

P . Recalling Eq. (4), the fourth equality follows from the identities (œï2P )p,q = œï2 

> p,œÉ (q)

and 

(P ‚ä§œï1)q,k = œï1

> œÉ(q),k

.24 Published as a conference paper at ICLR 2026 

## F LARGE LANGUAGE MODEL (LLM) U SAGE 

We used large language models (LLMs) exclusively to support the writing process. Their role was limited to improving clarity in technical explanations, refining grammar and style, and enhancing overall readability. All research contributions‚Äîincluding experimental design, data analysis, and conclusions‚Äîare entirely our own. The LLMs served solely as tools to improve presentation quality and were not employed to generate research content or influence the substance of our work. 25