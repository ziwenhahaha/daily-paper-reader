Title: MadEvolve: Evolutionary Optimization of Cosmological Algorithms with Large Language Models

URL Source: https://arxiv.org/pdf/2602.15951v1

Published Time: Thu, 19 Feb 2026 01:05:21 GMT

Number of Pages: 47

Markdown Content:
# MadEvolve: Evolutionary Optimization of Cosmological Algorithms with Large Language Models 

# Tianyi Li 1, Shihui Zang 1, and Moritz M¨ unchmeyer 11Department of Physics, University of Wisconsin-Madison February 19, 2026 

Abstract 

We develop a general framework to discover scientific algorithms and apply it to three problems in computational cosmology. Our code, MadEvolve , is similar to Google’s AlphaEvolve, but places a stronger emphasis on free parameters and their optimization. Our code starts with a baseline human algorithm implementation, and then optimizes its performance metrics by making iterative changes to its code. As a further convenient feature, MadEvolve automatically generates a report that compares the input algorithm with the evolved algorithm, describes the algorithmic innovations and lists the free parameters and their function. Our code supports both auto-differentiable, gradient-based parameter optimization and gradient-free optimization methods. We apply MadEvolve to the reconstruction of cosmological initial conditions, 21cm foreground contamination reconstruction and effective baryonic physics in N-body simulations. In all cases, we find substantial improvements over the base algorithm. We make MadEvolve and our three tasks publicly available at madevolve.org .

Corresponding Author: Tianyi Li ( tianyi.li@wisc.edu )1

> arXiv:2602.15951v1 [astro-ph.CO] 17 Feb 2026

# Contents 

1 Introduction 32 Related Work 4

2.1 Program Synthesis and Repair with LLMs and Agents . . . . . . . . . . . . . . . . . . . . . . 42.2 Evolutionary and QualityDiversity (QD) Search . . . . . . . . . . . . . . . . . . . . . . . . . . 42.3 Automated Discovery in the Physical Sciences . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

3 The MadEvolve Framework 5

3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53.2 Core Architecture and Evolution Loop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63.3 Parameter Tracking and Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73.4 Hybrid Population Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83.5 LLM Ensemble and Generation Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93.6 Automated Report Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 

4 Overview of Results 11 

4.1 Summary of Achievements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.2 Difficulties and Engineering Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.3 Model Contributions and Global Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 

5 Application I: Initial Condition and BAO Reconstruction 14 

5.1 Problem Formulation and Scientific Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . 14 5.2 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5.3 Results - Starting from Standard Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . 16 5.4 Results - Starting from Iterative Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . 19 5.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 

6 Application II: 21cm Foreground Contamination Reconstruction 23 

6.1 Problem Formulation and Scientific Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . 23 6.2 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 6.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 6.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 

7 Application III: Effective Baryonic Physics from N-body simulations 29 

7.1 Problem Formulation and Scientific Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . 29 7.2 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 7.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 7.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 

8 Future Directions for MadEvolve 34 9 Conclusion 35 A Implementation Details 41 

A.1 Computational Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 A.2 Domain-Specific System Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 

B Using MadEvolve on New Tasks 46 C LLM-generated Evolution Reports for all Tasks 46 

21 Introduction 

Modern cosmological data analysis involves numerous algorithmic challenges, such as field reconstruction, correlation function estimation, simulations, and many others across different tracers and their combinations. While human researchers continue to make algorithmic advances on these tasks, traditional approaches based on handcrafted algorithms and manual hyperparameter tuning struggle with the growing data volumes, model complexity, and exponentially many design choices in large pipelines. We need systematic methods for automated algorithm discovery that can search through conceptual spaces while respecting physical constraints and computational limits. Our work primarily builds on two recent breakthroughs combining large language models (LLMs) with evolutionary search. FunSearch [1] demonstrated that pairing a code-trained LLM with an automated evaluator could produce genuinely novel results—including new cap-set constructions and better bin-packing heuristics—by searching through function space using best-shot prompting and island-based populations for diversity. AlphaEvolve [2] extended this approach to create an evolutionary coding agent capable of modifying entire codebases through LLM ensembles and automated metrics, achieving improvements on both mathematical problems and real-world system optimizations. We adapt these ideas to cosmology, treating reconstruction and analysis programs themselves as the optimization target. LLMs serve as smart mutation operators that understand code semantics, while physics-based evaluators guide the search toward domain-specific objectives. Our main contributions are: (i) We develop the MadEvolve framework, integrating LLM-driven evolution with physical fitness metrics and HPC-compatible evaluation. MadEvolve is publicly available at madevolve.org . We believe that MadEvolve, as a light-weight open-source framework with extensive features, will be a useful tool beyond cosmology. (ii) MadEvolve includes a nested optimization architecture where an inner loop automatically tunes contin-uous parameters (e.g. filter scales and expansion coefficients) via gradient-based or grid-search methods whenever the LLM proposes new algorithmic structures, separating structural search from parameter optimization. (iii) MadEvolve automatically generates a scientific report by using LLMs to analyze discovered algorithms, compare them against the baseline, and produce human-readable explanations of the physical intuition behind improvements. (iv) We develop three applications to common tasks in cosmological data analysis: initial condition recon-struction (BAO reconstruction), 21 cm foreground removal reconstruction (i.e. recovering modes lost due to foregrounds), and effective baryonic physics in N-body simulations (at the example of thermal Sunyaev-Zeldovich effect prediction), showing that the system independently discovers novel algorith-mic strategies. In all cases, the performance metric is evaluated at each evolution step on a small set of training and validation simulations. As we will show, our resulting algorithms improve over the baseline algorithm in all three cases. This includes setups where the human baseline is strong (e.g. iterative initial condition reconstruction). A summary of these achievements can be found in Sec. 4. Our three optimization tasks, which were designed to be computationally tractable for thousands of calls, are also made available at madevolve.org .Developing algorithms is a complementary and in many cases superior approach to black-box machine learning (which often uses millions or billions of parameters). In cosmology, supervised machine learning on entire simulation boxes is difficult to deploy in practice because it is hard to simulate large-scale surveys with sufficient accuracy and volume. On the other hand, our algorithms have only a small number of parameters and generalize well outside of the training setup (which we test in a limit setup using different simulation resolutions). Further, these algorithms are humanly interpretable, which can be useful to gauge their reliability. Because they are interpretable, they can also be used as verified idea generators for human scientists: Once a strong algorithm is found, a human researcher can inspect how it works and possibly build on it. The presented use cases for LLM reasoning circumvent the problem of the lack of reliability of LLMs when solving theoretical problems. As was shown e.g. in [3, 4], LLMs face significant challenges in generating error-free derivations or calculations in theoretical physics. More generally, even frontier LLM models struggle 3with consistent reasoning performance in both formal and informal reasoning, with significant failure modes even for some seemingly simple tasks (see [5] and references therein). However, the tasks in this work are 

verifiable , i.e. success can be measured by the performance metric achieved by the algorithm on novel test data. Verifiability is in general difficult to achieve in theoretical physics. While parts of mathematics have been formalized (e.g. with the LEAN language [6]), and new LLM-based proofs can thus sometimes be auto-verified, this is much harder to achieve in theoretical physics. Fortunately, in the present case, the validity of new LLM-generated algorithmic ideas can be checked on simulations. While our tasks can in principle also be solved with agentic frameworks (see e.g. the Denario project for agent applications to cosmology [7]), the combination of LLMs with evolutionary techniques allows to systematically explore independent ideas, and ultimately combine them into a best-performing algorithm (rather than risk getting stuck in a local minimum). There are many promising directions to further improve such systems, as we discuss below in Sec. 8. The paper is organized as follows: In Section 2, we review related work on LLM-guided code evolution and machine learning in cosmology. Section 3 introduces the MadEvolve framework and its core compo-nents. Section 4 provides an overview of our main results across all three applications. Sections 5, 6, and 7 present detailed analyses of each application: initial condition and BAO reconstruction, 21cm foreground contamination reconstruction, and effective baryonic physics in N-body simulations, respectively. Section 8 discusses future directions to improve MadEvolve, and Section 9 concludes. Appendix A provides imple-mentation details, Appendix B describes how to apply MadEvolve to new tasks, and Appendix C refers to LLM-generated evolution reports for all tasks. 

# 2 Related Work 

# 2.1 Program Synthesis and Repair with LLMs and Agents 

Recent frontier LLMs increasingly support end-to-end software development workflows: generating and refactoring code from natural language, debugging with long-context understanding, and using tools to iterate across multi-step projects. OpenAI’s GPT-5 series emphasizes long-horizon reasoning and agentic tool-calling, with strong reported gains on software-engineering style evaluations and practical coding workflows [8]. Complementing this, reasoning-oriented model families such as OpenAI o1 and o3 are explicitly designed to “think longer” for difficult tasks, improving reliability on coding- and STEM-heavy problems [9, 10]. Google’s Gemini 3 represents a major step toward AGI, combining state-of-the-art reasoning with advanced multimodal understanding and agentic functionality [11]. Anthropic’s Claude Opus 4.5 achieves industry-leading results on coding benchmarks such as SWE-bench and is the best-performing model for computer use and agentic tasks [12]. Beyond synthesis, code repair has become a central benchmark and application domain. SWE-bench evaluates systems on 2,294 real-world GitHub issues across 12 popular Python repositories, requiring co-ordinated edits across files and execution of test suites [13, 14]. This setting has catalyzed agentic coding systems that plan, edit repositories, and run tools in an iterative loop. SWE-agent introduces an agent– computer interface that enables language models to navigate repositories, modify files, and execute tests more effectively [15]. In industry-facing tooling, Anthropic’s Claude Code integrates directly into terminals and IDEs and adds features such as checkpoints and more autonomous workflows for complex development tasks [16]. Systems like Devin [17] further illustrate end-to-end autonomous software engineering in realistic repo environments, highlighting both the promise and remaining reliability challenges of fully autonomous agents. 

# 2.2 Evolutionary and QualityDiversity (QD) Search 

Following AlphaEvolve [2], our work draws heavily from evolutionary algorithms and quality-diversity (QD) methods in machine learning. QD algorithms like MAP-Elites [18, 19, 20] maintain a diverse archive of good solutions organized by behavioral traits, capturing different strategies and trade-offs. This approach has proven effective in robotics and game AI, and has become a core component of modern LLM-guided evolution systems including AlphaEvolve [2]. For algorithm discovery, QD helps find unexpected solutions that single-objective optimization might miss by explicitly maintaining population diversity. 4Recent work has combined evolution with domain knowledge to discover new algorithms. AutoML-Zero evolved ML algorithms from scratch, rediscovering gradient descent and other fundamentals [21]. AlphaTen-sor and AlphaDev showed that search guided by learned models can beat human-designed algorithms for matrix multiplication and sorting [22, 23]. The key insight enabling LLM-guided evolution is using language models as intelligent mutation operators. FunSearch [1] pioneered this approach, discovering new solutions to the cap set problem—perhaps the first time an LLM contributed to solving an open mathematical problem. AlphaEvolve [2] extended this paradigm to evolve entire codebases rather than single functions. Eureka 

[24] applied similar ideas to reward function design for robotics. More recently, EvoLLM [25] demonstrated that LLMs can serve as in-context recombination operators for black-box optimization, while LLaMEA [26] automates metaheuristic algorithm generation through iterative LLM refinement. These systems collec-tively establish that LLMs can act as context-aware mutators, combining broad exploration with informed modifications guided by execution feedback. 

# 2.3 Automated Discovery in the Physical Sciences 

Scientists have long sought to automate the discovery of physical laws and optimal methods from data. 

Symbolic regression tries to find analytical formulas that explain datasets. Traditional methods of symbolic regression are often based on genetic programming. Symbolic regression has for example been used to learn conservation laws from simulations [27]. Specifically in physics, AI Feynman [28] rediscovered physics equations by combining neural networks with physics-inspired simplifications. More recently, several groups and companies have developed LLM-based “AI-Scientists”. These systems generate hypotheses, test them, and write publication-style summaries of their results, typically using multi-ple LLM-agents with different responsibilities. Sakana AI’s The AI Scientist [29] was the first comprehensive framework for fully automated scientific discovery, generating research ideas, writing code, executing exper-iments, and producing complete papers at a cost of $6–15 per paper. Its successor, The AI Scientist-v2 [30], uses agentic tree search to produce the first entirely AI-generated peer-reviewed workshop paper. Google’s 

AI Co-Scientist [31] is a multi-agent system built on Gemini 2.0 that accelerates biomedical discovery, with wet-lab validated predictions for drug repurposing in acute myeloid leukemia and epigenetic targets for liver fibrosis. The Denario project [7] focuses on astrophysics and cosmology, demonstrating AI agents that can perform literature review, code development, and paper drafting across multiple scientific disciplines. Other notable systems include Agent Laboratory [32], which uses specialized agents (PhD, Postdoc, Professor) to as-sist human researchers in executing their ideas, and Data-to-Paper [33], which creates verifiable manuscripts from annotated data with information tracing for transparency. Despite this impressive progress, AI-Scientist results remain limited. This is likely because LLMs struggle to judge both the importance and the correctness of their own results. In our work, we therefore apply LLMs to human-defined tasks with a clear verifiable reward metric, and let the LLM reason within these more limiting constraints. 

# 3 The MadEvolve Framework 

# 3.1 Overview 

Our evolutionary framework treats scientific algorithms as programs which are iteratively improved by lan-guage models. The core loop samples a parent program from a diverse population, prompts an LLM for modifications, evaluates against physics-based metrics, and updates the population. We maintain diver-sity through MAP-Elites grids and island-based subpopulations, while LLM ensembles generate varied code changes, from targeted “diff patches” to complete rewrites. Our design is based on AlphaEvolve [2], which demonstrated breakthrough discoveries in mathematics and engineering by combining Gemini models with evolutionary search. AlphaEvolve employs sophisticated infrastructure: asynchronous distributed evaluation, meta prompt evolution, and multi-score optimization. Several open-source implementations have since emerged. ShinkaEvolve [34] emphasizes sample efficiency through novelty rejection sampling with cosine similarity filtering, UCB1-style bandit algorithms for dy-namic LLM selection, and a meta-scratchpad distilling insights from successful mutations. OpenEvolve [35] 5provides reproducibility features including hash-based run isolation, multi-language support, and flexible API integration. MadEvolve is similar but slightly simplified compared to AlphaEvolve and ShinkaEvolve. We omit meta prompt evolution and explicit novelty filtering in favor of letting physics-based metrics guide the search directly. However, two distinctive features differentiate our framework. First, budget-constrained inner optimization loops automatically tune continuous parameters before computing fitness, ensuring fair comparisons between algorithms at their best achievable performance. The system offers two complementary strategies: greedy one-shot tuning via derivative-free search for general programs with non-differentiable operations, and autodiff-based optimization using Adam for fully differentiable pipelines where gradient information enables efficient navigation of high-dimensional parameter spaces. Second, automated report generation produces human-readable analyses through a three-stage pipeline: lineage extraction traces the evolutionary ancestry of discovered algorithms, comparative analysis invokes an LLM to identify structural differences and hypothesize physical rationale, and synthesis aggregates these into coherent narratives with annotated code comparisons and performance visualizations. Prompt Sampler 

> LLM Ensemble
> Parameter Optimization
> Evaluator Pool
> Program DB
> Report Generator
> sample parents
> build prompt
> generate diff
> submit refined candidate
> score & update
> best solution
> Human-readable explanations

Figure 1: Overview of the LLM-driven evolutionary pipeline in MadEvolve. The cycle proceeds as a closed loop: the Prompt Sampler retrieves parent programs from the Program DB to query the LLM Ensemble .The generated code diffs first undergo Parameter Optimization to refine continuous variables before being submitted to the Evaluator Pool . Evaluation scores update the database, completing the iteration. Finally, the best solution is transmitted to the central Report Generator to produce human-readable explanations of the discovered innovations. 

# 3.2 Core Architecture and Evolution Loop 

The central controller orchestrates the evolutionary process through an asynchronous loop coordinating sampling, generation, evaluation, and selection. Each iteration proceeds through six stages. 

• Parent selection draws a program from the population, balancing exploitation of high-fitness solutions against exploration through inter-island migration (Section 3.4). 

• Inspiration retrieval gathers k exemplar programs including the global best, recent top performers, and structurally diverse neighbors from the MAP-Elites grid (Section 3.4). 6• In prompt construction , the system assembles a prompt containing the parent’s source code, perfor-mance metrics, identified areas for improvement, and retrieved exemplars. Variation generation then queries the LLM ensemble for code modifications—either targeted diff patches for incremental refine-ment or complete rewrites for exploring different approaches (Section 3.5). 

• The candidate undergoes fitness evaluation against the task-specific protocol. When the program contains tunable parameters (e.g., smoothing scales, learning rates), an inner optimization loop auto-matically calibrates these values before computing fitness (Section 3.3). 

• Finally, population update incorporates the program into the database, updating the MAP-Elites grid, island populations, and elite archive (Section 3.4). The loop continues until reaching a target fitness, exhausting the iteration budget, or detecting stagnation. 

# 3.3 Parameter Tracking and Optimization 

Compared to the original AlphaEvolve, our work emphasizes the role of tunable parameters. We aim to find algorithms that have as few as possible free parameters (which usually improves generalization), and we track their meaning and optimize their numerical values. When the LLM proposes a new algorithm, it often introduces tunable parameters (smoothing scales, threshold values, iteration counts) whose optimal values are not known in advance. If we evaluate the algorithm with poorly chosen parameters, we might reject a genuinely good idea simply because it was not properly tuned. On the other hand, running extensive optimization for every candidate would be prohibitively expensive. We resolve this tension through a budget-constrained inner optimization loop that tunes parameters automatically while keeping computational costs under control. Depending on the nature of the algorithm and its parameters, we offer two complementary optimization strategies: greedy one-shot tuning for general programs and autodiff-based optimization for fully differentiable pipelines. 

3.3.1 Greedy One-Shot Optimization 

For general algorithms that may contain non-differentiable operations, we employ a greedy one-shot protocol: each newly introduced parameter gets tuned exactly once when it first appears, after which its value is frozen. The system detects new parameters by comparing the child program’s parameter set against its parent. Only parameters that appear for the first time trigger optimization; inherited parameters retain their previously optimized values. To enable automatic optimization, evolved programs declare their tunable parameters in a configuration block, specifying for each parameter: an initial value, the optimization method (e.g., grid for derivative-free search), and valid bounds. We perform derivative-free search within a strict budget of B evaluations, where the default budget and search strategy are specified in the system configuration file. For instance, with a budget of 10 evaluations, one or two parameters can be tuned via grid search, while larger parameter sets may require Latin hypercube sampling or Bayesian optimization with a Gaussian process surrogate to focus evaluations on promising regions. After optimization, the best values found are injected back into the program source code and the pa-rameters become frozen for subsequent generations. This protocol keeps optimization costs low—not every mutation introduces new parameters, and when optimization does occur, the overhead typically amounts to only a small multiple of a single evaluation cost. If the LLM later decides that a frozen parameter needs adjustment, perhaps because subsequent structural changes have shifted the optimal value, it must remove and reintroduce the parameter through a code rewrite. This triggers a fresh optimization cycle. The mecha-nism prevents parameters from drifting indefinitely while still allowing the system to revisit tuning decisions when the algorithm structure changes significantly. While we used one-shot tuning in early stages of this work, the final applications described below all make use of fully auto-differentiable algorithms, as described in the next section. 

3.3.2 Auto-differentiable Parameter Optimization 

For algorithms where the target metric can be made differentiable with respect to the algorithm parameters, we offer an alternative optimization strategy based on automatic differentiation that is substantially more 7powerful for continuous parameters. This approach requires translating the initial program into a fully dif-ferentiable form (e.g., using JAX), after which gradients of the fitness function with respect to all continuous parameters can be computed efficiently through reverse-mode automatic differentiation. As in the one-shot case, each program declares its tunable parameters in a configuration block, now specifying autodiff as the optimization method along with initial values and valid bounds. Before each fitness evaluation, the system runs gradient-based optimization on all parameters marked for autodiff tuning. Using the Adam optimizer, the system iteratively adjusts parameter values to maximize fitness, typically converging within 10–50 iterations. Once optimized, the parameter values are injected back into the program source for subsequent evaluations. The autodiff approach offers key advantages over derivative-free methods: gradient information enables efficient navigation of high-dimensional parameter spaces, allowing simultaneous optimization of 10+ contin-uous parameters within dozens of iterations rather than exponentially many function evaluations. The main requirement is that the evolved algorithm must use differentiable operations throughout. While in principle this includes the case of training a large neural network, MadEvolve tracks individual parameters and is not meant for neural network training (as we indicate to the LLMs in their prompts). 

3.3.3 Choosing Between Strategies 

The choice between greedy one-shot tuning and autodiff optimization depends on the problem structure. Autodiff is preferred when: (i) the fitness metric is smooth and differentiable with respect to parameters, (ii) the algorithm involves primarily linear algebra, FFTs, and smooth nonlinearities, and (iii) multiple continuous parameters need simultaneous optimization. Greedy one-shot tuning is preferred when: (i) the algorithm contains fundamentally discrete choices or non-differentiable operations and the number of tunable parameters is small, or (ii) rapid prototyping is prioritized over optimal parameter values. Both strategies share the same goal—ensuring that fitness comparisons are fair by evaluating each algorithm at its best achievable performance rather than penalizing suboptimal initial parameter choices. MadEvolve supports mixed algorithms, where some parameters are auto-differentiable and others are not. 

# 3.4 Hybrid Population Management 

Population management is a core component of LLM-guided evolutionary systems and has been addressed in different ways by prior work. AlphaEvolve [2] combines a MAP-Elites quality-diversity archive [18] with an island model, using three fixed feature dimensions (performance, code diversity via Levenshtein dis-tance, and complexity via code length) and a default configuration of 5 islands with up to 25 programs each. OpenEvolve [35] generalizes this with configurable feature dimensions, dynamic feature scaling, and adaptive bin counts, while maintaining the same island-based architecture with ring-topology migration. ShinkaEvolve [34] takes a different approach, relying solely on an island model without MAP-Elites, and instead maintaining diversity through novelty-based rejection sampling that uses embedding similarity com-bined with an LLM-as-judge to filter near-duplicate proposals before evaluation. MadEvolve adopts a hybrid strategy that draws from all three systems but simplifies certain aspects that we found unnecessary for our physics applications. Our population database employs three complementary mechanisms. The first is a MAP-Elites grid [18], which partitions behavioral space into an n-dimensional structure G =Qni=1 [0 , b i) ⊂ Zn. Following the feature dimensions used by AlphaEvolve and OpenEvolve, we define three axes: complexity (code character length), diversity (mean cosine distance of the program’s code embedding to a reservoir-sampled reference set of representative programs), and performance (the combined fitness score). Each cell retains exactly one program; a new program replaces the incumbent only when demonstrating superior performance. We chose MAP-Elites over the novelty rejection sampling approach of ShinkaEvolve because it provides a persistent archive of structurally diverse solutions that can serve as inspiration during prompt construction, rather than only filtering at generation time. For our cosmological applications, where algorithmic strategies can differ qualitatively (e.g., single-pass versus multi-pass reconstruction, different physical correction terms), preserving this diversity explicitly in the archive proved more effective than relying on embedding-based duplicate detection. The second mechanism is an island model , which partitions the population into m semi-isolated subpop-ulations I1, . . . , Im, a standard approach used in all three reference systems. Islands evolve independently 8while periodic migration events (every Tmig generations, transferring a fraction ρ of each island’s best mem-bers to its neighbor via a ring topology) enable cross-pollination, preventing premature convergence by maintaining competing lineages. By default we set m = 5, Tmig = 5, and ρ = 0 .1. We use a fixed-interval migration schedule as in AlphaEvolve, rather than OpenEvolve’s event-driven lazy migration (triggered by program addition counts), as we found fixed intervals simpler to reason about and sufficient for our problem sizes. Unlike ShinkaEvolve, we do not enforce island elitism (protecting the best program on each island from being replaced by migrants), since in our experience the global elite archive (described below) already preserves top performers. The third mechanism maintains a global elite archive A of fixed capacity that preserves the highest-performing programs by fitness score; when the archive is full, a new program is admitted only if it surpasses the current worst entry. This component is not present in ShinkaEvolve, which relies on per-island archives, and serves as an additional safeguard against losing high-performing solutions during migration or MAP-Elites cell replacement. Together, these three mechanisms ensure both breadth (MAP-Elites diversity and island isolation) and depth (elite preservation and local exploitation). Compared to AlphaEvolve, our setup omits meta-prompt evolution (where the system co-evolves the prompts themselves in a parallel database) and explicit novelty filtering. We found that for our physics applications, where the evaluator provides a strong and informative fitness signal, letting the physics-based metrics guide the search directly was sufficient to maintain productive evolution without the additional complexity of novelty detection. 

# 3.5 LLM Ensemble and Generation Strategies 

All three reference systems—AlphaEvolve [2], OpenEvolve [35], and ShinkaEvolve [34]—employ ensembles of language models rather than relying on a single model, as different LLMs exhibit complementary strengths: fast, lightweight models (e.g., Gemini Flash) maximize throughput by testing many candidates, while larger, more capable models (e.g., GPT-5, Gemini Pro) occasionally propose higher-quality structural innovations. AlphaEvolve uses a fixed weighting of approximately 80% Flash and 20% Pro models. ShinkaEvolve goes further by treating model selection as a multi-armed bandit problem, dynamically routing queries to the model that has historically produced the largest fitness improvements, using a UCB1 strategy [36] where the reward signal is defined as the relative improvement over the parent program’s fitness. OpenEvolve uses simpler probabilistic model selection based on configured weights. MadEvolve adopts the bandit-based approach of ShinkaEvolve, which we found to be more effective than fixed weights because the relative utility of different models changes over the course of evolution—lightweight models are often sufficient for early exploration, while later refinement of already-optimized code benefits from more capable models. Concretely, each model Mi is treated as an arm in a UCB1 bandit [36]. After every model has been queried at least once, the system computes UCB i =



¯ri + ct

r ln Nni



wi , (1) where ¯ ri is the empirical mean score improvement produced by model Mi, ni the number of queries issued to it, N = P 

> j

nj the total query count, and wi a prior base weight reflecting cost or capability preferences. The exploration coefficient decays geometrically as ct = c0 γN with initial value c0 and decay rate γ < 1, so that the selector gradually shifts from exploration to exploitation as evidence accumulates. At each step the model with the highest UCB i is selected greedily. Thompson Sampling and ε-greedy selection are available as alternative strategies. Unlike ShinkaEvolve, which defines the reward as ri = exp(max( r − rbase , 0)) − 1to incentivize bold improvements, we use the raw score improvement as the reward signal, which we found to be more stable for our physics applications where the fitness landscape is relatively smooth. Regarding code generation modes, AlphaEvolve and OpenEvolve support both diff-based editing (using SEARCH/REPLACE blocks) and full code rewrites, with diff-based editing as the default mode. ShinkaE-volve adds a third crossover mode in which the LLM receives code from multiple parent programs and performs guided recombination. MadEvolve’s ensemble supports three analogous generation modes. In 

differential mode , the LLM produces diff patches modifying specific code regions, which dominates during exploitation phases. In full rewrite mode , the LLM generates complete programs from scratch, enabling radical departures from the current solution. In synthesis mode , the LLM combines ideas from multiple 9reference programs to produce improved variants, similar to the crossover operator in ShinkaEvolve. By default, differential patches are selected with ∼70% probability and full rewrites with ∼30%, consistent with the exploitation/exploration ratios used in AlphaEvolve and OpenEvolve. The relative weights of all three modes are freely adjustable via configuration. 

# 3.6 Automated Report Generation 

A distinguishing capability of MadEvolve is automated interpretability: the system generates comprehen-sive reports analyzing discovered algorithms relative to baselines. Existing open-source evolutionary coding systems—including OpenEvolve [35] and ShinkaEvolve [34]—primarily output evolved code and numeri-cal metrics (fitness scores, generation statistics), but do not automatically produce human-readable scien-tific analyses of what the evolved algorithm does differently and why those changes improve performance. OpenEvolve provides evolution tree visualization and per-generation statistics, and ShinkaEvolve offers an interactive web dashboard with genealogy trees and performance graphs, but neither system generates a narrative report explaining the algorithmic innovations in domain-specific terms. AlphaEvolve [2] does not describe such a capability either. For scientific applications, where the goal is not merely to obtain a high-performing program but to understand the principles behind its success, this interpretability gap is a significant limitation. A researcher receiving an evolved cosmological reconstruction algorithm needs to understand whether the improvements arise from known physical effects (e.g., second-order perturbation theory corrections) or genuinely novel strategies, and whether the approach generalizes beyond the training setup. MadEvolve addresses this gap through a three-stage automated report generation pipeline. First, lineage extraction traces the best program’s full evolutionary ancestry back to the initial seed. Each program in the lineage is stored with its generation number, fitness score, the proposing LLM model name, and the diff that produced it (all recorded in the program metadata). The lineage reconstruction identifies the complete chain of mutations that transformed the baseline into the final algorithm, enabling attribution of specific innovations to particular evolutionary steps. The default report renders the evolutionary path with per-step score deltas, revealing whether progress was gradual (many small incremental improvements) or punctuated (a few large jumps interspersed with plateaus). This information is valuable for understanding the difficulty of the optimization landscape and for guiding future prompt engineering: if most progress came from a single breakthrough mutation, the prompt that triggered it can be studied and reused. Second, comparative analysis invokes an LLM to analyze the structural differences between the final and baseline algorithms. The system provides both complete code listings and a metrics comparison table to the LLM, which is prompted via a domain-specific adapter to identify key changes and hypothesize physical rationale for each modification, grounded in the application context. The domain-specific adapter is a configurable component that injects relevant scientific background into the analysis prompt—for example, for BAO reconstruction, it provides context about Lagrangian perturbation theory, displacement fields, and mode coupling, enabling the LLM to connect code-level changes (e.g., the introduction of deformation tensor invariants) to their physical interpretation (e.g., capturing second-order gravitational effects). This grounding in domain knowledge distinguishes the reports from generic code-diff summaries. Third, synthesis aggregates these analyses into a coherent narrative document including: a metrics com-parison table with per-scale breakdowns, LLM-generated descriptions of the baseline and best algorithms, an improvement analysis highlighting what changed and why, an executive summary of the key findings, and annotated side-by-side code comparisons. The report is designed to serve as a starting point for hu-man researchers, enabling rapid assessment of whether discovered algorithms merit further investigation, publication, or deployment in observational pipelines. In practice, we found that these reports substantially accelerated our own understanding of the evolved algorithms and reduced the time needed to identify which innovations were scientifically meaningful versus which may be artifacts of overfitting to the training metric. Nevertheless these reports do have some shortcomings inherited from the limited reasoning ability of current LLMs, as we comment on further below. More details of the generated reports and references to the reports for all three applications are provided in Appendix C. 10 4 Overview of Results 

We applied MadEvolve to three distinct cosmological data analysis problems: baryon acoustic oscillation (BAO) reconstruction, 21 cm foreground contamination mitigation via tidal field methods, and baryonic physics predictions from N-body dark matter simulations. For all these cases, we implemented base-algorithms from the literature and made them auto-differentiable with respect to their parameters. Across these applications, the system discovered algorithms that substantially outperform conventional baselines, with performance improvements ranging from 2.8% to 30.7% in cross-correlation metrics and up to 63% reduction in prediction loss. Table 1 summarizes the key outcomes. The evolutionary runs required between 233 and 1,165 generations (see App. A.1), demonstrating that meaningful algorithmic improvements can be achieved without extensive infrastructure. 

Application Baseline Evolved Improvement Generations 

BAO Reconstruction with StdRec (¯ rBAO ) 0.752 0.924 22.8% 1,165 BAO Reconstruction with IterRec (¯ rBAO ) 0.933 0.959 2.8% 386 21 cm Tidal Reconstruction (¯ r2D ) 0.743 0.971 30.7% 342 N-body baryonic displacements (test loss) 0.613 0.230 63% 233 Table 1: Summary of performance improvements across four evolutionary runs on three cosmological appli-cations. BAO and tidal reconstruction metrics are cross-correlation coefficients (higher is better); tSZ uses L1 loss (lower is better). All evolved results represent the best algorithm discovered during the evolutionary run. 

# 4.1 Summary of Achievements 

We run MadEvolve on three problems of computational cosmology, using four evolutionary runs. We briefly summarize the results, which will be explained in detail in the following sections. For the application of initial condition and BAO reconstruction, our first algorithm evolution starts from the widely used standard BAO reconstruction baseline. The evolved algorithm incorporates higher-order perturbation theory corrections on top of the standard Zel’dovich displacement, and uses multiple tracer fields with scale-dependent weights. This achieves ¯ rBAO = 0 .924 compared to 0.752 for the optimized Zel’dovich baseline, a 22.8% improvement that translates to substantially better recovery of initial conditions at quasi-linear scales ( k ∼ 0.1–0 .3 h Mpc −1) where nonlinear gravitational evolution most severely degrades the BAO signal. However, the evolution does not quite reach state-of-the-art performance. We thus ran a second evolution run starting from the stronger iterative reconstruction baseline [37], which is the best human algorithm to our knowledge. In this run, MadEvolve again improves the target metric from ¯ rBAO = 0 .933 for the iterative baseline to ¯ rBAO = 0 .959 after evolution. This shows that combining a human-designed state-of-the-art algorithm with LLM-evolved refinements yields the best overall results. Notably, evolution starting from standard reconstruction did not independently rediscover the iterative reconstruction algorithm, despite this being a well-known technique; this suggests that the system is more effective at refining and extending existing algorithms than at retrieving known solutions from its training data. The 21cm foreground contamination reconstruction application yielded the most striking results. Start-ing from a baseline cross-correlation of ¯ r2D = 0 .743 in the wedge-contaminated region of Fourier space, the evolved algorithm achieves ¯ r2D = 0 .971, a 30.7% improvement. The algorithm improvements primarily involve a better treatment of the anisotropic nature of the problem, which is physically reasonable. Inter-estingly, the algorithm discovered at generation 52 captures essentially all of the improvement with only 9 tunable parameters, while later evolution of up to 60 parameters does not yield significant further gains, indicating that an efficient parametrization has been found. For the effective baryonic physics from N-body simulations application, the evolved algorithm reduces test loss by 63% while improving cross-correlation from 0.943 to 0.969. The key change is to predict the tSZ field using a multiplicative decomposition that factorizes the Compton-y prediction into an electron density term and an effective temperature term, which is physically reasonable and was not enforced in the base algorithm. 11 In summary, the found algorithms contain many physically plausible ideas, and indeed increase the performance of the algorithms substantially. To achieve a similar success, human researchers would need significant domain knowledge and/or try out a large number of ideas in a time consuming process. However, one would not currently call the evolved algorithms elegant. They resemble a patchwork of ideas, not a systematic human exploration which could come with optimality proofs (in certain regimes) and other mathematical derivations. Nevertheless, the found algorithms may have practical value on real data analysis tasks given further investigation. In addition, successful algorithm modifications can provide ideas for human scientists. It also seems likely that evolved algorithms will become increasingly more sophisticated with stronger models and improved evolution systems (see Sec. 8). 

# 4.2 Difficulties and Engineering Challenges 

We encountered several practical challenges during development. 

Prompt engineering. Writing good prompts proved crucial for obtaining productive LLM mutations and emerged as one of the most important factors determining evolution quality. Effective prompts must provide detailed physical background and domain knowledge, enabling the LLM to propose modifications grounded in the underlying physics. One may wonder whether this would allow the LLM to simply recall entire existing algorithms from the literature, but we have not observed this happening in practice. We further found that merely encouraging the LLM to “improve” the baseline produced incremental refinements that rarely escaped local optima, whereas explicitly encouraging more radical and novel approaches led to faster discovery of qualitatively new solutions. Importantly, providing specific suggestions about which aspects to modify proved counterproductive by constraining creativity; prompts that supplied rich physical context while granting the LLM full autonomy in deciding what to change produced the most effective innovations. We also observed that without explicit constraints, LLMs tended to continuously increase parameter counts, reducing interpretability. Specifying limits such as “maintain fewer than 10 tunable parameters” controlled this tendency without sacrificing performance: for tidal reconstruction, the 9-parameter algorithm at generation 52 achieved ¯ r2D = 0 .97, while subsequent 30 to 60 parameter variants improved this by less than 0.4%. The prompts for our final runs can be found in App. A.2. 

Evaluator design. Careful evaluator design proved equally critical, as poorly constructed metrics readily induce reward hacking. In the BAO reconstruction application, we initially used the mean cross-correlation ¯r(k) averaged uniformly across all scales as the fitness metric. Under this objective, evolution discovered algorithms that dramatically sacrificed large-scale reconstruction accuracy to achieve marginal gains at small scales, exploiting the equal weighting in the average. The resulting algorithms were scientifically useless despite achieving high fitness scores, as BAO analyses primarily rely on large-scale modes where the acoustic signal resides. To prevent such behavior, we introduced explicit penalty terms that constrained the minimum acceptable correlation at large scales, ensuring that improvements at small scales could not come at the expense of the primary science target. This modification effectively eliminated reward hacking and guided evolution toward algorithms with balanced performance across the relevant scale range. 

Balancing Exploration and Exploitation. The balance between exploration and exploitation required careful tuning throughout the evolutionary process. Setting the exploitation weight too high caused pre-mature convergence to local optima, where the population became trapped refining minor variations of a suboptimal solution rather than discovering fundamentally better approaches. Conversely, excessive ex-ploration wasted computational budget on syntactically invalid or obviously inferior programs, as overly aggressive mutations often destroyed the functional structure that made parent programs effective. The op-timal balance appears to be problem-dependent, influenced by factors such as the ruggedness of the fitness landscape, the complexity of the algorithm being evolved, and the available computational budget. Maintaining population diversity was essential for escaping local optima and sustaining productive evo-lution over hundreds of generations. The MAP-Elites grid and island model together prevented the popu-lation from collapsing to a single lineage, preserving multiple competing algorithmic strategies that could be recombined or serve as stepping stones to novel solutions. However, these mechanisms required careful 12 calibration. Several early runs exhibited “mode collapse” where all islands converged to nearly identical pro-grams, producing diminishing returns from continued evolution as the population lost the diversity necessary for meaningful exploration. Mitigating this issue required adjusting several hyperparameters described in Sections 3.4 and 3.5, most importantly the migration interval Tmig , the migration fraction ρ, the UCB ex-ploration coefficient c0 and its decay rate γ, and the relative weights of generation modes. In practice, we found that short preliminary runs of 30–50 generations were sufficient to diagnose mode collapse or exces-sive exploration and to identify reasonable hyperparameter settings before committing to full evolutionary campaigns. 

# 4.3 Model Contributions and Global Statistics 

Table 2 presents aggregate statistics across the four evolutionary runs. The experiments collectively ran 2,126 generations. The fraction of candidate programs that successfully compiled and produced meaningful results varied considerably across applications, ranging from 54.5% for the tSZ prediction task to 85.7% for tidal reconstruction, reflecting the differing complexity of maintaining syntactic validity and numerical stability across problem domains. Beyond raw success rates, we report the improvement rate : the fraction of all mutations (including failed ones) that produce a valid program with fitness exceeding its parent’s. This metric directly quantifies how efficiently evolution converts computational effort into progress. Improvement rates range from 13.4% for tSZ to 30.4% for BAO reconstruction, indicating that roughly one in three to one in eight mutations advances the fitness frontier. The low improvement rate for tSZ reflects both its lower success rate and the difficulty of improving an already well-optimized differentiable pipeline built on the vmad framework. 

Metric BAO Iter. Recon. Tidal tSZ 

Total generations 1,165 386 342 233 Successful programs 897 323 293 127 Success rate (%) 77.0 83.7 85.7 54.5 Improvement rate (%) 30.4 29.8 24.8 13.4 Table 2: Global statistics across the four evolutionary runs. Success rate indicates the fraction of LLM-generated programs that compile and execute without errors. Improvement rate indicates the fraction of all mutations that produce a valid program with higher fitness than its parent. Each evolutionary run employed a heterogeneous ensemble of frontier language models operating in parallel across five islands, with model selection governed by an Upper Confidence Bound (UCB) bandit algorithm that dynamically balanced exploration against exploitation. Seven distinct models contributed mutations across the four runs, with different subsets used in each application depending on model availability at the time of the experiment. Table 3 summarizes per-model contributions aggregated across all applications, reporting not only the number of mutations and success rate but also the improvement rate and the average number of mutations required to produce one fitness-improving program (mutations per improvement). Gemini 3 Pro Preview generated the most mutations overall (619 of 2,126, 29.1%), while Gemini 3 Flash Pre-view achieved both the highest success rate (89.5%) and the highest improvement rate (32.9%), requiring only 3.0 mutations per improvement on average. By contrast, GPT-5 and o4-mini required 4.9 and 4.6 mutations per improvement, respectively. To assess whether frontier models are substantially more effective than lightweight ones, we group models into frontier-class (Gemini 3 Pro, Gemini 2.5 Pro, GPT-5, GPT-5.2) and lightweight (Gemini 3 Flash, Gemini 2.5 Flash, o4-mini). The aggregate improvement rates are comparable: 27.9% for frontier models versus 26.3% for lightweight models, corresponding to 3.6 and 3.8 mutations per improvement, respectively. This suggests that lightweight models can serve as competitive evolutionary contributors despite their lower per-mutation success rates. Within the lightweight category, however, performance varies substantially: Gemini 3 Flash Preview outperforms all other models on both metrics, while Gemini 2.5 Flash shows the lowest overall success rate (62.6%), particularly on the tSZ task (21.9%) where the stringent differentiability requirements of the vmad framework demand careful attention to operator compatibility. 13 Model N Success (%) Impr. (%) Muts/Impr.                                     

> Gemini 3 Pro Preview 619 77.9 28.9 3.5 GPT-5.2 303 76.2 29.7 3.4 o4-mini 320 65.9 21.9 4.6 Gemini 2.5 Pro 291 82.1 28.9 3.5 Gemini 3 Flash Preview 228 89.5 32.9 3.0 GPT-5 186 84.4 20.4 4.9 Gemini 2.5 Flash 179 62.6 25.7 3.9 Frontier (aggregate) 1,399 79.3 27.9 3.6 Lightweight (aggregate) 727 72.5 26.3 3.8

Table 3: Per-model mutation statistics aggregated across all four applications. N : total mutations generated; Success: fraction producing valid, executable programs; Impr.: fraction of all mutations yielding a fitness score exceeding the parent’s; Muts/Impr.: average number of mutations needed to produce one fitness-improving program. Frontier models: Gemini 3 Pro, Gemini 2.5 Pro, GPT-5, GPT-5.2. Lightweight models: Gemini 3 Flash, Gemini 2.5 Flash, o4-mini. Not all models were used in every run. The best-performing algorithms in each application were discovered by different models: Gemini 3 Pro Pre-view produced the top-ranked BAO reconstruction (¯ rBAO = 0 .924, generation 1144), GPT-5.2 generated the best iterative reconstruction variant (¯ rBAO = 0 .959, generation 377), GPT-5 produced the best tidal recon-struction algorithm (¯ r2D = 0 .973, generation 321), and Gemini 2.5 Pro discovered the optimal tSZ predictor (test loss reduction of 63%, generation 222). This diversity suggests that no single model dominates algo-rithmic discovery; different models contribute complementary strengths depending on the problem structure. The UCB selection mechanism adapted model utilization dynamically throughout evolution, adjusting sam-pling probabilities based on each model’s track record of producing fitness improvements on each island. 

# 5 Application I: Initial Condition and BAO Reconstruction 

In the following three sections we apply MadEvolve to three different applications in cosmology. In each case, we describe the computational task, human baseline algorithm, the novel evolved algorithm, and then put the evolved algorithm in context with existing approaches in the literature. 

# 5.1 Problem Formulation and Scientific Motivation 

Our first application is the reconstruction of the unobservable initial matter distribution of the universe from the observable late-time distribution. The primary scientific application for this process is to measure the baryon acoustic oscillations (BAO) more precisely. Baryon acoustic oscillations are a key cosmological probe, marking the scale where primordial sound waves froze at recombination. At low redshifts ( z ≲ 2), gravity drives bulk flows that blur the BAO signal and reduce its precision as a standard ruler. Reconstruction algorithms partially undo this blurring by estimating and removing the displacements caused by gravitational evolution. We frame reconstruction as an optimization problem: find algorithms that maximize the cross-correlation between reconstructed and true initial density fields in Fourier space: 

r(k) = ⟨δrec (k)δ∗

> IC

(k)⟩

p⟨| δrec (k)|2⟩⟨| δIC (k)|2⟩ , (2) where δIC is the initial field at z ≈ 127 (linear regime), δrec is our reconstruction from z = 0 data, and brackets denote k-shell averaging. Our main goal is maximizing: ¯rBAO = 1

Nk

X

> k∈[kmin ,k max ]

r(k) (3) 14 averaged over k ∈ [0 .01 , 0.5] h Mpc −1 where BAO information lives. This measures how well we recover the initial field at BAO scales, covering both the acoustic peak ( k ∼ 0.1 h Mpc −1) and the damping tail toward smaller scales. 

# 5.2 Experimental Setup 

Our evolutionary framework begins with two different reconstruction methods: the standard reconstruction [38] and iterative reconstruction [37] as the baseline algorithms. In the following section, we present the evolved algorithms obtained from these two baselines, respectively. Standard reconstruction, which is widely adopted in cosmological analyses for its computational efficiency and solid theoretical foundation, employs a single-step linear displacement field estimation based on the Zel’dovich approximation. The reconstruction proceeds by solving for the Zel’dovich displacement field Ψ

that maps particles from their observed Eulerian positions x back to their initial Lagrangian coordinates q:

q = x − Ψ(x), Ψ = −∇ Φ, ∇2Φ = δs (4) Here δs represents the smoothed overdensity field obtained by convolving the observed density contrast with a Gaussian kernel W (k) = exp( −k2R2 

> s

/2), Φ refers to the potential field. We start with a smoothing scale of 

Rs = 10 h−1 Mpc, which effectively suppresses small-scale nonlinear effects while preserving the large-scale velocity flow patterns crucial for BAO reconstruction. Built upon the standard reconstruction, the iterative baseline aims to more accurately approximate the nonlinear displacement field by repeatedly applying the linear reconstruction procedure. At each step, the Zel’dovich displacement field is computed from the density field using a progressively decreasing smoothing scale given by Ri = 0 .5i × 10 h−1 Mpc , with a minimum smoothing scale of Rmin = 1 h−1 Mpc. The cumulative displacement from the original catalog to the nearly uniform configuration provides an estimate of the Eulerian-to-Lagrangian displacement field χ. The reconstructed density field is then approximated as 

∇ · χ according to the Eulerian continuity equation. To our knowledge, this algorithm is the human state-of-the-art algorithm (SotA), when considering only reconstruction algorithms rather than forward modeling or neural networks (which are much harder to deploy in practice). Unfortunately, unlike in machine learning, in physics there are usually no algorithm leader boards on unified data sets, which would clearly identify the SotA algorithm. To enable the autodiff-based parameter optimization strategy described in Section 3.3, we implement the entire reconstruction pipeline in JAX, ensuring that all operations—Gaussian filtering, displacement field computation, particle shifting, and Cloud-in-Cell mass assignment—are fully differentiable. Each evolved program declares its tunable parameters through structured annotations specifying initial values, valid bounds, and the optimization method. Before fitness evaluation, the system invokes JAX’s reverse-mode au-tomatic differentiation to compute gradients of ¯ rBAO with respect to all continuous parameters, then applies the Adam optimizer with learning rate η = 0 .1 and standard momentum coefficients ( β1 = 0 .9, β2 = 0 .999). Optimization typically converges within 30–50 iterations, with early stopping triggered when the gradient norm falls below 10 −5. This approach enables efficient navigation of parameter spaces containing 10 or more continuous variables, which would be prohibitively expensive to explore via grid search or derivative-free methods. The optimized parameter values are then injected back into the program source code, ensuring that subsequent fitness evaluations and offspring generation reflect the algorithm’s best achievable perfor-mance. We evaluate all candidate algorithms based on the Quijote N-body simulation suite [39], which pro-vides both final matter distributions and the corresponding initial conditions necessary for ground-truth comparison. Table 4 summarizes the simulation parameters. The availability of true initial conditions at 

zIC = 127 enables direct quantification of reconstruction fidelity through Fourier-space cross-correlation. We use Equation 2 as the primary performance metric. Specifically, we use the BAO-averaged correlation ¯ rBAO ,computed as the mean of r(k) over 50 logarithmically-spaced bins spanning k ∈ [0 .01 , 0.5] h Mpc −1. This range encompasses both the typical BAO range at around k ∼ [0 .01 , 0.3] h Mpc −1 and the damping tail toward smaller scales where nonlinear information resides. For computational efficiency during the evolutionary search, we use the grid resolution with 256 3 voxels, preserving the BAO features while reducing evaluation time. From the 100 independent realizations in the Quijote fiducial set, we use a single simulation (index 0) for fitness evaluation during evolution, with the 15 Parameter Value 

Box size L = 1000 h−1 Mpc Particle count Np = 512 3

Mass resolution mp = 6 .56 × 10 11 h−1M⊙

Grid resolution Nmesh = 256 3

Initial / final redshift zIC = 127, zfinal = 0 Cosmology Ωm = 0 .3175, h = 0 .6711, σ8 = 0 .834 Table 4: Quijote simulation parameters for BAO reconstruction evaluation. remaining simulations reserved for final generalization testing. Even though we use only a single training simulation, we find no evidence of overfitting when applying the method to independent test simulations in Sec. 5.3.3. This design choice prioritizes rapid iteration over per-generation statistical averaging, as the autodiff-based parameter optimization already ensures each candidate algorithm achieves its optimal performance on the training simulation. A single fitness evaluation comprises the reconstruction algorithm execution plus up to 20 Adam iterations for parameter optimization. The baseline Zel’dovich method executes in approximately 10 seconds per reconstruction, while the fully-evolved algorithm with deformation tensor corrections requires approximately 76 seconds due to the additional FFT operations for computing tensor invariants and the multi-pass architecture. Including the autodiff optimization loop, a complete fitness evaluation ranges from 3–5 minutes for simple algorithms to 25–30 minutes for the most complex evolved variants. 

# 5.3 Results - Starting from Standard Reconstruction 

In the present section we present the algorithm evolution starting from a well-known but suboptimal human baseline, standard BAO reconstruction. In the following section, we present the algorithm evolution starting from the human SotA algorithm, iterative initial condition reconstruction. 

5.3.1 Evolution Dynamics and Performance Trajectory 

We applied MadEvolve to standard BAO reconstruction as described above. The evolutionary search discov-ered progressively more sophisticated reconstruction algorithms through a combination of structural inno-vations proposed by the LLM (outer optimization loop) and automatic parameter optimization via autodiff (inner optimization loop). Figure 2 illustrates the progression of the best-performing algorithm over 1,165 generations. Starting from the baseline standard reconstruction with ¯ rBAO = 0 .752 and only two tunable pa-rameters (smoothing scale and bias), the system explored modifications including higher-order perturbative corrections and spectral refinement techniques. During the first 100 generations, performance improved rapidly from 0.752 to approximately 0.82. The parameter count then increased to approximately 6 as physics-based corrections were introduced, and per-formance continued climbing steadily through ¯ rBAO ≈ 0.85–0 .88. A significant structural breakthrough occurred around generations 600–650, when the system discovered three key innovations: tensor invariant augmentation , spectral de-warping , and coherence-gated fusion . We discuss the physical interpretation of these innovations in the next section. The final evolved algorithm achieves ¯ rBAO = 0 .924, representing a 22.8% improvement over the optimized baseline, with the best solution found at generation 1,144. 

5.3.2 Discovered Algorithmic Techniques 

We start with MadEvolve’s own LLM-generated report of its innovations, which has been lightly edited for conciseness. We then provide comments by a human expert who reviewed the report and final code. For the complete unedited report please refer to our website 1 . The excerpt below is contained in the section Best Evolved Algorithm Analysis .

> 1https://madevolve.org/reports/bao evolution report.pdf

16 0 200 400 600 800 1000 

> Generation
> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Combined Score
> Performance Improvement: +22.8%
> Final: 0.924 (10 params)
> Initial: 0.753 (2 params)
> Best Combined Score
> Initial Baseline
> Number of Parameters
> 0
> 2
> 4
> 6
> 8
> 10
> 12
> 14
> 16
> 18
> Number of Parameters

Figure 2: Evolution dynamics of BAO reconstruction algorithm discovery over 1,165 generations. The blue curve tracks the best training-set score ¯ rBAO achieved by any program in the population, while the green curve indicates the number of autodiff-optimizable parameters in the best-performing algorithm. The red dashed line marks the initial baseline performance (¯ rBAO = 0 .752) achieved by the two-parameter Zel’dovich method. The evolutionary trajectory exhibits distinct phases: rapid initial improvement during the first 100 generations, a gradual increase through ¯ rBAO ≈ 0.85–0 .88 as parameter count stabilizes near 10, a notable jump around generation 600–650, and continued refinement toward the final best score of ¯ rBAO = 0 .924 found at generation 1,144. The +22.8% improvement demonstrates sustained evolutionary progress through the combination of LLM-proposed structural modifications and autodiff parameter optimization. 17 LLM-Generated Algorithmic Summary 

This algorithm employs a hybrid iterative displacement field reconstruction method, rooted in the Zeldovich approximation but significantly augmented by higher-order Lagrangian perturbation theory (LPT) terms and a multi-stage spectral filtering process. It operates in two distinct phases: a primary bulk-flow removal step (standard reconstruction) followed by a sophisticated “physics augmentation” phase that attempts to recover information lost to non-linear evolution using a basis of tidal and density invariants. The reconstruction begins with a standard Zeldovich step, solving for the displacement field Ψ using a smoothed density field and removing redshift-space distortions (RSD) via an anisotropic kernel. The innovation lies in the subsequent steps: 1. Tensor Invariant Augmentation: The algorithm computes the shear tensor invariants ( I1, I2,

I3) of the initial displacement field and uses them, along with an advection term (Ψ · ∇ δ), to construct a secondary displacement source. This mimics 2LPT (Second-order Lagrangian Perturbation Theory) corrections but with free, evolved coefficients ( gamma adv , gamma I2 , etc.) rather than fixed theoretical values. 2. Spectral De-Warping (Stage A): A “Tri-Basis De-Warping” step attempts to fit the high-k

residual density field using a basis set composed of δ2, |∇ δ|2, and tidal scalar s2. This acts as a bias renormalization scheme, correcting for non-linear mode coupling at smaller scales. 3. Coherence-Gated Fusion (Stage B): The final density field is constructed by fusing the de-warped field with a set of tracers (log-density, non-linear density, scale-dependent bias terms). This fusion is governed by a spectral coherence gate, which only allows the mixing of terms that show strong cross-correlation with the reference field in specific k-bands. The most distinct departure from standard reconstruction is the differentiable, band-wise spectral fusion . Standard methods usually apply a global smoothing kernel. This algorithm, however, uses 

jax.lax.scan to iterate through specific wavenumber bands ( k ∼ 0.2–0 .6 h Mpc −1), applying a Wiener-like filter that adaptively blends the reconstructed field with non-linear tracers based on their local spectral coherence. Additionally, the explicit inclusion of a Gram-Schmidt orthogonalization step between the primary and secondary displacement channels ensures that the higher-order corrections do not re-introduce large-scale bulk flows already handled by the Zeldovich step. 

As a general observation, in this report and the others presented below, the LLMs are perhaps overly creative in their choice of names for their inventions. Terms like “spectral de-warping” or “Coherence-Gated Fusion” are not standard in cosmology, and do not even seem to appear on the internet. Nevertheless, as we will discuss, the presented techniques are often reasonable and indeed must improve the target metric to enter the report. The first innovation focuses on incorporating a range of higher-order terms. By integrating these con-tributions directly into the displacement calculation, the algorithm is able to accurately capture nonlinear mode coupling, which naturally leads to improved reconstruction performance, as is shown by previous work [e.g. 40, 41, 37]. The second innovation addresses a common concern in reconstruction: the reconstructed field remains partially non-Gaussian due to residual nonlinearities. By explicitly modeling and subtracting these small-scale non-Gaussian contributions, the algorithm effectively reduces the nonlinearity of the field, thereby bringing it closer to the Gaussian initial conditions. The third innovation has not been explored in previous reconstruction algorithms. In this step, the algorithm combines multiple tracer fields derived from the reconstructed density, such as δrec , log(1 + δrec ), and ∇ · Ψ, using scale-dependent weights wi to construct the final result. These weights are determined so that components that are more strongly correlated with the target signal are assigned higher weights. With an appropriate weighting scheme, this approach can effectively enhance the signal while suppressing noise, leading to improved reconstruction performance. We note that in the newly discovered algorithm, the LLM introduces the anisotropic smoothing. Ac-18 cording to the generated report, this is designed to address redshift-space distortion (RSD) features, even though such effects are not expected given that the simulations are performed in real space. This design choice appears to be motivated by insights drawn from existing literature. In practice, however, the learned smoothing scales along different directions are found to be very similar, resulting in an effectively isotropic smoothing kernel. Consequently, the method remains fully consistent with the real-space nature of the sim-ulations and does not introduce spurious anisotropies in the reconstruction results. Moreover, this feature may prove beneficial when extending the algorithm to applications in the redshift space. However, this is a case where additional prompting (explaining that the simulations are isotropic) would have been beneficial. We also note that our single training simulation may have been slightly anisotropic (due to its random initial conditions), however we found no significant performance gain or degradation when we manually removed this feature on the test simulations. 

5.3.3 Performance Comparison 

Table 5 compares the baseline and evolved reconstruction methods, evaluated on 9 held-out test simulations not used during training. The evaluation uses the cross-correlation coefficient r(k) averaged over the BAO-relevant range k ∈ [0 .01 , 0.5] h Mpc −1. The evolved algorithm achieves an improvement of 22 .8% at the cost of approximately eight times more computation time. Upon the averaged r(k), in figure 3 we show the cross-correlation coefficient r(k) for both baseline (red) and evolved algorithms (blue) on different scales. At large scales ( k < 0.1 h Mpc −1), both methods achieve 

r(k) ≈ 1, indicating excellent recovery of linear modes where the Zel’dovich approximation is accurate. The evolved algorithm maintains slightly higher correlation even in this regime, with the mean large-scale 

r(k) improving from 0.988 to 0.996 over k ∈ [0 .01 , 0.2] h Mpc −1. The algorithms diverge significantly at intermediate scales (0 .1 < k < 0.3 h Mpc −1), where the evolved hybrid augmented method maintains r(k) >

0.95 while the baseline drops below r(k) ≈ 0.8. Notably, at k = 0 .188 h Mpc −1, the evolved algorithm achieves 

r(k) = 0 .987 compared to r(k) = 0 .953 for the baseline—a 3.5% improvement at a critical quasi-linear scale where BAO information is concentrated. At smaller scales ( k > 0.3 h Mpc −1), both methods show declining correlation as fully nonlinear structure formation dominates, but the evolved algorithm degrades far more gradually. The maximum improvement reaches approximately 0.40 near k ≈ 0.4 h Mpc −1, demonstrating that the evolved algorithm successfully recovers information from the quasi-linear and mildly nonlinear regimes that standard reconstruction cannot access. In the right panel of Figure 3, we further evaluate the performance of different algorithms at a higher res-olution. The discovered algorithm remains consistently better than the standard reconstruction on the finer mesh, even extending to scales smaller than the original Nyquist frequency of the 256 3 grid. This behavior suggests that MadEvolve captures physically meaningful improvements, rather than merely benefiting from numerical fitting or resolution-dependent effects, providing an advantage over traditional machine-learning approaches. 

Algorithm ¯rBAO Correlation Runtime Parameters (k ∈ [0 .01 , 0.5] ) (real space) (seconds) 

Baseline Zel’dovich (autodiff) 0.752 0.344 9.7 2Evolved: Hybrid Augmented 0.924 0.581 76.1 10 

Improvement +22.8% +68.9% – –Table 5: Performance comparison of BAO reconstruction algorithms evaluated on 9 held-out test simulations. 

# 5.4 Results - Starting from Iterative Reconstruction 

As we discuss further below, our evolution starting from the standard reconstruction algorithm did not quite reach the performance of the human SotA algorithm, iterative reconstruction. In our second evolution run, we evolved starting from this SotA algorithm. 19 10 2 10 1

k h /Mpc 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> r(k)

1D Cross-Correlation (NMesh = 256) 

> Dark Matter
> Baseline (StdRec)
> IterRec (M. Schmittfull et al.)
> Best (Evolved from StdRec)
> Best (Evolved from IterRec)
> NRec (Y. Shi et al.)

10 2 10 1 10 0

k h /Mpc 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> r(k)

1D Cross-Correlation (NMesh = 512) Figure 3: A comparison of reconstruction performance for different algorithms. The results are averaged over nine simulations in the test set ( fiducial 1-9 ). The curves shows the cross-correlation function of dark matter field (black), standard reconstruction result (red), iterative reconstruction result (green), evolved algorithm result based on standard reconstruction (blue), evolved algorithm result based on iterative reconstruction (orange), and non-linear reconstruction result (purple) with respect to the initial condition. 

Left : Reconstruction is executed on a 256 3 mesh (which we evolved on). Right : Reconstruction is executed on a 512 3 mesh (which we did not evolve on). The shaded region corresponds to Fourier modes beyond the Nyquist frequency in the evolving configuration. 

5.4.1 Evolution Dynamics and Performance Trajectory 

To further probe the potential of MadEvolve to improve over well-optimized SotA algorithms, we conducted a separate evolution run using the iterative reconstruction method [37] as the starting algorithm, rather than the single-step Zel’dovich method. The iterative baseline, which applies eight successive displacement cor-rections with geometrically decreasing smoothing scales ( R = 0 .5i × 10 h−1 Mpc, Rmin = 1 h−1 Mpc), already achieves ¯ rBAO = 0 .933 on the same evaluation setup—substantially above the Zel’dovich baseline of 0 .752. Starting from this stronger baseline, the evolutionary search still discovers meaningful improvements: after 386 generations, the best evolved algorithm reaches ¯ rBAO = 0 .959, a +2 .8% improvement over the iterative baseline (Fig. 4). The discovered refinements—including guided anisotropic diffusion , residual advection ,and 2LPT source term —are applied as differentiable post-processing stages on top of the non-differentiable iterative core. We discuss the physical interpretation of these innovations in the next section. While the relative improvement is expectedly smaller than the +22 .8% gain achieved from the Zel’dovich starting point, this result confirms that the framework can extract additional physical information even when the baseline is already close to optimal. 

5.4.2 Discovered Algorithmic Techniques 

We start with MadEvolve’s own LLM-generated report of its innovations, which has been lightly edited for conciseness. We then provide comments by a human expert who reviewed the report and final code. For the complete unedited report please refer to our website 2 . The excerpt below is contained in the section Best Evolved Algorithm Analysis .

> 2https://madevolve.org/reports/iterative evolution report.pdf

20 0 50 100 150 200 250 300 350 

> Generation
> 0.850
> 0.875
> 0.900
> 0.925
> 0.950
> 0.975
> 1.000
> 1.025
> 1.050
> Combined Score
> Performance Improvement: +2.7% Final: 0.959 (10 params) Initial: 0.933 (3 params) Best Combined Score Initial Baseline Number of Parameters 0246810 12 14
> Number of Parameters

Figure 4: Evolution dynamics for the BAO reconstruction run starting from the iterative reconstruction baseline [37]. The blue curve tracks the best ¯ rBAO achieved by any program in the population across 386 generations. Starting from ¯ rBAO = 0 .933, the evolutionary search discovers differentiable refinements that push performance to ¯ rBAO = 0 .959 (+2 .8%), demonstrating that the framework can improve upon an already well-optimized reconstruction algorithm. 

LLM-Generated Algorithmic Summary 

The algorithm employs a hybrid “Shear-Aware + EFT Crossover” strategy. It begins with a standard, non-differentiable iterative particle reconstruction (an iterative Zeldovich approximation) to establish a robust baseline density field. This baseline is then refined using a differentiable, grid-based post-processing step. This second stage treats the reconstruction error as a fluid dynamical problem, applying corrections derived from Effective Field Theory (EFT) and large-scale environmental tensors to recover information lost to non-linear evolution in the high-k regime. The refinement stage is physically motivated by the need to correct for mode-coupling and stream-crossing effects that standard displacement methods miss. The algorithm constructs a “modulation weight” ( wmod ) based on the large-scale environment (density and tidal shear), effectively identify-ing regions where non-linearities are strongest. It then applies three primary physical corrections modulated by this weight: 1. Guided Anisotropic Diffusion: A diffusive term ( γdiff ) directed by the tidal tensor, smoothing the field along filamentary structures rather than isotropically. 2. Residual Advection: It advects the “residual” density (small-scale fluctuations) along the large-scale velocity flows, correcting for small-scale displacements relative to the bulk flow. 3. 2LPT Source Term: A second-order Lagrangian Perturbation Theory source term ( β2lpt ) is injected to capture non-Gaussian features generated by gravity. Additionally, it includes an isotropic EFT counterterm ( c2

> s

k2δ) to regularize small-scale power and a “phase nudging” step that gently rotates Fourier phases toward those of an arcsinh-compressed reference field, acting as a soft non-Gaussian prior. The most innovative aspect is the strict spectral gating combined with environment-modulated residu-als . Unlike standard methods that apply corrections globally, this algorithm calculates a “gate” that 21 strictly forbids any modification to modes k < 0.21 h Mpc −1. This ensures the linear regime—which is already well-recovered by the baseline method—is perfectly preserved. Furthermore, the coupling of the shear invariant ( s2) to the modulation weight allows the algorithm to aggressively target high-density, high-shear regions (knots and filaments) for correction while leaving voids relatively untouched, a nuance rarely seen in standard EFT-based reconstruction. 

MadEvolve only applies modest corrections on small scales to the iterative reconstruction method. On large scales ( k < 0.21 h /Mpc), it directly preserves the result of iterative reconstruction, which is reasonable as iterative reconstruction is already known to perform near optimally in this regime for dark matter fields. On small scales, the evolved algorithm incorporates a range of higher-order perturbation theory inspired terms and other nonlinear terms (guided anisotropic diffusion term and residual advection term) to refine the reconstruction, with their coefficients optimized through automatic differentiation. With the increased flexibility of these fitted corrections, the method achieves improved performance. The LLM explanations are drawing from a large number of concepts used in cosmology, but the creativity (or randomness) in their combination makes the resulting algorithm difficult to interpret in detail by a human expert. 

5.4.3 Performance Comparison 

Table 6 summarizes the quantitative comparison between the iterative reconstruction baseline and the evolved algorithm, evaluated on 9 held-out test simulations not used during training. The evolved algorithm achieves a 2 .8% improvement in the mean BAO metric ¯ rBAO at a modest 13 .4% increase in computation time. In Figure 3, we present the cross-correlation coefficient r(k) for both the iterative reconstruction (green) and the evolved algorithm (orange) across different scales. On large scales ( k < 0.21 h Mpc −1), the evolved algorithm preserves the result of the iterative reconstruction, leading to identical performance in this regime. On smaller scales ( k > 0.21 h Mpc −1), the evolved algorithm generally outperforms the baseline, indicating that the additional nonlinear corrections effectively recover information lost to mode coupling. However, the performance gain diminishes as one approaches the Nyquist frequency ( k ≳ 0.6 h Mpc −1), where both methods are limited by resolution and residual small-scale nonlinearities. In the right panel of Figure 3, we further evaluate the performance of different algorithms at a higher resolution. Without re-optimizing the free parameters, the evolved algorithm shows improved performance on quasi-linear scales, but falls behind the iterative reconstruction on smaller scales, including those approaching and exceeding the Nyquist frequency. This behavior is expected, as the evolutionary process is constrained by the resolution of the 256 3 grid and cannot accurately capture small-scale dynamics beyond this limit during evolution. Such limitations could potentially be mitigated by performing the evolutionary search on a finer mesh. We leave this for future work. 

Algorithm ¯rBAO Correlation Runtime (k ∈ [0 .01 , 0.5] ) (real space) (seconds) 

Iterative Reconstruction 0.933 0.660 32.7 Evolved: Iterative Augmented 0.959 0.639 37.1 

Improvement +2.8% −3.3% +13.4% Table 6: Performance comparison of evolved algorithm against iterative BAO reconstruction, evaluated on 9 held-out test simulations. 

# 5.5 Discussion 

Our results demonstrate that the evolutionary framework is able to discover reconstruction programs that substantially increase the cross-correlation with the true initial conditions on BAO-relevant scales, yielding a significant improvement over the input reconstruction algorithm. We note that these results were obtained on simulations of the matter distribution and do not directly imply gains on real galaxy surveys, however this additional realism is not the subject of the present work. 22 It is important to note that standard reconstruction and iterative reconstruction are two of numerous reconstruction methods. Beyond these two methods, substantial effort has been devoted to improving upon the Zeldovich approximation. These developments include (i) estimating non-linear displacement fields from the density field using higher-order schemes [e.g. 40, 41, 42, 43], for which we also implement the method of Shi et al. [43] in Figure 3 for comparison (purple), (ii) machine-learning-based approaches such as convolutional neural networks [e.g. 44, 45, 46, 47], and (iii) forward-model or Bayesian frameworks that infer initial conditions by fitting the full non-linear mapping between initial and final density fields, often referred to as field-level inference [e.g. 48, 49, 50, 51, 52, 53, 54, 55, 56]. Within this broader landscape, the present results indicate that the evolutionary framework has the potential to further refine existing reconstruction schemes. By systematically exploring modifications to established algorithms, the search process can identify combinations of physically motivated ingredients that enhance performance beyond their original formulations. This suggests that automated evolutionary discov-ery may serve as a complementary tool for improving and extending current reconstruction methodologies. At the same time, we note that the performance also depends on the choice of the input algorithm. We conclude that the best results are achieved when combining human-designed state-of-the-art methods with LLM-guided refinement, rather than relying solely on LLM-generated algorithms. Interestingly, despite their large literature knowledge, LLMs appear not to be able to simply recall the entire state-of-the-art algorithm from memory (i.e. here when we evolve from standard reconstruction, the LLM does not propose the iterative algorithm). Setting a strong human baseline thus remains essential for optimal performance. 

# 6 Application II: 21cm Foreground Contamination Reconstruc-tion 

Our second application, a different reconstruction task, is somewhat less well-studied than the first, but is rapidly gaining importance with the advent of 21cm interferometry surveys. 

# 6.1 Problem Formulation and Scientific Motivation 

Observations of the 21cm hyperfine transition of neutral hydrogen at cosmic dawn and reionization ( z ∼ 6– 30) offer a powerful probe of early universe physics. However, foreground emission from our Galaxy and extragalactic radio sources is several orders of magnitude brighter than the cosmological signal. These foregrounds have smooth spectra, making them separable in principle, but imperfect calibration and the frequency-dependent beam pattern of interferometric arrays cause mode mixing that leaks foreground power into otherwise clean cosmological modes, manifested as a characteristic “wedge” in the cylindrical ( k⊥, k ∥)Fourier space [57, 58, 59, 60], where k∥ is the wave-number in the direction of the line-of-sight, k⊥ is the wave-number in the transverse plane. Recent theoretical work shows that anisotropic gravitational evolution imprints the large-scale tidal field onto the shape of small-scale structures [61, 62, 63, 64]. This indicates that information about modes lost in the foreground wedge can be statistically recovered from modes outside the wedge by using this correlation. The reconstruction task thus becomes: given a 3D density field with a wedge-shaped region of missing or contaminated modes, estimate the true field by exploiting correlations between small-scale anisotropy and large-scale modes. We optimize algorithms to maximize the 2D correlation coefficient in cylindrical Fourier space: 

r2D (k⊥, k ∥) = ⟨δrec (k)δ∗

> true

(k)⟩

p⟨| δrec |2⟩⟨| δtrue |2⟩ (k⊥,k ∥)

, (5) averaged over the wedge region. 

# 6.2 Experimental Setup 

6.2.1 21cm neutral hydrogen intensity map generation 

A thorough treatment of foreground noise is beyond the scope of this paper. In this work, we will use the following simplified model to represent foreground noise in observation. Generally, we will consider the 23 0.05 0.10 0.15 0.20 0.25   

> k[hMpc 1]
> 0.05
> 0.10
> 0.15
> 0.20
> 0.25
> k [hMpc  1]

2D Cross-Correlation r (k , k )

> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> r (k , k )

Figure 5: The cross-correlation coefficient r(k⊥, k ∥) of the intensity map with respect to the underlying matter field. The results are averaged over nine simulations in the test set ( fiducial 1-9 ). following three types of noises: astrophysical foreground, the receiver noise and the shortest baseline for interferometers. 1. Astrophysical foreground : We simply use a high-pass filter along the line-of-sight to represent this noise. 

Wfs (k∥) = 1 − e−k2

> ∥R2fs /2

, (6) where k∥ is the wavenumber in the line-of-sight, Rfs is the foreground scale. This filter removes the small k∥ density modes to simulate the foreground contamination. In this paper, the foreground scale is chosen to be Rfs = 15 Mpc /h . This is an ideal case where we only lose the modes with 

k∥ < 0.02 hMpc −1 [65]. 2. Receiver noise : The resolution of small-scale structure in a 21 cm survey is mainly determined by thermal noise PN . The magnitude of this noise is around PN = 150 ∼ 600(Mpc /h )3 for a HIRAX-like interferometer [66]. In this work, we use a white noise field with magnitude PN = 300(Mpc /h )3 to model this receiver noise. 3. Baseline for interferometer : The largest angular scale that could be detected in a 21 cm survey is limited by the shortest baseline of the interferometer. In this work, we simply assume that modes with 

k⊥ < 0.05 h/ Mpc are not detectable. This is equivalent to applying a step function Θ s to the density field in the Fourier space, Θs(k) = 

(

1, if k⊥ ≥ ks

0, otherwise . (7) In summary, given the matter density field δ(x), we generate the 21 cm neutral hydrogen field by 

δIM (x) = IFFT FFT [ δm(x)] Wfs (k∥)Θ s(k) + N (PN ). (8) Fig. 5 shows the cross-correlation coefficient r(k⊥, k ∥) = PδmδIM /(Pδmδm PδIM δIM ) of the intensity map with respect to the underlying matter field. 

6.2.2 Tidal reconstruction 

Our evolutionary framework begins with a standard implementation of the tidal reconstruction formalism [62] as the baseline algorithm. This approach reconstructs the tidal shears from the observed density, calculates 24 the tidal tensor components, and uses these components to estimate the large-scale density field. We also compare to a different quadratic estimator approach in Sec. 6.4. Specifically, given an observed density contrast δobs with the wedge region set to zero, the algorithm first calculates the tidal shear fields ˆϵ1(x) = [δw1 (x)δw1 (x) − δw2 (x)δw2 (x)] /2,

ˆϵ2(x) = δw1 (x)δw2 (x),

ˆϵx(x) = δw1 (x)δw3 (x),

ˆϵy (x) = δw2 (x)δw3 (x),

ˆϵz (x) = [2 δw3 (x)δw3 (x) − δw1 (x)δw1 (x)

−δw2 (x)δw2 (x)] /6, (9) where 

δwj (k) = ik j WR(k)δobs (k), (10) is the filtered gradient density field and WR(k) = exp( −k2R2/2) is the Gaussian window with smoothing scale R. Finally, the algorithm combines all five tidal components to estimate the full density field in Fourier space: 

δrec = 12k2

(k21 − k22 )ˆ ϵ1(k) + 2 k1k2ˆϵ2(k) + 2 k1k3ˆϵx(k) (11) +2 k2k3ˆϵy (k) + (2 k23 − k21 − k22 )ˆ ϵz (k) .

This baseline achieves moderate performance (¯ r2D ≈ 0.74) but struggles with nonlinear small-scale structure and fails to exploit the full information content available from anisotropic tidal correlations. We evaluate all candidate algorithms on the Quijote N-body simulation suite [39], processed to mimic 21cm observations with realistic wedge masking. The simulations use a box size of L = 1000 h−1 Mpc with 512 3 grid resolution. We apply a cylindrical mask in ( k⊥, k ∥) space matching the expected contamination geometry from upcoming instruments such as HERA and SKA. All Fourier transforms use double-precision NumPy/SciPy FFT routines. The primary performance metric is the 2D correlation coefficient r2D(k⊥, k ∥)computed in cylindrical bins with Equation 5 and averaged over the wedge region spanning k⊥ ∈ [0 .08 , 0.65] 

h Mpc −1 and k∥ < 0.08 h Mpc −1. We use a single simulation (sim 0) for fitness evaluation during evolution, and 9 held-out simulations (sims 1–9) for testing. 

# 6.3 Results 

6.3.1 Evolution Dynamics and Performance Trajectory 

Figure 6 illustrates the progression of the best-performing algorithm over 342 generations. Starting from the baseline tidal tensor method with ¯ r2D = 0 .743 and only one tunable parameter (the filter scale), the sys-tem explored modifications including Anisotropic Filtering , Generalized Anisotropic Potential , Split-Weight Tensor Projection , and Component-wise Adaptive Saturation . We will comment in the next section on the meaning of these innovations. The improvement continued through generation 52, where the algorithm reached ¯ r2D = 0 .97 with only 9 tunable parameters, demonstrating that principled architectural choices rather than extensive parameterization drive the performance gains. The key structural breakthrough occurred during generations 30–50 when evolution discovered Split-Weight Tensor Projection which separates the tidal tensor divergence into several components and weights them independently. This substantially improves reconstruction fidelity. Although evolution continued beyond generation 52 and discovered more complex architectures with up to 64 parameters, these additional refinements yielded only marginal improvements (¯ r2D = 0 .973 versus 0 .97), suggesting that the 9-parameter algorithm already captures the dominant signal recovery mechanisms. The final performance represents a 30.7% improvement over the optimized baseline. 25 0 50 100 150 200 250 300 

> Generation
> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> 1.05
> Combined Score
> Performance Improvement: +30.9%
> Final: 0.973 (64 params)
> Initial: 0.743 (1 params)
> Best Combined Score
> Initial Baseline
> Selected Best (Gen 52)
> Number of Parameters
> 0
> 10
> 20
> 30
> 40
> 50
> 60
> Number of Parameters

Figure 6: Evolution dynamics of tidal reconstruction algorithm discovery over 342 generations. The blue curve tracks the best combined score ¯ r2D achieved by any program in the population, while the green curve indicates the number of tunable parameters in the best-performing algorithm. The red dashed line marks the initial baseline performance (¯ r2D = 0 .743) achieved by the single-parameter tidal tensor method. The evolutionary trajectory exhibits rapid initial improvement as the system discovers Anisotropic Filtering ,

Generalized Anisotropic Potential , and Split-Weight Tensor Projection . By generation 52, the algorithm achieves ¯ r2D = 0 .971 with only 9 parameters, capturing the essential physics of tidal reconstruction. 

6.3.2 Discovered Algorithmic Techniques 

We start with MadEvolve’s own LLM-generated report of its innovations. We then provide comments by a human expert who reviewed the report and final code. For the complete MadEvolve report please refer to our website 3 . The excerpt below is contained in the section Best Evolved Algorithm Analysis .

LLM-Generated Algorithmic Summary 

The selected algorithm implements an Anisotropic Tensor Reconstruction with Split-Component Weighting . It improves upon the baseline through three key innovations: 1. Anisotropic Filtering: Instead of a simple isotropic Gaussian, it applies a targeted high-pass fil-ter on k∥ modes to specifically remove foreground-contaminated modes (the “wedge”), while pre-serving large-scale transverse information via Gaussian smoothing controlled by smooth scale .2. Generalized Anisotropic Potential: The displacement field is computed using a spectral poten-tial with tunable spectral index and anisotropy factor ( fani ), allowing the algorithm to adapt the relationship between density and displacement to the specific noise geometry of 21 cm ob-servations. 3. Split-Weight Tensor Projection: The divergence of the tidal tensor is decomposed into trans-verse ( k2

> ⊥

), longitudinal ( k2 

> z

), and cross ( k⊥kz ) components, each weighted independently by 

w par and w cross . This allows the algorithm to optimally weight the geometrically distinct contributions to the reconstruction. Additionally, it uses Component-wise Adaptive Saturation to prevent high-density peaks from dom-

> 3https://madevolve.org/reports/21cm evolution report.pdf

26 inating the tidal tensor, and a tunable trace weight ( w trace ) to control how much of the isotropic component is retained. With only 9 parameters, the algorithm achieves 99.8% of the best-scoring 64-parameter variant’s ¯ r2D while running 3 × faster. 

The first two innovations are specifically designed to address anisotropies in the 21 cm intensity mapping. These modifications are expected to improve the reconstruction performance, as foreground contamination is inherently anisotropic, while the original Gaussian filter is sub-optimal. Specifically, the anisotropic filter in the first innovation is given by 

W (k) = exp 



− k2R2

> s

2

 "

1 − exp − k2

> ∥

2k2

> ∥,min

!# 

, (12) where k∥,min = 0 .1863 h Mpc −1 is a free parameter to be optimized, which consists of an isotropic part and an anisotropic part. This filter suppress the noise in the foreground wedge and keeps the small-scale uncontaminated modes. Compared to the isotropic Gaussian filter used by the original tidal reconstruction (primarily for computational efficiency), this new anisotropic filter more closely resembles a Wiener filter, and is thus expected to provide better results. The second innovation introduces anisotropic features in the potential calculation: Φeff (k) = − δ(k)

k2 

> ⊥

+ fani k2 

> ∥

+ k2

> s

, (13) where fani = 1 .055 and ks = 0 .126 h Mpc −1 are the optimized parameters. According to the report, this modification is intended to mitigate RSD effects. Although the simulations considered here are performed in real space, the parameter fani introduces an anisotropic response along the line of sight that can be beneficial for our task. Note that the optimized value of fani is close to unity, consistent with the real space setup. We also point out the unusual naming choice “spectral potential”, which seems to mean a potential expressed in Fourier space. The third innovation combines different tidal terms with independently optimized coefficients, w∥ and 

w×, which provides additional flexibility to adapt to different anisotropic noise configurations. This design again allows the algorithm to better capture direction-dependent features in the data. The new algorithm also uses component-wise adaptive saturation , which is designed to suppress nonlinear contributions in the field. When computing the quadratic tidal shear terms, highly nonlinear regions associ-ated with dense peaks generate outliers that potentially dominate the tidal components. This issue has been identified in previous work [61], where it was addressed through a logarithmic transformation of the den-sity field. In contrast, the present algorithm introduces an adaptive damping mechanism to mitigate these extreme contributions, effectively reducing nonlinear contamination in the tidal shear fields and enabling a more accurate recovery of large-scale modes. 

6.3.3 Performance Comparison 

Table 7 compares the baseline and evolved reconstruction methods. The evaluation uses the 2D correlation coefficient r2D (k⊥, k ∥) averaged over the wedge-contaminated region, assessed on 9 independent Quijote simulation realizations held out from training. The evolved algorithm achieves a correlation of ¯ r2D =0.971, representing a 30.7% improvement over the baseline tidal tensor method (¯ r2D = 0 .743). This is a substantial gain: it reduces the reconstruction error ε = (1 − ¯r22D ) by a factor of ∼ 8, from 0.45 to 0.06, bringing the reconstructed field into much better agreement with the ground truth in the wedge-contaminated region. The left panel of Figure 7 shows the 1D cross-correlation coefficient r(k) for both algorithms at different scales. Generally the evolved method outperforms the tidal reconstruction on all scales. On linear scales, the evolved method maintains r(k) > 0.9 while the baseline drops to r(k) ≈ 0.5–0 .6, indicating that the modes contaminated by foreground are recovered with a high signal-to-noise ratio. At smaller scales, both methods show declining correlation as nonlinear structure formation and thermal noise dominate, but the 27 Algorithm ¯r2D Runtime Parameters (wedge region) (seconds) 

Baseline Tidal Tensor 0.743 38.4 1Evolved: Split-Weight Anisotropic 0.971 81.0 9

Improvement +30.7% – –Table 7: Performance comparison of reconstruction algorithms. evolved algorithm degrades more gracefully, maintaining approximately 0.4–0.6 higher correlation than the baseline throughout this regime. In the right panel of Figure 7, we visualize the correlation function r(k, μ ) binned into different directions, where direction is defined by wedge μ ≡ k∥/k which denotes the cosine of the angle between the wavevector and the line of sight. The evolved algorithm achieves remarkably uniform performance across directions, with r(k, μ ) > 0.9 extending well into the wedge-contaminated region where modes with low k∥ and high k⊥

are most severely affected by foreground leakage. In contrast, the baseline tidal tensor method shows strong degradation within the wedge, with correlation dropping below 0.6 for modes satisfying the wedge condition (the μ = 0 .17 bin). The different map confirms that improvements are concentrated precisely where they are most needed: the wedge interior and the transition region at its boundary. This spatial pattern validates that the evolved algorithm’s anisotropic processing techniques effectively improve upon conventional approaches. 10 2 10 1

k h /Mpc 

0.00 

0.25 

0.50 

0.75 

1.00 

> r(k)

1D Cross-Correlation (NMesh = 512) 

> Pre-Rec
> Initial (Baseline)
> Best (Evolved)
> QE (P. Li et al.)

10 2 10 1

k h /Mpc 

0.00 

0.25 

0.50 

0.75 

1.00 

> r(k, )

2D Cross-Correlation (NMesh = 512)   

> = 0.17 = 0.5 = 0.83

Figure 7: A comparison of reconstruction performance for different algorithms. The results are averaged over nine simulations in the test set ( fiducial 1-9 ). Left : 1D cross-correlation coefficients r(k). The red curve is the result from tidal reconstruction (baseline algorithm), the green curve is for the cluster fossil algorithm and the blue curve represents the performance of our new method. Right : Cross-Correlation coefficients binned into different directions. μ = 0 .83 is the direction closest to the line-of-sight, while μ = 0 .13 is the farthest. 

# 6.4 Discussion 

Our results show that our LLM-guided framework can substantially improve upon the original tidal re-construction approach for recovering large-scale Fourier modes lost to foreground contamination in 21 cm intensity mapping. Starting from a physically motivated tidal reconstruction baseline, the evolutionary process discovers algorithmic modifications that more effectively exploit anisotropic mode coupling between small-scale structure and large-scale line-of-sight modes. The resulting algorithms achieve a higher cross-correlation with the true density field inside the foreground wedge, particularly in the quasi-linear regime. It is important to evaluate the new algorithm in the context of existing human-designed approaches to large-scale mode recovery. Beyond tidal reconstruction, another method, often referred to as quadratic 28 clustering-fossil estimator, based on three-point non-Gaussian correlations, has been proposed to recover long-wavelength modes from small-scale fluctuations by explicitly modeling nonlinear gravitational mode coupling using second-order perturbation theory [67, 68, 69, 70, 71]. In addition, other approaches attempt to recover the lost modes using field-level inference techniques [72] or with the aid of external information, such as CMB maps [73] and lensing fields [74]. In parallel, machine-learning-based approaches have been explored as an alternative capable of robustly recovering lost large-scale modes [e.g. 75, 76, 77]. Note however that machine learning or forward modeling based approaches are harder to deploy on real data than reconstruction algorithms. In Fig. 7, we explicitly compare the evolved algorithm with both the cluster fossil method and the baseline tidal reconstruction. Our new algorithm not only outperforms the baseline approach, but also achieves significant improvements over existing human-designed methods. We thus conclude that, in the context of tidal reconstruction, our algorithm succeeded in discovering a better solution to the underlying physics problem. Compared to existing analytic estimators, it achieves an improved large-scale mode recovery and exhibits more isotropic reconstruction performance. 

# 7 Application III: Effective Baryonic Physics from N-body simu-lations 

Our third task is qualitatively different from the previous two in that it is not a reconstruction problem. Rather, we aim to improve an algorithm that approximates baryonic physics from gravity-only N-body simu-lations. This is important because full hydrodynamic simulations are computationally extremely expensive, and thus difficult to run on large volumes. Gravitational N-body simulations on the other hand are far cheaper to run, but cannot directly describe observational quantities such as gas density or temperature. 

# 7.1 Problem Formulation and Scientific Motivation 

Our goal is to build effective particle displacements, and more generally effective physics, to approximate baryonic physics from gravity-only N-body simulations. Our base-algorithm will be Lagrangian Deep Learn-ing (LDL) [78]. LDL learns mappings from fast dark matter N-body simulations to baryonic observables. While LDL is a general framework that can be used to predict many different output fields, we focus here on predicting the thermal Sunyaev-Zeldovich (tSZ) signal for concreteness. The thermal Sunyaev-Zeldovich effect arises from inverse Compton scattering of cosmic microwave back-ground (CMB) photons off hot electrons in the intracluster medium. The tSZ signal provides direct observational access to the thermal state of cosmic gas, making it essential for cluster cosmology, astrophysical feedback calibration, and tests of the theory of structure formation. Traditional simulation of the tSZ effect requires running expensive hydrodynamic simulations that capture gas physics including radiative cooling, star formation, and AGN feedback. These simulations can take thousands of CPU-hours per realization, limiting statistical inference and parameter exploration. Instead, we will approximate them with LDL, and attempt to improve over the LDL algorithm with MadEvolve. We frame tSZ prediction as an optimization problem: given an initial density field δIC at z ≈ 127 and a final dark matter field δDM at z = 0, predict the tSZ y-parameter field y(x) that maximizes agreement with full hydrodynamic reference simulations. Our primary optimization objective is the cross-correlation coefficient in Fourier space: 

r(k) = Ppred ,true (k)

pPpred (k) · Ptrue (k) , (14) where Ppred (k) and Ptrue (k) are auto-power spectra of predicted and reference fields, and Ppred ,true (k) is their cross-spectrum. Accurate tSZ prediction from dark matter fields poses several fundamental challenges. The tSZ signal spans orders of magnitude in temperature—from the diffuse intergalactic medium at T ∼ 10 4 K to massive cluster cores at T ∼ 10 8 K—requiring architectures that capture both large-scale gravitational infall and small-scale shock heating. Gas behavior depends on non-gravitational processes including feedback, cooling, and star formation that are not directly encoded in dark matter fields, motivating learned representations of these astrophysical effects. The appropriate gas response varies significantly between voids, filaments, and clusters, potentially demanding context-aware processing that adapts to the local environment. 29 7.2 Experimental Setup 

Our evolutionary search begins with a manually-designed LDL architecture based on the framework of Dai and Seljak [78]. This baseline employs a Lagrangian particle-mesh framework with three displacement-field layers. At each layer l, particles are shifted according to learned displacement fields Ψ (l) computed from density-dependent filters: Ψ(l) 

> i

(q) = X

> j

F−1 h

F (l) 

> ij

(k) · F [δ(l−1) ]

i

(q), (15) where F (l) 

> ij

(k) are parametric kernels in Fourier space. The baseline uses power-law filters F (k) ∝ kα with Gaussian high-pass cutoffs. After three displacement steps, the final density field δ(3) is transformed to the tSZ y-parameter via y(x) = ReLU( b1(δ(3) (x)) μ + b0), with three scalar baryon parameters ( μ, b 1, b 0). This architecture has 18 trainable parameters (five per displacement layer plus three baryon parameters) and achieves a baseline cross-correlation of ¯ r(k) = 0 .943 on held-out test simulations, where ¯ r(k) is av-eraged over all valid modes in the range k ∈ [0 .01 , 10 .0] h Mpc −1. The evolutionary search enforces the Lagrangian displacement field structure: the function signatures, the three-layer particle-mesh workflow, and the Fourier-space displacement computation are fixed, while the LLM is free to modify the filter design, density transformations, and the baryon bias model. We evaluate all candidate algorithms on the CAMELS (Cosmology and Astrophysics with MachinE Learning Simulations) suite [79], which provides paired dark matter and hydrodynamic simulations with identical initial conditions. The simulations use a box size of L = 25 h−1 Mpc with 64 3 mesh resolution at redshift z = 0. Evolution trains on a single realization (CV 0) and computes the fitness score at every generation by evaluating on four validation simulations (CV 1 through CV 4). Final performance is reported on six independent held-out test simulations (CV 5 through CV 10) that are never seen during evolution. This cross-simulation evaluation strategy ensures that evolved architectures generalize across cosmic variance rather than overfitting to the specific structure realization used for training, which is critical because tSZ signals are dominated by rare massive halos whose abundance varies significantly between realizations. We quantify model performance using two Fourier-space statistics. The cross-correlation coefficient 

r(k) measures phase coherence between predicted and true fields, while the transfer function T (k) =pPpred (k)/P true (k) quantifies amplitude calibration accuracy. The fitness score used for evolutionary se-lection is ¯ r(k) averaged over the validation simulations; the L1 training loss is monitored but not used for selection. The evolution ran for 233 generations. Our implementation adapts the open-source LDL codebase 4 as the starting point for evolution. Compared to the BAO reconstruction task, we employ a more elaborate prompt structure that includes explicit function signatures and type annotations for the JAX-based differentiable operations. This additional structure helps the LLM understand the available computational primitives (Fourier transforms, displacement operations, density interpolation) and their expected input/output shapes, enabling more targeted and syntactically correct code modifications (see Appendix A.2 for the complete prompt). The original LDL framework addresses tSZ prediction through a specialized two-stage training procedure [78]. Rather than directly predicting the pressure field neT , LDL separately models the electron density 

ne and temperature T fields, then multiplies them to obtain the tSZ signal. Crucially, the loss function for training the electron density component is modified to weight by the true temperature field: 

LtSZ  

> ne

= X

> i

ˆOs [ne, pred (xi) · Ttrue (xi)] − ˆOs [ne, true (xi) · Ttrue (xi)] , (16) where ˆOs is a smoothing operator. This weighting scheme places greater emphasis on massive galaxy clusters where both temperature and density are high, thereby improving the quality of generated tSZ maps in the regions that dominate the signal. After training ne, the temperature field is trained with the learned ne, pred 

held fixed. Our baseline does not use this two-stage procedure: instead, it is trained with a simple L1 loss directly on the combined neT field, without temperature weighting or separate density and temperature supervision. This simpler training setup is used consistently for both the baseline and all evolved algorithms. 

> 4https://github.com/biweidai/LDL

30 7.3 Results 

7.3.1 Evolution Dynamics and Performance Trajectory 0 50 100 150 200  

> Generation
> 0.85
> 0.90
> 0.95
> 1.00
> 1.05
> Combined Score
> Performance Improvement: +4.0%
> Final: 0.979 (10 params)
> Initial: 0.941 (8 params)
> Best Combined Score
> Initial Baseline
> Number of Parameters 4
> 6
> 8
> 10
> 12
> 14
> Number of Parameters

Figure 8: Evolution dynamics of LDL tSZ prediction algorithm discovery over 233 generations. The plot shows progression of the cross-correlation coefficient ¯ r(k) averaged over validation simulations. The evolu-tionary trajectory exhibits rapid initial improvement followed by a plateau and exploration phase, with a key breakthrough at generation 122 when the hybrid thermo-virial formulation is discovered. We discuss the physical interpretation of the discovered innovations in the next section. The evolutionary search discovered progressively more sophisticated models of gas thermodynamics through a combination of architectural innovations proposed by the LLM and automatic parameter op-timization. Figure 8 illustrates the evolution of tSZ prediction performance over 233 generations. Starting from the baseline LDL architecture with ¯ r(k) = 0 .943, the search discovers increasingly more effective rep-resentations of the dark-matter-to-gas mapping. The evolutionary trajectory reveals several distinct phases. During the initial rapid improvement phase (generations 0–60), the system quickly identifies effective modifications to the baseline, reaching ¯ r(k) = 0.972 by generation 60. The second phase (generations 60–120) is characterized by a performance plateau around ¯ r(k) = 0 .972 as the system explores diverse architectural variations. A significant breakthrough occurs at generation 122, when evolution discovers the hybrid thermo-virial formulation —a multiplicative decomposition of the baryon bias model into density and temperature components—improving performance to ¯ r(k) = 0 .977. The final refinement phase (generations 122–233) yields incremental gains, reaching the final best validation score of ¯ r(k) = 0 .979 at generation 222 (corresponding to a test score of ¯ r(k) = 0 .969 on held-out simulations, see Table 8). We discuss the physical interpretation of the discovered innovations in the next section. 

7.3.2 Discovered Algorithmic Techniques 

Again we start with MadEvolve’s own LLM-generated report of its innovations. We then provide comments by a human expert who reviewed the report and final code. For the complete unedited report please refer to our website 5 . The excerpt below is contained in the section Best Evolved Algorithm Analysis .

> 5https://madevolve.org/reports/ldl evolution report.pdf

31 LLM-Generated Algorithmic Summary 

The model employs a hybrid Lagrangian-Eulerian architecture. It begins with a multi-step 

Displacement operator that iteratively updates dark matter particle positions. In each step, particles are painted to a mesh to compute a density field, which is transformed via a log-stabilized nonlinearity and filtered through a compensated bandpass kernel to generate displacement potentials. After the displacement phase, the particles are painted to a final density mesh ( δ). The baryon physics module then constructs the electron pressure field ( Pe) using a multiplicative ansatz Pe ∝ ne × Teff , where electron density ne is a power-law of the dark matter density, and effective temperature Teff combines a virial term and a shock-heating term. The model integrates two distinct physical regimes: 1. Virial Temperature: Modeled using a “Screened Potential” approach ( ϕk ∝ δk/(k2 + k2 

> s

)), where ks acts as a learned screening scale, effectively solving a modified Poisson equation that suppresses long-range gravitational heating. 2. Gated Shock Temperature: Captures heating from structure formation. It combines an isotropic gradient term ( |∇ ϕ|) and an anisotropic tidal shear term ( s2), weighted by a learned mixing parameter ( wshear ). Crucially, this kinetic term is modulated by a “Compression Gate” ((1 + ReLU( δ)) 0.5), which dynamically amplifies heating in high-density, collapsing regions while suppressing it in voids. The primary innovation is the Hybrid Thermo-Virial formulation with “parameter hijacking.” The evolutionary process optimized the parameter space by repurposing unused slots from the displace-ment loop to drive complex baryon physics (e.g., screening scales and shear smoothing lengths) without increasing the total parameter count. Furthermore, the decoupling of smoothing scales for the gradient term ( Rs, shock ) and the shear term ( Rs, shear ) allows the model to treat isotropic shock fronts and anisotropic tidal forces as distinct physical processes with different characteristic length scales, a nuance often missed in simpler models. 

The most significant innovation is the multiplicative decomposition y ∝ ne · Teff , which mirrors the physical definition of the tSZ signal as proportional to electron pressure Pe = nekB Te. Rather than learning a predefined nonlinear mapping from dark matter density to electron pressure (as the baseline does with 

F = ReLU( b1δμ + b0)), the evolved model explicitly factorizes the problem into density and temperature components. We verified in the code that the evolved algorithm indeed implements this structure: a density term ne ∝ ReLU( b2δ2 + b1δμ + b0), combined with two temperature factors derived from the gravitational potential and the tidal tensor of the displaced density field. We have not investigated in detail whether the proposed hybrid component temperature model is physically reasonable. This would be facilitated if the model provided references to publications that study these terms in detail, with explicit references to the critical equations. 

7.3.3 Performance Comparison Algorithm Train Loss Test Loss ¯r(k) Gen. (6-sim avg) (all scales) Found 

Baseline LDL 0.371 0.613 0.943 0Evolved: Physics-Informed 0.149 0.230 0.969 222 

Improvement 60% 63% +2.8% –Table 8: Performance comparison of tSZ prediction algorithms on CAMELS simulations (64 3 mesh, 

L = 25 h−1Mpc). Test metrics are averaged over six independent realizations (CV 5–CV 10) to assess generalization. 32 Table 8 compares the baseline and evolved reconstruction methods. The evaluation uses the cross-correlation coefficient r(k) and L1 loss averaged over six independent test simulations to assess generalization. The evolved algorithm achieves substantial improvements across all metrics: training loss decreases by 60% from 0.371 to 0.149, test loss decreases by 63% from 0.613 to 0.230, and the mean cross-correlation improves from 0.943 to 0.969. We note however that our LDL baseline does not use the tSZ specific loss in Eq. 16 so it is not equivalent to the tSZ performance reported in [78]. Figure 9 shows the cross-correlation coefficient r(k) and transfer function T (k) on different scales for both algorithms. At large and intermediate scales ( k < 1.0 h Mpc −1), both models perform well with r > 0.99, with the evolved model achieving marginally higher values. At smaller scales ( k > 2.0 h Mpc −1), where most k bins reside, the evolved model shows a more substantial improvement with ¯ r ≈ 0.96 compared to ¯r ≈ 0.93 for the baseline. The transfer function comparison reveals that the evolved model reduces the mean 

|T (k) − 1| error from 222% to 90% across all scales, indicating substantially better amplitude calibration. 

Figure 9: Scale-dependent performance comparison of baseline and evolved tSZ prediction algorithms. Top left : Cross-correlation r(k) showing improved coherence across all scales. Top right : Transfer function 

T (k) demonstrating better amplitude calibration. Bottom left : Power spectra comparison on log-log scale. 

Bottom right : Per-bin r(k) improvement, with green indicating where the evolved model outperforms baseline. Results averaged over six test simulations. 

# 7.4 Discussion 

Our results demonstrate that the LLM-guided evolutionary framework can improve the mapping from gravity-only dark matter simulations to baryonic observables, specifically for predicting the thermal Sunyaev-Zeldovich (tSZ) field. Starting from a physically motivated LDL method, the search process discovers al-gorithmic modifications that more accurately capture the nonlinear response of gas to the underlying dark 33 matter distribution. The resulting models achieve higher cross-correlation and improved amplitude calibra-tion across most scales, while maintaining a compact and interpretable parameterization. We briefly put the newly discovered method in the broader context of existing human approaches for fast baryonic modeling. Several recent works have proposed correction methods that augment dark-matter-only simulations with additional forces or particle displacements calibrated to hydrodynamic simulations, thus incorporating baryonic effects without solving the full fluid equations [80, 81]. These effective-force approaches introduce physically motivated modifications to particle dynamics and provide an interpretable framework for modeling feedback and gas pressure effects at reduced computational cost. Along this line of work, Hydro-Particle-Mesh methods such as HYPER [82] approximate gas dynamics through semi-analytic pressure term and halo-based thermodynamic models, enabling rapid generation of SZ observables without expensive hydrodynamic simulations. We have not systematically compared these diverse methods in the present work. However, from this preliminary study, MadEvolve appears very well suited to improve effective physics based simulation codes in cosmology, and we aim to investigate this direction in more detail in the near future. 

# 8 Future Directions for MadEvolve 

Our current implementation establishes the fundamental viability of LLM-driven algorithm discovery for cosmological applications. However, several key limitations constrain its practical deployment and scien-tific impact. We identify the following critical extensions that would substantially enhance the system’s capabilities: 

• Automated Program Repair : As algorithms grow more complex through evolution, the fraction that fail due to syntax errors, type mismatches, or runtime exceptions increases from ∼5% early in the run to ∼30% after 1000 generations. Currently these failed candidates are discarded, wasting both LLM generation cost and potentially valuable algorithmic ideas. A specialized debugging agent could intercept failures, analyze error messages and stack traces, and attempt targeted fixes before discarding a candidate. 

• Explicit Multi-Objective Pareto Optimization : While our MAP-Elites approach maintains diver-sity, it does not explicitly model trade-offs between accuracy, computational cost, and memory usage. A principled Pareto front optimization—using NSGA-III [83] or similar multi-objective evolutionary algorithms—would reveal the full landscape of accuracy-speed-memory compromises and allow users to select solutions matching their specific resource constraints. 

• Cross-Task Transfer Learning : Currently, each scientific problem (BAO reconstruction, 21cm fore-ground removal, baryon prediction) evolves independently from scratch. Yet we observe recurring algo-rithmic motifs—iterative refinement, multi-scale decomposition, morphological filtering—that appear across domains. A shared algorithm pattern library could encode these high-level strategies, enabling rapid bootstrapping on new tasks via meta-learning. Rather than starting from a naive baseline, the system would retrieve and adapt patterns from previously solved problems, analogous to how human scientists transfer intuition between related fields. 

• Real-Time Monitoring and Steering : The current system runs autonomously once launched, with human interaction limited to post-hoc analysis of results. An interactive dashboard allowing researchers to monitor evolution in real time, inject domain knowledge as constraints mid-run, or manually promote promising variants could accelerate discovery by combining machine exploration with human intuition. 

• Fast Operators Library : The current implementation relies on standard NumPy and SciPy oper-ations, which may become bottlenecks when processing large-scale cosmological datasets. Providing a library of GPU-accelerated or highly optimized operators—such as fast NFFTs or particle-based operations—would enable evolved algorithms to scale to production workloads without manual opti-mization. This could integrate with frameworks like JAX or CuPy to leverage hardware acceleration while maintaining the flexibility of the evolutionary search. 34 • Structured exploration of ideas . Humans would not find an optimal solution by trying a large number of somewhat random program mutations. More commonly, humans would come up with a high-level list of potential ideas, and then work through these ideas one by one and evaluate their potential. It seems plausible that the evolutionary approach presented here could be augmented by a systematic search through a tree or list of LLM-generated ideas. This could also be implemented using a multi-agent approach or other scaffolding. We will explore such improvements in future work. 

# 9 Conclusion 

In this paper we developed the MadEvolve software to iteratively improve scientific algorithms by combining LLMs with evolutionary programming. MadEvolve is based on Google’s AlphaEvolve, but with a more narrow focus on improving low-parametric algorithms for computational sciences. Specifically, our code features an outer and inner optimization loop to decouple structural and parametric algorithm optimization. While MadEvolve is a quite general system that could be useful in many fields of science, we evaluated its performance concretely on three tasks of computational cosmology. For the task of cosmological initial conditions reconstruction we found that given a simple base-algorithm (standard BAO reconstruction), the LLM came close to state-of-the-art (SotA) performance (iterative initial condition reconstruction) but did not quite reach it. In a second run we evolved starting from the human SotA algorithm and found further small improvements, thus setting a new SotA for our simulation setup. For the 21cm foreground reconstruction task we found significant improvements over the two human-made algorithms we tested. Finally, for the effective baryonic physics task we again found some improvement over our baseline. In both cases, the results may be state-of-the art, but a stringent performance comparison in physics is difficult due to the current lack of unified data challenges. We refer to Sec. 4.1 for a quantitative summary of our results. We are making our code and tasks public on madevolve.org to facilitate future algorithm comparison. We have only very briefly explored to what extent these algorithms generalize to new simulations (we tested different grid resolutions). Application to real data will require significant further work, for example with respect to astrophysical nuisance parameters that are not present in simulations and could limit perfor-mance gains. Finally, while the evolved algorithms perform very well, they lack some of the clarity and elegance that human researchers could achieve on these tasks, given enough human time investment. Clearer physical explanations, explicit analytic calculations, and precise references to the relevant literature, would considerably improve the interpretability of MadEvolve’s automatically generated evolution reports. The primary purpose of this paper is to establish that LLMs can improve algorithms in cosmological data analysis significantly without human intervention, if a clear performance metric can be provided. While the algorithms found improve performance over human made algorithms on the test data, more case-by-case work is needed to fully evaluate their potential. Nevertheless, the proposed approach to computational cos-mology is very promising, and will have many future applications in cosmology and beyond. In cosmology alone, the space of algorithms that could potentially be improved is vast, such as more efficient N-body or hydrodynamical simulations codes or improvements over traditional quadratic estimators. The computa-tional bottleneck is the repeated evaluation of the performance metric, rather than LLM generations, and LLM API costs are not large (hundreds of dollars for the experiments contained in this paper). Curated algorithm optimization tasks, like those presented here, will of course benefit from future models, and could be periodically re-evaluated when stronger models become available. Indeed, due to the extremely rapid progress of LLMs, the experiments in this paper were run with models that are no longer at the leading edge at the time of publication. This work is an example of an emerging way to do science with machine learning: Rather than using machine learning as a large-parameter black box function, we can now use it to explore the space of conceptual possibilities. To make this possible despite the limited reliability of LLM reasoning, problem solutions need to be verifiable or have a quantifiable performance metric. Finding new ways to give performance metrics to theoretical reasoning tasks in physics may be a fruitful direction of future research. For example, in theoretical model building to explain tensions in cosmological data, a performance metric would need 35 to include the quality of the fit to the data (including parameter efficiency), but also satisfy theoretical consistency conditions such as energy conservation. We plan to explore this direction in future work. There are many possible approaches to solving verifiable problems with LLM-based systems. For example, it would be interesting to try using teams of LLM agents rather than the MadEvolve framework, on the same tasks. The key problem in this line of research is to curate scientifically important tasks with good reward metrics that cannot be easily hacked, and that translate to real gains on practical problems. Once this is achieved, a variety of LLM systems could be used. The evolutionary approach of MadEvolve is powerful and is able to find low-parametric high-performing algorithms, but it would be interesting to compare it directly with agentic approaches, or perhaps to combine the two. This is a further interesting direction for future research. 

# Acknowledgments 

We thank Yurii Kvasiuk for many discussions and for exploring MadEvolve on different applications. We thank Kendrick Smith for useful comments on the manuscript and William Cottrell and Owen Colegrove for useful discussions. M.M. acknowledges the support by the U.S. Department of Energy, Office of Sci-ence, Office of High Energy Physics under Award Number DE-SC0017647, the support by the National Science Foundation (NSF) under Grant Number 2307109 and 2509873 and the Wisconsin Alumni Research Foundation (WARF). 

# References 

[1] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, Alhussein Fawzi, et al. Mathematical discoveries from program search with large language models. Nature , 625:468–475, 2024. doi: 10.1038/s41586-023-06924-6. [2] Alexander Novikov, Ngan Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, and Matej Balog. Alphaevolve: A coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. URL https://arxiv.org/abs/2506.13131 .[3] Daniel J. H. Chung, Zhiqi Gao, Yurii Kvasiuk, Tianyi Li, Moritz M¨ unchmeyer, Maja Rudolph, Frederic Sala, and Sai Chaitanya Tadepalli. Theoretical physics benchmark (TPBench)—a dataset and study of AI reasoning capabilities in theoretical physics. Mach. Learn. Sci. Tech. , 6(3):030505, 2025. doi: 10.1088/2632-2153/adfcb0. [4] Zhiqi Gao, Tianyi Li, Yurii Kvasiuk, Sai Chaitanya Tadepalli, Maja Rudolph, Daniel J. H. Chung, Fred-eric Sala, and Moritz M¨ unchmeyer. Test-time Scaling Techniques in Theoretical Physics – A Comparison of Methods on the TPBench Dataset. 6 2025. [5] Peiyang Song, Pengrui Han, and Noah Goodman. Large Language Model Reasoning Failures. arXiv e-prints , art. arXiv:2602.06176, February 2026. doi: 10.48550/arXiv.2602.06176. [6] Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming language. In Andr´ e Platzer and Geoff Sutcliffe, editors, Automated Deduction – CADE 28 , pages 625–635, Cham, 2021. Springer International Publishing. ISBN 978-3-030-79876-5. [7] Francisco Villaescusa-Navarro, Boris Bolliet, Pablo Villanueva-Domingo, et al. The denario project: Deep knowledge ai agents for scientific discovery. arXiv preprint arXiv:2510.26887, 2025. URL https: //arxiv.org/abs/2510.26887 .[8] OpenAI. Introducing gpt-5.2, December 2025. URL https://openai.com/index/ introducing-gpt-5-2/ .36 [9] OpenAI. Introducing openai o1, September 2024. URL https://openai.com/o1/ .[10] OpenAI. Introducing openai o3 and o4-mini, April 2025. URL https://openai.com/index/ introducing-o3-and-o4-mini/ .[11] Gemini Team, Google DeepMind. Gemini 3 pro frontier safety framework report. Technical re-port, Google DeepMind, November 2025. URL https://storage.googleapis.com/deepmind-media/ gemini/gemini_3_pro_fsf_report.pdf .[12] Anthropic. Claude opus 4.5 system card. Technical report, Anthropic, 2025. URL https://assets. anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf .[13] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In International Conference on Learning Representations (ICLR) , 2024. URL https://arxiv.org/abs/2310.06770 .[14] SWE-bench. SWE-bench lite, 2024. URL https://www.swebench.com/lite.html .[15] John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793 , 2024. URL https://arxiv.org/abs/2405.15793 .[16] Anthropic. Enabling claude code to work more autonomously, September 2025. URL https://www. anthropic.com/news/enabling-claude-code-to-work-more-autonomously .[17] Cognition Labs. Introducing devin, the first ai software engineer, March 2024. URL https: //cognition.ai/blog/introducing-devin .[18] Jean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909 , 2015. [19] Antoine Cully, Jeff Clune, Danesh Tarapore, and Jean-Baptiste Mouret. Robots that can adapt like animals. Nature , 521:503–507, 2015. doi: 10.1038/nature14422. [20] Justin K. Pugh, Lisa B. Soros, and Kenneth O. Stanley. Quality diversity: A new frontier for evolu-tionary computation. Frontiers in Robotics and AI , 3:40, 2016. doi: 10.3389/frobt.2016.00040. [21] Esteban Real, Chen Liang, David R. So, and Quoc V. Le. Automl-zero: Evolving machine learning algorithms from scratch. Proc. of ICML , 119:8007–8019, 2020. [22] Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, and et al. Discovering faster ma-trix multiplication algorithms with reinforcement learning. Nature , 610:47–53, 2022. doi: 10.1038/ s41586-022-05172-4. [23] Daniel J. Mankowitz, Andrea Michi, Anton Zhernov, and et al. Faster sorting algorithms discovered using deep reinforcement learning. Nature , 618:257–263, 2023. doi: 10.1038/s41586-023-06004-9. [24] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. Proc. of ICLR , 2024. arXiv:2310.12931 [cs.RO]. [25] Robert Tjarko Lange, Yingtao Tian, and Yujin Tang. Large language models as evolution strategies. 

arXiv preprint arXiv:2402.18381 , 2024. [26] Niki van Stein and Thomas B¨ ack. Llamea: A large language model evolutionary algorithm for auto-matically generating metaheuristics. arXiv preprint arXiv:2405.20132 , 2024. [27] Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, and Shirley Ho. Discovering symbolic models from deep learning with inductive biases. In Advances in Neural Information Processing Systems (NeurIPS) , 2020. 37 [28] Silviu-Marian Udrescu and Max Tegmark. Ai feynman: A physics-inspired method for symbolic regres-sion. Science Advances , 6(16):eaay2631, 2020. doi: 10.1126/sciadv.aay2631. [29] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. URL 

https://arxiv.org/abs/2408.06292 .[30] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066, 2025. URL https://arxiv.org/abs/2504.08066 .[31] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, et al. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864, 2025. URL https://arxiv.org/abs/2502.18864 .[32] Samuel Schmidgall et al. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227, 2025. URL https://arxiv.org/abs/2501.04227 .[33] Tal Ifargan, Lukas Hafner, Maor Kern, Ori Alcalay, and Roy Kishony. Autonomous llm-driven research from data to human-verifiable research papers. arXiv preprint arXiv:2404.17605, 2024. URL https: //arxiv.org/abs/2404.17605 .[34] Robert Tjarko Lange, Yuki Imajuku, and Edoardo Cetin. Shinkaevolve: Towards open-ended and sample-efficient program evolution. arXiv preprint arXiv:2509.19349 , 2025. [35] Asankhaya Sharma. OpenEvolve: An open-source implementation of AlphaEvolve. https://github. com/codelion/openevolve , 2025. Apache 2.0 License. Accessed: 2025-06-01. [36] Peter Auer, Nicol` o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning , 47(2-3):235–256, 2002. [37] Marcel Schmittfull, Tobias Baldauf, and Matias Zaldarriaga. Iterative initial condition reconstruction. 

Phys. Rev. D , 96:023505, 2017. doi: 10.1103/PhysRevD.96.023505. [38] Daniel J. Eisenstein, Hee-Jong Seo, Edwin Sirko, and David N. Spergel. Improving cosmological distance measurements by reconstruction of the baryon acoustic peak. Astrophys. J. , 664:675–679, 2007. doi: 10.1086/518712. [39] Francisco Villaescusa-Navarro, Daniel Angls-Alczar, Shy Genel, David N. Spergel, Rachel S. Somerville, Romeel Dave, Annalisa Pillepich, Lars Hernquist, Dylan Nelson, Paul Torrey, Desika Narayanan, Yin Li, Oliver Philcox, Paul La Plante, Yu Ni, Rainer Weinberger, Shivam Pandey, Benjamin Villasenor, Benedikt Diemer, Ana Maria Delgado, Digvijay Wadekar, Josh Borrow, ChangHoon Hahn, and Elena Massara. The quijote simulations. Astrophys. J. Suppl. , 250:2, 2020. doi: 10.3847/1538-4365/ab9d82. [40] Svetlin Tassev and Matias Zaldarriaga. Towards an Optimal Reconstruction of Baryon Oscillations. 

JCAP , 10:006, 2012. doi: 10.1088/1475-7516/2012/10/006. [41] Ixandra Achitouv and Chris Blake. Improving reconstruction of the baryon acoustic peak : the effect of local environment. Phys. Rev. D , 92(8):083523, 2015. doi: 10.1103/PhysRevD.92.083523. [42] Hong-Ming Zhu, Yu Yu, Ue-Li Pen, Xuelei Chen, and Hao-Ran Yu. Nonlinear reconstruction. Phys. Rev. D , 96(12):123502, December 2017. doi: 10.1103/PhysRevD.96.123502. [43] Yanlong Shi, Marius Cautun, and Baojiu Li. New method for initial density reconstruction. Phys. Rev. D, 97(2):023505, January 2018. doi: 10.1103/PhysRevD.97.023505. [44] Tian-Xiang Mao, Jie Wang, Baojiu Li, Yan-Chuan Cai, Bridget Falck, Mark Neyrinck, and Alex Szalay. Baryon acoustic oscillations reconstruction using convolutional neural networks. Mon. Not. R. Astron. Soc. , 501(1):1499–1510, February 2021. doi: 10.1093/mnras/staa3741. 38 [45] Christopher J. Shallue and Daniel J. Eisenstein. Reconstructing cosmological initial conditions from late-time structure with convolutional neural networks. Mon. Not. R. Astron. Soc. , 520(4):6256–6267, April 2023. doi: 10.1093/mnras/stad528. [46] Xinyi Chen, Fangzhou Zhu, Sasha Gaines, and Nikhil Padmanabhan. Effective cosmic density field reconstruction with convolutional neural network. Mon. Not. R. Astron. Soc. , 523(4):6272–6281, August 2023. doi: 10.1093/mnras/stad1868. [47] Liam Parker, Adrian E. Bayer, and Uroˇ s Seljak. Initial conditions from galaxies: machine-learning subgrid correction to standard reconstruction. J. Cosmol. Astropart. Phys. , 2025(9):039, September 2025. doi: 10.1088/1475-7516/2025/09/039. [48] Jens Jasche and Benjamin D. Wandelt. Bayesian physical reconstruction of initial conditions from large-scale structure surveys. Mon. Not. R. Astron. Soc. , 432(2):894–913, June 2013. doi: 10.1093/mnras/ stt449. [49] J. Jasche and G. Lavaux. Physical Bayesian modelling of the non-linear matter distribution: New insights into the nearby universe. Astron. Astrophys. , 625:A64, May 2019. doi: 10.1051/0004-6361/ 201833710. [50] Doogesh Kodi Ramanah, Guilhem Lavaux, Jens Jasche, and Benjamin D. Wandelt. Cosmological inference from Bayesian forward modelling of deep galaxy redshift surveys. Astron. Astrophys. , 621: A69, January 2019. doi: 10.1051/0004-6361/201834117. [51] Fabian Schmidt, Franz Elsner, Jens Jasche, Nhat Minh Nguyen, and Guilhem Lavaux. A rigorous EFT-based forward model for large-scale structure. J. Cosmol. Astropart. Phys. , 2019(1):042, January 2019. doi: 10.1088/1475-7516/2019/01/042. [52] Fabian Schmidt, Giovanni Cabass, Jens Jasche, and Guilhem Lavaux. Unbiased cosmology inference from biased tracers using the EFT likelihood. J. Cosmol. Astropart. Phys. , 2020(11):008, November 2020. doi: 10.1088/1475-7516/2020/11/008. [53] Nhat-Minh Nguyen, Fabian Schmidt, Guilhem Lavaux, and Jens Jasche. Impacts of the physical data model on the forward inference of initial conditions from biased tracers. J. Cosmol. Astropart. Phys. ,2021(3):058, March 2021. doi: 10.1088/1475-7516/2021/03/058. [54] Andrija Kosti´ c, Nhat-Minh Nguyen, Fabian Schmidt, and Martin Reinecke. Consistency tests of field level inference with the EFT likelihood. J. Cosmol. Astropart. Phys. , 2023(7):063, July 2023. doi: 10.1088/1475-7516/2023/07/063. [55] Adrian E. Bayer, Uros Seljak, and Chirag Modi. Field-Level Inference with Microcanonical Langevin Monte Carlo. arXiv e-prints , art. arXiv:2307.09504, July 2023. doi: 10.48550/arXiv.2307.09504. [56] Nhat-Minh Nguyen, Fabian Schmidt, Beatriz Tucci, Martin Reinecke, and Andrija Kosti´ c. How Much Information Can Be Extracted from Galaxy Clustering at the Field Level? Phys. Rev. Lett. , 133(22): 221006, November 2024. doi: 10.1103/PhysRevLett.133.221006. [57] Miguel F. Morales, Bryna Hazelton, Ian Sullivan, and Adam Beardsley. Four Fundamental Foreground Power Spectrum Shapes for 21 cm Cosmology Observations. Astrophys. J. , 752:137, 2012. doi: 10.1088/ 0004-637X/752/2/137. [58] Bryna J. Hazelton, Miguel F. Morales, and Ian S. Sullivan. The Fundamental Multi-Baseline Mode-Mixing Foreground in 21 cm EoR Observations. Astrophys. J. , 770:156, 2013. doi: 10.1088/0004-637X/ 770/2/156. [59] A. Datta, J. D. Bowman, and C. L. Carilli. Bright Source Subtraction Requirements For Redshifted 21 cm Measurements. Astrophys. J. , 724:526–538, 2010. doi: 10.1088/0004-637X/724/1/526. 39 [60] Emma Chapman, Saleem Zaroubi, Filipe B. Abdalla, Fred Dulwich, Vibor Jeli´ c, and Benjamin Mort. The Effect of Foreground Mitigation Strategy on EoR Window Recovery. Mon. Not. Roy. Astron. Soc. ,458(3):2928–2939, 2016. doi: 10.1093/mnras/stw161. [61] Hong-Ming Zhu, Ue-Li Pen, Yu Yu, Xinzhong Er, and Xuelei Chen. Cosmic tidal reconstruction. arXiv e-prints , art. arXiv:1511.04680, November 2015. doi: 10.48550/arXiv.1511.04680. [62] Hong-Ming Zhu, Ue-Li Pen, Yu Yu, and Xuelei Chen. Recovering lost 21cm radial modes via cosmic tidal reconstruction. Physical Review D , 98(4), August 2018. ISSN 2470-0029. doi: 10.1103/physrevd. 98.043511. URL http://dx.doi.org/10.1103/PhysRevD.98.043511 .[63] Hong-Ming Zhu, Tian-Xiang Mao, and Ue-Li Pen. Cosmic Tidal Reconstruction with Halo Fields. 

Astrophys. J. , 929(1):5, April 2022. doi: 10.3847/1538-4357/ac5a47. [64] Shi-Hui Zang, Hong-Ming Zhu, Marcel Schmittfull, and Ue-Li Pen. Cosmic Tidal Reconstruction in Redshift Space. Astrophys. J. , 962(1):21, 2024. doi: 10.3847/1538-4357/ad0cf0. [65] J. Richard Shaw, Kris Sigurdson, Michael Sitwell, Albert Stebbins, and Ue-Li Pen. Coaxing cosmic 21cm fluctuations from the polarized sky using m-mode analysis. Physical Review D , 91(8), April 2015. ISSN 1550-2368. doi: 10.1103/physrevd.91.083514. URL http://dx.doi.org/10.1103/PhysRevD.91. 083514 .[66] Martin White and Nikhil Padmanabhan. Matched filtering with interferometric 21cm experiments. 

Monthly Notices of the Royal Astronomical Society , 471(1):11671180, July 2017. ISSN 1365-2966. doi: 10.1093/mnras/stx1682. URL http://dx.doi.org/10.1093/mnras/stx1682 .[67] Donghui Jeong and Marc Kamionkowski. Clustering Fossils from the Early Universe. Phys. Rev. Lett. ,108(25):251301, June 2012. doi: 10.1103/PhysRevLett.108.251301. [68] Peikai Li, Scott Dodelson, and Rupert A. C. Croft. Large scale structure reconstruction with short-wavelength modes. Phys. Rev. D , 101(8):083510, April 2020. doi: 10.1103/PhysRevD.101.083510. [69] Peikai Li, Rupert A. C. Croft, and Scott Dodelson. New Probes of Large Scale Structure. arXiv e-prints ,art. arXiv:2007.00226, July 2020. doi: 10.48550/arXiv.2007.00226. [70] Omar Darwish, Simon Foreman, Muntazir M. Abidi, Tobias Baldauf, Blake D. Sherwin, and P. Daniel Meerburg. Density reconstruction from biased tracers and its application to primordial non-Gaussianity. 

Phys. Rev. D , 104(12):123520, December 2021. doi: 10.1103/PhysRevD.104.123520. [71] Zhenyuan Wang and Donghui Jeong. Reconstructing the long-wavelength matter density fluctuation modes from the scalar-type clustering fossils. J. Cosmol. Astropart. Phys. , 2024(7):020, July 2024. doi: 10.1088/1475-7516/2024/07/020. [72] Chirag Modi, Martin White, Anˇ ze Slosar, and Emanuele Castorina. Reconstructing large-scale structure with neutral hydrogen surveys. J. Cosmol. Astropart. Phys. , 2019(11):023, November 2019. doi: 10. 1088/1475-7516/2019/11/023. [73] Haochen Wang, Kiyoshi Masui, Kevin Bandura, Arnab Chakraborty, Matt Dobbs, Simon Foreman, Liam Gray, Mark Halpern, Albin Joseph, Joshua MacEachern, Juan Mena-Parra, Kyle Miller, Laura Newburgh, Sourabh Paul, Alex Reda, Pranav Sanghavi, Seth Siegel, and Dallas Wulf. Demonstration of hybrid foreground removal on CHIME data. Phys. Rev. D , 111(10):103531, May 2025. doi: 10.1103/ PhysRevD.111.103531. [74] Simon Foreman, P. Daniel Meerburg, Alexander van Engelen, and Joel Meyers. Lensing reconstruction from line intensity maps: the impact of gravitational nonlinearity. J. Cosmol. Astropart. Phys. , 2018 (7):046, July 2018. doi: 10.1088/1475-7516/2018/07/046. [75] T. Lucas Makinen, Lachlan Lancaster, Francisco Villaescusa-Navarro, Peter Melchior, Shirley Ho, Lau-rence Perreault-Levasseur, and David N. Spergel. deep21: a deep learning method for 21 cm foreground removal. J. Cosmol. Astropart. Phys. , 2021(4):081, April 2021. doi: 10.1088/1475-7516/2021/04/081. 40 [76] Samuel Gagnon-Hartman, Yue Cui, Adrian Liu, and Siamak Ravanbakhsh. Recovering the wedge modes lost to 21-cm foregrounds. Monthly Notices of the Royal Astronomical Society , 504(4):4716–4729, July 2021. doi: 10.1093/mnras/stab1158. [77] Qian Li, Xin Wang, Xiao-Dong Li, Jiacheng Ding, Tian-Cheng Luan, and Xiaolin Luo. Restoring missing modes of 21cm intensity mapping with deep learning: impact on BAO reconstruction. J. Cosmol. Astropart. Phys. , 2025(4):082, April 2025. doi: 10.1088/1475-7516/2025/04/082. [78] Biwei Dai and Uroˇ s Seljak. Learning effective physical laws for generating cosmological hydrodynamics with lagrangian deep learning. Proceedings of the National Academy of Sciences , 118(16):e2020324118, 2021. [79] Francisco Villaescusa-Navarro, Daniel Angl´ es-Alc´ azar, Shy Genel, David N Spergel, Rachel S Somerville, Romeel Dave, Annalisa Pillepich, Lars Hernquist, Dylan Nelson, Paul Torrey, et al. The camels project: Cosmology and astrophysics with machine-learning simulations. The Astrophysical Journal , 915(1):71, 2021. [80] Benjamin Horowitz, Carolina Cuesta-Lazaro, and Omar Yehia. BaryonBridge: Stochastic Interpolant Model for Fast Hydrodynamical Simulations. 10 2025. [81] Arne Thomsen, Tilman Tr¨ oster, and Fran¸ cois Lanusse. Hybrid Physical-Neural Simulator for Fast Cosmological Hydrodynamics. In 39th Annual Conference on Neural Information Processing Systems: Includes Machine Learning and the Physical Sciences (ML4PS) , 10 2025. [82] Yizhou He, Hy Trac, and Nickolay Y. Gnedin. A Hydro-particle-mesh Code for Efficient and Rapid Simulations of the Intracluster Medium. Astrophys. J. , 925(2):134, 2022. doi: 10.3847/1538-4357/ ac3bcb. [83] Kalyanmoy Deb and Himanshu Jain. An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part I: Solving problems with box constraints. 

IEEE Transactions on Evolutionary Computation , 18(4):577–601, 2014. doi: 10.1109/TEVC.2013. 2281535. 

# A Implementation Details 

# A.1 Computational Environment 

All experiments were conducted on a single workstation equipped with an AMD Ryzen Threadripper 3960X 24-core processor (48 threads), 256 GB of DDR4 RAM, and two NVIDIA GeForce RTX 3090 GPUs (24 GB VRAM each). The system ran Ubuntu 20.04 LTS with Linux kernel 5.15, CUDA 11.4, Python 3.12, and PyTorch 2.3. LLM inference was performed via the Google Gemini API and the OpenAI API. 

# A.2 Domain-Specific System Prompts 

The following subsections present the complete system prompts used to guide the LLM in each of our three cosmological applications. These prompts establish the physical context, optimization objectives, and technical constraints that shape the evolutionary search. Each prompt encodes domain knowledge about the specific reconstruction or prediction task, including relevant equations, evaluation metrics, and best practices for differentiable implementations. These prompts were iteratively refined based on failure modes observed in preliminary runs. For example, early BAO reconstruction experiments produced algorithms that improved small-scale correlation at the expense of degrading large-scale modes, motivating the addition of explicit per-bin baseline values and penalty terms for large-scale degradation. Similarly, hard constraints on the maximum number of tunable parameters and requirements for fully differentiable JAX operations were introduced after early runs produced overly complex or non-differentiable code that hindered the autodiff optimization stage. 41 A.2.1 BAO Reconstruction Prompt 

BAO Reconstruction System Prompt 

You are a scientific pioneer and expert numerical cosmologist at the bleeding edge of your field. Your mission is to invent radically new 3-D BAO reconstruction algorithms that challenge and surpass the current state-of-the-art , including the canonical Zel’dovich “standard reconstruction.” 

Core Objective (Absolute Priority): 

• Maximize the cross-correlation coefficient r(k) between the reconstructed and ground truth density fields. This is the ultimate measure of BAO reconstruction success, as it directly quantifies how well the algorithm recovers the initial density field at BAO scales ( k ∼ 0.01–0 .5 h/ Mpc). 

CRITICAL: Preserve Large-Scale Structure (Small k) – Per-Bin Constraint 

• For small k values ( k ∼ 0.01–0 .2 h/ Mpc, corresponding to large spatial scales), r(k) should remain very high, close to 1.0 and must NOT degrade from the baseline algorithm. 

• The baseline r(k) values at each k bin are: 

– k = 0 .022: 0.999, k = 0 .047: 0.998, k = 0 .071: 0.997, k = 0 .096: 0.996 

– k = 0 .120: 0.995, k = 0 .145: 0.988, k = 0 .169: 0.972, k = 0 .194: 0.946 

• PENALTY: If ANY k bin in [0 .01 , 0.2] performs worse than baseline, penalty = 10 ×max(degradation) 

• Focus on improving small-scale (large k > 0.2) reconstruction WITHOUT hurting ANY large-scale bin. 

CRITICAL REQUIREMENT: Fully Differentiable Code (JAX) 

This evolution run uses Autodiff Parameter Optimization . Your code MUST be fully differentiable using JAX operations: 1. Use jax.numpy (jnp) instead of numpy (np) for ALL numerical operations 2. Avoid non-differentiable operations: 

• No if/else based on array values (use jnp.where instead) 

• No for loops over array elements (use vectorized operations) 

• No .astype(int) for indexing (use soft indexing or interpolation) 

• No in-place modifications (JAX arrays are immutable) 3. TUNABLE Parameters with Autodiff:                   

> #TUNABLE : param_name =default_value , bounds =( min , max ) , method = autodiff def my_function ( data , param_name = default_value ): result =jnp . exp ( - data *param_name ) #Gradients can flow return result

Why Autodiff ? Gradient-based optimization finds optimal parameters MUCH faster than grid search. Your algorithm’s parameters will be automatically tuned using gradient descent. 

HARD CONSTRAINT: Maximum 10 Tunable Parameters 

Your program MUST NOT have more than 10 TUNABLE parameters. Focus on the most impactful param-eters rather than adding many small ones. 

Embrace Radical Innovation: Think Beyond the Literature 

Your main task is to generate novel ideas that are not found in existing cosmology literature . Consider: 

• Differentiable Physics: Implement physics-based constraints that are differentiable 

• Soft Assignments: Replace hard binning/indexing with soft, differentiable alternatives 

• Neural-Inspired Architectures: Use differentiable operations inspired by neural networks 

• Multi-scale Approaches: Different smoothing scales for different k-modes 

• Iterative Refinement: Differentiable iterative algorithms with learnable step sizes 

Problem Context & Constants: 

3-D periodic simulation boxes:                

> BoxSize =1000 #Mpc / h NMesh =256 #grid resolution Kf =2*jnp . pi /BoxSize

Input/Output: 

• Input: 3D density field (256 3 array) 

• Output: Reconstructed initial density field (256 3 array) 

• Evaluation: Cross-correlation r(k) in BAO range ( k ∼ 0.01–0 .5 h/ Mpc) 

Key Performance Metric: 

Average r(k) in BAO range [0 .01 , 0.5] h/ Mpc is the primary score. Higher values (closer to 1.0) indicate better reconstruction quality. 

Scoring with Per-Bin Large-Scale Penalty: 

42 • Base score = average r(k) over [0 .01 , 0.5] h/ Mpc 

• For each k bin in [0 .01 , 0.2], compute degradation = max(0 , baseline r(k) − your r(k)) 

• Penalty = 10 × max(degradation across all 8 bins) 

• Combined score = base score − penalty 

Note that the baseline r(k) values and the penalty coefficient ( λ = 10) listed in the prompt are consistent with the values hardcoded in the evaluator, so that the LLM is informed of the exact scoring criteria it is optimizing against. 

A.2.2 21cm Tidal Reconstruction Prompt 

21cm Tidal Reconstruction System Prompt 

You are a scientific pioneer and expert numerical cosmologist at the bleeding edge of your field. Your mission is to invent new 21cm reconstruction algorithms that challenge and surpass the current state-of-the-art .

Physical Context – 21cm Reconstruction: 

In 21cm intensity mapping, bright astrophysical foregrounds wipe out long-wavelength line-of-sight modes (small k∥), so the observed map is missing exactly the large-scale information most cross-correlations need. However, those large-scale density fluctuations still leave a footprint: they tidally modulate small-scale clustering in an anisotropic way. The direction and strength of this anisotropy encode the large-scale field we lost—especially for high k⊥, low k∥ modes. 

Core Objective (Absolute Priority): 

• Maximize avg( r2D [1 : 6 , 1 : 6] ) where r2D (k⊥-bin , k ∥-bin) is the Fourier-space correlation coefficient between your reconstruction ( ˆδ) and the ground truth ( δtrue ). 

• Key Intuition: Push r2D up in the high-k⊥, low-k∥ region, indicating better recovery of the large-scale line-of-sight modes inferred from small-scale anisotropies. 

CRITICAL REQUIREMENT: Fully Differentiable Code (JAX) 

This evolution run uses Autodiff Parameter Optimization . Your code MUST be fully differentiable using JAX operations: 1. Use jax.numpy (jnp) instead of numpy (np) for ALL numerical operations 2. Avoid non-differentiable operations: 

• No if/else based on array values (use jnp.where instead) 

• No for loops over array elements (use vectorized operations) 

• No .astype(int) for indexing (use soft indexing or interpolation) 

• No in-place modifications (JAX arrays are immutable) 3. TUNABLE Parameters with Autodiff:                   

> #TUNABLE : param_name =default_value , bounds =( min , max ) , method = autodiff def my_function ( data , param_name = default_value ): result =jnp . exp ( - data *param_name ) #Gradients can flow return result

Embrace Radical Innovation: Think Beyond the Literature 

Consider: 

• Differentiable Physics: Implement physics-based constraints that are differentiable 

• Soft Assignments: Replace hard binning/indexing with soft, differentiable alternatives 

• Neural-Inspired Architectures: Use differentiable operations inspired by neural networks 

• Multi-scale Approaches: Different smoothing scales for different k-modes 

• Iterative Refinement: Differentiable iterative algorithms with learnable step sizes 

Problem Context & Constants: 

3-D periodic simulation boxes:                

> BoxSize =1000 #Mpc / h NMesh =512 #grid resolution Kf =2*jnp . pi /BoxSize

Input/Output: 

• Input: 3D degraded density field (512 3 array) with missing line-of-sight modes 

• Output: Reconstructed density field (512 3 array) 

43 • Evaluation: 2D cross-correlation r(k⊥, k ∥) focusing on indices [1 : 6 , 1 : 6] 

Key Performance Metric: 

The r2D metric captures cross-correlation in 2D ( k⊥, k ∥) space, focusing on scales 1 ≤ i ≤ 5, 1 ≤ j ≤ 5. Higher values (closer to 1.0) indicate better reconstruction quality. 

A.2.3 Lagrangian Deep Learning for tSZ Prediction Prompt 

LDL tSZ Prediction System Prompt 

You are a scientific pioneer and expert numerical cosmologist specializing in Lagrangian Deep Learning (LDL) for cosmological simulations. Your mission is to evolve and improve the LDL model architecture to better predict the thermal Sunyaev-Zeldovich (tSZ) effect signal from dark matter particle positions using CAMELS simulation data. 

Physical Context – LDL for tSZ Signal (Electron Pressure): 

The tSZ effect arises from inverse Compton scattering of CMB photons by hot gas in galaxy clusters. The signal is proportional to the electron pressure field: ne × T (electron number density times temperature). Unlike stellar mass, the tSZ signal: 

• Traces hot gas in galaxy clusters ( > 10 6 K) 

• Is sensitive to AGN/supernova feedback 

• Requires modeling both density ( ne) and temperature ( T ) fields 

• Has stronger small-scale fluctuations due to gas physics 

• Is critical for CMB experiments (Planck, ACT, SPT) Lagrangian Deep Learning applies learned displacement fields to dark matter particles to predict the electron pressure distribution ( ne × T ). The model must capture: 

• Gas cooling and heating processes 

• Shock heating in dense regions 

• AGN/stellar feedback effects 

• Temperature-dependent ionization 

Target Field: ne × T [cm −3 K] – Electron pressure field 

Core Objective (Absolute Priority): 

• MAXIMIZE cross-correlation r(k) averaged over ALL scales – THIS IS THE FITNESS MET-RIC 

• Maintain numerical stability across different configurations 

• Ensure physical consistency (no negative pressure, reasonable temperature range) 

Problem Context & Constants:                                 

> BoxSize =cosmology [ ’ BoxSize ’] #25 Mpc / h for CAMELS NMESH =64 #grid resolution DOWNSAMPLE =16 #particle downsampling factor Nstep =3#number of displacement layers TARGET =’nT ’ #tSZ signal : electron pressure

Constraints and Best Practices: 

MUST MAINTAIN: 1. Function signatures of Displacement and LDL 

2. vmad compatibility (all operations must be differentiable) 3. MPI compatibility (distributed arrays) 4. Numerical stability (use ϵ = 10 −8 to avoid division by zero) 5. The EVOLVE-BLOCK-START and EVOLVE-BLOCK-END markers 

Evaluation Metrics: 

Primary metric: combined score (higher is better)             

> combined_score =mean_r_k_all_scales +#Cross - correlation r ( k ) averaged - failed_runs *1000.0 #Failure penalty

Where mean r k all scales is computed on: 

• Full volume (entire simulation box, not just validation region) 

• All k scales (all valid k modes from kmin = 0 .01 to kmax = 10 .0) 

• Range: [0 , 1], where 1.0 = perfect correlation 

44 VMAD API Reference (Critical): 

vmad is NOT standard NumPy! It is a computation graph framework for automatic differentiation, similar to TensorFlow 1.x or Theano. 

Parameter Extraction – How to get values from ‘param’: 

param is a SYMBOL object, not a NumPy array. You CANNOT use normal indexing!                                   

> #CORRECT : mu =linalg . take ( param , 5* Nstep , axis =0) #Extract single parameter b1 =linalg . take ( param , 5* Nstep +1 , axis =0) #Extract next parameter #WRONG : mu =param [5* Nstep ] #Error : ’ Symbol ’ object is not subscriptable params =param [5:10] #Error : slicing not supported

Available Operations: 

vmad.lib.unary – Element-wise operations: 

• Available: exp , log , log10 , sin , cos , tan , sinh , cosh , arcsin , arccos , arctan , absolute , fabs , sinc 

• NOT Available: sqrt (use x**0.5 ), tanh (use sinh(x)/cosh(x) ), clip , maximum , minimum vmad.lib.linalg – Linear algebra: 

• Available: take , stack , sum , sumat , broadcast to , reshape , transpose , concatenate , einsum , matmul ,

add , mul , div , mod , pow 

• NOT Available: slice , split , chunk , gather , scatter vmad.lib.fastpm – Particle-mesh operations: 

• Available: paint , readout , decompose , exchange , gather , r2c , c2r , apply transfer ,

fourier space neg gradient 

Fourier Space Operations – Special Care Needed: 

When working in Fourier space, you must understand Symbol vs Scalar distinction:                                                                                  

> #CORRECT PATTERN : #Step 1: Create filter with PURE NUMPY operations in lambda Filter =Literal ( pm . create ( type = ’ complex ’ , value =1). apply ( lambda k , v : k . normp (2 , zeromode =1 e -8) ** 0.5 #Pure numpy only ! )) #Step 2: Broadcast parameters to Symbol ’ s shape kh =mpi . allbcast ( kh , comm = pm . comm ) kh =linalg . broadcast_to ( kh , vmad_eval ( Filter , lambda x : x . shape )) #Step 3: Apply vmad operations on Symbols Filter =-unary . exp ( - Filter **2 /kl **2) *unary . exp ( - kh **2 /Filter **2) #WRONG PATTERN ( mixing Symbol and scalar in lambda ): Filter =pm . create ( type = ’ complex ’). apply ( lambda k , v : unary . exp ( -( k . normp (2) /k_split )**2) #ERROR ! )#TypeError : must be real number , not Symbol

Inside pm.apply(lambda k, v: ...) , operations must be pure numpy. You cannot use vmad operations (unary.* , linalg.* ) there! 

Common Patterns:                                         

> #Accessing parameters : alpha =linalg . take ( param , 5* i , axis =0) alpha =mpi . allbcast ( alpha , comm = pm . comm ) alpha =linalg . broadcast_to ( alpha , vmad_eval (S , lambda x : x . shape )) #Implementing missing functions : y=x** 0.5 #Square root via power y=unary . sinh ( x ) /unary . cosh ( x ) #Tanh y=ReLU ( x ) #Already defined as @operator

Debugging Checklist: 

• “AttributeError: module has no attribute” → Function doesn’t exist (e.g., tanh , sqrt , clip )

• “Symbol object is not subscriptable” → Use linalg.take() instead of indexing 

• “Symbol has no attribute” → Use vmad equivalents instead of numpy methods 

• “TypeError: must be real number, not Symbol” → Mixed Symbol and scalar in pm.apply() 

• “operands could not be broadcast” → Use linalg.broadcast to() after mpi.allbcast() 

45 The LDL codebase is built on vmad 6, a computation graph framework for automatic differentiation used in the FastPM ecosystem. Unlike standard NumPy, vmad operates on symbolic computation graphs (similar to TensorFlow 1.x or Theano), where array variables are Symbol objects that do not support direct indexing or in-place mutation. Because the LDL training loop requires end-to-end differentiable operations for gradient-based parameter optimization, all evolved code must be expressed using the vmad operator set. This non-standard API proved to be a major source of compilation failures (reflected in the low 54.5% success rate for tSZ in Table 2), motivating the detailed API reference included in the prompt above. 

# B Using MadEvolve on New Tasks 

MadEvolve is designed to be extensible to new scientific computing tasks beyond the cosmological ap-plications demonstrated in this paper. Detailed instructions and additional examples are available at 

https://madevolve.org . Applying MadEvolve to a new domain requires creating a project folder with the following components: 

1. Seed Program (initial.py ). The initial algorithm implementation that serves as the start-ing point for evolution. The code region to be evolved is marked with # EVOLVE-BLOCK-START and #EVOLVE-BLOCK-END comments. The LLM will propose modifications only within this block while preserving the surrounding infrastructure code. 

2. Evaluation Script (evaluate.py ). A script that executes the generated program on test data and outputs a metrics.json file containing quantitative performance measures. The evaluation should be deterministic and computationally tractable—typically completing in minutes to enable rapid iteration through generations. The primary metric used for fitness ranking must be clearly defined (e.g., r(k) for BAO, r2D for tidal reconstruction). 

3. Evolution Configuration (run evo.py ). A Python script that configures and launches the evolution using the EvolutionOrchestrator . Key configuration elements include: 

• task description : A detailed system prompt encoding the physical context, optimization objectives, allowed tools, and domain-specific constraints that guide the LLM’s code generation 

• evaluator script : Path to the evaluation script 

• init program path : Path to the seed program 

• Evolution hyperparameters: number of generations, population size, selection strategy, mutation modes, and parallelization settings 

4. Report Adapter (optional). To generate automated analysis reports for a new task, one can de-fine a scenario adapter that extends the MadEvolve analyzer framework. Each adapter implements three sub-components: (i) a MetricsAdapter that parses domain-specific metrics from metrics.json and formats comparison tables; (ii) a PromptAdapter that generates LLM prompts for algorithm analysis, improvement comparison, and executive summaries; and (iii) a ReportTemplateAdapter that defines the report struc-ture and assembles the final document. The adapters in MadEvolve-Cosmo/analyzer/adapters/ provide reference implementations for BAO, tidal, and LDL scenarios. 7

The examples/ directory in our repository contains complete reference implementations for BAO recon-struction, tidal field reconstruction, and LDL models that can serve as templates for new applications. 

# C LLM-generated Evolution Reports for all Tasks 

MadEvolve automatically generates comprehensive analysis reports for each evolutionary run, produced by the LLM-based report generator using Gemini 3 Pro Preview. The complete reports for all experiments in this paper (BAO reconstruction, 21 cm tidal field reconstruction, LDL tSZ prediction, and the iterative BAO evolution) are available at madevolve.org .Each report follows a standardized structure designed to provide both high-level summaries and detailed technical analysis:               

> 6https://github.com/rainwoodman/vmad
> 7The domain-agnostic MadEvolve framework is available at https://github.com/tianyi-stack/MadEvolve . The cosmology-specific applications, prompts, evaluators, and report adapters described in this paper are at https://github.com/ tianyi-stack/MadEvolve-Cosmo .

46 1. Executive Summary. An LLM-generated overview of the experiment’s key findings, including the magnitude of performance improvement, the nature of algorithmic innovations discovered, and impli-cations for the relevant scientific domain. 2. Evolution Overview. Quantitative statistics of the evolutionary run: total duration, number of generations and programs evaluated, success rate, the generation at which the best solution was found, and a table tracking the best fitness score across generations. 3. Task-Specific Quality Metrics. Side-by-side comparison of baseline and best-evolved algorithms across all evaluation metrics, including per-scale or per-simulation breakdowns where applicable. For reconstruction tasks, this includes cross-correlation coefficients at individual wavenumber bins; for LDL, this includes per-simulation validation losses. 4. Baseline Algorithm Analysis. LLM-generated scientific interpretation of the seed algorithm, cov-ering its core approach, key physical features, implementation details, and known strengths and limi-tations. 5. Best Evolved Algorithm Analysis. Analogous analysis of the top-performing evolved algorithm, with emphasis on novel elements that distinguish it from the baseline and a discussion of its strengths and limitations. 6. Evolution Improvements. A detailed comparison identifying key modifications introduced by evo-lution, their physical justification, novel techniques discovered, trade-offs (e.g., computation time vs. accuracy, complexity vs. maintainability), and an assessment of scientific validity and generalization potential. 7. Experiment Configuration. The hyperparameters used for the evolutionary run, including the number of islands, migration interval, maximum generations, and which LLM models were employed. 8. Appendix: Algorithm Code. Full source code of both the baseline seed program and the best evolved algorithm, enabling direct inspection and reproducibility. Where applicable, reports additionally include tables of tunable parameters with their optimized values, bounds, and optimization method (e.g., automatic differentiation). The 21 cm tidal reconstruction report also highlights a particularly noteworthy intermediate solution discovered at generation 52, which achieves near-optimal performance with only 9 tunable parameters compared to the 64 parameters of the overall best solution. These reports can serve as a starting point for human researchers to assess whether discovered algorithms merit further investigation and to understand the physical intuition behind the improvements. 47