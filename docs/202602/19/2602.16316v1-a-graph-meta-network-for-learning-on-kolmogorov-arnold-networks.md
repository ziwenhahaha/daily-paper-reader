---
title: A Graph Meta-Network for Learning on Kolmogorov-Arnold Networks
title_zh: 一种用于在 Kolmogorov-Arnold 网络上进行学习的图元网络
authors: "Guy Bar-Shalom, Ami Tavory, Itay Evron, Maya Bechler-Speicher, Ido Guy, Haggai Maron"
date: 2026-02-18
pdf: "https://arxiv.org/pdf/2602.16316v1"
tags: ["query:sr"]
score: 7.0
evidence: 在KAN网络上进行学习，作为MLP的替代方案
tldr: 本研究针对新兴的 Kolmogorov-Arnold 网络（KANs）提出了首个权重空间学习架构 WS-KAN。由于传统方法在处理神经网络参数时难以捕捉结构对称性，作者证明了 KANs 具有与 MLP 相似的置换对称性，并据此设计了 KAN-graph 图表示。WS-KAN 通过在图结构上进行学习，能够有效处理 KANs 的参数空间。实验表明，WS-KAN 在预测 KANs 性能等任务上显著优于基准模型，并具备强大的表达能力。
motivation: 现有的权重空间模型主要针对 MLP 设计，缺乏能够有效处理 Kolmogorov-Arnold 网络（KANs）参数对称性的专用架构。
method: 提出将 KANs 表示为 KAN-graph，并开发了首个利用该图结构及其置换对称性的权重空间架构 WS-KAN。
result: 实验证明 WS-KAN 在多种任务上均显著优于忽略结构的基准模型，并被证明在理论上能复制输入 KAN 的前向传播过程。
conclusion: WS-KAN 为 KANs 的权重空间学习提供了有效的解决方案，展示了结合结构对称性在元学习任务中的重要性。
---

## 摘要
权重空间模型直接从神经网络的参数中学习，从而能够执行诸如预测其在新数据集上的准确率等任务。朴素的方法（例如将 MLP 应用于展平的参数）表现不佳，这使得设计更好的权重空间架构成为一个核心挑战。虽然先前的工作利用标准网络中的置换对称性来指导此类设计，但目前尚不存在针对 Kolmogorov-Arnold 网络 (KANs) 的类似分析或定制架构。在这项工作中，我们证明了 KAN 具有与 MLP 相同的置换对称性，并提出了 KAN-graph，即其计算的图表示。在此基础上，我们开发了 WS-KAN，这是第一个在 KAN 上学习的权重空间架构，它自然地考虑了它们的对称性。我们分析了 WS-KAN 的表达能力，表明它可以复制输入 KAN 的前向传递——这是评估权重空间架构表达能力的标准方法。我们构建了一个涵盖多种任务的经过训练的 KAN 的综合“模型动物园”（zoo），并将其作为基准来对 WS-KAN 进行实证评估。在所有任务中，WS-KAN 的表现始终优于与结构无关的基准模型，且通常具有显著优势。我们的代码可在 https://github.com/BarSGuy/KAN-Graph-Metanetwork 获取。

## Abstract
Weight-space models learn directly from the parameters of neural networks, enabling tasks such as predicting their accuracy on new datasets. Naive methods -- like applying MLPs to flattened parameters -- perform poorly, making the design of better weight-space architectures a central challenge. While prior work leveraged permutation symmetries in standard networks to guide such designs, no analogous analysis or tailored architecture yet exists for Kolmogorov-Arnold Networks (KANs). In this work, we show that KANs share the same permutation symmetries as MLPs, and propose the KAN-graph, a graph representation of their computation. Building on this, we develop WS-KAN, the first weight-space architecture that learns on KANs, which naturally accounts for their symmetry. We analyze WS-KAN's expressive power, showing it can replicate an input KAN's forward pass - a standard approach for assessing expressiveness in weight-space architectures. We construct a comprehensive ``zoo'' of trained KANs spanning diverse tasks, which we use as benchmarks to empirically evaluate WS-KAN. Across all tasks, WS-KAN consistently outperforms structure-agnostic baselines, often by a substantial margin. Our code is available at https://github.com/BarSGuy/KAN-Graph-Metanetwork.