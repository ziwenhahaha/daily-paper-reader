Title: Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation

URL Source: https://arxiv.org/pdf/2602.16551v1

Published Time: Thu, 19 Feb 2026 02:08:41 GMT

Number of Pages: 17

Markdown Content:
# Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation 

## Rui Hu 1‚Ä†, Yue Wu 1‚Ä†, Tianhao Su 1, Yin Wang 1, Shunbo Hu 1* ,Jizhong Huang 1

1Shanghai University, Shanghai, China. *Corresponding author(s). E-mail(s): shunbohu@shu.edu.cn; 

‚Ä†These authors contributed equally to this work. 

Abstract 

The preservation of cultural heritage is increasingly transitioning towards data-driven predictive maintenance and ‚ÄùDigital Twin‚Äù construction. However, the mechanical constitutive models required for high-fidelity simulations remain frag-mented across decades of unstructured scientific literature, creating a ‚ÄùData Silo‚Äù that hinders conservation engineering. To address this, we present an automated, two-stage agentic framework leveraging Large Language Models (LLMs) to extract mechanical constitutive equations, calibrated parameters, and metadata from PDF documents. The workflow employs a resource-efficient ‚ÄùGatekeeper‚Äù agent for relevance filtering and a high-capability ‚ÄùAnalyst‚Äù agent for fine-grained extraction, featuring a novel Context-Aware Symbolic Grounding mechanism to resolve mathematical ambiguities. Applied to a corpus of over 2,000 research papers, the system successfully isolated 113 core documents and constructed a structured database containing 185 constitutive model instances and over 450 calibrated parameters. The extraction precision reached 80.4%, establishing a highly efficient ‚ÄùHuman-in-the-loop‚Äù workflow that reduces manual data cura-tion time by approximately 90%. We demonstrate the system‚Äôs utility through a web-based Knowledge Retrieval Platform, which enables rapid parameter discov-ery for computational modeling. This work transforms scattered literature into a queryable digital asset, laying the data foundation for the ‚ÄùDigital Material Twin‚Äù of built heritage. 

1 

> arXiv:2602.16551v1 [cs.DB] 18 Feb 2026 Keywords: Cultural Heritage Conservation, Large Language Models, Knowledge Extraction, Constitutive Modeling, Digital Twins, Heritage Materials, Automated Data Mining

# 1 Introduction 

The preservation of cultural heritage is increasingly embracing digital technologies to enhance both understanding and protection of irreplaceable historical assets [1]. The concept of ‚ÄúDigital Twin‚Äù‚Äîa dynamic virtual replica that mirrors the physical state and behavior of a real-world entity‚Äîhas emerged as a transformative paradigm in heritage conservation [2, 3]. Coupled with ‚ÄúPreventive Conservation,‚Äù which empha-sizes proactive monitoring and intervention rather than reactive restoration, these approaches promise to revolutionize how we safeguard our built heritage for future generations [4]. However, a Digital Twin is more than just a high-fidelity 3D geometric scan; it requires faithful physical simulation to predict structural responses under stress. Central to this physical fidelity is numerical simulation, particularly Finite Element Analysis (FEA), which enables researchers to assess structural safety and predict long-term behavior [5, 6]. From the seismic vulnerability assessment of medieval cathedrals to the thermal stress analysis of ancient frescoes, numerical modeling has become an indispensable tool [7, 8]. Yet, the reliability of any such simulation‚Äîand by extension, the predictive util-ity of the Digital Twin‚Äîis fundamentally constrained by the accuracy of its input parameters. Without accurate mechanical constitutive models to describe how mate-rials respond to applied loads, even the most sophisticated Digital Twin remains a visual shell, incapable of reliable structural prognosis [9, 10]. Heritage materials present unique and formidable challenges for numerical mod-eling that distinguish them from modern engineering materials. Ancient mortars, weathered stones, decayed timber, and historical bricks exhibit significant variabil-ity arising from multiple sources: heterogeneous raw materials available to historical craftsmen, diverse manufacturing techniques that varied across regions and eras, cen-turies of environmental exposure and degradation, and the inherent complexity of aging processes [11, 12]. Over the past several decades, substantial experimental research has been devoted to characterizing these materials. Studies have investigated the mechanical properties of Roman concrete [13], the stress-strain behavior of Byzantine masonry [14], the creep and relaxation of historical timber structures [15], and the damage evolution in weathered limestone [16]. This body of work has yielded valuable constitutive models with experimentally calibrated parameters that could, in principle, enable accurate numerical simulations. Yet in practice, this wealth of knowledge remains effectively inaccessible. The data exists as fragmented information scattered across thousands of PDF documents published in diverse journals spanning heritage science, civil engineering, materials science, and geotechnical engineering. Each publication presents its findings using 2varying nomenclature, different equation formats, and inconsistent parameter defini-tions. There is no unified database, no standardized schema, and no efficient means for a researcher to retrieve the specific constitutive model and parameters needed for their simulation task. This ‚Äúdata silo‚Äù problem has tangible consequences. Researchers seeking material parameters must conduct extensive literature reviews‚Äîa process that is time-consuming, prone to oversight, and often incomplete. Alternatively, they resort to generic handbook values for ‚Äúbrick‚Äù or ‚Äúlimestone‚Äù that may bear little resemblance to the specific historical material under investigation. The result is either duplicated effort across research groups or simulations of questionable accuracy that could lead to misguided conservation decisions. The obvious solution‚Äîautomating the extraction of constitutive models from the scientific literature‚Äîfaces formidable technical hurdles that have limited pre-vious attempts, despite the growing momentum of materials informatics in other domains [17‚Äì19]. First, the multimodal complexity of scientific documents poses significant chal-lenges. Research papers are not merely text; they interleave prose, mathematical equations rendered in various formats (inline notation, displayed formulas, equation arrays), tables with numerical data, and figures with embedded information. Tradi-tional Natural Language Processing (NLP) pipelines, designed primarily for plain text, struggle to parse and interpret this heterogeneous content [20‚Äì22]. Second, the identification of the governing constitutive law is complicated by the narrative structure of theoretical mechanics literature. Unlike structured data fields, a single research paper typically utilizes a multitude of equations to narrate a deriva-tion process. These include precursor models introduced for comparison, intermediate derivation steps (e.g., thermodynamic consistency checks or yield surface evolution), and uncorrected trial formulations. Distinguishing the specific, calibrated constitutive model from this background of ‚Äúmathematical noise‚Äù requires a high-level seman-tic understanding of the document‚Äôs logical flow. An automated system must resolve long-range dependencies to determine which equation represents the author‚Äôs final contribution rather than a literature review or a transient step in the derivation. Third, and perhaps more fundamentally, is the challenge of symbolic grounding‚Äî the task of associating abstract mathematical symbols with their specific physical meanings. In constitutive modeling, the symbol E might denote Young‚Äôs modulus in one paper, activation energy in another, and total energy in a third. The Greek letter œÉ typically represents stress, but its subscripts and the surrounding context determine whether it refers to normal stress, effective stress, yield stress, or something else entirely. Successfully extracting a constitutive model requires not just identifying the equation, but understanding what each symbol means [23]. Fourth, contextual metadata is essential for proper application of extracted models. A stress-strain relationship for ‚Äúsandstone‚Äù is of limited utility without knowing the specific type of sandstone, its provenance, the test conditions (temperature, humidity, strain rate, confining pressure), and the range of applicability. Capturing this contex-tual information requires understanding natural language descriptions that may be scattered across multiple sections of a paper. 3Traditional approaches based on regular expressions, named entity recognition, or rule-based systems have achieved limited success in this domain. They can identify patterns but lack the reasoning capability to handle ambiguity, interpret context, and make inferences where information is implicit rather than explicit. Recent advances in Large Language Models (LLMs) offer new possibilities for addressing these challenges. State-of-the-art models have demonstrated remarkable capabilities in zero-shot reasoning, understanding context, following complex instruc-tions, and generating structured outputs [24, 25]. Crucially, these models can process and reason about mathematical notation, understand domain-specific terminology, and maintain coherence across long documents. The few-shot and zero-shot learning capabilities of LLMs are particularly advan-tageous for domain-specific knowledge extraction tasks. Unlike traditional machine learning approaches that require large labeled training datasets‚Äîimpractical to cre-ate for the specialized domain of heritage material constitutive models‚ÄîLLMs can be guided through carefully designed prompts to perform extraction tasks with minimal or no task-specific examples [26‚Äì28]. However, applying LLMs to scientific knowledge extraction is not without challenges. LLMs are prone to ‚Äúhallucination‚Äù‚Äîgenerating plausible-sounding but factually incorrect information [29]. Processing entire papers with large LLMs is computationally expensive, making brute-force approaches impractical for large-scale database construction. And ensuring consistent, structured outputs that conform to a predefined schema requires careful prompt engineering and validation mechanisms. In this paper, we present an automated workflow for extracting mechanical con-stitutive models from scientific literature, specifically targeting cultural heritage materials. Our contributions are summarized as follows: 1. Development of a novel two-stage agentic framework. We introduce a pipeline employing a cost-effective ‚ÄúGatekeeper‚Äù agent for relevance screening and a high-capability ‚ÄúAnalyst‚Äù agent for precise extraction [30]. This collabora-tive design significantly reduces computational overhead while maintaining high precision in handling complex multimodal documents. 2. Establishment of a specialized mechanical constitutive model database. 

Through the rigorous screening of over 2,000 papers, we identified 113 high-quality studies meeting strict criteria for heritage relevance and experimental valida-tion. This effort yielded a structured repository containing 185 constitutive model instances and over 450 calibrated parameters. 3. Implementation of the Heritage Materials Constitutive Database Plat-form. We developed a web-based system that facilitates intelligent data ingestion and semantic knowledge retrieval. This platform bridges the gap between static literature and engineering practice, enabling researchers to efficiently query range-bound parameters for downstream numerical simulations. 42 Methodology 

## 2.1 Overview of the Agentic Framework 

We propose a novel Two-Stage Agentic Framework designed to automate the extrac-tion of mechanical constitutive models from unstructured scientific literature. The framework addresses three primary challenges in this domain:(1) the high computa-tional cost of processing large volumes of irrelevant documents [31]; (2) the complexity of isolating the target constitutive model amidst a high density of mathematical expressions, which often include intermediate derivation steps and uncorrected base-line models; and (3) the need for precise semantic grounding of mathematical symbols in diverse contexts. As illustrated in Figure 1, the system architecture operates as a coherent pipeline comprising four functional modules. The workflow commences with the Data Ingestion Module, which parses raw PDF documents and serializes the heterogeneous content into a machine-readable text stream. This stream is then processed by Stage I (The Gatekeeper), a resource-efficient agent that acts as a relevance filter to discard non-pertinent literature based on broad-spectrum criteria. Surviving documents advance to Stage II (The Analyst), a high-reasoning capability agent that performs the core task of fine-grained extraction, utilizing symbolic grounding to map mathematical variables to their physical meanings. Finally, the validated outputs are consolidated into the Structured Database, where data is standardized according to a strict schema to ensure interoperability and ease of retrieval.  

> Fig. 1 Overview of the Two-Stage Agentic Framework. The workflow adopts a coarse-to-fine strat-egy: (1) Raw PDF ingestion and serialization; (2) The Gatekeeper acts as a low-cost filter to discard irrelevant literature; (3) The Analyst employs deep reasoning for symbolic grounding; (4) The final parameters are stored in a structured JSON database.

5This hierarchical design follows the principle of ‚Äúcoarse-to-fine‚Äù processing, ensur-ing that computational resources are allocated efficiently by reserving high-capability models only for highly relevant documents [32]. 

## 2.2 Data Acquisition and Preprocessing 

2.2.1 Source Selection Criteria 

To construct a representative dataset for this study, we sourced research papers exclu-sively from the arXiv preprint repository, targeting categories relevant to physics and engineering. The retrieval process employed a systematic boolean search strategy designed to identify the intersection of heritage science and solid mechanics. We con-structed search queries by combining keywords related to heritage materials with terms defining mechanical characterization and constitutive modeling. This initial retrieval phase focused on maximizing recall to ensure a comprehensive corpus for downstream filtering. 

2.2.2 PDF Parsing and Serialization 

Scientific papers in PDF format present a semi-structured data challenge. We utilized a custom parsing pipeline to serialize PDF documents into machine-readable text streams. To maintain the integrity of mathematical content, the parser was configured to retain raw character sequences, preserving inline notations where possible. For the subsequent Stage I processing, we implemented a Head-Truncation Strategy. Since key metadata regarding a paper‚Äôs scope and methodology are typ-ically concentrated in the opening sections, we extracted only the initial segment (approximately 8,000 characters) of each document. This strategy ensures that the Gatekeeper agent receives sufficient context for relevance judgment without incurring the computational cost of processing full-text body paragraphs. 

## 2.3 Stage I: The Gatekeeper Agent (Relevance Filtering) 

The first stage of our framework employs a resource-efficient Large Language Model designated as the ‚ÄúGatekeeper.‚Äù Its primary objective is to act as a semantic filter, discarding documents that do not strictly align with the database‚Äôs inclusion criteria. This hierarchical filtering prevents the downstream ‚ÄúAnalyst‚Äù agent from processing irrelevant literature. The Gatekeeper operates under a constrained reasoning protocol, evaluating each document against three non-negotiable dimensions. First, the agent verifies Domain Relevance to ensure the study focuses explicitly on cultural heritage materials rather than modern synthetic composites. Second, it checks for Theoretical Content, con-firming that the paper formulates or utilizes a specific mechanical constitutive model. Third, the agent mandates Experimental Validation, requiring the presence of empiri-cal data used for parameter calibration. Only documents satisfying all three conditions are classified as relevant and forwarded to the full-text processing pipeline. 62.4 Stage II: The Analyst Agent (Knowledge Extraction) 

Papers validated by the Gatekeeper are processed by the ‚ÄúAnalyst‚Äù agent, a mod-ule powered by a foundation model with advanced reasoning capabilities. Unlike the binary classification task in Stage I, this stage involves a complex sequence-to-structure generation task. We formalize this process as mapping an unstructured document D

to a structured knowledge graph G under a set of semantic constraints. 

2.4.1 Context-Aware Target Identification 

A critical challenge in mechanical literature is the ‚Äúdense mathematical landscape.‚Äù A single theoretical paper typically contains a multitude of equations, ranging from fun-damental physical laws (e.g., conservation of mass) to intermediate derivation steps and unmodified legacy models used for comparative analysis. Distinguishing the authors‚Äô proposed constitutive model from these auxiliary equations is non-trivial and cannot be achieved through visual formatting features alone. It requires a high level of contextual understanding to interpret the narrative flow‚Äîspecifically, identifying lin-guistic cues that signal the transition from derivation to the final proposal (e.g., ‚Äúwe modify the standard Maxwell model by introducing...‚Äù). The Analyst agent is explic-itly designed to navigate this complexity, filtering out derivational intermediates and baseline formulas to pinpoint the definitive constitutive relation. 

2.4.2 Mathematical Formalization of Symbolic Grounding 

The core challenge in extracting constitutive models is Symbolic Grounding‚Äîthe disambiguation of mathematical notation based on context [33]. A constitutive equation E extracted from the text typically contains a set of abstract symbols 

S = {s1, s 2, . . . , s n}. In isolation, a symbol si is semantically ambiguous. For example, the symbol œï could denote ‚Äùporosity‚Äù in a petrophysical study or ‚Äùinternal friction angle‚Äù in a geotechnical context. We define the grounding task as finding a bijective mapping function f : S ‚Üí P , where P is the set of physical definitions derived from the local context Clocal . The Analyst agent is optimized to maximize the conditional probability of the correct mapping: ÀÜP = argmax  

> P‚ààP

p(P | S, E, C local ) (1) By strictly enforcing this probabilistic mapping, the system ensures that generic variables are accurately bound to their specific rheological meanings (e.g., mapping 

œÉc specifically to ‚ÄùUniaxial Compressive Strength‚Äù rather than generic stress). 

2.4.3 Schema-Constrained Decoding 

To transform the extracted knowledge into a queryable format, we enforce a Schema-Constraint Decoding strategy [34]. Instead of free-form generation, the model‚Äôs output space is restricted to a pre-defined JSON schema J . This schema mandates the extraction of a 5-tuple vector for each material model: 

Vmodel = ‚ü®E , Vmap , Mmeta , Œ†params , Sval ‚ü© (2) 7where E is the constitutive equation in standard LaTeX format, Vmap repre-sents the symbol-definition pairs, Mmeta denotes material metadata (e.g., ‚ÄùAncient Sandstone‚Äù), Œ† params contains numerical parameter values, and Sval identifies the experimental validation method. 

2.4.4 Error Handling and Self-Correction 

To mitigate the inherent non-determinism of generative models, we implemented a closed-loop verification mechanism. The system parses the output string against the strict JSON schema J . Let Ot be the output at step t. If a syntax error or schema violation œµ is detected (i.e., Ot /‚àà J ), the error signal œµ is fed back into the model context as a negative constraint. The model then performs a self-correction step to generate Ot+1 , significantly improving the rigorousness of the final database. 

# 3 Results 

## 3.1 Database Construction Statistics 

To rigorously test the scalability of our framework, we processed an initial corpus of over 2,000 research papers sourced from the arXiv preprint repository. The Stage I (Gatekeeper) agent acted as a high-throughput filter, rejecting the vast majority of documents identified as irrelevant. This strict filtering yielded a refined set of 113 highly relevant papers. From these core documents, the Stage II (Analyst) agent successfully populated the Heritage Material Constitutive Database. Specifically, the system extracted 185 validated con-stitutive model instances and over 450 experimentally calibrated material parameters. Crucially, each entry is linked with rich contextual metadata, including material prove-nance and environmental testing conditions, ensuring the data is ready for high-fidelity numerical simulation. Figure 2 illustrates the distribution of the extracted constitutive models by mechan-ical mechanism. The dataset is dominated by Elasto-Plasticity models (31.9%) and Failure & Damage Criteria (24.8%), reflecting the heritage conservation community‚Äôs primary focus on assessing structural stability and collapse risks. Other significant categories include Time-dependent/Rheology models (12.4%), which are critical for analyzing long-term creep in historical timber and masonry. 

## 3.2 Performance Evaluation 

To quantitatively assess the extraction quality, we established a rigorous Ground Truth (GT) dataset. The entire set of 113 relevant papers identified in Stage I was manually annotated by domain experts in solid mechanics. This manual annotation process identified a total of 222 target constitutive model entities (Total Ground Truth), which served as the gold standard for validation. We defined the evaluation metrics as follows: A True Positive (TP) occurs when the agent correctly extracts a constitutive model that matches the expert annotation in both equation form and physical meaning. A False Positive (FP) represents an extracted entry that is either hallucinated or incorrectly identified (e.g., extracting 8Fig. 2 Distribution of Constitutive Mechanisms. The framework successfully categorized extracted models into distinct rheological behaviors. The prevalence of plasticity and damage models aligns with the need for safety assessment in heritage structures. 

an intermediate derivation step instead of the final model). A False Negative (FN) indicates a valid model present in the text that the agent failed to capture. Based on these definitions, the framework demonstrated robust capabilities with a Precision of 80.4% ( T P/ [T P + F P ]), a Recall of 83.3% ( T P/ [T P + F N ]), and an overall F1-Score of 81.9%. In addition to these aggregate metrics, Figure 3 visualizes the detailed performance characteristics through a binary classification analysis. The Confusion Matrix (Fig. 3a) reveals a True Positive (TP) count of 185 against a background of 1311 True Negatives (TN). Note that in this context, TNs repre-sent potential candidate text blocks or equations that were correctly rejected by the agent as non-constitutive models. The model maintains a balanced error distribu-tion between False Positives (45) and False Negatives (37). A retrospective analysis suggests that these residual errors are largely attributable to the parsing challenges of non-standard tabular structures in older PDFs, rather than semantic reasoning failures. The ROC curve (Fig. 3b) further characterizes the classifier‚Äôs discriminatory power, yielding an Area Under Curve (AUC) of 0.782. Notably, the operational point of our deployed model (indicated by the red dot) is situated at a False Positive Rate (FPR) of only 3.3%. This low FPR is strategically vital for automated database construction, as it minimizes the contamination of the repository with irrelevant noise, prioritizing data purity even at the slight expense of recall. The results indicate that while the system is highly effective (F1 ‚âà 0.82), a small fraction of extracted data benefits from human verification. Nevertheless, compared 9Predicted:      

> Equation
> Predicted:
> Not Eq
> Actual:
> Equation
> Actual:
> Not Eq
> TP
> 185
> 11.7%
> FN
> 37
> 2.3%
> FP
> 45
> 2.9%
> TN
> 1311
> 83.1%
> (A) Confusion Matrix
> (Extraction Performance)
> 0.0 0.2 0.4 0.6 0.8 1.0
> False Positive Rate (1 - Specificity)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> True Positive Rate (Sensitivity / Recall)
> F1 Score: 0.819
> Precision: 0.804
> Recall: 0.833
> (B) ROC Curve
> ROC Curve (AUC = 0.762)
> Random Classifier
> Model Performance
> (FPR=0.033, TPR=0.833)
> 200
> 400
> 600
> 800
> 1000
> 1200
> Count

Fig. 3 Quantitative Evaluation of the Framework. (a) Confusion Matrix showing the extraction performance (TP=185, TN=1311); (b) ROC Curve with an AUC of 0.782. The selected operating point (Red Dot) corresponds to a low False Positive Rate (3.3%), prioritizing the reliability of database entries. 

to fully manual literature review, this semi-automated workflow reduces the workload by orders of magnitude while maintaining a high standard of data integrity. 

## 3.3 Qualitative Analysis: Reasoning Capabilities 

Beyond statistical metrics, the qualitative capability of the framework is visually demonstrated in the case study of Figure 4. This example, extracted from a study on Kaolinite clay rheology [35], highlights two critical reasoning advantages of the Agentic framework over traditional rule-based parsers. First, the system demonstrates sophisticated Contextual Fusion and Noise Filter-ing. In scientific literature, constitutive models are rarely defined in a single contiguous block; instead, they are often distributed across disjointed sections, intermixed with derivation steps. As seen in Figure 4 (Panel A), the source document spatially sep-arates the stress evolution component from the structural kinetic law. Crucially, the text is also interspersed with intermediate derivation forms (e.g., uncorrected precur-sor definitions). Traditional extractors would blindly capture all mathematical blocks or treat them as unrelated strings. In contrast, our Analyst agent performed Govern-ing Equation Discrimination: it successfully filtered out the intermediate mathematical noise and aggregated the spatially distinct components into a single, self-contained constitutive formulation shown in the Verified Knowledge Entry (Panel C). Second, the agent demonstrates remarkable Domain-Adaptive Disambiguation and Physical Plausibility Reasoning. A persistent challenge in scientific extraction is the syntactic ambiguity of tabular headers. In Table I (Panel A), the column header ‚ÄúŒ∑‚àû √ó 10 3‚Äù is notationally ambiguous: it could imply a magnitude of 10 3 (common in geotechnical soil mechanics) or a scaling factor requiring multiplication by 10 ‚àí3

(to display small values legibly). A rule-based parser typically fails to resolve this without hard-coded heuristics. However, our Analyst agent successfully resolved this by grounding the extraction in the specific physical domain of the paper‚Äî‚Äúaqueous 10 (A)  Source PDF(Raw Context)          

> Contextual Constraint (Eq. 1 -3)
> Raw Tabular Data (Table I)
> (B) Structured Extraction ÔºàJSON Ôºâ
> Target Material&Model
> Target Material&Model
> (C) Verified Knowledge Entry
> Constitutive Formulation
> Calibrated Parameters
> ùúô =0.05 ,Œîùë° =10ùë†

Fig. 4 Qualitative Extraction Case Study. The extraction process for a Jeffreys-type Viscoelastic model. (A) The raw PDF presents the constitutive law components scattered across disjointed sections and data in a complex table. (B) The Agent fuses these components into a structured JSON while filtering derivation noise. (C) The final Verified Knowledge Entry shows correct Symbolic Grounding (mapping Œæ to structural kinetics) and Physical Plausibility Verification (resolving the ambiguous header scale via domain constraints). 

clay suspensions‚Äù rather than consolidated soils. Recognizing that suspension vis-cosities physically adhere to the order of water ( ‚àº 10 ‚àí3 Pa ¬∑s), the agent correctly interpreted the header as a scaling factor, converting the raw value 2 .33 to the stan-dardized 2 .33 √ó 10 ‚àí3 Pa ¬∑s. This proves the framework‚Äôs ability to utilize semantic context to impose physical constraints on extracted data, avoiding order-of-magnitude hallucinations. 

# 4 Application Demonstration 

To bridge the gap between unstructured literature and engineering practice, we devel-oped the Heritage Materials Constitutive Database Platform. This web-based system supports a closed-loop workflow, enabling both the intelligent ingestion of new data and the semantic retrieval of existing knowledge. 

## 4.1 Intelligent Data Ingestion 

The platform features an automated ingestion module designed to streamline the con-tribution of new data. As demonstrated in Figure 5, researchers can simply upload raw PDF literature via the web interface. Upon upload, the backend extraction agents immediately initiate a parsing sequence. First, the system identifies the specific con-stitutive model and renders its mathematical formulation using LaTeX for visual verification. Simultaneously, it extracts the fitted parameters‚Äîsuch as cohesion and friction angle‚Äîand automatically populates the corresponding database fields. This 11 workflow effectively transforms static, unstructured PDFs into structured digital assets without requiring manual data entry.  

> Fig. 5 Automated Data Ingestion Interface. The detail view shows the extraction results from an uploaded PDF. The system automatically identifies the constitutive model, renders the equation, and populates the fitted parameters for user verification.

## 4.2 Semantic Knowledge Retrieval 

Beyond individual file processing, the platform facilitates data discovery through a dedicated search engine. As illustrated in Figure 6, users can query the aggregated database by specific material categories or mechanical properties. In response to a query, the system retrieves all matching entries, displaying the calibrated parameters alongside their source validation status. Consequently, this capability allows con-servation engineers to rapidly establish range-bound reference values for numerical simulations, significantly reducing the lead time compared to conducting exhaustive manual literature reviews. 12 Fig. 6 Semantic Retrieval Interface. The search module allows researchers to filter constitutive models by material type or property. The results list displays key metadata, enabling rapid selection of appropriate parameters for simulation. 

# 5 Discussion 

The results of this study mark a departure from the traditional literature review workflow in heritage conservation. While manual compilation is accurate but labor-intensive, our Two-Stage Agentic Framework offers a scalable alternative. It is important to acknowledge that the system achieves an F1-score of approximately 0.82, not perfection. However, in an engineering context, this creates a valuable ‚ÄùHuman-in-the-loop‚Äù workflow: the Agent acts as a high-speed pre-screener that reduces approximately 90% of the manual workload, leaving domain experts to verify the ‚Äùfinal mile‚Äù of the data (as demonstrated in the verified entries in Section 4). This synergy combines the scalability of AI with the reliability of human expertise, making large-scale database construction feasible for the first time. Furthermore, our approach specifically addresses the unique ‚ÄùData Poverty‚Äù chal-lenge in heritage science. Unlike general computer vision domains (e.g., ImageNet) that benefit from millions of data points, valuable heritage experimental data is scarce 13 and scattered across decades of PDF publications. By structuring 185 constitutive models from 113 core papers, we have aggregated what is likely the largest structured repository of heritage mechanical properties to date. This transition from scattered PDFs to a unified SQL database enables researchers to move from heuristic guessing to data-driven probabilistic assessment, such as determining the confidence intervals of material stiffness. Despite these advancements, challenges remain in handling extreme document vari-ability. First, regarding document quality, the parser exhibits sensitivity to the layout of older, low-resolution scanned documents (pre-2000s), where significant noise can disrupt structural recognition. Second, regarding multimodal extraction, while our current agent utilizes Vision-Language capabilities to identify and interpret figures, the precise digitization of continuous data points from complex scientific charts (e.g., dense stress-strain hysteresis loops) remains a non-trivial task compared to textual parameter extraction. Future work will focus on enhancing the agent‚Äôs ability to per-form ‚ÄùChart-to-Data‚Äù conversion, creating a truly holistic extraction pipeline that fully digitizes both semantic and graphical information. 

# 6 Conclusion 

This paper presented an automated, agentic workflow for extracting mechanical con-stitutive models from scientific literature, explicitly tailored to address the ‚ÄùData Silo‚Äù problem in cultural heritage conservation. By leveraging a cost-effective Gatekeeper-Analyst architecture, we successfully fil-tered a corpus of over 2,000 papers to isolate 113 high-relevance documents. The system extracted 185 constitutive model instances and over 450 calibrated parameters with a precision of 80.4%. Crucially, the framework overcomes the challenge of sym-bol ambiguity through context-aware symbolic grounding, ensuring that mathematical variables are correctly mapped to their physical definitions. The utility of this framework is realized through the developed Heritage Materials Constitutive Database Platform, which transforms static literature into an interactive, searchable digital asset. As demonstrated in our application scenarios, this platform enables rapid retrieval of material parameters for computational modeling and struc-tural assessment. We believe this work lays the foundation for the ‚ÄùDigital Material Twin‚Äù of our built heritage, ensuring that the scientific knowledge of the past is preserved and operationalized for future conservation efforts. 

# Acknowledgments 

This work is supported by the Advanced Materials-National Science and Technology Major Project(2025ZD0618802) and the Shanghai Technical Service Center of Science and Engineering Computing, Shanghai University. 

# References 

[1] Jouan, P. & Hallot, P. Digital twin: Research framework to support preventive conservation policies. The International Archives of the Photogrammetry, Remote 

14 Sensing and Spatial Information Sciences 43 , 609‚Äì615 (2020). [2] Angjeliu, G., Coronelli, D. & Cardani, G. Development of the simulation model for digital twin applications in historical masonry buildings: The integration between numerical and experimental reality. Computers & Structures 238 ,106282 (2020). [3] Kujawa, M., Lubowiecka, I. & Szymczak, C. Finite element modelling of a historic church structure in the context of a masonry damage analysis. Engineering Failure Analysis 107 , 104233 (2020). [4] Della Torre, S. Italian perspective on the planned preventive conservation of architectural heritage. Frontiers of architectural research 10 , 108‚Äì116 (2021). [5] Roca, P., Cervera, M., Gariup, G. & Pela‚Äô, L. Structural analysis of masonry historical constructions. classical and advanced approaches. Archives of compu-tational methods in engineering 17 , 299‚Äì325 (2010). [6] D‚Äôaltri, A. M. et al. Modeling strategies for the computational analysis of unre-inforced masonry structures: Review and classification: Am d‚Äôaltri et al. Archives of computational methods in engineering 27 , 1153‚Äì1185 (2020). [7] Betti, M. & Vignoli, A. Numerical assessment of the static and seismic behaviour of the basilica of santa maria all‚Äôimpruneta (italy). Construction and Building Materials 25 , 4308‚Äì4324 (2011). [8] Ant¬¥ on, D., Pineda, P., Medjdoub, B. & Iranzo, A. As-built 3d heritage city modelling to support numerical structural analysis: Application to the assessment of an archaeological remain. Remote Sensing 11 , 1276 (2019). [9] Vuoto, A., Funari, M. F. & Louren¬∏ co, P. B. Shaping digital twin concept for built cultural heritage conservation: a systematic literature review. International journal of architectural heritage 18 , 1762‚Äì1795 (2024). [10] Funari, M. F., Hajjat, A. E., Masciotta, M. G., Oliveira, D. V. & Louren¬∏ co, P. B. A parametric scan-to-fem framework for the digital twin generation of historic masonry structures. Sustainability 13 , 11088 (2021). [11] Elsen, J. Microscopy of historic mortars‚Äîa review. Cement and concrete research 

36 , 1416‚Äì1424 (2006). [12] Winkler, E. Stone in architecture: properties, durability (Springer Science & Business Media, 2013). [13] Jackson, M. D. et al. Mechanical resilience and cementitious processes in imperial roman architectural mortar. Proceedings of the National Academy of Sciences 

111 , 18484‚Äì18489 (2014). 15 [14] Binda, L., Saisi, A. & Tiraboschi, C. Investigation procedures for the diagnosis of historic masonries. Construction and Building Materials 14 , 199‚Äì233 (2000). [15] Granello, G. & Palermo, A. Creep in timber: Research overview and comparison between code provisions. NZ Timber Des. J 27 , 6‚Äì22 (2019). [16] T¬® or¬® ok, ¬¥A. & PÀá rikryl, R. Current methods and future trends in testing, durability analyses and provenance studies of natural stones used in historical monuments. 

Engineering Geology 115 , 139‚Äì142 (2010). [17] Tshitoyan, V. et al. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature 571 , 95‚Äì98 (2019). [18] Kononova, O. et al. Text-mined dataset of inorganic materials synthesis recipes. 

Scientific data 6, 203 (2019). [19] Jablonka, K. M. et al. 14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. Digital discovery 

2, 1233‚Äì1250 (2023). [20] Lopez, P. Grobid: Combining automatic bibliographic data recognition and term extraction for scholarship publications. International Conference on Theory and Practice of Digital Libraries 473‚Äì474 (2009). [21] Blecher, L., Cucurull, G., Scialom, T. & Stojnic, R. Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418 (2023). [22] Smock, B., Pesala, R. & Abraham, R. Pubtables-1m: Towards comprehensive table extraction from unstructured documents. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 4634‚Äì4642 (2022). [23] Head, A. et al. Augmenting scientific papers with just-in-time, position-sensitive definitions of terms and symbols. Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems 1‚Äì18 (2021). [24] Wang, H. et al. Scientific discovery in the age of artificial intelligence. Nature 

620 , 47‚Äì60 (2023). [25] Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 , 24824‚Äì24837 (2022). [26] Brown, T. et al. Language models are few-shot learners. Advances in neural information processing systems 33 , 1877‚Äì1901 (2020). [27] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 ,22199‚Äì22213 (2022). 16 [28] Dunn, A. et al. Structured information extraction from complex scientific text with fine-tuned large language models. arXiv preprint arXiv:2212.05238 (2022). [29] Ji, Z. et al. Survey of hallucination in natural language generation. ACM computing surveys 55 , 1‚Äì38 (2023). [30] Xi, Z. et al. The rise and potential of large language model based agents: A survey. Science China Information Sciences 68 , 121101 (2025). [31] Chen, L., Zaharia, M. & Zou, J. Frugalgpt: How to use large language models while reducing cost and improving performance. Advances in Neural Information Processing Systems 36 , 10175‚Äì10200 (2023). [32] Yue, X. et al. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653 (2023). [33] Schubotz, M. et al. Semantification of identifiers in mathematics for better math information retrieval. Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval 135‚Äì144 (2016). [34] Willard, B. T. & Louf, R. Efficient guided generation for large language models. 

arXiv preprint arXiv:2307.09702 (2023). [35] Ran, R. et al. Understanding the rheology of kaolinite clay suspensions using bayesian inference. Journal of Rheology 67 , 241‚Äì252 (2023). 17