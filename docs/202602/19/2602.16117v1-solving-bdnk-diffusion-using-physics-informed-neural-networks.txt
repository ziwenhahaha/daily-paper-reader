Title: Solving BDNK diffusion using physics-informed neural networks

URL Source: https://arxiv.org/pdf/2602.16117v1

Published Time: Thu, 19 Feb 2026 01:21:41 GMT

Number of Pages: 30

Markdown Content:
# Solving BDNK diffusion using physics-informed neural networks 

Vicente Chomal´ ı-Castro, 1, ∗ Nick Clarisse, 1, † Nicki Mullins, 2, ‡ and Jorge Noronha 1, §

> 1

Illinois Center for Advanced Studies of the Universe and Department of Physics, University of Illinois Urbana-Champaign, Urbana, IL 61801, USA 

> 2

Department of Physics, North Carolina State University, Raleigh, NC 27695, USA 

(Dated: February 19, 2026) In this work, we reformulate the relativistic BDNK (Bemfica-Disconzi-Noronha-Kovtun) diffusion equation in flux-conservative form, and solve the resulting equations in (1+1)D using both a second-order Kurganov-Tadmor finite volume scheme and physics-informed neural networks (PINNs). In particular, we introduce the SA-PINN-ACTO framework, which combines the self-adaptive PINN technique with an exact enforcement of initial and periodic boundary conditions through an alge-braic transform of the network’s raw output, allowing the network to focus solely on minimizing the PDE residual. We test both approaches on smooth and discontinuous initial data, for both trivial and dynamically evolving velocity and temperature BDNK backgrounds, and for two characteris-tic speeds. The SA-PINN-ACTO method matches the converged Kurganov-Tadmor solutions for smooth profiles, while for discontinuous profiles the errors increase, reflecting an expected limitation of PINNs near sharp gradients. 

I. INTRODUCTION 

Hydrodynamics describes the long-wavelength, long-timescale behavior of systems governed by the dynamics of conserved quantities [1]. Its power and universality arise from the fact that the hydrodynamic equations of motion follow directly from conservation laws which, in the nonrelativistic setting, encode the local conservation of mass, energy, and momentum. For ideal fluids, the system closes once an equation of state is specified, and one can show that local solutions exist and are unique for appropriate initial data [2]. Beyond equilibrium, additional input is required to determine the behavior of dissipative fluxes and solve the corresponding system of partial differential equations. Hydrodynamics provides a reliable effective description whenever a clear scale separation exists, enabling a systematic gradient expansion of the constitutive relations [1]. This approximation applies when the characteristic gradient length scale ℓ of the macroscopic variables is much larger than the microscopic interaction scale lmicro (e.g. the mean free path in a dilute gas). This corresponds to the small Knudsen-number regime, Kn = lmicro /ℓ ≪ 1, in which hydrodynamics is expected to hold [1, 3]. Truncating the gradient expansion at first order yields the well-known Navier–Stokes equations [1], which successfully describe a wide range of phenomena from laminar flows to turbulent motion [4]. The same reasoning may be extended to incorporate relativity and determine how it modifies the equations of motion of fluid dynamics [5]. At zero chemical potential, relativistic hydrodynamics is built around the covariant conservation of the energy-momentum tensor, 

∂μT μν = 0 . (1) For an ideal relativistic fluid, the energy-momentum tensor takes the form 

T μν  

> 0

= εu μuν + P ∆μν , (2) where uμ is the fluid four-velocity (a future-directed timelike vector normalized as uμuμ = −1), ∆ μν = ημν + uμuν

projects orthogonally to uμ, ημν is the (Minkowski) spacetime metric, ε is the energy density, and P = P (ε) is the equilibrium pressure determined by the equation of state. Energy-momentum conservation then gives four equations, matching the four independent dynamical variables of ideal hydrodynamics, yielding a closed system. Dissipative effects can be incorporated via corrections encoded in a tensor Π μν , leading to 

T μν = T μν  

> 0

+ Π μν .    

> ∗faridc2@illinois.edu
> †nclari2@illinois.edu
> ‡nmmulli2@ncsu.edu
> §jn0508@illinois.edu
> arXiv:2602.16117v1 [nucl-th] 18 Feb 2026

2These viscous corrections depend on transport coefficients such as the shear viscosity η and the bulk viscosity ζ,which characterize the response to spacetime gradients. Modern formulations of relativistic dissipative hydrodynam-ics fall broadly into two classes: derivative-expansion formulations and second-order (Israel–Stewart-type) theories. The earliest derivative-expansion theories—those of Eckart [6] and Landau–Lifshitz [1]—define Π μν using first-order derivatives of the ideal hydrodynamic variables, with the two formulations corresponding to different hydrodynamic frames [7]. While these equations reduce to Navier–Stokes theory in the nonrelativistic limit, they face acausality and display unphysical instabilities around equilibrium [8] that prevent their use in most applications. The problems of the relativistic Navier–Stokes equations motivated early on a shift to frameworks where the dissipative fluxes are promoted to new dynamical variables. The prototypical example is Israel–Stewart (IS) theory [9], in which relaxation-type evolution equations for Π μν are derived either by expanding a non-equilibrium entropy current to second order in deviations from equilibrium or via a truncation of the relativistic Boltzmann equation [9, 10]. For a review, see [3]. Generalizations of the Israel–Stewart framework [10, 11] are widely used in modeling the quark-gluon plasma formed in ultrarelativistic heavy-ion collisions, see [12]. Israel–Stewart-type theories also naturally arise in astrophysics, prominently in studies of non-conserved currents and bulk-viscous effects in neutron star mergers [13–24]. Second-order theories enjoy significantly better physical and mathematical properties than the first-order Eckart or Landau–Lifshitz formulations. In the linear regime, causality and stability can be achieved with appropriate parameter choices [25], and for theories derived from an information-current principle [26, 27], strong hyperbolicity also holds near equilibrium [28]. Only recently have general nonlinear results become available. Indeed, general statements concerning causality and strong hyperbolicity have been rigorously established in the purely bulk-viscous case [29], in resistive relativistic magnetohydrodynamics around black holes [30], and for various second-order theories involving diffusion [31] and shear/bulk viscosity [32] (see also earlier work in [33]). Nevertheless, the full set of conditions ensuring causality, strong hyperbolicity, and local well-posedness—including all dissipative channels—remains unknown, with complexity further compounded by the large number of second-order contributions in general theories [10]. Global well-posedness statements have been obtained in the bulk-viscous case in Ref. [34] and, in [35], finite-time gradient blow-up and shock formation in Israel–Stewart theory with (1+1)-dimensional plane symmetry with bulk, shear, and diffusion were investigated. The Bemfica-Disconzi-Noronha-Kovtun (BDNK) theory [36–40] offers a simpler and more tractable approach to relativistic viscous hydrodynamics, at least near equilibrium. BDNK is the most general causal, stable, and strongly hyperbolic first-order formulation of relativistic hydrodynamics that includes shear, bulk, and diffusion effects while retaining only first-order derivatives of the hydrodynamic variables. Local well-posedness was proven in [40–43], and global well-posedness for small initial data in Minkowski space was established in [44, 45]. These results show that BDNK behaves precisely as a relativistic viscous hydrodynamic theory should, where small perturbations evolve nonlinearly toward equilibrium in a manner consistent with relativity. In that context, BDNK is unique among relativistic viscous frameworks because of this level of mathematical control. A distinguishing feature of BDNK is the presence of terms that involve first-order time derivatives even in the fluid rest frame, illustrating the flexibility in choosing the hydrodynamic frame. The additional transport parameters of BDNK, which supplement the usual η, ζ, and conductivity σ, encode this frame choice [36, 37]. A broad set of parameter choices can be used to maintain causality and stability for this theory, see [40, 46]. In BDNK hydrodynamics, hydrodynamic frame parameters can be chosen so that causality holds automatically, a striking contrast to Israel–Stewart-type theories, where causality must be checked through evolving nonlinear in-equalities [32, 47]. This structural simplicity makes BDNK not only mathematically appealing but also, at least in principle, highly convenient for numerical simulation. As a result, it has been applied in a variety of high-energy and astrophysical contexts—including linearized studies [48–50]—and has been used in numerous recent numerical works [51–60]. Diffusion plays a central role in relativistic hydrodynamics as it governs the relaxation of conserved-charge imbal-ances and determines how matter and charge are transported in fluids [1]. In this work, we adapt a flux-conservative formulation of BDNK relativistic viscous hydrodynamics introduced in [58], specifically to the case where only diffu-sion is taken into account. The flux-conservative equations are valid for a general U (1) conserved current, up to first order in derivatives. This flux-conservative formulation for relativistic diffusion is the first new result in this paper; the second novelty is of numerical origin, as here we show how to solve BDNK diffusion using neural networks. Many problems in theoretical and computational physics are governed by systems of partial differential equations (PDEs) that encode conservation laws, transport phenomena, or field dynamics. Traditional numerical methods, such as finite difference, finite volume, and spectral methods, are highly effective, but their implementation can be extremely challenging when handling contrived geometries and boundary conditions. Moreover, their accuracy and stability depend strongly on the discretization of the domain, whose refinement in space or time causes a rapid increase in computational cost, especially in multiphysics setups or in problems of high dimensionality. Despite these challenges, finite volume methods remain the standard tool for simulating systems governed by conservation laws, mainly because of their robustness and well-understood numerical properties. An example of such a method is the 3Kurganov-Tadmor (KT) central scheme [61, 62], which achieves second-order accuracy and remains numerically stable and robust under shock formation. In recent years, physics-informed neural networks (PINNs) [63] have emerged as a powerful alternative paradigm for tackling both forward and inverse problems governed by partial differential equations. The technique, which consists of embedding physical laws directly into the learning process of a neural network, is particularly useful because it constitutes a universal framework capable of learning solutions to a wide range of PDEs, such as nonlinear, stochastic, backward-in-time, or even fractional ones [63–66]. Further, experimental or incomplete data can be incorporated into the training process. The way physical laws are embedded into the training process of a PINN is by constructing loss functions that penalize violations of the governing equations (such as the Navier–Stokes equations, Einstein’s field equations, or, in this work, BDNK diffusion equation), as well as of any given initial and boundary conditions. Namely, one must define a residual—that is, a measure of how far the PINN’s predicted solution is from satisfying some constraints—and train the network to minimize this residual. In other words, one trains a neural network to generate data that approximately respects auxiliary conditions and the fundamental laws that govern some physical system. Unlike traditional numerical solvers, PINNs work with a mesh-free formulation, allowing one to easily handle complex geometries without requiring structured grids. Moreover, the continuous functional representation makes the approach well suited for problems involving many spatial or temporal variables [67]. A central motivation for using PINNs is that they incorporate the physical structure of the problem directly into the training process, unlike applications of neural networks in physics that are trained purely on historical data (as in weather-prediction models [68–71]), which behave as a “black-box” [72, 73]. Such purely data-driven neural-network models have been widely applied both in relativistic hydrodynamics [74–76] and in heavy-ion collision studies [77, 78]. In black-box settings, the network may learn correlations that reproduce the correct output but offer no insight into the underlying physical processes, and it is often unclear what features the model has actually captured. For physics applications where one is also interested in dynamical evolution rather than end-point predictions, such as in heavy-ion collisions [79, 80], this opacity may hide the underlying physics contained in the solution of the hydrodynamic PDEs. PINNs avoid this problem because embedding the governing equations in the loss function forces the network to generate solutions consistent with the physical model. In this sense, PINNs do not operate as black boxes that capture features of empirical data, but as models that output solutions that approximately satisfy the governing laws of some physical system. Despite these advantages, PINNs come with significant drawbacks. Their training can be computationally expen-sive, especially as the governing PDEs become more complex or involve multiple coupled fields, and their precision is generally lower than that of well-resolved finite volume simulations. Moreover, PINNs struggle to accurately ap-proximate solutions containing sharp gradients or discontinuities [81–83]. In this sense, PINNs offer universality and flexibility, while finite volume methods such as KT provide speed and accuracy for equations that can be cast in conservative form. In the context of physics, PINNs have been successfully applied to a wide range of systems including non-relativistic viscous fluid dynamics [63, 84], plasma physics [85], and relativistic ideal hydrodynamics [86, 87]. These methods not only often provide numerical accuracy comparable to classical solvers but also allow differentiability and smoothness properties advantageous for optimization and uncertainty quantification. Overall, PINNs bridge the gap between traditional numerical analysis and modern machine learning, enabling a new class of highly flexible solvers that respect fundamental physical principles. In this paper we employ PINNs for the first time to solve relativistic viscous hydrodynamic equations. We com-pare finite volume methods to PINNs by considering the case of BDNK diffusion [36–40]. In particular, we focus on relativistic BDNK diffusion in (1 + 1) dimensions. Several examples are considered: a simple Gaussian initial condition, a shock profile, and a Gaussian with non-trivial BDNK background constructed using the code presented 1

in [58]. Different scales of chemical potential (relative to temperature) are considered, which allows us to assess the hydrodynamic frame robustness of the simulations. The finite volume benchmark simulations are performed using standard techniques with the KT algorithm, while the neural network approach applies a new self-adaptive PINN introduced in this work, called SA-PINN-ACTO, which has exact enforcement of initial data and periodic boundary conditions. This handling of initial and boundary conditions via post-processing allows the neural network to focus solely on minimizing the residual of the partial differential equation, increasing the efficiency and accuracy of our simulations. This paper is organized as follows. In Sec. II, we discuss the theory of BDNK diffusion. In particular, in Sec. II A we present the equations that describe the diffusion of a conserved charge in BDNK hydrodynamics, in Sec. II B we    

> 1We do not solve the combined BDNK equations for energy-momentum and baryon number conservation. In this first work, we use the BDNK background of [58] in some of our simulations of diffusion, neglecting the backreaction to the energy-momentum sector.

4present the flux-conservative formulation of the equations, and in Sec. II C we state our choice of equation of state and transport parameters. Next, in Sec. III, we describe the numerical methods we employ: in Sec. III A we discuss our implementation of the Kurganov-Tadmor scheme, and in Sec. III B we present a detailed discussion of PINNs, first explaining what the vanilla PINN technique consists of, then introducing what we call the ACTO modification (implemented through two consecutive output transforms), and finally reviewing the SA-PINN technique [88]. To-gether, ACTO and SA-PINN define our new SA-PINN-ACTO framework. Then, in Sec. IV we present our numerical results for both the KT and SA-PINN-ACTO simulations. Finally, in Sec. V we state our conclusions and outlook. Further details about our simulations are presented in the appendices. 

> Notation

We use a mostly plus metric signature, ημν = diag( −1, 1, 1, 1), in (3 + 1)D Minkowski spacetime with Cartesian coordinates, and natural units ℏ = kB = c = 1.  

> II. BDNK THEORY FOR DIFFUSION

Diffusion describes the motion of conserved quantities within a fluid from regions of high concentration to low concentration. Such a system is typically described by a single charge density n, which is governed by a dynamical equation defined by the conservation of charge. For a relativistic system, we consider a single conserved current Jμ

which obeys 

∂μJμ = 0 . (3) In equilibrium, the only quantities are the density n, as well as the chemical potential μ, temperature T , and the background fluid velocity uμ. The density is related to the chemical potential and temperature by an equation of state n = n(μ, T ). The conserved current in equilibrium takes the form [1] 

Jμ 

> eq

= nu μ. (4) In this work, we consider a “probe approximation” where the temperature and fluid velocity are taken to be background fields, and backreactions between the diffusion and energy-momentum sector are neglected for simplicity. Dissipative terms can be included through a gradient expansion in terms of derivatives of the chemical potential divided by temperature. In BDNK theory, all possible terms are included up to first order in derivatives [36–38, 40]. Neglecting the variation of T and uμ in the probe limit results in the conserved current [89] 

Jμ = ( n + λT u ν ∂ν α) uμ − σT ∆μν ∂ν α, (5) where α = μ/T . In this construction, we have introduced two new parameters, λ and σ, which can both be functions of the temperature and density. By comparison to the nonrelativistic diffusion equation [7], we can identify σ as the charge conductivity. The parameter λ is a hydrodynamic frame parameter coefficient that appears in the relativistic case, which in a first-order theory can be removed by making a hydrodynamic frame transformation 2 of the form 

n → n − λT χ uμ∂μn. (6) where χ = ∂n/∂μ is the charge susceptibility. The fact that λ can be removed by such a transformation, at the level of precision of the first-order theory, does not mean that it is immaterial. In fact, it can be shown that the corresponding equation of motion is causal, hyperbolic, and stable if and only if [39, 89] 

λ, σ > 0, (7a) 1 > σλ > 0. (7b)  

> 2For an out-of-equilibrium system, the definition of the thermodynamic variables such as the temperature and density are not unique [90]. This means that such quantities can be freely redefined as long as such a redefinition recovers the same value in equilibrium. A definition of the relevant thermodynamic quantities is known as a hydrodynamic frame. More on hydrodynamic frames can be found in [37].

5When these inequalities are satisfied, the first-order theory defined by the constitutive relation (5) provides a theory for diffusion that preserves causality and is stable around equilibrium for all Lorentz observers [91]. The charge con-ductivity σ is a physical parameter in this first-order formulation that can be extracted from microscopic theories [3], while λ plays the role of a UV regulator that encodes our ignorance about high-order derivative effects. The regime of validity of the theory lies in the so-called frame robust regime, where solutions of the nonlinear equations of motion do not significantly depend on the choice of λ [54, 58].  

> A. Relativistic BDNK diffusion equation

We now consider the equation of motion for BDNK diffusion. Computing ∂μJμ = 0 in a Minkowski background, we find that 

∂μ(nu μ) + ∂μ (λT u μuν ∂ν α) − ∂μ (σT ∆μν ∂ν α) = 0 . (8) For a general observer in Minkowski spacetime, the fluid velocity can be decomposed in the form uμ = γ(1 ,⃗ v), where 

γ = (1 − v2)−1/2 is the Lorentz factor. In a general Lorentz frame with arbitrary background, one finds 

∂t(γn ) + ∂i(γnv i) + ∂t[γ2λT (∂tα + vi∂iα)] + ∂j [γ2vj λT (∂tα + vi∂iα)] 

− ∂t{σT [−∂tα + γ2(∂tα + vi∂iα)] } − ∂j {σT [∂j α + γ2vj (∂tα + vi∂iα)] } = 0 . (9) Note that T,⃗ v are not determined by the equations of motion, rather they are background fields that must be specified in our probe approximation.  

> B. Flux-conservative formulation

For numerical applications, it is useful to write the equations of motion in flux-conservative form, meaning that they take the form ∂tN + ∂iJ i = Φ, for some set of densities N , fluxes J i, and sources Φ. The fluxes and sources should be uniquely determined by the densities to truly have a flux-conservative structure. It might seem that the equation of motion as written already takes this form since it comes from a conservation law 

∂tJ0 + ∂iJi = 0 . (10) However, both the source Ji and the density J0 in (5) contain spatial and time derivatives. It is therefore convenient to perform an order-reduction by introducing the new field 

Nμ = −∂μα, (11) so that 

∂tα = −N0, (12) is in flux-conservative form. This equation along with the conservation law are still not closed because the flux Ji

depends on Ni, so we also need an equation for Ni. This can be obtained by taking a time derivative of Ni and using Eq. (11) to write 

∂tNi + ∂i∂tα = 0 → ∂tNi − ∂iN0 = 0 , (13) which is again in conservative form. We have thus converted the single BDNK diffusion equation to a first-order system in conservative form with densities J0, α, N i.To proceed, we would like to write both the fluxes and sources as functions of these densities. This can be done by writing the BDNK conserved current of Eq. (5) in terms of Nμ as 

Jμ = nu μ + ( −λT u μuν + σT ∆μν ) Nν . (14) Inverting this expression to find Nμ as a function of Jμ yields 

Nν =



− 1

λT uμuν + 1

σT ∆μν 



(Jμ − nu μ) . (15) 6Using these relations and the equations derived above, we can write the equations of motion in flux-conservative form 

∂t

J0

αNi

 + ∂j



Jj

0

−δji N0

 =

 0

−N0

0

 , (16) where the fluxes are determined by 3

N0 = −J0 + nγ + ( σ − λ)T γ 2viNi

σT + ( λ − σ)T γ 2 , (17) 

Ji = γnv i + σT N i + γ2T (−λ + σ)vivj Nj + γ2T (−λ + σ)viN0. (18) Substituting the form of N0, one may also write 

Ji = γnv i + σT N i + γ2T (−λ + σ)vivj Nj + γ2T (−λ + σ)vi

 −J0 + nγ + ( σ − λ)T γ 2vj Nj

σT + ( λ − σ)T γ 2



(19) so that it is clear that Ji = Ji(J0, α, N i) and N0 = N0(J0, α, N i) in the general case when vi̸ = 0. This fully specifies the equations of motion of BDNK diffusion in a form that can be simulated using standard approaches for flux-conservative equations. These equations are valid for arbitrary transport coefficients and background fields T,⃗ v,even if these background fields are functions of spacetime.  

> 1. (1 + 1) D dynamics

In the simple case in (1 + 1)D, where the background velocity vanishes, i.e., the local rest frame (LRF), and the temperature T = T0 is constant, one finds the simple equations 

∂t

 J0

αN x

 + ∂x

 Jx

0

−N0

 =

 0

−N0

0

 , (20) where the fluxes are determined by 

N0 = −J0 + nλT 0

, (21a) 

Jx = σT 0N x. (21b) We note that when introducing extra dynamical fields defined in terms of spatial derivatives, such as ⃗ N = −∇ α,constraints such as ∇ × ⃗ N = 0 do not appear in the flux-conservative dynamical equations. This demands greater care to ensure these constraints are propagated in the numerical evolution, see [58]. In this work, since we limit ourselves to (1+1)D simulations, this is not an issue.  

> C. Choice of equation of state and transport parameters

For simplicity, in this work we use as the equation of state (EOS) the case of a massless gas of Nf quarks and gluons with Nc colors where the pressure P is given by [92] 

P =



2( N 2 

> c

− 1) + 72 NcNf

 π2T 4

90 + NcNf

μ2T 2

54 + NcNf

μ4

972 π2 . (22) The energy density ε = 3 P and the baryon density is given by 

n =

 ∂P ∂μ 



> T

= NcNf T 3

 α

27 + α3

243 π2



(23)      

> 3Note that, in our metric signature, the spatial components satisfy Ni=Ni.

7where we used that α = μ/T . Note that we can write P and ε as functions of T and α. In this work, we use Nc = 3 and Nf = 3. For the conductivity, we use the expression employed in numerical studies of baryon diffusion in the quark-gluon plasma, see [93], where 

σ(T, α ) = CB nT 2

 13 coth ( α) − nT ε + P



. (24) This expression is derived from the relativistic Boltzmann equation in the relaxation-time approximation, and CB is a parameter that depends on the form of the relaxation time. We note that this definition is valid even for large values of the fugacity α. However, this expression can be further simplified and the form of CB can be determined for small 

α. In that limit, one finds that 

σ = 5CB

3Tnμ , (25) with 

CB = ηT ε + P , (26) for shear viscosity η. For this small α result, the parameter CB is approximately equal to the specific shear viscosity 

η/s , so we will set CB = 1 /4π for the simulation in the small fugacity limit (IV C), in accordance with [94]; for the higher-fugacity simulations (IV A, IV B), we use CB = 0 .4. In our BDNK simulation, we also need to define λ, which parameterizes the hydrodynamic frame. For simplicity, we may use here that c2ch = σ/λ is a constant (a parameter of our simulation) with cch ∈ (0 , 1) for causality and stability. Thus, in our setup, our simulation has as free parameters CB and cch , besides the background temperature and flow velocities.  

> III. NUMERICAL METHODS

For the sake of completeness, we briefly review the numerical tools used in this work. Neither the Kurganov-Tadmor scheme nor physics-informed neural networks are new methods in themselves; our contributions are limited to problem-specific choices and minor variations. 4  

> A. Kurganov-Tadmor scheme
> 1. Flux formulation and reconstruction

To solve our system of conservation equations (20) numerically, we use a central KT scheme that is second order in space (piecewise-linear reconstruction with a Total Variation Diminishing, TVD, limiter) and second order in time (SSP-RK2) [61]. The KT algorithm can be thought of as a particular choice of numerical flux and, unlike traditional upwind methods that rely on approximate Riemann solvers and characteristic decompositions, this scheme uses local speed estimates to compute numerical fluxes. This allows for high-resolution shock-capturing while maintaining a simpler, eigenstructure-free formulation. The semi-discrete form of the (1 + 1)D KT scheme is 

ddt qi = − Hi+1 /2 − Hi−1/2

∆x + S[qi], (27) where q is the conserved variable vector and is composed of cell-averaged solutions, S the source term, and H are the numerical fluxes incorporating local wave speeds. Second-order accuracy is achieved by using linear reconstructions and non-oscillatory limiters (e.g., minmod) to suppress spurious oscillations near discontinuities.   

> 4All our simulation codes are publicly available at https://github.com/vchomalicastro/1-1D-BDNK-diffusion-simulations/ .

8Each flux is computed as 

Hi±1/2 = a+ 

> i±1/2

F(q−

> i±1/2

) − a− 

> i±1/2

F(q+

> i±1/2

)

a+ 

> i±1/2

− a−

> i±1/2

+ a+

> i±1/2

a−

> i±1/2

a+ 

> i±1/2

− a−

> i±1/2

 q+ 

> i±1/2

− q−

> i±1/2

, (28) where q± 

> i±1/2

are the left/right reconstructed states at the interface. In our implementation, we use symmetric one-sided speeds 

a+ 

> i±1/2

= ci±1/2, a− 

> i±1/2

= − ci±1/2, (29) with ci±1/2 =

s σi±1/2

λi±1/2

, (30) which reduces the flux to the standard “single-a” form commonly encountered in the literature [62]: 

Hi±1/2 = 12

F(q−

> i±1/2

) + F(q+

> i±1/2

) − 12 ai±1/2

 q+ 

> i±1/2

− q−

> i±1/2

. (31) In (1 + 1)D, the left and right states at cell interfaces are reconstructed using a slope-limited linear interpolation: 

q+ 

> i+1 /2

= qi+1 − ∆x

2 (qx)i+1 ,

q− 

> i+1 /2

= qi + ∆x

2 (qx)i,

q+ 

> i−1/2

= qi − ∆x

2 (qx)i,

q− 

> i−1/2

= qi−1 + ∆x

2 (qx)i−1.

At each interface we evaluate the quantities T , v, σ, and λ by averaging their corresponding neighboring cell-centered values, and use these to compute F, N0, and c = pσ/λ at interfaces. Periodic boundary conditions are imposed by a simple index wrap-around ( q−1 ≡ qNx−1 and qNx ≡ q0). The slope ( qx)i is computed with the TVD minmod limiter applied to forward/backward differences: (qx)i = minmod 

 qi − qi−1

∆x , qi+1 − qi

∆x



, (32) minmod( a, b ) = 12

 sign( a) + sign( b) min( |a|, |b|). (33) All our KT simulations were executed on an Apple M1 Pro (ARM64, 8-core CPU, 16 GB memory) using a NumPy/Numba CPU implementation in double precision, with no GPU acceleration. 

2. Grid and periodic boundary conditions 

We discretize x ∈ [−L, L ] into N finite volume cells of width ∆ x = 2 L/N , so we store N + 1 edge points, with 

x0 = −L and xN = L. However, because we use periodic boundary conditions, x0 and xN represent the same physical location, so there are only N , and not N + 1, distinct interfaces, even though computationally we must create arrays that store N + 1 of them. Throughout the paper, all simulations are performed in grids with N , 2 N and 4 N spatial cells, with N = 1000 for simulations with continuous initial data and N = 2000 in the case of discontinuous initial data. 

3. Adaptive time step 

We use adaptive time step sizes in our KT implementation. At each time step, we compute the maximum local characteristic speed 

cmax (t) = max 

> x

s

σ(t)

λ(t) (34) 9and set ∆t = ∆x

8cmax (t) . (35) This time step size is held fixed throughout the two SSP-RK2 stages and recomputed for the next time step after updating T , v, α, and σ. In practice, however, since BDNK theory fixes some c2ch = σ/λ = const ∈ (0 , 1) parameter, we end up with a constant time step: ∆t = ∆x

8cch 

. (36)  

> 4. Convergence tests

We verify the convergence of our KT simulations using the Richardson extrapolation method [95, 96] applied to grid refinements. Convergence testing provides a systematic procedure for verifying that numerical solutions approach the continuum limit as the grid is refined. The central idea is that a consistent and stable discretization of order p should yield an error that decreases as a power of the grid spacing, |uN −u| ∼ (∆ x)p, where uN denotes the numerical solution obtained on a mesh with N spatial cells (and ∆ x = 2 L/N ) and u represents the continuum solution. In practice, since exact solutions are rarely available for nonlinear systems such as relativistic hydrodynamics, convergence is verified by comparing simulations performed at different resolutions and by measuring how the differences between them scale with grid refinement. A numerical method that exhibits the expected scaling is regarded as convergent within the tested regime. So, we compute 

Qn(t) = log 2

∥nN (t) − n2N (t)∥1

∥n2N (t) − n4N (t)∥1

, (37) where ∥ · ∥ 1 is an L1 norm, and the sub-indices N , 2 N and 4 N denote the runs with those numbers of spatial cells. For smooth solutions we expect Qn(t) → 2, whereas for solutions with discontinuities we expect Qn(t) → 1. The choice of the L1 norm is motivated by its robustness: it provides a global measure of the integrated error across the computational domain, it is less sensitive than the L∞ norm to isolated pointwise deviations, and it offers a clearer interpretation in the presence of shocks or sharp gradients than the L2 norm. The subtractions are performed on the base (shared, coarse) resolution corresponding to the run with N spatial cells: the higher-resolution solutions (2 N , 4 N ) are first restricted to the N -cell grid by cell-averaging before forming the differences. Snapshot times are the same across all runs, so no temporal interpolation is required. The L1 norm is computed as |f |1 = P 

> j

|fj | ∆x

with ∆ x = 2 L/N on the N -cell grid. All the results for convergence tests can be found in Appendix D.   

> B. Self-adaptive physics-informed neural network with algebraic enforcement of auxiliary conditions through transforms to the output (SA-PINN-ACTO)
> 1. Vanilla PINN

In this work, we implement a physics-informed neural network to tackle the forward (1 + 1)D BDNK diffusion problem. For clarity, we first discuss the standard (“vanilla”) formulation of PINNs. As discussed in Sec. I, PINNs can learn an approximate, mesh-free (i.e. continuous) solution to a given partial differential equation by attempting to enforce in its predictions the differential equations that govern the problem, as well as the chosen initial and boundary conditions. Let us consider a PDE in its most general form, 

F t, x, u, ∂ tu, . . . , ∂ kt u, ∇xu, . . .  = 0 , x ∈ Ω ⊂ Rd, t ∈ [0 , T ], (38) with initial and boundary conditions 

∂ jt u(t = 0 , x) = u(j)IC (x), x ∈ Ω, j = 0 , . . . , k − 1, (39) 

B[u] = 0 , x ∈ ∂Ω, t ∈ [0 , T ]. (40) Here, u(t, x) is the solution to the differential equation, F[·] is a differential operator, and B[·] is a boundary operator. 10 The PINN solution, called the prediction, here symbolized by the sub-index θ, is generated by training the neural network on minimizing a loss or residual L given by 

L ≡ L PDE + λIC LIC + λBC LBC , (41) where 

LPDE ≡ 1

|NPDE |

X

> i∈NPDE

F ti, x i, uθ , ∂ tuθ , . . . , ∂ kt uθ , ∇xuθ , . . .  2 , (42) 

LIC ≡ 1

|NIC |

X

> i∈NIC
> k−1

X

> j=0

∂ jt uθ (0 , x i) − u(j)IC (xi) 2

, (43) and, for example, for periodic boundary conditions, 

LBC ≡ 1

|NBC |

X

> i∈NBC

|uθ (ti, L ) − uθ (ti, −L)|2 . (44) Here, uθ (t, x ) is the network’s output (i.e., the prediction), and ( t, x ) ∈ [0 , T ] × [−L, L ] is the network’s input. Nu-merically, we compute the derivatives of uθ (t, x ) via automatic differentiation, which, unlike finite difference schemes, delivers exact gradients up to machine precision. λIC and λBC are simply weighting coefficients to adjust the magni-tude of the different loss terms according to their importance, numerically speaking. NPDE , NIC , and NBC are the sets of collocation points for each term, and | · | denotes their cardinalities. The collocation points are the points in spacetime on which the PINN is trained. The IC collocation points are placed at t = 0 at random xi, the BC collocation points are placed at x = ±L at random ti, and the PDE collocation points are placed at random xi

and ti, all throughout the domain. To ensure that these collocation points are a representative random sample of their respective domains, we use Latin Hypercube Sampling (LHS) [97], a random sampling method that distributes samples evenly over the sampling space. We choose different |NPDE | for the different simulations. From Eq. (20), and from our choice of periodic boundary conditions and arbitrary initial conditions for α and J0,it follows that, for the problem we are solving, we can define 

R2PDE ,i ≡ ∂tJ0 

> θ

+ ∂xJxθ 

> 2
> i

+ |∂tαθ + N0,θ |2 

> i

, (45) 

R2BC ,i ≡ | uθ (ti, L ) − uθ (ti, −L)|2 , (46) 

R2IC ,i ≡ | uθ (0 , x i) − uIC (xi)|2 , (47) where 

u =

J0

α



, (48) to then construct our PDE, BC, and IC losses, respectively, as 

LPDE ≡ 1

|NPDE |

X

> i∈NPDE

R2PDE ,i , (49) 

LBC ≡ 1

|NBC |

X

> i∈NBC

R2BC ,i , (50) 

LIC ≡ 1

|NIC |

X

> i∈NIC

R2IC ,i . (51) Note that for our (1 + 1)D BDNK diffusion problem, d = 1, so x ≡ x ∈ [−L, L ]. In practice, to implement our PINN, we utilize the open-source machine learning framework PyTorch [98]. For the architecture of our PINN, we use a feed-forward multilayer perceptron (see Appendix A). The input layer of our MLP has 2 neurons: one neuron outputs x and the other outputs t. Then, we use L = 10 hidden layers with N = 70 neurons each, with a σ(·) = tanh( ·) activation. At the end, we have an output layer with 2 neurons with a σ(·) = id( ·)(i.e., linear) activation; one neuron outputs J0 and the other outputs α. This is illustrated in Fig. 1. To train the PINN, on the other hand, we adopt a common two-stage optimization strategy: first, we perform Adam pre-training [99], which rapidly drives down the loss and places the network parameters in a good region for 11 J0            

> αx
> t
> input output hidden layers
> trainable parameters θ={(W, b )}L+1
> =1
> update θ
> to minimize L
> LAD ∇θL
> LBC
> LIC
> LPDE
> ∑
> iR2BC ,i
> ∑
> iR2IC ,i
> ∑
> iR2PDE ,i

FIG. 1. Vanilla PINN architecture and loss construction for the (1 + 1)D BDNK diffusion problem. The network takes spacetime inputs ( t, x ) and predicts ( J0, α ). It is fundamental that the quantities Jx and N0 are obtained from the constitutive relations from Sec. II B and not from the identities N0 = −∂tα and ∂xJx = −∂tJ0, since that would decouple the output variables α and J0 and incorrectly put them on independent grounds. Afterwards, automatic differentiation (AD) yields ∂tJ0,

∂tα, and ∂xJx. The residuals LIC , LBC and LPDE are calculated as shown in Eqs. (43)-(49), respectively, and the total loss 

L ≡ L PDE + λIC LIC + λBC LBC is minimized by computing ∇θ L and updating θ using gradient descent (Adam), followed by quasi-Newton refinement (L-BFGS). 

fine-tuning. For this stage of training, we initialize the Adam learning rate at 3 × 10 −3 and schedule it to reduce to 40% of its previous value whenever the loss plateaus for 700 Adam epochs (though we limit this decrease to 1 /100 of the initial learning rate). We use a different number of Adam pre-training epochs for each of the three simulations. Afterwards, once the loss has plateaued during Adam, we switch to the quasi-Newton limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm [100] which, using full-batch second-order information, fine-tunes all parameters to ensure a higher accuracy satisfaction of all three PDE, initial, and boundary condition residuals. For all our numerical simulations, we use up to 1 , 000 L-BFGS iterations, exiting either when the optimizer plateaus or when non-finite losses appear, which halts the L-BFGS procedure. Once training is finalized, we restore the best model. All our PINN simulations are run on an NVIDIA H200-SXM5-141GB GPU. 

2. PINN with algebraic enforcement of initial conditions 

A problem one might encounter when training a PINN, however, is that the initial condition might not be learned to the desired level of precision. This has been commonly reported in the literature [101, 102], and it raises a concern: even if the PDE and BC residuals are low, the solution might not be of interest if it is associated with predicted initial conditions that do not exactly match the ones the network was trained to replicate. This problem can be solved, nevertheless, by doing a hard or algebraic enforcement of the initial conditions [101, 103]. This is accomplished by applying a transformation to the raw output of the PINN (which so far we had treated as the final answer): letting ˆ uθ be the raw PINN output (which, in Sec. III B 1, we had called simply uθ ) and uIC the desired initial condition, we perform the transformation ˜uθ (t, x ) = uIC · e−βt + ˆ uθ (t, x ) · (1 − e−βt ). (52) where, together, e−βt and 1 − e−βt represent an arbitrary partition of unity, meaning that ˜ uθ is a nice blend of the IC and the network raw output. This aligns closely with the technique introduced by Ren et al. [103], as they also use two asymptotic functions of time that partition unity, though different ones. For our choice of functions, β > 0 is simply an adjustable parameter to control the partition dynamics. We use β = 1. Obviously, by construction, this transformation makes the transformed output of the network, ˜ uθ (t, x ), exactly equal to uIC at t = 0 and, therefore, 

LIC = 0 , (53) 12 from which it follows that the loss function is now simply 

L = LPDE + λBC LBC . (54)  

> 3. PINN with algebraic enforcement of boundary conditions

We employ one further technique to also algebraically enforce periodic boundary conditions, instead of imposing them as a soft, learned constraint. Here, we follow the idea of Hao et al. [101], though with substantially different methodology; instead of augmenting the input features to the network with a Fourier-like embedding, we simply apply yet another transformation to the result from Eq. (52): 

uθ (t, x ) = ˜ uθ (t, x ) − x + L

2L [˜ uθ (t, L ) − ˜uθ (t, −L)] . (55) This transformation guarantees that uθ (t, −L) = uθ (t, L ) for all t, without having to modify the network inputs or architecture. Therefore, 

LBC = 0 (56) by construction, meaning that now, 

L = LPDE . (57) That is, the PINN now only has to learn the solution to the PDE. Both auxiliary (initial and boundary) conditions are now exactly enforced via a transform to the output: we introduce this combination of techniques as the PINN-ACTO. Note that algebraic (also known as hard) enforcement of auxiliary conditions is not new: prior work enforces initial conditions using output transforms [101–103], and algebraically enforces either periodic boundary conditions by augmenting the inputs with Fourier features [101], or Dirichlet boundary conditions through an output transform [103]. In contrast, what our ACTO variant constitutes is an explicit output transform that enforces periodic boundary conditions, thus requiring no input embeddings or architectural changes, and, specifically, the combination of this periodic BC-enforcing output transform with the standard IC-enforcing output transform.  

> 4. Self-adaptive PINN and network-internal normalization

So far, we have introduced the vanilla PINN in Sec. III B 1 and two additional techniques in Secs. III B 2 and III B 3 that, together, give us the PINN-ACTO. Here, we take two further steps: we adopt the self-adaptive PINN (SA-PINN) technique proposed by McClenny et al. [88], and implement network-internal normalization of the magnitudes of the relevant fields ( α and J0), similar to the hierarchically normalized PINN (hnPINN) framework [104]; the former modifies the loss function of the PINN by introducing trainable and pointwise weights that make the PINN focus on points of spacetime where the solution is resulting to be harder to learn, while the latter is a simple though crucial numerical rescaling step that ensures that the network operates on well-conditioned numerical ranges, improving gradient conditioning and training stability. For the (1+1)D BDNK diffusion problem, we do this by redefining the first-order PDE residual at the ith collocation point, ( ti, x i), as 

R2PDE ,i ≡ ∂tJ0 

> θ

+ ∂xJxθ

> 2
> i

s2

> J0

+ |∂tαθ + N0,θ |2

> i

s2

> α

, (58) where sJ0 and sα (for network-internal normalization) are the magnitudes of the J0 and α fields, respectively, deter-mined by the maximum magnitude of their initial conditions ( sα = max x |α(t = 0 , x )| and sJ0 = max x |J0(t = 0 , x )|), to then redefine the PDE term of the loss function, LPDE , as 

LPDE ≡ 1

|NPDE |

X

> i∈NPDE

λ2 

> i

R2PDE ,i , (59) where each λi is the weighting factor for the PDE residual at the ith collocation point. 13 x              

> t
> input hidden layers
> trainable parameters θ={(W, b )}L+1
> =1
> J0
> α
> output raw output
> normalized
> ˆαsα
> /
> ˆJ0
> /sJ0
> ACTO
> transform
> and
> denormalization
> AD
> AD
> update θ
> to minimize L
> SA-PINN
> LPDE ≡L
> update all λi
> to maximize L
> ∇λiL
> ∇θL
> ∑
> iλ2
> iR2PDE ,i

FIG. 2. SA-PINN-ACTO architecture and loss construction for the (1 + 1)D BDNK diffusion problem. As in the vanilla PINN (see Fig. 1), the network takes spacetime inputs ( t, x ), but unlike it, the SA-PINN-ACTO predicts normalized raw outputs ˆJ0/s J0 and ˆ α/s α. These are first denormalized, then passed through the IC-enforcing transform in Eq. (52), and then through the BC-enforcing transform in Eq. (55) (together, the ACTO transform), yielding physical outputs ( J0, α ) that exactly satisfy the prescribed initial data and periodic boundary conditions. Automatic differentiation (AD) is then used to compute the normalized PDE residuals RPDE ,i defined in Eq. (58); these are weighted pointwise by the self-adaptive factors λ2 

> i

to build the PDE loss LPDE ≡ L in Eq. (59). The logits underlying λi = softplus( zi) are optimized jointly with the network parameters θ:AD provides ∇θ L to update θ so as to minimize L and ∇λi L to adapt the weights, focusing learning on collocation points with larger residuals. 

This differs from the original (vanilla) PINN technique in two senses: in the first place, in the vanilla PINN, the λ

weights are simply scalars that multiply the different loss terms (see Eq. (41)) to assign different numerical importance to each one of them; secondly, and more importantly, in the SA-PINN framework, the pointwise weights λi are not manually assigned; rather, they are learned by the PINN jointly with the set of network parameters θ. To keep the weights λi non-negative and numerically stable, we represent them by unconstrained parameters (logits) that get passed through a smooth, positive mapping. In this work we choose the softplus function [105], 

λi = softplus( zi) = ln(1 + ezi ) , (60) where zi is the logit associated with the ith collocation point. During training, the logits {zi}i∈NPDE are optimized jointly with the network parameters θ, using the same gradient-based procedure. The effect of this construction is that collocation points with larger residuals naturally receive larger adaptive weights, leading the neural network to give more attention to regions of spacetime where the solution is harder to learn. This self-adaptive mechanism has been shown to accelerate convergence and improve the accuracy of PINNs. As for network-internal normalization, we also multiply the network raw outputs for J0 and α by sJ0 and sα,respectively, so that, in some sense, we are asking the network to predict J0 and α not in their physical units, but in a dimensionless, normalized form, of order ∼ 1. These normalized predictions are then rescaled back to their physical units before the computation of spatial and temporal derivatives, and thus before evaluating the PDE residuals. Therefore, we say that the network raw outputs are ˆJ0/s J0 and ˆ α/s α, and the processed outputs (denormalized and ACTO-transformed) are J0 and α. This ensures that the network operates on well-conditioned numerical ranges while all physical laws remain expressed in GeV units. Although this normalization is conceptually independent of our ACTO technique, we find it practically indispensable for achieving robust and stable training across test cases involving widely separated physical scales. All our numerical simulations are thus run on our modified SA-PINN, which we call the SA-PINN-ACTO. We illustrate it in Fig. 2. Finally, we augment the set of pseudo-randomly collocated PDE points, NPDE , by appending 500 evenly spaced points along the slice t = 0 ( x ∈ [−L, L ]). We do this because our PINN ultimately solves a second-order PDE for 

α and, thus, beyond exactly reproducing the initial values of α and J0 through our ACTO technique, we also place special emphasis on correctly capturing the second-order initial conditions of α by accurately learning the solution to the coupled ODE system for α and J0 at t = 0. 14  

> IV. NUMERICAL RESULTS FOR BDNK DIFFUSION

In this section, we numerically solve three setups (IV A, IV B, IV C) using the SA-PINN-ACTO and benchmark the results against a high-resolution KT solver validated through convergence tests (see Appendix D). All runs use periodic boundaries on [ −L, L ] with L = 50 GeV −1 and evolve up to a setup-specific tend . For each of the three setups, we vary the maximum characteristic speed, cch ∈ { 0.5, 0.9}, which sets the propagation rate of structures; there are thus six tests in total. We track total charge conservation R L 

> −L

J0 dx , verify grid-refinement convergence for KT, and, for the SA-PINN-ACTO, monitor PDE residual decay during training. Moreover, as a validation metric, we report for each one of the six tests the relative L2 error between the KT and SA-PINN-ACTO solutions over the full spacetime domain [0 , t end ] × [−L, L ], 

Erel [ϕ] = ∥ϕPINN − ϕKT ∥2

∥ϕKT ∥2

, (61) where ϕ ∈ { n, J 0} and ∥ · ∥ 2 denotes the spacetime L2 norm 

∥ϕ∥22 =

Z tend 

> 0

Z L

> −L

|ϕ(t, x )|2 dx dt. (62) These errors are reported in Table I.  

> A. First setup: Gaussian initial condition

First, we simulate a Gaussian initial condition for both n and J0, described by 



n(t = 0 , x ) = 



0.2e−( 7xL )2

+ 1 



GeV 3,J0(t = 0 , x ) = 



0.05 e−( 10 xL )2

+ 1 .05 



GeV 3. (63) That is, n(t = 0 , x ) is a Gaussian of amplitude 0 .2 GeV 3 and standard deviation L/ (7 √2) standing on a pedestal of 1 GeV 3, and J0(t = 0 , x ) is a Gaussian of amplitude 0 .05 GeV 3 and standard deviation L/ (10 √2) standing on a pedestal of 1 .05 GeV 3. We perform this simulation first for cch = 0 .5, and then for cch = 0 .9. In this simulation, both the field temperature T and field velocity ⃗v are fixed: T = 0 .3 GeV and ⃗v = 0. The simulations were run up to 

t = tend = 20 GeV −1. Here, CB = 0 .4. In Fig. 3 we present the evolution of n and J0 for this setup obtained with the KT finite volume method for two characteristic speeds, cch = 0 .5 and 0 .9. The evolution of n, shown in subplots (a) and (b), is symmetric and retains its Gaussian character, with the minimum density rising in time and the extrema shrinking as the wave structure dissipates. This behavior is evident from both the time snapshots and the heatmap, where the color contrast decreases with time. In contrast, the evolution of J0, shown in subplots (c) and (d), exhibits a rapid restructuring of the initial Gaussian profile into a configuration with a central minimum near x = 0 and two symmetric local maxima. This new structure subsequently dissipates in time. Simulations with the larger characteristic speed yield smaller gradients in the evolution of both n and J0, as well as faster dissipation of the wave structure, as evidenced by the broader cone in the heatmaps for subplots (b) and (d). This difference is expected for large values of α (α ≳ 1) because at those scales, gradients are large and we are likely outside the regime of applicability of the hydrodynamic theory. Looking forward to Fig. 7, we can see that in the small α regime the value of cch does not affect the numerical results, which indicates that in that case we are in the frame robust regime where changes between causal hydrodynamic frames do not significantly change the numerical simulations [54, 55, 58]. In Fig. 4 we investigate the same setup, but instead of using the KT finite volume algorithm, we employed the SA-PINN-ACTO proposed in Sec. III. Regarding the evolution described previously, the PINN-based approach is very similar and shares many of the qualitative conclusions we have drawn before. Both n and J0 dissipate with time and the evolution for cch = 0 .9 has smaller gradients throughout. The evolution of both n and J0 is symmetric as before, where we have the left- and right-moving waves damp and spread. The cch = 0 .9 case shows smaller extrema in the evolution of n and J0, consistent with the behavior observed in the KT simulations. This difference is harder to see in the heatmap, but it is much clearer in the snapshot evolution. The dynamics with the larger characteristic speed are more damped and we posit that this difference arises from hydrodynamic frame dependence dominating at larger 

α. The field gradients propagate outward faster and dampen more rapidly in the cch = 0 .9 subplots, see Fig. 4 (b) 15 

(a) (b) 

(c) (d) FIG. 3. Results from KT for the first setup. (a) Evolution of n for cch = 0 .5. (b) Evolution of n for cch = 0 .9. (c) Evolution of J0 for cch = 0 .5. Total charge R L 

> −L

J0 dx conserved at all times up to a fraction of 4 .4 × 10 −15 of the initial charge. (d) Evolution of J0 for cch = 0 .9. Total charge R L 

> −L

J0 dx conserved at all times up to a fraction of 4 .9 × 10 −15 of the initial charge. 

and (d). It is therefore reasonable to conclude that the presence of larger characteristics can more rapidly damp the evolution, possibly even preventing discontinuous shock formation in a relativistic viscous theory. See Sec. IV B for our investigation into near-discontinuous initial data. Overall, the PINN demonstrates strong numerical performance, with residuals of order 10 −10 . The loss functions reach their minimum in the L-BFGS stage, with the cch = 0 .9 simulation achieving a slightly better (smaller) loss than the cch = 0 .5 one. Moreover, as shown in Table I, the relative L2 error between the SA-PINN-ACTO and KT solutions is of order 10 −3 for both n and J0.

B. Second setup: Smooth shock profile 

We also simulate a smooth shock-like initial condition described by 

(

n(t = 0 , x ) = 

h

1.1 − 0.1 tanh 



s

h  4xL

2 − 1

ii 

GeV 3,J0(t = 0 , x ) = 1 .05 GeV 3. (64) 16 

(a) (b) 

(c) (d) 

(e) (f) FIG. 4. Results from the SA-PINN-ACTO for the first setup. For these simulations, we use |NPDE | = 20 , 000 + 500, and 25 , 000 Adam epochs, followed by L-BFGS fine-tuning. (a) Evolution of n for cch = 0 .5. (b) Evolution of n for cch = 0 .9. (c) Evolution of J0 for cch = 0 .5. Total charge R L 

> −L

J0 dx conserved at all times up to a fraction of 1 .1 × 10 −4 of the initial charge. (d) Evolution of J0 for cch = 0 .9. Total charge R L 

> −L

J0 dx conserved at all times up to a fraction of 8 .6 × 10 −5 of the initial charge. (e) Total loss history for cch = 0 .5. Best loss was 7 .752 × 10 −10 . Total training time: 434 .64 seconds (Adam: 411 .78 seconds; L-BFGS: 22 .86 seconds). (f) Total loss history for cch = 0 .9. Best loss was 1 .862 × 10 −10 . Total training time: 429 .12 seconds (Adam: 406 .95 seconds; L-BFGS: 22 .17 seconds). 

This choice yields n(t=0 , x ) ∈ C∞(R) and, as s increases, n approaches a piecewise constant profile with n = 1 .2 GeV 3

for |x| < L/ 4 and n = 1 .0 GeV 3 elsewhere. Above, the initial data for J0(t = 0 , x ) is a pedestal of 1 .05 GeV 3. Here, we use s = 60, since with this value we simultaneously achieve both numerical convergence and sufficient detail in 17                                     

> (a) (b)
> (c) (d) FIG. 5. Results from KT for the second setup. (a) Evolution of nfor cch = 0 .5. (b) Evolution of nfor cch = 0 .9. (c) Evolution of J0for cch = 0 .5. Total charge RL
> −LJ0dx conserved at all times up to a fraction of 4 .1×10 −16 of the initial charge. (d) Evolution of J0for cch = 0 .9. Total charge RL
> −LJ0dx conserved at all times up to a fraction of 4 .1×10 −16 of the initial charge.

the simulations. We perform our simulations first for cch = 0 .5, and then for cch = 0 .9. For both cases, the field temperature T and field velocity ⃗v are fixed: T = 0 .3 GeV and ⃗v = 0. The simulations run up to t = t end = 15 GeV −1.Here, CB = 0 .4. Our results obtained using KT can be found in Fig. 5. In the case where cch = 0 .5 (subplot (a)), the evolution propagates more slowly in x. This is reflected in both the snapshots and the heatmaps, where the broadening of the discontinuous features persists and remains closer to x = 0 for longer than in the larger characteristic speed counterpart, in which the wave fronts travel further in x in the same amount of time. The discontinuities in this initial condition are preserved throughout the evolution. This is also evidenced in J0, where the discontinuous spikes come from the relationship to n, and they shrink but do not smoothen out in time. Once again, we see smaller overall gradients for the case of cch = 0 .9, subplots (b) and (d). While there are no conclusive data for shock formation in any of the investigated cases, the prerequisite of wavefronts accumulating near some discontinuity is precluded more effectively by larger characteristic speeds, as evidenced by the faster dissipation of the shock-like structure in the 

cch = 0 .9 case. Unlike the KT results, the SA-PINN-ACTO simulation does not preserve the discontinuous features throughout the evolution. Instead, Fig. 6 shows a clear damping and smoothing of structure for both n and J0. This result is 18 

(a) (b) 

(c) (d) 

(e) (f) FIG. 6. Results from the SA-PINN-ACTO for the second setup. For these simulations, we use |NPDE | = 50 , 000 + 500, and 35 , 000 Adam epochs, followed by L-BFGS fine-tuning. (a) Evolution of n for cch = 0 .5. (b) Evolution of n for cch = 0 .9. (c) Evolution of J0 for cch = 0 .5. Total charge R L 

> −L

J0 dx conserved at all times up to a fraction of 6 .5 × 10 −4 of the initial charge. (d) Evolution of J0 for cch = 0 .9. Total charge R L 

> −L

J0 dx conserved at all times up to a fraction of 1 .6 × 10 −4 of the initial charge. (e) Total loss history for cch = 0 .5. Best loss was 1 .272 × 10 −7. Total training time: 1039 .00 seconds (Adam: 1003 .95 seconds; L-BFGS: 35 .05 seconds). (f) Total loss history for cch = 0 .9. Best loss was 2 .234 × 10 −8. Total training time: 1030 .27 seconds (Adam: 995 .50 seconds; L-BFGS: 34 .77 seconds). 

most notable in the cch = 0 .9 simulation (subplots (b) and (d)). Consistent with this smoothing, Table I shows larger errors for J0 in this setup, of order 10 −2–10 −1. The loss function steadily decreases throughout the Adam stage and decreases further during the L-BFGS stage. Loss minima are of order 10 −7 and 10 −8 for cch = 0 .5 and cch = 0 .9, 19 

(a) (b) 

(c) (d) FIG. 7. Results from KT for the third setup. (a) Evolution of n for cch = 0 .5. (b) Evolution of n for cch = 0 .9. (c) Evolution of J0 for cch = 0 .5. Total charge R L 

> −L

J0 dx conserved at all times up to a fraction of 9 .3 × 10 −16 of the initial charge. (d) Evolution of J0 for cch = 0 .9. Total charge R L 

> −L

J0 dx conserved at all times up to a fraction of 5 .6 × 10 −15 of the initial charge. 

respectively. 

C. Third setup: Gaussian initial condition on BDNK background 

Lastly, we simulate a Gaussian initial condition for n and a simple pedestal for J0, described by 

(

n(t = 0 , x ) = 

h

0.2e−( 7xL )2

+ 1 

i

× 10 −3 GeV 3

J0(t = 0 , x ) = 1 .05 × 10 −3 GeV 3. (65) That is, n(t = 0 , x ) is a Gaussian of amplitude 0 .2 × 10 −3 GeV 3 and standard deviation L/ (7 √2) standing on a pedestal of 10 −3 GeV 3, and J0(t = 0 , x ) is a pedestal of 1 .05 × 10 −3 GeV 3. We perform this simulation first for 

cch = 0 .5, and then for cch = 0 .9. In this setup, temperature and velocity are not constant; rather, we use T and ⃗v profiles that result from conformal BDNK background simulations performed in [58]; these profiles are shown in Fig. 10. The simulations run up to t = tend = 20 GeV −1. Here, CB = 1 /4π.20 

(a) (b) 

(c) (d) 

(e) (f) FIG. 8. Results from the SA-PINN-ACTO for the third setup. For these simulations, we use |NPDE | = 20 , 000 + 500, and 25 , 000 Adam epochs, followed by L-BFGS fine-tuning. (a) Evolution of n for cch = 0 .5. (b) Evolution of n for cch = 0 .9. (c) Evolution of J0 for cch = 0 .5. Total charge R L 

> −L

J0 dx conserved at all times up to a fraction of 4 .9 × 10 −4 of the initial charge. (d) Evolution of J0 for cch = 0 .9. Total charge R L 

> −L

J0 dx conserved at all times up to a fraction of 1 .6 × 10 −3 of the initial charge. (e) Total loss history for cch = 0 .5. Best loss was 1 .728 × 10 −7. Total training time: 844 .62 seconds (Adam: 806 .14 seconds; L-BFGS: 38 .48 seconds). (f) Total loss history for cch = 0 .9. Best loss was 1 .517 × 10 −6. Total training time: 844 .27 seconds (Adam: 805 .74 seconds; L-BFGS: 38 .53 seconds). 

The initial conditions provide a difference in the evolution that is shared across both the KT solutions, Fig. 7 and the SA-PINN-ACTO solutions, Fig. 8. Both simulations report a left-moving wave in both n and J0 that is smaller in magnitude than the right-moving wave counterpart which propagates with a larger maximum height. These two 21                                                               

> Background Setup cch Erel [n]Erel [J0]Trivial (T= 0 .3 GeV, ⃗v= 0)
> nGaussian, J0Gaussian (IV A) 0.51.794 ×10 −33.449 ×10 −3
> 0.92.013 ×10 −32.560 ×10 −3
> nshock, J0pedestal (IV B) 0.57.096 ×10 −31.401 ×10 −1
> 0.97.259 ×10 −35.897 ×10 −2
> BDNK (see Fig. 10)
> nGaussian, J0pedestal (IV C) 0.52.246 ×10 −32.299 ×10 −3
> 0.98.283 ×10 −38.638 ×10 −3
> TABLE I. Relative L2error Erel between SA-PINN-ACTO and Kurganov-Tadmor results (see Eq. (61)), over the entire spacetime domain ( t, x )∈[0 , t end ]×[−L, L ], for both nand J0.

waves have maxima slightly above 1 .5 × 10 −3 GeV 3, and are separated by a shared minimum centered near x = 0 whose magnitude drops slightly below 0 .5 × 10 −3 GeV 3. In contrast to the first setup (IV A), we no longer see a stark difference in the extrema of the evolution between cch = 0 .5, 0.9. There are still subtle differences in the evolution depending on the characteristic speed, with sharper features appearing in the right-moving waves for the smaller 

cch = 0 .5 case, but this effect is less pronounced than in the first setup. Additionally, we would like to note that the minimum loss achieved in the cch = 0 .5 case is smaller than the cch = 0 .9 case, as seen in Fig. 8 caption. The agreement between the two numerical methods, KT and SA-PINN-ACTO, gives credence to the reliability of the predicted PDE solutions. That two fundamentally different numerical approaches yield such closely matching evolutions, especially considering that the PINN is informed solely through the PDE residual in the loss function and has no additional structural constraints on the dynamics, is a significant result. This is supported by Table I, which shows that the relative L2 errors between these methods are of order 10 −3 for the first (IV A) and third (IV C) setups.  

> V. CONCLUSIONS

In this work, we have reformulated the relativistic BDNK diffusion equation in flux-conservative form and solved the resulting equations in (1 + 1)D using two independent numerical methods: a second-order KT finite-volume solver and a physics-informed neural network (PINN). On the finite volume side, we implemented an SSP-RK2 Kurganov-Tadmor scheme and verified its convergence for both smooth and discontinuous initial data. On the machine learning side, we introduced the SA-PINN-ACTO method, which combines the self-adaptive collocation weights technique of the SA-PINN [88] with an algebraic (also known as hard) enforcement of initial and periodic boundary conditions via only an output transform, which we call the ACTO transform. This guarantees that those conditions are exactly satisfied without requiring any modification to the network architecture, while allowing the network to focus its attention on the regions of the domain where it is harder to properly approximate the solution to a given PDE, as well as having to minimize only the PDE residual. Across the set of smooth test problems, the SA-PINN-ACTO produced solutions that closely match the convergent KT solutions, with spacetime relative L2 errors of order 10 −3. The KT solver also displayed the expected second-order behavior in the smooth tests. For the shock initial condition, however, while the KT method correctly preserved the discontinuities, the PINN smoothed them out and reached error levels in the 10 −2–10 −1 range, an expected behavior due to the intrinsic smoothness of neural-network representations and well-established challenges PINNs face near sharp gradients [81–83]. We remark that the PINN remained much slower than the KT method in all cases. For the tests involving dynamical BDNK backgrounds, the PINN again achieved relative errors of order 10 −3 and successfully reproduced the asymmetric propagation induced by the background fields. In the large gradient regime, both the PINN and KT method exhibit hydrodynamic frame dependent behavior, as can be seen in Fig. 3. Despite this expected frame dependence, the residuals remain small in each frame. We remark that the PINN solutions preserve, for example, total charge across all simulations and symmetry in simulations where the initial data and backgrounds are symmetric. These properties were not imposed as constraints in the learning process; rather, they are natural consequences of the governing laws of the system, in this case the BDNK diffusion equation. This illustrates a central point of physics-informed machine learning: physical structure can arise from the equations themselves, even without being imposed through predefined constraints on the model. We found that the KT method remains significantly faster and more accurate, especially for discontinuous solutions, and it continues to be the appropriate tool when high-precision shock capture is required. The PINN approach, on the other hand, offers greater flexibility: it does not require rewriting the equations in flux-conservative form, provides a differentiable continuous solution representation, and effortlessly extends to inverse problems, irregular geometries, a large variety of types of PDEs, and even the inclusion of available empirical data during training. Furthermore, 22 different from KT which is defined for a set of first-order PDEs, our PINN technique can also equally handle sets of second-order PDEs. This is discussed in detail in Appendix B where we compare PINN results for BDNK diffusion using the first-order flux-conservative formulation developed here and also the standard formulation of this theory in terms of second-order PDEs. These complementary strengths highlight the distinct roles of traditional solvers and physics-informed networks. Future work should focus on accelerating the training process of PINNs, and importantly, improving their perfor-mance near discontinuities, potentially through hybrid finite volume-machine learning schemes that can take the best from each framework. We emphasize that the methods developed here can also be extended to higher-dimensional BDNK systems, or to inference of transport coefficients from partial data. In this context, it would be very inter-esting to implement the prescription for BDNK initial data recently discussed in [106], to better gauge the domain of applicability of BDNK hydrodynamics and its hydrodynamic-frame robustness in numerical simulations. Overall, the close agreement between the two fundamentally different approaches indicates that the SA-PINN-ACTO frame-work introduced in this paper provides a viable and flexible tool for solving PDEs and motivates further exploration of physics-informed machine learning in relativistic viscous hydrodynamics, with potential applications in heavy-ion collisions and astrophysics. 

> ACKNOWLEDGMENTS

We thank Thomas Woehrle for helpful discussions on neural networks. V. C.-C. acknowledges the use of the Illinois Campus Cluster, a computing resource that is operated by the Illinois Campus Cluster Program (ICCP) in conjunction with the National Center for Supercomputing Applications (NCSA) and which is supported by funds from the University of Illinois Urbana-Champaign. V. C.-C. also acknowledges the Lorella M. Jones Undergraduate Research Scholarship and the Department of Physics, University of Illinois Urbana-Champaign. N.M. was partially supported by U.S. Department of Energy, Office of Nuclear Physics, Contract DE-FG02-03ER41260. J.N. and N.C. are partly supported by the U.S. Department of Energy, Office of Science, Office of Nuclear Physics under Award No. DE-SC0023861. 

> Appendix A: Multilayer perceptrons (MLPs)

An MLP is a type of feedforward neural network that, mathematically, behaves as a parametric map fθ : Rdin →

Rdout given by 

fθ (z) = σL+1 (WL+1 σL(· · · σ1(W1z + b1) · · · ) + bL+1 ) , (A1) obtained by composing affine transforms with fixed elementwise nonlinearities, with a set of learnable parameters 

θ = {(Wℓ, b ℓ)}L+1  

> ℓ=1

, where Wℓ ∈ Rdℓ×dℓ−1 are weight matrices, bℓ ∈ Rdℓ (ℓ = 1 , . . . , L +1) are bias vectors, and {dℓ}L+1 

> ℓ=0

are the layer widths (i.e., the numbers of neurons in each layer), with d0 = din and dL+1 = dout . Each σℓ acts elementwise for 1 ≤ ℓ ≤ L + 1. Information flows strictly layer by layer in a forward, fully connected manner; there are no recurrences or convolu-tions. In our work, the input is z = ( t, x ) (the (1 + 1)D spacetime coordinate xμ), and the output is the prediction vector 

uθ (t, x ) := fθ (z) =  J0 

> θ

(t, x ), α θ (t, x ) . (A2) Hidden layers (that is, the layers between the input and output layers) use a smooth nonlinearity. Here we use 

σ(·) = tanh( ·). Therefore, each neuron in the hidden layers takes the full vector of outputs from the previous layer, takes the inner product of that vector with the neuron’s vector of learned weights, adds its learned bias, and finally applies the tanh transformation. This process results in the neuron’s output. This output, along with all the outputs of all the neurons in the same layer, is sent to the next layer, and the process is repeated until the last layer is reached. This is reflected by Eq. (A1). Without such nonlinear transformations, MLPs would not work, as the final output of the network would simply be a first-degree polynomial in the inputs t and x. For the output layer, we use σ(·) = id( ·); in a vanilla PINN, it is not in our interest to apply a transformation to the output of the neural network. Training does not result in a modification of the architecture of the MLP; it simply updates the elements of θ to minimize the physics-informed loss defined in Eq. (41). Training a PINN is therefore an optimization problem on the set of trainable parameters θ. Because Adam optimization is gradient-based, it requires ∂L/∂θ . We obtain these gradients via automatic differentiation (AD). 23 

(a) (b) FIG. 9. Results from the SA-PINN-ACTO for the first setup, with cch = 0 .5. For these simulations, we use |NPDE | =20 , 000 + 500, |NIC | = 1 , 000, and 50 , 000 Adam epochs, followed by L-BFGS fine-tuning. For this simulation, we obtain a spacetime relative L2 error of 2 .712 × 10 −3 for n and 5 .485 × 10 −3 for J0. (a) Evolution of n for cch = 0 .5. (b) Evolution of J0

for cch = 0 .5. Total charge R L 

> −L

J0 dx conserved at all times up to a fraction of 1 .6 × 10 −3 of the initial charge. Best loss was 3.055 × 10 −9 (PDE loss: 2 .345 × 10 −9; IC loss: 7 .109 × 10 −10 ). Total training time: 1328 .42 seconds (Adam: 1216 .03 seconds; L-BFGS: 112 .39 seconds). 

Appendix B: First- vs. second-order formulation of the BDNK diffusion problem for the SA-PINN-ACTO 

Instead of solving the flux-conservative formulation (20), the SA-PINN-ACTO (or, generally speaking, any PINN) can directly solve the BDNK diffusion problem in its second-order formulation, given by the second-order partial differential equation of motion (8), by choosing a PDE residual 

LPDE ≡ 1

|NPDE |

X

> i∈NPDE

λ2 

> i

R22nd ,i , (B1) where 

R2nd ,i ≡ [∂μ(nu μ) + ∂μ(λT u μuν ∂ν α) − ∂μ(σT ∆μν ∂ν α)] i /s α, (B2) and building the network with only the raw output ˆ α/s α, which would then get denormalized and ACTO-transformed into the final output α. For the (1 + 1)D BDNK diffusion problem in the rest frame, for example, this residual would be simply 

R2nd ,i ≡ [∂tn + ∂t(λT ∂ tα) − ∂x(σT ∂ xα)] i /s α. (B3) Once the PINN learns an α that approximately satisfies this equation, J0 can be simply reconstructed from the constitutive relation in Eq. (14). However, note that the ACTO transform enforces only α(t = 0), but not ∂tα(t = 0) (see Sec. III B 2), which means that the network would have the task of learning the second-order initial condition for α. Therefore, we would have to add the loss 

LIC = 1

|NIC |

X

> i∈NIC

|∂tαθ (0 , x i) − ∂tαIC (xi)|2, (B4) and train the network on minimizing 

L = LPDE + LIC . (B5) Solving the problem this way has the strong advantage that it becomes unnecessary to derive a flux-conservative formulation of the original second-order PDE, which is typically needed for finite-volume schemes such as Kurganov-Tadmor. However, the second-order formulation requires, obviously, computing second-order derivatives via automatic 24  

> (a) (b) FIG. 10. (a) Temperature profile for the third setup. (b) Velocity profile for the third setup.

differentiation (AD). This increases compute time and typically slows down training, which, in the end, makes the PINN slower. Moreover, the PINN would now have to learn a solution that satisfies certain relations between the second-order derivatives of the output (i.e., a solution that satisfies a second-order PDE), which naturally makes the learning task more complex. For these reasons, in this work we opted to solve the BDNK problem in its first-order (flux-conservative) form, but results of comparable accuracy can be achieved with a second-order PINN formulation, although with meaningfully longer training: for example, using |NPDE | = 20 , 000 + 500 and |NIC | = 1 , 000 collocation points, 50 , 000 Adam pre-training epochs, up to 2 , 000 L-BFGS iterations, and an initial learning rate of 1 × 10 −3,scheduled to decrease to 40% of its previous value every time the loss plateaus for 2 , 000 Adam epochs, we obtain, for the first setup, with cch = 0 .5, the results shown in Fig. 9. However, note that even when only a second- or higher-order PDE formulation is available and no flux-conservative form is known, first-order physics-informed neural networks (FO-PINNs) [107] can achieve higher accuracy, more stable training, and faster convergence; an FO-PINN would most likely outperform the “naive” second-order SA-PINN-ACTO implemented in this appendix. 

> Appendix C: Summary of background BDNK solution

The temperature and linear velocity profiles T ,⃗v used in the third setup, shown in Fig. 10, were generated from the numerical simulation of conformal (1 + 1)D BDNK hydrodynamics in Cartesian coordinates ( t, x ), at zero chemical potential, developed in [58]. This section serves as a high-level overview of how the corresponding equations of motion were solved. In [58], the BDNK equations for energy-momentum conservation ∂μT μν = 0 were recast in first-order flux-conservative form by defining some key new variables. In fact, in addition to studying the evolution of the stress tensor elements, we define a new field parallel to the flow Cμ = T u μ and the corresponding derivative Xμν = ∂μCν

used in the first-order form of the PDEs. The result is the energy-momentum tensor being recast into the ideal part plus viscous corrections in the following way, 

T μν = T μν  

> 0

+ 1

T 2 Hμναρ  δλρ + 2 uρuλ Xαλ − 1

T 2 Hμναρ Γλαρ Cλ. (C1) Further details about the validity of this approach are covered in [58]. The resulting equations of motion are 

∂0



T 0ν

C0

Ci

Xij 

Xi0

 + ∂k



T kν 

00i

−δki X0j

−δki X00 

 =



−Γμμλ T λν − Γνμλ T μλ 

X00 

X0i

0ij 

0i

 , (C2) 25 which allows for a clear conservation law form in (1 + 1)D that is favorable for finite volume methods, 

∂tq + ∂xFx = S. (C3) Alongside the aforementioned KT method discussed previously in Sec. III A, we employ the standard second-order Total Variation Diminishing Runge–Kutta scheme (TVDRK2) [108]. The time integration proceeds from step l to 

l + 1 with intermediate values denoted by the superscript (1): 

q(1) = ql − ht Z(ql), (C4) 

ql+1 = q(2) = 12 ql + 12 q(1) − 12 ht Z(q(1) ). (C5) The spatial residual is defined as 

Z(q) = ∂xFx(q) − S(q), (C6) and the Runge–Kutta coefficients are 

αij = {α10 = 1 , α 20 = 1 /2, α 21 = 1 /2}, βij = {β10 = 1 , β 20 = 0 , β 21 = 1 }. (C7) The CFL condition [109] imposes the constraint 0 < c CFL ≤ 0.5, (C8) which relates the temporal and spatial resolutions through 

ht ≤ cCFL hx. (C9) The specific details of the initial conditions for all of these fields, to then predict the evolution of T, v that was used in this work correspond to a system with vanishing A (the BDNK out-of-equilibrium correction to the energy density) and Qμ (the BDNK energy diffusion vector), but nonzero shear stress tensor πμν ̸ = 0. Hence, the only dissipative contribution to the initial energy–momentum tensor T μν arises from the shear tensor σμν . This setup ensures that the BDNK evolution begins close to an equilibrium state, consistent with the validity regime of the first-order formulation. Following the procedure detailed in [36], the general (3 + 1) expressions for the initial derivatives of flow velocity and energy density to properly initialize all the BDNK fields are as follows: 

∂0ε t=0 = 4εγ 

3 + 2( u)2

 ulum∂lum

1 + ( u)2 − ∂lul − ul∂lε

2ε



, (C10) 

γ ∂ 0uj t=0 =



γ2∂lul − ulum∂lum − ul∂lε

4ε

 uj

3 + 2( u)2 − ul∂luj − ∂j ε

4ε , (C11) where ( u)2 = uiui. The shear viscosity used in this simulation is such that η/s = 1 /4π, and the temperature and velocity profiles used as background fields are shown in Sec. IV. The hydrodynamic frame choice used is Frame 1 outlined in [58] Sec. II F, where the frame parameters are a1 = 25 /4, a2 = 25 /7 and maximum propagation speed is 

c+ = 1 .0. 

> Appendix D: Results of convergence tests

The results from the convergence tests of the KT simulations are shown in Fig. 11. For the simulations in Secs. IV A and IV C, which have smooth initial conditions, we use 1000, 2000, and 4000 spatial cells; for the smooth shock case in Sec. IV B, we use 2000, 4000, and 8000 spatial cells. As seen in Fig. 11, Qn converges to ∼ 2 for the first and third setups, as expected for continuous solutions, and to ∼ 1 for the second setup, as expected for solutions with discontinuities.    

> [1] L. D. Landau and E. M. Lifshitz, Fluid Mechanics: Volume 6 , Vol. 6 (Elsevier, 1987). [2] A. M. Anile, Relativistic Fluids and Magneto-fluids: With Applications in Astrophysics and Plasma Physics (Cambridge Monographs on Mathematical Physics) (Cambridge University Press; 1 edition, 1990).

26 

(a) (b) 

(c) (d) 

(e) (f) FIG. 11. Results of convergence tests of the KT simulations for: (a) first setup, with cch = 0 .5; (b) first setup, with cch = 0 .9; (c) second setup, with cch = 0 .5; (d) second setup, with cch = 0 .9; (e) third setup, with cch = 0 .5; (f) third setup, with cch = 0 .9. Execution times for all three runs for each of (a)-(f) are 9 .19 seconds, 11 .83 seconds, 12 .90 seconds, 17 .58 seconds, 9 .87 seconds, and 13 .18 seconds, respectively. [3] G. S. Rocha, D. Wagner, G. S. Denicol, J. Noronha, and D. H. Rischke, Theories of Relativistic Dissipative Fluid Dynamics, Entropy 26 , 189 (2024), arXiv:2311.15063 [nucl-th]. [4] U. Frisch, Turbulence: The Legacy of A. N. Kolmogorov (Cambridge University Press, 1995). [5] L. Rezzolla and O. Zanotti, Relativistic hydrodynamics (Oxford University Press, New York, 2013). [6] C. Eckart, The Thermodynamics of irreversible processes. 3.. Relativistic theory of the simple fluid, Phys. Rev. 58 , 919 (1940). [7] P. Kovtun, Lectures on hydrodynamic fluctuations in relativistic theories, J. Phys. A 45 , 473001 (2012), arXiv:1205.5040 [hep-th]. [8] W. A. Hiscock and L. Lindblom, Generic instabilities in first-order dissipative relativistic fluid theories, Phys. Rev. D 31 ,725 (1985). [9] W. Israel and J. M. Stewart, Transient relativistic thermodynamics and kinetic theory, Annals Phys. 118 , 341 (1979). [10] G. S. Denicol, H. Niemi, E. Moln´ ar, and D. H. Rischke, Derivation of transient relativistic fluid dynamics from the boltzmann equation, Phys. Rev. D 85 , 114047 (2012). [11] R. Baier, P. Romatschke, D. T. Son, A. O. Starinets, and M. A. Stephanov, Relativistic viscous hydrodynamics, conformal invariance, and holography, JHEP 04 , 100, arXiv:0712.2451 [hep-th]. [12] P. Romatschke and U. Romatschke, Relativistic Fluid Dynamics In and Out of Equilibrium , Cambridge Monographs on Mathematical Physics (Cambridge University Press, 2019) arXiv:1712.05815 [nucl-th]. 27 

[13] L. Gavassino, M. Antonelli, and B. Haskell, Bulk viscosity in relativistic fluids: from thermodynamics to hydrodynamics, Classical and Quantum Gravity 38 , 075001 (2021). [14] M. G. Alford, L. Bovard, M. Hanauske, L. Rezzolla, and K. Schwenzer, Viscous Dissipation and Heat Conduction in Binary Neutron-Star Mergers, Phys. Rev. Lett. 120 , 041101 (2018), arXiv:1707.09475 [gr-qc]. [15] E. R. Most, S. P. Harris, C. Plumberg, M. G. Alford, J. Noronha, J. Noronha-Hostler, F. Pretorius, H. Witek, and N. Yunes, Projecting the likely importance of weak-interaction-driven bulk viscosity in neutron star mergers, Mon. Not. Roy. Astron. Soc. 509 , 1096 (2021), arXiv:2107.05094 [astro-ph.HE]. [16] T. Celora, I. Hawke, P. C. Hammond, N. Andersson, and G. L. Comer, Formulating bulk viscosity for neutron star simulations, Phys. Rev. D 105 , 103016 (2022), arXiv:2202.01576 [astro-ph.HE]. [17] G. Camelio, L. Gavassino, M. Antonelli, S. Bernuzzi, and B. Haskell, Simulating bulk viscosity in neutron stars. II. Evolution in spherical symmetry, Phys. Rev. D 107 , 103032 (2023), arXiv:2204.11810 [gr-qc]. [18] G. Camelio, L. Gavassino, M. Antonelli, S. Bernuzzi, and B. Haskell, Simulating bulk viscosity in neutron stars. I. Formalism, Phys. Rev. D 107 , 103031 (2023), arXiv:2204.11809 [gr-qc]. [19] E. R. Most, A. Haber, S. P. Harris, Z. Zhang, M. G. Alford, and J. Noronha, Emergence of microphysical bulk viscosity in binary neutron star postmerger dynamics, The Astrophysical Journal Letters 967 , L14 (2024). [20] M. Chabanov and L. Rezzolla, Impact of Bulk Viscosity on the Postmerger Gravitational-Wave Signal from Merging Neutron Stars, Phys. Rev. Lett. 134 , 071402 (2025), arXiv:2307.10464 [gr-qc]. [21] L. Gavassino and J. Noronha, Relativistic bulk rheology: From neutron star mergers to viscous cosmology, Phys. Rev. D 

109 , 096040 (2024), arXiv:2305.04119 [gr-qc]. [22] J. L. Ripley, A. Hegade K. R., and N. Yunes, Probing internal dissipative processes of neutron stars with gravitational waves during the inspiral of neutron star binaries, Phys. Rev. D 108 , 103037 (2023), arXiv:2306.15633 [gr-qc]. [23] Y. Yang, M. Hippert, E. Speranza, and J. Noronha, Far-from-equilibrium bulk-viscous transport coefficients in neutron star mergers, Phys. Rev. C 109 , 015805 (2024), arXiv:2309.01864 [nucl-th]. [24] Y. Yang, M. Hippert, E. Speranza, and J. Noronha, Symmetry energy dependence of the bulk viscosity of nuclear matter, (2025), arXiv:2504.07805 [nucl-th]. [25] W. A. Hiscock and L. Lindblom, Stability and causality in dissipative relativistic fluids, Annals of Physics 151 , 466 (1983). [26] L. Gavassino, Applying the Gibbs stability criterion to relativistic hydrodynamics, Class. Quant. Grav. 38 , 21LT02 (2021), arXiv:2104.09142 [gr-qc]. [27] L. Gavassino, M. Antonelli, and B. Haskell, Thermodynamic Stability Implies Causality, Phys. Rev. Lett. 128 , 010606 (2022), arXiv:2105.14621 [gr-qc]. [28] L. Gavassino, M. M. Disconzi, and J. Noronha, Universality Classes of Relativistic Fluid Dynamics: Foundations, Phys. Rev. Lett. 132 , 222302 (2024), arXiv:2302.03478 [nucl-th]. [29] F. S. Bemfica, M. M. Disconzi, and J. Noronha, Causality of the Einstein-Israel-Stewart Theory with Bulk Viscosity, Phys. Rev. Lett. 122 , 221602 (2019), arXiv:1901.06701 [gr-qc]. [30] I. Cordeiro, E. Speranza, K. Ingles, F. S. Bemfica, and J. Noronha, Causality Bounds on Dissipative General-Relativistic Magnetohydrodynamics, Phys. Rev. Lett. 133 , 091401 (2024), arXiv:2312.09970 [astro-ph.HE]. [31] I. Cordeiro, F. S. Bemfica, E. Speranza, and J. Noronha, Nonlinear causality of Israel-Stewart theory with diffusion, (2025), arXiv:2507.20064 [nucl-th]. [32] F. S. Bemfica, M. M. Disconzi, V. Hoang, J. Noronha, and M. Radosz, Nonlinear Constraints on Relativistic Fluids Far from Equilibrium, Phys. Rev. Lett. 126 , 222301 (2021), arXiv:2005.11632 [hep-th]. [33] S. Floerchinger and E. Grossi, Causality of fluid dynamics for high-energy nuclear collisions, JHEP 08 , 186, arXiv:1711.06687 [nucl-th]. [34] M. M. Disconzi, V. Hoang, and M. Radosz, Breakdown of smooth solutions to the m¨ uller–israel–stewart equations of relativistic viscous fluids, Letters in Mathematical Physics 113 , 55 (2023). [35] F. S. Bemfica, Finite-time gradient blow-up and shock formation in Israel-Stewart theory: Bulk, shear, and diffusion regimes, Phys. Rev. E 112 , 065105 (2025), arXiv:2508.04717 [physics.gen-ph]. [36] F. S. Bemfica, M. M. Disconzi, and J. Noronha, Causality and existence of solutions of relativistic viscous fluid dynamics with gravity, Phys. Rev. D 98 , 104064 (2018), arXiv:1708.06255 [gr-qc]. [37] P. Kovtun, First-order relativistic hydrodynamics is stable, JHEP 10 , 034, arXiv:1907.08191 [hep-th]. [38] F. S. Bemfica, F. S. Bemfica, M. M. Disconzi, M. M. Disconzi, J. Noronha, and J. Noronha, Nonlinear Causality of General First-Order Relativistic Viscous Hydrodynamics, Phys. Rev. D 100 , 104020 (2019), [Erratum: Phys.Rev.D 105, 069902 (2022)], arXiv:1907.12695 [gr-qc]. [39] R. E. Hoult and P. Kovtun, Stable and causal relativistic Navier-Stokes equations, JHEP 06 , 067, arXiv:2004.04102 [hep-th]. [40] F. S. Bemfica, M. M. Disconzi, and J. Noronha, First-Order General-Relativistic Viscous Fluid Dynamics, Phys. Rev. X 

12 , 021044 (2022), arXiv:2009.11388 [gr-qc]. [41] F. S. Bemfica, M. M. Disconzi, C. Rodriguez, and Y. Shao, Local well-posedness in Sobolev spaces for first-order conformal causal relativistic viscous hydrodynamics, (2019), arXiv:1911.02504 [math.AP]. [42] F. S. Bemfica, M. M. Disconzi, and P. J. Graber, Local well-posedness in Sobolev spaces for first-order barotropic causal relativistic viscous hydrodynamics, (2020), arXiv:2009.01621 [math.AP]. [43] M. M. Disconzi and Y. Shao, Strongly hyperbolic quasilinear systems revisited, with applications to relativistic fluid dynamics, Asymptotic Analysis 140 , 281 (2024), arXiv:2308.09851 [math.AP]. 28 

[44] M. Sroczinski, Global existence and decay of small solutions for quasi-linear second-order uniformly dissipative hyperbolic-hyperbolic systems, Journal of Differential Equations 383 , 130 (2024), arXiv:2301.01685 [math.AP]. [45] H. Freist¨ uhler and M. Sroczinski, Nonlinear stability of first-order relativistic viscous hydrodynamics, J. Math. Phys. 66 ,Paper No. 093101 (2025). [46] N. Abboud, E. Speranza, and J. Noronha, Causal and stable first-order chiral hydrodynamics, Phys. Rev. D 109 , 094007 (2024), arXiv:2308.02928 [hep-th]. [47] C. Plumberg, D. Almaalol, T. Dore, J. Noronha, and J. Noronha-Hostler, Causality violations in realistic simulations of heavy-ion collisions, Phys. Rev. C 105 , L061901 (2022), arXiv:2103.15889 [nucl-th]. [48] J. Redondo-Yuste, Perturbations of relativistic dissipative stars, Class. Quant. Grav. 42 , 075012 (2025), arXiv:2411.16841 [gr-qc]. [49] D. A. Caballero and N. Yunes, Neutron star radial perturbations for causal, viscous, relativistic fluids, Phys. Rev. D 112 ,063050 (2025), arXiv:2506.09149 [gr-qc]. [50] J. Redondo-Yuste and V. Cardoso, Superradiant amplification by rotating viscous compact objects, Phys. Rev. D 112 ,L061501 (2025), arXiv:2506.13850 [gr-qc]. [51] A. Pandya, E. R. Most, and F. Pretorius, Conservative finite volume scheme for first-order viscous relativistic hydrody-namics, Physical Review D 105 , 10.1103/physrevd.105.123001 (2022). [52] A. Pandya, E. R. Most, and F. Pretorius, Causal, stable first-order viscous relativistic hydrodynamics with ideal gas microphysics, Physical Review D 106 , 10.1103/physrevd.106.123036 (2022). [53] H. Bantilan, Y. Bea, and P. Figueras, Evolutions in first-order viscous hydrodynamics, JHEP 08 , 298, arXiv:2201.13359 [hep-th]. [54] Y. Bea and P. Figueras, Field redefinitions and evolutions in relativistic Navier-Stokes, JHEP 11 , 110, arXiv:2312.16671 [hep-th]. [55] Y. Bea, Relativistic Navier-Stokes description of the quark-gluon plasma radial flow, (2025), arXiv:2503.02931 [hep-ph]. [56] J. Bhambure, A. Mazeliauskas, J.-F. Paquet, R. Singh, M. Singh, D. Teaney, and F. Zhou, Relativistic viscous hydro-dynamics in the density frame: Numerical tests and comparisons, Phys. Rev. C 111 , 064910 (2025), arXiv:2412.10303 [nucl-th]. [57] D. Fantini and M. E. Rubio, Constraint evolution in first-order viscous relativistic fluids, Phys. Rev. D 112 , 063038 (2025), arXiv:2506.06430 [gr-qc]. [58] N. Clarisse, E. O. Pinho, T. Patel, F. S. Bemfica, M. Hippert, and J. Noronha, Flux-conservative Bemfica-Disconzi-Noronha-Kovtun hydrodynamics and shock regularization, Phys. Rev. D 113 , 024051 (2026), arXiv:2510.16603 [gr-qc]. [59] L. S. Keeble and F. Pretorius, First-order viscous relativistic hydrodynamics on the two-sphere, Phys. Rev. D 112 , 124034 (2025), arXiv:2508.20998 [gr-qc]. [60] H. L. H. Shum, F. Abalos, Y. Bea, M. Bezares, P. Figueras, and C. Palenzuela, Neutron star evolution with BDNK viscous hydrodynamics framework, (2025), arXiv:2509.15303 [gr-qc]. [61] A. Kurganov and E. Tadmor, New high-resolution central schemes for nonlinear conservation laws and convection–diffusion equations, Journal of Computational Physics 160 , 241 (2000). [62] D. Bazow, U. W. Heinz, and M. Strickland, Massively parallel simulations of relativistic fluid dynamics on graphics processing units with CUDA, Comput. Phys. Commun. 225 , 92 (2018), arXiv:1608.06577 [physics.comp-ph]. [63] M. Raissi, P. Perdikaris, and G. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, Journal of Computational Physics 378 ,686 (2019). [64] Q. He and A. M. Tartakovsky, Physics-informed neural network method for forward and backward advection-dispersion equations, Water Resources Research 57 , e2020WR029479 (2021), e2020WR029479 2020WR029479, https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020WR029479. [65] D. Zhang, L. Guo, and G. E. Karniadakis, Learning in modal space: Solving time-dependent stochastic pdes using physics-informed neural networks, SIAM Journal on Scientific Computing 42 , A639 (2020), https://doi.org/10.1137/19M1260141. [66] G. Pang, L. Lu, and G. E. Karniadakis, fpinns: Fractional physics-informed neural networks, SIAM Journal on Scientific Computing 41 , A2603 (2019), https://doi.org/10.1137/18M1229845. [67] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang, Physics-informed machine learning, Nature Reviews Physics 3, 422 (2021). [68] S. Scher and G. Messori, Weather and climate forecasting with neural networks: using general circulation models (gcms) with different complexity as a study ground, Geoscientific Model Development 12 , 2797 (2019). [69] M. G. Schultz, C. Betancourt, B. Gong, F. Kleinert, M. Langguth, L. H. Leufen, A. Mozaffari, and S. Stadtler, Can deep learning beat numerical weather prediction?, Philosophical Transactions of the Royal Society A: Math-ematical, Physical and Engineering Sciences 379 , 20200097 (2021), https://royalsocietypublishing.org/rsta/article-pdf/doi/10.1098/rsta.2020.0097/248813/rsta.2020.0097.pdf. [70] J. Pathak, S. Subramanian, P. Harrington, S. Raja, A. Chattopadhyay, M. Mardani, T. Kurth, D. Hall, Z. Li, K. Az-izzadenesheli, P. Hassanzadeh, K. Kashinath, and A. Anandkumar, Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators (2022), arXiv:2202.11214 [physics.ao-ph]. [71] K. Bi, L. Xie, H. Zhang, X. Chen, X. Gu, and Q. Tian, Accurate medium-range global weather forecasting with 3d neural networks, Nature 619 , 533 (2023). [72] S. J. Wetzel, S. Ha, R. Iten, M. Klopotek, and Z. Liu, Interpretable machine learning in physics: A review (2025), arXiv:2503.23616 [physics.comp-ph]. [73] T. Roxlo, Opening the black box of neural nets: case studies in stop/top discrimination (Harvard University, 2018). 29 

[74] H. Huang, B. Xiao, Z. Liu, Z. Wu, Y. Mu, and H. Song, Applications of deep learning to relativistic hydrodynamics, Phys. Rev. Res. 3, 023256 (2021), arXiv:1801.03334 [nucl-th]. [75] H. Hirvonen, K. J. Eskola, and H. Niemi, Deep learning for flow observables in ultrarelativistic heavy-ion collisions, Phys. Rev. C 108 , 034905 (2023), arXiv:2303.04517 [hep-ph]. [76] L.-G. Pang, K. Zhou, N. Su, H. Petersen, H. St¨ ocker, and X.-N. Wang, An equation-of-state-meter of quantum chromo-dynamics transition from deep learning, Nature Commun. 9, 210 (2018), arXiv:1612.04262 [hep-ph]. [77] N. Mallick, S. Prasad, A. N. Mishra, R. Sahoo, and G. G. Barnaf¨ oldi, Deep learning predicted elliptic flow of identified particles in heavy-ion collisions at the RHIC and LHC energies, Phys. Rev. D 107 , 094001 (2023), arXiv:2301.10426 [hep-ph]. [78] J.-A. Sun, L. Yan, C. Gale, and S. Jeon, End-to-end generative diffusion model for heavy-ion collisions, Phys. Rev. C 

112 , L051903 (2025), arXiv:2410.13069 [nucl-th]. [79] C. Shen and L. Yan, Recent development of hydrodynamic modeling in heavy-ion collisions, Nucl. Sci. Tech. 31 , 122 (2020), arXiv:2010.12377 [nucl-th]. [80] W. Busza, K. Rajagopal, and W. van der Schee, Heavy Ion Collisions: The Big Picture, and the Big Questions, Ann. Rev. Nucl. Part. Sci. 68 , 339 (2018), arXiv:1802.04801 [hep-ph]. [81] L. Liu, S. Liu, H. Xie, F. Xiong, T. Yu, M. Xiao, L. Liu, and H. Yong, Discontinuity computing using physics-informed neural networks, Journal of Scientific Computing 98 , 22 (2024). [82] J. Abbasi, A. D. Jagtap, B. Moseley, A. Hiorth, and P. Østebø Andersen, Challenges and advancements in modeling shock fronts with physics-informed neural networks: A review and benchmarking study, Neurocomputing 657 , 131440 (2025). [83] T. De Ryck, S. Mishra, and R. Molinaro, wpinns: Weak physics informed neural networks for approximat-ing entropy solutions of hyperbolic conservation laws, SIAM Journal on Numerical Analysis 62 , 811 (2024), https://doi.org/10.1137/22M1522504. [84] K. Cai and J. Wang, Physics-informed neural networks for solving incompressible navier–stokes equations in wind engi-neering, Physics of Fluids 36 , 121303 (2024). [85] L. Lu, P. Jin, and G. E. Karniadakis, Learning nonlinear operators via deeponet based on the universal approximation theorem of operators, Nature Machine Intelligence 3, 218 (2021). [86] A. Ferrer-S´ anchez, J. D. Mart´ ın-Guerrero, R. R. de Austri-Bazan, A. Torres-Forn´ e, and J. A. Font, Gradient-annihilated pinns for solving riemann problems: Application to relativistic hydrodynamics, Computer Methods in Applied Mechanics and Engineering 424 , 116906 (2024). [87] J. F. Urb´ an and J. A. Pons, An approximate riemann solver approach in physics-informed neural networks for hyperbolic conservation laws, Physics of Fluids 37 , 10.1063/5.0285282 (2025). [88] L. D. McClenny and U. M. Braga-Neto, Self-adaptive physics-informed neural networks, Journal of Computational Physics 

474 , 111722 (2023). [89] N. Mullins, M. Hippert, L. Gavassino, and J. Noronha, Relativistic hydrodynamic fluctuations from an effective action: Causality, stability, and the information current, Phys. Rev. D 108 , 116019 (2023), arXiv:2309.00512 [hep-th]. [90] P. Kovtun, Temperature in relativistic fluids, Phys. Rev. D 107 , 086012 (2023), arXiv:2210.15605 [gr-qc]. [91] L. Gavassino, Can We Make Sense of Dissipation without Causality?, Phys. Rev. X 12 , 041001 (2022), arXiv:2111.05254 [gr-qc]. [92] C. Ratti and R. Bellwied, The deconfinement transition of qcd: theory meets experiment, Lecture notes in physics 981 

(2021). [93] G. S. Denicol, C. Gale, S. Jeon, A. Monnai, B. Schenke, and C. Shen, Net-baryon diffusion in fluid-dynamic simulations of relativistic heavy-ion collisions, Phys. Rev. C 98 , 034916 (2018). [94] P. Kovtun, D. T. Son, and A. O. Starinets, Viscosity in strongly interacting quantum field theories from black hole physics, Phys. Rev. Lett. 94 , 111601 (2005), arXiv:hep-th/0405231. [95] L. F. Richardson, The Approximate Arithmetical Solution by Finite Differences of Physical Problems Involving Differential Equations, with an Application to the Stresses in a Masonry Dam, Philosophical Transactions of the Royal Society of London Series A 210 , 307 (1911). [96] L. F. Richardson and J. A. Gaunt, The Deferred Approach to the Limit. Part I. Single Lattice. Part II. Interpenetrating Lattices, Philosophical Transactions of the Royal Society of London Series A 226 , 299 (1927). [97] M. Stein, Large sample properties of simulations using latin hypercube sampling, Technometrics 29 , 143 (1987). [98] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmai-son, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, Pytorch: An imperative style, high-performance deep learning library, in Advances in Neural Information Processing Systems , Vol. 32, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch´ e-Buc, E. Fox, and R. Garnett (Curran Associates, Inc., 2019). [99] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization (2017), arXiv:1412.6980 [cs.LG]. [100] D. C. Liu and J. Nocedal, On the limited memory BFGS method for large scale optimization, Math. Programming 45 ,503 (1989). [101] B. Hao, U. Braga-Neto, C. Liu, L. Wang, and M. Zhong, Stability in training pinns for stiff pdes: Why initial conditions matter (2025), arXiv:2404.16189 [math.NA]. [102] F. Cao, F. Gao, D. Yuan, and J. Liu, Multistep asymptotic pre-training strategy based on pinns for solving steep boundary singular perturbation problems, Computer Methods in Applied Mechanics and Engineering 431 , 117222 (2024). [103] M. Ren, Z. Fang, K. Li, and A. Mukherjee, Improving pinns by algebraic inclusion of boundary and initial conditions (2024), arXiv:2407.20741 [cs.LG]. 30 

[104] T. Le-Duc, S. Lee, H. Nguyen-Xuan, and J. Lee, A hierarchically normalized physics-informed neural network for solving differential equations: Application for solid mechanics problems, Engineering Applications of Artificial Intelligence 133 ,108400 (2024). [105] C. Dugas, Y. Bengio, F. B´ elisle, C. Nadeau, and R. Garcia, Incorporating second-order functional knowledge for better option pricing, in Advances in Neural Information Processing Systems , Vol. 13, edited by T. Leen, T. Dietterich, and V. Tresp (MIT Press, 2000). [106] L. Gavassino, ´A. D. Kov´ acs, and H. S. Reall, The initial data of effective field theories of relativistic viscous fluids and gravity, (2026), arXiv:2602.14541 [gr-qc]. [107] R. J. Gladstone, M. A. Nabian, N. Sukumar, A. Srivastava, and H. Meidani, Fo-pinn: A first-order formulation for physics-informed neural networks, Engineering Analysis with Boundary Elements 174 , 106161 (2025). [108] C.-W. Shu and S. Osher, Efficient implementation of essentially non-oscillatory shock-capturing schemes, J. Comput. Phys. 77 , 439 (1988). [109] R. Courant, K. Friedrichs, and H. Lewy, On the partial difference equations of mathematical physics, IBM Journal of Research and Development 11 , 215 (1967).