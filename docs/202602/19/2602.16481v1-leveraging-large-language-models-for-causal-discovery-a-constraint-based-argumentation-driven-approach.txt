Title: Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach

URL Source: https://arxiv.org/pdf/2602.16481v1

Published Time: Thu, 19 Feb 2026 01:59:09 GMT

Number of Pages: 26

Markdown Content:
# Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach 

Zihao Li 

zihao.li24@imperial.ac.uk 

Fabrizio Russo 

fabrizio@imperial.ac.uk Department of Computing Imperial College London 

# Abstract 

Causal discovery seeks to uncover causal re-lations from data, typically represented as causal graphs, and is essential for predict-ing the effects of interventions. While ex-pert knowledge is required to construct prin-cipled causal graphs, many statistical meth-ods have been proposed to leverage obser-vational data with varying formal guaran-tees. Causal Assumption-based Argumenta-tion (ABA) is a framework that uses sym-bolic reasoning to ensure correspondence be-tween input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imper-fect experts for Causal ABA, eliciting se-mantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experi-ments on standard benchmarks and semanti-cally grounded synthetic graphs demonstrate state-of-the-art performance, and we addi-tionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery. 

# 1 Introduction 

Understanding the causal structure that governs ob-servational and interventional data is central to expla-nation, prediction, and decision-making (Pearl, 2009; Peters et al., 2017; Spirtes et al., 2000). Constraint-and score-based discovery algorithms translate statis-tical regularities into equivalence classes of directed acyclic graphs (DAGs), but their reliability hinges on sometimes strong assumptions or large sample sizes, motivating practitioners to complement statistical ev-idence with mechanistic knowledge and experimental records when constructing the causal graphs necessary to perform causal inference (Cooper and Herskovits, 1992; Heckerman et al., 1995; Meek, 1995). Formulating such background knowledge is labour-intensive: domain experts must identify relevant vari-ables, specify admissible orientations, and articulate forbidden ancestral relations (Cooper and Herskovits, 1992; Meek, 1995). Existing pipelines either encode these judgements as hard structural constraints, fix-ing or forbidding edges before discovery (Meek, 1995), or as Bayesian priors that weight the search over graph structures (Cooper and Herskovits, 1992; Heck-erman et al., 1995). On the one hand, hard con-straints commit to the specified structure irrespective of whether conditional independencies in the data cor-roborate it, leaving no mechanism to expose conflicts between expertise and observations. Priors, on the other hand, require distributing probability mass over super-exponential hypothesis spaces and still provide limited feedback when they conflict with observations. Causal Assumption-Based Argumentation (Causal ABA) offers a principled alternative by encoding can-didate causal relations as defeasible assumptions and confronting them with independence constraints de-rived from data (Russo et al., 2024). ABA is a sym-bolic reasoning framework that constructs and evalu-ates arguments for and against specific claims based on a set of assumptions and rules (Bondarenko et al., 1997; Cyras et al., 2017). Causal ABA encodes d-separation (Pearl, 2009) within this framework to en-sure that any DAG inferred from data is consistent with (a defendable subset of) the input assumptions, whether in the form of independencies or structural constraints. Additionally, Causal ABA’s rule-based structure offers the possibility to trace each accepted or defeated claim back to the assumptions that sup-port it. This exposes the provenance of inferred edges, whether constraints are adopted or not, preserving transparency and rigour in the integration. 

> arXiv:2602.16481v1 [cs.AI] 18 Feb 2026 Leveraging Large Language Models for Causal Discovery

Large language models (LLMs) provide an attrac-tive yet imperfect source of knowledge. They encode broad semantic and scientific information and can per-form structured reasoning when prompted appropri-ately (Bommasani et al., 2021; Kojima et al., 2022). To leverage this information, we rely on semantically meaningful variable metadata (names and optional de-scriptions rather than anonymised identifiers such as 

X1, . . . , X n), which provides semantic signal unavail-able to data-only discovery pipelines. Yet, LLMs can hallucinate plausible-sounding but unsupported causal links (Ji et al., 2023), which goes against the rigour that causal discovery demands. Additionally, their training on web-scale corpora, where many benchmark causal discovery graphs are publicly available, makes it difficult to discern whether a suggested causal de-pendence stems from genuine reasoning or the ver-batim retrieval of a memorised solution. Perturbing descriptions beyond what the model has seen often leads to sharp degradation in the predicted relations (Carlini et al., 2021; Ji et al., 2023), underscoring the need for evaluation protocols that stress-test out-of-distribution generalisation. These tensions raise two research questions: how can we exploit the rich priors embedded in LLMs without sacrificing rigour and transparency, and how can we certify that the resulting causal graphs do not merely reflect memorised benchmarks? We address these questions by treating LLMs as defea-sible experts whose suggestions are scrutinised through argumentation. We make the following contributions: 

• We design a robust LLM integration pipeline 

that elicits high-precision constraints on causal direction ( required and forbidden arrows ), filters them via a consensus mechanism across multiple independent queries, and integrates them defea-sibly with data-derived conditional-independence evidence within a Causal ABA solver , improv-ing tractability via data-driven search optimisa-tion. 

• We develop a novel synthetic evaluation pro-tocol to mitigate memorisation concerns when evaluating LLMs for causal discovery. The proto-col creates random benchmarks by grounding ran-dom DAGs in the CauseNet (Heindorf et al., 2020) knowledge graph via sub-graph isomorphism and heuristic-guided selection, enabling a robust as-sessment of LLM generalisation. 

• We demonstrate empirically that our LLM-augmented Causal ABA pipeline achieves state-of-the-art performance on both standard bench-marks and our novel evaluation protocol. 

# 2 Related Work 

We build on three strands of literature: causal discov-ery from data, the integration of expert knowledge into causal discovery, and the use of LLMs for knowledge elicitation and causal discovery. 

Causal discovery from data. Causal discovery al-gorithms aim to reconstruct causal graphs from ob-servational and interventional data, often under the assumptions of acyclicity and faithfulness: that the causal structure is a DAG and that all and only the conditional independencies implied by the DAG are present in the data (Spirtes et al., 2000). Many meth-ods (including ours) additionally assume causal suffi-ciency, i.e., no unobserved confounders (Spirtes et al., 2000). A rich literature has proposed various algo-rithms with different assumptions, guarantees, and trade-offs between statistical and computational effi-ciency (Glymour et al., 2019; Zanga et al., 2022). Constraint-based algorithms such as Peter-Clark (PC) and Fast Causal Inference (Spirtes et al., 2000) infer Markov equivalence classes through conditional inde-pendence (CI) tests, whereas score-based procedures such as Greedy Equivalence Search (Chickering, 2002; Ramsey et al., 2017) trade combinatorial search for statistical fit criteria. We use an improved version of PC, Majority-PC (MPC) (Colombo and Maathuis, 2014), as the statistical engine to derive independence constraints from data, but our approach is agnostic to the choice of the underlying discovery algorithm. MPC remains a baseline in our empirical evaluation, to-gether with score-based Fast Greedy Search (Ramsey et al., 2017, FGS). We also benchmark against two re-cent order-based methods: GRaSP (Lam et al., 2022), a greedy relaxation of the sparsest permutation crite-rion, and BOSS (Andrews et al., 2023), an order-based score-search method that explores grow–shrink neigh-bourhoods. Amongst the score-based category, contin-uous optimisation methods have recently gained trac-tion by relaxing the combinatorial search over graphs into a continuous optimisation problem (Vowels et al., 2022). A pioneering method is NOTEARS (Zheng et al., 2018), which uses a smooth characterization of acyclicity to enable gradient-based optimisation. We use NOTEARS-MLP (Zheng et al., 2020), its more advanced, non-linear counterpart, as a baseline. 1

Hybrid approaches tighten these relaxations by combining constraint- and score-based approaches (Tsamardinos et al., 2006) while logic-based ap-     

> 1We include NOTEARS/NOTEARS-MLP as widely-used structure-learning baselines from the continuous-optimisation literature, without claiming causal identifi-ability from observational data alone; see also cautionary discussion in (Reisach et al., 2021). Li, Russo

proaches embed logical inference in the process e.g. via SAT-based encodings (Hyttinen et al., 2014). Causal ABA fits into the logical constraint-based literature but uses ABA (Bondarenko et al., 1997) to guarantee that every inferred orientation is backed by an explicit chain of assumptions and rules (Russo et al., 2024). Our work complements these efforts by integrating a scalable source of structured assumptions from LLMs. 

Integrating expert knowledge. Prior knowledge has long been recognised as essential to trim the search space and enforce domain restrictions in causal discov-ery (Cooper and Herskovits, 1992; Heckerman et al., 1995; Meek, 1995). Conventional approaches require experts to enumerate admissible edges, ancestral rela-tions, or forbidden paths before running an algorithm. More recent efforts incorporate domain constraints during search or as post-processing filters, yet they still rely on hand-crafted inputs and provide limited feed-back when constraints conflict (Hyttinen et al., 2014). By embedding LLM-sourced statements into the argu-mentative machinery of Causal ABA, we retain trans-parency about why and when constraints are overruled while creating a causal discovery pipeline that can use both traditional and unstructured sources of expertise, while blending them with data-driven insights. 

LLMs for causal discovery. LLMs are Transformer architectures (Vaswani et al., 2017) pretrained on web-scale corpora and exhibit strong few-shot generalisa-tion (Brown et al., 2020). Their internalised knowledge can be elicited through prompting to recover common-sense relations from variable-style descriptions (Zhao et al., 2023). For causal discovery, this ability enables LLMs to act as experts and translate metadata into candidate structural constraints. A growing literature leverages LLMs in three comple-mentary roles (Wan et al., 2025): (i) querying them to orient edges or assemble full causal graphs (Jiraler-spong et al., 2024; Kıcıman et al., 2023; Vashishtha et al., 2025); (ii) using them as assistants that cri-tique, refine, or document algorithmic outputs via re-finement loops or natural-language analytics (Abdu-laal et al., 2024; Gkountouras et al., 2024; Khatibi et al., 2024; Liu et al., 2024a); and (iii) coupling tex-tual priors with statistical search through hard (Chen et al., 2023) or soft constraints (Ban et al., 2023), or even by replacing CI tests with conversational judg-ments within PC (Cohrs et al., 2023). In our approach, MPC provides the CI statements from data, while the consensus-filtered LLM constraints are encoded as additional required/forbidden arrow facts and en-forced within the Causal ABA solver only when they do not contradict strong statistical counter-evidence (Section 4). As in (Jiralerspong et al., 2024; Kıcıman et al., 2023; Vashishtha et al., 2025), we solicit direct causal relations, yet we consolidate the queries into a single prompt for improved efficiency and context. Additionally, we adapt the consensus refinement strat-egy of (Cohrs et al., 2023) and benchmark against the BFS-style LLM-only method of (Jiralerspong et al., 2024), which iteratively expands a graph via pairwise queries. Despite this promise, LLM responses remain fallible. Benchmarks reveal gaps between memorised answers and genuine causal reasoning (Jin et al., 2023; Wan et al., 2025; Zeˇ cevi´ c et al., 2023), and repeated studies caution against using LLMs as sole decision-makers for causal discovery (Wu et al., 2025). Hallucinations of LLMs are well documented (Ji et al., 2023) and, when accepted as constraints, can eliminate true causal re-lations (Chen et al., 2023). To mitigate these risks, we use schema-guided prompting (567-labs, 2025; Liu et al., 2024b) to extract structured semantic con-straints, filter them via consensus, and integrate them with data-derived conditional-independence evidence within Causal ABA; we also introduce robust bench-marks to evaluate LLM outputs beyond memorisation. 

# 3 Background 

Let G = ( V, E ) be a graph with nodes V and edges 

E ⊆ V × V. Edges can be directed or undirected, the 

skeleton replaces every direction with an undirected link. A path path = x1 . . . x n is a sequence of dis-tinct adjacent nodes; it is directed if ( xi, x i+1 ) ∈ E for all i, and cyclic if additionally x1 = xn. A directed acyclic graph (DAG) has only directed edges and no cycles and will serve as our causal graph formalism. A DAG is ascribed a causal semantics by interpreting nodes as random variables and edges as direct causal effects (Pearl, 2009). A Bayesian Network (BN) is a pair ( G, P) where P is a joint distribution over V that satisfies the Markov condition relative to G: every variable is independent of its non-descendants given its parents. For pairwise disjoint sets X, Y, Z ⊆ V

we let ( X⊥⊥Y | Z) indicate that X and Y are inde-pendent given the conditioning set Z; ( X⊥⊥Y | ∅ ) is simply written as ( X⊥⊥Y) and singleton sets {x} are denoted by x (e.g., ( {x}⊥ ⊥{ y} | ∅ ) is written as ( x⊥⊥y). Also, ( X̸⊥⊥Y | Z) means that X and Y are dependent 

given Z. The link between DAG structure and CI is captured by the d-separation criterion (Pearl, 2009). Causal ABA (Russo et al., 2024) is a framework that combines computational argumentation (Dung, 1995; Toni, 2014) with causal reasoning. An ABA frame-work (Bondarenko et al., 1997, ABAF) is a tuple 

⟨L , R, A, ⟩ where L is a formal language , R is a set of inference rules of the form head ← body where 

head ∈ L and body is a finite (possibly empty) set of elements from L, A ⊆ L is a non-empty set of Leveraging Large Language Models for Causal Discovery 

assumptions , and is a total mapping from A into 

L (contrary ). In Causal ABA, the language L in-cludes statements about graphical (causal) relation-ships (with ( X, Y ) denoting a directed edge in the causal graph), independencies and d-separations; the rules R encode the principles of acyclicity and d-separation, and the assumptions A represent candi-date causal relations and/or independencies that can be accepted or rejected based on the evidence and their relations. An assumption a ∈ A attacks an assump-tion b ∈ A if and only if a can be used to derive the contrary of b, i.e., b, using the rules in R.An ABAF is evaluated using argumentation seman-tics (Baroni et al., 2011), which determine which sets of assumptions (called extensions ) can be collectively accepted based on their ability cohexist (conflict-freeness) and defend against external attacks (see (Russo et al., 2024; Toni, 2014) for formal definitions and examples). A set of assumptions S ⊆ A will pro-duce no stable extension if it contains conflicts, i.e., if there exists a, b ∈ S such that a attacks b. Causal ABA employs stable semantics to guarantee a one-to-one correspondence between the accepted assumptions and the d-separation relations induced by the output DAG (Russo et al., 2024). A bodyless rule ( head ←) is called a fact and repre-sents an unconditionally true statement in the ABAF i.e., it is by default included in every extension. In (Russo et al., 2024) ABAPC is proposed as an in-stantiation of Causal ABA using MPC (Colombo and Maathuis, 2014), as the statistical engine to derive CI constraints from data. The CI facts from MPC are ranked based on their associated p-values, and the al-gorithm iteratively attempts to construct a stable ex-tension by removing the least credible facts, until a stable extension is found. The resulting stable exten-sion corresponds to a DAG that is consistent with the strongest possible (subset of) CI statements. 

Example 3.1 Consider the simple four-variable DAG shown below (left): Education ( E) and Race ( R) influ-ence Occupation ( O), which in turn influences Income (I); Education also directly affects Income ( E → I). True Graph  

> E
> R
> OI

Majority-PC  

> E
> R
> OI

From synthetic data sampled from this ground-truth DAG we obtain 23 CI tests that become fact in Causal ABA including E ⊥⊥ R (correct) and E ⊥⊥ R | O (a finite-sample artefact). Conditioning on the collider O

should render E and R dependent, yet PC-style pro-cedures treat both statements as equally trustworthy. When MPC consume these tests, it returns the par-tially oriented DAG depicted above (right). Causal ABA ingests the same CI statements, but reasons about their joint consistency. Each test result is ranked by credibility, and the framework searches for a sta-ble extension. Including both independence statements yields no stable extension: the d-separation rules make the two statements inconsistent, because E ⊥⊥ R | O

can hold together with E ⊥⊥ R only if R and E are disconnected from the other variables—contradicting many of the other CI tests. After progressive exclu-sions, the stable extension contains only the correct in-dependence statement, yielding the ground-truth DAG. 

Having described our statistical and symbolic engines, we now turn to the integration of LLM-derived knowl-edge into Causal ABA. 

# 4 LLMs Knowledge for Causal ABA 

Our methodology integrates the semantic knowledge of LLMs with the symbolic rigour of Causal ABA. The core idea is to use an LLM as a proxy domain ex-pert to generate high-precision structural constraints, which are then encoded as facts within the argumenta-tive framework to prune the vast search space of pos-sible causal graphs. This hybrid approach, depicted in Figure 1, aims to enhance both the efficiency and the accuracy of the discovery process. The pipeline consists of a method to elicit structured constraints from LLMs (left), and the formal integration of these constraints into Causal ABA (right). Human expert knowledge is included as optional, and not currently part of the experiments, but can be readily integrated. 

4.1 Constraint Elicitation Pipeline 

The quality of LLM-derived knowledge is highly de-pendent on the elicitation process (Anthropic, 2024). We propose a robust pipeline to transform unstruc-tured metadata into formal, high-precision constraints. 

Eliciting Structural Priors. The process begins with a set of variables, each with a name and an op-tional description. We developed a detailed prompt that instructs a primary LLM ( gemini-2.5-flash ,LLM 1 in Figure 1) to act as a domain expert and to focus exclusively on high-precision judgments. The prompt explicitly asks the LLM to generate two lists: 

• Required Directions : Causal relationships (X, Y ) that are required based on logic, tempo-ral precedence, or established principles. 

• Forbidden Directions : Causal relationships (X, Y ) that are forbidden if they contradict es-tablished knowledge or logical consistency. Li, Russo 

Figure 1: LLM Integration Pipeline: Given a set of variables with their names and (possibly) descriptions, we prompt an LLM to generate pairwise causal statements. These statements are then parsed into structured assumptions for Causal ABA, which combines them with data-derived independence or arrow constraints and background knowledge to infer a set of causal graphs. Expert knowledge can be injected in the LLM prompts or as defeasible facts. Variables descriptions and LLM parsing are optional components of the pipeline but enhance the quality of the generated assumptions. Detailed prompts and parsing rules are provided in Appendix A. The LLM is instructed to format its response us-ing clear headers and a line-by-line structure for each prior, followed by a brief justification. The complete prompt is provided in Appendix A. 

Schema-Guided Output Extraction. To ensure reliability and avoid constraining the LLM’s reason-ing process with rigid formatting requirements (Tam et al., 2024), we decouple the complex reasoning task of prior generation from the simpler structured output extraction task. The free text output from the primary LLM is passed to a separate, dedicated process that uses a schema-enforcing tool (567-labs, 2025) to guide a second, lightweight LLM ( gemini-2.5-flash-lite ,LLM 3 in Figure 1) in robustly extracting and val-idating the constraints. This second LLM is highly effective at interpreting the semi-structured text and handling minor formatting inconsistencies, making the extraction process significantly more robust than tra-ditional methods like regular expressions. This pro-duces a structured list of constraints for the next stage. 

Consensus for High Precision. To mitigate the stochasticity of LLM outputs and adhere to the high-precision requirement of causal discovery, we adapt the majority voting idea from (Cohrs et al., 2023) and in-troduce a consensus-based filtering step. We query the primary LLM five times independently with the same prompt. The final sets of constraints consist only of the intersection of each of the forbidden and required arrows sets, across all five parsed responses. This con-servative approach ensures that only the most con-sistently suggested constraints are passed to Causal ABA, significantly reducing the risk of incorporating spurious constraints (see Appendix A.3 for details). Consensus does not guarantee correctness; it is a prag-matic variance-reduction step aimed at improving pre-cision before constraints are passed to the solver. 

4.2 Integration into Causal ABA 

We integrate the consensus-filtered LLM constraints as required and forbidden arrow facts, which are en-forced as constraints to further prune the search space. To avoid forcing semantic constraints that directly contradict strong statistical evidence, we apply them only after the one-off data-driven skeleton reduction described below, and we discard any required arrow whose corresponding undirected edge was removed by this reduction. The remaining CI statements are han-dled within Causal ABA as weighted constraints and are progressively relaxed until the program admits a stable extension (and thus a DAG). 

Data-Driven Skeleton Reduction. To improve the scalability of Causal ABA we perform an initial, one-off skeleton reduction : edges corresponding to a high-confidence independence test are removed from the graph. This follows causal theory and the strat-egy employed by both PC (Spirtes et al., 2000) and Causal ABA, with the difference that, during the sub-sequent iterative relaxation of lower-confidence CI con-straints, 2 these edges are not reintroduced. This modi-fication of Causal ABA avoids the computationally ex-

> 2This is implemented in clingo (Gebser et al., 2019) by changing from a hard constraint to a soft, weighted con-straint that the solver can either satisfy or violate, remov-ing its enforcing power without full program regeneration. Leveraging Large Language Models for Causal Discovery

pensive re-grounding of the logic program representing the ABAF, making the framework tractable for larger problems, at the cost of being unable to fully assess the relations around these hard constraints. This scal-ability optimisation is applied to both ABAPC and ABAPC-LLM in all experiments. Overall, ABAPC-LLM inherits the CI-testing cost of PC/MPC but adds overhead from solving the ABAF and from a one-off set of LLM calls per dataset; empirical runtimes are reported in Appendix C.3.3. 

LLM Constraints Enforcement. The LLM-derived constraints from the consensus step are then applied as additional hard constraints, further prun-ing the search space, while avoiding conflicts with the data-driven skeleton reduction. Specifically: 

• A required arrow (encoded as ( X, Y ) ←) is en-forced only if the edge X − Y was not already removed during the initial skeleton reduction. This prevents semantic constraints from conflict-ing with strong statistical counter-evidence. 

• A forbidden arrow (encoded as ( X, Y ) ←) re-stricts the solver from orienting a remaining edge in a certain direction. This two-stage process leverages statistical evidence to define the initial search space, then employs high-precision semantic knowledge to further guide the dis-covery process with information that is often orthogo-nal to what can be inferred from data alone. 

# 5 CauseNet Synthetic DAGs 

A fundamental challenge in evaluating LLMs for causal discovery is the risk of data leakage (Balloccu et al., 2024; Dong et al., 2024). Standard benchmarks, such as Asia (Lauritzen and Spiegelhalter, 2018) or Sachs 

(Sachs et al., 2005) in the bnlearn repository (Scu-tari, 2010), are widely published and likely part of the LLMs’ pre-training corpora. Consequently, high per-formance on these benchmarks may reflect memorisa-tion rather than generalisable causal reasoning. 

Why Semantic Grounding Is Necessary. Be-cause LLM elicitation relies on variable semantics, anonymising variables (e.g., X1, . . . , X n) would re-move the signal and reduce the model to an unin-formed oracle. We therefore ground randomly gen-erated structures in real concepts via CauseNet to ob-tain semantically meaningful yet structurally diverse graphs; while individual CauseNet facts may appear in pre-training corpora, composing them into random, previously unseen subgraphs makes verbatim retrieval of an entire target DAG implausible and helps distin-guish memorisation from generalisation. Our protocol, illustrated in Figure 2, embeds randomly generated graph structures within CauseNet , a large-scale knowledge graph of real-world cause-and-effect relationships (Heindorf et al., 2020). The pipeline con-sists of three main stages, as follows: 

1. Structural Scaffolding. We begin by generat-ing a structural “scaffold”. To ensure structural diver-sity in our benchmarks, we first define a DAG topol-ogy using one of three methods: generating Erd˝ os-R´ enyi (Erd¨ os and R´ enyi, 1959), Scale-Free (Barab´ asi and Albert, 1999) graphs, or directly constructing a DAG from a randomly populated lower triangular ad-jacency matrix (details in Appendix B). Once a DAG structure is sampled, we build a full BN by assign-ing randomly initialised conditional probability tables to each node. This two-step process yields a fully de-fined BN that allows to sample synthetic observational data. We sample CPTs at random purely to generate data consistent with the sampled structure; since we evaluate structural recovery rather than effect sizes, semantic “strength” need not match synthetic effect magnitudes, and the LLM is queried only about direc-tions (required/forbidden), not effect size. 

2. Semantic Grounding via Isomorphism. To as-sign meaningful concepts to the nodes of the random DAG, we use CauseNet , a large-scale knowledge graph of cause-and-effect relationships extracted from web text (Heindorf et al., 2020). We search for an induced sub-graph isomorphism from the random DAG to the CauseNet graph. Formally, given a random DAG 

H = ( VH , E H ) and CauseNet G = ( VG, E G), we find an injective function f : VH → VG such that for any pair of nodes u, v ∈ VH , an edge ( u, v ) ∈ EH

exists if and only if an edge ( f (u), f (v)) ∈ EG ex-ists. This strategy ensures that the selected sub-graph from CauseNet has the exact same structure as our random DAG. We employ the efficient VF2 algo-rithm (Cordella et al., 2004) for this search. 

3. Heuristic-Guided Candidate Selection. A sin-gle random DAG can have thousands or even millions of isomorphic matches within CauseNet . To find the most plausible causal scenario, we score and rank all candidate sub-graphs using a flexible, heuristic-based system that combines three distinct cost terms into a single objective function to be minimised. 

• Semantic Compactness: Measures the the-matic coherence of the concepts in a candidate graph. Using pre-computed vector embeddings for all concepts, we calculate the average cosine distance of each concept’s vector from their geo-metric centroid. A low cost indicates that the con-cepts form a tight semantic cluster (e.g., virus ,

fever , fatigue ). Li, Russo 

Figure 2: Synthetic Evaluation Protocol: We generate random DAGs and ground them in CauseNet by finding sub-graph isomorphisms. Given that there might be many possible isomorphisms, we use a heuristic composed of three scores to select the most suitable one. The output is a semantically grounded DAG with variable names that can be used to evaluate the robustness of LLMs in generating causal assumptions. 

• Node Specificity: Penalises candidate graphs that include overly general or ambiguous “hub” nodes (e.g., illness , problem ). The cost is the average logarithm of each node’s degree in the full 

CauseNet graph, favouring sub-graphs composed of more specific, well-defined concepts. 

• Structural-Semantic Correlation: Ensures the graph’s topology faithfully reflects the se-mantic relationships between its concepts. We compute the rank correlation (Spearman, 1904) between all-pairs shortest-path distances in the graph and the corresponding pairwise cosine dis-tances between semantic embeddings. Lower cost (inverse of correlation) means that concepts di-rectly connected in the graph are also semanti-cally very close, ensuring holistic consistency. The final objective is the candidate graph that minimises a weighted sum of these three heuris-tic cost terms. This pipeline produces high-quality, semantically-grounded causal graphs that serve as a robust foundation for evaluating causal discovery in-volving LLMs. We make our synthetic DAG generator publicly available as open-source software. 

# 6 Experimental Evaluation 

We now present an empirical evaluation of our LLM-augmented Causal ABA framework. Our experiments aim to assess the accuracy and robustness of the in-ferred causal graphs, particularly in comparison to es-tablished causal discovery algorithms and under vary-ing conditions of LLM input quality. Throughout, we focus on purely observational data; consequently, conditional-independence evidence alone identifies the causal graph only up to a Markov equivalence class (MEC). Semantic constraints can provide additional orienting information within the MEC, and we evalu-ate their downstream impact via standard structural metrics against the ground-truth DAG. 

6.1 Experimental Setup Data. We evaluate our approach on both standard causal discovery benchmarks and our novel synthetic datasets derived from CauseNet . For each dataset, we generate 5000 observational samples and repeat the experiment 50 times with different random seeds. The standard pseudo-real BNs used are from the bnlearn 

repository (Scutari, 2010). Our synthetic datasets consist of 54 random DAGs with number of nodes 

|V| ∈ {5, 10 , 15 }, generated using the protocol de-scribed in Section 5. We ensure that these synthetic graphs are semantically grounded in CauseNet and are novel to the LLMs by using random structures and a heuristic selection process. Detailed statistics of all datasets are provided in Appendix B and the synthetic BNs produced are available in our repository, together with the code to reproduce the experiments. The choice of CI testing method is consistent across MPC, ABAPC, and ABAPC-LLM: Wilks G2

likelihood-ratio test (Agresti, 2018), with significance level α = 0 .05. Within ABA-PC and ABA-PC-LLM, MPC is only used to compute (an efficient number of) CI tests and their associated p-values. 

Metadata Preparation and Prompting. We gen-erated variable descriptions for causal graphs using an LLM, providing it with the ground-truth graph as a basis for defining each variable’s role. The prompt was carefully designed to ensure the resulting descrip-tions were semantically consistent with the under-lying causal structure without explicitly stating any of the causal links. The full prompt is provided in Appendix A. We then used these variable names and descriptions as input to our constraint elicitation pipeline (Section 4) to generate the LLM-derived con-straints. We used Google’s gemini-2.5-flash for the primary LLM and gemini-2.5-flash-lite for the structured output extraction, leveraging Anthropic’s Leveraging Large Language Models for Causal Discovery 

Figure 3: Normalised Structural Hamming Distance (SHD, left-axis) and F1-score (right-axis) of LLM-augmented Causal ABA against baselines across synthetic datasets generated from the CauseNet Knowledge Graph (see Section 5) grouped by number of nodes ( |V| ∈ { 5, 10 , 15 }). Error bars are standard deviations over 50 repetitions. prompt improver (Anthropic, 2024) to iteratively re-fine our prompts. For each dataset, the same set of LLM-derived constraints was used across all 50 data repetitions. The same variable names are used for our method and the LLM-only baseline (Jiralerspong et al., 2024) to ensure a fair comparison. 

Metrics. We measure the accuracy of the inferred causal graphs using three metrics: Structural Ham-ming Distance (Tsamardinos et al., 2006, SHD), Struc-tural Intervention Distance (Peters and B¨ uhlmann, 2015, SID), and F1-score. SHD quantifies the number of edge modifications needed to transform the inferred graph into the true graph, while SID measures the cor-rectness of inferred causal effects under interventions. The F1-score provides a balanced measure of preci-sion and recall for the presence of edges in the inferred graph. Additional details are in Appendix C.1. 

Baselines. We compare our LLM-augmented ABAPC (ABAPC-LLM) against several baselines: ABAPC (Russo et al., 2024), MPC (Colombo and Maathuis, 2014), FGS (Ramsey et al., 2017), NOTEARS-MLP (Zheng et al., 2020), GRaSP (Lam et al., 2022), and BOSS (Andrews et al., 2023). These algorithms represent a mix of constraint-based, score-based, and continuous optimisation approaches, pro-viding a comprehensive benchmark for our method. MPC is also the statistical engine underlying ABAPC, allowing us to isolate the impact of LLM-derived con-straints. Additionally, we include an LLM-only base-line (Jiralerspong et al., 2024) to gauge impact of our data-LLM integration, and a random graph generation baseline to establish a lower bound on performance. 

6.2 Results and Analysis 

We summarise the experimental results, comparing ABAPC-LLM with the baselines. We first examine structural learning performance and then analyse how LLM-derived constraints interact with data-driven in-dependence tests. All details in Appendix C. 

Structural Learning Performance. Figure 3compares the structural accuracy achieved by our method against the baselines across the synthetic 

CauseNet graphs. 3 ABAPC-LLM attains the lowest normalised SHD and the highest F1-score for all graph sizes, with particularly large margins for five- and fifteen-node problems over MPC, FGS, NOTEARS-MLP, GRaSP, BOSS, and the LLM-only baseline. The gains persist on ten-node graphs, although the gap narrows as the statistical signal strengthens. BH-corrected (Benjamini and Hochberg, 1995) two-sample, unequal variance t-tests (Appendix C.3.2) demonstrate that the SHD and F1 improvements over all baselines are significant for all graph sizes. Differ-ences with the variant of Causal ABA that relies exclu-sively on independence tests are smaller, but remain positive, indicating that the semantic priors comple-ment rather than contradict the statistical evidence. Runtime measurements are reported in Ap-pendix C.3.3, showing that ABAPC-LLM remains practical for graphs up to 15 nodes. 

> 3SID, precision and recall curves are reported in Ap-pendix C.3 and show the same ordering. Li, Russo

LLM Constraints Quality and Interaction with Data. To understand how our LLM-augmented pipeline improves structural accuracy we inspect the quality of the priors entering the solver in relation to the data-driven constraints. For every dataset we compute the F1-score of the LLM-derived constraints against the ground-truth graph, the F1-score of the retained CI statements, and the resulting change in DAG F1 after adding the semantic constraints (∆F1). The heatmap in Figure 4 shows a clear synergy: high-quality constraints translate into consistent F1 gains when the underlying conditional independence infor-mation is also reliable, whereas noisy LLM outputs are either ignored or mildly detrimental when the sta-tistical evidence is weak. This interaction highlights the role of our structured integration, which privileges the more trustworthy source while still exploiting com-plementary information when both signals agree. The 

bnlearn benchmarks counterpart are reported in Ap-pendix C.3.4 and show the same pattern, albeit with a narrower range of LLM constraint quality due to po-tential memorisation. Splits by DAG characteristics are reported in Appendix C.4 and show that the im-provements are consistent across structural properties. 

# 7 Conclusion 

We introduced a hybrid causal discovery framework that injects LLM-derived semantic knowledge into Causal ABA, combining symbolic guarantees with rich and unstructured semantic information. A dedicated elicitation pipeline extracts high-precision structural constraints from natural-language metadata, while our 

CauseNet -grounded synthetic benchmarks provide a setting where memorisation is unlikely and improve-ments in structural, interventional and precision–recall metrics become apparent. Our findings show that consensus-filtered semantic constraints can boost structural accuracy complement-ing statistical evidence, and they motivate several av-enues for future work. In particular, we plan to en-rich the priors with knowledge mined from scientific corpora, extend the dialogue with LLMs to reason about potential unobserved confounders, and endow the solver with adaptive weighting schemes that cali-brate LLM assertions using confidence scores that can be used to rank against data-driven learning. Explor-ing alternative models and prompt designs will fur-ther clarify how different LLM capabilities influence the trade-off between coverage and precision. 

# References 

567-labs. Instructor: Structured outputs for llms. 

https://github.com/567-labs/instructor ,2025. GitHub repository. 

Figure 4: CauseNet Synthetic: Heatmap showing how the quality of LLM-derived and data-derived con-straints relates to changes in final graph reconstruction accuracy after integration (∆F1 against the true DAG, in brackets the number of constraints n). Ahmed Abdulaal, adamos hadjivasiliou, Nina Montana-Brown, Tiantian He, Ayodeji Ijishakin, Ivana Drobnjak, Daniel C. Castro, and Daniel C. Alexander. Causal modelling agents: Causal graph discovery through synergising metadata- and data-driven reasoning. In The Twelfth International Con-ference on Learning Representations , 2024. URL 

https://openreview.net/forum?id=pAoqRlTBtY .Alan Agresti. An Introduction to Categorical Data Analysis . Wiley, 3 edition, 2018. Bryan Andrews, Joseph Ramsey, Ruben Sanchez Romero, Jazmin Camchong, and Erich Kummerfeld. Fast scalable and accurate discovery of DAGs using the best order score search and grow–shrink trees. Advances in Neural Information Processing Systems , 36, 2023. Anthropic. Prompt improver, 2024. URL https:// www.anthropic.com/news/prompt-improver . Ac-cessed: 2025-09-29. Simone Balloccu, Patr´ ıcia Schmidtov´ a, Mateusz Lango, and Ondˇ rej Duˇ sek. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms, 2024. URL https://arxiv. org/abs/2402.03927 .Taiyu Ban, Lyvzhou Chen, Xiangyu Wang, and Huan-huan Chen. From query tools to causal archi-tects: Harnessing large language models for ad-vanced causal discovery from data, 2023. Albert-L´ aszl´ o Barab´ asi and R´ eka Albert. Emer-gence of scaling in random networks. Science , 286 (5439):509–512, 1999. doi: 10.1126/science.286. 5439.509. URL https://www.science.org/doi/ abs/10.1126/science.286.5439.509 .Leveraging Large Language Models for Causal Discovery 

Pietro Baroni, Martin Caminada, and Massimil-iano Giacomin. An introduction to argumenta-tion semantics. Knowl. Eng. Rev. , 26(4):365– 410, 2011. URL https://doi.org/10.1017/ S0269888911000166 .Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: a practical and powerful ap-proach to multiple testing. Journal of the Royal Statistical Society: Series B (Methodological) , 57(1): 289–300, 1995. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Sanjeev Arora, Sydney von Arx, et al. On the opportunities and risks of foundation mod-els. arXiv preprint arXiv:2108.07258 , 2021. Andrei Bondarenko, Phan Minh Dung, Robert A. Kowalski, and Francesca Toni. An abstract, argumentation-theoretic approach to default reason-ing. Artif. Intell. , 93:63–101, 1997. doi: 10.1016/ S0004-3702(97)00015-5. URL https://doi.org/ 10.1016/S0004-3702(97)00015-5 .Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165 .Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Nicolas Papernot. Ex-tracting training data from large language models. In Proceedings of the 30th USENIX Security Sym-posium , pages 2633–2650, 2021. Lyuzhou Chen, Taiyu Ban, Xiangyu Wang, Derui Lyu, and Huanhuan Chen. Mitigating prior errors in causal structure learning: Towards llm driven prior knowledge, 2023. URL https://arxiv.org/abs/ 2306.07032 .David Maxwell Chickering. Optimal structure identi-fication with greedy search. J. Mach. Learn. Res. ,3:507–554, 2002. URL https://jmlr.org/papers/ v3/chickering02b.html .Kai-Hendrik Cohrs, Emiliano Diaz, Vasileios Sitokon-stantinou, Gherardo Varando, and Gustau Camps-Valls. Large language models for constrained-based causal discovery. In AAAI 2024 Workshop on ”Are Large Language Models Simply Causal Parrots?” ,2023. URL https://openreview.net/forum?id= NEAoZRWHPN .Diego Colombo and Marloes H. Maathuis. Order-independent constraint-based causal structure learning. J. Mach. Learn. Res. , 15(1):3741–3782, 2014. URL https://dl.acm.org/doi/10.5555/ 2627435.2750365 .Gregory F. Cooper and Edward Herskovits. Abayesian method for the induction of probabilistic networks from data. Machine Learning , 9(4):309– 347, 1992. L.P. Cordella, P. Foggia, C. Sansone, and M. Vento. A (sub)graph isomorphism algorithm for matching large graphs. IEEE Transactions on Pattern Anal-ysis and Machine Intelligence , 26(10):1367–1372, 2004. doi: 10.1109/TPAMI.2004.75. Kristijonas Cyras, Xiuyi Fan, Claudia Schulz, and Francesca Toni. Assumption-based argumentation: Disputes, explanations, preferences. FLAP , 4(8), 2017. URL http://www.collegepublications. co.uk/downloads/ifcolog00017.pdf .Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. Generalization or memorization: Data contamination and trustworthy evaluation for large language models, 2024. URL 

https://arxiv.org/abs/2402.15938 .Phan Minh Dung. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artif. In-tell. , 77(2):321–358, 1995. URL https://doi.org/ 10.1016/0004-3702(94)00041-X .P Erd¨ os and A R´ enyi. On random graphs i. Publica-tiones Mathematicae Debrecen , 6:290–297, 1959. Martin Gebser, Roland Kaminski, Benjamin Kauf-mann, and Torsten Schaub. Multi-shot ASP solv-ing with clingo. Theory Pract. Log. Program. , 19 (1):27–82, 2019. URL https://doi.org/10.1017/ S1471068418000054 .John Gkountouras, Matthias Lindemann, Phillip Lippe, Efstratios Gavves, and Ivan Titov. Lan-guage agents meet causality – bridging llms and causal world models, 2024. URL https://arxiv. org/abs/2410.19923 .Clark Glymour, Kun Zhang, and Peter Spirtes. Re-view of causal discovery methods based on graphical models. Front. Genet., 04 June 2019 Sec. Statisti-cal Genetics and Methodology , 10:524, 2019. URL 

https://doi.org/10.3389/fgene.2019.00524 .David Heckerman, Dan Geiger, and David M. Chick-ering. Learning Bayesian Networks: The Combi-nation of Knowledge and Statistical Data. Ma-chine Learning , 20(3):197–243, 1995. ISSN 1573-Li, Russo 

0565. doi: 10.1023/A:1022623210503. URL https: //doi.org/10.1023/A:1022623210503 .Stefan Heindorf, Yan Scholten, Henning Wachsmuth, Axel-Cyrille Ngonga Ngomo, and Martin Potthast. Causenet: Towards a causality graph extracted from the web. In Proceedings of the 29th ACM In-ternational Conference on Information & Knowl-edge Management , CIKM ’20, page 3023–3030, New York, NY, USA, 2020. Association for Comput-ing Machinery. ISBN 9781450368599. doi: 10. 1145/3340531.3412763. URL https://doi.org/ 10.1145/3340531.3412763 .Antti Hyttinen, Frederick Eberhardt, and Matti J¨ arvisalo. Constraint-based causal discovery: Con-flict resolution with answer set programming. In 

Proceedings of the Thirtieth Conference on Uncer-tainty in Artificial Intelligence, UAI 2014, Quebec City, Quebec, Canada, July 23-27, 2014 , pages 340– 349. AUAI Press, 2014. Ziwei Ji, Nanyun Lee, Tianyi Yu, Qihui He, Ruibo Zhang, Xiang Ren, et al. A survey on hallucination in natural language generation. ACM Computing Surveys , 55(12):1–38, 2023. Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez, Max Kleiman-Weiner, Mrinmaya Sachan, and Bernhard Sch¨ olkopf. CLadder: Assessing causal reasoning in language models. In NeurIPS ,2023. URL https://openreview.net/forum?id= e2wtjx0Yqu .Thomas Jiralerspong, Xiaoyin Chen, Yash More, Vedant Shah, and Yoshua Bengio. Efficient causal graph discovery using large language models. In 

ICLR 2024 Workshop: How Far Are We From AGI ,2024. URL https://openreview.net/forum?id= 5RBUTx75yr .Elahe Khatibi, Mahyar Abbasian, Zhongqi Yang, Iman Azimi, and Amir M. Rahmani. Alcm: Autonomous llm-augmented causal discovery framework, 2024. URL https://arxiv.org/abs/2405.01744 .Emre Kıcıman, Robert Ness, Amit Sharma, and Chen-hao Tan. Causal reasoning and large language mod-els: Opening a new frontier for causality, 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neu-ral Information Processing Systems , 2022. Wai-Yin Lam, Bryan Andrews, and Joseph Ramsey. Greedy relaxations of the sparsest permutation al-gorithm. In Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence (UAI) , 2022. S. L. Lauritzen and D. J. Spiegelhalter. Local com-putations with probabilities on graphical structures and their application to expert systems. Jour-nal of the Royal Statistical Society: Series B(Methodological) , 50(2):157–194, 12 2018. ISSN 0035-9246. doi: 10.1111/j.2517-6161.1988.tb01721. x. URL https://doi.org/10.1111/j.2517-6161. 1988.tb01721.x .Chenxi Liu, Yongqiang Chen, Tongliang Liu, Ming-ming Gong, James Cheng, Bo Han, and Kun Zhang. Discovery of the hidden world with large language models, 2024a. URL https://arxiv.org/abs/ 2402.03941 .Michael Xieyang Liu, Frederick Liu, Alexander J. Fi-annaca, Terry Koo, Lucas Dixon, Michael Terry, and Carrie J. Cai. ”we need structured output”: To-wards user-centered constraints on large language model output. In Extended Abstracts of the CHI Conference on Human Factors in Computing Sys-tems , CHI ’24, pages 1–9. ACM, may 2024b. doi: 10.1145/3613905.3650756. URL http://dx.doi. org/10.1145/3613905.3650756 .Christopher Meek. Causal inference and causal ex-planation with background knowledge. In UAI ’95: Proceedings of the Eleventh Annual Confer-ence on Uncertainty in Artificial Intelligence, Mon-treal, Quebec, Canada, August 18-20, 1995 , pages 403–410. Morgan Kaufmann, 1995. URL https: //doi.org/10.48550/arXiv.1302.4972 .Judea Pearl. Causality: Models, Reasoning, and Infer-ence . Cambridge University Press, 2 edition, 2009. J. Peters, D. Janzing, and B. Sch¨ olkopf. 

Elements of Causal Inference: Founda-tions and Learning Algorithms . MIT Press, Cambridge, MA, USA, 2017. URL 

https://mitpress.mit.edu/9780262037310/ elements-of-causal-inference/ .Jonas Peters and Peter B¨ uhlmann. Structural inter-vention distance for evaluating causal graphs. Neu-ral Comput. , 27(3):771–799, 2015. URL https: //doi.org/10.1162/NECO_a_00708 .Joseph D. Ramsey, Madelyn Glymour, Ruben Sanchez-Romero, and Clark Glymour. A mil-lion variables and more: the fast greedy equiva-lence search algorithm for learning high-dimensional graphical causal models, with an application to func-tional magnetic resonance images. Int. J. Data Sci. Anal. , 3(2):121–129, 2017. URL https://doi.org/ 10.1007/s41060-016-0032-z .Alexander G. Reisach, Christof Seiler, and Sebastian Weichwald. Beware of the simulated DAG! causal discovery benchmarks may be easy to game, 2021. Fabrizio Russo, Anna Rapberger, and Francesca Toni. Argumentative causal discovery. In Proc. of KR 2024 , 2024. doi: 10.24963/KR.2024/88. URL 

https://doi.org/10.24963/kr.2024/88 .Karen Sachs, Omar Perez, Dana Pe’er, Dou-glas A. Lauffenburger, and Garry P. Nolan. Causal protein-signaling networks derived from multiparameter single-cell data. Science , 308 (5721):523–529, 2005. doi: 10.1126/science. 1105809. URL https://www.science.org/doi/ abs/10.1126/science.1105809 .Marco Scutari. Learning bayesian networks with the bnlearn R package. Journal of Statistical Software ,35(3):1–22, 2010. doi: 10.18637/jss.v035.i03. C. Spearman. The proof and measurement of associ-ation between two things. The American Journal of Psychology , 15(1):72–101, 1904. ISSN 00029556. URL http://www.jstor.org/stable/1412159 .Peter Spirtes, Clark Glymour, and Richard Scheines. 

Causation, Prediction, and Search . MIT Press, 2000. Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, and Yun-Nung Chen. Let me speak freely? a study on the impact of for-mat restrictions on large language model perfor-mance. In Proc. of EMNLP 2024 , pages 1218– 1236, 2024. URL https://aclanthology.org/ 2024.emnlp-industry.91/ .Francesca Toni. A tutorial on assumption-based argu-mentation. Argument Comput. , 5(1):89–117, 2014. URL https://doi.org/10.1080/19462166.2013. 869878 .Ioannis Tsamardinos, Laura E. Brown, and Con-stantin F. Aliferis. The max-min hill-climbing bayesian network structure learning algorithm. 

Mach. Learn. , 65(1):31–78, 2006. URL https: //doi.org/10.1007/s10994-006-6889-7 .Aniket Vashishtha, Abbavaram Gowtham Reddy, Abhinav Kumar, Saketh Bachu, Vineeth N. Bala-subramanian, and Amit Sharma. Causal order: The key to leveraging imperfect experts in causal infer-ence. In 2025 International Conference on Learning Representations , April 2025. URL https://www. microsoft.com/en-us/research/publication/ causal-order-the-key-to-leveraging-imperfect-experts-in-causal-inference/ .Ashish Vaswani, Matthew Shardlow, et al. Attention is all you need. Proceedings of the 34th International Conference on Machine Learning , 70(1), 2017. Matthew J. Vowels, Necati Cihan Camgoz, and Richard Bowden. D’ya Like DAGs? A Survey on Structure Learning and Causal Discovery. ACM Comput. Surv. , 55(4), November 2022. ISSN 0360-0300. URL https://doi.org/10.1145/3527154 .Guangya Wan, Yunsheng Lu, Yuqi Wu, Mengxuan Hu, and Sheng Li. Large language models for causal dis-covery: Current landscape and future directions, feb 2025. URL http://arxiv.org/abs/2402.11068 .Xingyu Wu, Kui Yu, Jibin Wu, and Kay Chen Tan. Llm cannot discover causality, and should be re-stricted to non-decisional support in causal discov-ery, 2025. URL https://arxiv.org/abs/2506. 00844 .Alessio Zanga, Elif Ozkirimli, and Fabio Stella. Asurvey on causal discovery: Theory and practice. 

Int. J. Approx. Reason. , 151:101–129, 2022. URL 

https://doi.org/10.1016/j.ijar.2022.09.004 .Matej Zeˇ cevi´ c, Moritz Willig, Devendra Singh Dhami, and Kristian Kersting. Causal parrots: Large lan-guage models may talk causality but are not causal. 

Transactions on Machine Learning Research , 2023. ISSN 2835-8856. URL https://openreview.net/ forum?id=tv46tCzs83 .Zirui Zhao, Wee Sun Lee, and David Hsu. Large lan-guage models as commonsense knowledge for large-scale task planning, 2023. URL https://arxiv. org/abs/2305.14078 .Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. Dags with NO TEARS: continu-ous imization for structure learning. In Proc. of NeurIPS 2018 , pages 9492–9503, 2018. URL https: //proceedings.neurips.cc/paper/2018/hash/ e347c51419ffb23ca3fd5050202f9c3d-Abstract. html .Xun Zheng, Chen Dan, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. Learning sparse nonparametric dags. In Proc. of AISTATS 2020 ,volume 108 of PMLR , pages 3414–3425. PMLR, 2020. URL http://proceedings.mlr.press/ v108/zheng20a.html .Li, Russo 

# Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach Supplementary Materials 

In this supplementary material we provide details omitted from the main text due to space limitations. In particular, we describe the prompts used for metadata enrichment and constraint elicitation (Appendix A), the consensus strategy for stabilising LLM outputs, the generation and semantic grounding of the synthetic CauseNet datasets (Appendix B), and the full experimental protocol, metrics, baselines, ablations, and additional quantitative analyses (Appendix C). 

# A Prompts 

This sections covers the details on the prompts used for the different tasks in which LLMs are involved in. See Figure 1 in the main text for the pipeline overview. The following prompts were built with various iterations of Anthropic’s prompt improver tool (Anthropic, 2024). 

A.1 Metadata Enrichment Prompt 

Below, we provide the prompt used for eliciting the variable descriptions before interrogating the main LLM (LLM 1 in Figure 1) about causal relations amongst variables. The following prompt is related to LLM 2 in Figure 1, which we deem an optional block as its necessity may depend on context. Ablations on the importance of variable descriptions are in Section 4. 

 

You are tasked with generating descriptions for causal variables in a randomly generated causal graph used for synthetic dataset evaluations of causal discovery algorithms . Your goal is to create meaningful descriptions for each variable that accurately reflect their role in the graph without revealing their relationships with other variables . Here is the description of the randomly generated causal graph : < causal_graph > { CAUSAL_GRAPH_DESCRIPTION } </ causal_graph > To complete this task , follow these steps : 1. Carefully read and analyze the causal graph description . 2. For each variable mentioned in the graph , create a description that : a ) Precisely describes its meaning within the context of the graph b ) Does NOT spoil its relationships with other variables , either explicitly or implicitly c ) Maintains a consistent context or scenario across all variable descriptions 3. If a variable has different contextual meanings in its relationships with other variables , provide a general description that could encompass these different meanings without revealing the specific relationships . 4. After generating all variable descriptions , assess the quality of the random causal graph based on how well the variables ’ meanings align with each other and how realistic the overall scenario is . 5. Present your output in the following format : Leveraging Large Language Models for Causal Discovery 

< title > [ Provide a concise title for the causal graph ] </ title > < variable_descriptions > [ List each variable and its description ] </ variable_descriptions > < graph_quality_assessment > [ Provide your assessment of the graph quality , including how well the variables ’ meanings align and how realistic the overall scenario is ] </ graph_quality_assessment > Remember , your primary goal is to create meaningful descriptions without revealing any causal relationships . Be creative in developing a consistent context that could plausibly connect all the variables . 

 

A.2 LLM constraints elicitation prompt 

Below, we provide the principal prompt used in our proposed LLM elicitation pipeline. Note that this prompt significantly differs from prior work on LLMs and causal discovery, e.g. (Jiralerspong et al., 2024; Kıcıman et al., 2023), since it involves a single call for all variables in the dataset. This is enabled by the much longer context window of modern LLMs (e.g., over 1 million tokens for gemini-2.5-flash ). This single-call approach is not only more computationally efficient but also allows the LLM to reason holistically about the entire system of variables, potentially capturing higher-order interactions that pairwise queries might miss. 

 

You will be analyzing a set of causal variables to determine the required and forbidden causal directions between them . Your goal is to achieve very high precision in identifying these relationships . < causal_variables > { CAUSAL_VARIABLES } </ causal_variables > Your task is to analyze these causal variables and generate two sets : 1. ** Required directions **: Causal relationships that must exist based on logical necessity , temporal ordering , or fundamental causal principles 2. ** Forbidden directions **: Causal relationships that cannot exist due to logical impossibility , temporal constraints , or definitional contradictions Here are the key principles to follow : ** Required directions ** should include : - Relationships where one variable definitionally or logically must cause another - Temporal precedence relationships ( causes must precede effects ) - Relationships where the causal mechanism is well - established and unavoidable ** Forbidden directions ** should include : - Relationships that would violate temporal ordering ( effects cannot cause their own causes ) - Relationships that are logically contradictory or definitionally impossible - Relationships where the direction would violate established causal mechanisms ** Important guidelines :** - Only include relationships where you have very high confidence - When in doubt about a relationship , do not include it in either set - Consider both direct and indirect causal pathways - Be precise about the direction of causality ( A → B is different from B → A ) Format your final answer as follows : ** Required Directions :** Li, Russo                                                               

> -[ Variable A ] →[ Variable B ]: [ Brief justification ] -[ Continue for all required directions ] ** Forbidden Directions :** -[ Variable C ] →[ Variable D ]: [ Brief justification ] -[ Continue for all forbidden directions ] Your final answer should only include the Required Directions and Forbidden Directions sections with their respective causal relationships and justifications . Aim for very high precision -only include relationships where you are highly confident in the causal direction requirement or prohibition .

 

A.3 LLM Consensus Details 

The consensus mechanism is designed to generate high-precision causal constraints by aggregating the outputs from multiple independent LLM queries. This is due to the inherent stochasticity of LLMs, and inspired by the majority voting strategy proposed by (Cohrs et al., 2023). For each causal discovery problem, the LLM is queried five times, yielding five distinct sets of constraints. Let 

Ri and Fi represent the set of “required” and “forbidden” arrows, respectively, from run i ∈ { 1, . . . , 5}. The final high-confidence consensus sets are obtained by taking the intersection across the five runs: Rconsensus = T5 

> i=1

Ri

and Fconsensus = T5 

> i=1

Fi. This guarantees that an arrow appears in the final constraints only if it was proposed unanimously across all independent evaluations. The repetition count of five was chosen empirically to balance precision and recall, with enough repeats to filter out stochastic or low-confidence suggestions while avoiding excessive computational cost. The conservative intersection rule therefore substantially improves the precision of the resulting constraints, as empirically validated in Appendix C.4. 

# B Graph and Data Generation Details 

The generation of our synthetic datasets is a two-stage process designed to create structurally diverse and semantically plausible causal graphs. First, we generate a structural scaffold (a Directed Acyclic Graph, or DAG) using one of three different topology generation methods. Second, we ground this abstract structure in real-world concepts by finding a matching sub-graph within the CauseNet knowledge graph, guided by specific heuristics. 

B.1 Structural Scaffolding and Graph Types 

We generate the initial DAG topologies using three distinct methods to ensure a variety of graph structures: 

• Erd˝ os-R´ enyi (ER) and Scale-Free (SF): These two methods are adapted from the well-established NOTEARS codebase (Zheng et al., 2018). The ER model (Erd¨ os and R´ enyi, 1959) produces graphs with a random, uniform edge distribution, while the SF model (Barab´ asi and Albert, 1999) generates graphs with power-law degree distributions, characterized by a few high-degree “hub” nodes. 

• Lower Triangle (LT): This custom method constructs a connected DAG by directly populating a lower triangular adjacency matrix. The algorithm proceeds in two stages. First, to guarantee connectivity, it ensures every node from 1 to n − 1 has at least one parent by randomly adding a single incoming edge for each from a node with a lower index. Second, it distributes the remaining specified number of edges by randomly placing them in the unoccupied positions of the lower triangle. This two-step process ensures both connectivity and the desired edge density. While the ER and SF implementations from NOTEARS can produce graphs with a slightly varied number of nodes and edges, the LT method allows for precise control over these parameters. Leveraging Large Language Models for Causal Discovery 

B.2 Semantic Grounding and Heuristics 

Once a structural scaffold is generated, we imbue it with semantic meaning by finding an isomorphic sub-graph within our CauseNet knowledge graph. Since multiple isomorphic sub-graphs can exist, we employ one of three heuristics to select the most suitable candidate match. 

• none : This baseline heuristic serves as a control. It performs no optimization and simply selects the first isomorphic sub-graph returned by the search algorithm. 

• degrees : This heuristic aims to find the most specific and least central concepts. It selects the candidate sub-graph that minimises the sum of the in-degrees and out-degrees of all its nodes within the larger CauseNet 

graph. The intuition is that nodes with lower total degrees represent more niche concepts. 

• semantics : This composite heuristic, as detailed in the main paper, selects the candidate sub-graph that minimizes a weighted quality score combining three metrics. For a given candidate subgraph with concept (node) set Sc, the metrics are: 

– Semantic Compactness ( Hcompact ): Measures thematic coherence by calculating the average cosine distance of each concept’s vector embedding from their geometric centroid: 

Hcompact (Sc) = avg v∈S c

 dcos (emb( v), μSc )

where emb( v) is the vector embedding of concept v, μSc is the centroid vector of all concept embeddings in the set Sc, and dcos is the cosine distance. 

– Node Specificity ( Hspec ): Penalises overly general “hub” nodes using the average log-degree of each node in the full CauseNet graph: 

Hspec (Sc) = avg v∈S c (log(deg( v) + 1)) where deg( v) is the degree (sum of in- and out-degrees) of node v in the full CauseNet graph. The logarithm dampens the penalty for extremely high-degree nodes. 

– Structural-Semantic Correlation ( Hcorr ): Ensures alignment between graph topology and semantic relationships. The score to be minimised is based on the Spearman’s rank correlation: 

Hcorr (Sc) = 1 − SpearmanCorr( dgraph , d sem )where dgraph is the set of all pairwise shortest-path distances between nodes in the candidate graph, and dsem is the corresponding set of pairwise cosine distances between their semantic embeddings. A high correlation (low score) indicates that structurally close nodes are also semantically close. The final score to be minimised is a weighted sum Hf inal = w1Hcompact + w2Hspec + w3Hcorr , where the weights wi are customisable. Additionally, our implementation supports user-defined heuristics, allowing researchers to specify custom scoring functions to rank candidate sub-graphs according to their own criteria or domain-specific preferences. 

B.3 Dataset Schema 

The full suite of 54 synthetic datasets was generated by creating one graph for each unique combination of the following parameters: 

• Number of Nodes: {5, 10, 15 }

• Graph Density: Two levels of edge counts for each node size: 

– 5 nodes: 5 and 7 edges 

– 10 nodes: 10 and 15 edges 

– 15 nodes: 15 and 22 edges 

• Graph Type: {ER, SF, LT }

• Heuristic: {none, degrees, semantics }

Some examples of the generated graphs are shown in Figure 5. Li, Russo dag_15_nodes_15_edges_degrees_random   

> lack_of_sleep
> behavioral_problems
> substance_abuse
> chronic_pain
> despair
> social_problems
> alcohol_abuse
> poor_sleep infertility
> emotional_problems
> drug_abuse
> burnout
> responsibilities lack_of_appropriate_supervision
> abandoned_child_syndrome
> dag_10_nodes_15_edges_none_ER
> osteoporosis
> degeneration
> blindness
> neck_pain
> spinal_stenosis
> high_blood_pressure
> eye_problems
> numbness
> brain_damage birth_defect
> dag_5_nodes_5_edges_semantics_SF
> chronic_pain
> irritability
> dysfunction
> tumors
> sleep_apnea

Figure 5: Examples of semantically grounded DAGs from our synthetic data pipeline, generated with different structural methods (ER, SF, LT) and semantic heuristics ( none , degrees , semantics ). 

B.3.1 CPT and Data Generation 

After a semantically grounded DAG is finalised, we use the pyAgrum library to construct the corresponding Bayesian Network. For each variable in the network, the domain size is set to 2, making all variables binary. The Conditional Probability Tables (CPTs) for the BN are randomly generated during the initialisation process. The full collection of the 54 generated Bayesian Networks in BIFXML and PNG format is available in the project’s public repository, allowing for full reproducibility of our experiments. The randomly initialised CPTs are not intended to reflect realistic effect sizes or context-specific independencies. Their sole purpose is to generate observational data that respects the sampled graph structure. Our evaluation targets structural recovery (presence and orientation of edges), not the calibration of causal effect magnitudes. Mismatches between semantic strength and statistical effect size are therefore expected and mirror low-signal regimes encountered in practice. 

# C Details on Experiments 

We provide here additional details on the experimental setup, metrics, and results that were omitted from the main text due to space constraints. Leveraging Large Language Models for Causal Discovery 

C.1 Metrics Definitions 

We evaluate the performance of causal discovery algorithms using standard metrics that capture different aspects of structural accuracy. Let Gtrue = ( V, E true ) be the ground truth and Gest = ( V, E est ) the estimated DAG. 

• Structural Hamming Distance (SHD): The SHD (Tsamardinos et al., 2006) measures the structural difference between two graphs. It is defined as the number of edge operations (additions, deletions, or reversals) required to transform Gest into Gtrue . A lower SHD indicates a better structural match. In our plots, we report the normalised SHD (NSHD), which is the SHD divided by the number of edges in the true graph, to facilitate comparison across graphs of different sizes. 

• Structural Intervention Distance (SID): The SID (Peters and B¨ uhlmann, 2015) is a metric that quan-tifies the difference in the interventional distributions implied by two causal graphs. It counts the number of pairs ( i, j ) for which the causal effect of intervening on node i on node j is incorrectly predicted by Gest 

compared to Gtrue . A lower SID indicates a better match in terms of causal predictions. We report the normalised SID, dividing by the total number of possible interventions. 

• Precision, Recall, and F1-Score: These metrics evaluate the accuracy of the learned graph skeleton (the set of adjacencies, ignoring direction). Let TP (True Positives) be the number of correctly identified edges, FP (False Positives) be the number of incorrectly identified edges, and FN (False Negatives) be the number of missed edges. 

– Precision is the fraction of correctly identified edges among all edges in the estimated graph: Precision = TP TP+FP .

– Recall is the fraction of true edges that were correctly identified: Recall = TP TP+FN .

– F1-Score is the harmonic mean of Precision and Recall, providing a single balanced measure: F 1 = 2 · Precision ·Recall Precision+Recall .

C.2 Baselines 

We used the following six baselines with respective implementations (see Section 6 for context): 

• A Random baseline (RND) as in (Russo et al., 2024), by just sampling 10 random graphs with the same number of nodes as the ground truth. 

• Fast Greedy equivalence Search 4 (FGS) (Ramsey et al., 2017) is a score-based Causal Discovery algorithm. It is a fast implementation of GES (Chickering, 2002) where graphs are evaluated using the Bayesian Infor-mation Criterion (BIC) upon addition or deletion of an edge, in a greedy fashion, involving the evaluation of insertion and removal of edges in a forward and backward fashion. 

• NOTEARS-MLP 5 (NT) (Zheng et al., 2020) learns a non-linear SEM via continuous optimisation. Having a Multi-Layer Perceptron (MLP) at its core, this method should adapt to different functional dependen-cies among the variables. The optimisation is carried out via augmented Lagrangian with a continuous formulation of acyclicity (Zheng et al., 2018). 

• GRaSP (Lam et al., 2022) is an order-based method based on greedy relaxations of the sparsest permutation criterion, providing a recent and competitive baseline for structure learning. 

• BOSS (Andrews et al., 2023) is an order-based score-search method that combines order search with grow– shrink-style neighbourhood exploration for fast DAG discovery. 

• Majority-PC 6 (MPC) (Colombo and Maathuis, 2014) is a constraint-based causal discovery algorithm. It uses independence tests and graphical rules based on d-separation to extract a CPDAG from the data. MPC is an improved version of the original Peter-Clark (PC) algorithm (Spirtes et al., 2000) that renders it order-independent while maintaining soundness and completeness with infinite data. This constitutes the statistical engine underlying ABAPC, isolating the impact of Causal ABA. 

> 4https://github.com/bd2kccd/py-causal
> 5https://github.com/xunzheng/notears
> 6https://github.com/briziorusso/ArgCausalDisco/blob/public/cd_algorithms/PC.py Li, Russo

Figure 6: Precision and Recall on Synthetic Datasets. Bar plots comparing the Precision and Recall of LLM-augmented Causal ABA against baselines across synthetic datasets generated from the CauseNet Knowledge Graph, grouped by number of nodes ( |V| ∈ { 5, 10 , 15 }). Error bars represent standard deviations over 50 repetitions. 

• ABAPC 7 (ABAPC) (Russo et al., 2024) is an instantiation of Causal ABA, a logic- and constraint-based Causal Discovery algorithm that uses an Assumption-Based Argumentation framework to resolve conflicts and ambiguities within observational data. The method uses the CI tests from the MPC algorithm as inputs into the Causal ABA (hence ABAPC) and excludes a progressively bigger amount of low-confidence tests to find a stable extension of the ABAF that contains a DAG which is guaranteed to imply the d-separations supported by the retained tests. This is the non-LLM variant of our proposed method, allowing us to isolate the impact of LLM-derived constraints. 

• LLM-BFS 8 (BFS) (Jiralerspong et al., 2024) is a breadth-first search-based approach that leverages LLMs for causal discovery. It explores the graph structure by iteratively expanding the search space and incorporating LLM-generated insights to build the causal graph, only leveraging variable names and descriptions. This method is the LLM-only baseline, allowing us to isolate the impact of data-driven constraints. 

C.3 Additional Results 

The following sections provide additional results and analyses that complement the findings presented in the main text. We include further performance metrics, statistical test results, and detailed evaluations on the 

bnlearn benchmark datasets. 

C.3.1 Additional Metrics 

Figures 6 and 7 complement Figure 3 from the main text by showing the performance on the synthetic CauseNet 

datasets with respect to Precision, Recall, and SID. The results confirm the trend observed with SHD and F1-score: our proposed ABAPC-LLM method consistently outperforms the baselines across all graph sizes, demonstrating superior precision and recall in edge detection and greater accuracy in predicting interventional effects. 

C.3.2 Statistical Test Results 

Here we provide details for the statistical tests used to measure the significance of the difference in the results presented in Figure 3 in the main text. In Table 2 we provide t-statistics and p-values for all comparisons against 

> 7https://github.com/briziorusso/ArgCausalDisco/blob/public/cd_algorithms/ABAPC.py
> 8https://github.com/superkaiba/causal-llm-bfs Leveraging Large Language Models for Causal Discovery

Figure 7: Structural Intervention Distance on Synthetic Datasets. Bar plots comparing the normalised Structural Intervention Distance (SID) of LLM-augmented Causal ABA against baselines across synthetic datasets generated from the CauseNet Knowledge Graph, grouped by number of nodes ( |V| ∈ { 5, 10 , 15 }). Error bars represent standard deviations over 50 repetitions. Table 1: Average runtime in seconds on synthetic CauseNet datasets (mean ±std over repetitions). LLM-BFS is omitted because its wall-clock time is dominated by external API calls. 

Method |V| = 5 |V| = 10 |V| = 15 Random 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 FGS 0.08 ± 0.00 0.09 ± 0.00 0.10 ± 0.01 MPC 0.00 ± 0.01 0.02 ± 0.01 0.03 ± 0.01 ABAPC 0.16 ± 0.11 3.31 ± 3.70 5.39 ± 5.19 ABAPC-LLM 0.18 ± 0.14 3.28 ± 3.47 5.31 ± 5.14 NOTEARS-MLP 0.11 ± 0.01 0.27 ± 0.05 0.45 ± 0.04 GRaSP 1.09 ± 0.10 5.45 ± 2.67 14 .06 ± 4.80 BOSS 1.29 ± 0.16 7.22 ± 3.20 18 .63 ± 7.05 our proposed method, ABAPC-LLM, on the synthetic CauseNet datasets. LLM-BFS is not included in these tests as it was only run once (due to its high computational cost). For all families of pairwise comparisons, we apply the Benjamini–Hochberg procedure to control the false discovery rate. Corrected p-values are reported in the pBH column of Tables 2 and subsequent tables. All significance claims in the main text refer to BH-corrected results. In the table we present pairwise comparisons of means, for F1 and Normalised SHD, scores presented in Figure 3 of the main text. We use two-sample, unequal variance t-tests, with degrees of freedom of 53 (10 seeds and 4 noise distributions, minus 1). The null hypothesis is that the means of the two samples are equal, and the alternative hypothesis is that they are not equal. We reject the null hypothesis for p-values below 0.05, indicating a statistically significant difference between the two methods being compared. All comparisons show statistically significant differences ( p < 0.001) between ABAPC-LLM and all baselines, across all graph sizes and both metrics. 

C.3.3 Runtime Analysis 

Table 1 reports average wall-clock runtime (in seconds) for all methods on the synthetic datasets, averaged over 50 repetitions. To focus on algorithmic scaling, we report runtimes excluding external LLM API latency; LLM Li, Russo 

Table 2: Two-sample, unequal variance t-tests (ABAPC-LLM vs others) on Synthetic Data for |V | ∈ { 5, 10 , 15 }

with nobs = 900 .0. Metrics: F1 (higher is better) and NSHD (lower is better). Significance levels: 0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ns’ 1. BH-corrected p-values are also reported to account for multiple comparisons.                                                                                                                                                                                                                                                             

> Dataset Metric Methods Means ±Std tp-value pBH
> |V|= 10
> F1
> ABAPC-LLM vs ABAPC 0.66 ±0.09 vs 0 .63 ±0.11 8.05 1.55e-15 *** 1.92e-15 *** ABAPC-LLM vs FGS 0.66 ±0.09 vs 0 .17 ±0.07 134 .20 0 *** 0 *** ABAPC-LLM vs MPC 0.66 ±0.09 vs 0 .36 ±0.13 58 .89 0 *** 0 *** ABAPC-LLM vs NOTEARS 0.66 ±0.09 vs 0 .16 ±0.00 169 .62 0 *** 0 *** ABAPC-LLM vs Random 0.66 ±0.09 vs 0 .19 ±0.08 114 .96 0 *** 0 ***
> NSHD
> ABAPC-LLM vs ABAPC 0.44 ±0.10 vs 0 .47 ±0.11 −6.29 3.92e-10 *** 4.34e-10 *** ABAPC-LLM vs FGS 0.44 ±0.10 vs 1 .31 ±0.15 −146 .55 0 *** 0 *** ABAPC-LLM vs MPC 0.44 ±0.10 vs 0 .88 ±0.14 −77 .21 0 *** 0 *** ABAPC-LLM vs NOTEARS 0.44 ±0.10 vs 1 .00 ±0.01 −174 .07 0 *** 0 *** ABAPC-LLM vs Random 0.44 ±0.10 vs 2 .34 ±0.53 −105 .08 0 *** 0 ***
> |V|= 15
> F1
> ABAPC-LLM vs ABAPC 0.68 ±0.08 vs 0 .62 ±0.10 14 .28 8.84e-44 *** 1.16e-43 *** ABAPC-LLM vs FGS 0.68 ±0.08 vs 0 .11 ±0.04 196 .72 0 *** 0 *** ABAPC-LLM vs MPC 0.68 ±0.08 vs 0 .29 ±0.12 84 .89 0 *** 0 *** ABAPC-LLM vs NOTEARS 0.68 ±0.08 vs 0 .12 ±0.00 224 .45 0 *** 0 *** ABAPC-LLM vs Random 0.68 ±0.08 vs 0 .13 ±0.05 183 .38 0 *** 0 ***
> NSHD
> ABAPC-LLM vs ABAPC 0.38 ±0.07 vs 0 .43 ±0.09 −12 .87 2.93e-36 *** 3.23e-36 *** ABAPC-LLM vs FGS 0.38 ±0.07 vs 1 .33 ±0.12 −201 .95 0 *** 0 *** ABAPC-LLM vs MPC 0.38 ±0.07 vs 0 .84 ±0.10 −107 .23 0 *** 0 *** ABAPC-LLM vs NOTEARS 0.38 ±0.07 vs 0 .93 ±0.01 −222 .07 0 *** 0 *** ABAPC-LLM vs Random 0.38 ±0.07 vs 3 .56 ±1.09 −87 .39 0 *** 0 ***
> |V|= 5
> F1
> ABAPC-LLM vs ABAPC 0.81 ±0.07 vs 0 .68 ±0.08 37 .41 5.92e-226 *** 8.88e-226 *** ABAPC-LLM vs FGS 0.81 ±0.07 vs 0 .32 ±0.10 120 .03 0 *** 0 *** ABAPC-LLM vs MPC 0.81 ±0.07 vs 0 .45 ±0.14 68 .68 0 *** 0 *** ABAPC-LLM vs NOTEARS 0.81 ±0.07 vs 0 .34 ±0.00 198 .82 0 *** 0 *** ABAPC-LLM vs Random 0.81 ±0.07 vs 0 .34 ±0.15 87 .12 0 *** 0 ***
> NSHD
> ABAPC-LLM vs ABAPC 0.21 ±0.08 vs 0 .33 ±0.08 −31 .88 6.47e-177 *** 9.06e-177 *** ABAPC-LLM vs FGS 0.21 ±0.08 vs 0 .96 ±0.18 −112 .64 0 *** 0 *** ABAPC-LLM vs MPC 0.21 ±0.08 vs 0 .83 ±0.12 −132 .23 0 *** 0 *** ABAPC-LLM vs NOTEARS 0.21 ±0.08 vs 0 .89 ±0.01 −264 .60 0 *** 0 *** ABAPC-LLM vs Random 0.21 ±0.08 vs 1 .18 ±0.25 −110 .54 0 *** 0 ***

inference is performed once per dataset and amortised across repetitions. 

C.3.4 Performance on bnlearn Benchmarks 

Figures 8, 9, and 10 show the performance of all methods on the standard benchmark datasets from the bnlearn 

repository. As discussed in the main text, these well-known datasets are likely part of the LLMs’ training data, which may lead to memorisation effects. This is reflected in the strong performance of both ABAPC-LLM and the LLM-only baseline (LLM-BFS) on several datasets like SACHS, ASIA, and CANCER. Nonetheless, our method remains competitive or superior across all metrics, and its performance on datasets like CHILD, which are more complex, demonstrates that the combination of data-driven reasoning and semantic priors is robust. Interesting to note is the 100% recall achieved by LLM-BFS on four of the six datasets (Figure 9), indicating that the LLM was able to identify all true edges in these cases, albeit with lower precision. This suggests that while the LLM can effectively capture the presence of causal relationships, it may also introduce spurious edges, highlighting the importance of combining LLM insights with data-driven methods for balanced performance. Additionally, the SID score of LLM-BFS on these same datasets is 0 (no errors in interventional predictions), which raises suspitions about the performance on these common benchmarks. 

C.4 LLM Constraints Evaluation 

We provide here additional analyses on the quality of the causal constraints generated by the LLM, as well as ablation studies to assess the impact of different components of our approach. In particular, we investigate the role of variable descriptions in enhancing the LLM’s understanding of the causal relationships, and we evaluate Leveraging Large Language Models for Causal Discovery 

Figure 8: Structural Learning Performance on bnlearn benchmarks. Bar plots comparing the normalised Struc-tural Hamming Distance (NSHD, left-axis) and F1-score (right-axis) of our method against baselines across standard benchmark datasets. Error bars represent standard deviations over 50 repetitions. the effectiveness of our consensus mechanism in improving the reliability of the generated constraints. Finally we provide additional heatmaps to illustrate the interaction between constraint quality and structural learning performance. Table 3 summarises the performance of the LLM in generating causal constraints for bnlearn and our CauseNet 

synthetic datasets, comparing the consensus-based approach with a simple average of single runs, and evaluating the impact of providing variable descriptions. Table 3: LLM-derived constraint quality, including average number of constraints, under different elicitation strategies. We compare consensus vs. the average of single runs, and with vs. without descriptions on bnlearn 

and bnlearn . Bold = higher of Average vs. Consensus within each dataset and description column ; ↑ “with desc” > “w/o desc” (same method), ↓ otherwise. Green = overall best across all settings per row.                                                                                                                                                                                                             

> bnlearn CauseNet
> Metric Average Consensus Average Consensus w/o desc with desc w/o desc with desc w/o desc with desc w/o desc with desc
> Forbidden constraints
> # constraints 19 .80 ±13 .90 25 .47 ±24 .39 8.00 ±6.60 8.17 ±1.72 17 .59 ±13 .02 20 .64 ±16 .60 4.85 ±4.83 4.07 ±3.42 Precision 0.927 ±0.186 0.973 ±0.041 ↑0.833 ±0.408 0.948 ±0.085 ↑0.952 ±0.132 0.942 ±0.155 ↓0.855 ±0.339 0.877 ±0.317 ↑
> Recall 0.409 ±0.234 0.472 ±0.219 ↑0.241 ±0.240 0.275 ±0.216 ↑0.293 ±0.216 0.312 ±0.221 ↑0.123 ±0.164 0.115 ±0.168 ↓
> F1 0.533 ±0.260 0.606 ±0.224 ↑0.338 ±0.316 0.392 ±0.281 ↑0.409 ±0.232 0.431 ±0.234 ↑0.189 ±0.216 0.174 ±0.218 ↓
> Required constraints
> # constraints 8.27 ±6.97 10 .80 ±11 .71 4.50 ±3.73 3.67 ±4.13 12 .57 ±9.60 14 .56 ±11 .52 4.11 ±3.29 4.74 ±3.33 Precision 0.468 ±0.288 0.597 ±0.235 ↑0.375 ±0.327 0.505 ±0.422 ↑0.487 ±0.249 0.464 ±0.230 ↓0.568 ±0.353 0.514 ±0.335 ↓
> Recall 0.445 ±0.352 0.611 ±0.328 ↑0.303 ±0.388 0.281 ±0.375 ↓0.486 ±0.255 0.516 ±0.242 ↑0.244 ±0.210 0.278 ±0.244 ↑
> F1 0.401 ±0.240 0.555 ±0.232 ↑0.284 ±0.284 0.321 ±0.337 ↑0.450 ±0.206 0.453 ±0.190 ↑0.314 ±0.228 0.334 ±0.249 ↑

The consensus approach is conservative and often results in a smaller set of constraints. In some cases, it may yield an empty set, which can artificially lower the aggregated performance metrics since they are set to 0 in such instances. To provide a clearer view of the performance when constraints are generated, Table 4 presents the same metrics but excludes cases where the number of generated constraints is zero. This is particularly relevant for the consensus method, which is more prone to this scenario. Based on the results in these tables, we can draw the following observations: Li, Russo 

Figure 9: Precision and Recall on bnlearn benchmarks. Bar plots comparing the Precision and Recall of our method against baselines across standard benchmark datasets. Error bars represent standard deviations over 50 repetitions. Table 4: LLM-derived constraint quality excluding cases with zero constraints. We include average number of constraints, under different elicitation strategies. We compare consensus vs. the average of single runs, and with vs. without descriptions on bnlearn and bnlearn . Bold = higher of Average vs. Consensus within each dataset and description column ; ↑ “with desc” > “w/o desc” (same method), ↓ otherwise. Green = overall best across all settings per row.                                                                                                                                                                                                           

> bnlearn CauseNet
> Metric Average Consensus Average Consensus w/o desc with desc w/o desc with desc w/o desc with desc w/o desc with desc
> Forbidden constraints
> # constraints 20 .48 ±13 .63 25 .47 ±24 .39 9.60 ±5.94 8.17 ±1.72 17 .86 ±12 .94 20 .79 ±16 .57 5.57 ±4.78 4.58 ±3.29 Precision 0.959 ±0.062 0.973 ±0.041 ↑1.000 0.948 ±0.085 ↓0.966 ±0.061 0.949 ±0.132 ↓0.982 ±0.069 0.986 ±0.053 ↑
> Recall 0.423 ±0.225 0.472 ±0.219 ↑0.289 ±0.234 0.275 ±0.216 ↓0.297 ±0.215 0.314 ±0.220 ↑0.141 ±0.168 0.129 ±0.172 ↓
> F1 0.551 ±0.244 0.606 ±0.224 ↑0.405 ±0.300 0.392 ±0.281 ↓0.415 ±0.228 0.434 ±0.231 ↑0.217 ±0.218 0.196 ±0.222 ↓
> Required constraints
> # constraints 9.92 ±6.45 11 .17 ±11 .74 6.75 ±1.71 5.50 ±3.87 12 .66 ±9.57 14 .66 ±11 .50 4.53 ±3.17 5.02 ±3.22 Precision 0.562 ±0.213 0.618 ±0.210 ↑0.562 ±0.195 0.757 ±0.206 ↑0.491 ±0.247 0.468 ±0.227 ↓0.626 ±0.317 0.544 ±0.320 ↓
> Recall 0.535 ±0.316 0.632 ±0.312 ↑0.454 ±0.399 0.422 ±0.394 ↓0.490 ±0.252 0.520 ±0.239 ↑0.269 ±0.204 0.294 ±0.241 ↑
> F1 0.482 ±0.172 0.574 ±0.211 ↑0.426 ±0.233 0.481 ±0.294 ↑0.454 ±0.203 0.457 ±0.187 ↑0.346 ±0.214 0.354 ±0.242 ↑

• Impact of Descriptions: Providing semantic descriptions alongside variable names generally improves the performance of the LLM in generating both forbidden and required constraints. As shown in Table 3, for forbidden constraints, descriptions lead to a notable increase in F1-score for both bnlearn (from 0.53 to 0.61 for average, and 0.34 to 0.39 for consensus) and synthetic datasets. A similar, even more pronounced, improvement is observed for required constraints, where the F1-score on bnlearn jumps from 0.4 to 0.56 for the average method. This suggests that richer context allows LLMs to make more accurate causal judgments. 

• Consensus vs. Average of Single Runs: The consensus mechanism acts as a high-precision filter. It generates a smaller number of constraints compared to the average of single runs, but with significantly higher precision when zero-constraint cases are excluded (Table 4). For instance, for forbidden constraints on bnlearn without descriptions, consensus precision reaches 1. However, this comes at the cost of lower recall, resulting in a lower F1-score overall compared to the average method. This highlights a trade-off between precision and recall: consensus is preferable when the priority is to avoid false positives, while averaging single runs yields a broader, higher-recall set of constraints. Leveraging Large Language Models for Causal Discovery 

Figure 10: Interventional Accuracy on bnlearn benchmarks. Bar plots comparing the normalised Structural Intervention Distance (SID) of our method against baselines across standard benchmark datasets. Error bars represent standard deviations over 50 repetitions. 

• bnlearn vs. CauseNet Datasets: The LLM generally performs better on the bnlearn datasets compared to the synthetic CauseNet ones, especially shown in terms of F1 in Table 4. This is likely due to the memorisa-tion of common benchmarks present in the LLM’s training data. The performance on the synthetic datasets, while lower, provides a more realistic measure of the LLM’s generalisable causal reasoning capabilities, and shows that the LLM is still capable of generating high-quality constraints for novel problems. 

C.4.1 Interaction Between Constraint Quality and Structural Performance 

Figures 12 and 13 show heatmaps of the interaction between the F1-score of LLM-derived constraints and data-derived independence tests, and their impact on the final DAG F1-score. These complement Figure 4 from the main text by providing a more granular view of how the quality of constraints influences structural learning performance across different dataset characteristics. The results indicate that high-quality constraints from the LLM can significantly enhance the performance of Causal ABA, especially when combined with reliable data-driven tests. The effects do not show distinctive trends across sizes or graph types, as shown in Figures 12 and 13, respectively. 

LLMs performance on CauseNet vs bnlearn . Figure 11 shows a side-by-side comparison of the interaction heatmaps on the CauseNet and bnlearn datasets. Figure 11a is the same as the one in the main text, while Figure 11b shows the results for the bnlearn benchmarks. We can see that the trends are consistent across both synthetic and real-world datasets, but the overall accuracy of LLM-derived constraints is higher in the bnlearn 

benchmarks, likely due to memorisation effects. Figure 14 provides a breakdown of the interaction heatmap for each individual bnlearn dataset using the same binning thresholds as the global heatmap in Figure 11b: [0,0.33), [0.33,0.66), [0.66,1]. This is the same as Figure 12 and 13 for the synthetic datasets. We can notice a much lower diversity of LLM-derived constraint quality within datasets, with high concentration in one or two bins. This is likely due to the smaller number of variables in these datasets, which makes it easier for the LLM to memorise the relationships. Finally, Figure 15 shows the same breakdown but using quantiles for each dataset to define the bins. This allows us to see more clearly the interaction between constraint quality and structural performance within each dataset, without being affected by the overall accuracy of the LLM on that dataset. We can see that even in datasets where the LLM show lower overall performance (e.g. CHILD ), there are still instances where high-quality constraints lead to improved structural learning performance. Li, Russo      

> (a) CauseNet synthetic datasets. (b) bnlearn benchmarks.

Figure 11: Interaction between the F1-score of LLM-derived constraints and data-derived independence tests; color denotes the mean change in the final DAG F1-score. 

Figure 12: CauseNet Synthetic: Heatmaps of the interaction between the F1 of LLM-derived and data-derived constraints, split by number of nodes. 

Figure 13: CauseNet Synthetic: Heatmaps of the interaction between the F1 of LLM-derived and data-derived constraints, split by graph type. Leveraging Large Language Models for Causal Discovery 

Figure 14: bnlearn Benchmarks: Heatmap of the interaction between the F1-score of LLM-derived constraints and data-derived independence tests per dataset using the same binning threshold as the global heatmap in Figure 11b: [0,0.33), [0.33,0.66), [0.66,1]. 

Figure 15: bnlearn Benchmarks: Heatmap of the interaction between the F1-score of LLM-derived constraints and data-derived independence tests per dataset using quantiles for each dataset to define the bins.