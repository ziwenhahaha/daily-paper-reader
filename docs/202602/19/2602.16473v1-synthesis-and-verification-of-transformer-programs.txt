Title: Synthesis and Verification of Transformer Programs

URL Source: https://arxiv.org/pdf/2602.16473v1

Published Time: Thu, 19 Feb 2026 01:48:39 GMT

Number of Pages: 25

Markdown Content:
# Synthesis and Verification of Transformer Programs 

## Hongjian Jiang 1, Matthew Hague 2, Philipp R¨ ummer 3,4, and Anthony W. Lin 1,5 

> 1

University of Kaiserslautern-Landau, Germany 

hongjian.jiang@rptu.de  

> 2

Royal Holloway, University of London, England 

matthew.hague@rhul.ac.uk  

> 3

Uppsala University, Sweden 

philipp.ruemmer@it.uu.se  

> 4

University of Regensburg, Germany  

> 5

MPI-SWS, Germany 

awlin@mpi-sws.org 

Abstract 

C-RASP is a simple programming language that was recently shown to capture concepts expressible by transformers. In this paper, we develop new algorithmic techniques for automatically verifying C-RASPs. To this end, we establish a connection to the verification of synchronous dataflow programs in Lustre, which enables us to exploit state-of-the-art model checkers utilizing highly optimized SMT-solvers. Our second contribution addresses learning a C-RASP program in the first place. To this end, we provide a new algorithm for learning a C-RASP from examples using local search. We demonstrate efficacy of our implementation for benchmarks of C-RASPs in the literature, in particular in connection to the following applications: (1) transformer program minimization, and (2) constrained learning of transformer programs (based on a partial specification). 

# 1 Introduction 

Transformers [27] are the underlying neural network architectures behind Large Language Mod-els (LLMs), using attention to aggregate information in the input sequence. Although trans-formers are expressive and at the same time efficiently parallelizable [2, 12, 32, 33, 25], they are rather tricky to analyze. In particular, verifying transformers is undecidable [24, 26]. Moreover, equipped with Chain of Thoughts (i.e. scratchpads for doing intermediate computation), they become Turing-complete [23, 20, 13]. To better understand the behavior of transformers, researchers developed declarative lan-guages which enjoy precisely defined semantics and at the same time faithfully capture the be-havior of transformers. In 2021 Weiss et al. [28] proposed the influential RASP (Restricted Ac-cess Sequence Processing Language) paradigm. Essentially, RASP is a programming language that operates on sequences via operations like selection, aggregation, and simple position-wise arithmetic operations. Over the years the RASP paradigm has been instantiated and formally proven to model specific classes of transformers. C-RASP (Counting RASP) [32] is the most recent variant of RASP that has been shown (both theoretically and experimentally) by [12] to capture real-world transformers. C-RASP is essentially a refinement of Counting Linear Temporal Logic [2], but without features that are not expressible by real-world transformers. More precisely, C-RASP allows a term of the form # φ,which counts the number of positions j to the left of the current position i where the formula 

φ holds. In this way, the Dyck-1 language of balanced parentheses over the alphabet {[, ]} is expressible in C-RASP; more on this in Section 2. In addition, C-RASP can express languages  

> arXiv:2602.16473v1 [cs.LG] 18 Feb 2026 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin

like MAJORITY (i.e. the input string contains more a’s than b’s), but cannot express the flip-flop language [17] — essentially, the regular language Σ ∗be ∗, for some letters b, e in the alphabet Σ — as well as PARITY (i.e. the number of a’s in the input string is even), both of which are not trainable by transformers. [12] showed that C-RASP is captured by length-generalizable transformers (in that a transformers can learn a target language, given a finite training dataset). In particular, C-RASP is conjectured by [12] to capture all length-generalizable transformer languages; this is a formal version of the so-called RASP conjecture [34]. Despite tight connections between C-RASP and transformers, the role of C-RASP has mostly been to help understand the expressivity of transformers. Here, we study the role of C-RASP as an interpretable surrogate model of transformers. “Interpretability” is a general concept in AI (e.g. see [21]), but for C-RASP (which has a formally defined semantics) it is best understood as the existence of an automatic method to analyze C-RASP programs. In particular, this is consistent with the term “interpretability” and “explainability” in FXAI (Formal eXplainable AI) [19] and investigations in the formal aspects of AI (e.g. [22, 3, 29, 9, 1]). To this end, there are two main challenges. The first concerns automatic verification , since hitherto no automatic method to analyze C-RASP exists. In fact, checking whether the language generated by a given C-RASP is nonempty is undecidable [30]. The second concerns synthesis , since there is no known method to actually learn a C-RASP program (from examples, or otherwise). 

Contributions: We develop the first techniques that address both the verification and synthesis of C-RASP programs. We have implemented them and shown their efficacy on an assortment of benchmarks from the literature [12, 31], in relation to the following applications: (1) transformer program minimization, and (2) constrained learning of transformer programs. Our first contribution is the first automatic method for verifying C-RASP programs . In par-ticular, we provide a reduction to the verification of Lustre , which is a synchronous datastream language with precise syntax and semantics. Since Lustre is supported by highly optimized model checkers (e.g. Kind2 Model Checker [6] with plugins to fast SMT solvers like CVC5 and Z3), we obtain an automatic method for verifying C-RASP programs against properties like language equivalence, inclusion, and emptiness. Our second contribution is the first automatic method for synthesizing C-RASP programs 

from positive and negative examples. The method exploits local search in the form of simulated annealing. This is orthogonal to stochastic gradient descent for training transformers, which might not yield an implementable method even since the current translations (e.g. see [32, 31]) work only from a specific class of idealized transformers. We have implemented both methods and demonstrate their efficacy against two applications: (i) C-RASP minimization (given a C-RASP, construct a smaller C-RASP) and (ii) constrained learning of C-RASP from a partial specification. In particular, we have tested our verifier against existing C-RASP benchmarks in the literature (e.g. [12, 31]), and show highly promising experimental results (see Table 1). In summary, our tool uses for both synthesis and verification of each benchmark a total of at most 300 seconds. In contrast, transformer training (e.g., GPT2) using the HuggingFace transformer library in PyTorch for some of these benchmarks alone could take up to several days to achieve the correct language! 

Organization: We define C-RASP and provide examples in Section 2. In Section 3, we define Lustre and a reduction from C-RASP to Lustre. Our synthesis algorithm is in Section 4. Experimental results and applications are in Section 5. We conclude with discussion and related work in Section 6. 2Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

# 2 Preliminaries 

Before giving the full definition of C-RASP, we begin with an example. Let Σ be a finite nonempty alphabet and Σ + denote nonempty words 1 a1 . . . a n. Let ⊤ and ⊥ denote the Boolean values true and false. C-RASP programs are given by a sequence of rules that are evaluated over each position of an input word from Σ +. Rules may evaluate to Boolean or integer values, referred to as Boolean and counting rules, respectively. The C-RASP program below recognizes the Dyck(-1) language; that is, the language of words over Σ = {[, ]} where each [ is matched by an ] in a well-nested fashion. For example [], [[]], and [[][]] are Dyck words, []] and ][ are not. 

Example 1. The following C-RASP program contains four rules and recognizes Dyck words. It is explained below. 

C[ := #[ C] := #] 

V := C[ < C ] D := #V = 0 ∧ C[ = C]

The counting rules rule C[ and C] count the number of [ and ] characters respectively. We use the Boolean rule V to detect violations of well-nestedness. That is, V is true at a position of the input word if the number of [ characters is less than the number of ] characters. Finally, D

is true only at the end of a Dyck word. It requires that the word contains no violations of the well-nested property and ultimately has the same number of [ and ]. The table below shows the full execution of the program over the word [[][]]][] which is not a Dyck word. 

[ [ ] [ ] ] ] [ ]

C[ 1 2 2 3 3 3 3 4 4

C] 0 0 1 1 2 3 4 4 5

V ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ ⊤ ⊥ ⊥

D ⊥ ⊥ ⊥ ⊥ ⊥ ⊤ ⊥ ⊥ ⊥

A word is accepted if the value of the last rule ( D) is ⊤ at the final position of the word. 

In the sequel, we follow closely the definition of C-RASP from [12], which also allows local and periodic positional encodings. 

Definition 2.1 (C-RASP) . A C-RASP program P over a finite alphabet Σ is a finite sequence of rules P = ( R1, . . . , R k) where each Ri is either a Boolean rule or a Count rule of the form 

B := BExp or C := CExp respectively. B and C are the names of the rules. To recognize languages, Rk is a Boolean rule that determines acceptance. 

Definition 2.2 (Boolean Expressions) . The syntax of Boolean expressions is given by the following grammar, in which a ∈ Σ is an atomic letter predicate, B is the name of a Boolean rule, ⋄ ∈ {∧ , ∨} denotes Boolean conjunction and disjunction, ▷◁ ∈ { =, ≤, < } denotes a comparison between counting expressions, and o < m ∈ N:BExp ::= ⊤ | ⊥ | a | B | ¬ BExp | BExp ⋄ BExp 

| CExp ▷◁ CExp | m%o. 

The semantics of Boolean expressions is as expected; in particular, B evaluates to the value of the referenced rule and m%o is true at positions j satisfying j mod m = o.

> 1We use “words” and “strings” synonymously in the paper, as as is standard in formal language theory.

3Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

Definition 2.3 (Counting Expressions) . The syntax of counting expressions is defined induc-tively as follows, where k ∈ N, C is the name of a counting rule, and rs < r e ∈ N:CExp ::= k | C | #( BExp ) | #rs,r e (BExp )

| CExp + CExp | CExp − CExp 

| min( CExp , CExp ) | max( CExp , CExp )

| BExp ? CExp : CExp .

Here, at position j, #( BExp ) denotes the number of positions satisfying the Boolean ex-pression at or before position j. The expression # rs,r e (BExp ) is a variant that only counts positions occurring between rs and re positions before the current position. The conditional expression eb ? e1 : e2 evaluates to e1 if the Boolean expression eb holds and to e2 otherwise. Constants k ∈ N denote fixed integer values and C evaluates to the value of the referenced rule. Counting expressions thus provide numerical summaries of Boolean rules, which can be compared in Boolean expressions to form complex logical conditions. The expressions m%o and #rs,r e are extensions of C-RASP to handle LOCAL and PERIODIC positional predicates [12]. C-RASP programs are evaluated over words. Given a word w = a1 . . . a n ∈ Σ+ we induc-tively define the value of expressions. Let each rule be of the form Ri = V := eV , that is, eV is the expression associated to the rule named V . We define νw : ( BExp × { 1, . . . , n } → {⊤ , ⊥} ) ∪

(CExp × { 1, . . . , n } → Z) to be the value of the expression at position j in the word. The full definition is given in the supplementary material (e.g. νw(e1 ∧ e2, j ) := νw(e1, j ) ∧ νw(e2, j )). Here we show only a selection of interesting cases. 

νw(a, j ) := ⊤ if a = aj and ⊥ otherwise 

νw(B, j ) := νw(eB , j )

νw(m%o, j ) := j mod m = oνw(C, j ) := νw(eC , j )

νw(# e, j ) := X

> i≤j

νw(e ? 1 : 0 , i )

νw(# rs,r e e, j ) := X

> j−re≤i≤j−rs

νw(e ? 1 : 0 , i )

νw(e1 ? e2 : e3, j ) := 

(

νw(e2, j ) if νw(e1, j ) = ⊤

νw(e3, j ) otherwise. A word w = a1 . . . a n ∈ Σ+ is accepted by a C-RASP program ( R1, . . . , R k) if νw(B, n ) = ⊤

where Rk = B := e. [Remark: for simplicity, we do not allow an empty input, but this is easy to handle, e.g., by allowing an end-of-string symbol (see [32]).] 

# 3 Verification of C-RASP Programs 

We verify properties of C-RASP programs via a translation to the synthronous dataflow lan-guage Lustre [5], which is tailored to implementing embedded systems. Properties of Lustre programs can be checked by highly optimized tools such as Kind2 [6]. 4Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

## 3.1 Lustre 

We do not present Lustre in full, but show a fragment that is sufficient to show how C-RASP can be simulated. We will begin with an example to show how Lustre programs can recognize Dyck words. The program operates in a similar way to the C-RASP program of Example 1. In the full definition, a Lustre program contains a set N1, . . . , N n of nodes. For simplicity of presentation we will use a single node. Intuitively, nodes can be used to define functions that are reusable and aid readability. Each node has input, output, and local variables. Our node will have one input variable (the input word) and no output variables. We use two types: BOOL and INT. Each variable represents an infinite stream of values of the declared type and will broadly correspond to a single C-RASP rule. Local and output variables are defined using standard mathematical and Boolean combina-tions of variables and constants. For example X := ¬Y means that at position i in the stream, 

X takes the negation of the value of Y at position i. The pre operator accesses the value of a variable at the previous position. The → operator can be used to replace the initial value of a stream. A typical example of these two operators is X := 1 → pre( X) + 1, which defines the sequence 1 , 2, 3, . . . where 1 replaces the initial (undefined) value of pre( X) + 1. Kind2 supports a check keyword to verify that a given Boolean expression evaluates to true at all positions. 

Example 2. The following Lustre program recognizes Dyck words. It has an input variable I

of type INT and local variables Nl, Cl, Nr , Cr , NV , and CV of type INT and V and D of type 

BOOL . D is designated as an output variable. For this example we will make the simplifying assumption that I only takes integer values 0 or 1 (corresponding to [ and ]). Our full encoding will need to enforce this assumption and use end of stream markers. The equations defining each variable are shown below and then explained.   

> node DyckCounters(I: int) returns (D: bool); var Nl, Nr, Cl, Cr, Nv, Cv : int; V : bool; let Nl = if I = 0 then 1 else 0; Nr = if I = 1 then 1 else 0; Cl = (0 -> pre(Cl)) + Nl; Cr = (0 -> pre(Cr)) + Nr; V= Cl < Cr; Nv = if V then 1 else 0; Cv = (0 -> pre(Cv)) + Nv; D= (Cv = 0) and (Cl = Cr); tel

The equations Nl, Nr and NV convert Boolean values into an integer. Cl, Cr then count how many times the input was [ or ]. V is true whenever the property Cl ≥ Cr is violated and 

CV counts how many times a violation has occurred. Finally D is true at all positions that have not seen any violations in the past and are currently well-balanced. The table below shows an execution over the infinite stream 00101000 . . . that represents [[][]]]]] . . . . The value of D is ⊤

5Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

when the prefix of the stream is a Dyck word. 

0 0 1 0 1 1 1 . . . Nl 1 1 0 1 0 0 0 . . . Nr 0 0 1 0 1 1 1 . . . Cl 1 2 2 3 3 3 3 . . . Cr 0 0 1 1 2 3 4 . . . V ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ ⊤ . . . NV 0 0 0 0 0 0 1 . . . CV 0 0 0 0 0 0 1 . . . D ⊥ ⊥ ⊥ ⊥ ⊥ ⊤ ⊥ . . . 

Adding a check statement check ¬(Cr = 3) ∨ D would require the property that whenever three 

] characters have been seen, then the word so far is a Dyck word. This property is true of the stream above, but not true of all input streams. 

Definition 3.1 (Lustre (fragment)) . A Lustre program in our fragment is a node with: one in-put variable I of type INT ; local variables B1, . . . , B mb of type BOOL ; local variables C1, . . . , C mc

of type INT ; one equation for each local variable; and a check statement. Let B = {B1, . . . , B mb } and C = {I, C 1, . . . , C mc } denote the Boolean and integer variables. Boolean variables are defined using equations B := LBExp and integer variables with C := 

LIExp where LBExp ::= ⊤ | ⊥ | B | ¬ LBExp 

| LBExp ⋄b LBExp | LIExp ▷◁ b LIExp 

| pre( LBExp ) | LBExp → LBExp with B ∈ B , ⋄b ∈ {∧ , ∨} and ▷◁ b∈ { <, =, ≤} and LIExp ::= k | C | LIExp ⋄i LIExp 

| if LBExp then LIExp else LIExp 

| pre( LIExp ) | LIExp → LIExp with C ∈ C , ⋄b ∈ { +, −, mod } and k ∈ Z. The check statement is of the form check LBExp. 

Variables hold infinite streams of integers or Booleans. Let ν : ( B × N → {⊤ , ⊥} ) ∪ (C × N →

Z) be a valuation of the Boolean and integer variables. That is ν(V, i ) is the value of the ith position of the stream assigned to the variable V . We generalize ν to expressions e. The full generalization is given in the supplementary material (e.g. ν(e1 ∧ e2, i ) = ν(e1, i ) ∧ ν(e2, i )). Interesting cases are given below. 

ν(pre( e), i ) = 

(

ν(e, i − 1) when i > 0undefined otherwise 

ν(e1 → e2, i ) = 

(

ν(e1, i ) if i = 0 

ν(e2, i ) otherwise 

ν(if e1 then e2 else e3, i ) = 

(

ν(e2, i ) if ν(e1, i ) = ⊤

ν(e3, i ) otherwise Each equation V := e means V takes a value such that ν(V, i ) = ν(e, i ) for all i. An assertion check e is violated if there exists some assignment ν consistent with the equations of the program 6Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

such that for some i we have ν(e, i ) = ⊥. We say a Lustre program is valid if no check statement is violated by any value of the input variable I.While the semantics is largely straightforward, it is worth considering some examples of pre and →. An expression 1 → 2 defines a stream 1 , 2, 2, 2, . . . . However, notice that the expression 1 → (1 → 2) also defines 1 , 2, 2, 2, . . . as → only changes the first value. To define a stream 1, 2, 3, 4, 4, 4, . . . we use 1 → pre(2 → pre(3 → 4)). 

## 3.2 C-RASP to Lustre 

We encode a C-RASP program ( R1, . . . , R k) over an alphabet Σ into Lustre as follows. An outline of the translation can be seen by comparing Example 1 and Example 2. Consider the C-RASP expression # V . This is simulated in Lustre by introducing new variables NV and CV

with NV := if V then 1 else 0 

CV := NV → pre (CV ) + NV .

The expression # V is then translated directly to CV . Most other C-RASP constructs allow almost direct translations, with some additional subtlety for the counting operators. We also need to bridge the gap between C-RASP operating over finite words and Lustre over infinite streams of integers. The translations are given below. 

3.2.1 Representing the Input Word 

We introduce two fresh symbols # (end of stream) and Ω (eternity). Let Σ ′ = Σ ∪ { #, Ω}.Choose any injective map ι : Σ ′ → Z. A finite word a1 . . . a n will be encoded by an infinite stream ι(a1) . . . ι (an)ι(#) ι(Ω) ι(Ω) ι(Ω) . . . .For convenience, let I ∈ S for a set S ⊆ Σ′ denote W 

> a∈S

I = ι(a). Similarly I = a denotes 

I = ι(a). We introduce a new Boolean Lustre variable BI and assign it the following Boolean expres-sion that checks at each position i if I is a correct input word. It asserts that each position encodes a character. Then, it asserts that the first symbol is not Ω and Ω is the only character seen after #. Let RI =

BI := I ∈ Σ′ ∧ ¬ (I = Ω) → (¬pre( I ∈ { #, Ω}) ∨ I = Ω) .

We introduce a second variable BˆI to track whether BI was false at any point in the word. Let 

RˆI = BˆI := BI → (BI ∧ pre( BˆI )). Finally, let RI = {RI , R ˆI }.

3.2.2 Translating C-RASP Rules 

Let ( R1, . . . , R k) be a C-RASP program. For each Boolean rule Ri = B := e, introduce a Lustre variable VB that is of type BOOL. Similarly, introduce a variable VC of type INT for each counting rule Ri = C := e. For each C-RASP rule Ri = X := e we define T (Ri) to be a set of equations including the rule VX := T (e) and possibly some auxiliary equations. We define T (e) in full in the supplementary material (e.g. T (¬e) := ¬T (e)). Interesting cases are shown here. For Boolean expressions (recall that I = a denotes I = ι(a)) 

T (a) := I = a T (B) := VB

T (m%o) := P mod m = o T (C) := VC

T (#( e)) := C#( e) T (# rs,r e (e)) := C#rs,re (e)

7Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

where P is a fresh integer variable encoding the current position. It uses an auxiliary equation 

P := 0 → pre( P ) + 1. C#( e), C#rs,re (e), Ce1 , and Ce2 are fresh integer variables defined via the auxiliary equations. The auxiliary equations of T (Ri) for the examples shown are as follows. For each instance of C#( e) we create a fresh a integer variable Ce that is 0 when e is false and 1 when e is true. We add the equations 

Ce := if T (e) then 1 else 0 

C#( e) := Ce → pre( C#( e)) + Ce

For C#rs,re (e) we use the shorthand pre i to define a “safe” i-fold application of pre that evaluates to 0 if there are fewer than i previous positions. That is pre 0(e) = e and pre i(e) = 0 → pre i−1(e)when i > 0. We also create a fresh integer variable Ce defined as above. We then add 

C#rs,re (e) := X

> re≤i≤rs

pre i(Ce).

3.2.3 Checking Properties of C-RASP 

We can verify a number of properties of C-RASP programs by translation to Lustre. We show here how to check language inclusion. A similar encoding can check equality Given two C-RASP programs P1 = ( R11, . . . , R 1 

> k1

) and P2 = ( R21, . . . , R 2 

> k2

) we can check whether P1 only accepts words that are also accepted by P2. We assume that P1 and P2 have disjoint rule names and translate P1 and P2 to Lustre. The only shared variable between the programs is the input variable I. Suppose V1 and V2 are the variables corresponding to the output equations of the two programs. Notice that, in C-RASP, at least one character is needed to determine the value of the output equations. The following check constraint asserts that after the first position, either the input I is incorrect, the end of the stream marker has not been reached, or P2 accepts whenever P1 does. check ⊤ → (¬(BˆI ∧ I = #) ∨ ¬ pre( V1) ∨ pre( V2)) If the property does not hold, Kind2 reports a witnessing trace of I. This trace gives counter-example word after applying the inverse of ι and removing # and Ω. We define T⊆(P1, P w) to be the Lustre program with the equations RI ∪S 

> i

T (R1 

> i

)∪S 

> i

T (R2 

> i

)and the check statement defined above. 

Proposition 3.2. Take C-RASP programs P1 and P2. P1 accepts only words accepted by P2

if T⊆(P1, P 2) is valid. 

The translation from C-RASP to Lustre is polynomial except for # rs,r e . To translate # rs,r e

we use pre i(Ce) for each re ≤ i ≤ rs. If rs and re are encoded in binary, the translation is exponential. Hence, we assume that rs and re are given in unary. We do not need a similar restriction for m%o.

Proposition 3.3. Take C-RASP programs P1 and P2. Assume each rs and re appearing in the #rs,r e operators is encoded in unary. The Lustre program T⊆(P1, P 2) is of size polynomial in the size of P1 and P2.

8Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

# 4 Synthesis of C-RASP Programs 

We synthesize C-RASP programs with a local search procedure based on simulated annealing [15, 8]. It starts from an initial program and repeatedly proposes a neighboring program obtained by applying syntactic mutations. Given a set of words labeled as positive and negative samples, we aim to synthesize a program that is consistent with the samples while remaining small and structurally simple. 

State Space A C-RASP program over an alphabet Σ is written as a finite sequence P =(R1, . . . , R k) where each Ri is either a Boolean rule or a Count rule, and Rk is the distinguished Boolean rule that determines language acceptance. To bias the search toward a concise and interpretable model, we restrict the search to programs of a fixed program shape . Formally, a shape is a tuple 

σ = ( Nb, N c, K ),

where Nb and Nc are the number of Boolean and Counting rules, respectively, and K bounds the range of numeric constants that may appear in Counting expressions. Only programs that satisfy these bounds are considered during search. Let Pσ denote the set of all well-formed C-RASP programs over Σ that contain exactly Nb Boolean rules and Nc counting rules and use only constants from {0, . . . , K }. The local search explores the finite state space Pσ .

Mutation Operator and Neighborhood Local search proceeds by repeatedly modifying the current program using a stochastic mutation operator. The mutation operator induces the neighborhood relation explored by simulated annealing. The mutation operator defines a Markov transition 

Mutate : Pσ → P σ → R

mapping each program P to a distribution Mutate (P ) over the successor programs; we require 

Mutate (P )( P ′) ≥ 0 and P  

> P′∈P σ

Mutate (P )( P ′) = 1 for all P, P ′ ∈ P σ . The neighborhood of a program P is defined as 

Nσ (P ) = { P ′ ∈ P σ | Mutate (P )( P ′) > 0 }.

Each neighbour differs from P by a single conservative rewrite of the right-hand side of one rule. Although individual mutations are local, the neighborhood is sufficiently rich that any two programs of the same program shape can be connected by a finite sequence of mutations. At each iteration, the mutator selects either a Boolean rule or a Counting rule in a pro-gram P , rewrites the right-hand side of the rule, and removes rules that are unreachable from the final Boolean expression Rk. All mutations preserve well-formedness and acyclicity by construction. 

Boolean Rule Mutation Boolean rules are mutated by resampling or locally modifying their right-hand sides. The mutation operator only uses a non-nested fragment of expressions to bias the search toward simple programs. In particular, Boolean expressions are limited to expressions in which logical connectives and negation may be applied only to Boolean atoms, and nesting of Boolean operators is disallowed. This restriction ensures that the program remains simple, while more complex behavior can still be expressed through dependencies between expressions. A Boolean atom occurring in the right-hand side of a Boolean rule Bi may be one of the following: an alphabet test a ∈ Σ; a reference to an earlier Boolean rule Bj with j < i ; a 9Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

position–period predicate m%o, where m ∈ N and o < m ; or a comparison CExp 1 ▷◁ CExp 2,where ▷◁ ∈ { =, ≤, < }.In addition to full resampling, we allow micro-mutations that preserve expression depth while modifying only a local syntactic feature. Examples include swapping ∧ and ∨, toggling negation, flipping the strictness of a comparison, or resampling a single leaf atom. These micro-mutations define small, incremental moves in the neighborhood. 

Counting Rule Mutation As with Boolean expressions, we work with a shallow fragment of the syntax defined in Definition 2.3. During mutation, the right-hand side of each counting rule is required to have syntactic depth at most 2. Counting expressions are therefore built from count atoms using at most one arithmetic operator or one conditional expression. A new right-hand side is either resampled entirely or modified using a micro-mutation. Resampled Counting expressions are drawn from the grammar 

CExp ::= x | op (x, y ) | BExp ? x : y, 

where x, y are count atoms and op ∈ { +, −, min , max }.Count atoms are drawn from a finite, type-directed pool: integer constants k ∈ N; references to earlier counting rules Cj with j < i ; or count of PERIODIC predicate #( ϕ) or LOCAL predicate #rs,r e (ϕ). The predicate ϕ is a Boolean atom that does not contain comparisons or equalities. For LOCAL predicates, bounds are normalized so that rs ≤ re.

Objective Function Each candidate program P is evaluated on a fixed set of labeled ex-amples D = D+ ∪ D−. The primary objective is to minimize the number of misclassified samples: mis( P ) = X

> x∈D+

I[P (x) = 0] + X

> x∈D−

I[P (x) = 1] .

Here, I[·] is the indicator function, equal to 1 if P misclassifies x and 0 otherwise. Among programs with equal classification behavior, we prefer simpler programs. We penalize on ex-pressions unreachable from Rk in the dependency graph, and the total size of the abstract syntax tree. The resulting scoring function is 

E(P ) = λmis · mis( P ) + λU · unreach( P ) + λS · size( P ),

where λmis ≫ 1. This demands that the search first minimizes mis( P ) and, among programs with equal misclassified count, prefers structurally simpler programs. 

Annealing Dynamics At each iteration, the algorithm maintains a current program P and proposes a neighbouring program P ′. Let ∆ = E(P ′) − E(P ). If ∆ ≤ 0, the new program P ′

is always accepted. Otherwise, it is accepted with probability exp  − ∆

> T

, where T > 0 is the current temperature. The temperature is initialized to T0 and decreased using geometric cooling. To mitigate premature convergence, we optionally apply periodic reheating. The complete simulated-annealing synthesis procedure is presented in Algorithm 1, which maintains the current program and the best program encountered so far, applies mutation-based proposals, and accepts or rejects them according to the probability above. Note that if ∆ ≤ 0, then exp( −∆/T ) ≥ 1 and the new program P ′ is accepted with probability 1 in line 7. Termination occurs when the iteration budget is exhausted or when no further improvement is observed under a sufficiently low temperature. 10 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

Algorithm 1 Simulated-Annealing Synthesis for C-RASP Programs  

> 1:

Input: initial program P0; sample set D; temperature parameters ( T0, α, K, ρ ); iteration budget N 

> 2:

Output: best program P ⋆ 

> 3:

P ← P0, P ⋆ ← P , T ← T0 

> 4:

for i = 0 to N − 1 do  

> 5:

Pick P ′ according to distribution Mutate (P ) 

> 6:

Pick r ∈ [0 , 1] uniformly at random  

> 7:

if r ≤ exp( −(E(P ′) − E(P )) /T ) then  

> 8:

P ← P ′ 

> 9:

end if  

> 10:

if E(P ) < E (P ⋆) then  

> 11:

P ⋆ ← P 

> 12:

end if  

> 13:

T ← α · T 

> 14:

if K > 0 and ( i + 1) mod K = 0 then  

> 15:

T ← ρ · T 

> 16:

end if  

> 17:

end for  

> 18:

return P ⋆

As a refinement (not shown in Algorithm 1), once a program with mis( P ) = 0 is discovered, the search may continue under a hard constraint that forbids reintroducing misclassification, while increasing structural penalties to accelerate simplification. 

# 5 Experiments 

We implement a unified Scala framework that integrates C-RASP synthesis and verification 

within a single modular toolchain. The framework combines example-driven local search with formal verification and consists of three core components: 1. C-RASP synthesis based on simulated annealing, 2. a lightweight C-RASP evaluator for fast scoring on finite datasets, and 3. a verification back-end that translates C-RASP programs to Lustre and checks properties using Kind2. Building on this integration, our framework supports two verification-guided synthesis ap-plications. (1) Transformer Program Minimization . Given a specification program Pspec , the goal is to syn-thesize a program P such that L(P ) = L(Pspec ) while minimizing structural complexity. The procedure alternates between local search and equivalence checking; when equivalence fails, Kind2 provides a counterexample trace that is added to the trace set to guide further search. The process terminates once equivalence is established or a predefined limit is reached. (2) Constraint Learning . Given labeled examples D = D+ ∪ D− and a partial specification program Pspec , constraint learning aims to synthesize a program P that fits all examples and satisfies L(P ) ⊆ L(Pspec ). Local search is guided by D and each candidate is checked for 11 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

inclusion; if the check fails, the resulting counterexample is added to D as a negative example and synthesis continues. We evaluate on a benchmark suite of regular, counting-based, and context-free–style lan-guages (see supplementary material for details). For each task, synthesis is trained on 1 ,000 balanced examples with string lengths in [ ℓmin , 100], where ℓmin is the minimum valid length for the target language. Simulated annealing runs a budget of 100 ,000 iterations. The temperature is initialized to T0 = 1 .0, cooled exponentially with rate α = 0 .9995, and reheated every 4 ,000 iterations by a factor of 1 .2. 

> Table 1: C-RASP synthesis results across benchmark languages (timeout: 300s). SL synthesis reports sample-based accuracy and time; program minimization and constraint learning report refinement rounds (R), synthesis time (Synth), and verification time (Verif). “–” indicates that no 100%-accurate program was found within the timeout.

SL Synthesis Program Minimization Constraint Learning Language Acc (%) Time (s) R Synth (s) Verif (s) R Synth (s) Verif (s) 

Dyck-1 100 3.92 21 27.7 41.7 12 – –AStarBStar 100 24.62 11 7.7 46.8 1 10.2 8.9 AnBnCn 100 80.05 22 88.7 42.4 2 – –AAStar 100 1.57 4 1.0 10.0 1 15.5 8.3 ContainsAB 100 17.35 6 2.2 12.5 1 145.0 10.4 Majority 100 6.02 9 4.8 19.9 1 61.4 8.9 Existential 100 4.62 6 2.2 12.5 4 48.0 13.6 Parity – – – – – – – –PT-2 100 13.30 12 9.2 29.7 1 133.7 12.0 PT-3 100 59.06 21 91.3 46.4 3 – –PT-5 – – – – – – – –

D2 100 2.26 27 63.9 90.8 11 68.5 48.2 

D3 100 1.54 12 – – 15 – –

D4 100 1.32 22 – – 17 – –Tomita 1 100 3.45 5 1.4 11.2 1 28.9 8.3 Tomita 2 100 3.03 21 33.6 58.3 9 78.4 38.3 Tomita 3 – – – – – – – –Tomita 4 100 21.32 21 50.2 52.0 2 – –Tomita 5 – – – – – – – –Tomita 6 – – – – – – – –Tomita 7 100 9.32 22 79.1 59.8 1 279.4 18.3 Next(Argmax) 100 7.20 27 153.5 46.9 2 185.7 6.3 As shown in Table 1, we achieve perfect accuracy on most benchmarks, including languages that require non-trivial counting structures. The observed timeouts are consistent with theoret-ical results [12, 31], which show that these languages are not expressible in C-RASP. Conse-quently, synthesis does not terminate within the timeout for these benchmarks. These failures therefore reflect fundamental limitations of the hypothesis language rather than deficiencies of the simulated annealing procedure. Program Minimization shows synthesis and verification alternate over multiple refinement rounds: local search quickly finds correct but non-minimal programs, while verification dominates runtime by repeatedly identifying redundancies and driving structural simplification, particularly for languages with richer counting structure. The number of refinement rounds in Constraint Learning is typically small, since satisfying the labeled example set directly restricts the hypothesis space. As a result, verification cost remains low, while synthesis time increases as local search operates under tighter semantic constraints and the tool fails to terminate primarily because the target language is not expressible in C-RASP. 12 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

# 6 Discussion and Related Work 

Formal models of transformers: The intricacy of formally modeling transformers arises from the following features (among others): precision (polynomial, logarithmic, and fixed), attention mechanisms (hard attention vs. softmax attention), positional encodings (periodic, local, relative, etc.), and uniformity (a uniform description independent of the input length). These gave rise to a number of different formal models (e.g. see excellent survey [25]). Over the years, experimental results and results concerning learnability (in terms of length general-izability) revealed that several variants of softmax transformers [12, 31] that are tightly related to C-RASP capture concepts that are learnable by transformers. [12] proposed a formal version of the RASP conjecture [34] that C-RASP captures precisely languages that are expressible by length-generalizable transformers. 

RASPs: [7] proposed learning a RASP by pre-restricting the architecture and parameters for transformer training. The difficulty with this approach (especially for C-RASP) is to find the right “restriction” so that a C-RASP can be extracted; we are not aware of such work. Our synthesis algorithm directly finds a C-RASP without going through a transformer, which we showed to be competitive with transformer training on our benchmarks. B-RASP [33] is a version of RASP, which supports manipulations of bit sequences. B-RASP can express languages that are not efficiently learnable (see [12]), including the flip-flop language [17], which makes it not a suitable model for real-world transformers. The model overapproximates concepts expressible by fixed-precision transformers [16]. 

Verification of neural networks: Such works (e.g. [14, 10, 18, 11]) focus mostly on verifying neural networks with a fixed number of input nodes. Yearly competition (e.g. see [4]) is now held on such verification tasks. Here, the verification problem is decidable (in fact, NP-complete), which is not the case for transformers [24]. 

Future Work: Numerous open problems lie ahead including the extension of our methods to other properties, e.g., including “robustness”. This is a common property in neural networks verification (e.g. [4]), but the right formulation (that is motivated by practical applications) for sequential models is yet to be explored, e.g., perhaps via metrics on strings like hamming/edit distances. 

# Acknowledgments 

This material is based in part upon work supported by the European Union 2 (ERC, LASD, 101089343, https://doi.org/10.3030/101089343) and by the Swedish Research Council through grant 2021-06327. 

# References 

[1] Marcelo Arenas, Daniel B´ aez, Pablo Barcel´ o, Jorge P´ erez, and Bernardo Subercaseaux. Foundations of symbolic languages for model interpretability. In Marc’Aurelio Ranzato, 

> 2Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.

13 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, edi-tors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , pages 11690–11701, 2021. [2] Pablo Barcel´ o, Alexander Kozachinskiy, Anthony Widjaja Lin, and Vladimir V. Podolskii. Logical languages accepted by transformer encoders with hard attention. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. [3] Pablo Barcel´ o, Mika¨ el Monet, Jorge P´ erez, and Bernardo Subercaseaux. Model inter-pretability through the lens of computational complexity. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020. [4] Christopher Brix, Stanley Bak, Taylor T. Johnson, and Haoze Wu. The fifth international verification of neural networks competition (VNN-COMP 2024): Summary and results. 

CoRR , abs/2412.19985, 2024. [5] P. Caspi, D. Pilaud, N. Halbwachs, and J. A. Plaice. LUSTRE: a declarative language for real-time programming. In Proceedings of the 14th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages , POPL ’87, page 178–188, New York, NY, USA, 1987. Association for Computing Machinery. [6] Adrien Champion, Alain Mebsout, Christoph Sticksel, and Cesare Tinelli. The Kind2 model checker. In Swarat Chaudhuri and Azadeh Farzan, editors, Computer Aided Verifi-cation , pages 510–517, Cham, 2016. Springer International Publishing. [7] Dan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. In 

Proceedings of the 37th International Conference on Neural Information Processing Sys-tems , NIPS ’23, Red Hook, NY, USA, 2023. Curran Associates Inc. [8] Darrall Henderson, Sheldon H. Jacobson, and Alan W. Johnson. The Theory and Practice of Simulated Annealing , pages 287–319. Springer US, Boston, MA, 2003. [9] Chih-Duo Hong, Hongjian Jiang, Anthony W. Lin, Oliver Markgraf, Julian Parsert, and Tony Tan. Extracting robust register automata from neural networks over data sequences. 

CoRR , abs/2511.19100, 2025. [10] Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, and Xinping Yi. A survey of safety and trustworthiness of deep neu-ral networks: Verification, testing, adversarial attack and defence, and interpretability. 

Computer Science Review , 37:100270, 2020. [11] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural networks. In Rupak Majumdar and Viktor Kuncak, editors, Computer Aided Ver-ification - 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I , volume 10426 of Lecture Notes in Computer Science , pages 3–29. Springer, 2017. 14 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

[12] Xinting Huang, Andy Yang, Satwik Bhattamishra, Yash Raj Sarrof, Andreas Krebs, Hattie Zhou, Preetum Nakkiran, and Michael Hahn. A formal framework for understanding length generalization in transformers. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 . OpenReview.net, 2025. [13] Hongjian Jiang, Michael Hahn, Georg Zetzsche, and Anthony Widjaja Lin. Softmax trans-formers are turing-complete. CoRR , abs/2511.20038, 2025. [14] Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex: a calculus for reasoning about deep neural networks. Formal Methods Syst. Des. , 60(1):87–116, 2022. [15] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. 

Science , 220(4598):671–680, 1983. [16] Jiaoda Li and Ryan Cotterell. Characterizing the expressivity of fixed-precision trans-former language models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems , 2025. [17] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Ex-posing attention glitches with flip-flop language modeling. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. [18] Changliu Liu, Tomer Arnon, Christopher Lazarus, Christopher A. Strong, Clark W. Bar-rett, and Mykel J. Kochenderfer. Algorithms for verifying deep neural networks. Found. Trends Optim. , 4(3-4):244–404, 2021. [19] Joao Marques-Silva and Alexey Ignatiev. No silver bullet: interpretable ml models must be explained. Frontiers in Artificial Intelligence , 6, 2023. [20] William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. [21] Christoph Molnar. Interpretable Machine Learning . 2nd edition, 2022. [22] Takamasa Okudono, Masaki Waga, Taro Sekiyama, and Ichiro Hasuo. Weighted automata extraction from recurrent neural networks via regression on state spaces. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second In-novative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , pages 5306–5314. AAAI Press, 2020. [23] Jorge P´ erez, Pablo Barcel´ o, and Javier Marinkovic. Attention is turing-complete. J. Mach. Learn. Res. , 22:75:1–75:35, 2021. [24] Marco S¨ alzer, Eric Alsmann, and Martin Lange. Transformer encoder satisfiability: Com-plexity and impact on formal reasoning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 . OpenReview.net, 2025. [25] Lena Strobl, William Merrill, Gail Weiss, David Chiang, and Dana Angluin. What formal languages can transformers express? A survey. Trans. Assoc. Comput. Linguistics , 12:543– 561, 2024. 15 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

[26] Marco S¨ alzer, Chris K¨ ocher, Alexander Kozachinskiy, Georg Zetzsche, and Anthony Wid-jaja Lin. The counting power of transformers. CoRR , abs/2505.11199, 2025. [27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: An-nual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 5998–6008, 2017. [28] Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In International Conference on Machine Learning , pages 11080–11090. PMLR, 2021. [29] Gail Weiss, Yoav Goldberg, and Eran Yahav. Extracting automata from recurrent neural networks using queries and counterexamples (extended version). Mach. Learn. ,113(5):2877–2919, 2024. [30] Andy Yang, Pascal Bergstr¨ aßer, Georg Zetzsche, David Chiang, and Anthony Lin. Length generalization bounds for transformers, 2026. Under submission. [31] Andy Yang, Micha¨ el Cadilhac, and David Chiang. Knee-deep in c-RASP: A transformer depth hierarchy. In The Thirty-ninth Annual Conference on Neural Information Processing Systems , 2025. [32] Andy Yang and David Chiang. Counting like transformers: Compiling temporal counting logic into softmax transformers. CoRR , abs/2404.04393, 2024. [33] Andy Yang, David Chiang, and Dana Angluin. Masked hard-attention transformers recog-nize exactly the star-free languages. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024 , 2024. [34] Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua M. Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? A study in length generalization. In The Twelfth International Conference on Learning Represen-tations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. 16 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

# A C-RASP Semantics 

We give the full definition of νw(e, j ) over a word w = a1 . . . a n. For Boolean expressions we have νw(⊤, j ) := ⊤

νw(⊥, j ) := ⊥

νw(a, j ) := 

(

⊤ if a = aj

⊥ otherwise 

νw(B, j ) := νw(eB , j )

νw(¬e, j ) := ¬νw(e, j )

νw(e1 ⋄ e2, j ) := νw(e1, j ) ⋄ νw(e2, j )

νw(e1 ▷◁ e 2, j ) := νw(e1, j ) ▷◁ ν w(e2, j )

νw(m%o, j ) := j mod m = o

and for Counting expressions 

νw(k, j ) := kνw(C, j ) := νw(eC , j )

νw(# e, j ) := X

> i≤j

νw(e ? 1 : 0 , i )

νw(# rs,r e e, j ) := X

> j−re≤i≤j−rs

νw(e ? 1 : 0 , i )

νw(e1 + e2, j ) := νw(e1, j ) + νw(e2, j )

νw(e1 − e2, j ) := νw(e1, j ) − νw(e2, j )

νw(min( e1, e 2), j ) := min( νw(e1, j ), ν w(e2, j )) 

νw(max( e1, e 2), j ) := max( νw(e1, j ), ν w(e2, j )) 

νw(e1 ? e2 : e3, j ) := 

(

νw(e2, j ) if νw(e1, j ) = ⊤

νw(e3, j ) otherwise. 

## A.1 Additional Definitions for Lustre 

A.1.1 Definition of Expression Evaluation 

We define ν(e, i ) in full for Lustre expressions. For Boolean expressions 

ν(⊤, i ) = ⊤

ν(⊥, i ) = ⊥

ν(¬e, i ) = ¬ν(e, i )

ν(e1 ⋄b e2, i ) = ν(e1, i ) ⋄b ν(e2, i )

ν(e1 ▷◁ b, i ) = ν(e1, i ) ▷◁ b ν(e2, i )

ν(pre( e), i ) = 

(

ν(e, i − 1) when i > 0undefined otherwise 

ν(e1 → e2, i ) = 

(

ν(e1, i ) if i = 0 

ν(e2, i ) otherwise 17 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

and for integer expressions 

ν(k, i ) = kν(e1 ⋄c e2, i ) = ν(e1, i ) ⋄c ν(e2, i )

ν(if e1 then e2 else e3, i ) = 

(

ν(e2, i ) if ν(e1, i ) = ⊤

ν(e3, i ) otherwise 

ν(pre( e), i ) = ν(e, i − 1) 

ν(e1 → e2, i ) = 

(

ν(e1, i ) if i = 0 

ν(e2, i ) otherwise. 

A.1.2 Translating C-RASP to Lustre 

We define T (e) in full. For Boolean expressions (recalling that I = a denotes I = ι(a)) 

T (⊤) := ⊤

T (⊥) := ⊥

T (a) := I = aT (B) := VB

T (¬e) := ¬T (e)

T (e1 ⋄ e2) := T (e1) ⋄ T (e2)

T (e1 ▷◁ e 2) := T (e1) ▷◁ T (e2)

T (m%o) := P mod m = o

where P is a fresh integer variable encoding the current position. It uses an auxiliary equation 

P := 0 → pre( P ) + 1. For counting expressions 

T (k) := kT (C) := VC

T (#( e)) := C#( e)

T (# rs,r e (e)) := C#rs,re (e)

T (e1 + e2) := T (e1) + T (e2)

T (e1 − e2) := T (e1) − T (e2)

T (min( e1, e 2)) := if Ce1 < C e2 then Ce1 else Ce2

T (max( e1, e 2)) := if Ce2 < C e1 then Ce1 else Ce2

T (e1 ? e2 : e3) := if T (e1) then T (e2) else T (e3)where C#( e), C#rs,re (e), Ce1 , and Ce2 are fresh integer variables defined via the auxiliary equa-tions. The additional auxiliary equations are: for each Cei add the rule 

Cei := T (ei).

A.1.3 Additional Verification Properties Universality Let P = ( R1, . . . , R k) be a C-RASP program. Universality can be checked by inspecting the value of Rk = B := e (corresponding to the final C-RASP rule). For universality, 18 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

we require that Rk always evaluates to ⊤. It requires the following check constraint which asserts that after the first position, either the input I is incorrect, the end of the stream marker has not been reached, or the value of Rk is ⊤.check ⊤ → (¬(BˆI ∧ I = #) ∨ pre( VB )) We define T∀(P ) to be the Lustre program with the equations RI ∪ S 

> i

T (Ri) and the check statement defined above. 

Proposition A.1. A C-RASP program P accepts all words w ∈ Σ+ if T∀(P ) is valid. 

Language Equality We can check the equivalence of two C-RASP programs P1 and P2 using a similar encoding to language inclusion. We need to adjust the check equation as shown below. check ⊤ → 

 ¬(BˆI ∧ I = #) 

∨ (pre( V1) ∧ pre( V2)) 

∨ (¬pre( V1) ∧ ¬ pre( V2)) 



We define T=(P1, P w) to be the Lustre program with the equations RI ∪S 

> i

T (R1 

> i

)∪S 

> i

T (R2 

> i

)and the check statement defined above. 

Proposition A.2. A C-RASP program P1 accepts the same words w ∈ Σ+ accepted by a C-RASP program P2 if T=(P1, P 2) is valid. 

# B Benchmark Languages and C-RASP Constructions 

Dyck-1 is the language of well-balanced parentheses over Σ = {l, r }: every prefix contains at least as many l as r, and the total numbers of l and r are equal. 

C-RASP program for Dyck-1 over Σ = {l, r }

L = lR = rV = CL < CR M = CV = C0

B = CL = CR Out = M ∧ BCL = # lCR = # rCV = # VC0 = 0 

a∗b∗ is the language of strings over Σ = {a, b } consisting of a block of a’s followed by a block of b’s (possibly empty blocks). 19 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

C-RASP program for a∗b∗ over Σ = {l, r }

Qa = aQb = bV = Qa ∧ (0 < Cb )

Y = CV = 0 

Cb = # Qb CV = # VOut = Y

MAJORITY is the language of strings over Σ = {a, b } with at least as many a symbols as 

b symbols. 

C-RASP program for Majority over Σ = {a, b }

P a = aP b = bOut = Ca ≤ Cb Ca = # P a Cb = # P b 

(aa) ∗ is the language of even-length strings over Σ = {a}.

C-RASP program for ( aa )∗ over Σ = {a}

CnotA = #( ¬a)

Out = (%2 = 1) ∧ (CnotA = 0) 

anbncn is the language {anbncn | n ≥ 0} over Σ = {a, b, c }.20 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

C-RASP program for anbncn over Σ = {a, b, c }

Qa = aQb = bQc = cCa = # Qa Cb = # Qb Cc = # Qc A = Cb + Cc = 0 

B = Cc = 0 

QaA = Qa ∧ AQbB = Qb ∧ BCA = # QaA CB = # QbB Ga = CA = Ca Gb = CB = Cb Gabc = ( Ca = Cb ) ∧ (Cb = Cc )

Out = Ga ∧ Gb ∧ Gabc 

Σ∗ab Σ∗ over Σ = {a, b } is the language of strings containing the substring ab at some position. 

C-RASP program for Σ ∗abΣ ∗ over Σ = {a, b }

CaP re = # < (1) aP aP re = 1 ≤ CaP re Qab = b ∧ P aP re Cab = # Qab Out = 1 ≤ Cab 

Existential is the language of strings over Σ = {a, b } containing at least one occurrence of a.

C-RASP program for Existential over Σ = {a, b }

A = aB = bC1 = # AC2 = # BOut = 1 <= C2

Next(Argmax) is the language over Σ in which, for each position i > 1, the observed successor xi maximises (ties allowed) the empirical count of transitions from the previous symbol 21 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

xi−1, compared against all alternative successors c ∈ Σ, using statistics computed from the prefix up to position i − 1. 

C-RASP program for Next(Argmax) over Σ = {a, b, c }

C1 = #(1 , 1) aB1 = 1 ≤ C1

B2 = a ∧ B1

B3 = b ∧ B1

B4 = c ∧ B1

C2 = # B2

C3 = # B3

C4 = # B4

B5 = C3 ≤ C2

B6 = C4 ≤ C2

B7 = 1 ≤ (C2 + C3 + C4) 

B8 = a ∧ B7 ∧ B5 ∧ B6

Out = 1 ≤ #B8

Tomita 1 is the regular language a∗ over Σ = {a, b }, i.e., strings containing no occurrences of b.

C-RASP program for Tomita 1 over Σ = {a, b }

B1 = bC1 = # B1

Out = ( C1 = 0) 

Tomita 2 is the regular language ( ab )∗ over Σ = {a, b }, i.e., strings of even length that alternate starting with a and ending with b.

C-RASP program for Tomita 2 over Σ = {a, b }

B1 = ( a ∧ %2 = 0) ∨ (b ∧ %2 = 1) 

B2 = #( ¬B1) 

Out = (%2 = 1) ∧ (B2 = 0) 

Tomita 4 is the regular language over Σ = {a, b } that forbids any substring of the form 

a b 2n+1 a2m+1 b for n, m ≥ 0. Equivalently, w ∈ Tomita-4 iff no occurrence of a is followed by an odd run of b’s, then an odd run of a’s, and then a b.22 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

C-RASP program for Tomita 4 over Σ = {a, b }

A1 = #(1 , 1) aP 1 = 1 ≤ A1

A2 = #(2 , 2) aP 2 = 1 ≤ A2

AAApos = a ∧ P 1 ∧ P 2

CAAA = # AAApos Out = ( CAAA = 0) 

Tomita 7 is the regular language a∗b∗a∗b∗ over Σ = {a, b }, i.e., strings that can be partitioned into at most four consecutive blocks, alternating between a’s and b’s starting with a.

C-RASP program for Tomita 7 over Σ = {a, b }

B = bA = aCb = # BB1 = 1 ≤ Cb B2 = A ∧ B1

C1 = # B2

B3 = 1 ≤ C1

B4 = B ∧ B3

CBAB = # B4

B1AB = 1 ≤ CBAB B5 = A ∧ B1AB CBadA = # B5

Out = ( CBadA = 0) 

PT-5 is the piecewise testable language over Σ = {a, b, c, d, e } in which a, b, c, d, e occur in this order as a (not necessarily contiguous) subsequence. Formally, w ∈ PT-5 iff there exist 

i1 < i 2 < i 3 < i 4 < i 5 with w[i1] = a, w[i2] = b, w[i3] = c, w[i4] = d, and w[i5] = e.23 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

C-RASP program for PT-5 over Σ = {a, b, c, d, e }

A = aB = bC = cD = dE = eCA = # AL1 = 1 <= CA QB = B ∧ L1

CB = # QB L2 = 1 ≤ CB QC = C ∧ L2

CC = # QC L3 = 1 ≤ CC QD = D ∧ L3

CD = # QD L4 = 1 ≤ CD QE = E ∧ L4

CE = # QE L5 = 1 ≤ CE Out = L5

PT-3 over Σ = {a, b, c } in which a, then b, then c occur in this order (not necessarily consec-utively). Formally, w ∈ PT-3 iff there exist indices i < j < k such that w[i] = a, w[j] = b, and 

w[k] = c.24 Synthesis and Verification of Transformer Programs H. Jiang M. Hague P. R¨ ummer A. W. Lin 

C-RASP program for PT-3 over Σ = {a, b, c }

A = aB = bC = cCA = # AL1 = 1 ≤ CA QB = B ∧ L1

CB = # QB L2 = 1 ≤ CB QC = C ∧ L2

CC = # QC L3 = 1 ≤ CC Out = L3

PT-2 over Σ = {a, b } consists of strings in which an a occurs before some later b.

C-RASP program for PT-2 over Σ = {a, b }

A = aB = bCA = # AL1 = 1 ≤ CA QB = B ∧ L1

CB = # QB L2 = 1 ≤ CB Out = L225