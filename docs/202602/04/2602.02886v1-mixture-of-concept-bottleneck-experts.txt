Title: Mixture of Concept Bottleneck Experts

URL Source: https://arxiv.org/pdf/2602.02886v1

Published Time: Wed, 04 Feb 2026 01:20:20 GMT

Number of Pages: 23

Markdown Content:
# Mixture of Concept Bottleneck Experts 

Francesco De Santis 1 Gabriele Ciravegna 2 Giovanni De Felice 3 Arianna Casanova 4 Francesco Giannini 5

Michelangelo Diligenti 6 Mateo Espinosa Zarlenga 7 Pietro Barbiero 8 Johannes Schneider 4 Danilo Giordano 1

Abstract 

Concept Bottleneck Models (CBMs) promote in-terpretability by grounding predictions in human-understandable concepts. However, existing CBMs typically fix their task predictor to a single linear or Boolean expression, limiting both pre-dictive accuracy and adaptability to diverse user needs. We propose Mixture of Concept Bottle-neck Experts (M-CBEs), a framework that gen-eralizes existing CBMs along two dimensions: the number of experts and the functional form of each expert, exposing an underexplored re-gion of the design space. We investigate this region by instantiating two novel models: Lin-ear M-CBE, which learns a finite set of linear expressions, and Symbolic M-CBE, which lever-ages symbolic regression to discover expert func-tions from data under user-specified operator vo-cabularies. Empirical evaluation demonstrates that varying the mixture size and functional form provides a robust framework for navigating the accuracy-interpretability trade-off, adapting to dif-ferent user and task needs. 

1. Introduction 

In recent years, Deep Learning (DL) models have achieved remarkable performance across a wide range of tasks, yet their growing complexity and opacity (Rudin, 2019; Colom-bini et al., 2025) prevent their adoption in high-stakes do-mains where transparency is essential (EUGDPR, 2017; Dur ´an & Jongsma, 2021; Act, 2024). Concept Bottle-neck Models (CBMs) (Koh et al., 2020) have emerged as a promising approach to align models with human reasoning. CBMs decompose prediction into two stages: a concept en-coder that maps raw inputs to human-interpretable variables, called concepts (e.g., “ round ”, “ red ”), and a task predictor 

> 1

Polytechnic of Torino 2CENTAI Institute 3USI 4University of Liechtenstein 5University of Pisa 6University of Siena 7University of Oxford 8IBM Research. Correspondence to: Francesco De Santis <francesco.desantis@polito.it >.

Preprint. February 4, 2026. Functional form   

> Number of Experts
> Simple Boolean Linear User-de fi ned subspace CMR
> INTRACTABLE INTERPRETABILITY SENN LICEM
> UNCHARTED RESEARCH AREA
> KNOTTY
> DCR Over-parameterized MLP
> Lin-M-CBE
> LEN CBM
> SIMPLE
> Task constr.
> Human constr.
> Sym-M-CBE
> Simple Boolean User-de fi ned subspace
> Functional form

Figure 1. Left : The plane defined by functional form and number of experts. Different CBMs occupy distinct positions in this space, with regions highlighted in different colors. In red we indicate our newly proposed instantiations. Right : The red curve denotes human constraints and the green curve denotes task constraints. The region bounded by the two curves represents the set of feasible models. 

that maps these concepts into the task variable. While substantial effort has been devoted to defining and learning meaningful concepts (Oikarinen et al., 2023; Do-minici et al., 2025; De Felice et al., 2025), little attention has been paid to the design of the task predictor. Indeed, most CBMs constrain the task predictor to provide predictions through a unique, global function (Koh et al., 2020; Vanden-hirtz et al., 2024; Ciravegna et al., 2023; Marconato et al., 2022), restricting expressiveness and often failing to cap-ture the true concept-to-task generating process (Mahinpei et al., 2021; Espinosa Zarlenga et al., 2022b). A promising way to improve the expressiveness of task predictors is to use multiple specialized functions, as in Mixture of Experts (MoE) models (Jacobs et al., 1991; Shazeer et al., 2017). While MoE models are traditionally designed to route inputs to experts so as to maximize predictive performance under computational constraints (Lepikhin et al., 2020; Artetxe et al., 2021), in an interpretability-oriented framework this objective naturally translates into jointly optimizing predic-tive performance and interpretability. However, transposing MoEs to this new setting requires more than just expert routing; it demands explicit control over the functional form 

of the experts, the class of expressions mapping concepts to targets (e.g., Boolean or linear). Since interpretability is inherently user-dependent (Lipton, 2018; Miller, 2019; Gkintoni et al., 2025), fixing the functional form a priori 

1

> arXiv:2602.02886v1 [cs.LG] 2 Feb 2026 Mixture of Concept Bottleneck Experts

constrains interpretability and may unnecessarily sacrifice predictive performance, as we show in Section 5. Conse-quently, enabling flexible control over both the functional form and the number of experts is central for achieving the optimal accuracy-interpretability trade-off. 

Contributions. We introduce a new framework, Mixture of Concept Bottleneck Experts (M-CBEs), which general-izes CBMs by modeling the task predictor as a mixture of specialized functions (experts) with controllable functional forms. We adopt expression trees as our modeling abstrac-tion, for a rigorous yet flexible definition of each function. We show that several existing concept-based models are spe-cial cases of our framework, while a substantial area remains unexplored (Figure 1, Left). To address this, we propose the Linear M-CBE model, which generalizes the linear CBM to multiple experts. Beyond instantiating specific functions, M-CBEs can be specified with the set of operators the user understands (e.g., +, ×, exp ). We demonstrate this capa-bility with the Symbolic M-CBE model, which leverages symbolic regression to automatically discover the optimal expert functions using user-defined operators. We empirically validate M-CBEs on classification and re-gression benchmarks, showing: i) navigating the design space allows finding the intersection of human and task constraints (Figure 1, Right), matching black-box accu-racy without compromising interpretability; ii) algebraic forms ensure scalability outperforming rigid Boolean logic in high-dimensional concept spaces (up to + 65%); iii) mul-tiple experts compensate for incomplete concept bottlenecks or varying task logic; iv) finite discrete expressions ensure global interpretability allowing user inspection; and v) while M-CBEs adapts to any functional form requested, Symbolic M-CBE also supports post-hoc adaptation, allowing users to modify operator vocabularies without retraining. 

2. Preliminaries 

Concept Bottleneck Models. Let X and Y be random vari-ables representing the input and task, taking values in spaces 

X and Y, respectively. We denote realizations by x ∈ X 

and y ∈ Y . CBMs (Koh et al., 2020) introduce an inter-mediate and human-interpretable concepts C mapping into 

C ⊆ Rk, that mediates the relationship between X and Y .The model is trained to approximate the joint distribution: 

p(y, c | x) = p(y | c) p(c | x), (1) where c ∈ C , p(c | x) is the concept encoder that predicts concepts from raw inputs, and p(y | c) is the task predictor 

that maps concepts to the task variable, preserving semantic transparency .

Symbolic Regression. Given a dataset {(x(i), y (i))}Ni=1 ,Symbolic Regression (SR) searches the space of mathemati-cal expressions to find the function f : X → Y closest to the true data-generating process (Schmidt & Lipson, 2009). SR methods employ heuristic search strategies, e.g. genetic algorithms, to evolve populations of expressions within a multi-objective optimization framework that jointly mini-mizes prediction error and expression complexity (Langley, 1979; Langley et al., 1981; Koza, 1994; Cranmer, 2023). 

3. Mixture of Concept Bottleneck Experts 

This section presents Mixture of Concept Bottleneck Ex-perts (M-CBEs), a class of models that improves upon exist-ing CBMs in two ways: (i) by allowing flexible, user-aligned functional forms, ensuring that the task predictor can only retrieve functions the user is capable of understanding while tailoring expressiveness to the problem at hand; and (ii) by operating over ensembles of expressions (experts), en-abling the task predictor to match accuracy through multiple simpler expressions or to handle inputs requiring distinct reasoning patterns. M-CBEs represents each task-predictor function as an ex-pression tree and models it as a random variable T (with realization t) conditioned on the input x. Conditioning T

on X allows the model to dynamically select different ex-pressions across inputs, maintaining high accuracy while preserving semantic transparency (Barbiero et al., 2023; De Santis et al., 2025; De Felice et al., 2025). Indeed, the task prediction depends on the expression tree t, which op-erates solely on the predicted concepts c. This formulation induces the probabilistic graphical model (PGM) illustrated in Figure 2 (Left). While our framework shares structural similarities with the PGM proposed by Debot et al. (2024), we do not restrict T to represent a fixed Boolean expression; instead, we generalize the model to accommodate arbitrary functional forms. Accordingly, the joint distribution factor-izes as: 

p(x, c, t, y ) = p(x) p(c | x) p(t | x) p(y | c, t ) (2) The first factor, p(c | x), represents the concept encoder ,while p(t | x) defines the distribution over expression trees. The final term, p(y | c, t ), is the task predictor , which spec-ifies the target distribution conditioned on both predicted concepts and the selected tree. 

3.1. Expression Trees in M-CBEs. 

An expression tree (Mitchell, 1991) is a directed acyclic graph (DAG) representing a mathematical expression (Fig-ure 2, right). We define a class T of expression trees w.r.t. a vocabulary of operations W via admissible edge configura-tions E on a set of nodes N = V ∪ O ∪ P , with: (i) V: set of input nodes instantiated with specific concepts .(ii) O: set of operator nodes (e.g., {× , sin , exp }) from W.(iii ) P: set of parameter nodes representing real numbers. 2Mixture of Concept Bottleneck Experts               

> Figure 2. Left: PGM representing the assumed generative process for M-CBEs. Right: Example of an expression tree ( T) represent-ing a linear expression. Green nodes are placeholder variables V,blue nodes are operators Odrawn from the vocabulary W, and red nodes are learnable parameters Θ. At inference time, each placeholder viis instantiated with the corresponding predicted concept ci.

Let O, E, Θ, and V denote the random variables for op-erators, edges, parameters, and placeholder inputs, taking values in O, E, P, and V, respectively. An expression tree is defined by the tuple t = ( o, e, θ, v ), where o, e, θ and 

v represent specific realizations of these random variables. Notably, we decouple the input variable V from the con-cepts C predicted by an encoder, allowing the expression tree to selectively learn and operate on a subset of relevant concepts. We denote the space of functions representable by trees in class T as H(T ). Given concept values c assigned to the input variables, evaluating the tree yields a prediction 

y = ft(c), where ft represents the function defined by the expression tree t.For instance, the set of linear functions can be represented by selecting W = {+, ×} , where + is the binary summa-tion and × the binary multiplication, and having O and E

only allowing a multiplication layer between pairs of input and parameter nodes, and a unique summation on top (cf. Figure 2). Each selection of P defines a different linear function. We refer to the set of expression trees whose represented functions are linear as Tlin .

3.2. Design Choices in M-CBEs Functional form. Navigation through the functional form dimension is realized by performing different inferences over the generative process. For instance, the user can fix 

o, e, and v, reducing the problem to a parametric linear function where only the parameters θ are learned from data. Alternatively, one can fix only part of the structure by set-ting a sub-expression (specific o and e for a subset of nodes) while learning the remaining operators, edges and subset of concepts v from data, enabling the incorporation of do-main knowledge without fully specifying the expression tree. Finally, one can provide only the vocabulary W of inter-pretable operators while learning the entire expression tree t

from data, subject to the constraint that all operators belong to W. Each scenario is naturally accommodated through appropriate conditioning and marginalization within our probabilistic framework. 

Number of Experts. Models based on a finite set of pro-totypes are often considered a strict requirement to ensure global interpretability (Rudin, 2019; Rudin et al., 2022). In the proposed model, each prediction is obtained by execut-ing a tree selected from a finite set. This allows the global behavior of the task predictor to be inspected and validated by the user. To realize this, we model p(t | x) as a mixture obtained by marginalizing over M discrete indices: 

p(t | x) = 

> M

X

> m=1

p(t | m) p(m | x), (3) where p(m | x), called the selector , is a learnable distri-bution that routes each input x to one of the M available expressions (Debot et al., 2024). To ensure that each index 

m corresponds to a unique expression, p(t | m) is modeled as a degenerate distribution that places all its probability mass on the m-th expression tree δt(tm). By varying M ,we adjust the number of experts, balancing expressiveness (obtained with high values of M ) and interpretability (ob-tained by lower values of M ). Under the above assumptions Equation (2) can be rewritten in conditional form as: 

p(y, c | x) = p(c | x)

> M

X

> m=1

p(m | x) p(y | c, t m) . (4) 

Expressiveness-Interpretability trade-off. The flexibil-ity on the functional form allows M-CBEs to connect with different variants of the universal approximation theorem (Cybenko, 1989). Moreover, T and the number of experts 

M play a crucial role in calibrating the trade-off between the expressiveness and the complexity of the functions rep-resented by any expert, as we demonstrate in the following. 

Proposition 1. (1) Let T ′ = {t ∈ T : o ∈ { +, ×, σ }} ,being σ a unary non-polynomial operation. Then for each continuous function f and ϵ > 0 it exists an M-CBE f ∗

over T ′ with M = 1 experts, such that ∥f − f ∗∥∞ < ϵ . (2) Let Tlin denote the set of expression trees whose represented functions are linear. For each continuous function f and 

ϵ > 0, there exists a certain M > 0 and an M-CBE f ∗ over 

Tlin with M experts, such that ∥f − f ∗∥∞ < ϵ .

See Appendix A for a proof. Clearly, to approximate a function with only linear experts, 

M cannot be fixed a priori, but any function may require a different one. Next, we highlight how the approximation error can be bound given a set of polynomial experts. 

Proposition 2. Let Ω ⊆ [a, b ]n be a convex finite do-main, f : Ω → R a function of class C∞, and Ω be partitioned into M non-overlapping subspaces with equal 

3Mixture of Concept Bottleneck Experts Concept Encoder Selector  

> An object is launched with an initial speed of 10 m/s, at an angle of 45°. Find its horizontal position after 1 second. Sample 1 Sample 2 An object starts with an initial speed of 5 m/s and acceleration of 2 m/s 2along an horizontal path. Find its position after 3 seconds

Figure 3. Overview of M-CBEs. The selector identifies the appropriate expression based on the input (e.g., parabolic motion). Simultaneously, the Concept Encoder predicts the underlying physical concepts ( x0, v 0, α, t ) from the raw input. The architecture accommodates different user mental models by allowing functional forms to be represented as either parametric linear equations or symbolic expressions restricted to interpretable operators. The final output y is obtained by executing the selected function with the predicted concepts as arguments. 

measure, being Im the indicator function of the m-th sub-space. Given the space of expressions trees computing poly-nomial functions Tp, there exists a selection of polynomials 

ft ∈ H p(T ), t = 1 , . . . , M with maximum degree k, such that their composition f ⋆(x) = 

> M

X

> t=1

ft(x) · It(x) approxi-mates f with error: 

Error max = || f − f ⋆|| ∞ ≤ C · (b − a)k+1 

M k+1 

> n

· || f (k+1) || ,

where C is a fixed constant, || f (k+1) || is a Sobolev norm measuring how regular the target function is in terms of its 

k + 1 -th partial derivative. 

Proof is in the Appendix B. Interestingly, Theorem 2 provides an estimation of the ap-proximation errors for the general case of polynomial ex-perts. For instance, assuming the experts are linear (k = 1) 

the approximation error is bound by C · (b−a)2 

> M2
> n

· || f (2) || .This means that the error is low if the target function f is close to linear, e.g., || f (2) || ≈ 0, and that it can be arbitrarily lowered by increasing the number of experts M . We also notice that Theorem 2 assumes experts are selected from equal-measure subspaces, which corresponds to having each expert assigned to similar portion of the data in the empiri-cal distribution. Even if beyond the scope of this paper, this property could be enforced via an appropriate regularization mechanism in the selector. 

4. Instantiations of M-CBE 

M-CBEs encompass a broad spectrum of CBM architec-tures, including existing concept-based methods that can be interpreted as particular inference choices within our unified framework. For a mapping between specific choices in our framework and existing CBMs 1, we refer to Appendix C. Beyond capturing existing models, M-CBEs enables the creation of CBMs that explore previously unconsidered con-figurations. These instances model p(t | x) as a mixture of expression trees (Equation (3)), ensuring predictive accu-racy while maintaining global interpretability. For instance, models employing linear functions with more than one ex-pert have yet to be defined; at the same time, functional forms different from linear or Boolean expressions have yet to be considered. To fill this gap, we first propose a model fixing the structure (o, e, v) of the expression tree to a para-metric linear form over all concepts, while learning only its parameters. Second, we consider a scenario in which the user aims to discover the structure of an expression while retaining control by restricting the vocabulary to a set of known, admissible operators W. Under this setting, the functional form is fully determined by the chosen operators 

W.

Linear M-CBE (Lin-M-CBE). Our first instantiation ex-tends the standard CBM (Koh et al., 2020) to accommodate a mixture of specialized linear expressions while retaining its original functional form. Lin-M-CBE restricts the ex-pression tree to a linear structure but permits selection from a finite set of M distinct linear expressions. The conditional distribution in Equation (4) can be rewritten as: 

p(y, c | x) = p(c | x)

> M

X

> m=1

p(m | x) p(y | c, o l, e l, v, θ m) , (5) 

where ol and el are fixed realizations defining a linear structure, and v instantiates the function over all concepts. 

> 1

Some concept-based models (Espinosa Zarlenga et al., 2022a; Ismail et al., 2023) leak extra information from x, achieving higher accuracy at the expense of the semantic transparency. As we want to preserve semantic transparency, we do not consider these approaches in our framework. 

4Mixture of Concept Bottleneck Experts 

For example, in a regression task p(y | c, o l, e l, v, θ m) = 

N  y; f(ol,e l,v )(c, θ m), σ 2, where σ2 represents the vari-ance, f(ol,e l,v )(c; θm) = w⊤

> m

c + bm and θm = ( wm, b m).The model selects among these M expressions based on 

p(m | x), allowing flexibility in how concepts are combined to make predictions while maintaining a linear structure. 

Symbolic M-CBE (Sym-M-CBE). 

Beyond specifying the expression tree, a user can constrain the expression by only defining the vocabulary W of inter-pretable operators. The conditional distribution retains the structure of Equation (4). Learning the expression trees can be approached in two ways. The first uses differentiable symbolic regression meth-ods (Martius & Lampert, 2016) to learn each tm end-to-end with the selector and concept encoder. The second approach distills symbolic expressions from placeholder differentiable black-boxes (Alaa & Van der Schaar, 2019; Cranmer et al., 2020; Liu et al., 2025). Specifically, the latter approach decouples learning into three stages: (i) jointly training the concept encoder, selector, and placeholder black-box predic-tors (e.g., MLPs), allowing the data to be partitioned into M

mechanism-specific subsets; (ii) applying symbolic regres-sion to each subset to recover the corresponding expression tree tm; and (iii) replacing the placeholders with the ex-tracted symbolic expressions and fine-tuning the expression parameters θm end-to-end with the concept encoder and selector. This decoupled approach enables user adaptability: after initial training, different users can specify their own requirements to obtain expression trees aligned with their domain expertise, without retraining the encoder or selector. For this reason, we adopt this second strategy. To distill symbolic expressions from the placeholder black-box pre-dictors, we use the multi-population evolutionary algorithm provided by PySR (Cranmer, 2023). An ablation comparing with KANs (Liu et al., 2025) is provided in Appendix D. 

4.1. Training Objective 

Let D = {(x(i), c (i), y (i))}Ni=1 be a concept-annotated dataset of size N . Following the factorization in Equa-tion (5), we train Lin-M-CBE by maximizing the corre-sponding log-likelihood: 

L =

> N

X

> i=1

h

log p(c(i) | x(i)) + + log 

> M

X

> m=1

p(m | x(i)) p(y(i) | c(i), o, e, v, θ m)

i

,

where the first term corresponds to the concept prediction loss, and the second term corresponds to the task prediction loss. As previously noted, o = ol and e = el specifies a linear structure. For Sym-M-CBE, the first training stage jointly learns the concept encoder, selector, and a set of placeholder predictors by optimizing the same objective, where the predictors are implemented as MLPs with differ-ent operator and edge configurations (o, e ). In the second stage, the training data assigned to each expression is used to recover a symbolic expression tree via symbolic regression. In the final stage, the learned symbolic expressions replace the placeholders, and their parameters, θm, are fine-tuned end-to-end with the concept encoder and selector. Further training details are provided in Appendix E. 

5. Experimental Results 

We empirically validate the benefits of exploring the design plane. Our experiments assess four key properties of inter-pretable models: (i) Accuracy vs. Interpretability (Sec-tion 5.1): whether changing both functional form and num-ber of experts improves the accuracy-interpretability trade-off; (ii) Intervenability (Section 5.2): whether mechanism-aligned predictors respond more effectively to human inter-ventions on concepts; and (iii) Adaptability (Section 5.3): whether the proposed framework can accommodate user-specified constraints without retraining the full model. 

Datasets. We evaluate on four synthetic and five real-world datasets, covering classification and regression with both categorical and continuous concepts. Synthetic : MNIST-Arithm , a modified version of MNIST (LeCun et al., 2010) with digit pairs separated by an arithmetic operator to pre-dict; dSprites-Exp , a variant of dSprites (Matthey et al., 2017) where the target is an exponential function of the object’s coordinates; and Pendulum (Yang et al., 2020), a dataset of pendulum images with the task of predicting the pendulum’s x-axis position. For real-world data, we use 

MAWP (Koncel-Kedziorski et al., 2016), a dataset contain-ing simple math problems in a textual form, AWA2 (Xian et al., 2017), a classification dataset with 50 animal classes and 85 concepts, and CUB-200 (He & Peng, 2019), a bird classification dataset with 200 species and 112 concepts. Following Espinosa Zarlenga et al. (2025), we also evaluate incomplete versions of these datasets to study performance under missing concepts. Finally, to assess settings with-out concept annotations, we apply the label-free method of Oikarinen et al. (2023) to CIFAR-10 (Krizhevsky et al., 2009). 

Baselines. For our evaluation, we compare several concept-based architectures: CBM (Koh et al., 2020) (equivalent to Linear M-CBE with M =1 ); CEM (Espinosa Zarlenga et al., 2022a) with a non-interpretable predictor; locally inter-pretable DCR (Barbiero et al., 2023) and LICEM (De San-tis et al., 2025); and globally interpretable CMR (Debot et al., 2024). Logic-based baselines (CMR, DCR) are omit-ted in regression tasks because they assume discrete con-cepts and outputs. We also include a standard Blackbox 

5Mixture of Concept Bottleneck Experts 7 30 127 ∞0.8                              

> 0.9
> 1.0
> 1.1
> 1.2
> MAE
> dSprites-Exp.
> 632 175 ∞0
> 1
> 2
> 3
> Pendulum
> 322 175 ∞
> 2
> 4
> 6
> 8
> MNIST-Arith.
> 437 325 ∞0
> 2
> 4
> 6MAWPS BlackBox
> CEM
> LICEM
> DCR
> CBM
> CMR
> MLP-M-CBE
> Prior-M-CBE
> Lin-M-CBE
> Sym-M-CBE
> 3K 45K 519K ∞
> Complexity
> 20
> 30
> 40
> 50 CUB200-Incomplete
> 126 13K 1445K ∞
> Complexity
> 2
> 3
> 4
> 5
> 70
> 80
> 90 AWA2
> 442 2K 19K ∞
> Complexity
> 2
> 3
> 4
> 25
> 26
> 27 AWA2-Incomplete
> 556 74K 9999K ∞
> Complexity
> 20
> 30
> 80
> 100 CUB200
> 321 21K 1390K ∞
> Complexity
> 10
> 15
> 20
> 80
> 100 CIFAR10
> Error Rate

Figure 4. The x-axis denotes model complexity, measured as the total number of nodes across all expression trees used by the task predictor. The y-axis reports MAE for regression tasks (top) and error rate for classification tasks (bottom). For multi-expert models, multiple points are shown corresponding to different numbers of experts (1–5). The dotted curve indicates the Pareto frontier, while the shaded region marks dominated solutions. Prior-M-CBE is excluded from the Pareto frontier as it relies on ground-truth expressions. CEM and BlackBox are plotted as horizontal lines, since their label-predictor complexity is undefined (they do not operate on concept predictions). Error bars indicate 95% confidence intervals over five random seeds. 

DNN as an accuracy upper-bound, and we evaluate two other instances of our framework: Prior-M-CBE , which uses ground-truth expressions when available, and MLP-M-CBE , which uses a mixture of MLPs. CMR and M-CBEs are tested with 1-5 experts (more details in Appendix F.1). 

Metrics. For accuracy , we evaluate performance using the Mean Absolute Error (MAE) for regression and error rate ( 100 −Accuracy %) for classification. For interpretabil-ity , guided by the principle that brevity facilitates human comprehension (Miller, 1956; Narayanan et al., 2018), and assuming individual symbols are interpretable by the user, we assess complexity via standard metrics from symbolic regression: node count, maximum depth, and the number of variables and operators (Smits & Kotanchek, 2005). Fig-ure 4 represents complexity as the total number of nodes in the expression tree. Further results and additional details are reported in Appendix G. When the number of expression trees is higher than one, the complexities are summed across all expression trees. For intervenability , we follow Koh et al. (2020); Espinosa Zarlenga et al. (2022a) and measure task error as a function of the fraction of concepts replaced with ground-truth values. To evaluate the adaptability of the pro-posed model, we compare the performance of Sym-M-CBE when employing different operator sets. 

5.1. Accuracy vs. Interpretability 

We evaluate whether changing the number of functions, along with their complexity, improves the accuracy-interpretability trade-off. Figure 4 summarizes our findings, while detailed results, including those for other complexity metrics and concept accuracy, are provided in Appendix K. 

Exploring M-CBEs design space allows finding the best trade-offs (Figure 4). Our results demonstrate that no sin-gle architectural choice is universally superior; rather, only exploring the M-CBEs design space guarantees finding the most accurate and interpretable model for the task at hand. According to the dataset, we find that relaxing functional constraints with Symbolic M-CBE enables the discovery of compact, high-accuracy expressions that dominate the Pareto frontier, particularly in regression tasks (e.g., on Pendulum). Conversely, for high-dimensional classifica-tion tasks, instantiating the framework with Linear M-CBE experts provides a robust balance, offering competitive ac-curacy with low complexity. This highlights that explicit control over the number of experts and functional form is key for finding the best accuracy-interpretability trade-off. 

Scalability challenges for Boolean functional forms (Fig-ure 4, bottom). Instantiations employing Boolean expres-sions (e.g., recovering CMR, DCR) perform well when con-cept sets are small (e.g., AWA2-Incomplete and CUB200-Incomplete), offering good accuracy and very low complex-ity. However, they degrade substantially as the number of concepts increases, reaching near-100% error on CUB200 and CIFAR10 where concepts exceed 100. We attribute this to the rigid nature of purely conjunctive rules: when predictions rely on formulas like c1 ∧ . . . ∧ ck, a single mispredicted concept invalidates the entire rule. This fail-ure mode becomes increasingly critical as K grows (see Appendix H for an ablation on concept size). In contrast, 6Mixture of Concept Bottleneck Experts 

instantiations like Lin-M-CBE and Sym-M-CBE maintain robust performance across all concept set sizes. Moreover, these flexible forms naturally adapt to continuous concepts and targets, as demonstrated in the regression tasks. 

Black-box task predictors are generally Pareto-dominated (Figure 4). Employing unconstrained MLPs as task predictors is rarely optimal. M-CBEs instantiations with structured functional forms (e.g., Lin-M-CBE, Sym-M-CBE) frequently match or exceed MLP accuracy on both classification and regression tasks, while being substantially less complex. Consequently, black-box predictors sporad-ically appear on the Pareto front, demonstrating that over-parameterized predictors are suboptimal when the functional form aligns well with the underlying task. 

Multiple experts compensate for incomplete concepts and variable task logic (Figure 4). Finally, we analyze the dimension of expert cardinality. We find that increas-ing the number of experts ( M > 1) is critical in two key scenarios: first, when concept sets are incomplete (AWA2-Incomplete, CUB200-Incomplete), selecting among mul-tiple functions recovers predictive performance; second, when the concept-to-task relationship varies across the in-put space (e.g., MAWPS), multiple experts allow the model to adapt to local semantic contexts. This highlights that exploring expert cardinality is also vital to ensure model performance. 0 2 4 6

Weight 

> 0.0
> 0.2
> Density
> 0.0
> 0.5
> Probability

Figure 5. Weight distributions for the concept “has underparts color brown” toward class ”Crested Auklet”, comparing LICEM (blue) and Lin-M-CBE (orange, memory size = 2 ). 

Finite number of experts allows global interpretability. (Figure 5) Models generating input-dependent parameters (LICEM and DCR) produce a distinct expression tree for each sample. This implies an infinite model complexity, as each input corresponds to a new expression tree, making global inspection of the model infeasible. In contrast, M-CBEs learn from a finite set of expressions, which can be fully inspected and verified in finite time. As shown in Figure 5, LICEM produces a continuous distribution of weights around zero, reflecting its unconstrained and non-discrete parameterization. By contrast, when constrained to two parameter sets, the Linear M-CBE model selects between two discrete weight configurations, resulting in a decision process that a human can directly inspect. 

Table 1. Tree Edit Distance (TED) to the ground truth expres-sion. In three datasets, Symbolic M-CBE recovers the exact data-generating mechanism (TED ≈ 0).                 

> Model dSprites-Exp. Pendulum MNIST-Arith. MAWPS Lin-M-CBE 18 .00 ±0.00 5.71 ±0.00 4.71 ±3.23 15 .00 ±0.00
> MLP-M-CBE 28 .00 ±0.00 43 .63 ±0.00 22 .24 ±1.75 47 .00 ±0.00
> Sym-M-CBE 13 .00 ±0.00 0.00 ±0.00 0.05 ±0.09 0.00 ±0.00

Table 2. Performance of Sym-M-CBE across small (S), medium (M), and complete (C) operator sets, measured by MAE and model complexity ( ±95% confidence interval).                      

> Pendulum MAWPS Model MAE Complexity MAE Complexity MLP-M-CBE 0.46 ±0.08 706 .0<0.01 1.05 ±0.04 3592 .0<0.01
> Sym-M-CBE (S) 3.80 ±0.02 3.0<0.01 2.73 ±0.03 4.0<0.01
> Sym-M-CBE (M) 2.12 ±1.53 19 .3±5.81.29 ±0.08 24 .0<0.01
> Sym-M-CBE (C) 0.46 ±0.13 6.0<0.01 1.36 ±0.21 24 .0<0.01

5.2. Intervenability 

CBMs allow humans to intervene by replacing predicted concepts with counterfactual values. We evaluate the model’s response to oracle interventions (replacing pre-dicted concepts with ground-truth values) by progressively increasing the fraction of corrected concepts. 

Symbolic predictors align and discover true mechanisms (Figure 6, top, Table 1). On regression tasks, Sym-M-CBE displays near-ideal intervenability: the MAE drops sharply to zero as the intervention probability approaches unity ( pint → 1). This intervention response stems from Sym-M-CBE’s ability to recover expressions that closely approximate the true concept-to-task function, (e.g., on Pen-dulum, 8.00 ∗ sin (θ) + 10 .00 ). Table 1 confirms that Sym-M-CBE consistently achieves the lowest Tree Edit Distance (TED) to ground-truth expressions. The expressions learned by the methods are reported in Appendix J. 

Mixture of linear predictors excel on classification (Fig-ure 6, bottom). On classification tasks, Lin-M-CBE emerges as the globally interpretable model most responsive to interventions. This is likely related to the setting: the diffuse semantic evidence required for tasks like CUB200 and CIFAR10 makes the weighted summation of concepts a more stable and responsive mechanism for intervention. 

5.3. Adaptability 

While M-CBEs allow users to instantiate any interpretable functional form (e.g., linear, polynomial), Symbolic M-CBE also supports post-hoc customization without retraining .Once the encoder and expert selector are trained, users can modify their operator vocabulary W to generate tailored symbolic expressions on demand. This enables switching from expert-level trigonometric forms to student-level sim-7Mixture of Concept Bottleneck Experts 0 0.25 0.5 0.75 10.00                                     

> 0.25
> 0.50
> 0.75
> 1.00
> 1.25
> 1.50
> MAE
> dSprites-Exp.
> 00.25 0.5 0.75 10.00
> 2.00
> 4.00
> 6.00
> MAWPS CEM
> LICEM
> DCR
> CBM
> CMR
> MLP-M-CBE
> Prior-M-CBE
> Lin-M-CBE
> Sym-M-CBE
> 00.25 0.5 0.75 1
> pint
> 0.00
> 10.00
> 20.00
> 30.00
> 40.00
> CUB200-Incomplete
> 00.25 0.5 0.75 10.00
> 1.00
> 2.00
> 3.00
> Pendulum
> 00.25 0.5 0.75 10.00
> 2.00
> 4.00
> 8.50
> 9.00 MNIST-Arith.
> 00.25 0.5 0.75 1
> pint
> 0.00
> 2.00
> 4.00
> 70.00
> 75.00
> 80.00 AWA2
> 00.25 0.5 0.75 1
> pint
> 0.00
> 2.00
> 4.00
> 22.50
> 25.00
> AWA2-Incomplete
> 00.25 0.5 0.75 1
> pint
> 0.00
> 20.00
> 40.00
> 80.00
> 100.00 CUB200
> 00.25 0.5 0.75 1
> pint
> 10.00
> 15.00
> 20.00
> 80.00
> 90.00
> CIFAR10
> Error Rate

Figure 6. Effect of interventions on model performance. MAE (top) and error rate (bottom) as a function of intervention probability pint .Shaded areas show 95% confidence intervals over 5 seeds. For models with multiple experts, we use the configuration at the Pareto knee .The knee point is identified using the maximum distance to chord method: the point with maximum perpendicular distance to the line connecting the best complexity and best accuracy extremes, representing the optimal trade-off. 

ple polynomials, using the same learned representations. 

Sym-M-CBE allows post-hoc adaptation to different users (Table 2). This capability is shown by distilling functions for three simulated user profiles: W = {+, −} 

(Small); W = {+, −, ×} (Medium); and an extended set including transcendental operators (Complete). Results on 

Pendulum and MAWPS (Table 2) confirm that Sym-M-CBE robustly adapts to these diverse constraints, without com-promising its performance, showing higher flexibility. 

6. Related works 

Concept-based XAI (C-XAI) (Kim et al., 2018; Poeta et al., 2023) emerged to address the limited interpretability of standard attribution methods for laypeople (Rudin, 2019; Kim et al., 2023), by interpreting intermediate model rep-resentations via human-understandable concepts. Concept Bottleneck Models (CBMs) (Koh et al., 2020) extend this approach by explicitly training models to align with human semantics. Still, CBMs face significant issues: reduced ac-curacy compared to unrestricted models (Debot et al., 2024), limited global interpretability (Barbiero et al., 2023; De San-tis et al., 2025), and costly concept annotations (Oikarinen et al., 2023; Debole et al., 2025). Our work addresses the first two trade-offs by combining multiple task predictors with user-defined functional forms to balance accuracy and transparency, while experimentally demonstrating compati-bility with label-efficient methods (Oikarinen et al., 2023). In the context of XAI, Symbolic Regression (SR) is increas-ingly used to replace opaque models with explicit mathe-matical expressions (Dong & Zhong, 2025). These efforts fall into two categories: intrinsic approaches that embed symbolic operators directly into architectures (Sahoo et al., 2018; Biggio et al., 2021), and post-hoc approaches that approximate black-boxes with symbolic surrogates (Alaa & Van der Schaar, 2019; Bendinelli et al., 2023). While these methods effectively balance approximation error and expression complexity (Langley, 1979; Langley et al., 1981; Koza, 1994), they fundamentally assume a single global equation , failing to capture context-dependent reasoning. Moreover, to our knowledge, no prior work adapts SR to operate on top of concept-based representations. 

7. Conclusion 

We introduced M-CBEs, a unified framework that general-izes CBMs by enabling control over two key dimensions: the functional form and the number of experts. Our frame-work subsumes existing concept-based methods while ex-posing a largely unexplored two-dimensional space, en-abling two novel instantiations: Linear M-CBE and Sym-bolic M-CBE. Empirical results demonstrate that navigating this space is essential for optimal accuracy-interpretability trade-offs: algebraic forms outperform Boolean logic in high-dimensional spaces (up to +65% accuracy), multiple experts compensate for incomplete concepts, and Sym-M-CBE recovers ground-truth expressions with superior inter-vention responsiveness. M-CBEs establishes a principled approach for developing interpretable models that adapt to both task requirements and diverse user needs. 

Limitations. While individual expert functions in M-CBEs are interpretable by design, the selector network p(m | x)

that routes inputs to experts operates as a black-box, limit-8Mixture of Concept Bottleneck Experts 

ing transparency about when and why a particular expert is selected. Additionally, Sym-M-CBE relies on heuristic search methods that are computationally intensive, partic-ularly for large operator vocabularies or high-dimensional concept spaces. Finally, the number of experts M must be specified as a hyperparameter, requiring users to balance expressiveness and interpretability through model selection. 

Impact Statement 

The societal implications of this work are predominantly positive. By enabling users to align model reasoning with computations they may interpret, M-CBEs democratizes AI accessibility across varying expertise levels, from domain specialists utilizing complex mathematical expressions to lay users favoring simplified formulations. Ethically, our framework advances responsible AI deployment by ensuring predictions remain grounded in finite, inspectable sets of interpretable expressions. 

References 

Act, E. A. I. The eu artificial intelligence act. European Union , 2024. Alaa, A. M. and Van der Schaar, M. Demystifying black-box models with symbolic metamodels. Advances in neural information processing systems , 32, 2019. Artetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M., Shleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R., et al. Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684 , 2021. Barbiero, P., Ciravegna, G., Giannini, F., Espinosa Zarlenga, M., Magister, L. C., Tonda, A., Li ´o, P., Precioso, F., Jam-nik, M., and Marra, G. Interpretable neural-symbolic concept reasoning. In International Conference on Ma-chine Learning , pp. 1801–1825. PMLR, 2023. Barbiero, P., De Felice, G., Espinosa Zarlenga, M., Ciravegna, G., Dominici, G., De Santis, F., Casanova, A., Debot, D., Giannini, F., Diligenti, M., and Marra, G. PyTorch Concepts, 3 2025. URL https://github. com/pyc-team/pytorch_concepts .Bendinelli, T., Biggio, L., and Kamienny, P.-A. Controllable neural symbolic regression. In International Conference on Machine Learning , pp. 2063–2077. PMLR, 2023. Biggio, L., Bendinelli, T., Neitz, A., Lucchi, A., and Paras-candolo, G. Neural symbolic regression that scales. In 

International Conference on Machine Learning , pp. 936– 945. Pmlr, 2021. Ciravegna, G., Barbiero, P., Giannini, F., Gori, M., Li ´o, P., Maggini, M., and Melacci, S. Logic explained networks. 

Artificial Intelligence , 314:103822, 2023. Colombini, J. J., Bonchi, F., Giannini, F., Giannotti, F., Pellungrini, R., Frosini, P., et al. Mathematical foundation of interpretable equivariant surrogate models. In The 3nd World Conference on eXplainable Artificial Intelligence, XAI-2025 , 2025. Cranmer, M. Interpretable machine learning for science with pysr and symbolicregression. jl. arXiv preprint arXiv:2305.01582 , 2023. Cranmer, M., Sanchez Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D., and Ho, S. Discovering sym-bolic models from deep learning with inductive biases. 

Advances in neural information processing systems , 33: 17429–17442, 2020. Cybenko, G. Approximation by superpositions of a sig-moidal function. Mathematics of control, signals and systems , 2(4):303–314, 1989. De Felice, G., Flores, A. C., De Santis, F., Santini, S., Schneider, J., Barbiero, P., and Termine, A. Causally reliable concept bottleneck models. arXiv preprint arXiv:2503.04363 , 2025. De Santis, F., Bich, P., Ciravegna, G., Barbiero, P., Giordano, D., and Cerquitelli, T. Linearly-interpretable concept embedding models for text analysis. Machine Learning ,114(10):224, 2025. Debole, N., Barbiero, P., Giannini, F., Passerini, A., Teso, S., and Marconato, E. If concept bottlenecks are the ques-tion, are foundation models the answer? arXiv preprint arXiv:2504.19774 , 2025. Debot, D., Barbiero, P., Giannini, F., Ciravegna, G., Dili-genti, M., and Marra, G. Interpretable concept-based memory reasoning. In The Thirty-eighth Annual Confer-ence on Neural Information Processing Systems , 2024. Dominici, G., Barbiero, P., Espinosa Zarlenga, M., Termine, A., Gjoreski, M., Marra, G., and Langheinrich, M. Causal concept graph models: Beyond causal opacity in deep learning. In The Thirteenth International Conference on Learning Representations , 2025. URL https:// openreview.net/forum?id=lmKJ1b6PaL .Dong, J. and Zhong, J. Recent advances in symbolic regres-sion. ACM Computing Surveys , 57(11):1–37, 2025. Dur ´an, J. M. and Jongsma, K. R. Who is afraid of black box algorithms? on the epistemological and ethical basis of trust in medical ai. Journal of medical ethics , 47(5): 329–335, 2021. 9Mixture of Concept Bottleneck Experts 

Espinosa Zarlenga, M., Barbiero, P., Ciravegna, G., Marra, G., Giannini, F., Diligenti, M., Precioso, F., Melacci, S., Weller, A., Lio, P., et al. Concept embedding models. In 

NeurIPS 2022-36th Conference on Neural Information Processing Systems , 2022a. Espinosa Zarlenga, M., Barbiero, P., Ciravegna, G., Marra, G., Giannini, F., Diligenti, M., Shams, Z., Precioso, F., Melacci, S., Weller, A., et al. Concept embedding models: Beyond the accuracy-explainability trade-off. 

Advances in neural information processing systems , 35: 21400–21413, 2022b. Espinosa Zarlenga, M., Dominici, G., Barbiero, P., Shams, Z., and Jamnik, M. Avoiding leakage poisoning: Concept interventions under distribution shifts. arXiv preprint arXiv:2504.17921 , 2025. EUGDPR. GDPR. General data protection regulation, 2017. Gkintoni, E., Antonopoulou, H., Sortwell, A., and Halkiopoulos, C. Challenging cognitive load theory: The role of educational neuroscience and artificial intelligence in redefining learning efficacy. Brain Sciences , 15(2):203, 2025. He, X. and Peng, Y. Fine-grained visual-textual representa-tion learning. IEEE Transactions on Circuits and Systems for Video Technology , 30(2):520–531, 2019. Ismail, A. A., Adebayo, J., Bravo, H. C., Ra, S., and Cho, K. Concept bottleneck generative models. In The Twelfth International Conference on Learning Representations ,2023. Ismail, A. A., Oikarinen, T., Wang, A., Adebayo, J., Stanton, S., Joren, T., Kleinhenz, J., Goodman, A., Bravo, H. C., Cho, K., et al. Concept bottleneck language models for protein design. arXiv preprint arXiv:2411.06090 , 2024. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. Neural computation ,3(1):79–87, 1991. Jang, E., Gu, S., and Poole, B. Categorical repa-rameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144 , 2016. Keijzer, M. and Foster, J. Crossover bias in genetic program-ming. In European Conference on Genetic Programming ,pp. 33–44. Springer, 2007. Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., et al. Interpretability beyond feature attribu-tion: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning ,pp. 2668–2677. PMLR, 2018. Kim, S. S., Watkins, E. A., Russakovsky, O., Fong, R., and Monroy-Hern ´andez, A. ” help me help the ai”: Under-standing how explainability can support human-ai inter-action. In proceedings of the 2023 CHI conference on human factors in computing systems , pp. 1–17, 2023. Koh, P. W., Nguyen, T., Tang, Y. S., Mussmann, S., Pierson, E., Kim, B., and Liang, P. Concept bottleneck models. In 

International conference on machine learning , pp. 5338– 5348. PMLR, 2020. Koncel-Kedziorski, R., Roy, S., Amini, A., Kushman, N., and Hajishirzi, H. Mawps: A math word problem reposi-tory. In Proceedings of the 2016 conference of the north american chapter of the association for computational lin-guistics: human language technologies , pp. 1152–1157, 2016. Koza, J. R. Genetic programming as a means for program-ming computers by natural selection. Statistics and com-puting , 4(2):87–112, 1994. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Langley, P. Rediscovering physics with bacon. 3. In IJCAI ,volume 6, pp. 161–188, 1979. Langley, P., Bradshaw, G. L., and Simon, H. A. Bacon. 5: The discovery of conservation laws. In IJCAI , volume 81, pp. 121–126, 1981. LeCun, Y., Cortes, C., Burges, C., et al. Mnist handwritten digit database, 2010. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668 , 2020. Leshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. Mul-tilayer feedforward networks with a nonpolynomial ac-tivation function can approximate any function. Neural networks , 6(6):861–867, 1993. Lipton, Z. C. The mythos of model interpretability. Com-munications of the ACM , 61(10):36–43, 2018. Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halverson, J., Soljacic, M., Hou, T. Y., and Tegmark, M. KAN: Kol-mogorov–arnold networks. In The Thirteenth Interna-tional Conference on Learning Representations , 2025. Mahinpei, A., Clark, J., Lage, I., Doshi-Velez, F., and Pan, W. Promises and pitfalls of black-box concept learning models. arXiv preprint arXiv:2106.13314 , 2021. 10 Mixture of Concept Bottleneck Experts 

Marconato, E., Passerini, A., and Teso, S. Glancenets: In-terpretable, leak-proof concept-based models. Advances in Neural Information Processing Systems , 35:21212– 21227, 2022. Martius, G. and Lampert, C. H. Extrapolation and learning equations. arXiv preprint arXiv:1610.02995 , 2016. Matthey, L., Higgins, I., Hassabis, D., and Lerchner, A. dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017. Miller, G. A. The magical number seven, plus or minus two: Some limits on our capacity for processing information. 

Psychological review , 63(2):81, 1956. Miller, T. Explanation in artificial intelligence: Insights from the social sciences. Artificial intelligence , 267:1–38, 2019. Mitchell, R. J. Expression Trees , pp. 219–231. Macmillan Education UK, London, 1991. ISBN 978-1-349-12439-8. doi: 10.1007/978-1-349-12439-8 18. URL https:// doi.org/10.1007/978-1-349-12439-8_18 .M ¨oßner, B. and Reif, U. Error bounds for polynomial tensor product interpolation. Computing , 86(2):185–197, 2009. Narayanan, M., Chen, E., He, J., Kim, B., Gershman, S., and Doshi-Velez, F. How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation. arXiv preprint arXiv:1802.00682 , 2018. Oikarinen, T., Das, S., Nguyen, L. M., and Weng, T.-W. Label-free concept bottleneck models. In The Eleventh International Conference on Learning Representations ,2023. Poeta, E., Ciravegna, G., Pastor, E., Cerquitelli, T., and Bar-alis, E. Concept-based explainable artificial intelligence: A survey. arXiv preprint arXiv:2312.12936 , 2023. Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature machine intelligence , 1(5):206– 215, 2019. Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., and Zhong, C. Interpretable machine learning: Funda-mental principles and 10 grand challenges. Statistic Sur-veys , 16:1–85, 2022. Sahoo, S., Lampert, C., and Martius, G. Learning equations for extrapolation and control. In International Conference on Machine Learning , pp. 4442–4450. Pmlr, 2018. Schmidt, M. and Lipson, H. Distilling free-form natural laws from experimental data. science , 324(5923):81–85, 2009. Schumaker, L. Spline functions: basic theory . Cambridge university press, 2007. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. 

arXiv preprint arXiv:1701.06538 , 2017. Smits, G. F. and Kotanchek, M. Pareto-front exploitation in symbolic regression. In Genetic programming theory and practice II , pp. 283–299. Springer, 2005. Vandenhirtz, M., Laguna, S., Marcinkevi ˇcs, R., and Vogt, J. Stochastic concept bottleneck models. Advances in Neu-ral Information Processing Systems , 37:51787–51810, 2024. Xian, Y., Schiele, B., and Akata, Z. Zero-shot learning-the good, the bad and the ugly. In Proceedings of the IEEE conference on computer vision and pattern recognition ,pp. 4582–4591, 2017. Yang, M., Liu, F., Chen, Z., Shen, X., Hao, J., and Wang, J. Causalvae: Structured causal disentanglement in vari-ational autoencoder. arXiv preprint arXiv:2004.08697 ,2020. 11 Mixture of Concept Bottleneck Experts 

A. Proof of Theorem 1 

Proof. The claims are straight consequences of existing universal approximation theorems. In particular, (1) follows from the fact that an MLP (that can be represented as an expression tree with + and × among operation nodes and σ activation functions) with a single hidden layer is a universal approximator if and only if its activation function is non-polynomial (Leshno et al., 1993). On the other hand, (2) is a consequence of the classic result in mathematical analysis that any continuous function can be approximated by a piecewise linear function (Schumaker, 2007). 

B. Proof of Theorem 2 

Proof. The result follows from standard approximation bounds developed for numerical finite methods and interpolation methods with multivariate polynomials (M¨ oßner & Reif, 2009). The bound is generally defined to be: 

|| f − f ⋆|| ∞ ≤ C · hk+1 · || f (k+1) || , (6) where h is the grid size over the input dimensions. By assuming a regular spacing so that the nM splines cover nM subspaces with equal measure, we can determine h = b−a 

> N
> √nM

. Replacing the estimation of the value of h into Equation (6) completes the proof. 

C. Out-of-the-box M-CBEs 

M-CBE characterize a wide range of CBMs architectures, including existing concept-based methods that correspond to specific inference choices within our generalized framework. In all methods discussed below, the operator set o and expression tree structure e are fixed by the model class; we therefore condition on (o, e ) and focus on how the parameters θ

are modeled. 

CBM. The original CBM (Koh et al., 2020) fixes the expression tree to a linear function over concepts and learns a single set of global parameters shared across all samples. In our framework, this corresponds to approximating p(θ | x)

with a delta distribution that places all its mass on a single, input-independent parameter value θ, where (o, e ) define a linear expression (i.e., o = {+, ×} with appropriate edges). The conditional distribution induced by the CBM can be expressed as p(y, c | x; o, e ) = p(y | c; o, e, θ ) p(c | x). For a regression task, the task predictor is modeled as a Gaussian: 

p(y | c; o, e, θ ) = N  y; f(o,e )(c; θ), σ 2, where the mean is a linear function of the concepts: f(o,e )(c; θ) = θ⊤c + b, with 

θ = w representing the weights on the concepts, b the bias term, and σ2 denoting the variance of the Gaussian noise. 

CMR. CMR (Debot et al., 2024) instead constrains the expression tree to Boolean formulas and restricts parameters to a finite discrete set. Each concept can appear positively (+1) , negated (−1) , or be absent (0) from the formula. CMR learns a memory of M parameter configurations {θ1, . . . , θ M } and models p(θ | x) = PMm=1 p(m | x) p(θ | m).In the limiting case M → ∞ , the distribution is no longer discretized but instead produces sample-specific parameters. Both 

LICEM (De Santis et al., 2025) and DCR (Barbiero et al., 2023) correspond to this limiting case, differing only in the structure enforced by (o, e ). LICEM constrains the expression tree’s structure to represent a linear equation, while DCR constrains it to represent a Boolean expression. 

D. Symbolic predictor ablation 

We compare Sym-M-CBE with Kan-M-CBE, an alternative instantiation that uses Kolmogorov-Arnold Networks (Liu et al., 2025). Table 3 shows that both models achieve comparable MAE, with Kan-M-CBE obtaining slightly lower errors on all datasets except MNIST-Arith. Figure 7 demonstrates that Kan-M-CBE exhibits strong responsiveness to interventions, achieving MAE values at pint ≈ 1.0

comparable to Prior-M-CBE. However, despite achieving competitive predictive accuracy and intervention responsiveness, Kan-M-CBE produces expressions with substantially higher complexity (Table 4) compared to other methods. This stems from the KAN training procedure: the model first learns network activations (splines), then applies sparsification, and finally adds affine parameters to each spline (e.g., transforming an activation g(x) into a · g(b · x + c) + d), resulting in considerably larger expressions. 12 Mixture of Concept Bottleneck Experts 

Table 3. Predictive performance (MAE) of all models on regression tasks, reported as mean ± 95% confidence interval. Model dSprites-Exp. Pendulum MNIST-Arith. MAWPS BlackBox 0.93 ±0.03 0.14 ±0.05 2.12 ±0.02 0.79 ±0.04 

CEM 0.88 ±0.03 0.60 ±0.10 1.80 ±0.18 0.73 ±0.02 

LICEM 0.90 ±0.01 0.77 ±0.07 1.55 ±0.11 0.74 ±0.03 

MLP-M-CBE 1.06 ±0.05 2.30 ±0.46 2.81 ±0.14 1.95 ±0.23 

Prior-M-CBE 0.90 ±0.01 0.24 ±0.01 2.67 ±0.11 1.00 ±0.01 

Kan-M-CBE 0.91 ±0.01 0.24 ±0.02 4.47 ±1.26 0.96 ±0.02 

Lin-M-CBE 1.13 ±0.01 3.04 ±0.00 2.74 ±0.05 2.63 ±0.12 

Sym-M-CBE 1.02 ±0.01 0.33 ±0.12 2.92 ±0.16 1.33 ±0.18 0 0.25 0.5 0.75 1             

> pint
> 0.00
> 0.50
> 1.00
> MAE
> dSprites-Exp.
> 00.25 0.5 0.75 1
> pint
> 0.00
> 1.00
> 2.00
> 3.00
> Pendulum
> 00.25 0.5 0.75 1
> pint
> 0.00
> 2.00
> 4.00
> 6.00 MNIST-Arith.
> 00.25 0.5 0.75 1
> pint
> 0.00
> 2.00
> 4.00
> 6.00
> MAWPS
> CEM
> LICEM
> MLP-M-CBE
> Prior-M-CBE
> Kan-M-CBE
> Lin-M-CBE
> Sym-M-CBE

Figure 7. Effect of interventions on model performance. MAE as a function of intervention probability pint . Shaded areas show 95% 

confidence intervals over 5 seeds. 

The compactness of expressions extracted from KAN networks depends critically on multiple hyperparameters, including entropy regularization for sparsification, pruning strength, and the symbolic substitution process that replaces activations with mathematical symbols. This increased complexity makes Kan-M-CBE more difficult to tune in practice while yielding inferior performance in terms of expression size and, ultimately, alignment with ground-truth mechanisms, as shown in Table 5. We emphasize that this does not imply KANs are inherently inferior; rather, we found them more challenging to train and tune compared to the genetic programming-based symbolic regression approach implemented in PySR (Cranmer, 2023). 

E. Training details 

All model variants share a common training framework implemented in PyTorch Lightning. We use AdamW as the optimizer with dataset-specific learning rates (ranging from 10 −4 to 10 −1) and a ReduceLROnPlateau scheduler that decreases the learning rate by a factor of γ = 0 .5 when the validation loss fails to improve for 25 consecutive epochs. Early stopping is applied with a patience of 50 . Training proceeds for a maximum of 600 epochs. To minimize leakage (Marconato et al., 2022), all the concept-based methodologies are trained in a disjoint manner (Koh et al., 2020): the task predictor uses ground-truth concept labels during training. In addition to that, when the concepts are binary, we apply hard thresholding. The total loss is a weighted combination of concept and task prediction losses: 

Ltotal = λcLconcept + λy Ltask (7) where λc = 1 .0 and λy = 0 .1 across all models. For classification tasks, we use binary cross-entropy for concept prediction and cross-entropy for task prediction. For regression tasks, mean squared error is employed for both. In all methodologies, sparsity is promoted by tuning the respective hyperparameters in order to maximize the accuracy-interpretability trade-off. Nevertheless, for the methodologies we proposed, we followed a multi-stage pipeline. For all the methodologies using a mixture of experts, we employ a selector implemented as an MLP with output size equal to the number of experts. During training, we employ a Gumbel-Softmax (Jang et al., 2016) to sample an index according to the distribution produced by the selector p(m | x) for the specific sample. We gradually reduce the temperature τ of the 

Gumbel-Softmax from τ = 2 to τ = 0 .05 following a cosine decay. This schedule ensures that the selection distribution becomes increasingly peaked as training progresses. At test time, a single expression index m is sampled from p(m | x),and the prediction is computed using only the selected expression. 13 Mixture of Concept Bottleneck Experts 

Table 4. Complexity (Node Count) of learned expressions, reported as mean ± 95% confidence interval. Model dSprites-Exp. Pendulum MNIST-Arith. MAWPS MLP-M-CBE 41 .6±8.6 41 .6±8.6 184 .0±0.0 310 .4±65 .9

Prior-M-CBE 10 .0±0.0 6.0±0.0 16 .0±0.0 24 .0±0.0

Kan-M-CBE 113 .8±14 .7 101 .6±15 .2 267 .0±35 .4 891 .2±41 .6

Lin-M-CBE 7.4±1.2 8.0±0.0 32 .0±0.0 44 .0±0.0

Sym-M-CBE 13 .2±2.0 10 .6±6.7 12 .0±2.0 25 .0±2.0

Table 5. TED between learned and ground truth expressions, reported as mean ± 95% confidence interval. Model dSprites-Exp. Pendulum MNIST-Arith. MAWPS MLP-M-CBE 45 .45 ±8.55 39 .02 ±8.66 43 .92 ±0.08 80 .60 ±16 .46 

Prior-M-CBE 0.00 ±0.00 0.00 ±0.00 0.00 ±0.00 0.00 ±0.00 

Lin-M-CBE 17 .40 ±1.18 5.71 ±0.00 4.70 ±0.01 15 .00 ±0.00 

Sym-M-CBE 9.85 ±8.19 7.02 ±10 .90 0.05 ±0.02 0.70 ±1.37 

Lin-M-CBE. Training proceeds in two stages. The first stage trains all components (concept encoder, selector, and linear memory) end-to-end with ℓ1 regularization on weight matrices ( λ1 = 10 −5) and ℓ2 regularization on bias terms. Upon initial convergence, the second stage applies hard thresholding: weights with absolute values below τ = 10 −6 are set to zero and frozen. The remaining non-zero parameters are then fine-tuned for up to 600 additional epochs, promoting sparse, more interpretable linear equations. 

Sym-M-CBE. Training follows a three-stage pipeline. In the first stage, the complete model is trained with black-box neural network predictors (MLPs) using the shared training configuration. Upon convergence, the symbolic regression phase begins: predictions are collected from the trained model on the entire training set. PySR then discovers symbolic equations for each placeholder blackbox independently, fitting equations using the subset of data points selected by the selector for that specific placeholder blackbox. The symbolic regression search uses the following configuration: 40 populations of size 60 evolve for 100 iterations with 380 cycles per iteration. For classification tasks, operators are restricted to {+, −, ×} 

with maximum equation complexity 5 × k (where k is the number of concepts). For regression tasks, the operator set is expanded to include {sin , cos , exp , log , tan , tanh , x 2, x 3, √x, x −1} with maximum complexity 40. Discovered equations are substituted into the model as symbolic predictor modules with trainable numeric parameters (exponents remain fixed). In the third stage, the entire model is fine-tuned with the symbolic predictor for up to 600 epochs. To account for the reduced parameter count, the learning rate is increased by a factor of 5 relative to the initial training phase. Only the numeric parameters in the symbolic equations and the upstream networks (concept encoder and selector) are trainable; the structural form of the equations remains fixed. 

F. Implementation details 

F.1. Baselines 

This section provides implementation details for all methods. We implemented all concept-based baselines using the PyC library (Barbiero et al., 2025). To ensure computational efficiency, all models operate on pre-computed embeddings rather than raw inputs. We employ pre-trained backbones: facebook/dinov2-base for high-resolution images, ResNet18 

for low-resolution images, and google/flan-t5-large for textual data. Unless otherwise specified, all networks use LeakyReLU activations with a default hidden dimensionality of 64 (adjusted per dataset). Complete hyperparameter configurations are provided in the supplementary materials. The Blackbox baseline is a single hidden-layer MLP mapping embeddings directly to task predictions. CBM (Koh et al., 2020) follows the standard architecture with a linear task predictor. CEM (Espinosa Zarlenga et al., 2022a) uses concept embeddings of dimensionality 16 with an MLP as task predictor. DCR (Barbiero et al., 2023), LICEM (De Santis et al., 2025), and CMR (Debot et al., 2024) all use concept embeddings of dimensionality 16 . Both CEM and LICEM are adapted to continuous concepts following Ismail et al. (2024). Specifically, in order to preserve responsiveness to interventions, the concept embedding ˆc of each concept is multiplied by the respective concept prediction. 14 Mixture of Concept Bottleneck Experts 

F.2. Proposed methods 

We implement three variants of our M-CBEs framework, each employing different symbolic reasoning strategies over learned concept representations. All variants share a common architecture consisting of: (i) a concept encoder, (ii) a selector network that produces a probability distribution over M expressions (experts), and (iii) a task predictor that executes the selected expression. The models differ primarily in how the expressions are obtained and parameterized. 

Prior-M-CBE. Symbolic equations are provided as SymPy expressions. Each memory slot contains a fixed equation fm

whose structure and parameters remain frozen. 

MLP-M-CBE In MLP-M-CBE each expert is an MLP with 1 hidden layer with hidden size set as specified in Section F.1. We apply an L1 regularization to the parameters of each MLP. This loss term is multiplied by 1e − 5 and added to the loss. 

Kan-M-CBE. In Kan-M-CBE, each expert in the mixture is implemented using a KAN. The architecture is specified by a width vector w, which varies based on the task. We use a deeper architecture: w = [ nc, n c + 1 , n c + 1 , 1] , where nc is the number of concepts. Each edge (i, j ) between layers is parameterized by a univariate B-spline function ϕi,j : R → R

with 5 grid points and cubic basis functions ( k = 3 ). KANs inherently learn smooth, interpretable functions and support automatic symbolic conversion. During training, we apply KAN-specific regularization to encourage sparsity. We set the hyperparameter related to this sparsity to λsparsity = 0 .001 . After the first training, each KAN expert is pruned and each spline is substitute with the symbol in {+, −, ×, sin , cos , exp , log , tan , tanh , x 2, x 3, √x, x −1, x −2} the best fit the spline 

Lin-M-CBE. Each memory slot stores a weight matrix Wm and bias vector bm. We apply ℓ1 regularization on weights with coefficient λ1 = 10 −5 and hard thresholding with τ = 10 −6 after initial training. 

Sym-M-CBE. We use PySR (Cranmer, 2023) with 40 populations of size 60, 100 iterations, and 380 cycles per iteration. For classification: operators {+, −, ×} , maximum complexity 5 × k (where k is the number of concepts). For regression: additional operators {sin , cos , exp , log , tan , tanh , x 2, x 3, √x, x −1, x −2}, maximum complexity 40. Discovered equations are converted to SymPy expressions with trainable parameters. 

G. Complexity metrics 

To evaluate the complexity of the symbolic expressions discovered, we employ several metrics that quantify different structural properties of the expression trees. Using the notation from Section 3.1, we additionally denote by Tn the subtree rooted at node n ∈ N , by NTn its node set, and by d(n) the depth of n (i.e., the path length from the root to n). For multi-mechanism predictors with M expression trees {T (1) , . . . , T (M )}, we report aggregate metrics summed across all trees. Using this notation, the complexity metrics are defined as follows: • Node count : The total number of elements in the expression tree: NodeCount (T ) = |N | = |V | + |O| + |Θ|. This is the default complexity metric employed in the PySR library (Cranmer, 2023). • Tree depth : The maximum nesting level of operations, reflecting the hierarchical complexity of the expression: Depth (T ) = max n∈N d(n).• Expression complexity : The sum of subtree sizes across all nodes, which penalizes deeply nested structures more heavily than shallow ones (Keijzer & Foster, 2007; Smits & Kotanchek, 2005): ExprComplexity (T ) = P 

> n∈N

|NTn |.• Total variables : The number of unique concept variables appearing in the expression: TotalVars (T ) = |V |. Note that the same concept ck may appear multiple times in the tree; this metric counts unique variables, not occurrences. • Total operations : The count of operator nodes in the tree: TotalOps (T ) = |O|. Since operators correspond to internal (non-leaf) nodes, this equals the number of non-terminal nodes in T .• Weighted node count : A variant of node count that assigns different weights to operators based on their complexity. Let w : O → R+ be a weight function, where basic arithmetic operators {+, −, ×, ÷} receive unit weight w(o) = 1 ,while transcendental functions (e.g., sin , cos , exp , log ) receive weight w(o) = 2 . Variables and constants receive unit weight. The weighted node count is WeightedCount (T ) = |V | + |Θ| + P 

> o∈O

w(o). This metric favors expressions composed of simpler primitives. 15 Mixture of Concept Bottleneck Experts 

H. Concept Size ablation 

In this section, we investigate why Boolean functional forms fail to scale with the number of concepts. To systematically study this phenomenon, we evaluate Lin-M-CBE and concept-based baselines (CBM, CMR, DCR, LICEM) on CUB200 and CIFAR10 datasets, both of which feature concept bottlenecks with more than 100 concepts. We systematically vary the bottleneck size by randomly subsampling different subsets of concepts from the original set. For each bottleneck size, we train all models on the resulting modified dataset and evaluate their performance. This process is repeated across multiple bottleneck sizes to observe how model accuracy changes as a function of the number of available concepts. For Lin-M-CBE and CMR, we set the number of experts to 2.As shown in Figure 8, models employing Boolean functional forms (CMR, DCR) exhibit a clear degradation in accuracy as the concept bottleneck size increases. This behavior strengthens our hypothesis that conjunctive Boolean rules become increasingly brittle with larger concept sets: the probability of mispredicting at least one concept grows with dimensionality, causing the entire logical formula to fail. In contrast, models with more flexible functional forms (Lin-M-CBE, CBM, LICEM) demonstrate improved accuracy as the bottleneck size increases. These architectures benefit from the additional task-relevant information provided by larger concept sets, as their continuous aggregation mechanisms are more robust to individual concept prediction errors. 0.2 0.4 0.6 0.8 1.0 

Concept Percentage 

0.0 

0.2 

0.4 

0.6 

0.8 

> Accuracy

CUB200 

0.2 0.4 0.6 0.8 1.0 

Concept Percentage 

0.0 

0.2 

0.4 

0.6 

0.8 

> Accuracy

CIFAR10         

> CEM LICEM DCR CBM CMR Lin-M-CBE
> Figure 8. Effect of concept bottleneck size on model accuracy for CUB200 (left) and CIFAR10 (right). The x-axis shows the percentage of selected concepts, and the y-axis reports accuracy. For task predictors with Boolean functional forms (CMR, DCR), accuracy decreases as more concepts are included, while models with flexible functional forms (Lin-M-CBE, Sym-M-CBE) improve as bottleneck size increases. Error bars indicate 95% confidence intervals over five random seeds.

I. Datasets details 

I.1. Synthetic Datasets MNIST-Arithm The MNIST dataset (LeCun et al., 2010) is a widely used benchmark consisting of grayscale images of handwritten digits. It contains 60,000 training examples and 10,000 test examples, sampled from the same distribution, with each image annotated by its corresponding digit label. MNIST-Arithm is derived from MNIST through the following procedure. First, we specify the total number of images to generate (in our experiments, 100000). Each generated image is created by randomly sampling two images from MNIST, extracting their digit labels, and combining them into a new image separated by an arithmetic operation (addition, subtraction, multiplication, or division). Correspondingly, each image is annotated with: (i) two concept variables representing the digits contained in the image, and (ii) a task variable corresponding to the result of the arithmetic operation. Then, the resulting dataset is split into training (70%), validation (10%), and test (20%) sets. Finally, each image is preprocessed using a pre-trained facebook/dinov2-base model with default Hugging Face weights. 

dSprites-Exp The dSprites dataset (Matthey et al., 2017) is a widely used dataset containing 737280 images of 2D white shapes on a black background generated from 6 ground truth independent latent factors: color, shape, scale, rotation, and x and 16 Mixture of Concept Bottleneck Experts 

y positions of a sprite. dSprites-Exp dataset is derived from dSprites through the following procedure. First, we specify the total number of images to generate (in our experiments, 100,000). Each generated image is sampled from the original dataset and it is labeled with original latent factors, which serve as our concepts, except for color, which is omitted since it is constant (“white”). Next, a target variable is defined for each sample as target = exp 



sin(2 π x position ) + cos(2 π y position )



.Then, the resulting dataset is split into training (70%), validation (10%), and test (20%) sets. Finally, all images are preprocessed using a pre-trained facebook/dinov2-base model with default weights from Hugging Face. 

Pendulum Pendulum is a synthetic dataset originally introduced in Yang et al. (2020). It consists of ∼7k generated images of a swinging pendulum with a moving light source. The positions of the illumination source and the pendulum angle relative to the vertical determine the position and length of the pendulum’s shadow on a horizontal plane. We consider a subset of 100000 images from the original dataset, obtained by sampling 100 pendulum angles ranging from −200 ◦ to 200 ◦

and 1000 light source angles between 60 ◦ and 140 ◦ (where the light source angle is defined as the angle between the line connecting the pendulum’s center of rotation to the center of the light bulb and the pendulum’s vertical line). As concepts, we use the radiant representation of the angles, and as a target, we consider the x-position of the pendulum ball, to avoid overly complex ground-truth mechanisms. The dataset is then split into training ( 70% ), validation ( 10% ), and test ( 20% )sets. All images are finally preprocessed using the pre-trained facebook/dinov2-base model with default weights from Hugging Face. 

I.2. Real-world Datasets AWA2. This dataset is the Animals with Attributes 2 dataset (Xian et al., 2017), consisting of RGB images depicting one of 50 animal species. Each image is annotated with a species label and 85 numeric attributes, which we treat as concept labels, while the species serves as the target in our classification problem. Following prior work (Alaa & Van der Schaar, 2019), we generate train-validation-test splits using a random 60%–20%–20% partition. During training, samples are randomly cropped and flipped. Finally, all images are preprocessed using the pre-trained facebook/dinov2-base model with default weights from Hugging Face. 

AWA2-Incomplete. This dataset is derived from Animals with Attributes 2 (Xian et al., 2017), following the same procedure as AWA2, with the only difference that we consider only a subset of the 85 concepts. Specifically, we retain the following concepts: “black”, “gray”, “stripes”, “hairless”, “flippers”, “paws”, “plains”, “fierce”, “solitary”. 

CUB-200. This dataset is the Caltech-UCSD Birds-200-2011 dataset (He & Peng, 2019). Specifically, each sample consists of an RGB image of a bird annotated with one of 200 species and 312 binary attributes. In our experiments, we adopt the 112 bird attributes selected in (Koh et al., 2020) as binary concept annotations and use bird species as the downstream classification task. All images are preprocessed, and the dataset is then split following the same procedure described in Espinosa Zarlenga et al. (2022a). Finally, all images are encoded using the pre-trained facebook/dinov2-base model with the default Hugging Face weights. 

CUB-200-Incomplete. This dataset is a subset of CUB-200 where we select the following concepts: “has bill shape”, “has head pattern”, “has breast color”, “has bill length”, “has wing shape”, “has tail pattern”, “has bill color”. CIFAR-10. 

The original dataset (Krizhevsky et al., 2009) contains 60,000 RGB images, each belonging to one of 10 object categories. Following prior work of Oikarinen et al. (2023), we annotate each sample with a set of binary concepts. We adopt the original train-test split, reserving 10% of the training set for validation. Finally, all images are encoded using the pre-trained 

google/vit-base-patch32-224-in21k model with the default Hugging Face weights. 

MAWPS. Is a benchmark dataset for evaluating models on math word problem solving (Koncel-Kedziorski et al., 2016). It contains ∼ 3.3K English arithmetic and simple algebra problems, each annotated with a ground-truth equation and solution. 

J. Learned Expressions 

This section presents the symbolic expressions learned by Lin-M-CBE and Sym-M-CBE across experimental datasets. For regression tasks (Table 6), we set the number of experts M equal to the number of underlying mechanisms mapping concepts to task outputs. For classification tasks (Table 7), due to space constraints, we only report results for CUB200-Incomplete and AWA2-Incomplete, as these datasets have a reduced number of concepts, making the expressions more compact and interpretable. These expressions provide concrete examples of how M-CBEs instantiations translate concept predictions into task predic-17 Mixture of Concept Bottleneck Experts 

tions. 

Table 6. Equations learned by the proposed instantiations on the regression datasets. 

Dataset Model Equations 

dSprites-Exp. Lin-M-CBE −2.5430 ∗ value x position − 0.0001 ∗ value y position + 2 .9265 

dSprites-Exp. Prior-M-CBE exp (sin (6 .28000020980835 ∗ value x position ) + cos (6 .28000020980835 ∗ value y position )) 

dSprites-Exp. Sym-M-CBE exp (sin (6 .28318309783936 ∗ value x position ) + cos (6 .28318452835083 ∗ value y position )) 

Pendulum Lin-M-CBE 2.3583 ∗ theta − 0.0042 ∗ phi + 10 .0064 

Pendulum Prior-M-CBE 8.0 ∗ sin (theta ) + 10 .0

Pendulum Sym-M-CBE 8.00006008148193 ∗ sin (theta ) + 9 .9999361038208 

MNIST-Arith. Lin-M-CBE 3.7693 ∗ f irst digit + 3 .7327 ∗ second digit − 1.0437 0.6945 ∗ f irst digit − 0.5778 ∗ second digit + 0 .3109 1.0767 ∗ f irst digit + 1 .0075 ∗ second digit − 0.2216 2.6496 ∗ f irst digit + 2 .6543 ∗ second digit − 1.3471 

MNIST-Arith. Prior-M-CBE f irst digit ∗ second digit f irst digit/second digit f irst digit + second digit f irst digit − 1.0 ∗ second digit 

MNIST-Arith. Sym-M-CBE f irst digit ∗ second digit f irst digit − 0.834531188011169 ∗ second digit f irst digit + second digit 

MAWPS Lin-M-CBE −0.9211 ∗ N 00 + 0 .1138 ∗ N 01 + 0 .1780 ∗ N 02 − 0.1387 

−4.3256 ∗ N 00 + 0 .0659 ∗ N 01 − 0.0764 ∗ N 02 − 0.2263 1.2093 ∗ N 00 − 0.0052 ∗ N 01 + 0 .0454 ∗ N 02 − 0.0351 4.5133 ∗ N 00 − 0.1206 ∗ N 01 − 0.0568 ∗ N 02 + 0 .2450 

MAWPS Prior-M-CBE N 00 ∗ (N 01 − 1.0 ∗ N 02) 

N 00 ∗ (N 01 + N 02) 

N 02 ∗ (N 00 + N 01) 

N 02 ∗ (N 00 − 1.0 ∗ N 01) 

MAWPS Sym-M-CBE N 00 ∗ (N 01 − 0.999999642372131 ∗ N 02) 

N 00 ∗ (N 01 + N 02) 

N 02 ∗ (N 00 + N 01) 

N 02 ∗ (N 00 − 1.00000178813934 ∗ N 01) 

K. Detailed Results 

K.1. Task accuracy and complexity 

In this section, we provide detailed experimental results (Tables 8 to 12) showing the accuracy, MAE, and all complexity metrics for the various methods across different datasets. Additionally, since there are as many Pareto frontiers as there are complexity metrics, each table includes a column representing the number of times a model appeared on the Pareto frontier. 

K.2. Concept accuracy 

In this subsection, we report the concept prediction performance for all methods across the different datasets (Table 13). Specifically, we show the concept accuracy for datasets having binary concepts, and MAE and MSE for datasets having continuous concepts. 18 Mixture of Concept Bottleneck Experts 

Table 7. Learned expressions for classification tasks. For each dataset, we randomly select three classes and report the learned expression mapping concept predictions to that class. 

Dataset Selected class Model Explanation 

AWA2 incomplete Squirrel Lin −8.3725 ∗ black + 4 .4533 ∗ gray − 1.2107 ∗ stripes − 5.6324 ∗ hairless − 1.7003 ∗ f lippers +1.8803 ∗ paws − 3.8328 ∗ plains − 4.6689 ∗ f ierce + 2 .3280 ∗ solitary + 0 .4597 

AWA2 incomplete Squirrel Sym −2.33258175849915 ∗ black − 2.04653811454773 ∗ f ierce − 1.71986174583435 ∗ f lippers +1.87418258190155 ∗ gray − 2.48390746116638 ∗ hairless + 1.33075654506683 ∗ paws −

2.27515959739685 ∗ plains + 1 .38696956634521 ∗ solitary − 1.98490846157074 ∗ stripes +0.515300989151001 

AWA2 incomplete Deer Lin −5.5206 ∗ black − 2.8409 ∗ gray − 1.9280 ∗ stripes − 1.6717 ∗ hairless − 1.6070 ∗ f lippers −

2.7865 ∗ paws + 4 .2462 ∗ plains − 2.6639 ∗ f ierce − 3.7920 ∗ solitary + 4 .0240 

AWA2 incomplete Deer Sym −2.21854496002197 ∗ black 2 − 1.93461465835571 ∗ f ierce − 1.4765248298645 ∗ f lippers −

1.88623571395874 ∗ gray − 1.58335506916046 ∗ hairless − 1.48845815658569 ∗ paws +2.08511114120483 ∗ plains − 1.87786483764648 ∗ solitary − 2.1214337348938 ∗ stripes +2.2183256149292 

AWA2 incomplete Rabbit Lin 3.5934 ∗ black + 0 .4045 ∗ gray − 2.0988 ∗ stripes − 1.6350 ∗ hairless − 1.6533 ∗ f lippers + 6 .4211 ∗

paws + 2 .5394 ∗ plains − 6.1008 ∗ f ierce − 8.1255 ∗ solitary − 3.5315 

AWA2 incomplete Rabbit Sym 1.18550539016724 ∗ black − 1.97509169578552 ∗ f ierce − 1.74324333667755 ∗ f lippers +1.56745767593384 ∗ gray 2 − 0.0243255645036697 ∗ gray − 1.87004804611206 ∗ hairless +2.10452556610107 ∗ paws + 1 .38752210140228 ∗ plains − 2.72675681114197 ∗ solitary −

2.14433598518372 ∗ stripes − 1.12651097774506 

CUB200 incomplete Laysan Albatross Lin −1.6006 ∗ has bill shape curved up or down + 5 .3314 ∗ has bill shape dagger − 1.6190 ∗

has bill shape hooked − 1.8209 ∗ has bill shape needle − 0.7035 ∗ has upperparts color red −

3.4107 ∗ has upperparts color buf f − 1.2424 ∗ has underparts color blue −

1.2530 ∗ has underparts color brown − 1.2431 ∗ has underparts color iridescent −

1.7109 ∗ has underparts color purple + 0.6020 ∗ has underparts color ruf ous −

1.2875 ∗ has underparts color grey + 1.3830 ∗ has underparts color white −

2.5163 ∗ has underparts color red − 1.8923 ∗ has tail shape f an shaped tail −

1.2673 ∗ has tail shape pointed tail − 2.0584 ∗ has upper tail color olive −

1.5512 ∗ has upper tail color green − 2.2977 ∗ has upper tail color pink − 1.1097 ∗

has head pattern eyebrow − 2.8353 ∗ has head pattern eyering + 9.5280 ∗

has head pattern plain − 0.4318 

CUB200 incomplete Laysan Albatross Sym −2.12568759918213 ∗ has bill shape curved up or down + 1.16855323314667 ∗

has bill shape dagger − 1.65060842037201 ∗ has bill shape hooked − 1.96898102760315 ∗

has bill shape needle − 1.50070405006409 ∗ has head pattern eyebrow − 2.59437537193298 ∗

has head pattern eyering + 1 .62642562389374 ∗ has head pattern plain − 1.6626957654953 ∗

has tail shape f an shaped tail − 2.22473764419556 ∗ has tail shape pointed tail −

2.65573024749756 ∗ has underparts color blue − 1.64292740821838 ∗

has underparts color brown + 1.66115808486938 ∗ has underparts color grey ∗

(−1.14893710613251 ∗ has underparts color grey − 0.398883730173111) − 1.63640189170837 ∗

has underparts color purple − 1.66675746440887 ∗ has underparts color red +1.53731632232666 ∗ has underparts color ruf ous + 0.8258376121521 ∗

has underparts color white − 2.24085354804993 ∗ has upper tail color green −

2.45426249504089 ∗ has upper tail color olive − 2.42568469047546 ∗

has upper tail color pink − 3.17689180374146 ∗ has upperparts color buf f −

1.25888073444366 ∗ has upperparts color red + 1 .48992025852203 ∗ (−1.14381647109985 ∗

has underparts color iridescent + (has upper tail color olive − 1.51177990436554) ∗

(has upper tail color olive + 0.373325765132904)) ∗ (−0.848836362361908 ∗

has underparts color blue + has underparts color ruf ous + 0.892222940921783) +1.045170545578 

19 Mixture of Concept Bottleneck Experts 

Table 8. Predictive performance and model complexity for all methods on the AWA2 and AWA2-Incomplete datasets. 

Dataset Model Accuracy Nodes Depth Expr.-Comp. Vars Ops Weighted Pareto AWA2 BlackBox 97 .6±0.1 – – – – – – 0/6

CEM 97 .6±0.1 – – – – – – 0/6

LICEM 97 .6±0.2 – – – – – – 0/6

DCR 26 .0±13 .1 – – – – – – 0/6

CMR (1) 95 .3±1.4 127 .0±95 .6 5.4±3.9 299 .8±224 .8 77 .6±59 .4 49 .4±36 .2 176 .4±131 .8 6/6

CMR (2) 96 .4±0.4 918 .4±1271 .6 22 .8±30 .9 2172 .8±2994 .5 567 .2±799 .6 351 .2±472 .0 1269 .6±1743 .6 0/6

CMR (3) 96 .6±0.4 1400 .6±986 .3 35 .4±26 .0 3324 .2±2332 .5 854 .0±609 .1 546 .6±377 .4 1947 .2±1363 .5 6/6

CMR (4) 96 .3±0.5 928 .6±1416 .4 24 .0±34 .1 2204 .6±3363 .3 565 .2±863 .1 363 .4±553 .3 1292 .0±1969 .7 0/6

CMR (5) 96 .5±0.5 889 .6±1095 .7 22 .8±26 .6 2115 .2±2608 .3 538 .4±661 .2 351 .2±434 .6 1240 .8±1530 .3 6/6

MLP-M-CBE (1) 97 .5±0.1 1105100 .0±0.0 300 .0±0.0 6213650 .0±0.0 4250 .0±0.0 374050 .0±0.0 1109350 .0±0.0 0/6

MLP-M-CBE (2) 97 .7±0.1 1379164 .8±12105 .8 374 .4±3.3 7754635 .2±68067 .1 5304 .0±46 .6 466814 .4±4097 .5 1384468 .8±12152 .3 6/6

MLP-M-CBE (3) 97 .6±0.1 1410084 .8±65189 .1 382 .8±17 .7 7928488 .2±366538 .9 5423 .0±250 .7 477280 .2±22065 .0 1415507 .8±65439 .8 0/6

MLP-M-CBE (4) 97 .6±0.1 1445455 .2±89237 .5 392 .4±24 .2 8127365 .8±501756 .3 5559 .0±343 .2 489252 .2±30204 .7 1451014 .2±89580 .7 0/6

MLP-M-CBE (5) 97 .6±0.1 1423356 .8±40166 .0 386 .4±10 .9 8003113 .2±225842 .0 5474 .0±154 .4 481772 .4±13595 .1 1428830 .8±40320 .4 0/6

Lin-M-CBE (1) 97 .6±0.1 12847 .0±2.1 150 .0±0.0 34142 .0±5.7 4249 .0±0.7 4299 .0±0.7 12847 .0±2.1 6/6

Lin-M-CBE (2) 97 .6±0.1 14081 .8±216 .3 164 .4±2.5 37423 .6±574 .8 4657 .4±71 .5 4712 .2±72 .4 14081 .8±216 .3 0/6

Lin-M-CBE (3) 97 .6±0.1 14234 .2±825 .8 166 .2±9.6 37828 .6±2194 .8 4707 .8±273 .1 4763 .2±276 .3 14234 .2±825 .8 0/6

Lin-M-CBE (4) 97 .6±0.1 13870 .8±316 .6 162 .0±3.7 36862 .8±841 .4 4587 .6±104 .7 4641 .6±105 .9 13870 .8±316 .6 6/6

Lin-M-CBE (5) 97 .5±0.1 14437 .4±709 .4 168 .6±8.3 38368 .6±1885 .2 4775 .0±234 .6 4831 .2±237 .4 14437 .4±709 .4 0/6

Sym-M-CBE (1) 97 .0±0.1 15838 .7±226 .2 671 .0±27 .0 107720 .0±5885 .9 2112 .0±32 .7 5636 .7±56 .5 15849 .7±230 .8 0/6

Sym-M-CBE (2) 97 .0±0.0 9419 .7±853 .3 556 .7±54 .4 64022 .3±8817 .1 1473 .7±105 .6 3325 .0±327 .6 9428 .0±854 .0 0/6

Sym-M-CBE (3) 97 .1±0.1 9551 .0±528 .1 604 .3±29 .6 66481 .0±1260 .7 1464 .7±62 .7 3380 .3±177 .2 9557 .3±531 .1 0/6

Sym-M-CBE (4) 97 .1±0.1 8264 .7±1074 .0 538 .7±63 .1 53462 .0±10607 .0 1309 .7±147 .5 2908 .3±372 .6 8272 .0±1077 .0 4/6

Sym-M-CBE (5) 97 .2±0.1 9899 .3±600 .6 633 .7±26 .3 69109 .0±5813 .6 1547 .0±53 .8 3515 .3±230 .4 9907 .7±597 .7 4/6

AWA2-Incomplete BlackBox 97 .6±0.1 – – – – – – 0/6

CEM 97 .4±0.1 – – – – – – 0/6

LICEM 97 .5±0.1 – – – – – – 0/6

DCR 96 .6±0.0 – – – – – – 0/6

CMR (1) 74 .1±0.4 615 .4±12 .2 121 .2±2.7 1401 .8±26 .9 363 .6±8.0 251 .8±4.4 867 .2±16 .5 0/6

CMR (2) 96 .5±0.1 591 .2±118 .0 119 .2±26 .3 1336 .2±258 .7 357 .8±78 .5 233 .4±40 .8 824 .6±158 .1 0/6

CMR (3) 96 .6±0.1 535 .4±135 .8 107 .4±29 .0 1212 .8±301 .3 321 .8±87 .3 213 .6±49 .2 749 .0±184 .7 0/6

CMR (4) 96 .6±0.1 442 .8±98 .1 87 .0±20 .9 1011 .0±216 .9 259 .4±63 .7 183 .4±34 .5 626 .2±132 .6 5/6

CMR (5) 96 .6±0.1 451 .0±101 .2 90 .0±24 .1 1023 .8±216 .5 269 .2±71 .1 181 .8±30 .3 632 .8±131 .4 6/6

MLP-M-CBE (1) 74 .5±0.1 10788 .0±242 .6 223 .2±5.0 57027 .6±1282 .6 334 .8±7.5 4054 .8±91 .2 11122 .8±250 .2 0/6

MLP-M-CBE (2) 96 .7±0.3 18908 .0±378 .1 391 .2±7.8 99951 .6±1998 .8 586 .8±11 .7 7106 .8±142 .1 19494 .8±389 .8 0/6

MLP-M-CBE (3) 97 .0±0.2 18328 .0±1270 .7 379 .2±26 .3 96885 .6±6717 .3 568 .8±39 .4 6888 .8±477 .6 18896 .8±1310 .2 1/6

MLP-M-CBE (4) 97 .1±0.2 19488 .0±1202 .7 403 .2±24 .9 103017 .6±6357 .8 604 .8±37 .3 7324 .8±452 .1 20092 .8±1240 .0 0/6

MLP-M-CBE (5) 97 .1±0.1 19082 .0±1053 .6 394 .8±21 .8 100871 .4±5569 .7 592 .2±32 .7 7172 .2±396 .0 19674 .2±1086 .3 0/6

Lin-M-CBE (1) 74 .2±0.3 1119 .4±33 .1 115 .8±3.4 2895 .0±85 .5 347 .4±10 .3 386 .0±11 .4 1119 .4±33 .1 0/6

Lin-M-CBE (2) 97 .3±0.1 1925 .6±60 .1 199 .2±6.2 4980 .0±155 .5 597 .6±18 .7 664 .0±20 .7 1925 .6±60 .1 6/6

Lin-M-CBE (3) 97 .3±0.1 1983 .6±105 .8 205 .2±10 .9 5130 .0±273 .5 615 .6±32 .8 684 .0±36 .5 1983 .6±105 .8 6/6

Lin-M-CBE (4) 97 .3±0.1 1849 .0±84 .0 191 .4±8.6 4781 .8±217 .3 573 .8±26 .1 637 .6±29 .0 1849 .0±84 .0 6/6

Lin-M-CBE (5) 97 .3±0.1 1977 .8±75 .1 204 .6±7.8 5115 .0±194 .1 613 .8±23 .3 682 .0±25 .9 1977 .8±75 .1 0/6

Sym-M-CBE (1) 74 .4±0.1 1106 .3±24 .9 121 .3±4.9 2908 .0±75 .4 333 .0±9.0 383 .7±9.0 1110 .7±24 .8 0/6

Sym-M-CBE (2) 96 .5±0.9 1097 .3±54 .2 226 .7±6.4 3290 .3±170 .3 309 .7±17 .6 392 .3±21 .5 1101 .7±51 .8 0/6

Sym-M-CBE (3) 96 .8±0.3 1257 .3±132 .3 260 .7±22 .4 3823 .7±351 .4 363 .7±45 .4 443 .0±43 .6 1262 .3±134 .3 5/6

Sym-M-CBE (4) 96 .9±0.1 1522 .0±94 .7 305 .3±15 .9 4724 .3±306 .0 419 .7±28 .5 547 .3±37 .2 1527 .7±96 .5 0/6

Sym-M-CBE (5) 96 .9±0.1 1469 .7±170 .4 291 .0±18 .2 4390 .7±430 .6 411 .0±52 .8 518 .3±53 .4 1477 .7±171 .4 5/6

20 Mixture of Concept Bottleneck Experts 

Table 9. Predictive performance and model complexity for all methods on the CUB200 and CUB200-Incomplete datasets. 

Dataset Model Accuracy Nodes Depth Expr.-Comp. Vars Ops Weighted Pareto CUB200 BlackBox 81 .7±0.3 – – – – – – 0/6

CEM 81 .8±0.4 – – – – – – 0/6

LICEM 70 .4±2.9 – – – – – – 0/6

DCR 19 .8±2.8 – – – – – – 0/6

CMR (1) 0.5±0.3 556 .6±449 .2 16 .2±12 .8 1300 .8±1045 .3 358 .2±293 .8 198 .4±155 .7 755 .0±604 .7 0/6

CMR (2) 0.5±0.3 1769 .4±753 .4 51 .0±21 .3 4131 .2±1755 .8 1143 .0±490 .4 626 .4±263 .4 2395 .8±1016 .6 0/6

CMR (3) 0.5±0.5 1859 .4±1131 .8 53 .4±32 .4 4338 .6±2635 .0 1204 .0±738 .7 655 .4±393 .2 2514 .8±1524 .9 1/6

CMR (4) 0.6±0.4 1854 .6±708 .0 55 .2±20 .4 4326 .4±1650 .8 1200 .6±459 .9 654 .0±248 .7 2508 .6±956 .4 6/6

CMR (5) 0.4±0.3 2874 .0±1060 .7 84 .0±30 .5 6706 .4±2465 .8 1859 .6±696 .1 1014 .4±364 .9 3888 .4±1425 .5 0/6

MLP-M-CBE (1) 70 .4±0.4 7638680 .0±268 .3 1200 .0±0.0 43030320 .0±1520 .5 22400 .0±0.0 2576160 .0±89 .4 7661080 .0±268 .3 1/6

MLP-M-CBE (2) 75 .5±0.6 9937863 .8±155989 .7 1561 .2±24 .5 55982117 .2±878718 .2 29142 .4±457 .7 3351564 .0±52608 .5 9967005 .8±156447 .6 6/6

MLP-M-CBE (3) 75 .4±0.6 9999189 .2±252485 .2 1570 .8±39 .7 56327579 .0±1422303 .1 29321 .6±740 .4 3372245 .8±85151 .1 10028510 .8±253225 .5 0/6

MLP-M-CBE (4) 75 .2±0.5 9876860 .6±203143 .9 1551 .6±31 .9 55638474 .4±1144353 .8 28963 .2±595 .8 3330990 .4±68510 .6 9905823 .6±203739 .6 6/6

MLP-M-CBE (5) 74 .7±0.5 9838715 .0±163454 .9 1545 .6±25 .7 55423591 .4±920778 .8 28851 .2±479 .1 3318125 .8±55125 .2 9867566 .2±163934 .0 6/6

Lin-M-CBE (1) 68 .6±1.1 67458 .2±183 .2 598 .8±1.6 179422 .8±487 .3 22353 .0±60 .7 22552 .6±61 .3 67458 .2±183 .2 2/6

Lin-M-CBE (2) 70 .0±0.8 81508 .2±2394 .1 723 .6±21 .3 216792 .4±6367 .8 27008 .6±793 .3 27249 .8±800 .4 81508 .2±2394 .1 5/6

Lin-M-CBE (3) 70 .0±0.8 84550 .8±3304 .1 750 .6±29 .3 224885 .0±8788 .2 28016 .8±1094 .9 28267 .0±1104 .6 84550 .8±3304 .1 0/6

Lin-M-CBE (4) 70 .4±0.6 88460 .2±4598 .3 785 .4±40 .9 235283 .0±12230 .2 29312 .2±1523 .7 29574 .0±1537 .3 88460 .2±4598 .3 5/6

Lin-M-CBE (5) 70 .5±0.9 89130 .0±2078 .7 791 .4±18 .4 237064 .6±5529 .1 29534 .2±688 .9 29798 .0±695 .1 89130 .0±2078 .7 5/6

Sym-M-CBE (1) 63 .9±0.4 65408 .0±459 .0 2190 .0±44 .0 373305 .0±4470 .0 10956 .0±330 .0 22598 .0±570 .0 65431 .0±460 .0 0/6

Sym-M-CBE (2) 69 .0±0.7 60210 .0±420 .0 2667 .0±53 .0 399542 .0±4000 .0 9656 .0±290 .0 20979 .0±620 .0 60234 .0±420 .0 3/6

Sym-M-CBE (3) 69 .7±0.7 60680 .0±430 .0 2809 .0±56 .0 425458 .0±4300 .0 9622 .0±290 .0 21149 .0±630 .0 60713 .0±430 .0 4/6

Sym-M-CBE (4) 69 .4±0.7 73574 .0±520 .0 2987 .0±60 .0 478115 .0±4800 .0 11806 .0±590 .0 25596 .0±640 .0 73614 .0±520 .0 0/6

Sym-M-CBE (5) 69 .7±0.7 71656 .0±510 .0 3185 .0±64 .0 481466 .0±4850 .0 11364 .0±570 .0 24949 .0±625 .0 71689 .0±510 .0 0/6

CUB200-Incomplete BlackBox 81 .3±1.2 – – – – – – 0/6

CEM 77 .7±1.8 – – – – – – 0/6

LICEM 73 .0±2.8 – – – – – – 0/6

DCR 61 .5±0.9 – – – – – – 0/6

CMR (1) 53 .5±0.8 6269 .2±236 .1 468 .0±18 .1 15063 .6±563 .3 3432 .0±132 .9 2837 .2±103 .3 9106 .4±339 .3 0/6

CMR (2) 61 .0±0.8 5552 .0±359 .9 496 .8±29 .1 13039 .8±882 .4 3284 .2±201 .7 2267 .8±187 .7 7819 .8±537 .4 6/6

CMR (3) 61 .0±0.8 5128 .4±970 .8 456 .8±89 .9 12058 .8±2269 .1 3021 .2±591 .3 2107 .2±391 .3 7235 .6±1357 .0 6/6

CMR (4) 60 .8±0.6 4301 .4±726 .6 375 .6±59 .0 10155 .6±1743 .6 2498 .2±410 .0 1803 .2±333 .1 6104 .6±1053 .4 6/6

CMR (5) 60 .6±0.9 3946 .0±579 .3 345 .8±72 .7 9317 .8±1318 .0 2289 .4±403 .9 1656 .6±227 .5 5602 .6±782 .0 6/6

MLP-M-CBE (1) 56 .1±0.2 295908 .8±2319 .8 1135 .2±8.9 1628066 .0±12763 .3 4162 .4±32 .6 104249 .2±817 .3 300071 .2±2352 .4 0/6

MLP-M-CBE (2) 75 .4±0.2 466370 .6±8429 .1 1789 .2±32 .4 2565932 .8±46376 .0 6560 .4±118 .9 164303 .2±2969 .6 472930 .8±8547 .7 6/6

MLP-M-CBE (3) 76 .2±0.9 507674 .4±5810 .0 1947 .6±22 .3 2793183 .0±31966 .2 7141 .2±81 .7 178854 .6±2046 .9 514815 .6±5891 .7 6/6

MLP-M-CBE (4) 76 .5±0.8 510162 .6±6735 .4 1957 .2±25 .9 2806872 .8±37057 .4 7176 .4±95 .1 179731 .2±2372 .9 517338 .8±6830 .1 6/6

MLP-M-CBE (5) 77 .2±0.7 519248 .0±14629 .9 1992 .0±56 .1 2856860 .0±80492 .4 7304 .0±205 .8 182932 .0±5154 .1 526552 .0±14835 .7 6/6

Lin-M-CBE (1) 54 .6±0.9 12539 .2±156 .5 553 .2±6.9 33007 .6±412 .1 4056 .8±50 .6 4241 .2±52 .9 12539 .2±156 .5 0/6

Lin-M-CBE (2) 68 .4±1.5 18413 .2±373 .8 812 .4±16 .5 48470 .0±983 .9 5957 .2±120 .9 6228 .0±126 .4 18413 .2±373 .8 2/6

Lin-M-CBE (3) 68 .5±1.0 18658 .0±567 .1 823 .2±25 .0 49114 .4±1493 .0 6036 .4±183 .5 6310 .8±191 .8 18658 .0±567 .1 2/6

Lin-M-CBE (4) 68 .6±0.5 19061 .2±314 .3 841 .2±13 .8 50175 .6±827 .4 6166 .8±101 .7 6447 .2±106 .3 19061 .2±314 .3 2/6

Lin-M-CBE (5) 69 .1±0.6 19446 .8±675 .6 858 .0±29 .8 51190 .8±1778 .3 6291 .6±218 .6 6577 .6±228 .5 19446 .8±675 .6 2/6

Sym-M-CBE (1) 55 .6±1.2 13182 .0±205 .0 683 .0±32 .0 35933 .0±850 .0 4066 .0±50 .0 4476 .0±120 .0 13188 .0±205 .0 0/6

Sym-M-CBE (2) 60 .4±1.5 16102 .0±310 .0 1359 .0±65 .0 58572 .0±1500 .0 4091 .0±70 .0 5653 .0±150 .0 16126 .0±310 .0 0/6

Sym-M-CBE (3) 64 .7±3.9 15807 .8±1117 .5 1423 .0±107 .5 57834 .5±6948 .5 4039 .5±182 .1 5557 .0±476 .5 15826 .5±1118 .5 2/6

Sym-M-CBE (4) 68 .9±6.2 15513 .5±2124 .9 1487 .0±149 .9 57097 .0±12384 .3 3988 .0±294 .2 5461 .0±803 .3 15527 .0±2127 .0 3/6

Sym-M-CBE (5) 70 .9±1.8 15629 .0±420 .0 1514 .0±80 .0 61364 .0±2100 .0 3884 .0±90 .0 5511 .0±200 .0 15636 .0±420 .0 6/6

Table 10. Predictive performance and model complexity for all methods on the CIFAR10 dataset. 

Dataset Model Accuracy Nodes Depth Expr.-Comp. Vars Ops Weighted Pareto CIFAR10 BlackBox 87 .6±0.4 – – – – – – 0/6

CEM 87 .7±0.1 – – – – – – 0/6

LICEM 87 .3±0.2 – – – – – – 0/6

DCR 17 .2±16 .0 – – – – – – 0/6

CMR (1) 10 .3±5.4 322 .0±206 .6 7.2±4.5 749 .2±478 .2 212 .0±138 .7 110 .0±68 .1 432 .0±274 .6 6/6

CMR (2) 9.8±3.4 399 .4±168 .2 9.0±3.7 926 .8±389 .6 265 .4±112 .8 134 .0±55 .6 533 .4±223 .8 0/6

CMR (3) 8.9±4.1 611 .4±194 .6 13 .8±4.0 1418 .2±458 .5 406 .8±123 .1 204 .6±72 .4 816 .0±266 .6 0/6

CMR (4) 10 .3±2.6 463 .0±249 .4 10 .2±5.4 1080 .2±580 .7 302 .0±164 .0 161 .0±85 .8 624 .0±335 .0 5/6

CMR (5) 13 .6±5.0 533 .6±372 .6 12 .0±7.9 1241 .6±869 .0 351 .2±243 .6 182 .4±129 .1 716 .0±501 .7 5/6

MLP-M-CBE (1) 81 .8±0.5 620634 .0±13 .4 60 .0±0.0 3500636 .0±76 .0 1430 .0±0.0 208788 .0±4.5 622064 .0±13 .4 0/6

MLP-M-CBE (2) 85 .8±0.2 1141966 .2±33990 .6 110 .4±3.3 6441168 .2±191721 .0 2631 .2±78 .3 384169 .8±11434 .8 1144597 .4±34068 .9 6/6

MLP-M-CBE (3) 84 .8±1.9 1117132 .8±294380 .7 108 .0±28 .5 6301097 .2±1660430 .3 2574 .0±678 .3 375815 .6±99032 .9 1119706 .8±295059 .0 0/6

MLP-M-CBE (4) 85 .5±0.4 1390197 .6±120968 .5 134 .4±11 .7 7841296 .8±682312 .5 3203 .2±278 .8 467677 .6±40695 .1 1393400 .8±121247 .2 0/6

MLP-M-CBE (5) 84 .3±1.2 1166793 .6±278935 .6 112 .8±27 .0 6581205 .2±1573313 .7 2688 .4±642 .7 392522 .0±93837 .0 1169482 .0±279578 .3 0/6

Lin-M-CBE (1) 81 .5±0.3 4308 .2±2.7 30 .0±0.0 11465 .2±7.2 1429 .4±0.9 1439 .4±0.9 4308 .2±2.7 2/6

Lin-M-CBE (2) 85 .5±0.3 8445 .8±234 .4 58 .8±1.6 22476 .4±623 .9 2802 .2±77 .8 2821 .8±78 .3 8445 .8±234 .4 5/6

Lin-M-CBE (3) 85 .7±0.2 10513 .4±784 .6 73 .2±5.4 27978 .8±2088 .1 3488 .2±260 .3 3512 .6±262 .1 10513 .4±784 .6 5/6

Lin-M-CBE (4) 85 .6±0.5 10941 .4±1082 .1 76 .2±7.5 29117 .8±2879 .8 3630 .2±359 .0 3655 .6±361 .5 10941 .4±1082 .1 0/6

Lin-M-CBE (5) 85 .6±0.5 11290 .4±933 .9 78 .6±6.5 30046 .6±2485 .3 3746 .0±309 .9 3772 .2±312 .0 11290 .4±933 .9 0/6

Sym-M-CBE (1) 78 .6±0.3 1058 .0±147 .2 56 .3±6.1 3639 .3±837 .8 299 .3±35 .7 352 .7±51 .2 1058 .3±147 .2 5/6

Sym-M-CBE (2) 84 .5±1.1 3733 .7±810 .1 149 .0±24 .3 18092 .7±5546 .4 737 .3±131 .5 1279 .3±269 .3 3736 .3±812 .1 5/6

Sym-M-CBE (3) 84 .9±0.4 5197 .7±882 .0 195 .3±32 .7 29891 .3±4271 .5 895 .3±101 .5 1784 .7±301 .9 5198 .7±882 .0 4/6

Sym-M-CBE (4) 85 .0±0.9 6988 .3±1143 .1 244 .0±28 .6 38697 .7±6142 .7 1184 .3±221 .5 2407 .0±397 .1 6990 .3±1141 .4 0/6

Sym-M-CBE (5) 85 .3±0.2 6540 .0±327 .2 258 .0±34 .1 47484 .7±8442 .5 1057 .3±51 .4 2270 .0±118 .5 6544 .0±325 .9 4/6

21 Mixture of Concept Bottleneck Experts 

Table 11. Predictive performance and model complexity for all methods on the dSprites-Exp. and Pendulum datasets. 

Dataset Model MAE MSE Nodes Depth Expr.-Comp. Vars Ops Weighted Pareto dSprites-Exp. BlackBox 0.933 ±0.033 1.778 ±0.060 – – – – – – 0/6

CEM 0.880 ±0.028 1.972 ±0.137 – – – – – – 0/6

LICEM 0.908 ±0.011 2.014 ±0.062 – – – – – – 0/6

MLP-M-CBE (1) 1.131 ±0.120 3.491 ±2.340 32 .8±12 .0 6.0±0.0 145 .8±55 .9 2.0±0.0 15 .0±5.5 35 .6±13 .1 0/6

MLP-M-CBE (2) 0.993 ±0.092 2.198 ±0.111 60 .4±30 .3 9.6±3.3 270 .0±138 .1 3.2±1.1 27 .6±13 .8 65 .6±33 .0 0/6

MLP-M-CBE (3) 0.999 ±0.103 2.129 ±0.219 92 .8±48 .1 14 .4±5.4 415 .2±218 .5 4.8±1.8 42 .4±21 .9 100 .8±52 .3 0/6

MLP-M-CBE (4) 0.959 ±0.038 2.199 ±0.246 93 .0±67 .0 15 .0±7.7 415 .5±303 .7 5.0±2.6 42 .5±30 .6 101 .0±72 .9 0/6

MLP-M-CBE (5) 1.004 ±0.087 2.214 ±0.255 128 .0±84 .4 21 .0±11 .5 571 .5±381 .7 7.0±3.8 58 .5±38 .5 139 .0±91 .8 0/6

Prior-M-CBE (1) 0.902 ±0.007 2.146 ±0.027 10 .0±0.0 5.0±0.0 37 .0±0.0 2.0±0.0 6.0±0.0 13 .0±0.0 6/6

Lin-M-CBE (1) 1.128 ±0.006 2.307 ±0.028 7.4±1.3 3.0±0.0 17 .4±3.6 1.8±0.4 2.8±0.4 7.4±1.3 6/6

Lin-M-CBE (2) 1.018 ±0.069 2.032 ±0.170 13 .8±4.9 5.4±1.3 32 .6±12 .1 3.4±1.3 5.2±1.8 13 .8±4.9 2/6

Lin-M-CBE (3) 0.967 ±0.027 1.884 ±0.055 24 .0±0.0 9.0±0.0 57 .0±0.0 6.0±0.0 9.0±0.0 24 .0±0.0 0/6

Lin-M-CBE (4) 1.022 ±0.072 2.024 ±0.211 26 .0±12 .0 9.8±4.5 61 .8±28 .5 6.5±3.0 9.8±4.5 26 .0±12 .0 0/6

Lin-M-CBE (5) 0.944 ±0.016 1.810 ±0.021 37 .2±5.5 14 .2±1.5 88 .2±13 .5 9.2±1.5 14 .0±2.0 37 .2±5.5 0/6

Sym-M-CBE (1) 1.014 ±0.016 2.611 ±0.092 16 .4±5.9 7.4±2.1 83 .2±49 .8 2.0±0.0 9.4±3.4 20 .0±7.1 0/6

Sym-M-CBE (2) 1.011 ±0.030 2.615 ±0.137 13 .2±1.8 6.6±0.9 57 .6±13 .2 2.0±0.0 7.6±0.9 16 .2±1.8 0/6

Sym-M-CBE (3) 1.009 ±0.041 2.598 ±0.204 15 .2±3.0 7.6±1.8 72 .6±23 .5 2.0±0.0 9.0±2.0 19 .2±4.4 0/6

Sym-M-CBE (4) 0.991 ±0.029 2.512 ±0.158 12 .5±3.8 5.8±1.0 52 .5±23 .7 2.0±0.0 7.2±1.9 15 .5±3.8 0/6

Sym-M-CBE (5) 0.985 ±0.015 2.432 ±0.070 13 .5±1.9 6.5±0.6 58 .8±12 .1 2.0±0.0 7.8±1.0 16 .5±1.9 0/6

Pendulum BlackBox 0.144 ±0.059 0.036 ±0.026 – – – – – – 0/6

CEM 0.599 ±0.117 0.678 ±0.250 – – – – – – 0/6

LICEM 0.774 ±0.079 1.030 ±0.206 – – – – – – 0/6

MLP-M-CBE (1) 2.445 ±0.561 8.150 ±3.317 37 .2±12 .0 6.0±0.0 166 .2±55 .9 2.0±0.0 17 .0±5.5 40 .4±13 .1 0/6

MLP-M-CBE (2) 1.519 ±0.097 3.392 ±0.275 74 .4±24 .1 12 .0±0.0 332 .4±111 .7 4.0±0.0 34 .0±11 .0 80 .8±26 .3 0/6

MLP-M-CBE (3) 1.185 ±0.132 2.249 ±0.479 111 .6±36 .1 18 .0±0.0 498 .6±167 .6 6.0±0.0 51 .0±16 .4 121 .2±39 .4 0/6

MLP-M-CBE (4) 0.997 ±0.035 1.628 ±0.096 140 .0±50 .8 24 .0±0.0 624 .0±235 .6 8.0±0.0 64 .0±23 .1 152 .0±55 .4 0/6

MLP-M-CBE (5) 0.895 ±0.149 1.336 ±0.436 175 .0±63 .5 30 .0±0.0 780 .0±294 .4 10 .0±0.0 80 .0±28 .9 190 .0±69 .3 0/6

Prior-M-CBE (1) 0.237 ±0.010 0.147 ±0.011 6.0±0.0 4.0±0.0 15 .0±0.0 1.0±0.0 3.0±0.0 7.0±0.0 6/6

Lin-M-CBE (1) 3.040 ±0.002 11 .909 ±0.016 8.0±0.0 3.0±0.0 19 .0±0.0 2.0±0.0 3.0±0.0 8.0±0.0 1/6

Lin-M-CBE (2) 1.735 ±0.084 4.288 ±0.446 16 .0±0.0 6.0±0.0 38 .0±0.0 4.0±0.0 6.0±0.0 16 .0±0.0 0/6

Lin-M-CBE (3) 1.150 ±0.121 2.046 ±0.338 24 .0±0.0 9.0±0.0 57 .0±0.0 6.0±0.0 9.0±0.0 24 .0±0.0 0/6

Lin-M-CBE (4) 0.909 ±0.104 1.339 ±0.272 32 .0±0.0 12 .0±0.0 76 .0±0.0 8.0±0.0 12 .0±0.0 32 .0±0.0 0/6

Lin-M-CBE (5) 0.720 ±0.095 0.897 ±0.260 40 .0±0.0 15 .0±0.0 95 .0±0.0 10 .0±0.0 15 .0±0.0 40 .0±0.0 0/6

Sym-M-CBE (1) 0.331 ±0.134 0.357 ±0.342 13 .2±8.0 6.0±2.5 54 .0±46 .1 1.2±0.4 7.0±4.6 16 .0±10 .5 0/6

Sym-M-CBE (2) 0.261 ±0.092 0.199 ±0.139 6.0±0.0 4.0±0.0 15 .0±0.0 1.0±0.0 3.0±0.0 7.0±0.0 0/6

Sym-M-CBE (3) 0.348 ±0.111 0.384 ±0.242 10 .8±7.6 4.8±1.5 34 .2±33 .4 1.0±0.0 5.8±4.9 13 .2±10 .6 0/6

Sym-M-CBE (4) 0.267 ±0.037 0.178 ±0.040 6.0±0.0 4.0±0.0 15 .0±0.0 1.0±0.0 3.0±0.0 7.0±0.0 0/6

Sym-M-CBE (5) 0.358 ±0.168 0.382 ±0.298 8.5±5.0 5.5±3.0 30 .5±31 .0 1.0±0.0 5.0±4.0 11 .0±8.0 0/6

22 Mixture of Concept Bottleneck Experts 

Table 12. Predictive performance and model complexity for all methods on the MNIST-Arith. and MAWPS datasets. 

Dataset Model MAE MSE Nodes Depth Expr.-Comp. Vars Ops Weighted Pareto MNIST-Arith. BlackBox 2.123 ±0.026 16 .025 ±0.626 – – – – – – 0/6

CEM 1.805 ±0.211 15 .003 ±1.930 – – – – – – 0/6

LICEM 1.546 ±0.128 12 .927 ±1.246 – – – – – – 0/6

MLP-M-CBE (1) 8.650 ±0.214 354 .843 ±301 .231 32 .8±12 .0 6.0±0.0 145 .8±55 .9 2.0±0.0 15 .0±5.5 35 .6±13 .1 0/6

MLP-M-CBE (2) 5.154 ±0.199 104 .757 ±51 .415 74 .4±24 .1 12 .0±0.0 332 .4±111 .7 4.0±0.0 34 .0±11 .0 80 .8±26 .3 0/6

MLP-M-CBE (3) 2.791 ±0.117 29 .396 ±8.131 111 .6±36 .1 18 .0±0.0 498 .6±167 .6 6.0±0.0 51 .0±16 .4 121 .2±39 .4 1/6

MLP-M-CBE (4) 2.885 ±0.183 41 .241 ±23 .051 140 .0±50 .8 24 .0±0.0 624 .0±235 .6 8.0±0.0 64 .0±23 .1 152 .0±55 .4 0/6

MLP-M-CBE (5) 2.497 ±0.171 26 .950 ±6.720 175 .0±63 .5 30 .0±0.0 780 .0±294 .4 10 .0±0.0 80 .0±28 .9 190 .0±69 .3 6/6

Prior-M-CBE (4) 2.667 ±0.130 782 .072 ±777 .048 16 .0±0.0 10 .0±0.0 32 .0±0.0 8.0±0.0 6.0±0.0 17 .0±0.0 6/6

Lin-M-CBE (1) 8.544 ±0.096 168 .821 ±2.143 8.0±0.0 3.0±0.0 19 .0±0.0 2.0±0.0 3.0±0.0 8.0±0.0 2/6

Lin-M-CBE (2) 5.940 ±0.058 69 .892 ±1.141 16 .0±0.0 6.0±0.0 38 .0±0.0 4.0±0.0 6.0±0.0 16 .0±0.0 0/6

Lin-M-CBE (3) 3.499 ±0.074 34 .529 ±1.378 24 .0±0.0 9.0±0.0 57 .0±0.0 6.0±0.0 9.0±0.0 24 .0±0.0 0/6

Lin-M-CBE (4) 2.739 ±0.066 23 .332 ±1.277 32 .0±0.0 12 .0±0.0 76 .0±0.0 8.0±0.0 12 .0±0.0 32 .0±0.0 0/6

Lin-M-CBE (5) 2.524 ±0.050 19 .624 ±0.736 40 .0±0.0 15 .0±0.0 95 .0±0.0 10 .0±0.0 15 .0±0.0 40 .0±0.0 5/6

Sym-M-CBE (1) 8.575 ±0.092 173 .141 ±2.915 3.0±0.0 2.0±0.0 5.0±0.0 2.0±0.0 1.0±0.0 3.0±0.0 5/6

Sym-M-CBE (2) 5.132 ±0.183 53 .198 ±6.903 6.0±0.0 4.0±0.0 10 .0±0.0 3.0±0.0 2.0±0.0 6.0±0.0 6/6

Sym-M-CBE (3) 2.948 ±0.147 24 .491 ±2.150 11 .0±0.0 7.0±0.0 21 .0±0.0 6.0±0.0 4.0±0.0 11 .0±0.0 5/6

Sym-M-CBE (4) 2.847 ±0.108 63 .271 ±78 .301 12 .2±2.5 7.8±1.5 23 .8±5.5 6.5±1.0 4.5±1.0 12 .5±3.0 5/6

Sym-M-CBE (5) 3.218 ±0.576 7304 .442 ±8578 .163 16 .0±0.0 10 .0±0.0 32 .0±0.0 8.0±0.0 6.0±0.0 17 .0±0.0 0/6

MAWPS BlackBox 0.787 ±0.044 1.292 ±0.142 – – – – – – 0/6

CEM 0.756 ±0.017 1.149 ±0.057 – – – – – – 0/6

LICEM 0.768 ±0.031 1.195 ±0.091 – – – – – – 0/6

MLP-M-CBE (1) 4.968 ±0.119 46 .966 ±1.458 60 .8±23 .0 6.0±0.0 288 .6±111 .7 3.0±0.0 26 .2±9.9 65 .0±24 .6 1/6

MLP-M-CBE (2) 3.451 ±0.222 22 .871 ±2.562 138 .4±46 .0 12 .0±0.0 658 .8±223 .5 6.0±0.0 59 .6±19 .7 148 .0±49 .3 1/6

MLP-M-CBE (3) 2.460 ±0.298 12 .382 ±3.186 207 .6±69 .0 18 .0±0.0 988 .2±335 .2 9.0±0.0 89 .4±29 .6 222 .0±73 .9 0/6

MLP-M-CBE (4) 2.164 ±0.250 9.330 ±1.999 260 .0±97 .0 24 .0±0.0 1236 .0±471 .1 12 .0±0.0 112 .0±41 .6 278 .0±103 .9 0/6

MLP-M-CBE (5) 1.925 ±0.352 7.607 ±2.785 325 .0±121 .2 30 .0±0.0 1545 .0±588 .9 15 .0±0.0 140 .0±52 .0 347 .5±129 .9 0/6

Prior-M-CBE (4) 0.996 ±0.010 2.121 ±0.064 24 .0±0.0 14 .0±0.0 60 .0±0.0 12 .0±0.0 10 .0±0.0 24 .0±0.0 6/6

Lin-M-CBE (1) 5.433 ±0.090 58 .836 ±1.768 11 .0±0.0 3.0±0.0 27 .0±0.0 3.0±0.0 4.0±0.0 11 .0±0.0 0/6

Lin-M-CBE (2) 4.154 ±0.040 30 .719 ±0.703 22 .0±0.0 6.0±0.0 54 .0±0.0 6.0±0.0 8.0±0.0 22 .0±0.0 0/6

Lin-M-CBE (3) 2.981 ±0.045 17 .304 ±0.532 33 .0±0.0 9.0±0.0 81 .0±0.0 9.0±0.0 12 .0±0.0 33 .0±0.0 1/6

Lin-M-CBE (4) 2.635 ±0.155 13 .376 ±1.387 44 .0±0.0 12 .0±0.0 108 .0±0.0 12 .0±0.0 16 .0±0.0 44 .0±0.0 0/6

Lin-M-CBE (5) 2.491 ±0.278 12 .036 ±2.217 55 .0±0.0 15 .0±0.0 135 .0±0.0 15 .0±0.0 20 .0±0.0 55 .0±0.0 0/6

Sym-M-CBE (1) 5.123 ±0.202 51 .517 ±3.897 4.4±0.9 3.0±0.0 9.2±2.7 2.2±0.4 2.2±0.4 5.4±0.9 6/6

Sym-M-CBE (2) 3.572 ±0.166 24 .606 ±1.502 9.8±2.2 5.6±0.9 22 .6±7.1 4.8±0.4 3.6±0.9 9.8±2.2 6/6

Sym-M-CBE (3) 2.025 ±0.253 9.277 ±2.933 18 .6±0.9 10 .8±0.4 47 .8±2.7 8.8±0.4 7.8±0.4 18 .6±0.9 6/6

Sym-M-CBE (4) 1.397 ±0.154 4.173 ±0.959 24 .0±0.0 14 .0±0.0 60 .0±0.0 12 .0±0.0 10 .0±0.0 24 .0±0.0 0/6

Sym-M-CBE (5) 1.528 ±0.366 5.107 ±2.640 25 .2±2.5 14 .2±0.5 65 .8±11 .5 12 .0±0.0 10 .5±1.0 25 .2±2.5 0/6

Table 13. Concept accuracy across methods and datasets. 

Model AWA2 AWA2-Incomplete CUB200 CUB200-Incomplete CIFAR10 dSprites-Exp. Pendulum MNIST-Arith. MAWPS (Accuracy) (Accuracy) (Accuracy) (Accuracy) (Accuracy) (MAE) (MAE) (MAE) (MAE) CEM 99 .54 ±0.01 99 .22 ±0.02 95 .70 ±0.38 95 .50 ±0.24 79 .29 ±0.09 0.1235 ±0.0042 0.1568 ±0.0193 0.6370 ±0.0499 0.1953 ±0.0039 

LICEM 99 .59 ±0.01 99 .30 ±0.03 94 .26 ±0.51 95 .31 ±0.55 79 .07 ±0.12 0.1264 ±0.0008 0.1343 ±0.0302 0.5789 ±0.0358 0.2005 ±0.0084 

DCR 99 .52 ±0.06 99 .25 ±0.01 96 .49 ±0.11 95 .91 ±0.09 79 .33 ±0.31 – – – –CMR 99 .54 ±0.00 99 .25 ±0.01 95 .14 ±0.02 95 .80 ±0.02 78 .53 ±0.11 – – – –MLP-M-CBE 99 .54 ±0.00 99 .26 ±0.01 95 .25 ±0.02 95 .97 ±0.03 79 .06 ±0.02 0.1230 ±0.0021 0.0733 ±0.0022 0.9157 ±0.0071 0.2554 ±0.0041 

Prior-M-CBE – – – – – 0.1231 ±0.0013 0.0652 ±0.0007 0.9150 ±0.0116 0.2523 ±0.0028 

Lin-M-CBE 99 .54 ±0.00 99 .26 ±0.01 95 .24 ±0.02 95 .87 ±0.02 79 .09 ±0.03 0.1235 ±0.0020 0.0753 ±0.0033 0.9266 ±0.0083 0.2558 ±0.0045 

Sym-M-CBE 99 .52 ±0.00 99 .18 ±0.02 95 .30 ±0.03 95 .51 ±0.27 78 .84 ±0.03 0.1456 ±0.0034 0.0728 ±0.0057 1.0192 ±0.0342 0.3528 ±0.0238 

23