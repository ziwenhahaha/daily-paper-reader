Title: KANFIS A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning

URL Source: https://arxiv.org/pdf/2602.03034v1

Published Time: Wed, 04 Feb 2026 01:31:24 GMT

Number of Pages: 14

Markdown Content:
# KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning 

Binbin Yong 1 Haoran Pei 1 Jun Shen 2 Haoran Li 3 Qingguo Zhou 1 Zhao Su 1

# Abstract 

Adaptive Neuro-Fuzzy Inference System (ANFIS) was designed to combine the learning capabili-ties of neural network with the reasoning trans-parency of fuzzy logic. However, conventional ANFIS architectures suffer from structural com-plexity, where the product-based inference mech-anism causes an exponential explosion of rules in high-dimensional spaces. We herein propose the Kolmogorov-Arnold Neuro-Fuzzy Inference 

System (KANFIS), a compact neuro-symbolic architecture that unifies fuzzy reasoning with ad-ditive function decomposition. KANFIS employs an additive aggregation mechanism, under which both model parameters and rule complexity scale linearly with input dimensionality rather than ex-ponentially. Furthermore, KANFIS is compati-ble with both Type-1 (T1) and Interval Type-2 (IT2) fuzzy logic systems, enabling explicit mod-eling of uncertainty and ambiguity in fuzzy rep-resentations. By using sparse masking mecha-nisms, KANFIS generates compact and structured rule sets, resulting in an intrinsically interpretable model with clear rule semantics and transparent inference processes. Empirical results demon-strate that KANFIS achieves competitive perfor-mance against representative neural and neuro-fuzzy baselines. 

# 1. Introduction 

Interpretable machine learning has re-emerged as a cen-tral research focus as modern neural networks continue to grow in scale, complexity, and opacity, raising increasing concerns about transparency, reliability, and trustworthi-ness (Somvanshi et al., 2025; Minh et al., 2022). Despite 

> 1

School of Information Science and Engineering, Lanzhou University, China 2School of Computing and Information Tech-nology, University of Wollongong, Australia 3Department of Data Science and Artificial Intelligence, Monash University, Australia. Correspondence to: Zhao Su <szhao2024@lzu.edu.cn >.

Preprint. February 4, 2026. 

Figure 1. Limitations of Existing Interpretable Models. Con-ventional ANFIS and KAN models suffer from several limitations in terms of interpretability and model complexity. 

their impressive empirical performance across domains such as vision, language, and time-series forecasting, conven-tional architectures such as Multilayer Perceptron (MLP) and Transformer often operate as black boxes (Ibrahim & Shafiq, 2023; Hassija et al., 2024), offering limited insights into their internal reasoning processes or decision structures. This lack of transparency becomes particularly critical in high-stake applications (Liao et al., 2024; Khan et al., 2024), including energy forecasting, finance, and autonomous sys-tems, where model reliability, uncertainty awareness, and interpretability are increasingly regarded as top priority re-quirements (Arrieta et al., 2020; Rudin et al., 2022). Consequently, there has been growing interest in developing models that combine strong predictive performance with intrinsic interpretability. One representative direction is the Neuro-Fuzzy System (NFS), which bridges statistical learn-ing with symbolic reasoning (Minh et al., 2022; Hassija et al., 2024). The Adaptive Neuro-Fuzzy Inference System (ANFIS) represents a representative NFS architecture that combines fuzzy rule-based reasoning with gradient-based parameter learning, enabling models to express structured linguistic rules while adapting to data. However, these tra-ditional frameworks typically rely on fixed linear conse-quent mappings and static rule structures, which severely limit their expressivity and scalability, particularly in high-dimensional or highly nonlinear settings. Subsequent de-velopments have sought to mitigate these limitations by introducing self-organizing rule structures that evolve from data streams (Meng et al., 2025), Interval Type-2 (IT2) fuzzy sets for enhanced uncertainty modeling (Yao et al., 2025), 1

> arXiv:2602.03034v1 [cs.AI] 3 Feb 2026 KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning

and hybrid neuro-fuzzy architectures designed to capture hierarchical or complex features (Shihabudheen & Pillai, 2018). Despite these advancements, a fundamental trade-off persists: most neuro-fuzzy systems struggle to simultane-ously achieve strong nonlinear approximation, compact and interpretable rule structures, and robustness under uncer-tainty, particularly as model complexity increases. In parallel, recent work on the Kolmogorov-Arnold Net-work (KAN) (Somvanshi et al., 2025) demonstrates the expressive power of functional composition as a replace-ment for conventional linear weight matrices. By learn-ing adaptive univariate basis functions along each path of a computational graph, KAN significantly enhances rep-resentational capacity while maintaining a structured de-composition aligned with the classical Kolmogorov-Arnold superposition theorem. Although KAN does not provide ex-plicitly interpretable rules, these developments still illustrate the potential advantages of replacing fixed linear transfor-mations with learnable nonlinear operators, particularly in systems that inherently rely on compositional mappings, such as fuzzy inference models. Recent systematic reviews on fuzzy systems and interpretable machine learning high-light that evolving and hybrid fuzzy architectures have been increasingly studied for their ability to balance approxi-mation power and rule interpretability, yet they still face challenges in scalability and structural transparency (Gu et al., 2023; Ouifak & Idri, 2025). As illustrated in Fig-ure 1, existing interpretable models exhibit complementary limitations: ANFIS suffers from rigid structures and rule explosion, whereas KAN encounters semantic opacity and scalability bottlenecks in high-dimensional regimes. Motivated by the aforementioned problems, we propose the 

Kolmogorov-Arnold Neuro-Fuzzy Inference System (KAN-FIS). This architecture integrates the symbolic reasoning of ANFIS with the additivity theorem of Kolmogorov-Arnold networks. Specifically, we design the network edges to function as learnable membership functions, which are then aggregated using the additivity principle. This configuration enhances expressive power without sacrificing interpretabil-ity, as it naturally accommodates Type-1 (T1) and IT2 rea-soning to explicitly encode uncertainty, while the functional operators provide transparent pathways describing how each input feature contributes to the prediction. Furthermore, to guarantee model compactness and interpretability, we further introduce specific regularization terms. One term induces sparsity to control the number of active features in each rule, while the other enforces diversity to main-tain distinctiveness between rules. Consequently, KANFIS achieves competitive prediction accuracy while generating concise, human-readable rule-based explanations. Our contributions are summarized as follows: • We pro-pose KANFIS, the first framework to realize the symbolic reasoning of ANFIS within a Kolmogorov-Arnold-style additive architecture. By leveraging the superposition of learnable functions, KANFIS achieves exceptional nonlin-ear approximation capabilities. • We achieve a compact architectural design by leveraging the Kolmogorov-Arnold additivity principle together with a dedicated regulariza-tion strategy, enabling efficient function approximation with fewer rules while suppressing redundant features and rule proliferation. • Benefiting from its unique design, KANFIS maintains compact and well-structured rule representations in high-dimensional settings, mitigating rule explosion and interpretability degradation observed in conventional AN-FIS. 

# 2. Related Work 

Our work focuses on intrinsically interpretable learning models with transparent reasoning processes. A broad range of related studies has explored interpretable architectures (Tursunalieva et al., 2024), neuro-fuzzy inference systems, and function-based model representations (Somvanshi et al., 2025), forming the foundation of this line of research. 

2.1. Intrinsic Interpretability and Additive Models 

The notion of interpretability is inherently ambiguous and has been shown to encompass multiple, often conflated, concepts (Lipton, 2018). In the context of Explainable AI (XAI) (Gao et al., 2024; Mehdiyev et al., 2025), a key distinction lies between post-hoc explanations and intrin-sically interpretable models. Post-hoc methods such as SHAP (Lundberg & Lee, 2017; Enouen & Liu, 2025) and LIME (Ribeiro et al., 2016) provide local approximations of black-box predictors, yet their faithfulness to the underlying decision process is frequently questioned. Rudin (Rudin, 2019; Rudin et al., 2022) argues that reliable transparency should instead be achieved through models that are inter-pretable by design. Generalized Additive Models (GAMs) constitute a foundational class of intrinsically interpretable models, expressing predictions as sums of feature-wise ef-fects. Recent variants, such as M-GAM, extend GAMs to practical settings by explicitly handling missing data while preserving interpretability (McTavish et al., 2024). Neural Additive Models (NAMs) (Agarwal et al., 2021) enhance GAMs by parameterizing univariate functions with neural networks, substantially improving expressive power while retaining additive transparency. By enabling direct inspec-tion of learned feature functions, NAMs offer an effective balance between accuracy and interpretability, a property further examined by Mariotti et al. (Mariotti et al., 2023). To capture feature interactions, several extensions have been proposed, including GA 2M with pairwise terms (Lou et al., 2013), NODE-GAM with tree-based components (Chang et al., 2021), and GAMI-Net with hierarchical regulariza-2KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning 

tion (Yang et al., 2021). Nevertheless, modeling complex or high-order interactions in a globally interpretable man-ner remains challenging. In contrast, our approach exploits the compositional, rule-based structure of fuzzy systems to encode rich interactions while maintaining semantic trans-parency. 

2.2. Adaptive Neuro-Fuzzy Inference System (ANFIS) 

ANFIS combine the learning capabilities of neural networks with the transparent, rule-based reasoning of fuzzy logic (Rawal et al., 2025). The seminal ANFIS architecture maps a Takagi-Sugeno fuzzy inference system into a five-layer adaptive network, enabling interpretable “IF-THEN” rules alongside gradient-based optimization. ANFIS has been widely applied in control, regression, and pattern recog-nition tasks due to this balance of adaptability and trans-parency. However, as input dimensionality grows, the num-ber of fuzzy rules increases exponentially, leading to the curse of dimensionality and limiting scalability in high-dimensional or industrial data scenarios (Xue et al., 2023; Cui et al., 2022). To address these limitations, a wide range of extensions have been proposed. Clustering-based rule generation and optimization strategies reduce rule complex-ity, while recurrent and deep fuzzy architectures enhance modeling capacity for temporal and high-dimensional data. For example, fuzzy recurrent stochastic configuration net-works integrate TSK fuzzy inference with recurrent stochas-tic mechanisms, achieving robust performance on indus-trial datasets (Wang & Dang, 2025). Deep spatio-temporal fuzzy models have been applied to geospatial forecasting tasks such as NDVI prediction, effectively capturing spatio-temporal correlations while maintaining interpretability (Su et al., 2025a). Hybrid structure-learnable models like SL-ANFIS-LSTM combine fuzzy reasoning with LSTM dy-namics for ultra-short-term photovoltaic power forecasting, balancing nonlinear temporal modeling and transparent rule extraction (Su et al., 2025b). Recent surveys further high-light the continuous evolution of neuro-fuzzy architectures toward scalable, high-dimensional, and interpretable designs (Gu et al., 2023; Apiecionek, 2025). Despite these advances, most contemporary neuro-fuzzy systems still rely on conven-tional neural parameterizations in antecedent or consequent parts, limiting parameter efficiency and expressive power in very high-dimensional tasks. In contrast, our approach introduces KAN as a parameter-efficient, compositional al-ternative within the fuzzy inference framework, preserving semantic transparency while encoding complex interactions naturally. This design addresses the longstanding trade-off between interpretability and expressive capacity, situating our work at the intersection of structured fuzzy reasoning and function-based neural architectures. 

2.3. Kolmogorov-Arnold Network (KAN) 

The Kolmogorov-Arnold representation theorem states that any multivariate continuous function can be represented as a finite composition of univariate functions and addition, offering a theoretical foundation for structured function approximation. Historically, this theorem was largely dis-missed in neural network design due to smoothness and constructive concerns. Recently, Liu et al. (Liu et al., 2024) revitalized this theory through KAN, which replaces con-ventional linear weight matrices with learnable univariate functions along edges, effectively embedding functional decomposition into the network architecture. Unlike MLP with fixed activation functions, KAN allow each edge to have a learnable activation function, often parameterized as B-splines. This design improves both expressive efficiency and interpretability, as the learned univariate functions can be visualized and semantically analyzed (Somvanshi et al., 2025; Ta, 2025). Extensions of KAN explore alternative bases, such as sinusoidal activations (SineKAN) (Reinhardt et al., 2025) or hybrid B-spline + radial basis functions (BSRBF-KAN) (Ta, 2025), which improve approximation fidelity and training stability across various tasks. Empiri-cal studies demonstrate that KAN can match or outperform larger MLP in both data fitting and symbolic regression tasks while maintaining significantly fewer parameters (Liu et al., 2024; Wang et al., 2025). Complementary theoretical work establishes generalization bounds for KAN, showing that the structured composition of univariate functions al-lows for a rigorous analysis of the complexity of the model (Zhang & Zhou, 2024). In this work, we reinterpret these learnable univariate functions as adaptive fuzzy member-ship functions, grounding KAN within a semantic logical framework. This enables a natural integration with fuzzy inference systems, achieving compositional transparency and interpretable reasoning in high-dimensional predictive modeling. 

2.4. Theoretical Connection: Splines and Fuzzy Sets 

The integration of KAN into fuzzy inference systems is theoretically supported by the well-established connection between B-splines and fuzzy logic. Classical results show that the B-spline neural network can be interpreted as a class of fuzzy systems under specific overlap and normalization conditions . This equivalence provides a rigorous mathe-matical foundation for viewing learned univariate functions as fuzzy membership functions, enabling a principled fu-sion of functional approximation and symbolic reasoning. Traditional B-spline networks, however, typically relied on fixed grid structures, constraining their adaptability and lim-iting expressive capacity in high-dimensional tasks. KAN introduces adaptive, learnable grids along edges, allowing each univariate function to self-organize in response to data. By incorporating these adaptive B-spline functions into the 3KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning          

> Figure 2. Top: KANFIS structures with one and two layers, respectively. Dashed lines indicate paths that may be selected during learning.
> Bottom: Computational flow of the model. xdenotes the input features, ythe predicted output, and ωthe weight of each rule, reflecting the influence of each rule pattern on the final prediction.

antecedent part of fuzzy rules, our approach bridges modern spline-based networks with interpretable logical systems, yielding models that are both mathematically sound and semantically meaningful. Recent work has further extended this connection, demonstrating that structured spline expan-sions within KAN can rigorously approximate continuous multivariate functions while preserving interpretability (Ta, 2025; Reinhardt et al., 2025; Zhang & Zhou, 2024). These advances provide strong theoretical justification for our KANFIS framework, where fuzzy membership semantics are naturally encoded in learnable univariate components of KAN. 

# 3. Methodology 

The KANFIS model reorganizes the classical ANFIS ar-chitecture based on the Kolmogorov-Arnold representation theorem, resulting in a simple yet expressive interpretable neural network. The overall architecture and computational flow of KANFIS are illustrated in Figure 2. This section presents a detailed description of the model architecture and the proposed methodology. 

3.1. Learnable Membership Functions 

Traditional neural networks rely on crisp scalar weights wi,j 

to modulate the contribution of input xi to hidden neuron j.In contrast, KANFIS replaces each such weight with a set of K learnable fuzzy functional mappings. To accommo-date diverse data distributions and modeling requirements, the proposed architecture supports both T1 and IT2 config-urations. In the T1 setting, we incorporate three distinct membership functions to capture local nonlinearities: Gaus-sian, Generalized Bell, and Sigmoid. Detailed formulations for the Bell and Sigmoid functions are provided in Ap-pendix C for completeness. Taking the Gaussian function as the primary example, the membership grade for an input 

xi is parameterized by a learnable center μi,j,k and width 

σi,j,k :

Mi,j,k (xi) = exp − (xi − μi,j,k )2

2σ2

> i,j,k

!

. (1) To enable the model to capture aleatoric uncertainty along-side nonlinearity, we extend the Gaussian basis to the IT2 setting. Each basis function is parameterized by a center 

μi,j,k and a pair of standard deviations (σ(l)

> i,j,k

, σ (u)

> i,j,k

) that satisfy 

σ(l) 

> i,j,k

> 0, σ(u) 

> i,j,k

> σ (l)

> i,j,k

. (2) 4KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning   

> Table 1. Comparative experimental results. We adopt a clustering-based IT2-ANFIS to ensure its suitability as a baseline under high-dimensional feature settings. The symbol ‘-’ indicates that the model cannot be trained due to the curse of dimensionality.

Dataset Metrics IT2-KANFIS T1-KANFIS MLP T1-ANFIS IT2-ANFIS KAN CCPP MAPE 0.7047 0.6777 0.7164 0.6759 0.6696 0.6389 

RMSE 4.1240 3.9542 4.1883 3.9980 4.0047 3.9313 

MAE 3.1975 3.0760 3.2553 3.0688 3.0439 2.8988 

Parkinsons MAPE 14.891 14.477 19.604 16.167 15.446 14.610 RMSE 0.0397 0.0405 0.0527 0.0449 0.0433 0.0404 MAE 0.0291 0.0289 0.0389 0.0320 0.0310 0.0293 BCW ACC 0.9912 0.9912 0.9912 – 0.9737 0.9737 F1 0.9912 0.9912 0.9912 – 0.9737 0.9736 AUROC 0.9990 0.9987 0.9845 – 0.9954 0.9940 Spambase ACC 0.9392 0.9381 0.9175 – 0.8827 –F1 0.9392 0.9379 0.9172 – 0.8821 –AUROC 0.9797 0.9769 0.9599 – 0.9397 –MHR ACC 0.7931 0.7931 0.7094 0.7438 0.6847 0.7783 F1 0.7942 0.7941 0.7028 0.7428 0.6714 0.7777 AUROC 0.9057 0.9157 0.8427 0.8881 0.8535 0.9071 Given an input value xi, its upper and lower membership degrees are 

UMF i,j,k (xi) = exp 

− (xi − μi,j,k )2

2



σ(u)

> i,j,k

2

 , (3) 

LMF i,j,k (xi) = exp 

− (xi − μi,j,k )2

2



σ(l)

> i,j,k

2

 . (4) These upper and lower surfaces define a Footprint of Un-certainty (FOU) associated with the connection i → j. To obtain a crisp quantity during inference, we employ the center-of-sets type-reduction: 

ϕi,j,k (xi) = UMF i,j,k (xi) + LMF i,j,k (xi)2 . (5) The quantity ϕi,j,k (xi) measures how strongly input xi acti-vates the k-th fuzzy basis along the (i, j ) edge. We provide a theoretical proof justifying the validity of this additive aggregation scheme in Appendix B .

3.2. Functional Edges as Fuzzy Rule Antecedents 

Rather than using a single weight, each edge aggregates K

basis memberships: 

ei,j (xi) = 

> K

X

> k=1

ϕi,j,k (xi). (6) This quantity plays the role of a soft rule antecedent . Specif-ically, the interaction between input xi and hidden unit j is interpreted as: 

“If feature xi belongs to one of the fuzzy sets associated with unit j, then the rule fires with strength ei,j (xi).” 

Because different features activate different fuzzy sets, the model naturally learns localized nonlinear transformations without requiring explicit rule enumeration. 

3.3. Kolmogorov-Arnold Aggregation Over Edges 

For hidden unit j, all incoming fuzzy edges are combined through the Kolmogorov-style summation: 

hj =

> din

X

> i=1

ei,j (xi). (7) Equation (7) is motivated directly by the Kolmogorov-Arnold representation theorem, which states that any con-tinuous multivariate function can be represented as a finite superposition of univariate functions and addition. In KAN-FIS, each ϕi,j,k is a univariate fuzzy transformation, and 

ei,j aggregates these univariate transforms, and hj forms a superposition across input dimensions. The resulting hidden representation thus acts as a learned nonlinear embedding constructed entirely through fuzzy functional components. 

3.4. Deep Stacking of Fuzzy-Functional Layers 

Multiple KANFIS layers can be stacked to form a deep fuzzy inference hierarchy. Let h(0) = x denote the input. For the ℓ-th fuzzy layer, the transformation is: 

h(ℓ) = F(ℓ)

h(ℓ−1) 

, (8) 5KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning 

Table 2. Interpretability analysis of fuzzy rules on the CCPP dataset. The six most important rules learned by the model and their associated physical principles are presented in the table, with their validity supported by evidence from the cited relevant physical studies. 

IF THEN Interpretation 

AT is HIGH −0.3407 · (7 .26 · M AT (xAT )) High ambient temperature reduces air density and mass flow, lowering power output (Saravanamuttoo et al., 2001). V is LOW & AP is MED & RH is HIGH 

0.3070 · (2 .96 · M V(xV) + 0 .14 · M AP (xAP ) + 0.16 · M RH (xRH )) 

Low exhaust vacuum and high relative humidity reduce back pressure and increase specific heat, creating optimal operating conditions (Cengel & Boles, 2002). AT is MED & AP is HIGH 0.2541 · (3 .38 · M AT (xAT ) + 0 .48 · M AP (xAP )) 

Higher ambient pressure increases air density, enhancing turbine intake flow and efficiency even at moderate temperatures (Saravanamuttoo et al., 2001). V is LOW & RH is LOW 0.2038 · (2 .31 · M V(xV) + 1 .61 · M RH (xRH )) 

Low exhaust vacuum dominates efficiency; reduced condenser back pressure ensures high power output even at low humidity (Moran et al., 2010). AT is MED & V is HIGH −0.1831 · (3 · M AT (xAT ) + 0 .53 · M V(xV)) 

High exhaust vacuum increases condenser back pressure, restricting steam expansion and offsetting medium-temperature benefits (Moran et al., 2010). AT is HIGH & RH is LOW −0.1351 · (3 .88 · M AT (xAT ) + 0 .93 · M RH (xRH )) 

Low relative humidity implies lower specific heat, slightly reducing gas turbine output at similar temperatures (Cengel & Boles, 2002). 

where F(ℓ) encapsulates Equation (3) -(7) applied to all units in the layer. To stabilize training and maintain numeric consistency across layers, a normalization is applied after each layer except the final one: 

h(ℓ) ← Norm( h(ℓ)), ℓ < L. (9) This structure resembles a deep neural network, but with crisp weights replaced entirely by IT2 fuzzy functional edges, resulting in richer representational capacity and im-proved robustness to uncertainty. 

3.5. Defuzzification via Linear Projection 

After the final fuzzy layer, its output h(L) is mapped to the prediction space via a linear defuzzification layer: 

ˆy = W h(L) + b. (10) This operation aggregates all activated fuzzy rules into a single crisp prediction. It can be interpreted as a generalized consequent layer of a Takagi-Sugeno fuzzy system, where the antecedent firing strengths are given by the KANFIS pipeline and the consequent functions are linear mappings. 

3.6. Regularization 

While KANFIS possesses strong approximation capabilities, unconstrained training of fuzzy systems can lead to redun-dant rules with dense antecedents, which severely hinders interpretability. To ensure the learned rules are both concise (sparse) and diverse (distinct), we augment the primary task loss Ltask with two specific regularization objectives. The total objective function is formulated as: 

Ltotal = Ltask + λsRsparse + λdRdistinct , (11) where λs and λd are hyperparameters that control the strength of the regularization. 

Entropy-based Antecedent Sparsity. To improve human readability, each fuzzy rule should ideally condition on a minimal subset of relevant input features (typically 2-3 an-tecedents). We introduce a learnable soft-masking matrix 

M ∈ [0 , 1] D×K , where Mi,k represents the importance of input feature i to rule k. To avoid the optimization diffi-culties of discrete selection, we relax the problem by mini-mizing the pixel-wise binary entropy of M. This forces the mask values to converge towards either 0 (irrelevant) or 1 (selected): 

Rsparse = 1

D · K

> K

X

> k=1
> D

X

> i=1

Φ( Mi,k ), (12) where Φ( p) = −p log p−(1 −p) log(1 −p). By minimizing 

Rsparse , the model is encouraged to prune unnecessary con-nections, naturally revealing the underlying sparse structure of the data. 

Rule Distinctiveness. A common pathology in neuro-fuzzy learning is mode collapse, where multiple rules con-verge to characterize identical local regions, leading to re-dundancy. To enforce diversity, we propose a distinctiveness regularization term that penalizes the overlap between the firing patterns of different rules. Let wj ∈ RB denote the vector of firing strengths for rule j across a mini-batch of size B. We minimize the pairwise cosine similarity between all distinct pairs of rules: 

Rdistinct =

> K

X

> j=1
> K

X

> k=j+1

w⊤ 

> j

wk

∥wj ∥2∥wk∥2

. (13) 6KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning   

> Figure 3. Comparison of the average number of features per rule across datasets. The figure illustrates the comparison be-tween the number of features used in the rules and the original number of features in each dataset, depending on whether regular-ization is applied.

Minimizing this term encourages the rules to be orthogonal in their activation space, ensuring that each rule special-izes in a unique sub-region of the input domain, thereby maximizing the informational capacity of the limited rule base. 

3.7. Overall Forward Computation 

The complete forward propagation of the KANFIS architec-ture can be summarized as 

h(0) = x, (14) 

h(ℓ) = F(ℓ)

h(ℓ−1) 

, ℓ = 1 , . . . , L, (15) 

ˆy = W h(L) + b. (16) Together, Equation (2) - (16) constitute a complete mathemat-ical description of the proposed KANFIS model. Regarding the universal approximation capability of KANFIS, we pro-vide the proof in the Appendix A .

# 4. Experiments 

To evaluate the capabilities of the proposed model, we select datasets spanning diverse domains and conduct compara-tive experiments against baseline models, including MLP, ANFIS, and KAN. These datasets exhibit significant vari-ations in terms of sample size and feature dimensionality, providing a challenging testbed for performance evaluation. 

4.1. Comparison Experiments 

Table 1 presents the comparative experimental results of KANFIS against other baseline models. The results indicate that KANFIS achieves the best performance across multiple datasets, outperforming the other comparative methods. Fur-thermore, in contrast to MLP, which typically require deep architectures to capture complex patterns, KANFIS often achieves optimal convergence with a single-layer structure, thereby highlighting its structural efficiency. Traditional ANFIS methods are not well-suited for high-dimensional datasets. To enable a comprehensive com-parison, T1-ANFIS retains the conventional partitioning strategy, while IT2-ANFIS adopts scatter partitioning, al-lowing meaningful evaluation in high-dimensional settings. A closer inspection of the comparative results reveals sig-nificant limitations in existing approaches that our model successfully overcomes: While traditional ANFIS performs adequately in low-dimensional spaces, it suffers severely from the curse of dimensionality. In high-dimensional settings, standard grid partitioning becomes computation-ally intractable due to the exponential explosion of rules. Although alternative techniques like scatter partitioning (employed in IT2-ANFIS) allow for execution on high-dimensional data, our results show that this adaptation comes at the cost of a significant drop in accuracy and a degradation of the model’s inherent interpretability. The standard KAN architecture exhibits poor scalability. We observed that the computational cost of KAN escalates drastically with the increase in data volume and feature dimensionality. Consequently, on larger datasets such as Spambase and ForestCover, the baseline KAN failed to com-plete the training process due to prohibitive computational demands. Overall, KANFIS stands out as the most robust interpretable learner, delivering high performance without sacrificing model transparency. We explicitly demonstrate this interpretability through a case study in the subsequent section. 

4.2. Interpretability Analysis 

In this section, we demonstrate the interpretability of our model using an industrial regression dataset, the Combined Cycle Power Plant (CCPP), and a medical classification dataset, the Medical Health Records (MHR). CCPP includes four features, i.e., Ambient Temperature (AT), Exhaust Vac-uum (V), Ambient Pressure (AP), and Relative Humidity (RH), with the target being Energy Output. MHR con-tains seven features, i.e., Age, Systolic Blood Pressure (Sys-tolicBP), Diastolic Blood Pressure (DiastolicBP), Blood Sugar (BS), Body Temperature (BodyTemp), and Heart Rate (HeartRate), with the target being Risk Level. The fea-tures and targets of both datasets are supported by domain-specific prior knowledge, providing a reference to verify whether the model captures genuine physical or physiologi-cal patterns rather than spurious correlations. Furthermore, to better highlight interpretability, we reduced the number of rules in the model. KANFIS possesses structural trans-parency, allowing symbolic IF-THEN rules to be explicitly extracted. Table 2 presents a subset of the dominant rules extracted from the model trained on the CCPP dataset, while 7KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning 

Table 3. Interpretability analysis of fuzzy rules on the MHR dataset. The table presents the six most important rules learned by the model along with their corresponding medical principles, and their validity is supported by evidence from the cited relevant studies. For clarity, only the class formula that best matches each rule is reported in the THEN clause. 

IF THEN Interpretation 

SystolicBP is HIGH & DiastolicBP is MED 

Phighrisk =4.0611 ·(7 .28 ·M SystolicBP (xSystolicBP )+2 .09 ·M DiastolicBP (xDiastolicBP )) 

Elevated systolic and diastolic BP increase cardiovascular risk (He & Whelton, 1999). Age is LOW & SystolicBP is HIGH & BS is LOW 

Plowrisk = 0 .814 · (3 .59 · M Age (xAge ) + 0 .94 ·MSystolicBP (xSystolicBP ) + 1 .56 · M BS (xBS )) 

Young age with slightly high BP and low glucose indicates low overall risk (D’Agostino Sr et al., 2008). DiastolicBP is LOW Phighrisk = 0 .2811 · (4 .49 · M DiastolicBP (xDiastolicBP )) 

Low diastolic BP may reduce perfusion and raise cardiovascular risk (Amin et al., 2006). SystolicBP is HIGH & BS is LOW 

Plowrisk = 1 .4001 ·(1 .48 ·M SystolicBP (xSystolicBP )+3 .81 ·M BS (xBS )) 

Pmediumrisk =1.1433 · (1 .48 · M SystolicBP (xSystolicBP ) + 3 .81 · M BS (xBS )) 

Slightly high SBP with low glucose corresponds to low-to-moderate risk (D’Agostino Sr et al., 2008). SystolicBP is MED & BS is LOW & BodyTemp is LOW 

Pmediumrisk = 1 .0646 · (2 .2 · M SystolicBP (xSystolicBP ) + 3 .21 ·MBS (xBS ) + 1 .34 · M BodyTemp (xBodyTemp )) 

High SBP with low glucose and normal/low temp indicates moderate risk (D’Agostino Sr et al., 2008). Age is HIGH Phighrisk = −0.3013 · (2 .81 · M Age (xAge )) 

Advanced age independently increases cardiovascular risk (D’Agostino Sr et al., 2008). 

Table 3 shows a subset of the dominant rules extracted from the model trained on the MHR dataset. As shown in the two tables above, and supported by references from rel-evant domain literature, we consider the IF-THEN rules provided by the model to be rigorous and consistent with expert knowledge. In the relatively simple CCPP dataset, almost every rule follows the basic principles: AT and V are negatively correlated with energy output, while AP and RH are positively correlated. In the more complex MHR dataset, each rule can also be reasonably interpreted based on relevant medical literature. This provides strong evidence that KANFIS does more than merely fit statistical distribu-tions; it successfully rediscovers the underlying physical or physiological laws governing the data in an unsupervised manner. For higher-dimensional datasets, KANFIS can also control the number of features included in each rule through regularization. 

4.3. Ablation Experiments 

The proposed regularization mechanism is designed to ex-ert precise control over the model structure. By applying regularization, we aim to ensure that the features used in the rules remain within a reasonable range, thereby max-imizing interpretability. To validate this, we conducted ablation studies on five datasets to investigate the impact of regularization on model behavior. As shown in Figure 3, the regularization term plays a crucial role in control-ling model complexity and promoting interpretability. In the absence of regularization, rules across all datasets tend to incorporate all available features, leading to dense and entangled logic that is difficult to interpret. Introducing regularization enforces implicit feature selection at the rule level, substantially reducing the average number of active features per rule. Consequently, the model learns rules that better align with domain knowledge, with each rule focus-ing exclusively on the most informative variables, thereby significantly enhancing interpretability. Our regularization method substantially reduces the number of features utilized by each rule, achieving a maximum reduction of 63.94% on the Spambase dataset. Importantly, despite this increase in sparsity, predictive performance remains largely preserved, suggesting that the proposed regularization effectively elimi-nates redundant or noisy connections without compromising critical information. 

# 5. Conclusion 

This work revisits the classical ANFIS model from the per-spective of the Kolmogorov-Arnold representation theorem and proposes a compact neuro-fuzzy architecture, termed KANFIS. Without introducing additional structural com-plexity, KANFIS combines additive functional decomposi-tion with Interval Type-2 fuzzy modeling, achieving com-petitive predictive performance while preserving intrinsic in-terpretability and robustness to uncertainty through explicit fuzzy reasoning, as well as improved scalability in high-dimensional settings. More broadly, KANFIS offers a unify-ing perspective for integrating classical neuro-fuzzy models with modern function representation principles. Future work may explore the incorporation of richer fuzzy reasoning mechanisms or logical operators within this framework, as well as extensions to more complex tasks and application scenarios, further advancing interpretable neuro-symbolic learning. 8KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning 

# References 

Agarwal, R., Melnick, L., Frosst, N., Zhang, X., Lengerich, B., Caruana, R., and Hinton, G. E. Neural additive models: Interpretable machine learning with neural nets. 

Advances in neural information processing systems , 34: 4699–4711, 2021. Amin, S., Zhang, Y., Felson, D. T., Sawin, C. T., Hannan, M. T., Wilson, P. W., and Kiel, D. P. Estradiol, testos-terone, and the risk for hip fractures in elderly men from the framingham study. The American journal of medicine ,119(5):426–433, 2006. Apiecionek, L. Fuzzy neural networks—a review with case study. Applied Sciences , 15(13):6980, 2025. Arrieta, A. B., D ´ıaz-Rodr ´ıguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Garc ´ıa, S., Gil-L ´opez, S., Molina, D., Benjamins, R., et al. Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information fusion ,58:82–115, 2020. Cengel, Y. A. and Boles, M. A. Thermodynamics: an engineering approach. Sea , 1000(8862):287–93, 2002. Chang, C.-H., Caruana, R., and Goldenberg, A. Node-gam: Neural generalized additive model for interpretable deep learning. arXiv preprint arXiv:2106.01613 , 2021. Cui, Y., Xu, Y., Peng, R., and Wu, D. Layer normalization for tsk fuzzy system optimization in regression problems. 

IEEE Transactions on Fuzzy Systems , 31(1):254–264, 2022. D’Agostino Sr, R. B., Vasan, R. S., Pencina, M. J., Wolf, P. A., Cobain, M., Massaro, J. M., and Kannel, W. B. General cardiovascular risk profile for use in primary care: the framingham heart study. Circulation , 117(6): 743–753, 2008. Enouen, J. and Liu, Y. Instashap: Interpretable additive models explain shapley values instantly. arXiv preprint arXiv:2502.14177 , 2025. Gao, Y., Gu, S., Jiang, J., Hong, S. R., Yu, D., and Zhao, L. Going beyond xai: A systematic survey for explanation-guided learning. ACM Computing Surveys , 56(7):1–39, 2024. Gu, X., Han, J., Shen, Q., and Angelov, P. P. Autonomous learning for fuzzy systems: a review. Artificial Intelli-gence Review , 56(8):7549–7595, 2023. Hassija, V., Chamola, V., Mahapatra, A., Singal, A., Goel, D., Huang, K., Scardapane, S., Spinelli, I., Mahmud, M., and Hussain, A. Interpreting black-box models: a review on explainable artificial intelligence. Cognitive Computation , 16(1):45–74, 2024. He, J. and Whelton, P. K. Elevated systolic blood pressure and risk of cardiovascular and renal disease: overview of evidence from observational epidemiologic studies and randomized controlled trials. American heart journal ,138(3):S211–S219, 1999. Ibrahim, R. and Shafiq, M. O. Explainable convolutional neural networks: a taxonomy, review, and future direc-tions. ACM Computing Surveys , 55(10):1–37, 2023. Khan, F. A., Umar, Z., Jolfaei, A., and Tariq, M. Explainable fuzzy deep learning for prediction of epileptic seizures using eeg. IEEE Transactions on Fuzzy Systems , 2024. Liao, W., Fang, J., Ye, L., Bak-Jensen, B., Yang, Z., and Porte-Agel, F. Can we trust explainable artificial intelli-gence in wind power forecasting? Applied Energy , 376: 124273, 2024. Lipton, Z. C. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue , 16(3):31–57, 2018. Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halver-son, J., Solja ˇci ´c, M., Hou, T. Y., and Tegmark, M. Kan: Kolmogorov-arnold networks. arXiv preprint arXiv:2404.19756 , 2024. Lou, Y., Caruana, R., Gehrke, J., and Hooker, G. Accurate intelligible models with pairwise interactions. In Proceed-ings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining , pp. 623–631, 2013. Lundberg, S. M. and Lee, S.-I. A unified approach to inter-preting model predictions. Advances in neural informa-tion processing systems , 30, 2017. Mariotti, E., Moral, J. M. A., and Gatt, A. Exploring the balance between interpretability and performance with carefully designed constrainable neural additive models. 

Information Fusion , 99:101882, 2023. McTavish, H., Donnelly, J., Seltzer, M., and Rudin, C. In-terpretable generalized additive models for datasets with missing values. Advances in Neural Information Process-ing Systems , 37:11904–11945, 2024. Mehdiyev, N., Majlatow, M., and Fettke, P. Interpretable and explainable machine learning methods for predic-tive process monitoring: A systematic literature review. 

Artificial Intelligence Review , 58(12):378, 2025. Meng, X., Hou, Q., Quan, L., and Qiao, J. Self-organizing fuzzy neural network with adaptive evolution strategy for nonlinear and nonstationary processes. Artificial Intelli-gence Review , 58(9):280, 2025. 9KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning 

Minh, D., Wang, H. X., Li, Y. F., and Nguyen, T. N. Ex-plainable artificial intelligence: a comprehensive review. 

Artificial Intelligence Review , 55(5):3503–3568, 2022. Moran, M. J., Shapiro, H. N., Boettner, D. D., and Bailey, M. B. Fundamentals of engineering thermodynamics .John Wiley & Sons, 2010. Ouifak, H. and Idri, A. A comprehensive review of fuzzy logic based interpretability and explainability of machine learning techniques across domains. Neurocomputing , pp. 130602, 2025. Rawal, R., Chug, A., and Singh, A. P. Review of deep learning revolution on neuro-fuzzy systems. Advances in Data Science and Adaptive Analysis , 17(03):2530001, 2025. Reinhardt, E., Ramakrishnan, D., and Gleyzer, S. Sinekan: Kolmogorov-arnold networks using sinusoidal activation functions. Frontiers in Artificial Intelligence , 7:1462952, 2025. Ribeiro, M. T., Singh, S., and Guestrin, C. ” why should i trust you?” explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining , pp. 1135–1144, 2016. Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature machine intelligence , 1(5):206– 215, 2019. Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., and Zhong, C. Interpretable machine learning: Funda-mental principles and 10 grand challenges. Statistic Sur-veys , 16:1–85, 2022. Saravanamuttoo, H. I. H., Rogers, G. F. C., and Cohen, H. 

Gas turbine theory . Pearson education, 2001. Shihabudheen, K. and Pillai, G. N. Recent advances in neuro-fuzzy system: A survey. Knowledge-Based Sys-tems , 152:136–162, 2018. Somvanshi, S., Javed, S. A., Islam, M. M., Pandit, D., and Das, S. A survey on kolmogorov-arnold network. ACM Computing Surveys , 58(2):1–35, 2025. Su, Z., Shen, J., Sun, Y., Hu, R., Zhou, Q., and Yong, B. Deep spatio-temporal fuzzy model for ndvi forecasting. 

IEEE Transactions on Fuzzy Systems , 33(1):290–301, 2025a. doi: 10.1109/TFUZZ.2024.3409894. Su, Z., Shen, J., Zhou, Q., and Yong, B. Sl-anfis-lstm: A structure learnable fuzzy neural network for ultra-short-term pv power forecasting. IEEE Transactions on Fuzzy Systems , 33(12):4252–4262, 2025b. doi: 10.1109/TFUZZ.2025.3604847. Ta, H.-T. Bsrbf-kan: A combination of b-splines and ra-dial basis functions in kolmogorov-arnold networks. In Buntine, W., Fjeld, M., Tran, T., Tran, M.-T., Huynh Thi Thanh, B., and Miyoshi, T. (eds.), Information and Communication Technology , pp. 3–15, Singapore, 2025. Springer Nature Singapore. ISBN 978-981-96-4288-5. Tursunalieva, A., Alexander, D. L., Dunne, R., Li, J., Riera, L., and Zhao, Y. Making sense of machine learning: a review of interpretation techniques and their applications. 

Applied Sciences , 14(2):496, 2024. Wang, D. and Dang, G. Fuzzy recurrent stochastic con-figuration networks for industrial data analytics. IEEE Transactions on Fuzzy Systems , 33(4):1178–1191, 2025. doi: 10.1109/TFUZZ.2024.3511695. Wang, Z., Lin, X., Wang, D., Cui, C., and Hao, X. Research directions on kolmogorov–arnold networks: A compre-hensive review. Symmetry , 18(1):60, 2025. Xue, G., Wang, J., Zhang, K., and Pal, N. R. High-dimensional fuzzy inference systems. IEEE Transactions on Systems, Man, and Cybernetics: Systems , 54(1):507– 519, 2023. Yang, Z., Zhang, A., and Sudjianto, A. Gami-net: An explainable neural network based on generalized additive models with structured interactions. Pattern Recognition ,120:108192, 2021. Yao, F., Zhao, W., Forshaw, M., and Song, Y. A self-organizing interval type-2 fuzzy neural network for multi-step time series prediction. Applied Soft Computing , pp. 113221, 2025. Zhang, X. and Zhou, H. Generalization bounds and model complexity for kolmogorov-arnold networks. arXiv preprint arXiv:2410.08026 , 2024. 10 KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning 

# A. Universal Approximation Theorem of KANFIS 

This section formally establishes the universal approximation capability of the KANFIS model. The proof demonstrates that by leveraging the inherent approximation power of its Interval Type-2 (IT2) Gaussian basis functions, the KANFIS architecture can approximate any continuous multivariate function on a compact domain. 

A.1. KANFIS Model Formulation 

The single-layer KANFIS model is an additive network structure where the mapping from the input x = ( x1, . . . , x n) to the output fKANFIS (x) is defined by: 

fKANFIS (x) = 

> R

X

> j=1

wj · hj (x) + b

The rule activation hj (x) aggregates the feature contributions via summation: hj (x) = Pni=1 ϕij (xi). The core component, the edge function ϕij (xi), is a summation of M center type-reduced IT2 Gaussian MFs: 

ϕij (xi) = 

> M

X

> m=1

12 [μU,ijm (xi) + μL,ijm (xi)] 

A.2. Key Lemma: RBF Approximation Power 

The proof hinges on the ability of the parameterized edge functions to model single-variable functions. 

Lemma A.1. Let ψ(x) be any continuous function defined on a compact set C ⊂ R. For any ϵ1 > 0, the KANFIS edge function ϕij (x) can be parameterized by adjusting its centers and widths {μ, σ l, σ u} such that it approximates ψ(x) within the error tolerance ϵ1.

∥ϕij (x) − ψ(x)∥∞ < ϵ 1

Proof. The structure of ϕij (x) constitutes a linear combination of Gaussian basis functions with adjustable parameters. Based on the universal approximation theorem for Radial Basis Function Network (RBFN), this specific form of basis function superposition is sufficient to approximate any continuous single-variable function ψ(x) on the compact domain 

C to arbitrary precision. The IT2 nature enhances the model’s capacity to handle uncertainty but retains the necessary mathematical property of basis function approximation. 

A.3. Universal Approximation Theorem 

Theorem A.1 allows us to establish the main result for the overall KANFIS architecture. 

Theorem A.2. Let K ⊂ Rn be a compact set and f : K → R be any continuous function. For any ϵ > 0, there exists a KANFIS model fKANFIS (x) such that: 

sup 

> x∈K

|f (x) − fKANFIS (x)| < ϵ 

Proof. The proof follows the established methodology for showing universal approximation of additive structures. 

1. Approximation by Ideal Additive Structure ( f ∗): By the generalized universal approximation theorems for additive networks (e.g., TSK models), for any ϵ2 > 0, we assume the existence of an ideal function f ∗(x) with the KANFIS structure: 

f ∗(x) = 

> R

X

> j=1

cj ·

" nX

> i=1

ψij (xi)

#

+ c0

such that: 

∥f (x) − f ∗(x)∥∞ < ϵ 2

2. Edge Function Substitution and Error Control: We construct the KANFIS model fK by replacing ψij with the learned edge functions ϕij , setting wj = cj and b = c0. Let Cmax = max j |cj |. Based on Theorem A.1, we choose the 11 KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning 

approximation error ϵ1 for ϕij to satisfy the required global bound: 

∥ψij (xi) − ϕij (xi)∥∞ < ϵ 1 = ϵ − 2ϵ2

2nRC max 

3. Bounding the Structural Error ( ∥f ∗ − fK ∥∞): The error introduced by substituting ψij with ϕij is bounded using the triangle inequality: 

∥f ∗ − fK ∥∞ ≤

> R

X

> j=1

|cj |

> n

X

> i=1

∥ϕij (xi) − ψij (xi)∥∞

< RC max n · ϵ1

= RC max n ·

 ϵ − 2ϵ2

2nRC max 



= ϵ

2 − ϵ2

4. Final Error Analysis: The total approximation error is the sum of the ideal approximation error ( ϵ2) and the structural error ( ∥f ∗ − fK ∥∞): 

∥f − fK ∥∞ ≤ ∥ f − f ∗∥∞ + ∥f ∗ − fK ∥∞

< ϵ 2 +

 ϵ

2 − ϵ2



= ϵ

2

By selecting ϵ2 sufficiently small (e.g., ϵ2 = ϵ/ 4), the total error is less than ϵ. Since ϵ is arbitrary, the proof is complete. 

# B. Comparison of Approximation Properties: Additive vs. Product Fuzzy Systems 

This appendix provides a formal analysis comparing the function approximation capabilities and complexity scaling of Additive Fuzzy Systems (AFS, exemplified by KANFIS) and Product Fuzzy Systems (PFS, exemplified by ANFIS). Let N

be the input dimension and X ⊂ RN be a compact input space. 

B.1. Structural Definitions Definition B.1 (Product Fuzzy System (PFS) Structure) . The output fprod (x) of a PFS (e.g., TSK-ANFIS) uses the Product T-Norm for rule aggregation. The firing strength τj (x) is: 

τj (x) = 

> N

Y

> i=1

μi,j (xi)

The system output is calculated by the weighted sum of consequents: 

fprod (x) = 

PRj=1 τj (x) · cj

PRj=1 τj (x)

where R is the number of rules, and cj are the consequent parameters. 

Definition B.2 (Additive Fuzzy System (AFS) Structure) . The output fsum (x) of an AFS (e.g., KANFIS) is defined by an additive decomposition, following the structure of a Kolmogorov-Arnold Network: 

fsum (x) = 

> H

X

> j=1
> N

X

> i=1

ψi,j (xi)

!

where H is the hidden layer width, and ψi,j (xi) is the single-variable function derived from the input xi to the hidden node 

j.12 KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning 

B.2. Approximation Capability and Scalability Disparity Theorem B.3 (Universal Approximation Equivalence) . Both Product Fuzzy Systems fprod and Additive Fuzzy Systems fsum 

are universal function approximators on a compact set X.Proof. 1. PFS ( fprod ): Established by the fact that the normalized basis functions of a TSK system form a partition of unity, allowing them to uniformly approximate any continuous function on a compact set (Stone-Weierstrass Theorem). 2. AFS ( fsum ): The structure is a two-layer KAN, which is a universal approximator based on the properties derived from the Kolmogorov-Arnold Representation Theorem. The use of continuous and trainable ψi,j (xi) satisfies the necessary conditions for this property. Thus, their theoretical ability to approximate any function is equivalent. 

Theorem B.4 (Efficiency Disparity in High Dimensions) . When approximating functions f : X → R, the parameter complexity of the Additive Fuzzy System fsum scales linearly with the dimension N , while the complexity of the Product Fuzzy System fprod scales exponentially with N .Proof. Let M be the number of MFs per dimension ( M ≥ 2), and Pψ be the parameter count for a single function ψi,j .1. Complexity of PFS ( fprod ): A complete rule base requires R = M N rules. The total parameter complexity Pprod is: 

Pprod ∝ O(M N )

This exponential scaling severely limits the practical application of PFS in high-dimensional space ( N ). 2. Complexity of AFS ( fsum ): The total number of single-variable functions ψi,j is N · H. The total parameter complexity 

Psum is: 

Psum ∝ O(N · H · Pψ )

This demonstrates linear scaling with respect to the input dimension N .Due to the exponential vs. linear scaling, lim N →∞ Pprod /P sum = ∞. Consequently, the Additive Fuzzy System (KANFIS) offers superior practical scalability and efficiency for complex, high-dimensional function approximation. □

# C. T1 Membership Function Specifications 

This appendix provides the explicit definitions of the Type-1 membership functions used in this work. These formulations complement the Gaussian case presented in the main text and are included here for completeness and reproducibility. 

C.1. Structural Definitions Definition C.1 (Generalized Bell Membership Function) . The Generalized Bell membership function defines a smooth and localized fuzzy response for an input variable xi. It is parameterized as 

Mi,j,k (xi) = 11 + xi−ci,j,k 

> ai,j,k
> 2bi,j,k

,where ai,j,k > 0 controls the width of the membership region, bi,j,k > 0 adjusts the slope around the center, and ci,j,k 

denotes the center location. All parameters are learnable. 

Definition C.2 (Sigmoid Membership Function) . The Sigmoid membership function models monotonic fuzzy relationships and is defined as 

Mi,j,k (xi) = 11 + exp ( −αi,j,k (xi − βi,j,k )) 

, where αi,j,k controls the slope and direction of monotonicity, and βi,j,k specifies the transition center. Both parameters are learnable. 13 KANFIS: A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning 

C.2. Functional Characteristics 

The Generalized Bell function provides symmetric, compactly supported responses with adjustable tail behavior, enabling flexible modeling of localized nonlinearities. In contrast, the Sigmoid function introduces asymmetric and monotonic responses, which are suitable for representing threshold-like or directional dependencies. Together with the Gaussian membership function described in the main text, these Type-1 membership functions form a complementary set of basis functions for capturing heterogeneous nonlinear structures. 14