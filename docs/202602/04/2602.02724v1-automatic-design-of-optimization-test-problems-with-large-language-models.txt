Title: Automatic Design of Optimization Test Problems with Large Language Models

URL Source: https://arxiv.org/pdf/2602.02724v1

Published Time: Wed, 04 Feb 2026 01:16:12 GMT

Number of Pages: 25

Markdown Content:
# Automatic Design of Optimization Test Problems with Large Language Models 

## Wojciech Achtelik ∗† Hubert Guzowski †

## Maciej Smo lka † Jacek Ma´ ndziuk ‡

Abstract 

The development of black-box optimization algorithms depends on the availability of benchmark suites that are both diverse and represen-tative of real-world problem landscapes. Widely used collections such as BBOB and CEC remain dominated by hand-crafted synthetic functions and provide limited coverage of the high-dimensional space of Exploratory Landscape Analysis (ELA) features, which in turn biases evaluation and hinders training of meta-black-box optimizers. We introduce Evolution of Test Functions (EoTF), a framework that automatically generates contin-uous optimization test functions whose landscapes match a specified tar-get ELA feature vector. EoTF adapts LLM-driven evolutionary search, originally proposed for heuristic discovery, to evolve interpretable, self-contained numpy implementations of objective functions by minimizing the distance between sampled ELA features of generated candidates and a target profile. In experiments on 24 noiseless BBOB functions and a contamination-mitigating suite of 24 MA-BBOB hybrid functions, EoTF reliably produces non-trivial functions with closely matching ELA charac-teristics and preserves optimizer performance rankings under fixed evalu-ation budgets, supporting their validity as surrogate benchmarks. While a baseline neural-network-based generator achieves higher accuracy in 2D, EoTF substantially outperforms it in 3D and exhibits stable solution qual-ity as dimensionality increases, highlighting favorable scalability. Over-all, EoTF offers a practical route to scalable, portable, and interpretable benchmark generation targeted to desired landscape properties. 

> ∗

This work was partially supported by the National Science Centre, Poland, grant number 2023/49/B/ST6/01404. 

> †

Wojciech Achtelik, Hubert Guzowski, and Maciej Smolka are with the Faculty of Com-puter Science, AGH University of Krakow, Al. A. Mickiewicza 30, 30-059 Krak´ ow, Poland (e-mail: {wachtelik,guzowski,smolka }@agh.edu.pl). 

> ‡

Jacek Ma´ ndziuk is with the Faculty of Mathematics and Information Science, Warsaw University of Technology, ul. Koszykowa 75, 00-662 Warsaw, Poland and with the Faculty of Computer Science, AGH University of Krakow, Al. A. Mickiewicza 30, 30-059 Krakow, Poland (email: mandziuk@mini.pw.edu.pl). 

1

> arXiv:2602.02724v1 [cs.NE] 2 Feb 2026

# 1 Introduction 

The development and analysis of black-box optimization algorithms heavily rely on the availability of diverse and challenging benchmark functions. These test problems represent the principal means by which algorithmic performance is evaluated, different solvers are compared, and their respective strengths and weaknesses are understood. Ideal benchmarks should be computationally ef-ficient, interpretable, and representative of the complex characteristics found in real-world optimization tasks. However, widely-used benchmark suites like the Black-Box Optimization Benchmarking (BBOB) [4] and IEEE CEC Com-petition series [12] rely on artificial functions. These often do not reflect the complexities of real-world applications, such as Hyperparameter Optimization (HPO) [24], and neural network training [15]. Moreover existing benchmarks lack diversity needed for training Meta-Black-Box Optimization algorithms [30]. To systematically characterize and compare optimization problems, the field of Evolutionary Computation (EC) has broadly adopted Exploratory Landscape Analysis [16]. ELA provides a set of quantitative metrics describing the fitness landscape, which have proven valuable for tasks such as automated algorithm selection [1, 27]. Despite the utility of ELA, a significant deficiency persists: existing benchmark suites do not adequately cover the high-dimensional space of possible ELA feature vectors [15]. This coverage gap implies that algorithms are often evaluated on a limited and potentially biased set of problem landscapes, which complicates the generalization of performance findings to new, unobserved problems. While new functions can be designed manually to address these gaps, the process is intricate, time-intensive, and demands considerable mathematical expertise. In response, the research community has investigated methods for the auto-mated generation of benchmark functions. Prominent approaches include affine transformations [29], Genetic Programming (GP) [6, 10, 17, 30] and Neural Net-works (NNs) [20]. The above methods, however, exhibit significant limitations. Affine transformations span only a subset of the entire ELA feature space and remain fundamentally constrained by the structural properties of the underlying BBOB functions [30]. GP can be constrained by a predefined grammar of op-erators and tends to produce convoluted symbolic expressions that are difficult to analyze. NNs, while highly flexible, are opaque models themselves, yielding a set of network weights rather than a transparent mathematical formulation. A critical, shared weakness of GP and NN-based methods is the limited control over the analytical properties of the resulting functions. 

## 1.1 Contribution 

This paper introduces a new method for automatic test function design that leverages the capabilities of modern Large Language Models (LLMs). LLMs present a unique combination of attributes well-suited for this task. Having been trained on extensive corpora of scientific and mathematical literature, they possess a substantial ”expert prior” relevant to optimization theory and practice 2[25, 26]. This enables them to generate human-readable, symbolic functions in code, offering a compelling alternative that balances the structural rigidity of GP with the flexibility of NNs. By producing interpretable expressions, LLMs facilitate a more profound understanding of the generated problem landscapes and shift the role of the human expert from direct design to high-level guidance and validation. The primary contribution of this work is the introduction and validation of ELA-guided benchmark generation using LLMs. We demonstrate the ability of LLMs to generate novel, non-trivial optimization problems that conform to a target ELA feature vector. Through a series of experiments, we demonstrate that the LLM-generated functions not only match the target ELA characteristics but also induce similar performance rankings among a portfolio of optimization algorithms, indicating their efficacy as analytical surrogates. The proposed LLM-based methodology represents a promising, efficient, and accessible approach to creating diverse and insightful benchmarks required to advance the field of Black-Box Optimization. 

# 2 Background and Motivation 

The automated generation of mathematical functions and algorithms, tradition-ally addressed by methods like GP [6], has been recently revolutionized by the advent of LLMs. Unlike GP, which is constrained by a predefined grammar, LLMs leverage vast training corpora of text and code to generate syntactically correct and semantically meaningful expressions in a more flexible, human-readable format. This capability has established LLMs as powerful tools for code generation and scientific discovery. A pioneering application of LLMs in the considered domain is FunSearch [22], which successfully discovered new mathematical functions to solve challenging problems in extremal combinatorics. Building on this concept of evolutionary improvement, subsequent research has applied LLMs to the automatic design of optimization algorithms themselves. Notable examples include Algorithm Evo-lution using Large Language Model (AEL) [13], Evolution of Heuristics (EoH) [14] and LlaMEA [26], which maintain a population of optimizer implemen-tations and use an LLM as a mutation operator to evolve better-performing heuristics. In a task closely related to our objective, LLMs have also been successfully employed for Symbolic Regression [23], where models like LLM-SR [25] find concise mathematical formulas to fit data, capitalizing on the ”expert prior” gained from scientific literature. Another example is LLaMEA-BO, which leverages LLMs to generate novel acquisition functions for Bayesian Optimiza-tion [11]. Prager et al. in [20] propose a pipeline for constructing synthetic test func-tions whose ELA descriptors match a user-specified target. Their method first generates a random point cloud X in the decision space together with random objective values y ∈ [0 , 1], and then optimizes only the objective values y so as to minimize the Euclidean distance between the resulting ELA feature vector and 3a prescribed target vector. Once an optimized point cloud ( X, y ∗) is obtained, they fit a neural network surrogate that deliberately overfits this dataset, and the trained network is used as a new benchmark function. We consider the problem they formulate highly relevant: it directly targets controlled diversity in benchmark suites, described via landscape properties such as ELA features. However, their approach exhibits a fundamental scalability bottleneck. The optimization variable is the full vector y, whose dimensionality equals the point-cloud size; with their recommended sampling rate of 250 · D,the resulting optimization problem scales linearly in D but quickly becomes extremely high-dimensional. This motivated us to explore alternative approaches that sample functions directly, without an explicit intermediate dataset construction step. In this con-text, we view LLMs as implicit distributions over executable program text (e.g., Python code). Sampling from such a distribution yields candidate test func-tions, potentially avoiding the expensive optimization over y while still enabling diversity control. 

## 2.1 Research Gap 

The above works collectively demonstrate that LLMs excel at tasks involving the discovery and evolution of code, functions, and algorithms. However, to the best of our knowledge, their application to the targeted generation of benchmark functions for Black-Box Optimization, guided by specific landscape properties, remains unexplored. While EoH and LlaMEA evolve optimizers, we evolve test functions to rigorously evaluate those optimizers. 

# 3 Proposed method 

We propose Evolution of Test Functions (EoTF), a framework that extends EoH [14] from algorithm design to test function generation. While EoH evolves optimization algorithms, EoTF applies the same LLM-driven evolutionary ar-chitecture to evolve interpretable Python functions whose fitness landscapes match a target ELA feature vector. 

## 3.1 Problem Formulation 

Extending the methodology introduced in [20], we frame test function generation as a landscape feature-matching problem. Given a target vector of ELA features 

ϕ∗ ∈ Rk, the goal is to construct a single-objective continuous function f : X → 

R, with X ⊂ Rd, whose landscape features approximate ϕ∗. In contrast to prior approach that use NNs to model landscapes [20], yielding uninterpretable weight configurations, we search over a space F of human-readable symbolic expressions. Formally, we seek a function f ∗ minimizing: 

f ∗ = arg min   

> f∈F

∥ϕf − ϕ∗∥2 (1) 4where ϕf denotes the feature vector extracted from a sample of the landscape induced by f . Regarding feature selection, we utilize the set of ELA features identified in [20] and extracted via the pflacco [19] package. This selection is motivated primarily by the need for direct comparability with the prior NN-based generation method. Furthermore, this specific subset provides a balanced representation of landscape characteristics. The features include: 

• ela meta.lin simple.adj r2 : Adjusted coefficient of determination for a linear regression model without interaction terms [16]. 

• ela meta.lin w interact.adj r2 : Adjusted coefficient of determination for a linear regression model including interaction terms [16]. 

• ela meta.quad simple.adj r2 : Adjusted coefficient of determination for a quadratic regression model without interaction terms [16]. 

• ela meta.quad w interact.adj r2 : Adjusted coefficient of determina-tion for a quadratic regression model including interaction terms [16]. 

• ela distr.skewness : Skewness of the sample’s objective values [16]. 

• nbc.nb.fitness.cor : Correlation between the fitness values and the in-degree of points in the nearest-better graph [9]. 

• nbc.nn nb.sd ratio : Ratio of the standard deviations of all nearest neigh-bor distances to the nearest better neighbor distances [9]. 

• fitness distance.fitness std : Standard deviation of the objective val-ues within the sample [8]. It is important to note that our method is not structurally sensitive to this specific choice of features. The method is flexible and can be adapted to arbi-trary subsets of ELA features. LLMs support context windows exceeding 1M tokens [28], scaling the method to include a more comprehensive or distinct set of landscape descriptors is feasible and remains a promising direction for future study. In our experiments, target feature vector ϕ∗ is calculated as an average of ELA features calculated for 100 different independent samples, each generated with a different random seed, from the same target benchmark problem instance. All feature vectors are normalized via min-max scaling, where the minimum and maximum values for each feature are obtained from sampling across all problems within a given benchmark suite and dimensionality. 

## 3.2 Evolution of Test Functions 

We adopt the operator taxonomy introduced in EoH [14], comprising one ini-tialization strategy, two exploration operators, and three mutation operators. While the operator semantics remain unchanged, we design domain-specific 5Figure 1: A summary of the proposed EoTF method. prompts tailored to the test function generation task. Full prompt templates are provided in Appendix A. Compared to generating complete optimizers, producing benchmark func-tions is structurally simpler: the output is a Python function without the object-oriented complexity. However, the task imposes its own challenges: the LLM must synthesize mathematically coherent, landscape-diverse expressions that exhibit prescribed statistical and structural properties. An example of a generated function is shown in Listing 1. The function combines quadratic, trigonometric, and cubic terms to produce a non-trivial fitness landscape. This particular instance was produced by Gemini 2.0 Flash to match the ELA profile of a reference function from the BBOB suite. 

## 3.3 Interpretability and Reusability Advantages 

A distinctive advantage of employing LLMs for benchmark function generation lies in the nature of the final artifacts. The output of our method is a self-contained Python function, minimal in dependencies, lightweight in execution, and readily portable. Such functions can be shared without additional com-plexity associated with distributing benchmarks based on NNs or other machine learning models, which often require large model files, specialized frameworks, and non-trivial installation procedures. The reduced dependency footprint fa-cilitates reproducibility and adoption in the research community. Equally important is the interpretability of the generated functions. While the decision-making process of the LLM during generation remains opaque, the produced code is fully transparent. Researchers can inspect the source, parse docstrings, and directly trace the mathematical structure of the function. This property enables both empirical and theoretical analysis: one can study gradient behavior, curvature, modality, or other analytical characteristics directly from the symbolic formulation. In contrast, NN-based benchmarks only yield weight sets, making such analyses far more cumbersome. 61 import numpy as np  

> 23

def problem(x: np.ndarray) -> float:  

> 4

quadratic_term = 0.13 * (  

> 5

x[0] ** 2 + x[1] ** 2  

> 6

) 

> 7

cosine_modulation = 0.13 * np.cos(  

> 8

x[0] - x[1]  

> 9

) 

> 10

linear_interaction_term = 0.045 * (  

> 11

x[0] + x[1] + x[0] * x[1]  

> 12

) 

> 13

skewed_cubic_term = 0.027 * (  

> 14

x[0] ** 3 + 0.5 * x[1] ** 3  

> 15

) 

> 16

bias = 0.05  

> 17

return (  

> 18

quadratic_term  

> 19

+ cosine_modulation  

> 20

+ linear_interaction_term  

> 21

+ skewed_cubic_term  

> 22

+ bias  

> 23

)

Listing 1: Example Python function generated by the EoTF method. We argue that this interpretability significantly enhances the practical util-ity of LLM-generated functions. Beyond their immediate use as benchmarks, they serve as a foundation for theoretical investigations into problem landscape design. Future work may leverage this property to systematically categorize LLM-generated functions, identify recurring structural motifs, and streamline the creation of new test functions tailored to specific algorithmic evaluation needs. 

## 3.4 Scalability to Higher Dimensions 

Practical black-box optimization tasks frequently operate in high-dimensional search spaces. Scaling to such spaces represents a critical bottleneck for prior generative methods. For instance, the recent NN-based approach [20] struggles to generalize across dimensions: reported results for three-dimensional func-tions are notably worse than those for two-dimensional cases. Additionally, NN methods are typically constrained to fixed input sizes. In contrast, LLMs offer an advantage by generating symbolic Python code. By using dimension-agnostic operations (e.g., via numpy arrays [5]), the generated functions can be dimension-independent. A single symbolic expression produced by the LLM can 7thus define a valid landscape for arbitrary input dimensions without modifica-tion or retraining. 

## 3.5 Model selection 

LLMs are evolving at a rapid pace, with new models emerging every few months. For our evaluation, we select both the lightweight models and the more advanced ones capable of reasoning. We adopt Gemini 2.0 Flash [3] as the default model for experiments, motivated by its use in AlphaEvolve [18]. In addition, we include Gemini 2.5 Flash, and Gemini 3.0 Flash in our experiments to analyze the effect of using more recent and more capable LLMs. According to LMArena [2] (as of 21 December 2025), these models rank 89th, 50th, and 3rd, respectively, on the global leaderboard, indicating substantial diversity in capability. These differences are also reflected in latency and cost per token trade-offs. 

## 3.6 Function Selection 

In our setup, we employ multiple target test functions to assess the quality of LLM-generated functions. The primary source of target ELA features is the BBOB benchmark set [4]. We execute EoTF on each function independently. A notable challenge in this setting is data contamination, as many research papers analyzing the ELA features of BBOB functions are likely included in the LLMs’ training corpora. To mitigate this risk, we supplement the benchmark with selected functions from MA-BBOB and randomly generated ELA values. Consistent with [20], we focus on 2, and 3-dimensional problem instances. Akey distinction from [20] is that our approach is substantially faster and can be readily applied to high-dimensional domains. 

# 4 Experimental Setup 

Our experiments are designed to address the following four research questions regarding the feasibility and efficacy of LLM-driven test function generation. 1. RQ1: Can EoTF effectively generate functions with target ELA features? 2. RQ2: Does the method maintain its effectiveness when applied to prob-lems beyond canonical benchmarks? 3. RQ3: What is the impact of LLM on generated test function quality? 4. RQ4: How does the method scale with number of dimensions? 

## 4.1 RQ1: EoTF Performance 

Our main baseline is the NN-based method [20]. Furthermore, to justify the selection of the LLM-based evolution architecture; we compare EoTF against 8LlaMEA, another LLM-based method. LlaMEA maintains a population of can-didate solutions, apply mutation and selection operators implemented via LLM prompts. Since LlaMEA focuses on generation of metaheuristics for continuous optimization, we adopt it to the domain of benchmark function generation by redefining the prompts and the fitness function as the distance between a candi-date’s ELA vector and a predefined target vector. We also introduce a stateless Zero Shot baseline, where all generations are independent and utilize the same prompt. This baseline tests whether EoTF’s added complexity is necessary, or whether a simpler prompt-only LLM approach can achieve comparable quality. For consistency, EoTF, LlaMEA, and the Zero Shot method all employ Gem-ini 2.0 Flash with a fixed budget of 250 queries per method. This model was chosen for its balance of performance and efficiency, as observed in preliminary tests and its use in AlphaEvolve [18]. We employ the standard BBOB suite [4], utilizing the first instance of all 24 functions. 

Evaluation Metrics 

The quality of the synthesized functions ( f ∗) is assessed via a comprehensive evaluation protocol: 

• Method Comparison. To ensure that the generated functions are ro-bust and not artifacts of a specific sampling seed, we do not rely on fitness function values (best distance). Instead, we recalculate the ELA vector over 100 independent samples (of the same generated function) and cal-culate a median distance for each function. Then we use these values to compare different methods and report a single number: percentage of wins between methods. Method A is better than method B for a given prob-lem (function id) if function generated using method A has lower median distance of 100 samples than method B. 

• Visual Verification. For 2D functions, we provide a qualitative assess-ment using scatter plots to visually compare the landscape of generated test functions against their respective targets. 

• Optimizer Ranking. To validate that the generated functions present similar optimization challenges to the targets, we compare the ranking of diverse optimization algorithms on function sets. Following the methodol-ogy of [20], we employ a fixed-budget setup (10 , 000 · D evaluations) across a portfolio of seven algorithms from Nevergrad [21]: Differential Evolution, PSO, EMNA, Nelder-Mead, dCMA-ES, Cobyla, and Random Search. We aggregate results using Critical Difference diagrams, derived from a non-parametric Friedman test with Nemenyi post-hoc analysis from autorank 

[7]. 

## 4.2 RQ2: EoTF Generality 

We employ the standard BBOB suite [4], utilizing the first instance of all 24 noiseless functions. While these provide a recognized baseline, there is a risk 9that modern LLMs have memorized the source code for these specific problems during pre-training. To mitigate the risk of data contamination and test gen-eralization, we constructed a custom suite of 24 ”hybrid” functions using the MA-BBOB generation methodology. We generated 24 hybrid functions by pair-ing the BBOB functions in a cyclic ring topology (Function 1 with 2, 2 with 3, . . . , 24 with 1) and setting the mixing coefficient α = 0 .5. This forces the LLM to generate code for novel ELA features, that are unlikely to exist in its training data. 

## 4.3 RQ3: LLM selection impact 

This question investigates whether more advanced LLMs, with superior reason-ing abilities, produce higher-quality benchmark functions. We compare three models from the Gemini family, representing different tiers of capability, cost, and latency: Gemini 2.0 Flash , Gemini 2.5 Flash , and Gemini 3.0 Flash .The primary metric for this experiment is the average sampled ELA distance achieved for each model. The dataset used is a standard BBOB. Our hypothesis is that more capable models will converge to solutions with a lower distance to the target ELA vector, indicating a better fit to the desired landscape properties. 

## 4.4 RQ4: EoTF Scalability 

The results presented in [20] do not scale well in terms of both latency and qual-ity of solutions. Especially concerning is a discrepancy between best distances and sampled distances for 3 D problems and the fact that the authors restricted the reported results to 2 D and 3 D only, due to computational complexity of their method. Our method scales much better with number of dimensions. We present the results for D = 2 , 3, 4, and 5. The goal is to prove that quality of solutions does not degrade with dimensionality. We report average median sampled distance of all functions in the BBOB. 

# 5 Results 

This section presents empirical results answering the four research questions posed in Section 4. 

## 5.1 RQ1: EoTF Performance 

To provide a direct statistical comparison, we compute pairwise win rates be-tween all methods. For each BBOB function, the method achieving a lower median ELA distance (computed over 100 samples) is declared the winner. Figure 2 presents results for two dimensional problems. The win percentage represents the fraction of the 24 BBOB functions where a row method outper-forms a column method. As can be seen in the figure, neural networks are more accurate than LLMs and win on all 24 problems. Since practical optimization 10 problems have usually dimensionality higher than 2, we also compare methods for 3D search space. Figure 3 presents the results, which are drastically different – now NNs lose in more than 75% of the cases. 

Figure 2: Win-percentage matrix comparing EoTF with three baselines: NNs [20], zero shot, and LLaMEA. The results generally show that EoTF performs well for D = 3, however, its advantage over LLaMEA is slight; while EoTF has a higher win rate against the baseline, the two methods perform similarly in direct comparison. This similarity can be explained by similar architectures of EoTF and LLaMEA and also similar prompts used. These findings suggest that the specific choice of evolutionary architecture may be secondary in this context. This is a point that should be studied in more detail in future work. Simultaneously, both matrices indicate that the Zero Shot approach fails to match the performance of EoTF and LLaMEA. In 2D, the LLM-based evolutionary approaches outperform the Zero Shot baseline in over 90% of the cases, while in 3D, this advantage decreases slightly to 75%. Figure 4 presents critical difference plots comparing algorithm rankings on the original BBOB benchmark suite against the EoTF-generated functions for 2D and 3D problem instances. In the 2D setting, the optimizer ordering is almost unchanged and deviates from BBOB in only two aspects. First, Nelder-Mead and Random Search swap positions; this is a minor effect given that their average ranks are very close and fall within overlapping CD groups. Second, DiagonalCMA deteriorates substantially: while it is mid-ranked on BBOB, it 11 Figure 3: Win-percentage matrix comparing EoTF with three baselines for D =3. becomes the worst performer on the EoTF-generated set. This systematic drop suggests that the synthesized 2D landscapes may be relatively less favorable to CMA-style adaptation, potentially increasing the challenge for covariance-based search dynamics. While this warrants deeper analysis (e.g., ELA-feature shifts that particularly affect CMA behavior), the remainder of the ranking structure is preserved, indicating that EoTF retains the overall optimizer difficulty profile with only localized deviations. Similarly, for 3D the ordering pattern is also stable, with the main change being an effective swap between Cobyla and DiagonalCMA: DiagonalCMA per-forms worse on EoTF than on BBOB, whereas Cobyla improves markedly and moves into the better-performing group. The ordering of the remaining optimiz-ers is largely retained, reinforcing that EoTF preserves the relative performance of most methods when moving to higher dimension. An additional noteworthy difference is that EMNA becomes more clearly separated from the rest on the EoTF-generated set: its rank remains worst, but the gap to the mid-performing cluster increases, suggesting that the generated 3D functions accentuate the mismatch between EMNA’s modeling assumptions and the induced landscapes without altering the overall rank hierarchy. Figure 5 depicts contour visualizations comparing the fitness landscapes of BBOB benchmark functions with their corresponding EoTF-generated counter-parts in 2D. Visual inspection reveals that for the first four function instances 12 Figure 4: Critical distance plot comparing results of a portfolio of algorithms on original and generated (EoTF) problems. (FID 1, 2, 11, and 14), the generated landscapes exhibit strong qualitative sim-ilarity to their target functions, successfully capturing essential characteristics such as the gradual increase in fitness values toward the domain boundaries and the preservation of global structure. However, a notable discrepancy is observed for FID 21, where the generated function fails to reproduce the complex, rugged multimodal landscape. The generated variant exhibits a considerably simplified structure with clearly defined global features. This limitation is quantitatively reflected in the ELA feature space, where FID 21 demonstrates a substantially larger feature distance compared to the other instances. When contrasted with the neural network-based function generation ap-proach proposed by [20], our method produces landscapes that more closely approximate the original BBOB functions. The NN-generated problems exhibit artifacts consistent with overfitting, resulting in landscapes that appear less smooth and lack the natural structure inherent to the benchmark suite. 13 Figure 5: Contour grids for few selected functions generated to match BBOB problems with function ids: 1, 2, 11, 14, 21. On the left are the generated functions, on the right original problems. ELA feature values are presented in the middle column 14 5.2 RQ2: EoTF Generality 

A critical concern in LLM-driven code generation is the potential for data con-tamination, specifically, whether the model is solving the problem analytically or simply recalling memorized solutions for known BBOB functions. To address this, we compared the framework’s performance on the standard BBOB suite against the novel MA-BBOB suite. Figure 6 presents a side-by-side comparison of the final ELA distances achieved for the 24 standard functions versus the 24 hybrid functions. As observed, the distribution of distances is remarkably similar between the two sets. In several instances, the framework achieved a better fit for the novel MA-BBOB functions than for the standard BBOB functions, and vice versa. While this is not a direct one-to-one comparison, as the underlying target functions and their landscapes differ significantly, the consistency in performance across both suites is a strong indicator of robustness. It demonstrates that the method generalizes effectively to unseen problem instances and does not rely solely on the memorization of standard benchmark code. The results shown are for 2D functions. Overall MA-BBOB has better median distance values for around 42% of functions. To complement the MA-BBOB generalization study with a qualitative check, we manually inspected the generated code for all 24 BBOB targets in the D = 2 setting. Only one generated function ( f = 10) contained an explicit reference to the Ackley function; the remaining functions were implemented directly using elementary mathematical primitives (e.g., sin, cos, √·, | · | ) and did not match the structure of standard BBOB reference implementations. While manual in-spection cannot rule out all forms of implicit memorization, it provides evidence that the framework is not simply recalling benchmark code. 

## 5.3 RQ3: LLM selection impact 

To evaluate the influence of model capability on function generation quality, we conducted pairwise comparisons across all 24 benchmark functions. The win-percentage matrix (Figure 7) presents the proportion of functions for which the row method achieved a lower median ELA distance than the column method. This asymmetric matrix reveals a clear capability hierarchy among the evaluated models. 

Gemini 3.0 Flash demonstrates superior performance, achieving wins against 

Gemini 2.5 Flash on 70.8% of functions and against Gemini 2.0 Flash on 54.2% of functions. The intermediate model, Gemini 2.5 Flash , outperforms 

Gemini 2.0 Flash on 54.2% of functions while losing to Gemini 3.0 Flash 

on the majority of comparisons. These results support the hypothesis that increased model capability correlates with improved function generation qual-ity, as measured by ELA distance to the target. In particular, Gemini 2.0 Flash remains competitive despite being the least capable model, winning ap-proximately 45% of pairwise comparisons against both successors. This finding underscores the viability of the EoTF approach even with earlier-generation LLMs. 15 Figure 6: Comparison of the sampled distances for BBOB and MA-BBOB prob-lems. Figure 8 provides a function-level breakdown of median sampled distances, with error bars indicating the interquartile range ( q25 –q75 ). While the win-percentage matrix establishes an overall ranking, the per-function analysis re-veals substantial overlap in performance distributions across models. The rela-16 Figure 7: Win-percentage matrix comparing three Gemini model variants across 24 two-dimensional BBOB functions. Each cell indicates the percentage of func-tions for which the row method achieved a lower median ELA distance than the column method. tively modest absolute differences suggest that model capability improvements yield incremental rather than transformative gains in generation quality for this task. Nevertheless, as LLM capabilities continue to advance alongside declin-ing inference costs, we anticipate cumulative improvements that will further enhance the practical applicability of the proposed method. 

Figure 8: Median sampled ELA distances for all 24 two-dimensional BBOB functions, grouped by function class. Error bars represent the range between 

q25 and q75 .17 5.4 RQ4: EoTF Scalability 

Figure 9 illustrates the scalability of EoTF compared to the neural network-based approach proposed in [20]. EoTF demonstrates remarkable stability in solution quality as dimensionality increases from 2D to 5D, with average me-dian ELA distance remaining consistently between 0.17 and 0.20. This near-constant performance across dimensions suggests that EoTF does not suffer from the curse of dimensionality that typically affects landscape approximation methods. In contrast, results for the neural network approach are available only for 2D and 3D functions due to prohibitive computational costs; as reported in [20], generating a single 2D point cloud requires up to 4 CPU hours, while a 3D point cloud demands nearly 3 CPU days. This computational barrier prevented evaluation in higher dimensions entirely. Notably, while the neural network method achieves superior accuracy in two dimensions, its performance degrades substantially when extended to three dimensions, exceeding an average median ELA distance of 0.27. By comparison, EoTF achieves better solution quality in 5D (approximately 0.17) than the neural network approach attains in 3D. This demonstrates that although EoTF exhibits marginally lower accuracy in the lowest-dimensional case, it offers significantly more favorable scaling proper-ties, a critical advantage for practical applications involving higher-dimensional optimization landscapes. 

Figure 9: Average median ELA distance across dimensions for EoTF and NNs on the BBOB suite. 18 6 Conclusions 

This paper introduced EoTF, a novel framework for the automated genera-tion of benchmark functions guided by target ELA features. By leveraging LLMs within an evolutionary architecture, EoTF produces interpretable, self-contained Python functions that approximate prescribed landscape characteris-tics while circumventing the opacity inherent to NN-based approaches. Our experimental evaluation addressed four research questions, yielding the following principal findings. First, EoTF demonstrates competitive perfor-mance against neural network baselines, with a pronounced advantage in higher-dimensional settings: while neural networks achieve superior accuracy in two dimensions, EoTF prevails on over 80% of three-dimensional problems. Second, the method generalizes effectively to novel problem instances, as evidenced by consistent performance on the MA-BBOB suite, mitigating concerns regarding data contamination from memorized benchmark code. Third, more capable LLMs yield incremental improvements in generation quality, suggesting that continued advances in foundation models will further enhance the method’s ef-ficacy. Fourth, EoTF exhibits favorable scaling properties, maintaining stable solution quality from two to five dimensions, whereas the neural network base-line suffers from both degraded accuracy and prohibitive computational costs beyond three dimensions. The interpretability of the generated artifacts constitutes a distinctive ad-vantage of the proposed approach. Unlike neural network weights, the symbolic Python functions produced by EoTF permit direct inspection, theoretical anal-ysis, and straightforward dissemination within the research community. Fur-thermore, the algorithm ranking experiments confirm that generated functions preserve the discriminative characteristics of the original benchmarks, validating their utility as analytical surrogates. Several limitations warrant acknowledgment. NNs retain an accuracy advan-tage in the lowest-dimensional regime, suggesting that hybrid approaches may prove beneficial. Promising directions for future research include the systematic categorization of structural motifs in LLM-generated functions, the extension to constrained and multi-objective optimization landscapes, and the develop-ment of prompting strategies that explicitly target analytical properties such as specified optima locations or gradient characteristics. As LLM capabilities continue to advance and inference costs decline, we anticipate that the proposed methodology will become an increasingly practical tool for constructing the di-verse, interpretable benchmarks required to rigorously evaluate and advance black-box optimization algorithms. 

# References 

[1] B. Bischl et al. “Algorithm selection based on exploratory landscape anal-ysis and cost-sensitive learning”. In: Annual Conference on Genetic and 

19 Evolutionary Computation . 2012. url : https://api.semanticscholar. org/CorpusID:5015211 .[2] Wei-Lin Chiang et al. “Chatbot arena: an open platform for evaluating LLMs by human preference”. In: Proceedings of the 41st International Conference on Machine Learning . ICML’24. Vienna, Austria: JMLR.org, 2024. [3] Gheorghe Comanici et al. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities . 2025. arXiv: 2507.06261 [cs.CL] . url : https://arxiv. org/abs/2507.06261 .[4] Nikolaus Hansen et al. “COCO: a platform for comparing continuous op-timizers in a black-box setting”. In: Optimization Methods and Software 

36.1 (2021), pp. 114–144. doi : 10.1080/10556788.2020.1808977 . eprint: 

https : / / doi . org / 10 . 1080 / 10556788 . 2020 . 1808977 . url : https : //doi.org/10.1080/10556788.2020.1808977 .[5] Charles R. Harris et al. “Array programming with NumPy”. In: Nature 

585.7825 (Sept. 2020), pp. 357–362. doi : 10.1038/s41586-020-2649-2 .

url : https://doi.org/10.1038/s41586-020-2649-2 .[6] Yifan He and Claus Aranha. “Evolving Benchmark Functions to Com-pare Evolutionary Algorithms via Genetic Programming”. In: 2024 IEEE Congress on Evolutionary Computation (CEC) . 2024, pp. 1–8. doi : 10. 1109/CEC60901.2024.10611801 .[7] Steffen Herbold. “Autorank: A Python package for automated ranking of classifiers”. In: Journal of Open Source Software 5.48 (2020), p. 2173. doi :

10.21105/joss.02173 . url : https://doi.org/10.21105/joss.02173 .[8] Terry Jones and Stephanie Forrest. “Fitness Distance Correlation as a Measure of Problem Difficulty for Genetic Algorithms”. In: Proceedings of the 6th International Conference on Genetic Algorithms . San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1995, pp. 184–192. isbn :1558603700. [9] Pascal Kerschke et al. “Detecting Funnel Structures by Means of Ex-ploratory Landscape Analysis”. In: Proceedings of the 2015 Annual Con-ference on Genetic and Evolutionary Computation . GECCO ’15. Madrid, Spain: Association for Computing Machinery, 2015, pp. 265–272. isbn :9781450334723. doi : 10.1145/2739480.2754642 . url : https://doi. org/10.1145/2739480.2754642 .[10] W. B. Langdon and Riccardo Poli. “Evolving Problems to Learn About Particle Swarm Optimizers and Other Search Algorithms”. In: IEEE Trans-actions on Evolutionary Computation 11.5 (2007), pp. 561–578. doi : 10. 1109/TEVC.2006.886448 .20 [11] Wenhu Li et al. LLaMEA-BO: A Large Language Model Evolutionary Al-gorithm for Automatically Generating Bayesian Optimization Algorithms .2025. arXiv: 2505.21034 [cs.LG] . url : https://arxiv.org/abs/2505. 21034 .[12] Jing Liang et al. Problem Definitions and Evaluation Criteria for the CEC 2020 Special Session on Multimodal Multiobjective Optimization .Dec. 2019. doi : 10.13140/RG.2.2.31746.02247 .[13] Fei Liu et al. Algorithm Evolution Using Large Language Model . 2023. arXiv: 2311.15249 [cs.NE] . url : https://arxiv.org/abs/2311.15249 .[14] Fei Liu et al. “Evolution of heuristics: towards efficient automatic algo-rithm design using large language model”. In: Proceedings of the 41st In-ternational Conference on Machine Learning . ICML’24. Vienna, Austria: JMLR.org, 2024. [15] Katherine Mary Malan and Mario Andr´ es Mu˜ noz. “Why We Should be Benchmarking Evolutionary Algorithms on Neural Network Training Tasks”. In: Proceedings of the Genetic and Evolutionary Computation Conference .New York, NY, USA: Association for Computing Machinery, 2025, pp. 30– 38. isbn : 9798400714658. url : https://doi.org/10.1145/3712256. 3726318 .[16] Olaf Mersmann et al. “Exploratory landscape analysis”. In: Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation .GECCO ’11. Dublin, Ireland: Association for Computing Machinery, 2011, pp. 829–836. isbn : 9781450305570. doi : 10.1145/2001576.2001690 . url :

https://doi.org/10.1145/2001576.2001690 .[17] Mario A. Mu˜ noz and Kate Smith-Miles. “Generating New Space-Filling Test Instances for Continuous Black-Box Optimization”. In: Evolutionary Computation 28.3 (2020), pp. 379–404. doi : 10.1162/evco_a_00262 .[18] Alexander Novikov et al. AlphaEvolve: A coding agent for scientific and algorithmic discovery . 2025. arXiv: 2506 . 13131 [cs.AI] . url : https : //arxiv.org/abs/2506.13131 .[19] Raphael Patrick Prager and Heike Trautmann. “Pflacco: Feature-Based Landscape Analysis of Continuous and Constrained Optimization Prob-lems in Python”. In: Evolutionary Computation (July 2023), pp. 1–25. 

issn : 1063-6560. doi : 10.1162/evco_a_00341 . eprint: https://direct. mit.edu/evco/article-pdf/doi/10.1162/evco\_a\_00341/2148122/ evco\_a\_00341.pdf . url : https://doi.org/10.1162/evco%5C_a%5C_ 00341 .[20] Raphael Patrick Prager et al. “Neural Networks as Black-Box Benchmark Functions Optimized for Exploratory Landscape Features”. In: Proceed-ings of the 17th ACM/SIGEVO Conference on Foundations of Genetic Al-gorithms . FOGA ’23. Potsdam, Germany: Association for Computing Ma-chinery, 2023, pp. 129–139. isbn : 9798400702020. doi : 10.1145/3594805. 3607136 . url : https://doi.org/10.1145/3594805.3607136 .21 [21] J. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization plat-form . https://GitHub.com/FacebookResearch/Nevergrad . 2018. [22] Bernardino Romera-Paredes et al. “Mathematical discoveries from pro-gram search with large language models”. In: Nature 625.7995 (2024), pp. 468–475. [23] Michael Schmidt and Hod Lipson. “Distilling Free-Form Natural Laws from Experimental Data”. In: Science 324.5923 (2009), pp. 81–85. doi :

10.1126/science.1165893 . eprint: https://www.science.org/doi/ pdf/10.1126/science.1165893 . url : https://www.science.org/doi/ abs/10.1126/science.1165893 .[24] Lennart Schneider et al. “HPO × ELA: Investigating Hyperparameter Op-timization Landscapes by Means of Exploratory Landscape Analysis”. In: 

Parallel Problem Solving from Nature – PPSN XVII: 17th International Conference, PPSN 2022, Dortmund, Germany, September 10–14, 2022, Proceedings, Part I . Dortmund, Germany: Springer-Verlag, 2022, pp. 575– 589. isbn : 978-3-031-14713-5. doi : 10.1007/978- 3- 031- 14714- 2_40 .

url : https://doi.org/10.1007/978-3-031-14714-2_40 .[25] Parshin Shojaee et al. “Llm-sr: Scientific equation discovery via program-ming with large language models”. In: arXiv preprint arXiv:2404.18400 

(2024). [26] Niki van Stein and Thomas B¨ ack. “LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics”. In: Trans. Evol. Comp 29.2 (Nov. 2024), pp. 331–345. issn : 1089-778X. 

doi : 10.1109/TEVC.2024.3497793 . url : https://doi.org/10.1109/ TEVC.2024.3497793 .[27] Ryoji Tanabe. “Benchmarking Feature-Based Algorithm Selection Sys-tems for Black-Box Numerical Optimization”. In: IEEE Transactions on Evolutionary Computation 26.6 (2022), pp. 1321–1335. doi : 10 . 1109 / TEVC.2022.3169770 .[28] Gemini Team et al. “Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context”. In: arXiv preprint arXiv:2403.05530 

(2024). [29] Diederick Vermetten et al. “MA-BBOB: A Problem Generator for Black-Box Optimization Using Affine Combinations and Shifts”. In: 5.1 (Mar. 2025). doi : 10 . 1145 / 3673908 . url : https : / / doi . org / 10 . 1145 / 3673908 .[30] Chen Wang et al. Instance Generation for Meta-Black-Box Optimiza-tion through Latent Space Reverse Engineering . 2025. arXiv: 2509.15810 [cs.LG] . url : https://arxiv.org/abs/2509.15810 .22 A Prompt Templates 

This appendix presents the complete prompt templates employed in our evolu-tionary framework for synthetic benchmark function generation. Following the EoH taxonomy [14], we provide one initialization prompt (I1), two exploration prompts (E1, E2), and three mutation prompts (M1, M2, M3). Each prompt incorporates domain-specific instructions for Exploratory Landscape Analysis while preserving the original operator semantics. All prompts share a common prefix that establishes the task context, spec-ifies target ELA features, provides feature descriptions, and defines implemen-tation requirements. For brevity, we present this shared component once in Prompt 1, followed by the operator-specific instructions in Prompts 2–6. 

## A.1 Dynamic Context Variables 

Each prompt template contains placeholders that are populated at runtime: 

• {ela features }: The target normalized ELA feature vector that the gen-erated function should exhibit. 

• {context }: Previously generated function(s) from the evolutionary pro-cess. The content varies by operator type. Exploration prompts (E1, E2) receive multiple functions from the population history, encouraging the model to generate diverse alternatives or identify common structural patterns. Mutation prompts (M1, M2, M3) receive a single parent function selected for modification, enabling focused refinement of promis-ing candidates. Listing 1: Prompt used to generate a synthetic benchmark function (I1) and a common part, shared preamble of the other prompts. 

> You are an expert in Exploratory Landscape Analysis (ELA), advanced optimization benchmarks, and high-dimensional function design. Your task is to generate a single, synthetic benchmark function in Python for testing global optimization algorithms. The primary goal is to create a function whose ELA features closely match the target values provided below. Target Normalized ELA Features: (These are the values the generated function 's landscape should ideally exhibit) {ela_features} ELA Feature Descriptions: - ela_meta.lin_simple.adj_r2: Adjusted R^2 of a linear model. High values suggest linearity. - ela_meta.lin_w_interact.adj_r2: Adjusted R^2 of a linear model with pairwise interactions. - ela_meta.quad_simple.adj_r2: Adjusted R^2 of a quadratic model without interactions. - ela_meta.quad_w_interact.adj_r2: Adjusted R^2 of a full quadratic model.

23 - ela_distr.skewness: Skewness of the objective value distribution. - nbc.nb_fitness.cor: Correlation between fitness and nearest-better connectivity .- nbc.nn_nb.sd_ratio: Ratio of standard deviations (nearest neighbor distance / nearest-better distance). - fitness_distance.fitness_std: Standard deviation of objective values. Implementation Requirements: 1. Language & Libraries: Implement the function in Python, using only NumPy for mathematical operations. 2. Function Signature: 

``` python def problem(x: np.ndarray) -> float: # Docstring goes here pass 

``` 

Your code MUST BE included in a markdown code block. 3. Input: x is a 1D NumPy array of shape (N,). 4. Domain: The function should be designed considering the domain [-5, 5]^N. Ensure operations are valid within this domain. 5. Docstring: Include a concise docstring explaining the mathematical structure of the function. If possible, include the formula. Be specific about the components used. 6. Self-Contained Code: The final output block should only contain the necessary import (import numpy as np) and the function definition. 7. The function must be deterministic: Do not use np.random or any stochastic elements. 

Listing 2: Exploration Prompt (E1): Divergent Generation 

{I1} History: You already generated these functions: {context} Instructions: Please help me create a new function that has a totally different form from the given ones. 

Listing 3: Exploration Prompt (E2): Backbone Extraction 

{I1} History: You already generated these functions: {context} Instructions: Please help me create a new function that has a totally different form from the given ones but can be motivated from them. Firstly, identify the common backbone idea in the provided functions. Secondly, based on the backbone idea create a new solution. 

24 Listing 4: Mutation Prompt (M1): Structural Modification 

{I1} Generated Function: You already generated this function: {context} Instructions: Please assist me in creating a new function that has a different form but can be a modified version of the function provided. 

Listing 5: Mutation Prompt (M2): Parameter Refinement 

{I1} Generated Function: You already generated this function: {context} Instructions: Please identify the main parameters of the generated function and assist me in creating a new version of the function with improved parameter settings. 

Listing 6: Mutation Prompt (M3): Simplification 

{I1} Generated Function: You already generated this function: {context} Instructions: First, you need to identify the main components in the function above. Next, analyze whether any of these components can be overfit to the specific sample of points used to calculate ELA features. Then, based on your analysis, simplify the components to enhance the generalization to other samples. 

25