Title: SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving

URL Source: https://arxiv.org/pdf/2602.03816v1

Published Time: Wed, 04 Feb 2026 02:38:08 GMT

Number of Pages: 27

Markdown Content:
# SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

Yesom Park 1 Annie C. Lu 1 Shao-Ching Huang 2 Qiyang Hu 2 Y. Sungtaek Ju 3 Stanley Osher 1

# Abstract 

We propose SymPlex , a reinforcement learn-ing framework for discovering analytical sym-bolic solutions to partial differential equations (PDEs) without access to ground-truth expres-sions. SymPlex formulates symbolic PDE solv-ing as tree-structured decision-making and op-timizes candidate solutions using only the PDE and its boundary conditions. At its core is Sym-Former , a structure-aware Transformer that mod-els hierarchical symbolic dependencies via tree-relative self-attention and enforces syntactic valid-ity through grammar-constrained autoregressive decoding, overcoming the limited expressivity of sequence-based generators. Unlike numerical and neural approaches that approximate solutions in discretized or implicit function spaces, SymPlex operates directly in symbolic expression space, enabling interpretable and human-readable solu-tions that naturally represent non-smooth behavior and explicit parametric dependence. Empirical re-sults demonstrate exact recovery of non-smooth and parametric PDE solutions using deep learn-ingâ€“based symbolic methods. 

# 1. Introduction 

Analytical solutions to partial differential equations (PDEs) play a fundamental role in science and engineering, pro-viding exact descriptions of physical phenomena and direct interpretability. Unlike numerical approximations, closed-form expressions can represent non-smooth behavior ex-actly, generalize analytically beyond a computational do-main, and expose parametric dependencies critical for in-verse modeling, control, and bifurcation analysis. Despite their importance, finding analytical PDE solutions automati-cally remains a largely unsolved challenge. 

> 1

Department of Mathematics, University of California, Los Angeles, Los Angeles, CA, USA 2Office of Advanced Research Computing (OARC), University of California, Los Angeles, Los Angeles, CA, USA 3Mechanical and Aerospace Engineering, Uni-versity of California, Los Angeles, Los Angeles, CA, USA . Cor-respondence to: Stanley Osher <sjo@math.ucla.edu >.

Figure 1. Comparison of discontinuous solutions of a Hamilton-Jacobi PDE using grid-based, neural network, and our symbolic method. The symbolic approach achieves exact solutions with minimal storage memory, whereas other methods exhibit larger errors and higher storage requirements. 

Most existing PDE solvers operate in approximate repre-sentation spaces. Classical numerical methods, including finite difference (FDM) (Richtmyer & Dill, 1959; LeVeque, 1998), finite volume (FVM) (LeVeque, 2002; Toro, 2013), and finite element (FEM) methods (Ciarlet, 2002; Hughes, 2003), approximate solutions on discretized grids, intro-ducing numerical diffusion and degrading accuracy near sharp gradients or discontinuities. Neural approaches such as physics-informed neural networks (PINNs) (Raissi et al., 2019) represent solutions implicitly via fixed architectures, but suffer from approximation bias, limited extrapolation, and lack of symbolic interpretability. As a result, both paradigms struggle when exactness, symbolic structure, and global generalization are simultaneously required (Fig. 1). A fundamentally different perspective is to solve PDEs by discovering symbolic expressions. From a computational standpoint, symbolic PDE solution discovery is a discrete combinatorial search problem over expression trees com-posed of mathematical operators, variables, and constants. While the formulation closely parallels natural language generation, the present setting poses two key challenges. First, symbolic expressions are inherently tree-structured, with strict grammatical constraints defined by operator arity and hierarchical relationships, rather than linear token se-quences. Second, the problem is inherently unsupervised: ground-truth expressions are unknown, and supervision is provided only implicitly through PDE and boundary condi-tions, yielding sparse and delayed learning signals, arriving only after a complete expression has been generated. Recent work (Wei et al., 2025) explored symbolic PDE dis-covery using reinforcement learning (RL) with recurrent 1 

> arXiv:2602.03816v1 [cs.LG] 3 Feb 2026 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving
> Table 1. Comparison of PDE solution paradigms. Symbolic solutions offer interpretability, exactness, and parametric generalization.

Method Interpretability Solution Accuracy Generalization Parametric Storage Discovery Classical Numerical Methods Low High Approximate Grid-limited âœ—

Neural Network Methods Low High Non-guaranteed Limited âœ—

SymPlex (Ours) High Very Low Exact Analytic âœ“

neural networks (RNNs). While promising, such recurrent approaches struggle to model the long-range dependencies and hierarchical structure inherent to symbolic mathemat-ics, limiting scalability to complex PDEs. Transformers naturally address long-range dependencies, yet standard sequence-based Transformers lack inductive biases for tree structure and grammatical validity, and existing tree-based variants rely on externally provided parse treesâ€”an assump-tion incompatible with symbolic PDE discovery. In this work, we formulate symbolic PDE discovery as a structured decision-making problem over expression trees and introduce SymPlex (SYMbolic PDE Learning EX-plorer), a framework for discovering analytical PDE solu-tions without access to ground-truth expressions. At its core is SymFormer , a structure-aware Transformer that defines a fully differentiable policy over symbolic expressions. Sym-Former integrates tree-relative self-attention to model hier-archical dependencies, grammar-constrained autoregressive decoding to enforce syntactic validity, and traversal-aware positional encoding to preserve structural context during generation. To our knowledge, this is the first Transformer-based architecture for symbolic PDE solution discovery. Because candidate expressions can only be evaluated after complete generation and because no target solutions are available, training necessarily departs from standard super-vised learning for Transformers. We therefore cast symbolic PDE discovery as an RL problem, training SymFormer with PDE-based rewards and augmenting policy optimization with diversity-aware top-k memory for exploration, imita-tion of high-reward expressions for stability, and curricu-lum learning to manage the rapidly growing combinatorial search space as expression complexity increases. Through extensive experiments, we show that SymPlex re-liably discovers exact analytical solutions, including non-smooth solutions that challenge numerical and neural meth-ods. Furthermore, by treating physical coefficients as sym-bolic variables, SymPlex recovers parametric solutions that explicitly reveal how solutions depend on underlying pa-rameters, providing direct physical insight and enabling downstream applications such as inverse problems. Table 5 summarizes the differences between classical numerical methods, neural networkâ€“based approaches, and our sym-bolic framework. In summary, our contributions are: 1. We formulate PDE solving as symbolic expression discovery, highlighting exact representation of non-smooth solutions, analytic generalization, and explicit parametric dependence. 2. We introduce SymFormer , a structure-aware Trans-former for symbolic expression generation that incor-porates explicit hierarchical inductive biases. 3. We present SymPlex , an RL framework for PDE-guided symbolic discovery without requiring access to ground-truth solutions, integrating policy optimiza-tion with constant refinement, diversity-aware top-k

memory, and curriculum learning. Together, these contributions establish a new paradigm for PDE solution discovery that unifies symbolic computation, hierarchical modeling, and reinforcement learning. 

# 2. Related Work 

Symbolic PDE and expression discovery. Automatically discovering analytical or symbolic solutions for PDEs has been studied extensively using symbolic regression tech-niques. Classical methods include genetic programming (GP) and heuristic search strategies (Forrest, 1993; Koza, 1994; Schmidt & Lipson, 2009; BÅ‚adek & Krawiec, 2019; Randall et al., 2022; Jiang & Xue, 2023), which explore the space of candidate expressions to minimize residuals or other fitness measures. While effective for small-scale problems, these approaches often struggle with combinato-rial search spaces, producing overly complex expressions without corresponding performance gains. To improve efficiency and scalability, deterministic sym-bolic search methods (Liang & Yang, 2025) systematically combine operators to construct PDE solutions, providing a structured approach without relying on stochastic explo-ration. More recently, neural networks have been integrated to guide symbolic search via reinforcement learning, such as policy gradients (Wei et al., 2025) or Monte Carlo tree search (Sahoo et al., 2018; Zhang et al., 2023; Dong et al., 2024), enabling candidate expressions that satisfy differen-tial constraints. However, these methods have so far been 2SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

demonstrated only on very simple PDEs, typically admit-ting smooth solutions and shallow symbolic structures, lim-iting their applicability to more complex PDEs. Supervised learningâ€“based symbolic regression methods, pre-trained on large synthetic datasets, can produce expressions in a single forward pass (Li et al., 2023; Kamienny et al., 2022; Biggio et al., 2021), though their performance degrades when the target PDE distribution diverges from the training data. Recent efforts integrate PINNs with symbolic regression to derive interpretable or approximate analytical solutions. Majumdar et al. (2022) combined neural network representa-tions with a small symbolic basis to encode PDE constraints; while effective for simple PDEs, their fixed-basis formula-tion limits expressiveness for general expressions. Other works first train a PINN and then fit a symbolic expression to its output (Changdar et al., 2024; Huang et al., 2025; Das et al., 2025), yielding interpretable solutions for sim-ple PDEs but constrained by the PINNâ€™s representation and approximation errors. Consequently, these methods are lim-ited in expressiveness, accuracy, and scalability for long or complex PDE solutions. 

Tree-structured and grammar-aware Transformers. 

Transformers have been adapted to hierarchical and struc-tured data, such as trees or graphs, to enable reasoning over compositional representations (Wang et al., 2019; Peng et al., 2021; Hu et al., 2021; Zhu et al., 2025; Fu et al., 2025). In symbolic mathematics and program synthesis, grammar-constrained and arity-aware decoding has been used to enforce syntactic correctness during generation (Yin & Neubig, 2017; Saxton et al., 2019; Allamanis et al., 2018). Existing approaches typically assume fixed parse trees or require full supervision, limiting their applicability for solving PDEs, where the expression tree must be in-ferred dynamically. Our SymFormer architecture combines tree-relative self-attention, traversal-aware positional en-coding, and grammar-constrained autoregressive decoding, providing the inductive biases needed to generate valid, hi-erarchical, and semantically meaningful symbolic solutions directly from PDEs. 

# 3. Preliminary 

3.1. Parametric PDEs 

We consider discovering analytical solutions to parametric partial differential equations (PDEs) . Let Î© âŠ‚ Rn denote a spatial domain with boundary âˆ‚Î©, and t âˆˆ [0 , T ] denote time. A general parametric PDE is written as 

F[u]( x, t ; Îº) = f (x, t ; Îº), (x, t ) âˆˆ Î© Ã— [0 , T ], (1) where L is a differential operator, f a source term, and 

Îº âˆˆ Rp denotes physical parameters. For well-posedness, the PDE is supplemented with boundary conditions 

B[u]( x, t ; Îº) = 0 , x âˆˆ âˆ‚Î©, (2) and, for time-dependent problems, an initial condition 

u(x, 0; Îº) = u0(x; Îº), x âˆˆ Î©. (3) Consequently, the solution u(x, t ; Îº) depends explicitly on both the independent variables (x, t ) and the parameters Îº.

3.2. Symbolic Expression Representation 

We represent solutions u(x, t ; Îº) as structured expressions encoded by abstract syntax trees (ASTs) , generated via prefix traversal to enable unambiguous, autoregressive construc-tion while preserving hierarchy. Tokens in the expression belong to three categories: â€¢ Binary operators: B = {+, âˆ’, Ã—, / }, each with arity 

Î±(b) = 2 .â€¢ Unary operators: U = {sin , cos , exp , âˆš, . . . }, each with arity Î±(u) = 1 .â€¢ Variables, parameters, and constants: 

T = {x1, . . . , x n, t, Îº, const },

which are leaf nodes with arity Î±(t) = 0 .The full vocabulary is V = B âˆª U âˆª T . Token arities define a deterministic grammar: at each step, the valid next tokens are uniquely determined, ensuring every prefix sequence forms a syntactically correct AST. This structural constraint supports both symbolic generation and reinforcement learn-ing optimization, guaranteeing well-formed, parametric ex-pressions for u(x, t ; Îº).

# 4. SymFormer: Structure-Aware Attention for Symbolic Expressions 

Symbolic PDE discovery is fundamentally a structured decision-making problem over trees , rather than a flat se-quence prediction task. SymFormer extends the standard Transformer to explicitly incorporate symbolic hierarchy and grammar, enabling differentiable, grammar-preserving generation of expression treesâ€”critical when ground-truth solutions are unavailable 

4.1. Tree-Relative Self-Attention for Hierarchical Reasoning 

Semantic meaning in symbolic expressions arises from hi-erarchical relationships: operators depend on operands, which may be non-adjacent in prefix notation. Standard 3SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving Standard Transforme r:

No structural informatio n +

# *

# +

# *

# ð‘¥ 

## sin 

# ð‘¦ 2  

> Parent -Child
> Siblings

SymFormer : Structure Aware 

# ++ * ð‘¦ 2ð‘¥ sin   

> Figure 2. Left : Standard attention ignores tree structure. Right : SymFormer explicitly models hierarchical relations.

self-attention, based on token identity and linear position, cannot capture such hierarchical dependencies effectively. SymFormer introduces tree-relative self-attention , which conditions attention scores on structural relations inferred from the partial AST. Each token pair (i, j ) is assigned a discrete relation: 

rij =

ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³

0, i = j (self )1, p[j] = i (parent )2, p[i] = j (child )3, p[i] = p[j]Ì¸ = âˆ’1 (sibling )4, i is ancestor of j

5, otherwise ,

where p[Â·] denotes the parent index in the inferred tree. Each relation type is associated with a learnable embedding Rrij ,which augments the attention computation: 

Attn( Q, K, V ) = softmax 

 Q(K + R)âŠ¤

âˆšd



V. 

This allows operators to naturally attend to their operands, operands to their parents, and siblings to each other, en-abling hierarchical information flow even in deep and par-tially completed expression trees. Unlike prior tree-based Transformers with fixed parse trees, SymFormer infers tree structure dynamically during autoregressive genera-tion, which is critical for symbolic PDE discovery where the expression tree is unknown a priori. 

4.2. Traversal-Aware Positional Encoding 

To distinguish nodes in different subtrees that occupy similar structural roles, SymFormer retains sinusoidal positional encodings along the prefix traversal: 

X â† X + PosEnc( prefix position ).

This preserves information about generation order and rel-ative depth without introducing additional tree-specific pa-rameters. Together with tree-relative self-attention, this allows structurally similar but contextually distinct nodes to be represented distinctly, supporting robust modeling of expressions with repeated substructures. 

4.3. Grammar-Constrained Autoregressive Generation 

Expression generation is autoregressive, with dynamic gram-mar and depth constraints enforced at each step. Each token is only expanded if the remaining depth budget allows its minimal required subtree height, ensuring that the generated expressions are syntactically valid and semantically mean-ingful. The autoregressive generation proceeds as follows: 1. Conditions on the previously generated prefix, 2. Reconstructs the partial AST from arity constraints, 3. Computes tree-relative self-attention over the inferred structure, 4. Samples the next token from the grammar- and depth-constrained action space. Dynamic constraints ensure that leaf nodes are restricted to variables or constants, internal nodes satisfy operator arities, degenerate sub-expressions (e.g., x âˆ’ x, x/x ) are filtered, and each token is only expanded if it fits within the remaining depth budget. This separation between symbolic validity, subtree feasibil-ity, and representation learning allows SymFormer to focus on meaningful compositional reasoning. Depth-aware ex-pansion is particularly beneficial for PDE symbolic expres-sion discovery, where repeated sub-expressions and nested operator hierarchies are common. By explicitly modeling hierarchy, grammar, and depth feasibility, SymFormer faith-fully generates deeper and more complex expressions than prior sequential or tree-based symbolic models. In contrast to the RNN-based symbolic solver (Wei et al., 2025), which encode structure implicitly over flat sequences, and prior tree-based Transformers (Wang et al., 2019; Peng et al., 2021; Hu et al., 2021; Zhu et al., 2025), which rely on fixed or externally provided parse trees, SymFormer rep-resents hierarchy explicitly, enforces depth- and grammar-aware constraints, and leverages Transformer self-attention to model long-range dependencies across repeated subtrees. 4SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

4.4. Structure-Conditioned Expressive Power of SymFormer 

In symbolic PDE discovery, solutions are structured expres-sions rather than flat token sequences. Standard sequence models, such as RNNs or vanilla Transformers, reason only over linear prefixes, which can conflate distant sub-expressions and limit generalization over hierarchical struc-ture. SymFormer explicitly models tree hierarchy and gram-mar via tree-relative self-attention and grammar-constrained generation. This allows the model to condition its next-token decisions on the structure of the partially generated expression tree rather than just the linear token order. 

Theorem 4.1 (Informal) . Any grammar-compatible next-token policy that depends only on the structure of a partial abstract syntax tree of bounded depth can be represented by SymFormer. Equivalently, SymFormer can realize any decision rule defined over symbolic tree states, independent of the particular linear prefix representation. 

A formal statement and rigorous proof are provided in Ap-pendix D.2. This result highlights two key strengths of SymFormer: (i) Tree-relative attention enables direct in-formation flow between operators, operands, and subtrees, faithfully capturing the hierarchical dependencies inherent in compositional symbolic expressions. (ii) Dynamic gram-mar constraints ensure that every generated token preserves syntactic validity, thereby restricting the modelâ€™s action space to well-formed symbolic expressions and enabling safe exploration during unsupervised PDE discovery. Together, these properties provide a theoretical justification for SymFormerâ€™s architecture: it has sufficient expressive power to implement any tree-structured token-selection pol-icy, enabling discovery of complex, hierarchical symbolic PDE solutions that cannot, in general, be represented by sequence-based symbolic models. 

# 5. Reinforcement Learning for Symbolic PDE Solutions 

SymPlex trains SymFormer to discover symbolic PDE solu-tions without ground-truth expressions. Unlike conventional supervised sequential training, PDE solution discovery pro-vides feedback only through PDEs and boundary conditions, yielding sparse, delayed rewards defined over entire ex-pression trees. We formulate symbolic PDE discovery as a reinforcement learning problem: SymFormer acts as a stochastic policy over symbolic trees, rewards are derived solely from PDE and boundarg conditions, and learning proceeds without any target expressions. Algorithm 1 summarizes the pipeline: â€¢ SymFormer as a structure-aware autoregressive policy, 

Algorithm 1 Training SymPlex for Symbolic PDE Solutions 

Input: SymFormer policy Ï€Î¸ , top-k memory, curriculum stages S, iterations per stage Ts, number of sequences N ,learning rate Î·

for curriculum stage s = 1 to S do for iteration t = 1 to Ts do 

Sample N sequences {seq i} âˆ¼ Ï€Î¸

for each seq i do 

Optimize constants ci for PDE residual Compute reward Ri using PDE residual + BC loss 

end for 

Update top-k memory with diversity-aware selection Compute policy loss Lpolicy and imitation loss Limit 

Update policy parameters: Î¸ â† Î¸ âˆ’ Î·âˆ‡Î¸ (Lpolicy +

Limit )

end for end for 

â€¢ Gradient-based optimization of continuous constants, â€¢ A diversity-aware top-k memory with imitation, â€¢ Entropy-regularized policy optimization. 

5.1. PDE-Aware Reward 

Let T âˆ¼ Ï€Î¸ be a generated symbolic expression tree and 

c the continuous constants optimized for T . We define the PDE-based reward as 

R(T, c ) = 11 + pE(T, c ) , E(T, c ) = ( LPDE + Î»BC LBC ) [ uT ],

where uT (x; c) denotes the function obtained by evaluating 

T with constants c, Î»BC > 0 is a regularization parameter, and LPDE and LBC are the PDE and boundary residuals mea-sured as L2 norms, similar to PINNs (Raissi et al., 2019). Constants c are optimized separately via gradient descent prior to reward evaluation, decoupling discrete structure search from continuous parameter fitting. This design en-sures that the reward reflects the expressive adequacy of the symbolic structure rather than sensitivity to constant initial-ization, thereby reducing reward variance and stabilizing policy learning. 

5.2. Policy Loss with Entropy and Imitation 

At each node of a generated tree T , the policy predicts a distribution over the next token, with node-wise entropy 

H(p) = âˆ’ X

> vâˆˆV

p(v) log p(v),

encouraging exploration and preventing premature conver-gence in the sparse, combinatorial symbolic search space. 5SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving  

> Table 2. Summary of tested differential equations with problem settings.

Problem Name PDE IC / Source Term Smooth Problem Poisson âˆ’uxx âˆ’ uyy = f (x, y ) f (x, y ) = âˆ’12 x2 âˆ’ 4.8y2

Advection ut + ux + uy = 0 u(x, y, 0) = exp  âˆ’  x2 + y2 /0.5

Heat ut âˆ’ uxx âˆ’ uyy = 0 u(x, y, 0) = sin( x) cos( y)

Non-Smooth Problem Eikonal ut +

q

u2 

> x

+ u2 

> y

= 0 u(x, y, 0) = px2 + y2

Burgers ut âˆ’ 12

 u2 

> x

+ u2

> y

 = 0 u(x, y, 0) = |x| + |y|

Parametric Solution Advection ut + Îº(ux + uy ) = 0 u(x, y, 0) = 

(

1 âˆ’ | x| âˆ’ | y|, if |x| + |y| â‰¤ 1,

0, if |x| + |y| > 1.

Heat ut âˆ’ Îº(uxx + uyy ) = 0 u(x, y, 0) = exp( âˆ’x) exp( âˆ’y)

The RL policy loss is 

Lpolicy = âˆ’ET âˆ¼Ï€Î¸

h

log Ï€Î¸ (T )Â·R (T, c )Â·wdepth 

i

âˆ’Î»ent H[Ï€Î¸ ],

where wdepth penalizes excessively deep trees. The entropy term ensures that the policy continues to explore diverse tree structures, mitigating early collapse to suboptimal patterns. In addition, an imitation loss reinforces previously discov-ered high-reward trees stored in the top-k memory: 

Limit = X 

> (Tâˆ—,c âˆ—)âˆˆtop-k

wR NLL (Ï€Î¸ (T âˆ—) | câˆ—),

where wR is a reward-based weight and NLL denotes the negative log-likelihood of reproducing stored trees. This im-proves sample efficiency by encouraging reuse of successful structures while maintaining exploration. The total loss for updating Î¸ is 

Ltotal = Lpolicy + Î» Limit ,

where Î» balances imitation and policy objectives. 

5.3. Theoretical Guarantee for Symbolic Recovery 

The reward R is designed so that its global maxima corre-spond to the PDE solution whenever it is representable in the symbolic hypothesis class. This ensures that SymPlex can recover exact solutions under ideal optimization while providing probabilistic guarantees for near-optimal policies. 

Theorem 5.1 (Symbolic Recovery by SymPlex) . Let Ï€Î¸

denote the learned SymPlex policy. 

(i) Exact Recovery: If Ï€Î¸ is globally optimal, there exists a tree T âˆ— in its support with constants câˆ— such that 

uT âˆ— (x; câˆ—) = uâˆ—(x) a.e. in Î©.

(ii) Near-Optimal Probabilistic Recovery: For any policy 

Ï€Î¸ with expected reward 

ET âˆ¼Ï€Î¸ [R(T )] â‰¥ 1 âˆ’ Ïµ, Ïµ âˆˆ [0 , 1) ,

there exists at least one tree T in the support with constants 

câˆ— 

> T

satisfying 

inf  

> c

E(T, c ) â‰¤ Ïµ2

(1 âˆ’ Ïµ)2 .

A formal statement and proof are provided in Appen-dices D.3 and D.4. This theorem establishes that SymPlex recovers exact symbolic solutions under globally optimal policies, and that high expected reward under near-optimal policies guarantees the presence of approximate solutions with small PDE residuals. 

5.4. Curriculum Learning 

Symbolic exploration suffers from an exponentially growing search space as expression depth, number of variables, or vocabulary size increases, making complex PDEs with deep expressions or high-dimensional domains challenging. Ap-proaches that treat variables independently (Wei et al., 2025) are generally infeasible, since even separable solutions do not guarantee variable-wise separability in the PDE itself. To address this, we adopt a curriculum learning strategy that gradually increases problem complexity across three stages, using a single shared SymFormer model: â€¢ Stage 1 (Spatial variables): Train on spatial variables 

x only, with t = 0 and Îº fixed. The reward is based solely on the initial condition, allowing the model to learn meaningful spatial expressions first. â€¢ Stage 2 (Full PDE with fixed Îº): Introduce t and train on the full PDE residual, boundary conditions, and 6SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

Table 3. Comparison of PDE solvers on MSE and SRR                                                                                                                                             

> Problem SymPlex SSDE FEX PINN+DSR KAN MSE ( â†“)SRR ( â†‘)MSE ( â†“)SRR ( â†‘)MSE ( â†“)SRR ( â†‘)MSE ( â†“)SRR ( â†‘)MSE ( â†“)SRR ( â†‘)Poisson 0100% 1.24 Ã—10 âˆ’13 5% 1.04 Ã—10 âˆ’15 100% 3.17 Ã—10 âˆ’30% 9.25 Ã—10 âˆ’40% Advection 0100% 1.98 Ã—10 âˆ’10% 2.37 Ã—10 âˆ’20% 1.85 Ã—10 âˆ’20% 3.01 Ã—10 âˆ’30% Heat 0100% 6.12 Ã—10 âˆ’20% 3.73 Ã—10 âˆ’20% 4.30 Ã—10 âˆ’20% 4.87 Ã—10 âˆ’20% Eikonal 0100% 3.43 Ã—10 âˆ’10% 1.26 Ã—10 âˆ’20% 3.27 Ã—10 âˆ’20% 5.15 Ã—10 âˆ’30% Burgers 0100% 1.36 Ã—10 âˆ’32 0% 7.59 Ã—10 âˆ’15 10% 3.50 Ã—10 âˆ’60% 3.85 Ã—10 âˆ’40% Parametric Advection 0100% 1.87 Ã—10 âˆ’10% 3.32 Ã—10 âˆ’20% 5.40 Ã—10 âˆ’20% 2.46 Ã—10 âˆ’20% Parametric Heat 0100% 4.11 Ã—10 âˆ’10% 3.29 Ã—10 âˆ’30% 7.97 Ã—10 âˆ’20% 1.41 Ã—10 âˆ’20%

initial condition, keeping Îº fixed. Stage 1 expressions serve as priors, capturing temporal dynamics while retaining the previously discovered spatial structure. â€¢ Stage 3 (Parametric PDE solution): Include parame-ters Îº as variables, training the model to produce the full parametric solution u(x, t ; Îº). Stage 1â€“2 expres-sions serve as priors, facilitating efficient of the larger search space. This curriculum strengthens the training signal gradually, letting the model first discover useful sub-expressions and then compose them in more complex settings, without as-suming variable-wise separability. 

# 6. Experiments 

We experimentally evaluate the proposed SymPlex frame-work. All experiments are conducted on a single NVIDIA GV100 (TITAN V) GPU. Implementation details are pro-vided in Appendix A. 

6.1. Problem Settings 

Table 2 summarizes the PDE problems considered, covering three categories. Non-smooth and parametric solutions are included to highlight the advantages of symbolic approach. â€¢ Smooth Solutions: Benchmark PDEs with smooth solutions, including the Poisson, advection, and heat equations, are solved on a two-dimensional spatial do-main. These tests evaluate the basic ability of SymPlex to recover PDE solutions. â€¢ Non-smooth Solutions: To demonstrate the capability of symbolic solutions to reduce numerical errors near kinks, we consider two Hamilton-Jacobi PDEs with non-smooth solutions. â€¢ Parametric Solutions: We assess whether symbolic expressions capture the parametric dependence Îº in (1) using advection and heat equations, including some non-smooth solutions, and learn both the solution and its dependence on velocity and diffusion coefficients. 

6.2. Baselines 

We compare SymPlex against existing approaches for sym-bolic PDE solutions: â€¢ SSDE (Wei et al., 2025): RNN-based symbolic regres-sion for PDEs trained with reinforcement learning. â€¢ FEX (Liang & Yang, 2025): Deterministic tree-based symbolic search by combining operators. â€¢ PINN+DSR : Symbolic regression via DSR (Pe-tersen et al., 2019) applied to solutions obtained by PINNs (Raissi et al., 2019). â€¢ KAN (Liu et al., 2024): Symbolic regression using the Kolmogorovâ€“Arnold representation for constructing structured PDE solutions. 

6.3. Evaluation Metrics 

We measure both numerical and symbolic accuracy: â€¢ MSE: Mean squared error between predicted and true solutions. â€¢ Symbolic Recovery Rate (SRR): Fraction of expres-sions that fully recover the true solution, defined as those whose simplified skeleton matches the true so-lution and whose MSE after constant optimization is below 10 âˆ’8, based on (Wei et al., 2025). 

6.4. Results 

Table 3 reports the MSE and SRR for all methods across the PDEs summarized in Table 2, while Table 4 presents the symbolic solutions recovered by SymPlex; baseline expres-sions are provided in Appendix E.1. SymPlex achieves exact recovery (SRR = 100% ) with zero numerical error across all PDEs, including nonlinear, discon-tinuous, and parametric cases. In contrast, existing methods show limited symbolic recovery. SSDE (Wei et al., 2025) occasionally attains low MSE but fails to recover correct 7SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

Table 4. True solution and symbolic expression attained by SymPlex                                                          

> Name Analytic Solution Predict Solution Poisson x4+ 1 .2y4((y )Ë†4 * 1.2) - (-xË†4)
> Advection exp( âˆ’(( xâˆ’t)2+ ( yâˆ’t)2)/0.5) exp(-(2.0*((x - t)Ë†2)))*exp(((t + (-y))*(-2.0*(t +(-y)))))
> Heat sin( x) cos( y) exp( âˆ’2t)(sin(x) * (exp((-2.0*(k * t))) * (0.99 * cos(y))))
> Eikonal
> (p x2+y2âˆ’t, px2+y2â‰¥t
> 0,px2+y2< t max[0,(sqrt((((1.0*x)*x) + (y*y))) -((t))]
> Burgers |x|+|y|+t(abs(y) + ((-0.0 + abs(x)) - (-0.2436 * (t / 0.2436))))
> Parametric Advection max {1âˆ’ | xâˆ’Îºt | âˆ’ | yâˆ’Îºt |,0}max[(1-(abs(((k * t) - y))+(1.0*abs((x -(abs(t)*k))))))]
> Parametric Heat exp( âˆ’x) exp( âˆ’y) exp(2 Îºt )exp((-((k*(-t)))-(x)))* (exp(-(y)) * exp(((1.0*k) * t)))

symbolic structures, particularly for nonlinear and paramet-ric cases, highlighting the difficulty of long-horizon sym-bolic search with recurrent policies. FEX (Liang & Yang, 2025) succeeds on simple linear problems but degradesas expressions become deeper or more compositional. Neu-ral and regression-based pipelines (PINN+DSR and KAN) achieve moderate numerical accuracy but never recover ex-act symbolic solutions, reflecting approximation bias and the lack of explicit structural constraints. These results demonstrate that SymPlex provides a practi-cal and reliable approach for discovering closed-form PDE solutions, in settings requiring exact symbolic recovery and parametric generalization. Additional results are provided in Appendix E. 

6.5. Comparison with Standard PDE Solvers 

To further evaluate the proposed symbolic PDE solver Sym-Plex , we compare it against two representative baselines: the grid-based WENO scheme (Jiang & Peng, 2000) and neural network-based PINNs (Raissi et al., 2019) on Burg-ersâ€™ equation with a non-smooth solution. We report the following metrics: Computation time, the total time to ob-tain the solution; inference time, the time to evaluate the solution at 100 new spatiotemporal points (requiring recom-putation for WENO, but performed post-training for PINN and SymPlex); peak memory, the maximum memory used during computation; and solution storage, the memory re-quired to store the obtained solution for later use. Table 5 summarizes key performance metrics, while qualitative re-sults of the numerical solutions are shown in Figure 1. 

Table 5. Performance comparison of SymPlex with standard base-lines on Burgersâ€™ equation.                      

> Metric WENO PINN SymPlex
> Applicability Domain Grid-only Trained domain only Global Error ( â†“)1.0e-4 7.0e-4 0.0
> Computation Time (s) 0.35 1105.06 642.01 Inference Time (s) 0.26 0.073 6.27e-5
> Peak Memory (MB) 0.06 52.92 251.37 Solution Storage (MB) 131.17 69.94 0.07

The results highlight a key potential of symbolic represen-tations. Unlike WENO and PINN, which show large error spikes near kinks, SymPlex accurately recovers the solution in non-smooth regions. This illustrates that, when the sym-bolic vocabulary includes relevant functions and operators, symbolic expressions can capture non-smoothness with high fidelity, and produce a global solution allowing evaluation beyond the computational domain. Earlier results on para-metric solution recovery further illustrate the advantage of symbolic expressions. Although training SymPlex requires a Transformer-based architecture and RL, resulting in higher memory usage and longer training times, inference is fast. Additionally, storing the solution is highly memory-efficient as only a compact symbolic expression is needed. Overall, these results demon-strate the stength and potential of symbolic PDE solvers to provide readable, global, high-fidelity solutions where tradi-tional numerical or neural methods face challenges. 

# 7. Conclusion 

We presented SymPlex , a reinforcement learning frame-work for discovering analytical PDE solutions via symbolic expressions. By combining a structure-aware Transformer (SymFormer ) with grammar-constrained generation and curriculum learning, SymPlex can reliably generate valid, hierarchical expressions, recover non-smooth solutions ex-actly, and reveal parametric dependencies. This approach provides a scalable and principled paradigm for symbolic PDE discovery, unifying symbolic computation, hierarchi-cal modeling, and reinforcement learning. While SymPlex demonstrates improved performance com-pared to existing symbolic PDE methods, it remains an early-stage approach with several open challenges. High-dimensional PDEs are still difficult due to the combinatorial growth of the search space, and rigorous analysis of conver-gence and accuracy is required to scale symbolic discovery to more complex scientific problems. Another important direction for future work is the selection of the symbolic vo-cabulary: in practical PDE problems, choosing suitable oper-8SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

ators and functions may be nontrivial and require substantial mathematical understanding of the PDE and its solution. 

# Acknowledgment 

This material is based upon work supported by the De-fense Advanced Research Projects Agency (DARPA) under Agreement No. HR00112590074. 

# Impact Statement 

This work introduces a new paradigm for symbolic PDE solution discovery, with potential impact in both scientific computing and mathematical understanding of differential equations. It also demonstrates a novel extension of Trans-former architectures to structured, symbolic reasoning. We do not anticipate any direct ethical or societal risks. 

# References 

Allamanis, M., Barr, E. T., Devanbu, P., and Sutton, C. A survey of machine learning for big code and naturalness. 

ACM Computing Surveys (CSUR) , 51(4):1â€“37, 2018. Biggio, L., Bendinelli, T., Neitz, A., Lucchi, A., and Paras-candolo, G. Neural symbolic regression that scales. In 

International Conference on Machine Learning , pp. 936â€“ 945. Pmlr, 2021. BÅ‚adek, I. and Krawiec, K. Solving symbolic regression problems with formal constraints. In Proceedings of the Genetic and Evolutionary Computation Conference , pp. 977â€“984, 2019. Changdar, S., Bhaumik, B., Sadhukhan, N., Pandey, S., Mukhopadhyay, S., De, S., and Bakalis, S. Integrating symbolic regression with physics-informed neural net-works for simulating nonlinear wave dynamics in arterial blood flow. Physics of Fluids , 36(12), 2024. Ciarlet, P. G. The finite element method for elliptic problems .SIAM, 2002. Das, J., Bhaumik, B., De, S., and Changdar, S. Physics-informed neural network with symbolic regression for deriving analytical approximate solutions to nonlinear partial differential equations. Neural Computing and Applications , pp. 1â€“36, 2025. Dong, J., Zhong, J., Liu, W.-L., and Zhang, J. Evolving equation learner for symbolic regression. IEEE Transac-tions on Evolutionary Computation , 2024. Forrest, S. Genetic algorithms: principles of natural selec-tion applied to computation. Science , 261(5123):872â€“878, 1993. Fu, P., Xiao, G., and Yang, H. Satd: syntax-aware handwrit-ten mathematical expression recognition based on tree-structured transformer decoder. The Visual Computer , 41 (2):883â€“900, 2025. Hu, X., Mi, H., Wen, Z., Wang, Y., Su, Y., Zheng, J., and De Melo, G. R2d2: Recursive transformer based on differentiable tree for interpretable hierarchical language modeling. arXiv preprint arXiv:2107.00967 , 2021. Huang, M., Xu, Z., Cai, C., Hu, C., Qiu, J., and Yin, W. Physics-informed neural networks and symbolic regres-sion for equation discovery in non-destructive evaluation of composite plates. Measurement , pp. 119324, 2025. Hughes, T. J. The finite element method: linear static and dynamic finite element analysis . Courier Corporation, 2003. Jiang, G.-S. and Peng, D. Weighted eno schemes for hamiltonâ€“jacobi equations. SIAM Journal on Scientific computing , 21(6):2126â€“2143, 2000. Jiang, N. and Xue, Y. Symbolic regression via control variable genetic programming. In Joint European Con-ference on Machine Learning and Knowledge Discovery in Databases , pp. 178â€“195. Springer, 2023. Kamienny, P.-A., dâ€™Ascoli, S., Lample, G., and Charton, F. End-to-end symbolic regression with transformers. 

Advances in Neural Information Processing Systems , 35: 10269â€“10281, 2022. Koza, J. R. Genetic programming as a means for program-ming computers by natural selection. Statistics and com-puting , 4(2):87â€“112, 1994. LeVeque, R. J. Finite difference methods for differential equations. Draft version for use in AMath , 585(6):112, 1998. LeVeque, R. J. Finite volume methods for hyperbolic prob-lems , volume 31. Cambridge university press, 2002. Li, W., Li, W., Yu, L., Wu, M., Sun, L., Liu, J., Li, Y., Wei, S., Deng, Y., and Hao, M. A neural-guided dynamic symbolic network for exploring mathematical expressions from data. arXiv preprint arXiv:2309.13705 , 2023. Liang, S. and Yang, H. Finite expression method for solving high-dimensional partial differential equations. Journal of Machine Learning Research , 26(138):1â€“31, 2025. Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halver-son, J., Solja Ë‡ci Â´c, M., Hou, T. Y., and Tegmark, M. Kan: Kolmogorov-arnold networks. arXiv preprint arXiv:2404.19756 , 2024. 9SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

Majumdar, R., Jadhav, V., Deodhar, A., Karande, S., Vig, L., and Runkana, V. Physics informed symbolic networks. 

arXiv preprint arXiv:2207.06240 , 2022. Park, Y. and Osher, S. Neural implicit solution formula for efficiently solving hamilton-jacobi equations. 2025. URL 

https://arxiv.org/abs/2501.19351 .Peng, H., Li, G., Wang, W., Zhao, Y., and Jin, Z. Inte-grating tree path in transformer for code representation. 

Advances in Neural Information Processing Systems , 34: 9343â€“9354, 2021. Petersen, B. K., Landajuela, M., Mundhenk, T. N., Santi-ago, C. P., Kim, S. K., and Kim, J. T. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. arXiv preprint arXiv:1912.04871 , 2019. Raissi, M., Perdikaris, P., and Karniadakis, G. E. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics , 378:686â€“707, 2019. Randall, D. L., Townsend, T. S., Hochhalter, J. D., and Bomarito, G. F. Bingo: a customizable framework for symbolic regression with genetic programming. In Pro-ceedings of the genetic and evolutionary computation conference companion , pp. 2282â€“2288, 2022. Richtmyer, R. D. and Dill, E. Difference methods for initial-value problems. Physics Today , 12(4):50â€“50, 1959. Sahoo, S., Lampert, C., and Martius, G. Learning equations for extrapolation and control. In International Conference on Machine Learning , pp. 4442â€“4450. Pmlr, 2018. Saxton, D., Grefenstette, E., Hill, F., and Kohli, P. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557 , 2019. Schmidt, M. and Lipson, H. Distilling free-form natural laws from experimental data. science , 324(5923):81â€“85, 2009. Toro, E. F. Riemann solvers and numerical methods for fluid dynamics: a practical introduction . Springer Science & Business Media, 2013. Wang, Y., Lee, H.-Y., and Chen, Y.-N. Tree transformer: Integrating tree structures into self-attention. In Pro-ceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-tional Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 1061â€“1070, 2019. Wei, S., Li, Y., Yu, L., Li, W., Wu, M., Sun, L., Liu, J., Qin, H., Deng, Y., Han, J., et al. Closed-form solutions: A new perspective on solving differential equations. Inter-national Conference on Machine Learning , 2025. Yin, P. and Neubig, G. A syntactic neural model for general-purpose code generation. arXiv preprint arXiv:1704.01696 , 2017. Zhang, M., Kim, S., Lu, P. Y., and Solja Ë‡ci Â´c, M. Deep learn-ing and symbolic regression for discovering parametric equations. IEEE Transactions on Neural Networks and Learning Systems , 2023. Zhu, J., Zhao, W., Li, Y., Hu, X., and Gao, L. Tamer: Tree-aware transformer for handwritten mathematical expres-sion recognition. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 39, pp. 10950â€“10958, 2025. 10 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

# A. Implementation Details of SymFormer 

This section provides a detailed description of the SymFormer architecture used for symbolic PDE discovery, complementing the high-level discussion in Section 4. It covers the token vocabulary, hierarchical attention mechanism, autoregressive generation procedure, and hyperparameter settings. 

A.1. Token Vocabulary and Arity 

SymFormer operates over a fixed vocabulary V comprising variables, constants, and unary and binary operators: â€¢ Variables: x0, x 1, . . . , x nâˆ’1, t, optionally including Îº for parametric problems. â€¢ Constants: trainable â€˜ const â€˜ tokens. â€¢ Unary operators: neg, relu, sqrt, square, ... .â€¢ Binary operators: +, âˆ’, âˆ—, / .Each token t âˆˆ V is associated with an arity function arity (t), defining the number of children in the expression tree: arity (t) = 

ï£±ï£´ï£²ï£´ï£³

0, t âˆˆ Variables or Constants 

1, t âˆˆ Unary operators 

2, t âˆˆ Binary operators .

A.2. Tree-Relative Multi-Head Self-Attention 

To capture hierarchical dependencies in symbolic expressions, SymFormer extends standard multi-head attention with 

tree-relative embeddings . Given a partially generated prefix sequence, the AST is reconstructed dynamically based on token arities. The relation between any two tokens i and j is categorized as: 

rij =

ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³

0, i = j (self )1, p[j] = i (parent )2, p[i] = j (child )3, p[i] = p[j]Ì¸ = âˆ’1 (sibling )4, i is ancestor of j

5, otherwise ,

where p[Â·] denotes the parent index in the inferred tree. Each relation type rij is associated with a learnable embedding Rrij ,which are initialized randomly and optimized jointly with all other model parameters during training. For a partially generated sequence, the corresponding AST is reconstructed dynamically based on token arities. A relation matrix R is then formed by looking up the embedding of each pairwise relation rij . This matrix is added to the projected keys in multi-head attention: 

Attn( Q, K, V ) = softmax Q(K + R)âŠ¤

âˆšdhead 

!

V. 

By directly incorporating Rrij into the attention computation, the model is able to condition attention scores on the syntactic roles of tokens within the tree, enabling operators to attend preferentially to their operands, siblings, and ancestors. This approach preserves the full differentiability of the Transformer while explicitly embedding symbolic structure, allowing the model to learn the relative importance of different hierarchical relationships during training. 

A.3. Traversal-Aware Positional Encoding 

While tree-relative attention captures hierarchical relations, nodes occupying similar structural roles in different subtrees remain ambiguous. SymFormer applies sinusoidal positional encodings along the prefix traversal order: 

X â† X + PosEnc( prefix position ),

ensuring distinguishability of structurally similar but contextually distinct nodes. 11 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

A.4. Grammar-Constrained Autoregressive Generation 

Expression generation is performed autoregressively with dynamic grammar constraints: 1. The partial AST is reconstructed from token arities. 2. Tree-relative attention is computed over the current sequence. 3. The next token is sampled only from the grammar-valid action space: â€¢ Leaf nodes â†’ variables or constants. â€¢ Internal nodes â†’ operators consistent with arity. â€¢ Degenerate expressions (e.g., x âˆ’ x, x/x ) are filtered through semantic resampling. This ensures all generated sequences are syntactically valid while maintaining differentiability for reinforcement learning objectives. 

A.5. Expression Evaluation and Constant Optimization 

Given a prefix sequence T = [ t1, . . . , t L] and associated constants c, expressions are evaluated recursively. Constants are broadcast across the batch to preserve gradient flow for optimization. This enables end-to-end differentiable training of both the symbolic structure and associated parameters. 

A.6. Model Architecture 

SymFormer comprises a stack of N decoder layers, each containing: 1. Embedding Layer: Token embeddings et âˆˆ Rdmodel .2. Traversal-Aware Positional Encoding: Applied to et as described above. 3. Tree-Relative Multi-Head Attention: Incorporates relation embeddings Rrij in multi-head attention. 4. Feed-Forward Network: Two-layer position-wise MLP with ReLU activations. 5. Layer Normalization and Residual Connections: Applied to stabilize training and propagate hierarchical features. 6. Decoder Output Layer: Maps the last tokenâ€™s hidden representation to the vocabulary distribution. This design explicitly encodes symbolic hierarchy, enforces grammar, and allows differentiable autoregressive generation without exposing implementation-specific details. 

A.7. Sequence Sampling and Entropy Regularization 

During generation, SymFormer controls tree depth and applies semantic filters: â€¢ Maximum tree depth dmax prevents excessively large expressions. â€¢ Degenerate or all-constant subtrees are resampled. â€¢ Node-wise entropy is computed to regularize the RL policy. 

A.8. Hyperparameters 

All experiments used a fixed set of hyperparameters: â€¢ Embedding dimension: dmodel = 64 

â€¢ Feed-forward hidden dimension: 128 

12 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

â€¢ Number of relation types: 6

â€¢ Decoder layers: 4

â€¢ Multi-head attention: 8 heads â€¢ Maximum generation depth: 7â€“10 

These values were chosen to balance representational capacity with computational efficiency across all symbolic PDE discovery tasks. 

# B. Reinforcement Learning Training Details for Symbolic PDE Discovery 

Our RL-based symbolic PDE discovery uses a tree-structured Transformer decoder to generate candidate expressions, optimize their constants, and select top-k diverse solutions. The training procedure is outlined as follows: â€¢ Sequence Sampling: At each training epoch, SymFormer samples 64 candidate token sequences from the Transformer policy. Sampling is performed under grammar constraints with a controlled maximum tree depth and a fixed set of allowed variables. â€¢ Loss computation: SymFormer is trained by minimizing an energy-based objective defined on the generated symbolic expression uT :

E(T, c ) =  LPDE + Î»BC LBC 

[uT ],

where LPDE and LBC denote the PDE and boundary-condition residuals, respectively. Both residuals are approximated using Monte Carlo sampling at each training epoch: 

LPDE [uT ] = 1

NPDE 

> NPDE

X

> i=1

 F[uT ]( xi, t i; Îºi) âˆ’ f (xi, t i; Îºi)2,

LBC [uT ] = 1

NBC 

> NBC

X

> i=1

 B [uT ] ( xbi , t bi ; Îºbi )2,

where (xi, t i) are sampled uniformly from the interior of the domain, (xbi , t bi ) are sampled uniformly from the domain boundary, and for parametric PDEs, Îºi and Îºbi are sampled uniformly from the range of parameter values used during training. For time-dependent PDEs, we additionally include the initial condition loss: 

LIC [uT ] = 1

NIC 

> NIC

X

> i=1

 uT (x0 

> i

, 0; Îº0 

> i

) âˆ’ u0(x0 

> i

; Îº0 

> i

)2,

where x0 

> i

are uniformly sampled from the spatial domain and Îº0 

> i

are uniformly sampled from the parameter range. The final training objective is given by L[uT ] =  LPDE + Î»BC LBC + Î»IC LIC 

[uT ]. Across all experiments, we fix 

Î»BC = Î»IC = 10 and use NPDE = 200 , NBC = NIC = 80 .For Hamiltonâ€“Jacobi-type equations such as the Burgersâ€™ and Eikonal equations, the PDE is ill-posed and admits infinitely many weak solutions. The physically relevant solution is the viscosity solution , which is not guaranteed to be recovered by the standard PINN residual alone. To address this issue, for these two equations we replace LPDE with an implicit characteristic-based loss adapted from Park & Osher (2025): 

1

NPDE 

> NPDE

X

> i=1



u + tH (âˆ‡u) âˆ’ tâˆ‡uTâˆ‡H(âˆ‡u) âˆ’ u0

 x âˆ’ tâˆ‡H(âˆ‡u)2

This formulation enforces consistency with the characteristic flow and biases the optimization toward the viscosity solution. 13 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

â€¢ Constant Optimization: For each sampled sequence, any symbolic constants are optimized via gradient-based minimization of the PDE residual and boundary losses. The objective function is: 

min  

> c

E(T, c ).

In practice, we perform gradient-based optimization using the Adam optimizer with a learning rate of 0.02 for 50 steps per sequence. If a sequence contains no constants, this step is skipped. â€¢ Reward Computation: Each candidate sequence is assigned a reward based on the inverse of its PDE loss E:

R = 11 + âˆšE .

â€¢ Diversity-Preserving Top-K Memory: A memory buffer stores up to K = 10 high-quality candidate expressions. To ensure that stored solutions are not only structurally different but also semantically distinct, we apply a two-stage filtering procedure: 1. Structural Diversity: Each symbolic expression is first canonicalized into a sequence of tokens representing the expression tree with commutative operators sorted consistently. The Levenshtein distance dLev (S1, S 2) between two sequences S1 and S2 is computed, and a candidate is considered structurally diverse if 

dLev (Snew , S mem ) â‰¥ Î´s, âˆ€Smem âˆˆ Memory ,

where Î´s is a predefined threshold. This prevents near-duplicate sequences from being added to memory. 2. Semantic/Behavioral Diversity: Even if two sequences differ structurally, they may represent mathematically equivalent expressions. To capture this, each candidate expression unew (x; cnew ) is evaluated numerically at a set of random test points xj :

unew (xj ; cnew ) = eval expression (Snew , x j , c new ), j = 1 , . . . , N test .

Let umem (xj ; cmem ) denote the output of an expression stored in memory. The candidate is considered behaviorally diverse if 

1

Ntest 

> Ntest

X

> j=1

unew (xj ; cnew ) âˆ’ umem (xj ; cmem ) â‰¥ Î´b, âˆ€(Smem , c mem ) âˆˆ Memory ,

where Î´b is a numerical threshold. This ensures that two expressions with different trees but equivalent functional forms (e.g., x and 1.0 Â· x + y âˆ’ y) are recognized as semantically identical and not redundantly stored. The combination of structural and semantic filtering allows the top-K memory to maintain truly diverse solutions in both the syntactic and functional sense, which is critical for efficiently exploring the symbolic search space. We consider this semantic-aware memory curation a key contribution of our method. â€¢ Policy Objective. The SymFormer policy is optimized using a policy-gradient objective with stabilization techniques. Given a batch of sampled sequences {Ti}Ni=1 with corresponding losses {E i}, rewards are first converted into rank-based scores to reduce sensitivity to outliers. Specifically, sequences are ranked according to Ei, and the rank-based reward is defined as 

ri = 1 âˆ’ rank( Ei)

N âˆ’ 1 .

When applicable, rewards are normalized to zero mean and unit variance. To discourage overly complex symbolic expressions, we apply a depth-based weighting scheme. For each sequence Ti,let Di denote the maximum depth of its expression tree, and define the depth weight as 

wi = 1

Di + 1 .

14 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

The resulting policy objective is given by 

Lpolicy = âˆ’ 1

N

> N

X

> i=1

wi ri log Ï€Î¸ (Ti) âˆ’ Î»ent E[H(Ï€Î¸ )] .

An entropy regularization term is included to encourage exploration: 

Lpolicy = âˆ’E log Ï€Î¸ (T ) Ë†R âˆ’ Î»ent EH(Ï€Î¸ ),

where Ë†R denotes the normalized rank-based reward and Î»ent = 0 .3.â€¢ Imitation Loss To further stabilize training and accelerate convergence, we optionally incorporate an imitation loss derived from the top-K memory. When the best observed reward exceeds a threshold ( 0.8 in all experiments), the policy is encouraged to imitate high-reward sequences stored in the memory buffer. Let {Tj }Kj=1 denote the sequences stored in the top-K memory with corresponding rewards {rj }. Each sequence is assigned a weight using a softmax over rewards with temperature Ï„ :

Î±j = exp( rj /Ï„ )

PKk=1 exp( rk/Ï„ ) , Ï„ = 0 .1.

The imitation loss is defined as 

Limit =

> K

X

> j=1

Î±j

1

|Tj | 

> |Tj|

X

> t=1

âˆ’ log Ï€Î¸



T (t) 

> j

| T (<t )

> j



,

where T (t) 

> j

denotes the token at position t in sequence Tj . This loss is added to the policy objective with a fixed weighting factor. â€¢ Policy Optimization: The total loss (policy loss with imitation loss) is minimized using Adam with learning rate 

5 Ã— 10 âˆ’4 and a ReduceLROnPlateau scheduler (factor 0.9, patience 10 ). Gradients are clipped to a maximum â„“2 norm of 5.0 to stabilize training. â€¢ Memory Refinement. To improve the quality of stored solutions, symbolic constants associated with expressions in the top-K memory are periodically re-optimized. Every 10 training epochs, or upon transitioning to a new curriculum stage, the constants are refined via gradient-based minimization of the PDE residual using a larger number of optimization steps (200) with the same optimizer (Adam with learning rate 0.02). The updated expressions are then re-evaluated, and the top-K memory is re-sorted according to the refined rewards. â€¢ Curriculum and Staging: Training is performed using a three-stage curriculum that gradually increases problem complexity. 1. Stage 1 (Spatial structure): We initially exclude the temporal variable t and the parameter variables Îº to focus on learning meaningful spatial structures. The vocabulary is defined as 

VStage1 = B âˆª U âˆª { x1, . . . , x n, const }.

During this stage, the loss is computed solely using the initial condition loss LIC , with the parameter fixed as 

Îº0 

> i

= 1 .2. Stage 2 (Spatiotemporal dynamics with fixed parameters): We introduce the temporal variable t and define the vocabulary as 

VStage2 = B âˆª U âˆª { x1, . . . , x n, t, const }.

The full loss 

LPDE + Î»BC LBC + Î»IC LIC 

is used for training, while all parameter values Îº are fixed to 1. 15 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

3. Stage 3 (Parametric PDE learning): Finally, we include all variables and define the vocabulary as 

VStage3 = B âˆª U âˆª { x1, . . . , x n, t, Îº, const }.

The model is trained using the full loss to learn the complete parametric solution. For each stage, the maximum number of training epochs is set to 500. If the reward exceeds 0.99 within this limit, training proceeds to the next stage immediately; otherwise, the model advances to the next stage after completing 200 epochs. This implementation enables effective RL-based discovery of symbolic expressions that satisfy PDE constraints while preserving interpretability and diversity. Although the procedure involves multiple components and can be computation-ally demanding, operations such as top-K memory updates and constant refinement are parallelizable across candidate expressions, so they do not introduce significant bottlenecks. 

# C. Implementation Details of Baselines 

C.1. SSDE 

We used the official SSDE repository 1 without modification. All configurations and training parameters were kept as close as possible to those reported by the original authors. For each experimental setting, we ran SSDE with 20 different random seeds and report the best-performing result in terms of MSE. Both the pre-constant-optimization and post-constant-optimization symbolic expressions provided by SSDE were evaluated, and the expression achieving the lowest MSE was selected for reporting. For newly implemented PDE settings, the official 2D Poisson and 2D/3D Heat equation implementations were used as the foundation and configuration templates. 

C.2. FEX 

We used the official repository code 2 without modification. All configurations and training parameters were kept as close as possible to those reported by the original authors. For each experiment, FEX was run with 20 different random seeds, and the best-performing symbolic expression in terms of MSE was reported. For all experiments, the Poisson equation implementation provided in the official repository was used as the foundation for configuring new PDE settings. 

C.3. PINN+DSR 

Following (Wei et al., 2025), we performed symbolic regression using a PINN+DSR framework to obtain symbolic PDE expressions and establish these baselines. We utilized the DeepXDE and DSO packages for the PINN implementation and symbolic optimization, respectively. In our implementation, DeepXDE utilized the PyTorch backend while DSO employed TensorFlow, with CUDA-enabled GPU acceleration for both frameworks. The PINN was trained using a two-stage optimization strategy, first Adam then L-BFGS iterations, on a set of collocation points sampled from the domain, boundaries, and initial conditions. Our results demonstrate strong agreement with (Wei et al., 2025). Specifically, for the Poisson equation, we obtained a physics loss ( LPHY ) of 6.74 Ã— 10 âˆ’1 Â± 6.11 Ã— 10 âˆ’2 which is consistent with their reported value of 5.71 Ã— 10 âˆ’1 Â± 7.88 Ã— 10 âˆ’2. Due to the stochastic nature of symbolic regression, precise replication of results is inherently difficult. To ensure robustness, we conducted 20 independent trials for each PDE case using unique random seeds. The Mean Squared Error (MSE) was calculated for every identified expression, and the candidate yielding the minimum MSE was selected as best discovered expression. 

C.4. KAN 

These baselines use a physics-informed KAN trained by the standard PINN objective (denoted as LPhy ), rather than a purely data-driven fit. Using pykan 3, the KAN architecture is kept deliberately minimal with a single hidden layer width=[d, d, 1] , where d matches the number of physical input variables in each problem (e.g., (x, y, t, Îº, . . . )). Training uses a two-stage optimizer (Adam warm-up followed by L-BFGS refinement) on a fixed set of collocation/constraint points 

> 1https://github.com/Hintonein/SSDE
> 2https://github.com/LeungSamWai/Finite-expression-method
> 3https://github.com/KindXiaoming/pykan

16 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

(NF , N B, N I ) sampled from the domain, boundary, and initial-time manifold, while all reported MSE metrics are computed pointwise on the full, standardized evaluation grid ( ND ) shared across baseline models. Finally, the symbolic form is extracted only through the built-in KAN pipeline auto symbolic() â†’ symbolic formula() with the specified unary basis library, without any additional external simplification or post-processing. 

# D. Theoretical Guarantees for Symbolic Recovery 

D.1. Notation 

Let V = X âˆª C âˆª U âˆª B be a finite vocabulary consisting of variables, constants, unary operators, and binary operators. Each token v âˆˆ V is equipped with a fixed arity Î±(v) âˆˆ { 0, 1, 2}.Let Dmax < âˆž denote a fixed maximum expression depth. We denote by Tâ‰¤Dmax the set of all finite rooted ordered abstract syntax trees (ASTs) whose node labels lie in V, whose arities match Î±, and whose depth does not exceed Dmax .We denote by Tâ‰¤Dmax the set of all such partial ASTs. Since both V and Dmax are finite, |Tâ‰¤Dmax | < âˆž.For a finite set V, we denote by 

âˆ†( V) = 

(

Ï€ : V â†’ [0 , 1] X

> vâˆˆV

Ï€(v) = 1 

)

the probability simplex over V. Elements of âˆ†( V) are discrete probability distributions over the vocabulary. 

Prefix-induced partial abstract syntax trees. Let 

p = ( t1, . . . , t k)

be a valid prefix generated under the arity constraints. By deterministically applying the arity-based reconstruction rule, p

induces a unique partial abstract syntax tree , denoted by A(p). This partial tree consists of instantiated internal nodes and a set of open argument slots (frontier nodes). Let PV,D max denote the set of all such valid prefixes whose induced partial trees have depth at most Dmax .

Structural equivalence. Let A1 and A2 be two partial abstract syntax trees induced by valid prefixes under the arity constraints. We say that A1 and A2 are structurally isomorphic , denoted by A1 âˆ¼= A2, if there exists a bijection between their nodes that preserves: 1. the arity and operator type of each internal node, 2. the parentâ€“child relations between nodes, 3. the leftâ€“right ordering of children. Terminal nodes corresponding to variables or constants are treated as unlabeled leaves of arity zero; their specific identities are not considered part of the structure. Traversal indices and linear prefix positions are likewise not considered part of the structure. For example, consider the two prefixes given in prefix notation: 

p = (+ x) and q = (+ y).

Both prefixes are valid and induce partial abstract syntax trees consisting of a single binary operator node labeled â€œ +â€, whose left child is a terminal node and whose right child is an unfilled argument position. Although the terminal symbols x and y

differ, the induced partial trees have identical operator structure and frontier configuration, and are therefore structurally isomorphic. 

Structural state space. Let eTâ‰¤Dmax denote the set of all partial abstract syntax trees that can arise as prefix-induced states of valid expression trees with maximum depth at most Dmax . We define the structural state space as the quotient set 

Tâ‰¤Dmax = eTâ‰¤Dmax 

 âˆ¼=,

17 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

that is, the set of equivalence classes of partial trees under structural isomorphism. Since the vocabulary V is finite, the arity of each symbol is bounded, and the depth is bounded by Dmax , the number of distinct structural states is finite, and hence 

|Tâ‰¤Dmax | < âˆž.

D.2. Structure-Conditioned Policy Universality 

We formalize the expressive power of SymFormer as an autoregressive policy over symbolic expressions. Unlike trivial universality results based solely on finiteness of the action space, our analysis explicitly leverages SymFormerâ€™s tree-relative self-attention and grammar-constrained decoding, showing that the model can represent policies defined over symbolic tree structures rather than linear token prefixes. 

Theorem D.1 (Structure-Conditioned Policy Universality of SymFormer) . Let V, Î±, and Dmax be defined as above. Consider any target policy 

Ï€â‹† : Tâ‰¤Dmax â†’ âˆ†( V)

satisfying the following conditions: 1. Ï€â‹†(Â· | A ) depends only on the partial abstract syntax tree A, and not on the specific linear prefix that induces it. 2. The support of Ï€â‹†(Â· | A ) is contained in Vvalid (A), the set of grammar-valid next tokens given the partial tree A.Then, for any Îµ > 0, there exists a parameter vector Î¸Îµ of the SymFormer model such that the induced autoregressive policy 

Ï€Î¸Îµ satisfies 

sup 

> pâˆˆP V,D max

âˆ¥Ï€Î¸Îµ (Â· | p) âˆ’ Ï€â‹†(Â· | A (p)) âˆ¥1 â‰¤ Îµ. 

In particular, for any two prefixes p, q âˆˆ P V,D max such that A(p) âˆ¼= A(q), we have 

âˆ¥Ï€Î¸Îµ (Â· | p) âˆ’ Ï€Î¸Îµ (Â· | q)âˆ¥1 â‰¤ Îµ. 

Proof. Since V is finite and the maximum depth is bounded by Dmax , the number of distinct partial ASTs in Tâ‰¤Dmax is finite. Let p = ( t1, . . . , t m) and q = ( s1, . . . , s n) be two prefixes inducing partial ASTs A(p) and A(q), respectively. Define the tree-relation matrix for prefix p as 

R(p) âˆˆ { 0, 1, . . . , 5}mÃ—m, R(p) 

> ij

:= rij (A(p)) ,

where rij denotes the discrete relation between tokens i and j (self, parent, child, sibling, ancestor, other). If A(p) âˆ¼= A(q) (structurally isomorphic), then there exists a bijection Ï• : nodes( A(p)) â†’ nodes( A(q)) preserving parent-child relations and left-right order. It follows that 

R(q) = P âŠ¤R(p)P

for some permutation matrix P induced by Ï•.Let H(0) (p) âˆˆ RmÃ—d0 denote the input embeddings of p. SymFormer layer â„“ applies tree-relative self-attention and a feedforward update: 

H(â„“+1) (p) = FFN 



Attn  H(â„“)(p), R (p)

,

where Attn is the tree-relative attention function. By construction, the attention operation is equivariant under node permutations that preserve the tree structure. Therefore, there exists a choice of parameters for which the final hidden representation after L layers satisfies 

h(p) := H(L)(p) = Î¦( A(p)) âˆˆ Rd,

18 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

for some injective mapping Î¦ : Tâ‰¤Dmax â†’ Rd. Injectivity is achievable since |Tâ‰¤Dmax | < âˆž and d can be chosen sufficiently large. Let Ï€â‹† : Tâ‰¤Dmax â†’ âˆ†( V) denote the target structure-conditioned policy, where âˆ†( V) is the probability simplex over V.Let the output logits of SymFormer be given by a feedforward mapping gÎ¸ : Rd â†’ R|V| followed by a masked softmax enforcing the valid action set 

Vvalid (A(p)) := {t âˆˆ V | token t satisfies arity/grammar constraints given A(p)}.

Then, the autoregressive policy of SymFormer is 

Ï€Î¸ (Â· | p) = softmax 



gÎ¸ (h(p)) âŠ™ 1Vvalid (A(p)) 



,

where 1Vvalid (A(p)) masks out invalid tokens. Since Tâ‰¤Dmax is finite, for each A âˆˆ Tâ‰¤Dmax , we can set the logits gÎ¸ (Î¦( A)) âˆˆ R|V| such that 

Ï€Î¸ (Â· | p) = Ï€â‹†(Â· | A (p)) , âˆ€p with A(p) = A.

This exactly realizes the target policy. By construction, the masked softmax ensures that 

supp  Ï€Î¸ (Â· | p) âŠ† V valid (A(p)) ,

so the generated tokens always respect the grammar constraints. Combining the above,we conclude that there exists a choice of parameters Î¸Îµ such that 

Ï€Î¸Îµ (Â· | p) = Ï€â‹†(Â· | A (p)) , âˆ€p âˆˆ P V,D max .

Equivalently, 

sup 

> pâˆˆP V,D max

Ï€Î¸Îµ (Â· | p) âˆ’ Ï€â‹†(Â· | A (p)) 1 = 0 .

In other words, SymFormer can exactly realize any grammar-compatible, structure-conditioned target policy over the finite set of partial ASTs. This completes the proof. Theorem D.1 establishes expressive universality of SymFormer at the level of symbolic tree states , rather than linear token prefixes. Note that his result should not be interpreted as a generic function approximation capabilities for Transformers. Instead, the theoretical contribution lies in showing that SymFormer can realize arbitrary policies defined over symbolic tree-structured states, provided that these policies respect the underlying grammar constraints. The expressive burden of the model is therefore concentrated entirely on the construction of representations that are invariant to the specific linear prefix realization and that separate distinct partial abstract syntax tree structures. Once such structure-invariant and structure-separating representations are obtained, universality of the induced policy follows directly from the finiteness of the structural state space, without requiring any appeal to general approximation results for deep neural networks. This reliance on explicit structural modeling is critical. Standard Transformers operating on linear token sequences, as well as RNN-based symbolic generators, lack mechanisms to enforce invariance across structurally isomorphic partial abstract syntax trees. As a consequence, even when the set of symbolic tree states is finite, such models cannot, in general, represent arbitrary structure-conditioned policies. 

D.3. Exact Recovery under Global Optimality Theorem D.2 (Conditional Exact Symbolic Recovery by SymPlex) . Let Î© âŠ‚ Rn be a bounded domain with Lipschitz boundary, and consider the PDE 

L[u]( x) = f (x), x âˆˆ Î©, B[u]( x) = 0 , x âˆˆ âˆ‚Î©.

Assume: 

19 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

(A1) Well-posedness: The PDE admits a unique solution uâˆ— âˆˆ L2(Î©) .(A2) Symbolic realizability: There exists a finite-depth expression tree T âˆ— over the SymPlex vocabulary V and constants câˆ—

such that 

uâˆ—(x) = uT âˆ— (x; câˆ—), a.e. x âˆˆ Î©.

(A3) Hypothesis class coverage: The SymFormer policy generates trees in TD (V) with D â‰¥ depth( T âˆ—).(A4) Exact residual characterization: For any tree T ,

inf  

> c

E(T, c ) = 0 â‡â‡’ uT (Â·; c) = uâˆ— in L2(Î©) .

If SymPlex converges to a globally optimal policy Ï€âˆ— that maximizes 

J(Ï€) = ET âˆ¼Ï€

"

11 + pE(T, c âˆ— 

> T

)

#

,

then Ï€âˆ— assigns nonzero probability to at least one tree T âˆ— satisfying 

âˆ¥uT âˆ— (Â·; câˆ—  

> Tâˆ—

) âˆ’ uâˆ—âˆ¥L2(Î©) = 0 .

Thus, SymPlex achieves exact symbolic recovery of the PDE solution. Proof. By Assumption (A2), there exists T âˆ— and câˆ— such that uT âˆ— (Â·; câˆ—) = uâˆ—. Assumption (A4) implies E(T âˆ—, c âˆ—) = 0 ,hence R(T âˆ—) = 1 .For any TÌ¸ = T âˆ—, uniqueness implies inf c E(T, c ) > 0, so R(T ) < 1. Hence T âˆ— is a global maximizer of the reward. A globally optimal policy Ï€âˆ— assigns nonzero probability to all reward-maximizing trees, including T âˆ—, yielding exact recovery. 

D.4. Probabilistic Guarantees under Near-Optimal Policies 

Let TD (V) denote the set of all symbolic expression trees over the SymPlex vocabulary V up to depth D, and let Ï€Î¸ be a stochastic policy (SymFormer) over TD (V). Define the PDE residual-based reward for a tree T âˆˆ T D (V) as 

R(T, c âˆ— 

> T

) := 11 + pinf c E(T, c ) , câˆ— 

> T

:= arg min  

> c

E(T, c ),

where E(T, c ) is the PDE residual with boundary conditions evaluated for tree T and constants c. Assume that the PDE admits an exact symbolic solution T âˆ— with constants câˆ— such that E(T âˆ—, c âˆ—) = 0 , hence R(T âˆ—, c âˆ—) = 1 .

Theorem D.3 (Near-Optimal Probabilistic Recovery) . Let Ï€Î¸ be any policy over TD (V), not necessarily optimal, and assume the expected reward satisfies 

J(Ï€Î¸ ) := ET âˆ¼Ï€Î¸ [R(T, c âˆ— 

> T

)] â‰¥ 1 âˆ’ Ïµ, Ïµ âˆˆ [0 , 1) .

Then there exists at least one tree T âˆˆ supp( Ï€Î¸ ) such that 

R(T, c âˆ— 

> T

) â‰¥ 1 âˆ’ Ïµ. 

Equivalently, the policy assigns nonzero probability to at least one high-reward symbolic solution whose PDE residual is 

inf  

> c

E(T, c ) â‰¤ Ïµ2

(1 âˆ’ Ïµ)2 .

20 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

Proof. Let supp( Ï€Î¸ ) = {T âˆˆ T D (V) : Ï€Î¸ (T ) > 0}. Then 

J(Ï€Î¸ ) = X  

> Tâˆˆsupp( Ï€Î¸)

Ï€Î¸ (T ) R(T, c âˆ— 

> T

), X  

> Tâˆˆsupp( Ï€Î¸)

Ï€Î¸ (T ) = 1 .

Suppose, for contradiction, that every tree in the support satisfies 

R(T, c âˆ— 

> T

) < 1 âˆ’ Ïµ, âˆ€T âˆˆ supp( Ï€Î¸ ).

Then 

J(Ï€Î¸ ) = X  

> Tâˆˆsupp( Ï€Î¸)

Ï€Î¸ (T ) R(T, c âˆ— 

> T

) < X  

> Tâˆˆsupp( Ï€Î¸)

Ï€Î¸ (T )(1 âˆ’ Ïµ) = 1 âˆ’ Ïµ. 

This contradicts the assumption that J(Ï€Î¸ ) â‰¥ 1 âˆ’ Ïµ. Therefore, there must exist at least one tree T âˆˆ supp( Ï€Î¸ ) such that 

R(T, c âˆ— 

> T

) â‰¥ 1 âˆ’ Ïµ. 

By the reward definition, 

R(T, c âˆ— 

> T

) = 11 + pinf c E(T, c ) â‰¥ 1 âˆ’ Ïµ =â‡’ inf  

> c

E(T, c ) â‰¤ Ïµ2

(1 âˆ’ Ïµ)2 .

This completes the proof. Theorem D.3 provides a probabilistic guarantee: any policy achieving high expected reward necessarily includes at least one near-exact symbolic solution in its support. This result holds without any assumptions on smoothness, continuity, or convexity of the PDE residual, making it broadly applicable for symbolic PDE discovery. 

Scope of the theoretical guarantees. The theoretical results in Sections D.3 and D.4 serve to characterize the identifiability and representational sufficiency of the proposed SymPlex framework for symbolic PDE discovery. Specifically, these results establish that, under exact symbolic realizability and global or near-global optimality of the induced policy, the PDE residualâ€“based objective admits no spurious symbolic optima: any globally optimal policy must assign nonzero probability to at least one expression tree representing the true solution. In this sense, the theory guarantees that the search space and reward design are well aligned with the underlying symbolic recovery problem, and that exact recovery is information-theoretically possible within the hypothesis class. At the same time, the analysis is decoupled from the practical optimization dynamics of deep neural network training. The PDE residual involves differential operators that are unbounded and may be highly nonlinear, and the resulting objective is generally nonconvex with respect to both symbolic structure T and continuous parameters c. As a consequence, small residual values do not, in general, guarantee a uniform or quantitative bound on the error âˆ¥uT (Â·; c) âˆ’ uâˆ—âˆ¥L2(Î©) outside the exact realizability regime. Establishing such stability or error bounds would require additional assumptions on the PDE operator, solution regularity, and discretization scheme, which are beyond the scope of the present work. Moreover, the theory does not claim convergence guarantees for stochastic gradient-based training, nor does it provide quantitative error bounds outside the exact realizability regime. Nevertheless, these results provide a principled foundation for the proposed approach by clarifying when exact symbolic recovery is theoretically attainable within the proposed hypothesis class. 

# E. Additional Results 

This section presents additional experimental results that further validate the robustness and generality of SymPlex across smooth, non-smooth, and parametric PDEs. 21 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

Table 6. Additional PDE benchmarks and their analytic solutions. 

Problem Name PDE Analytic Solution Smooth Problem Poisson âˆ’uxx âˆ’ uyy = f (x, y ) u(x, y ) = exp( x) + exp( y)

Advection ut + ux + uy = 0 u(x, y, t ) = sin( âˆ’1.5( x âˆ’ y âˆ’ 2t)) 

Heat ut âˆ’ uxx âˆ’ uyy = f (x, y, t ) u(x, y, t ) = cos(2 y) + 2 .5xt âˆ’ 0.5x2

Non-Smooth Problem Convex Hamilton-Jacobi ut + 12 u2 

> x

= 0 u(x, t ) = 

(

x âˆ’ t 

> 2

x â‰¤ t

> 2

0 x > t

> 2

Concave Hamilton-Jacobi ut âˆ’ 12 u2 

> x

= 0 u(x, t ) = |x| + t

> 2

Parametric Solution Advection ut + Îº(ux + uy ) = 0 u(x, y, t ; Îº) = 2 sin( x âˆ’ Îºt ) sin( y âˆ’ Îºt )

Heat ut âˆ’ Îº(uxx + uyy ) = 0 u(x, y, t ; Îº) = sin( x) cos( y)eâˆ’2Îºt 

Table 7. Comparison of PDE solvers on mean squared error (MSE) and symbolic recovery rate (SRR) for additional benchmarks.                                                                                                                                     

> Problem SymPlex SSDE FEX PINN+DSR KAN MSE ( â†“)SRR ( â†‘)MSE ( â†“)SRR ( â†‘)MSE ( â†“)SRR ( â†‘)MSE ( â†“)SRR ( â†‘)MSE ( â†“)SRR ( â†‘)Poisson 0100% 015% 3.71 Ã—10 âˆ’14 100% 0100% 4.43 Ã—10 âˆ’20% Advection 0100% 8.00 Ã—10 âˆ’10% 4.53 Ã—10 âˆ’10% 2.43 Ã—10 âˆ’20% 5.58 Ã—10 âˆ’10% Heat 0100% 2.91 Ã—10 âˆ’10% 1.14 0% 5.14 0% 2.09 Ã—10 âˆ’10% Convex HJ 0100% 9.85 Ã—10 âˆ’30% 5.29 Ã—10 âˆ’17 15% 2.78 Ã—10 âˆ’10% 3.37 Ã—10 âˆ’30% Concave HJ 0100% 1.76 Ã—10 âˆ’13 0% 1.59 Ã—10 âˆ’15 0% 5.84 Ã—10 âˆ’40% 2.85 Ã—10 âˆ’20% Parametric Advection 0100% 7.89 Ã—10 âˆ’10% 2.21 Ã—10 âˆ’10% 2.5Ã—10 âˆ’10% 3.02 Ã—10 âˆ’10% Parametric Heat 0100% 1.25 Ã—10 âˆ’10% 4.04 Ã—10 âˆ’20% 1.0Ã—10 âˆ’20% 3.06 Ã—10 âˆ’20%

E.1. Symbolic Solutions Poisson 

â€¢ True solution: u(x, y ) = x4 + 1 .2y4

â€“ SymPlex: ((y )Ë†4 * 1.2) - (-xË†4) 

â€“ SSDE: 1.0*xË†4 *abs(-1) + abs(1.20000114240109*yË†2*abs(y)Ë†2) 

â€“ FEX: (0.6752*(((0.7573*((x)Ë†4)+0.8074*((y)Ë†4)+-0.0361))+ ((0.7238*((x)Ë†4)+0.9699*((y)Ë†4)+-0.0427)))+0.0532) 

â€“ PINN+DSR: x1**4 + x2**3 

â€“ KAN: 0.02458*(-0.04722*(11.49856*x + 0.20392)**2 + 0.00017*(18.3992*y -1.02424)**2 + 0.0996)**2 + 0.05557*(8.5816e-5*(18.95968*x + 0.40416)**2 +0.024693*(12.90672*y + 1.40832)**2 - 0.42082)**2 + 0.00317 

â€¢ True solution: u(x, y ) = exp( x) + exp( y)

â€“ SymPlex: exp(y) + exp(x) 

â€“ SSDE: 1.0 * exp(x) + 1.0 * exp(y) 

â€“ FEX: (-0.7571*(((-0.6470*(exp(x))+-0.8156*(exp(y))+0.1126)) -((0.6738*(exp(x))+0.5051*(exp(y))+-0.0746)))+0.1417) 

â€“ PINN+DSR: exp(x1) + exp(x2) 

â€“ KAN: -0.0757*(0.00193*(14.96192*x + 0.58808)**2 + 0.00238*(15.66704*y +0.39208)**2 - 5.08753)**2 - 26.05107*log(0.64468*log(1.868 - 0.936*y) + 0.64242*log(12.83984 - 6.82176*x) + 7.53739) + 62.843 

Advection 

22 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

â€¢ True solution: u(x, y, t ) = exp( âˆ’(( x âˆ’ t)2 + ( y âˆ’ t)2)/0.5) 

â€“ SymPlex: exp(-(2.0*((x - t)Ë†2)))*exp(((t + (-y))*(-2.0*(t + (-y))))) 

â€“ SSDE: exp(-4*tË†2 + t - t*exp(-2*xË†2) - 2*xË†2) 

â€“ FEX: (0.3507*sin((((-3.2797*(t)+-0.2790*(x)+0.9215*(y)+0.2614)) -((0.6268*(exp(t))+-1.3011*(exp(x))+-0.5694*(exp(y))+-0.2636))))+0.4102) 

â€“ PINN+DSR: 0.24384981189819324*exp(-1.2714642227548834*sin(x1*t + x1 + x2 -t - 1.355995834300186*exp(t))) )

â€“ KAN: 0.03160*(3.15352*sin(2.92*t - 3.04648) + 1.22909*sin(2.94400*x - 3.06072) + 1.22897*sin(2.944*y - 3.06072) + 0.13287)*2 + 0.09498(1.77166*sin(2.9456*t + 7.93832) - 0.69882*sin(2.96672*x - 1.49456) - 0.69868*sin(2.96656*y - 1.49448) + 0.00878)**2 + 0.09405*sin(-190.95396*sin(0.03072*t + 9.20464) + 9.80012*sin(0.29296*x - 9.53936) + 9.79645*sin(0.29312*y - 9.53888) + 41.01414) - 0.05497 

â€¢ True solution: u(x, y, t ) = sin( âˆ’1.5( x âˆ’ y âˆ’ 2t)) 

â€“ SymPlex: sin(-(1.49*(((x-2.0*t)-1.5*y))) 

â€“ SSDE: -sin(3*t + cos(1.493825166410878*x - 1.493825166410878 *exp(0.47106554937118056*sin(1.5016945847398988*y)))/cos(1)) 

â€“ FEX: (0.4659*cos((((0.3394*(cos(t))+0.2019*(cos(x))+0.1051*(cos(y))+-0.4466)) ((4.3637*((t)**3)+-0.0688*((x)**3)+0.0582*((y)**3)+-2.1432))))+-0.2713) 

â€“ PINN+DSR: cos(-2*x1 + x2 + 3.571486073506636*t + 4.571486073506636 log(0.13426961654716008*x1 + 0.13426961654716008*x2 - 0.13426961654716008*t + 0.59328704350404904) 

â€“ KAN: -0.233*(0.0004*(19.47968*t - 1.2085)**2 - 0.5835*sin(1.4559*x - 12.9665) + 1.1314*sin(1.4514*y - 8.3577) - 0.025)**2 -1.6098*sin(0.1695*sin(1.344*t + 3.273) - 0.3104*sin(1.5279*x - 14.8) + 0.6178*sin(1.4258*y + 2.7329) - 1.536) - 0.4736*sin(0.48*sin(2.1766*t + 5.4754) + 13.683*sin(0.1224*x - 3.1829) + 7.9633*sin(0.2095*y - 1.0298) -1.4289) - 1.2431 

Heat 

â€¢ True solution: u(x, y, t ) = sin( x) cos( y) exp( âˆ’2t)

â€“ SymPlex: sin(x) * (exp((-2.0*(k * t))) * (0.99 * cos(y))) 

â€“ SSDE: 0

â€“ FEX: (-0.2037*(-(((-0.1043*(t)+-0.4521*(x)+0.0168*(y)+1.4702)) ((0.0255*(cos(t))+-0.0466*(cos(x))+0.8119*(cos(y))+0.0158))))+0.0007) 

â€“ PINN+DSR: exp(-t)*sin(x1)*cos(x2) 

â€“ KAN: -3.6052*exp(-0.0837*sin(1.8127*x - 7.2926) - 0.0508*sin(1.946*y -4.5382) - 1.7669*cos(0.8083*t - 1.59)) - 2.3564*sin(0.003*(17.4019*t -12.8574)**2 + 0.591*sin(0.9649*x + 6.3775) + 0.5314*cos(0.8948*y - 2.8096) - 9.129) - 2.3379*cos(0.9595*log(7.0464*t + 3.9153) + 0.4821*sin(0.9362*y -1.3687) + 0.5542*cos(0.9647*x - 1.4504) - 3.7186) + 0.6401 

â€¢ True solution: u(x, y, t ) = cos(2 y) + 2 .5xt âˆ’ 0.5x2

â€“ SymPlex: (0.5*(x*(-(x) + (5.0*t)))) + cos((2.0*y)) 

â€“ SSDE: -0.02555*t*xË†2 + t*sin(2*y + 1.5725536) + t - 0.02555*xË†2 + sin(2*y +1.5725536) 

â€“ FEX: (-0.9307*((((1.9975*(sin(t))+0.0184*(sin(x))+-0.0045*(sin(y))+0.2556)) -((-3.8508*(-(t))+0.6605*(-(x))+-0.0006*(-(y))+-0.2578))))Ë†2+0.8235) 

23 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

â€“ PINN+DSR: -2*x1 + t*(2*x1 - cos(x1)) 

â€“ KAN: 0.1721*(0.005*log(8.9166 - 1.3873*x) + 2.0578*sin(1.0079*y + 1.4314) -0.0037*cos(6.5794*t + 4.5099) - 0.2353)**2 - 0.4106*(84.4225*log(0.1299*x + 9.3657) + 10.2754*sin(0.2658*t - 3.3628) - 0.0019*cos(1.0499*y -6.078) - 191.2278)**2 + 14.4212*cos(0.0007*(17.2938*t + 0.5879)**2 + 6.97e-5*(2.2204*x - 0.4122)**2 + 0.0412*cos(2.0621*y - 6.2932) - 8.12) +3.3748 

Hamilton-Jacobi 

â€¢ True solution: u(x, y, t ) = 

(p x2 + y2 âˆ’ t, px2 + y2 â‰¥ t

0, px2 + y2 < t 

â€“ SymPlex: max[0,(sqrt((((1.0*x)*x) + (y*y))) - ((t))] 

â€“ SSDE: -t*abs(-xË†2)Ë†0.5 + t + abs(-xË†2)Ë†0.5 

â€“ FEX: (0.6810*abs(max(((-0.0003*(sin(t))+-0.0547*(sin(x))+0.0017*(sin(y))+-0.0054)), ((-1.0607*(abs(t))+0.8034*(abs(x))+0.8259*(abs(y))+0.3004))))+-0.0039) 

â€“ PINN+DSR: (x2 - cos(1.422708857249582*x1) + 0.9382428805260224)* exp(-1.0529386972249486*t) 

â€“ KAN: 0.0146*(1.6309*log(12.8874 - 9.7645*x) - 3.5841*sin(1.792*t + 2.2865) + 27.996*sin(0.4789*y + 1.5966) - 31.7366)**2 + 0.0028*exp(0.0021*(18.6*y -8.288)**2 + 2.8372*sin(1.6814*t + 1.5954) - 1.4728*sin(3.719*x - 5.26)) + 0.1995*sin(1.7957*exp(0.9773*t) + 1.7014*log(11.2713 - 7.9086*x) + 3.34736*sin(1.0034*y + 1.7932) - 5.5221) + 0.2447 

â€¢ True solution: u(x, y, t ) = |x| + |y| + t

â€“ SymPlex: (abs(y) + ((-0.0 + abs(x)) - (-0.2436 * (t / 0.2436)))) 

â€“ SSDE: max(1.0 - cos(t), t + max(1.0 - cos(x), x + max(-y + abs(y*cos(y*(y -abs(y)))), y))) 

â€“ FEX: (-0.6246*(-(((0.7982*(abs(t))+0.7998*(abs(x))+0.8005*(abs(y))+-0.1432)) -((-0.8029*(abs((t))+-0.8013*(abs(x))+-0.8006*(abs(y))+0.1407))))+0.1773) 

â€“ PINN+DSR: -x1*sin(0.2336406634604177*x1 - 1.7473279933305932) +1.0023923461698605*x2 + t

â€“ KAN: 1.5615*exp(-0.0591*sin(3.344*t - 6.271) + 0.7764*sin(0.7765*x +5.6002) - 1.034*sin(0.5987*y + 2.4682)) + 1.0189*sin(0.654*sqrt(9.5133*t + 4.9446) - 0.0936*sin(1.4299*x + 2.2603) + 0.0671*sin(2.0483*y - 7.4278) -1.8406) - 0.5665*sin(0.0009*(15.18*t - 8.7903)**2 + 0.8967*sqrt(6.0986*x +5.8906) + 0.8265*sqrt(6.9141*y + 6.314) - 7.7371) + 0.1685 

â€¢ True solution: u(x, t ) = 

(

x âˆ’ t 

> 2

x â‰¤ t

> 2

0 x > t

> 2

â€“ SymPlex: max[0,(((t + t) + x) + (-1.5 * t))] 

â€“ SSDE: t*(1 - max(cos(1.0*max(max(1.0, t), 1.0*t)), max(-x + abs(1.0*abs(x)), 1.00000001))) 

â€“ FEX: (0.8446*(-max(((-0.5920*(-(t))+1.1840*(-(x))+-0.0674)), ((0.0000*((t)Ë†2)+-0.0000*((x)Ë†2)+-0.0674))))+-0.0569) 

â€“ PINN+DSR: x1 + 0.4973913295197155*t - 0.00585512341908971 +0.003366022247412346*cos(x1**2)/(0.05801743054817531*x1 - 1)**2 

â€“ KAN: 0.0367*(3.6919*sin(1.5374*x - 7.0127) - 1.7691*cos(1.488*t -7.7062) + 4.0454)**2 - 0.0241*exp(-0.3833*sin(4.6902*t + 5.2557) + 3.1955*cos(1.7174*x - 2.214)) - 0.016 

24 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

â€¢ u(x, t ) = |x| + t

> 2

â€“ SymPlex: abs((0.5 * t)) + abs(x) 

â€“ SSDE: 0.5*t + max(x + 0.49999958, xË†2*(0.90578896 - x)/(xË†3 +cos((1.1099373*x - 0.075162254)/x))) - 0.5 

â€“ FEX: (0.5909*(-(((-0.4223*(t)+-0.8425*(x)+0.1570))+((-0.4238*(abs(t)) +-0.8498*(abs(x))+0.1427))))+0.1771) 

â€“ PINN+DSR: 0.3685911419651037*exp(sin(t)) + 0.3685911419651037 log(4.698437728826789*x1**2 + 0.41390626642149014) 

â€“ KAN: 1.5535*sin(0.28*sin(3.3566*x - 1.5729) + 46.5592 -40.6184*exp(-0.008*t)) - 0.0652*cos(0.0005*(8.5555 - 15.0792*t)**2 -1.9124*cos(4.8933*x - 0.0014) + 3.4232) + 0.9716 

Parametric Advection 

â€¢ True solution: u(x, y, t ; Îº) = max {1 âˆ’ | x âˆ’ Îºt | âˆ’ | y âˆ’ Îºt |, 0}

â€“ SymPlex: max[(1-(abs(((k * t) - y))+(1.0*abs((x - (abs(t)*k))))))] 

â€“ SSDE: -(1 + 1/cos(exp(exp(tË†2 - t))))*log(cos(2.49999998480632e-9*x)) 

â€“ FEX: (-0.2690*sin((((-0.3752*(cos(t))+-0.1229*(cos(x))+-0.1252*(cos(y)) +-0.2002*(cos(k))+-0.2516))+((-1.4345*(exp(t))+1.2719*(exp(x))+1.3023*(exp(y)) +-0.7243*(exp(k))+-0.2560))))+0.2257) 

â€“ PINN+DSR: sin(k/(x1**2 + x2*k/t + k)) 

â€“ KAN: 0.0359*(0.0035*(5.5666 - 15.1003*t)**2 - 0.5089*sin(2.0533*k - 5.0106) + 1.9593*sin(2.1323*y + 5.1286) - 1.9572*cos(2.1382*x - 5.8693) - 1.3651)**2 + 0.0081*(-0.0112*(13.2166 - 19.913*k)**2 - 0.0063*(18.004 - 19.616*t)**2 - 1.0794*cos(2.816*x + 2.0031) -1.093*cos(2.7966*y + 2.0105) + 2.1919)**3 + 0.0281*(37.0255*sqrt(0.8082*t + 7.9752) + 2.9374*cos(2.7307*k - 2.7356) - 1.2098*cos(1.8331*x - 3.1545) - 1.2004*cos(1.8374*y - 3.1542) - 105.5795)**2 -5.7881e-6*exp(0.5272*sin(2.2357*y - 4.3275) + 1.4967*cos(2.4549*k -2.2367) - 11.8356*cos(0.381*t + 2.4254) + 0.5254*cos(2.2469*x + 6.6636)) - 0.0287 

â€¢ True solution: u(x, y, t ; Îº) = 2 sin( x âˆ’ Îºt ) sin( y âˆ’ Îºt )

â€“ SymPlex: 2.0*(sin(((k * t) - x))*sin(((k * t) - (-0.0 + y)))) 

â€“ SSDE: (-t + 0.04676*(t*x + t*cos(x) + x*exp(sin(exp(exp(exp(-exp(t)))))) +x + exp(sin(exp(exp(exp(-exp(t))))))*cos(x) + cos(x))* exp(2.812062879082025*cos(0.233057645364174*y)) - exp(sin(exp(exp(exp(-exp(t)))))) - 1) *exp(-2.812062879082025*cos(0.233057645364174*y))*sin(x) 

â€“ FEX: (-0.9510*(-(((0.2382*(sin(t))+-1.4255*(sin(x))+-0.0039*(sin(y)) +0.0583*(sin(k))+-0.0063))*((0.2376*(sin(t))+0.0045*(sin(x))+-1.4162*(sin(y)) +0.0632*(sin(k))+-0.0249))))+-0.0051) 

â€“ PINN+DSR: exp(-t*log(t))*sin(x1)*sin(x2) 

â€“ KAN: -0.9746*sin(-0.001*exp(3.7939*k) - 36.0416*log(0.2584*x + 8.4656) +44.753*log(0.0661*y + 2.733) + 0.0048*cos(4.0712*t + 5.1798) + 24.1655) +0.0736*sin(-2.3913*sin(2.6296*k - 3.8908) + 4.7596*sin(1.112*t - 2.3346) +2.7388*sin(0.4017*x + 7.9546) + 1.7815*sin(1.1002*y + 1.5346) + 4.4654) +0.1914*sin(1.1142*sin(4.1014*k + 1.3522) + 4.7857*sin(0.2152*x + 2.3378) + 3.7566*sin(0.2915*y + 2.293) + 0.6573*cos(1.844*t - 5.7138) - 9.0414) -0.8441*cos(1.2081*sin(0.8658*t - 9.4502) + 7.5212*sin(0.1352*x + 5.7821) - 0.3877*cos(2.648*k - 2.4958) + 8.7439*cos(0.1146*y - 1.9749) - 5.2825) +0.0042 

25 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving 

Parametric Heat 

â€¢ True solution: u(x, y, t ; Îº) = exp( âˆ’x) exp( âˆ’y) exp(2 Îºt )

â€“ SymPlex: exp((-((k*(-t)))-(x)))* (exp(-(y)) * exp(((1.0*k) * t))) 

â€“ SSDE: exp(-t*exp(-x - y) - x - y) 

â€“ FEX: (0.4461*exp((((-0.0074*((t)Ë†2)+-0.4391*((x)Ë†2)+-0.4455*((y)Ë†2)+-0.1241*((k)Ë†2)+0.4223))-((0.5872*(sin(t))+0.4642*(sin(x))+0.4686*(sin(y)) +0.2501*(sin(k))+-0.4364))))+-0.0702) 

â€“ PINN+DSR: exp(-x2 + k*(t - sin(x1))*exp(k)) 

â€“ KAN: 1.091e-12*(9.3426 - 1.1838*y)**6.0009*(9.5974 - 1.1842*x)**5.9934 exp(-0.0016*(4.9682 - 19.5366*t)**2 + 0.3216*cos(2.68*k -1.1166)) + 5.5791e-49*(7.5669*k + 5.9751)**3.7466*(6.9658*t + 3.2789)**2.8071*exp(64.2979*exp(-0.016*y) + 32.4132*exp(-0.032*x)) -0.1706*cos(119.1116*exp(0.008*y) - 0.1836*log(14.4347 - 10.975*t) + 0.1195*log(14.901*k + 1.1798) + 3.3667*log(1.9598*x + 6.2394) - 132.9443) + 0.1175*cos(-0.4637*log(2.4778*t + 0.417) + 0.13*sin(2.3288*y - 7.5748) -0.1632*cos(2.8358*k + 3.7684) + 0.1842*cos(2.2986*x - 8.9304) + 1.0616) +0.1117 

â€¢ True solution: u(x, y, t ; Îº) = sin( x) cos( y)eâˆ’2Îºt 

â€“ SymPlex: (-1.0*exp((k*t)))*(sin((-x))*cos(y)) 

â€“ SSDE: -2.629e-7 - 1.0*exp(-21.239998988740282*cos(0.0005091568214335449*y)) sin(log(exp(x))) 

â€“ FEX: (0.6203*sin((((-0.0033*(cos(t))+-0.0144*(cos(x))+0.1902*(cos(y)) +0.0015*(cos(k))+0.0093))*((-1.1294*((t)Ë†3)+-0.0909*((x)Ë†3)+-0.0003*((y)Ë†3) +-0.4277*((k)Ë†3)+3.9474))))+0.0039) 

â€“ PINN+DSR: sin(x1)*cos(x2)*cos(t+k) 

â€“ KAN: 0.6537*sin(0.4163*log(15.3317*t + 1.4123) + 0.833*sin(0.878*y -1.1872) + 0.5411*cos(0.9977*x - 4.7147) - 7.5457 - 1.143*exp(-2.5046*k)) - 1.0089*cos(0.3796*log(14.3003*t + 1.2277) - 0.8264*sin(1.0267*y -1.6535) + 0.355*cos(0.9995*x - 1.5578) + 0.4552 - 1.071*exp(-2.4462*k)) -0.6581*cos(-0.4131*log(14.872*t + 1.364) + 0.5304*sin(1.0003*x - 3.1342) - 0.8351*sin(0.8725*y - 1.1702) + 9.1139 + 1.1386*exp(-2.5285*k)) + 1.0283*cos(-0.3737*log(16.4254*t + 1.3994) + 0.3582*sin(0.9968*x + 6.2803) + 0.8163*sin(1.0295*y - 1.6621) + 5.8859 + 1.0541*exp(-2.4509*k)) - 0.024 

E.2. Training Dynamics Across Curriculum Stages 

To verify both the stability of the training process and the effectiveness of the proposed curriculum learning strategy, we visualize the reward evolution over training steps. Figure 3 shows the results for the two parametric PDE examples listed in Table 2. The figure compares the training dynamics of a single-stage approach, which attempts to solve the full problem at once without curriculum learning, and the proposed curriculum learning strategy. For curriculum learning, a single SymFormer model is trained sequentially through Stages 1, 2, and 3, and we plot the reward as a function of epochs across all stages. Note that the reward definition differs across stages, reflecting the increasing task complexity. In Stage 1, a reward of 1 indicates that the model has correctly learned the spatial representation of the initial condition. In Stage 2, a reward of 1 corresponds to accurately solving the PDE with the parameter Îº fixed to 1. Finally, in Stage 3, a reward of 1 signifies that the model has successfully learned the full parametric PDE solution, including the dependency on Îº, which matches the reward definition used in the single-stage setting. From the results, we observe that the reward consistently improves within each stage, without severe saturation at specific expressions or noticeable training instability. In comparison to training without curriculum learning, these results indicate that expressions learned in earlier stages effectively serve as useful priors for subsequent, more complex settings. Compared to 26 SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving    

> (a) Parametric Advection
> (b) Parametric Heat
> Figure 3. Reward evolution of SymPlex with and without curriculum learning for two parametric PDEs in Table 2. The single-stage setting learns the full problem at once without curriculum learning, while curriculum learning trains a single SymFormer sequentially through Stages 1, 2, and 3. The results show stable optimization across stages under the proposed curriculum.

the single-stage approach, which struggles to effectively identify solutions due to the significantly enlarged symbolic search space, the proposed curriculum strategy empirically demonstrates its effectiveness in mitigating search space complexity and facilitating more efficient optimization. Overall, this empirical evidence supports the stability of our reinforcement learning training scheme and demonstrates the effectiveness of the proposed curriculum in handling increasingly large symbolic search spaces without training collapse. 27