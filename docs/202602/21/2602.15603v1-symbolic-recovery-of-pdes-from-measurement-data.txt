Title: Symbolic recovery of PDEs from measurement data

URL Source: https://arxiv.org/pdf/2602.15603v1

Published Time: Wed, 18 Feb 2026 01:49:00 GMT

Number of Pages: 41

Markdown Content:
# Symbolic Recovery of PDEs from Measurement Data 

## Erion Morina ∗ Philipp Scholl † Martin Holler ∗

## February 18, 2026 

Abstract 

Models based on partial differential equations (PDEs) are powerful for describing a wide range of complex relationships in the natural sciences. Accurately identifying the PDE model, which represents the underlying physical law, is essential for a proper understanding of the problem. This reconstruction typically relies on indirect and noisy measurements of the system’s state and, without specifically tailored methods, rarely yields sym-bolic expressions, thereby hindering interpretability. In this work, we ad-dress this issue by considering existing neural network architectures based on rational functions for the symbolic representation of physical laws. These networks leverage the approximation power of rational functions while also benefiting from their flexibility in representing arithmetic operations. Our main contribution is an identifiability result, showing that, in the limit of noiseless, complete measurements, such symbolic networks can uniquely re-construct the simplest physical law within the PDE model. Specifically, reconstructed laws remain expressible within the symbolic network archi-tecture, with regularization-minimizing parameterizations promoting inter-pretability and sparsity in case of L1-regularization. In addition, we provide regularity results for symbolic networks. Empirical validation using the Par-Fam architecture supports these theoretical findings, providing evidence for the practical reconstructibility of physical laws. 

Keywords: Physical law, model learning, symbolic recovery, neural networks, rational functions, inverse problems 

MSC Codes: 35R30, 93B30, 68Q32, 65M32, 41A20 

> ∗

IDea Lab - The Interdisciplinary Digital Lab at the University of Graz, Austria. 

{erion.morina@uni-graz.at, martin.holler@uni-graz.at }. MH is also a member of NAWI Graz (www.nawigraz.at) and BioTechMed Graz (biotechmedgraz.at). 

> †

Aleph Alpha Research, Germany. (philipp.scholl@aleph-alpha-research.com). 

1

> arXiv:2602.15603v1 [cs.LG] 17 Feb 2026

# 1 Introduction 

Many complex relationships in the natural sciences are governed by systems that evolve in space and time, driven by rates of change. The underlying physical laws can be effectively modeled using partial differential equations (PDEs), which pro-vide a powerful mathematical framework for describing such effects while enabling the analysis and prediction of system behavior. Examples include modeling fluid dynamics with the Navier-Stokes equations [9], behavior of quantum systems with Schr¨ odinger’s equation [37], biological pattern formation through reaction-diffusion equations [93], climate dynamics and weather prediction [95], and biological pro-cesses such as population dynamics or transport mechanisms [68, 69, 73]. Despite their versatility, a central challenge in PDE-based modeling lies in identifying the precise form of the governing equations, which is often unclear or imprecise. This uncertainty stems from multiple factors, such as insufficient understanding of the underlying mechanisms or oversimplifications during the abstraction process that prevent accurate representation of reality. Moreover, the data available in practice are often indirect, incomplete, and noisy, further complicating the task. 

Inverse problems perspective on model learning. The reconstruction of physical laws in PDE-based models from measurement data can be formulated as an inverse problem. Applications range from estimating single parameters, through identifying structural components that augment approximate models, to discover-ing the full governing physics that best explain observed dynamics. Approaches span classical techniques, purely data-driven model learning, and hybrid methods. In parameter identification the physical model structure is assumed to be known and only the physical parameters are learned from data. Early contributions in-clude [2], [4], and [61] on elliptic equations. Related works on reconstructing non-linear heat conduction laws from boundary measurements, together with stability estimates, include [19], [32], and [78]. An excellent overview on aspects of param-eter identification and beyond is provided by the foundational reference works [8] on parameter estimation and control for PDE-governed systems, [60] on different aspects of coefficient inverse problems, [33] on deterministic regularization theory for ill-posedness, and [49] on a Bayesian framework for inverse problems. Beyond coefficient estimation, model identification infers unknown structural com-ponents of PDEs, e.g., full functional terms, from data, including recovery of sources in reaction-diffusion systems [31, 52, 55], joint reconstruction of conduc-tivity and reaction kinetics [53], determination of nonlinearities in semilinear equa-tions [35, 57, 58], inverse source problems for hyperbolic dynamics [97] and hidden reaction law discovery [72]. See also the related works [18, 48, 70]. Efficiency is a central consideration, as measurement data are often high dimen-sional. For that reason, dimensionality reduction is a natural preprocessing step 2prior to inferring physical laws. Representative approaches include projection-based model reduction for parametric PDE systems [11] and data-driven dynamic mode decomposition (DMD), which extracts dominant spatio-temporal modes from measurements [82]. Another central consideration is uncertainty. The frame-work [90] for Bayesian inverse problems on function spaces enables probabilistic inference with principled uncertainty quantification, facilitating the identification of governing parameters and laws from partial, noisy data. In recent years, scientific machine learning has shown strong potential by linking data-driven methods with classical models. For an overview of learning PDE-based models from data, see the comprehensive reviews [7, 15, 16, 26, 71]. A selection of methods include DeepONets [65], Fourier Neural Operators [64], DeepGreen [36] and model reduction with neural operator methods [12]. Hybrid methods have also proven effective in optimal control, including learning-informed opti-mal control [28, 29, 30], nonlinearity identification in the monodomain model [24] and identification of semilinear PDEs [21]. In the setting of learning-informed PDE-constrained optimization, we refer to [77, 89]. These contributions formulate and analyze PDE-constrained optimization problems augmented with neural net-works, provide adjoint-based optimization frameworks, and establish global con-vergence guarantees for neural PDEs and neural-network-enhanced PDE models. On learning-informed identification of PDE nonlinearities, we refer further to [1] for well-posedness of the learning problem and [42] for uniqueness of reconstructing the physical law, along with the references therein. These contributions employ an all-at-once formulation, justified in [50, 51], that facilitates the direct use of measurements and avoids explicit parameter-to-state maps. A growing body of re-cent research investigates how embedding problem-specific conservation laws into machine-learning frameworks can aid in discovering governing physical laws. We refer to the related works discussed in [44] for a recent overview. 

Learning interpretable models from data. A key objective in learning sci-entific models from data is to obtain representations that are not only accurate but also interpretable, compact, and aligned with physical principles. In many ap-plications, one aims to identify physical laws that are as simple as possible while still adequately capturing the data (see, e.g., [5, 76]). Beyond interpretability, symbolic models are advantageous over black-box alternatives for capturing fun-damental physical principles and are more robust in terms of generalization [80]. Interpretable models also offer reduced computational complexity for real-time applications [83]. Finally, symbolic expressions offer the additional advantage of being easier to verify for physical consistency, while analytical properties (such as stability and equilibria) are easier to extract [17]. For background of vari-ous methods on symbolic regression, see [62]. This includes genetic programming 3approaches [6, 25, 83, 84], reinforcement learning formulations [67, 74, 91], neural-network based methods [39, 63, 81, 87], and transformer-driven expression models [13, 45, 56]. Within symbolic recovery of dynamical systems, substantial progress has been made on identifying parsimonious laws for nonlinear dynamics. The SINDy framework [17] introduced sparse regression to discover governing ODEs, and PDEs in [80]. Physical structure can be promoted through priors and con-straints, including symmetry-based inductive biases [94] and the incorporation of established scientific laws [23], improving physical consistency, and data efficiency. 

Identifiability of physical laws. In physical law learning, fundamentally an identification problem, it is crucial that the learned equations and parameters are uniquely determined by the data, ensuring the model captures the true un-derlying mechanism rather than one of many indistinguishable alternatives. The works [10, 22, 27, 66] establish the theoretical foundations of structural and pa-rameter identifiability in dynamical systems. They rigorously define conditions for unique parameter inference from input-output data and analyze ambiguities that arise from observability and model structure. See for example [46] investi-gating conditions under which inverse problems for semilinear parabolic equations admit unique solutions and [54] on identifiability and constructive recovery of a nonlinear diffusion coefficient in parabolic equations. In the context of symbolic recovery of differential equations, identifiability, specifically, the classification of uniqueness, has been addressed by [38, 85, 86] both for specific classes of physical laws (e.g., linear, algebraic) and for robust symbolic recovery of differential equa-tions. More recently, [20] study the structural identifiability of sparse linear ODEs and show that, unlike in the dense case, non-identifiability occurs with positive probability under realistic sparsity assumptions. An interesting recent result by [88] shows that a system’s governing equations are, in general, discoverable from data only when the system exhibits chaotic behavior. Finally, we also refer to [42], addressed above, which discusses an analysis-based guideline for expecting unique reconstructability of practical physical law learning setups in the limit of full measurements in a variational formulation. 

Scope and contributions of this work. Our focus is the joint recovery of an unknown physical law f in a concise symbolic form and state u satisfying the PDE 

∂tu(t, x ) = f (t, u (t, x )) , (t, x ) ∈ (0 , T ) × Ω,

on a given spatio-temporal domain, from indirect, noisy and possibly incomplete measurements of the state. The aim is not merely to fit the physical law f numer-ically, but to recover it as a human-interpretable expression that is verifiable by domain experts. To achieve this, we approximate f by a family of parameterized 4models designed for symbolic expressiveness and interpretability. Concretely, we employ symbolic networks , a neural-network based architecture whose trainable components are rational functions (fractions of polynomials with trainable coef-ficients) and whose activations are user-defined base functions (such as trigono-metric or exponential functions). Architectures commonly used for neural-network based symbolic regression, including EQL -type designs [63, 81] and ParFam [87], inspired our choice of symbolic neural networks and are subsumed as special cases within our framework. The design outlined above is crucial since rational functions naturally encode arithmetic operations, such as products and divisions, while the base functions provide the vocabulary for non-algebraic relationships. Together, they yield a modeling class that can represent compact formulas and is supported by strong approximation capabilities grounded in those of rational functions (see [14, 43, 92]). We integrate symbolic networks into the model learning framework proposed in [42], that recovers the state and the physical law simultaneously from data. This allows to incorporate measurement operators and noise models directly into the learning scheme. Within this formulation, sparsity-promoting regulariza-tion on the model parameters is used to enhance interpretability of the resulting physical law. As a result, among all admissible models consistent with the data, a concise symbolic law within the chosen architecture is preferentially selected. Our main theoretical contribution is an identifiability result tailored to this sym-bolic setting. This result shows that, for ground truth physical laws expressible within a user-defined symbolic network architecture, the proposed model learning approach recovers both the physical law and the state in the limit of complete measurements. Furthermore, the recovered parameterization of the physical law is regularization minimizing, which is particularly interesting if the regularization scheme is designed to promote sparsity in the parameterization. When the phys-ical law is not representable by a fixed architecture, techniques developed in [42] can be employed, but fall outside the scope of this work. Beyond theory, we pro-vide a numerical validation using ParFam, demonstrating the practical feasibility of reconstructing interpretable laws from indirect data. All numerical experiments of this work can be reproduced with the publicly available source code 1.What distinguishes this work from existing literature on neural-network based symbolic regression is a rigorous analysis-driven framework for identifying human-interpretable physical laws from realistic measurement data, together with a for-mulation that preserves the continuous nature of the underlying PDE. Rather than assuming idealized, full, noise-free observations, we model measurements as they arise in practice. In this regime, we prove reconstruction of both the state and an admissible symbolic representation of the physical law within the chosen architecture. A central methodological choice is to model the dynamics directly  

> 1See https://anonymous.4open.science/r/unique_sym_anon-3CDD/README.md .

5in function space prior to discretization. This function-space formulation retains the continuous structure of the PDE and enables consistency guarantees before numerical approximation. Within this setting, we introduce symbolic networks for compact, interpretable representations of physical laws. We analyze these net-works as operators on function spaces, establishing regularity properties required for the learning framework. These elements provide a foundation for symbolic recovery of physical laws from measurement data, combining expressive modeling with identifiability and convergence guarantees in the limit of vanishing noise. 

Structure of the paper. Section 2 introduces symbolic networks based on ra-tional functions, as detailed in Subsection 2.1. The network design is outlined in Subsection 2.2, and approximation properties are discussed in Subsection 2.3. Subsection 2.4 discusses the extension to function space, including regularity prop-erties of symbolic networks. The main identification results for symbolic model learning are presented in Section 3, with numerical experiments provided in Section 4. The detailed proofs of the regularity result from Subsection 2.4 can be found in Appendix A, and those for Section 3 in Appendix B. Appendix C verifies the regularity properties of sampling operators utilized in the numerical experiments. 

# 2 Symbolic networks 

Neural network-based symbolic regression provides an alternative to traditional black-box models by aiming to discover human-interpretable equations that de-scribe data (e.g., originating from physical systems). Unlike genetic programming approaches, which rely on evolutionary algorithms to combine mathematical sym-bols, these methods exploit the potential of optimization techniques for neural networks (e.g., gradient-based methods). In addition, neural network-based ap-proaches enable the construction of concise symbolic expressions and, when prop-erly trained, generalize well to unseen data. We focus on symbolic networks , a generic class of architectures which we motivate by the parameterized feed-forward neural networks proposed in [63, 81, 87] in the context of symbolic regression. These networks utilize user-defined base functions (e.g., trigonometric, exponen-tial) as activation functions, which may vary across different layers. Transforma-tions between layers are parameterized functions which are ”simpler” than the activations and can range from affine linear transformations (as used in [63, 81]) to polynomial and rational transformations (see [87]). The symbolic networks in-vestigated here specifically employ certain rational functions as transformations between layers, i.e., quotients of polynomials, encompassing polynomial and lin-ear functions as special cases. Rational transformations can represent products and divisions, which are essential arithmetic building blocks for symbolic expres-6sions. Additionally, they exhibit strong approximation capabilities, which will be discussed in more detail later. 

## 2.1 Rational functions 

From both a theoretical and practical perspective, it is advantageous to restrict transformations between layers to rational functions that do not exhibit poles. Rational functions with poles (e.g., outside the domain of the PDE model) are beyond the scope of this work, as the focus here is to enable a more universal and comprehensive analysis. This restriction guarantees essential regularity properties of symbolic networks, as proven in Subsection 2.4, while also simplifying the train-ing process in practice. Furthermore, as we will discuss in Subsection 2.3, this limitation does not affect the approximation capabilities of rational functions. Before introducing rational functions in the context described above, we first recall some definitions related to multivariate polynomials. For m, n ∈ N the degree of a multivariate polynomial π : Rn → R of the form 

Rn ∋ x 7 →

> m

X

> k1,...,k n=0

ak1,...,k n

> n

Y

> l=1

xkl

> l

is defined by deg( π) := max( k1 + · · · + kn : ak1,...,k n̸ = 0). Note that a polynomial of degree d in n variables involves at most  n+dd

 = (n+d)!  

> d!n!

non-zero coefficients. Additionally, note that two multivariate polynomials p, q are coprime if they do not attain a nontrivial common polynomial divisor. Building on these definitions, we can now introduce rational functions and define an associated notion of degree. 

Definition 1 (Rational and base function) . For n ∈ N, a function r : Rn → R is called rational if there exist coprime multivariate polynomials p, q : Rn → R with 

q(x) > 0 for all x ∈ Rn, such that r(x) = p(x) 

> q(x)

for x ∈ Rn. The degree of r is given by deg( r) := max(deg( p), deg( q)) if p̸ = 0 and zero otherwise. For n, m ∈ N afunction r : Rn → Rm with r(x) = ( ri(x)) mi=1 for x ∈ Rn is called rational if ri is rational for 1 ≤ i ≤ m and the degree is given by deg( r) := max 1≤i≤m(deg( ri)) .A continuous function σ : Rn → Rm is called base function if it is not rational. 

To simplify the parameterization of denominator polynomials in the representa-tion of rational functions (Definition 1), we introduce the set of coefficients for polynomials of a given degree that are positive over the real numbers. 

Definition 2 (Positive polynomials) . For d, n ∈ N let Q(d, n ) ⊂ R(n+dd ) be the set of coefficients parameterizing positive polynomials of degree ≤ d in n variables, 

Q(d, n ) = {a ∈ R(n+dd ) : q(a, x ) := X

> 0≤k1+··· +kn≤d

ak1,...,k n

> n

Y

> l=1

xkl 

> l

> 0 for all x ∈ Rn}.

7To characterize the set of coefficients Q(d, n ) more explicitly, it is crucial to exam-ine the geometric properties of polynomial roots, which form a rich area of research. However, as this is not the focus of the present work, we limit our discussion to classical one-dimensional results presented in [47, Chapter 5], specifically Sturm’s theorem [47, Theorem 5.5] and Tarski’s theorem [47, Theorem 5.9]. By Sturm’s theorem, the number of roots of a polynomial within a given interval is determined by the difference in the number of sign changes in the standard sequence evaluated at the endpoints of the interval. For n = 1, this establishes an implicit charac-terization of Q(d, n ) insofar as the number of variations in sign of the standard sequence remains constant for every real number. While an explicit characteriza-tion of Q(d, n ) is likely infeasible, it is typically sufficient to work with the following subset: Denominator polynomials consisting solely of monomial terms with even degrees, nonnegative coefficients, and a positive constant term. Such polynomials are positive over the real line and hold practical significance, as discussed next. In [14], it is demonstrated that suitably regular functions can be uniformly approx-imated by rational functions on compact sets. The rational functions employed are Zolotarev sign-type functions, whose denominators, as noted previously, ex-clusively consist of monomials with even degrees. Similarly, this applies to the 

Newman sign-type functions used in [43], which establish a first-order universal approximation result with rational functions. From a practical perspective, this eliminates the need for explicitly characterizing Q(d, n ), allowing one to instead focus on working with this proper subset without losing approximation properties. 

## 2.2 Network design 

The symbolic networks introduced in this work are inspired by the feed-forward neural networks proposed in [63, 81, 87]. They extend the (shallow) ParFam architecture presented in [87] by incorporating multiple layers, and generalize the EQL architectures in [63, 81] by utilizing rational functions, according to Definition 1, as transformations between layers. The proposed symbolic network architecture is formally defined as follows. 

Architecture. The depth of the network is denoted by L ∈ N and the widths by ( nσi )Li=0 , (nri )Li=1 ⊂ N. With this, we consider rational functions 

ri : Rnσi−1 → Rnri

of degree di ∈ N for 1 ≤ i ≤ L as transformations between the intermediate layers. The activation functions are given by fixed multivalued nri -ary base functions 

σi : Rnri → Rnσi

8...

## ... ... .... . . ... ...       

> r1σ1rLσL
> rL+1
> ∈
> ∈
> ∈
> ∈
> ∈
> ∈
> ∈
> Input
> Output
> Rnσ
> 0
> Rnr
> 1Rnσ
> 1RnσL−1RnrLRnσL
> R

Figure 1: Scheme of symbolic network Sσ

for 1 ≤ i ≤ L. Recall Definition 1 that base function cannot be expressed by a rational function. Note further that the presented setup also covers single unary base functions stacked within a multi-dimensional array by choosing nri = nσi for 1 ≤ i ≤ L. This is the case, for example, in the shallow ParFam architecture in [87]. Finally, the transformation of the output layer is given by a rational function, including a skip connection, as described in [87], taking the form 

rL+1 : RnσL+nσ 

> 0

→ R.

The proposed symbolic network architecture is now generally given by 

Sσ : Rnσ 

> 0

→ R

x 7 → rL+1 (( σL ◦ rL ◦ · · · ◦ σ1 ◦ r1)( x), x ). (1) A schematic illustration of Sσ is provided in Figure 1. We denote parameter-izations of Sσ by Sθσ for θ ∈ Θ for a suitable parameter set Θ, which will be explained in detail below. The complexity of the network Sθσ is controlled by the choice of the depth L ∈ N, the layer widths ( nσi )Li=0 , (nri )Li=1 ⊂ N, and the degrees (di)L+1  

> i=1

⊂ N of the underlying rational functions. 

Parameterization. The trainable parameters θ = ( θi)L+1  

> i=1

of the network in (1), denoted in parameterized form by Sθσ, consist of the coefficients θi of the polynomials that define the rational functions ri for 1 ≤ i ≤ L + 1 (see Definition 1). In practice, the parameterization is restricted to ensure both closedness of the parameter set and numerical stability during parameter training. Notably, the set 

Q(d, n ), from which the coefficients of the denominator polynomials of the rational functions ri are drawn (see Definition 2), is not closed. To ensure closedness of Θ, we require a restriction of the denominator polynomials as follows. We define for 

d, n ∈ N and ϵ > 0, for q : R(n+dd ) × Rn → R given as in Definition 2, the set 

Qϵ(d, n ) := 

n

b ∈ R(n+dd ) : q(b, x ) ≥ ϵ for all x ∈ Rn

o

.

9Closedness of Qϵ(d, n ) follows from continuity of q, since 

Qϵ(d, n ) = \

> x∈Rn

[q(·, x )] −1([ ϵ, ∞)) .

Another issue that arises alongside closedness is that rational functions, as in-troduced in Definition 1, are scaling-invariant, i.e., the numerator and denomina-tor polynomials can be scaled by a positive factor without changing the rational function itself. To avoid scaling invariance, we normalize the coefficients of the denominator polynomials with respect to the standard Euclidean norm ∥ · ∥ 2, i.e., ¯Qϵ(d, n ) = {b ∈ Qϵ(d, n ) : ∥b∥2 = 1 } .

Note that this strategy is also applied in ParFam [87]. Taking this normalization into account, along with the earlier considerations regarding the design of the neural network, we can now define the closed parameter set Θ, with mi = nσi for 0 ≤ i ≤ L − 1 and mL = nσL + nσ 

> 0

, for a small ϵ > 0 by Θ = 

> L+1

O

> i=1



⊗nri

> k=1

R(mi−1+didi ) × ⊗ nri 

> k=1

¯Qϵ(di, m i−1)



,

where, for i = 1 , . . . , L + 1, the first component ⊗nri

> k=1

R(mi−1+didi ) captures the coeffi-cients of the numerator polynomials, and the second component ⊗nri 

> k=1

¯Qϵ(di, m i−1)captures the coefficients of the denominator polynomials. 

## 2.3 Universal approximation 

This subsection provides an overview of the universal approximation properties of symbolic networks Sσ as introduced in (1). The ability of Sσ to approximate functions of suitable regularity is fundamentally connected to the approximation capabilities of rational functions, which form the trainable components of the net-work. Reducing the question of approximability to rational functions is particu-larly significant, as it allows deriving results that are qualitatively independent of the specific choice of the activation (base) functions σ within Sσ.In the context of rational functions, we highlight the seminal work [92], which inves-tigates the reciprocal approximability of certain ReLU-activated neural networks and rational functions. Following this, related results in [14] focus on rational functions defined as compositions of low-degree rational functions, showing that comparatively simpler rational functions retain significant approximation capabil-ities. Moreover, for the uniform first-order approximation of smooth functions by rational functions, we refer to recent advances in [43]. Further classical results on rational approximation theory can be found in [75]. 10 To conclude, we briefly discuss some key approximation properties of the symbolic network (1) as studied in [43]. It is shown that functions possessing sufficient regu-larity can be uniformly approximated up to their first-order derivatives by rational functions, as introduced in Definition 1, with positive denominator polynomials. This result can be extended directly to Sσ, given its architecture, which incorpo-rates a rational skip connection. Finally, we provide a restricted formulation of the results in [43, Lemma 16, Corollary 18], which discusses universal approximation properties of the network in (1) in the case where the layers are of equal size. 

Proposition 3. Consider symbolic networks in (1) with d = nri = nσi and Lipschitz continuous activations σi : Rd → Rd, such that σ−1 

> i

∈ C 3([0 , 1] d) for 1 ≤ i ≤ L.Then, for every f ∈ C 3([0 , 1] d) there exists a sequence of networks (Smσ )m, with 

lim  

> m→∞

∥Smσ − f ∥L∞([0 ,1] d) = 0 .

If ∇σi is Lipschitz continuous for 1 ≤ i ≤ L, then (Smσ )m can be chosen such that 

lim  

> m→∞

∥Smσ − f ∥L∞([0 ,1] d) + ∥∇ Smσ − ∇ f ∥L∞([0 ,1] d) = 0 .

Proof. See [43, Lemma 16, Corollary 18]. 

## 2.4 Extension to function space 

The data arising from physical problems, while often measured discretely, is in-herently functional in nature, representing some observable state variable (e.g., temperature or concentration) and potentially its higher-order derivatives. This motivates studying symbolic networks Sσ within a function space formulation. Modeling the dynamics directly in function space preserves the inherent structure of the continuous problem and ensures consistency and stability prior to numerical discretization (see [40, 49, 90]). To rigorously justify the application of symbolic networks to functional data, it is necessary to analyze the behavior of their archi-tecture within the context of function space analysis. 

2.4.1 Framework 

We follow the setup of [42] and consider for fixed T > 0 the state u ∈ V as 

u : (0 , T ) → V with V the dynamic extension of V , the static state space consisting of functions v : Ω → R. Here, Ω ⊂ Rd denotes a bounded Lipschitz domain for some d ∈ R. For κ ∈ N0, the order of differentiation, Jκ is a derivative operator 

Jκ : V → ⊗ κk=0 V ×

> k

v 7 → (v, J 1v, . . . , J κv) (2) 11 and Jacobian mappings Jk : V → V × 

> k

, v 7 → (Dβ v)|β|=k, for 1 ≤ k ≤ κ. The spaces 

Vk are such that V , → Vk and Dβ v ∈ Vk for 1 ≤ k = |β| = β1 + · · · + βd ≤ κ and 

β ∈ Nd

> 0

. With V0 := V the spaces V × 

> k

are further defined as V × 

> k

= ⊗pk

> i=1

Vk, where 

pk =  d+k−1

> k

 for 0 ≤ k ≤ κ. For W the static image space, with W its dynamic extension, some unknown physical law f is given as the Nemytskii operator of 

f : (0 , T ) × ⊗ κk=0 V × 

> k

→ W

where the latter is obtained by extending f : (0 , T )×⊗ κk=0 Rpk → R via f (t, v )( x) = 

f (t, v (x)). In the setup discussed in Section 2, we aim to determine a symbolic expression for f using parameterized networks Sθσ of the form in (1). Since Sθσ

operates pointwise in time and space, we require the operator Jκ for the parame-terizations Sθσ to depend on derivatives of the state. In the following, we specify the framework outlined above on the basis of [42, Subsection 2.1]. 

Assumption 4 (Space setup) . Suppose that the spaces Vk, for 1 ≤ k ≤ κ, the state space V , the image space W and the space ˜V are reflexive, separable Banach spaces. Assume for some 1 ≤ ˆq ≤ ˆp < ∞ the embeddings 

V , → ˜V , → W, V , →→ W κ, ˆp(Ω) , V , → Y, L ˆq(Ω) ,→ W, Lˆp(Ω) ,→ Vk ,→ Lˆq(Ω) , for 1 ≤ k ≤ κ, and either W κ, ˆp(Ω) ,→ ˜V or ˜V , → W κ, ˆp(Ω) .

The dynamic spaces are defined as Sobolev-Bochner spaces [79, Chapter 7], by 

V = Lp(0 , T ; V ) ∩ W 1,p,p (0 , T ; ˜V ), W = Lq(0 , T ; W ), Y = Lr(0 , T ; Y ),

V0 = V× 

> 0

:= V, Vk = Lp(0 , T ; Vk), V× 

> k

= Lp(0 , T ; V × 

> k

) for 1 ≤ k ≤ κ

for some 1 ≤ p, q, r < ∞ with p ≥ q. Finally, for some constant cV > 0, for all 

v ∈ V , we assume uniform state space regularity 

∥J κv∥L∞((0 ,T )×Ω) ≤ cV ∥v∥V . (3) 

Remark 5 (State space regularity) . The required compact embedding V , →→ W κ, ˆp(Ω) 

holds, e.g., for V being a Sobolev space with suitable parameters, such that the Rellich-Kondrachov Theorem (see [3, Theorem 6.3] and [34, §5.7]) applies. The extended state space embedding behind (3) follows for ˜V being a sufficiently regular Sobolev space by [79, Lemma 7.1]. We refer to [42, Remark 9] for the details. 

2.4.2 Regularity 

We justify that the parameterized symbolic networks Sθσ, of the form as introduced in (1), can be extended to corresponding operators acting on underlying function 12 spaces. Furthermore, we establish first regularity properties of these operators, formulated in function space. Note that these properties are necessary for [42, Assumption 3] to hold true. To this end, we apply analogous arguments as in [1, Lemma 4] for the extension to function space and [42, Lemma 17] for the regularity result. The proof of the following statement is provided in Appendix A. 

Proposition 6. Suppose that Assumption 4 holds true. Then the symbolic net-work Sθσ : Rnσ 

> 0

→ R induces well-defined Nemytskii operators Sθσ : ⊗κk=0 V× 

> k

→

Lq(0 , T ; Lˆq(Ω)) and Sθσ : ⊗κk=0 V× 

> k

→ W , both via [Sθσ(u)]( t) = Sθσ(u(t, ·)) . Fur-thermore, if the (σl)1≤l≤L are locally Lipschitz continuous, then 

S : Θ × V → W 

(θ, v ) 7 → S(θ, v ) =: Sθσ(Jκv)

is weak-strong continuous. Moreover, for bounded U ⊂ Rnσ 

> 0

, the map given by 

Θ ∋ θ → Sθσ ∈ L∞(U )

is continuous. Proof. See Appendix A. 

# 3 Symbolic model learning 

The central objective of this work is the identification of a (partially) unknown state u† ∈ V and a corresponding physical law f † : ⊗κk=0 V× 

> k

→ W , ideally expressed in a symbolically simple form, that satisfies the partial differential equation 

∂tu(t, x ) = f (t, Jκu(t, x )) , for ( t, x ) ∈ (0 , T ) × Ω,

s.t. K†u = y†. (E)Here, Y ∋ y† = K†u† denotes the full measurement data and K† is the measure-ment operator mapping the state u† to the observation y†. The measurement data 

y is modeled as y : (0 , T ) → Y , with a static measurement space Y and time extension Y. In practice, the data y† is provided via approximate measurements 

Y ∋ ym ≈ Kmu†, where Km : V → Y , for m ∈ N, are reduced measurement operators. The measurements are further assumed to satisfy the noise estimate 

∥ym − Kmu†∥r 

> Y

≤ δ(m), (4) where δ : N → R≥0 fulfills lim m→∞ δ(m) = 0. For the purpose of reconstruction, we approximate the physical law using symbolic networks Sθσ, parameterized by 13 θ ∈ Θ, as introduced in (1) in Section 2. In this work, we adopt the assumption that f † ∈ Sθσ | θ ∈ Θ is representable by a fixed architecture and briefly discuss a more general setup in the context of [42] later. Motivated by [42] and following an approach similar to that in [87], we aim to solve the reconstruction problem using the all-at-once formulation min  

> u∈V ,θ ∈Θ

λm∥∂tu − Sθσ(Jκu)∥q 

> W

+ μm∥Kmu − ym∥r 

> Y

+ R(u, θ ) (Pm)where λm, μ m > 0 for m ∈ N, and R denotes a suitable regularization functional. Next, we provide a more detailed discussion of the underlying framework. 

## 3.1 Framework 

In addition to Assumption 4, we pose the following assumptions on the data ac-quisition, the existence of an admissible solution to ( E), and the regularization. 

Assumption 7. 

Measurements. Let the full measurement operator K† be injective and weak-strong continuous. Suppose further that the operators Km : V → Y are weak-weak con-tinuous, for m ∈ N, and that for any weakly convergent sequence (um)m ⊂ V 

Kmum − K†um → 0 in Y as m → ∞ . (5) 

Admissible solution. Assume that for the given full measurement data y† ∈ Y ,there exists f † : ⊗κk=0 V× 

> k

→ W , to be understood as the space-time extension of an underlying scalar version f † : (0 , T ) × ⊗ κk=0 Rpk → R, and u† ∈ V fulfilling (E).Regularization. Suppose that the regularization functional R is of the form 

R : V × Θ → [0 , ∞], R(u, θ ) = ∥u∥p 

> V

+ R0(θ),

for R0 : Θ → [0 , ∞] a proper, coercive and weakly lower semicontinuous functional. 

Remark 8 (Approximation of K†). The abstract notion of convergence in (5) 

encompasses a broad range of scenarios. It is applicable to sequences of bounded linear operators (Km)m that converge to K† in the operator norm. In case of nonlinear operators, this concept can be extended to sequences (Km)m that converge uniformly to K† on bounded subsets of V, using boundedness of weakly convergent sequences. Notably, there exist specific examples, such as the sampling operators introduced in Subsection 4.1, where the general condition (5) is satisfied, as shown in Appendix C, while the stricter conditions mentioned above fail to hold. 

14 Remark 9 (Choice of R0). A sensible choice for R0 is the L1-norm, as it promotes sparsity of the parameters governing fθ. This, in turn, leads to a more concise and interpretable symbolic representation of the reconstructed physical law (see, e.g., [87]). Accordingly, in the experiments in Section 4, we adopt R0(·) = ∥ · ∥ L1 .

Remark 10 (Uniqueness of admissible solution) . The uniqueness of u† ∈ V sat-isfying K†u† = y† is guaranteed by the injectivity of K†. Conditions ensuring the uniqueness of the physical law f † have been comprehensively analyzed in [85]. 

Remark 11 (Regularity of admissible solution) . Ensuring the existence of an admissible solution to (E) with state regularity V, as required by Assumption 7, may in practice be difficult, given the regularity typically provided by the under-lying equation. For instance, in the case of the transport equation, this issue is discussed after [42, Proposition 1]. Nevertheless, it is important to observe that this requirement is implicitly an assumption on the model f , since the regularity of the state is generally inherited from the regularity of the model itself. A com-prehensive discussion of these regularity assumptions, together with further related considerations, can be found in [42, Subsection 2.1]. 

As a first step toward establishing the reconstructibility of a solution to ( E) via solutions to ( Pm), we argue well-posedness of ( Pm) under Assumption 4 and 7. 

Lemma 12. Under Assumption 4 and 7 problem (Pm) is well-posed for m ∈ N.Proof. See Appendix B for the details. 

## 3.2 Identification results 

In the following, we examine the problem of physical law learning, where an ad-missible state u† and physical law f † are reconstructed by um and Sθm 

> σ

for m ∈ N,respectively, obtained from ( um, θ m) solving ( Pm). The analysis focuses on the setup where f † can be represented within a predefined architecture parameterized by a set Θ. This approach relies on the assumption that f † admits a concise and interpretable symbolic representation and presumes that the chosen architecture, designed by the user (e.g., ParFam from [87]), is sufficiently expressive to capture such representations. We state our main result: 

Theorem 13. Let Assumption 4 and 7 hold true and suppose that there exists 

f † ∈ Sθσ | θ ∈ Θ ,

admissible to (E). Let (um, θ m) solve (Pm) for m ∈ N and λm, μ m > 0 such that 

λm → ∞ and μm → ∞ with μmδ(m) → 0 as m → ∞ .

15 Then there exists a subsequence (θml )l converging to some ˜θ ∈ Θ such that 

∂tu† = S˜θσ(Jκu†) and Sθml 

> σ

→ S˜θσ in L∞

> loc

(Rnσ 

> 0

) as l → ∞ . (6) 

This holds for any convergent subsequence. In addition also um ⇀ u † as m → ∞ .Moreover, the tuple (u†, ˜θ) is a regularization minimizing solution, i.e., for all 

(u†, θ †) solving ∂tu† = Sθ† 

> σ

(Jκu†) it holds true that R(u†, ˜θ) ≤ R (u†, θ †).Proof. See Appendix B for the details. The concluding assertion of Theorem 13 is particularly interesting from a practi-cal standpoint. Regularization of the parameter θ in the L1-norm promotes the reconstruction of a sparse parameterization of f † through the solution of ( Pm). This parameterization can be understood as a concise symbolic expression of the underlying physical law, offering enhanced interpretability. Another important observation is that if an appropriate identifiability condition, as established in [85], holds, a stronger result can be obtained. Specifically, [85] demonstrates that the unique identifiability of a linear/algebraic function f based on full state mea-surements is equivalent to the linear/algebraic independence of the state variables (e.g., derivatives up to order κ) on which f acts. Here, we state the identifiability condition under consideration in a general formulation as follows: For f1, f 2 : ⊗κk=0 V× 

> k

→ W with f1(Jκu†) = f2(Jκu†) ⇒ f1 = f2 (I)Assuming condition ( I), the convergence in (6) holds for the entire sequence ( θm)m.

Corollary 14. Let the assumptions of Theorem 13 and (I) apply. Then 

Sθm 

> σ

→ Sθ† 

> σ

= f † in L∞

> loc

(Rnσ 

> 0

) as m → ∞ .

Proof. See Appendix B for the details. The results of Theorem 13 and Corollary 14 can be directly extended to more general setups. One such generalization considers the case where, instead of a fixed architecture Sθσ for θ ∈ Θ, we consider growing architectures Sθ,m σm for θ ∈ Θm such that f † is only representable by some Sθ,m σm for sufficiently large m ∈ N.Another scenario is the case where admissible physical laws f † are not representable by any finite-dimensional architecture, regardless of size. This occurs when f † is not expressible in a symbolically concise form (e.g., the Gaussian error function) or when the architecture is not designed in a suitable way, e.g., a purely rational network. Such networks essentially represent rational functions and inherently fail to express base functions such as the exponential. From a practical perspective, 16 along with theoretical results on the approximation properties of symbolic net-works [43], it is natural to approximate f † using growing architectures in the limit as regularization-minimizing solution. This approach aligns with [42], which study unique reconstructibility in this generalized setting. However, two technical chal-lenges associated with these reconstructions require careful consideration (see [42, Assumption 2-5]), namely i) the weak lower semicontinuity of the C1(U )-seminorm of Sθσ for bounded U ⊆ Rnσ 

> 0

with respect to the parameterization θ ∈ Θ and ii) an approximation capacity condition as in [42, Assumption 5 iii)]. We believe that i) can be similarly obtained following the arguments proving Proposition 6 under additional notational technicalities. For ii), the underlying approximation (rate) follows for symbolic networks from the considerations in [43] on first order approx-imation results in case the target function is sufficiently regular. Although the growth of the parameters that realize the approximating networks, as considered in the work [41], is not addressed there, we believe that this can also be achieved with technical effort. Once a regularization-minimizing solution ˜f has been reconstructed, a practical strategy to refine its representation involves identifying a suitable parameterization 

f˜θ ≈ ˜f . This can be accomplished using a fixed, expressive symbolic network and applying L1-regularization to ˜θ, promoting sparsity in the symbolic representation. 

# 4 Numerical experiments 

In this section, we present a numerical setup to illustrate the analytical identifica-tion results established in Section 3. Specifically, we aim to evaluate whether the results predicted by Theorem 13 and Corollary 14 can be observed in practice. To achieve this, we focus on the identification of unknown physical laws f and states 

u using one-dimensional PDEs of the form 

∂tu = f (t, u, ∂ xu),

where f depends on the state u and its first spatial derivative ∂xu. This simplified framework allows us to test the validity and practical applicability of the theoretical results within a controlled computational environment. All numerical experiments of this work can be reproduced based on the publicly available source code 2.

## 4.1 Implementation framework 

To numerically evaluate the identification results, we outline a framework consist-ing of two components. The analytic setup specifies the theoretical conditions, while the implementation setup outlines the computational and numerical details.  

> 2See https://anonymous.4open.science/r/unique_sym_anon-3CDD/README.md .

17 4.1.1 Analysis setup 

In view of Assumption 4, we adopt a Hilbert space framework with parameters 

p = q = ˆ p = ˆ q = r = 2, domain Ω = [0 , 4], time T = 3, state space V = H2(Ω), image space W = L2(Ω), measurement space Y = L2(Ω) and the corresponding dynamic spaces as defined in Assumption 4. The state space V is assumed to have higher regularity to ensure a compact embedding into H1(Ω), which is crucial, since the unknown physical law depends on up to κ = 1 spatial derivatives of state u (compare with Remark 11). Similarly, the space ˜V requires additional regularity to guarantee condition (3), as ensured by [42, Remark 9] for ˜V = H2(Ω). We further choose the full measurement operator K† as the embedding operator 

ι : V → Y , i.e., K†u = ιu . This operator is injective and weak-strong continuous, due to the compact embedding V ,→→ Y , as guaranteed by the Aubin-Lions Lemma [79, Lemma 7.7]. Specifically, with ⟨· , ·⟩ denoting the inner product in L2(Ω) and an orthonormal system ( ei)i∈N of L2(Ω) (e.g., for Ω = [0 , 1], Ω ∋ x 7 → ei(x) = cos( iπx ) ∈ L2(Ω)), we can express K† by 

K† : V → Y , [K†u]( t) = X

> i∈N

⟨ei, u (t)⟩ei

for u ∈ V and t ∈ [0 , T ]. The reduced measurement operators, corresponding to low-frequency sampling operators (or truncated Fourier measurement operators), are given as follows. Let 0 = t1 < t 2 < · · · < t m < t m+1 = T be the m-equidistant grid on [0 , T ] for m ∈ [0 , T ] and ∆ m := T /m . Then, for 1 ≤ j ≤ m − 1 and 

t ∈ [tj , t j+1 ) or j = m and t ∈ [tm, t m+1 ], we define Km for m ∈ N by 

Km : V → Y , [Kmu]( t) = 

> m

X

> i=1

∆−1

> m

ˆ tj+1 

> tj

⟨ei, u (s)⟩ ds

!

ei (7) for u ∈ V . Note that the operator Km performs time averaging over the respective time intervals. The well-definedness of K† and Km for m ∈ N, along with the convergence condition (5) in Assumption 7, are discussed in detail in Appendix C. 

4.1.2 Simulation setup 

We now detail the concrete choices made for the quantities appearing in Theo-rem 13. Since letting m → ∞ is infeasible in practice, we fix a even maximal value M ∈ N and consider measurements for m = M/ 2, . . . , M − 1. Guided by the preceding analysis, we apply a low-pass filtering strategy by defining Km to retain the lowest m/M fraction of frequency components. Throughout all experiments we set q = r = 2 and use the parameter choices 

λm = m3

M , μm = mM , δm = 0 .1 Mm3 . (8) 18 Measurement noise is simulated by multiplicative noise according to 

ym = Km(u†) · ϵδm

, (9) with uniform perturbations ϵ ∼ Unif(0 .5, 1.5). As δm decreases with m, the factor 

ϵδm

approaches 1 for large m, producing weaker distortion. We employ multiplica-tive noise to reflect the wide dynamic range of the solution function, ensuring that small values are perturbed less strongly than large ones. Following Theorem 13, we model fθ using ParFam [87] and represent u as func-tion evaluations on a discretized grid. The ParFam architecture, introduced in Equation (1), adopts the practical design of Scholl et al. [87], who observed that a single hidden layer ( L = 1) is sufficient for expressive modeling while simplifying optimization. We select numerator polynomials of degree 3, denominator polyno-mials of degree 2, and sine and exponential activation functions. ParFam permits custom loss functions, which we specify using the objective of ( Pm). Parameter optimization is performed using basin-hopping [96] combined with BFGS, both run for 100 iterations with default settings otherwise. To approximately solve the minimization problem ( Pm), we employ an alternating optimization scheme. Specifically, we iteratively optimize with respect to θ, keep-ing u fixed, and then optimize with respect to u while fixing θ. The parameter θ

is updated using the ParFam procedure described above, while u is trained using ADAM [59] with a learning rate of 0 .0004, up to 300 epochs, with early stopping. This alternating optimization scheme is repeated for 100 full cycles. 

## 4.2 Numerical results 

In this subsection, we present results on two different PDEs, selected from the numerical section in Scholl et al. [85, Subsection 5.1]. 

4.2.1 Uniquely identifiable PDE 

First, we focus on an uniquely identifiable PDE, specifically the equation 

ut = au + bu x,

which attains the analytical solution u†(t, x ) = ( x + bt ) exp( at ). We set a = 1 and 

b = 2, and sample u on an equidistant grid over the domain [0 , 3] × [0 , 4], with 100 grid points along spatial x-direction and 80 along temporal t-direction. In Figure 2 (a), we report the results for M = 100 and m = 50 , . . . , 99. To assess the statements from Theorem 13, we present the L2-distance between fθm (u†, u †

> x

) and 

u† 

> t

, as well as between the learned solution um and the ground-truth u†. These re-sults clearly demonstrate the convergence predicted by Theorem 13. Furthermore, 19 since the PDE is uniquely identifiable, Corollary 14 implies that fθm converges to 

f † where f †(v, w ) = v + 2 w. This is supported by the learned approximation: 

fθm (u, u x) = 1 .006 u − 0.005 u2 

> x

+ 2 .116 ux − 0.535 ,

for m = 100, which closely matches the true operator. Note that in Figure 2 (a) the deviation of fθm from f † is considered on the domain U0 depicted in Figure 2 (c) (the range of ( u†, u †

> x

) corresponds to U ). In Figure 2 (c) this deviation is shown for different choices of the underlying domain. 

4.2.2 Not uniquely identifiable PDE 

As a second example, we consider the function u(t, x ) = exp( x − at ), which solves for any choice of coefficients a1, a 2 ∈ R, satisfying a1 + a2 = −a, the equation 

ut = a1u + a2ux.

We set a = 1 and sample u on an equidistant grid over the domain [0 , 3] × [0 , 4], with 100 grid points along x-direction and 80 grid points along t-direction. In Figure 2 (b), we report the results for M = 100 and m = 50 , . . . , 99. To assess the statements from Theorem 13, we present the L2-distance between fθm (u†, u †

> x

) and 

u† 

> t

, as well as between the learned solution um and the ground-truth u†. These results clearly demonstrate the convergence predicted by Theorem 13. Since u† does not solve a unique PDE, we cannot employ Corollary 14. Neverthe-less, the result in Theorem 13 guarantees that the learned parameters θm converge to a L1-minimizing parameterization of the underlying PDE model fθm which is solved by u†. This is supported by the learned approximation for m = 100: 

fθm (u, u x) = −0.982 u − 0.016 ux.

# 5 Conclusions 

This work focused on investigating the identifiability of symbolic (human inter-pretable) expressions for unknown physical laws corresponding to the right-hand side of a PDE model, as well as reconstructing the unknown state based on noisy and incomplete measurements. We proposed an all-at-once minimization formu-lation in function space and introduced symbolic networks for approximating the physical law. These networks leverage rational functions as transformations and base functions as activations, harnessing both the approximation capabilities of ra-tional functions and their inherent flexibility to represent arithmetic formulas. In 20 50 60 70 80 90 100 

m

10 3

10 2

> Error

Different deviations vs m

|| ut f m(u , ux )|| 2

|| u um|| 2

|| f f m|| 

10 1

10 2

> Model error || f f m||

(a) Deviations for uniquely identi-fiable PDE. 50 60 70 80 90 100 

m

10 5

10 4

10 3

> Error

Different deviations vs m

|| ut f m(u , ux )|| 2

|| u um|| 2

|| m|| 1

10 1

10 0

10 1

> Parameter norm ||  m|| 1

(b) Deviations for not uniquely identifiable PDE. 0 50 100 150 200 

u

2.5 

5.0 

7.5 

10.0 

12.5 

15.0 

17.5 

20.0 

> ux

U0

U1

U2

U3

U4

U5

U

Different domains 

50 60 70 80 90 100 

m

10 1

10 2

> Error

Error || f f m|| on domain Ui vs m

> U0
> U1
> U2
> U3
> U4
> U5

0 50 100 150 200 

u

2.5 

5.0 

7.5 

10.0 

12.5 

15.0 

17.5 

20.0 

> ux

U0

U1

U2

U3

U4

U5

U

Different domains 

50 60 70 80 90 100 

m

10 1

10 2

> Error

Error || f f m|| on domain Ui vs m

> U0
> U1
> U2
> U3
> U4
> U5

(c) Model error for different domains for the uniquely identifiable PDE. 

Figure 2: Numerical performance. 21 addition to reconstructibility results, another key contribution of our work are cer-tain regularity properties of symbolic networks in function space. Our main result addresses the framework where the physical law can be represented by a specific symbolic network architecture. We show in the limit of complete measurements: 1) State reconstruction: The state is reconstructed, and the sequence of symbolic networks converges to an admissible physical law of the PDE model. 2) Symbolic network representation: The limit of these networks remains expressible within the underlying symbolic network architecture, and its pa-rameterization is regularization minimizing. Notably, if the regularization functional is designed to promote sparsification, the proposed approach re-constructs preferentially a simple admissible expression of the physical law. 3) Unique identifiability: When the underlying physical law is uniquely iden-tifiable, subject to appropriate conditions, the reconstructed symbolic net-work in the limit aligns exactly with the unique true physical law. These theoretical results were supported by first numerical experiments, which provide further confidence in the validity and practicality of the proposed approach. The theoretical and numerical results of our work pave the way for addressing deeper challenges in both symbolic representation and reconstruction of physical laws in PDE models. A promising research direction is to also investigate the case where the physical law cannot be represented within the symbolic network architecture. Even though we briefly outlined an idea for addressing this case, we expect the underlying problem to be significantly more complex. Also interesting are convergence rates and statistical aspects, which we have not considered in this work. Investigating broader designs for symbolic networks could further enrich their applicability. Examples include allowing rational functions with poles or introducing more general base activation functions. From a practical point of view, it is interesting to extend and deepen the numerical experiments to more general classes of PDEs, which of course depends heavily on the problem at hand. 

# Acknowledgement 

This research was funded in whole or in part by the Austrian Science Fund (FWF) 10.55776/F100800. 

# A Regularity of symbolic networks 

In this section, we provide the proof of the extendability and regularity result stated in Proposition 6 from Subsection 2.4. We first establish the extension 22 property of symbolic networks to function spaces, following [1, Lemma 4]. 

Lemma 15. Suppose that Assumption 4 holds true. Then the symbolic net-work Sθσ : Rnσ 

> 0

→ R induces well-defined Nemytskii operators Sθσ : ⊗κk=0 V× 

> k

→

Lq(0 , T ; Lˆq(Ω)) and Sθσ : ⊗κk=0 V× 

> k

→ W , both via [Sθσ(u)]( t) = Sθσ(u(t, ·)) .Proof. For fixed θ ∈ Θ we derive continuity of Sθσ by continuity of the base func-tions σi for 1 ≤ i ≤ L (see Definition 1) and continuity of the parameterized rational functions ri for 1 ≤ i ≤ L + 1. As a consequence, for u ∈ V implying 

Jκu ∈ L∞((0 , T ) × Ω) by (3), we obtain that sup 

> 0≤t≤T,x ∈Ω

|Sθσ(t, Jκu(t, x )) | < ∞ (10) and hence, the inclusion Sθσ(t, Jκu(t, ·)) ∈ Lˆq(Ω) (which is separable) for a.e. t ∈

(0 , T ). Now the map t 7 → Sθσ(t, Jκu(t, ·)) is weakly measurable, which is Lebesgue measurability of t 7 → ´ 

> Ω

Sθσ(t, Jκu(t, x )) w(x) d x for all w ∈ Lˆq∗

(Ω). This follows from continuity of Sθσ, Lebesgue measurability of w, Jκu(t, ·) and due to preserva-tion of measurability under integration. Using Pettis theorem [79, Theorem 1.34] we obtain Bochner measurability of t 7 → Sθσ(t, Jκu(t, ·)) ∈ Lˆq(Ω) and with (10) well-definedness of the Nemytskii operator Sθσ : ⊗κk=0 V× 

> k

→ Lq(0 , T ; Lˆq(Ω)). The remaining assertion follows immediately by p ≥ q and Lˆq(Ω) ,→ W .We now turn to the regularity of symbolic networks in function space. 

Rational functions. For that, we first require the following auxiliary continuity property of rational functions with respect to their parameterization. 

Lemma 16. Let d, n ∈ N and U ⊂ R(n+dd ) be bounded, then the map 

R(n+dd ) × Q(d, n ) ∋ (a, b ) 7 →

P 

> 0≤k1+··· +kn≤d

ak1,...,k n

Qnl=1 xkl

> l

P 

> 0≤k1+··· +kn≤d

bk1,...,k n

Qnl=1 xkl

> l

∈ C 1(U )

is strong-strong continuous, with the set Q(d, n ) given as in Definition 2. Proof. Let ( am, b m)m∈N ⊂ R(n+dd ) × Q(d, n ) and ( a, b ) ∈ R(n+dd ) × Q(d, n ) such that (am, b m) → (a, b ) as m → ∞ . Denote 

P m(x) = X

> 0≤k1+··· +kn≤d

amk1,...,k n

> n

Y

> l=1

xkl 

> l

and Qm(x) = X

> 0≤k1+··· +kn≤d

bmk1,...,k n

> n

Y

> l=1

xkl 

> l

.

Similarly, denote by P, Q the polynomials resulting from the coefficients a and b,respectively. We aim to show 

PQ − P m

Qm C1(U )

→ 0 as m → ∞ . (11) 23 For that, first note that 

PQ − P m

Qm L∞(U )

≤ 1

Q L∞(U )

1

Qm L∞(U )

∥P Q m − P mQ∥L∞(U ). (12) By definition of Q(d, n ) (see Definition 2) there exists ϵ > 0 with Q(x) > ϵ 

for all x ∈ Rn. Thus, it holds true that ∥Q−1∥L∞(U ) ≤ 1/ϵ . Denote by M =max( |xi| : x ∈ U, 1 ≤ i ≤ n) < ∞. Since bm → b as m → ∞ , we derive for x ∈ U

|Qm(x) − Q(x)| ≤ X

> 0≤k1+··· +kn≤d

|bmk1,...,k n − bk1,...,k n |M k1+··· +kn ≤ C|bm − b|

for some C > 0 independent of m ∈ N. Since this estimation is independent of 

x ∈ U we infer that ∥Qm − Q∥L∞(U ) → 0 as m → ∞ . Similarly one can show that 

P m → P in L∞(U ) as m → ∞ . As a consequence, it holds true, using Q > ϵ , for sufficiently large m ∈ N with ∥Qm − Q∥L∞(U ) ≤ ϵ/ 2, that 

1

Qm L∞(U )

≤ 1

Q − ∥ Qm − Q∥L∞(U ) L∞(U )

< 2

ϵ . (13) Using (13) and that 

∥P Q m −P mQ∥L∞(U ) ≤ ∥ P ∥L∞(U )∥Qm −Q∥L∞(U ) +∥Q∥L∞(U )∥P m −P ∥L∞(U ) → 

> m→∞

0by the previously discussed convergence of the polynomials P m to P and Qm to 

Q, respectively, implies that the the term on the left hand side of (12) converges to 0 as m → ∞ . It remains to show that 

∇

 PQ − P m

Qm

 

> L∞(U)

→ 0 as m → ∞ .

Due to ∇(P/Q ) = ( Q∇P − P ∇Q)/Q 2 and similarly for ∇(P m/Q m) we obtain 

∇

 PQ − P m

Qm

 

> L∞(U)

≤ (Q∇P − P ∇Q) − (Qm∇P m − P m∇Qm)

Q2 L∞(U )

+ (Qm∇P m − P m∇Qm)

 1

Q2 − 1(Qm)2

 

> L∞(U)

. (14) We show that both terms converge to zero as m → ∞ . As ∥Q−2∥L∞(U ) ≤ ϵ−2 for the first term, it suffices to show that Qm∇P m → Q∇P in L∞(U ) as m → ∞ .The convergence P m∇Qm → P ∇Q in L∞(U ) as m → ∞ follows by a symmetrical argument. By a similar argument as previously, one can show that ∇P m → ∇ P

24 in L∞(U ) as m → ∞ . Together with Qm → Q in L∞(U ) as m → ∞ , convergence of the first term on the right hand side of (14) to zero follows. For the second term note that ∥Qm∇P m − P m∇Qm∥L∞(U ) ≤ ∥ Q∇P − P ∇Q∥L∞(U ) + 1 for sufficiently large m ∈ N. By (13) for sufficiently large m with ∥Qm − Q∥L∞(U ) ≤ 1 we have 

∥Q−2 − (Qm)−2∥L∞(U ) ≤ 4

ϵ4 (2 ∥Q∥L∞(U ) + 1) ∥Qm − Q∥L∞(U ),

which converges to zero as m → ∞ , proving convergence of (14) to zero and finally, the claim in (11). This concludes the assertions of the lemma. 

Symbolic Networks. We address the regularity statements in Proposition 6 following a similar strategy as in the proof of [42, Lemma 17] based on Lemma 16. 

Lemma 17 (Weak-strong continuity of S). Suppose that Assumption 4 holds true and that the base functions (σi)1≤i≤L are locally Lipschitz continuous, then 

S : Θ × V → W 

(θ, v ) 7 → S(θ, v ) =: Sθσ(Jκv)

is weak-strong continuous. Proof. We follow a similar strategy as in the proof of [42, Lemma 17]. Let (θm, u m) ⇀ (θ, u ) ∈ Θ×V weakly as m → ∞ . We prove that S(θm, u m) → S(θ, u )in W as m → ∞ . Since Θ is finite dimensional, the convergence of the parameters (θm)m holds in the strong sense. The weak convergence of ( um)m in V implies strong convergence in Lp(0 , T ; W κ, ˆp(Ω)), which can be seen as follows. In case 

W κ, ˆp(Ω) ,→ ˜V the Aubin-Lions Lemma [79, Lemma 7.7] implies 

V = Lp(0 , T ; V ) ∩ W 1,p,p (0 , T ; ˜V ) ,→→ Lp(0 , T ; W κ, ˆp(Ω)) .

Otherwise for ˜V , → W κ, ˆp(Ω) above embedding follows again by the Aubin-Lions lemma, since in this case V ⊆ Lp(0 , T ; V )∩W 1,p,p (0 , T ; W κ, ˆp(Ω)). With this, apply-ing [42, Lemma 31] we derive that Jκum → J κu strongly in ⊗κk=0 Lp(0 , T ; Lˆp(Ω) pk )as m → ∞ . Another important conclusion that can be drawn from the weak convergence of ( um)m in V is boundedness of ( Jκum)m due to (3). Thus, there exists an origin-centered ball BM0 (0) ⊆ R1+ Pκk=0 pk of radius M0 > 0 such that (t, Jκum(t, x )) ∈ U for a.e. ( t, x ) ∈ (0 , T ) × Ω for all m ∈ N. In the following we denote by z = Jκu, z m = Jκum ∈ ⊗ κk=0 Lp(0 , T ; Lˆp(Ω) pk ) for m ∈ N and show first 

Sθm 

> σ

(zm) → Sθσ(z) in Lq(0 , T ; Lˆq(Ω)) as m → ∞ . (15) 25 Omitting the notational dependence of z, (zm)m on time and space we estimate for a.e. ( t, x ) ∈ (0 , T ) × Ω pointwise 

|Sθm 

> σ

(t, z m) − Sθσ(t, z )| ≤ | Sθm 

> σ

(t, z m) − Sθm 

> σ

(t, z )|

| {z }

> (I)

+ |Sθm 

> σ

(t, z ) − Sθσ(t, z )|

| {z }

> (II)

. (16) 

Estimation of (I). We recall that for θm parameterizing the rational functions rmi

(and θ parameterizing ri) for 1 ≤ i ≤ L + 1 we have for ( t, y ) ∈ R1+ Pκk=0 pk = Rnσ

> 0

Sθm 

> σ

: Rnσ 

> 0

→ R

(t, y ) 7 → rmL+1 (( σL ◦ rmL ◦ · · · ◦ σ1 ◦ rm 

> 1

)( t, y ), (t, y )) .

For 1 ≤ i ≤ L we define iteratively origin-centered balls BMi (0) ⊆ Rnσi as follows. We fix ϵ > 0. As the rational function ri is continuous, the image ri(BMi−1 (0)) is bounded and included in B ˜Mi (0) ⊆ Rnri for some ˜Mi > 0. By Lemma 16, for sufficiently large m ∈ N, it holds true that rmi (BMi−1 (0)) ⊆ B (1+ ϵ) ˜Mi (0). Similarly, by continuity of σi there exists some Mi > 0 with σi(B(1+ ϵ) ˜Mi (0)) ⊆ B Mi (0) ⊆ Rnσi .Thus, with Ui := BMi (0) ⊆ Rnσi and Vi := B(1+ ϵ) ˜Mi (0) ⊆ Rnri for 0 ≤ i ≤ L, we derive that rmi (Ui−1), r i(Ui−1) ⊆ Vi and σi(Vi) ⊆ Ui for 1 ≤ i ≤ L. Furthermore, it holds true that zm(t, x ), z (t, x ) ∈ U0 for a.e. ( t, x ) ∈ (0 , T ) × Ω. Now as ri ∈ C 1(Ui−1) it is Lipschitz continuous on Ui−1 with some constant Lri > 0(indeed one can choose Lri = |ri|C1(Ui−1)) for 1 ≤ i ≤ L + 1. Employing Lemma 16, we infer that for sufficiently large m ∈ N also the rmi are Lipschitz continuous on Ui−1 with Lipschitz constant Lri + ϵ for 1 ≤ i ≤ L + 1. Using local Lipschitz continuity of the σi, we obtain Lipschitz continuity of the σi on Vi with constant 

Lσi > 0 for 1 ≤ i ≤ L. As a consequence, we can estimate the term in (I) by 

|Sθm 

> σ

(t, z m) − Sθm 

> σ

(t, z )| ≤ (LrL+1 + ϵ)(1 + 

> L

Y

> i=1

(Lri + ϵ)Lσi )

| {z }

> =: L

|zm − z| (17) 

Estimation of (II). Assume w.l.o.g. that the Lipschitz constants fulfill Lri , Lσi ≥

1. Again employing Lemma 16 we can choose m ∈ N sufficiently large such that 

∥ri − rmi ∥L∞(Ui−1) < ϵ (18) for 1 ≤ i ≤ L + 1. We define the auxiliary networks ( Si)L+1  

> i=0

as follows 

S0(θm, θ, ·) = S(θm, ·), SL+1 (θm, θ, ·) = S(θ, ·) and for 1 ≤ s ≤ L

Ss(θm, θ, ·) = rL+1 (( σL ◦rL ◦· · ·◦ σL−s+2 ◦rL−s+2 ◦σL−s+1 ◦rmL−s+1 ◦· · ·◦ σ1 ◦rm 

> 1

)( ·), ·).

26 Note first that due to the considerations on the estimation of (I) (with 0 ∈ U0) it follows that there exists some C > 0 such that for sufficiently large m ∈ N

(σs ◦ rms ◦ · · · ◦ σ1 ◦ rm 

> 1

)(0) < C 

for 1 ≤ s ≤ L. We estimate the term in (II) by the telescope sum 

|Sθm 

> σ

(t, z ) − Sθσ(t, z )| ≤ 

> L

X

> s=0

|S s+1 (θm, θ, t, z ) − S s(θm, θ, t, z )|. (19) Using the Lipschitz constants derived in view of the estimation of (I) we obtain for sufficiently large m ∈ N, defining T ms = σs ◦ rms ◦ · · · ◦ σ1 ◦ rm 

> 1

, that 

|S s+1 (θm, θ, t, z ) − S s(θm, θ, t, z )| (20) 

≤ (LrL+1 + ϵ)

> L

Y

> i=L−s+2

(Lri + ϵ)Lσi |(σL−s+1 ◦ rL−s+1 ) − (σL−s+1 ◦ rmL−s+1 )|(T mL−s(t, z )) .

Since both rL−s+1 (T mL−s(t, z )) , r mL−s+1 (T mL−s(t, z )) ∈ VL−s+1 and T mL−s(t, z ) ∈ UL−s

we can estimate using (18) 

|(σL−s+1 ◦ rL−s+1 ) − (σL−s+1 ◦ rmL−s+1 )|(T mL−s(t, z )) 

≤ L σL−s+1 |rL−s+1 (T mL−s(t, z )) − rmL−s+1 (T mL−s(t, z )) | ≤ ϵLσL−s+1 . (21) Combining (19),(20),(21) together with the Lipschitz constants assumed to be larger than one, we conclude that 

|Sθm 

> σ

(t, z ) − Sθσ(t, z )| ≤ ϵL L. (22) Finally, by (16), (17) and (22) we derive the pointwise estimate 

|Sθm 

> σ

(t, z m) − Sθσ(t, z )| ≤ L| zm − z| + ϵL L. (23) 

Convergence in function space. In view of (15) we derive using (23) and the triangle inequality that 

∥Sθm 

> σ

(zm) − Sθσ(z)∥Lq (0 ,T ;Lˆq (Ω)) ≤ L∥ zm − z∥Lq (0 ,T ;Lˆq (Ω)) + ϵL LT 1/q |Ω|1/ˆq. (24) Employing H¨ older’s inequality, we can estimate for some generic constant C > 0

∥zm − z∥Lq (0 ,T ;Lˆq (Ω)) ≤ C∥J κum − J κu∥⊗Lp(0 ,T ;Lˆp(Ω) pk ) ≤ C∥um − u∥Lp(0 ,T ;W κ, ˆp(Ω)) 

(25) 27 where the last inequality follows by definition of the differential operator Jκ. Since 

um → u in Lp(0 , T ; W κ, ˆp(Ω)) as m → ∞ and ϵ can be chosen arbitrarily small (with resulting larger m to fulfill underlying inequalities), we conclude by (24) and (25) that (15) holds true. With the embedding Lq(0 , T ; Lˆq(Ω)) ,→ W this implies that 

S(θm, u m) → S(θ, u ) as m → ∞ in W,

proving the claimed weak-strong continuity of the joint operator S.Following the proof of Lemma 17 we can extract the following regularity property. 

Corollary 18. Suppose that Assumption 4 holds true and that the base functions 

(σi)1≤i≤L are locally Lipschitz continuous. Then for bounded U ⊂ Rnσ 

> 0

the map 

Θ ∋ θ → Sθσ ∈ L∞(U )

is continuous. Proof. Let ( θm)m ⊂ Θ with θm → θ ∈ Θ as m → ∞ . Following the proof of Lemma 17 we derive by (23) that for every ϵ > 0 the estimation 

|Sθm 

> σ

(w) − Sθσ(w)| ≤ ϵL L

holds true for sufficiently large m ∈ N for all w ∈ U , proving the assertion. Combining Corollary 18 with Lemmata 12 and 17 completes the proof of the statement in Proposition 6. Note that a crucial key property to derive Lemma 17 and, consequently, Proposition 6, apart from Lemma 12, is the uniform state space regularity assumption in (3). This raises the question whether one can avoid the underlying embedding under stronger regularity assumptions on the activation base functions ( σi)1≤i≤L. An alternative approach is presented e.g., in [42, Lemma 17], where global Lipschitz continuity of the activation functions is sufficient, rather than local Lipschitz continuity, in the case of affine linear transformations. However, this result does not cover rational transformations, which are the focus here. Some initial considerations addressing this question are outlined below. 

Remark 19. We conjecture that the regularity assumption (3) can be avoided. For the special case of rational transformations r = p/q with deg( p) ≥ deg( q) ≥

deg( p)−1 and q > 0, already covering a large class of rational functions with rather strong approximation properties (see [14] and [43]), one can argue w.l.o.g. in one dimension as follows. Rational functions of the type above behave like affine linear functions towards ±∞ and their seminorm fulfills |r|C1(R) < ∞. Furthermore, for 

r parameterized by coefficients θ ∈ Θ and coefficients θm parameterizing rationals 

28 rm = pm/q m (fulfilling deg( pm) ≥ deg( qm) ≥ deg( pm) − 1 and qm > 0) with 

θm → θ as m → ∞ it follows that |rm|C1(R) → | r|C1(R) as m → ∞ . In other words 

r and (rm)m are jointly globally Lipschitz continuous with the same constant. With this, an analogous result as in [42, Lemma 17] can be obtained following its proof in combination with the one of Lemma 17. If the rational transformations are general polynomials of degree d, we conjecture that (3) can be weakened to the regularity assumption that the state space is of the form that (Jκum)m converges in space in Lˆpρ 0 (Ω) and is bounded in L(d−1) i ˆpρ i (Ω) for the layer index 1 ≤ i ≤ L where PLi=0 ρ−1 

> i

= 1 . We expect a similar result for general rational transformations 

r = p/q for polynomials p, q with deg( p) − deg( q) instead of d. These regularity assumptions are obviously more difficult to fulfill the deeper the network is and the more complex the rational transformations are. 

# B Proofs of identification results 

In the following we provide the detailed arguments of the assertions in Section 3, starting with the proof of Lemma 12. 

Proof of Lemma 12. The assertion essentially follows from the direct method . Since the regularization R is proper, there exists an infimizing sequence ( uk, θ k)k ⊂ V × Θof problem ( Pm). Due to coercivity of R, ( uk)k is bounded in V and ( θk)k in Θ. As a consequence, due to reflexivity of V, the sequence ( uk)k admits a weakly convergent subsequence in V with limit ˆ u ∈ V , and ( θk)k a strongly convergent subsequence in Θ with limit ˆθ ∈ Θ, since Θ is finite dimensional (w.l.o.g. for the entire sequences) and closed by design (see Subsection 2.2). We derive 

Sθk 

> σ

(Jκuk) → Sˆθσ(Jκ ˆu) in W as k → ∞ 

using Proposition 6. Now since ∂tuk ⇀ ∂ t ˆu in Lp(0 , T ; ˜V ) as k → ∞ the weak convergence also holds in W as Lp(0 , T ; ˜V ) ,→ W by ˜V ,→ W . Furthermore, weak-weak continuity of Km implies Kmuk ⇀ K m ˆu in Y as k → ∞ . Combining these convergences with weak lower semicontinuity of λm∥ · ∥ q 

> W

, μm∥ · ∥ r 

> Y

and the regularization R in the respective spaces, it follows that (ˆ u, ˆθ) solves ( Pm). We conclude this section by providing the proof of our main result, Theorem 13. 

Proof of Theorem 13. Since f † is representable, there exists θ† ∈ Θ such that 

f † = Sθ† 

> σ

. We estimate the objective functional of ( Pm) by 

λm∥∂tum − Sθm 

> σ

(Jκum)∥q 

> W

+ μm∥Kmum − ym∥r 

> Y

+ R(um, θ m)

≤ λm∥∂tu† − Sθ† 

> σ

(Jκu†)∥q 

> W

+ μm∥Kmu† − ym∥r 

> Y

+ R(u†, θ †). (26) 29 As ∂tu† = Sθ† 

> σ

(Jκu†) and ∥Kmu† − ym∥r 

> Y

= δ(m), the right hand side of (26) converges to R(u†, θ †) as m → ∞ . This implies boundedness of ( ∥um∥V )m and (∥θm∥)m by coercivity of R. As a consequence, there exists a weakly convergent subsequence of the ( um)m with limit ˜ u in V. We denote it w.l.o.g. by the original indices as we will show um ⇀ u † in V as m → ∞ . Since lim m→∞ μm = ∞ the convergence lim m→∞ ∥Kmum − ym∥Y = 0 follows. We estimate 

∥K† ˜u − K†u†∥Y ≤ ∥ K† ˜u − K†um∥Y + ∥K†um − Kmum∥Y

+ ∥Kmum − Kmu†∥Y + ∥Kmu† − K†u†∥Y .

The first term converges to zero by weak-strong continuity of K†. The second and fourth term converge to zero by (5), since ( um)m and the constant sequence ( u†)m

are weakly convergent. The third term converges to zero by (4), since Kmu† = ym

and lim m→∞ δ(m) = 0. Thus, we conclude that K† ˜u = K†u† and finally, ˜ u = u† by injectivity of K†. Since ( ∥θm∥)m is bounded, there exists a convergent subsequence (θml )l with limit ˜θ ∈ Θ by closedness of Θ. Using λm → ∞ as m → ∞ we derive lim  

> l→∞

∥∂tuml − Sθml 

> σ

(Jκuml )∥W = 0 (27) and with ∂tuml ⇀ ∂ tu†, as in the proof of Lemma 12, boundedness of ( ∂tuml )ml . As a consequence, also ( Sθml 

> σ

(Jκuml )) ml is bounded and there exists g ∈ W such that 

Sθml 

> σ

(Jκuml ) ⇀ g in W as l → ∞ (w.l.o.g. for the entire sequence). Employing weak lower semicontinuity of the ∥ · ∥ W -norm in (27) yields g = ∂tu†. Using Proposition 6 we obtain that u† and the reconstructed physical law S˜θσ fulfill 

∂tu† = S˜θσ(Jκu†). (28) The assertion on L∞

> loc

(Rnσ 

> 0

)-convergence follows directly by Proposition 6. Finally, for any solution ( u†, θ †) of ∂tu† = Sθ† 

> σ

it follows by (26) that lim inf  

> m→∞

R(um, θ m) ≤ R (u†, θ †)which by weak lower semicontinuity of R implies R(u†, ˜θ) ≤ R (u†, θ †). As a consequence of Theorem 13 the result in Corollary 14 holds. 

Proof of Corollary 14. Following Theorem 13, we conclude from (28) that 

S˜θσ(Jκu†) = ∂tu† = Sθ† 

> σ

(Jκu†).

Using the identifiability condition ( I), we deduce that S˜θσ = Sθ† 

> σ

. Since this equality is independent of the convergent subsequence of ( θm)m, together with the result of Theorem 13, we conclude, as claimed that 

Sθm 

> σ

→ Sθ† 

> σ

= f † in L∞

> loc

(Rnσ 

> 0

) as m → ∞ .

30 C Convergence condition for sampling operators 

In this section we provide a proof of the convergence condition (5) for the measure-ment operators defined in Subsection 4.1. Recall the full measurement operator 

K† : V → Y , [K†u]( t) = X

> i∈N

⟨ei, u (t)⟩ei (29) for u ∈ V and t ∈ [0 , T ] with ( ei)i∈N a fixed orthonormal system of L2(Ω). Note that 

K†u is Bochner measurable for u ∈ V by Pettis theorem since L2(Ω) is separable and for w ∈ L2(Ω) the map [0 , T ] ∋ t 7 → ⟨ [K†u]( t), w ⟩ = ⟨w, u (t)⟩ is Lebesgue measurable (by Bochner measurability of u). Well-definedness follows from Bessel’s inequality. The reduced measurement operators are given for 1 ≤ j ≤ m − 1 and 

t ∈ [tj , t j+1 ) or j = m and t ∈ [tm, t m+1 ] by 

Km : V → Y , [Kmu]( t) = 

> m

X

> i=1

∆−1

> m

ˆ tj+1 

> tj

⟨ei, u (s)⟩ ds

!

ei (30) for u ∈ V . Here 0 = t1 < t 2 < · · · < t m < t m+1 = T is the m-equidistant grid on [0 , T ] for m ∈ [0 , T ] and ∆ m := T /m . Bochner measurability follows again by Pettis theorem since [0 , T ] ∋ t 7 → ⟨ [Kmu]( t), w ⟩ is a step function for w ∈ L2(Ω). We will verify shortly that the operator in (30) is in fact well defined. Now since 

Km is linear, weak-weak continuity is equivalent to continuity. Since ( ei)i is an orthonormal system we derive for t ∈ [tj , t j+1 ] that 

∥[Kmu]( t)∥2 

> L2(Ω)

= ∆ −2

> mm

X

> i=1

ˆ tj+1 

> tj

⟨ei, u (s)⟩ ds

!2

.

As a consequence, for u ∈ V it holds 

∥Kmu∥2 

> Y

=

ˆ T

> 0

∥[Kmu]( t)∥2 

> L2(Ω)

dt =

> m

X

> j=1

∆m∆−2

> mm

X

> i=1

ˆ tj+1 

> tj

⟨ei, u (s)⟩ ds

!2

which due to H¨ older’s inequality using ∆ m = tj+1 − tj implies that 

∥Kmu∥2 

> Y

≤

> m

X

> j=1

∆−1

> mm

X

> i=1

∆m

ˆ tj+1 

> tj

|⟨ ei, u (s)⟩| 2 ds =

> m

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

|⟨ ei, u (s)⟩| 2

!

ds. 

By employing Bessel’s inequality and the embedding V ,→ Y we derive 

∥Kmu∥2 

> Y

≤

> m

X

> j=1

ˆ tj+1 

> tj

∥u(s)∥2 

> L2(Ω)

ds =

ˆ T

> 0

∥u(s)∥2 

> L2(Ω)

ds = ∥u∥2 

> Y

≤ c∥u∥2

> V

31 for some suitable c > 0 proving continuity of Km and more importantly well definedness of the operator Km. It remains to show (5) that Kmum − K†um → 0in Y as m → ∞ for any weakly convergent sequence ( um)m ⊂ V . For that, let 

um ⇀ u in V as m → ∞ . Then, similar transformations as above yield 

∥Kmum − K†um∥2

> Y

=

> m

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

∆−1

> m

ˆ tj+1 

> tj

⟨ei, u m(s)⟩ ds

!

ei − X

> i∈N

⟨ei, u m(t)⟩ei

> 2
> L2(Ω)

dt. 

Again using that ( ei)i is an orthonormal system gives 

> m

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

∆−1

> m

ˆ tj+1 

> tj

⟨ei, u m(s)⟩ ds

!

ei − X

> i∈N

⟨ei, u m(t)⟩ei

> 2
> L2(Ω)

dt

=

> m

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

⟨ei, u m(t)⟩ − ∆−1

> m

ˆ tj+1 

> tj

⟨ei, u m(s)⟩ ds

!

ei

> 2
> L2(Ω)

dt

+

> m

X

> j=1

ˆ tj+1 

> tj

X

> i≥m+1

⟨ei, u m(t)⟩ei

> 2
> L2(Ω)

dt. (31) We argue that the right-hand side of (31) converges to zero as m → ∞ . For the second term on the right-hand side of (31) we derive by previous arguments 

> m

X

> j=1

ˆ tj+1 

> tj

X

> i≥m+1

⟨ei, u m(t)⟩ei

> 2
> L2(Ω)

dt =

ˆ T

> 0

X

> i≥m+1

⟨ei, u m(t)⟩ei

> 2
> L2(Ω)

dt

≤ 2

ˆ T

> 0

X

> i≥m+1

⟨ei, u m(t) − u(t)⟩ei

> 2
> L2(Ω)

dt + 2 

ˆ T

> 0

X

> i≥m+1

⟨ei, u (t)⟩ei

> 2
> L2(Ω)

dt

= 2 

ˆ T

> 0

X

> i≥m+1

|⟨ ei, u m(t) − u(t)⟩| 2 dt

| {z }

> =:I

+2 

ˆ T

> 0

X

> i≥m+1

|⟨ ei, u (t)⟩| 2 dt

| {z }

> =:II

.

The term I can be estimated using Bessel’s inequality by I ≤

ˆ T

> 0

∥um(t) − u(t)∥2 

> L2(Ω)

dt = ∥um − u∥2

> Y

which converges to zero as m → ∞ due to the compact embedding V ,→→ Y . For term II note that due to Bessel’s inequality, t 7 → P 

> i≥m+1

|⟨ ei, u (t)⟩| 2 is majorized by t 7 → ∥ u(t)∥2 

> L2(Ω)

which is integrable on [0 , T ] by V ,→ Y . As a consequence, 32 since P 

> i∈N

|⟨ ei, u (t)⟩| 2 is convergent by Parseval’s identity, its tails converge to zero, such that with Lebesgue’s dominated convergence we recover convergence of the term II to zero as m → ∞ . It remains to verify that the first term on the right-hand side of (31) converges to zero as m → ∞ . It can be rewritten by 

> m

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

⟨ei, u m(t)⟩ − ∆−1

> m

ˆ tj+1 

> tj

⟨ei, u m(s)⟩ ds

!2

dt

= ∆ −2

> mm

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

ˆ tj+1 

> tj

⟨ei, u m(t) − um(s)⟩ ds

!2

dt. (32) Using that for t, s ∈ [0 , T ] and i ∈ N it holds 

⟨ei, u m(t) − um(s)⟩ = ⟨ei, u m(t) − u(t)⟩ + ⟨ei, u (t) − u(s)⟩ + ⟨ei, u (s) − um(s)⟩

we can estimate (32), employing the scalar H¨ older inequality by 3∆ −2

> mm

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

ˆ tj+1 

> tj

⟨ei, u m(t) − u(t)⟩ ds

!2

dt

+ 3∆ −2

> mm

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

ˆ tj+1 

> tj

⟨ei, u (t) − u(s)⟩ ds

!2

dt

+ 3∆ −2

> mm

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

ˆ tj+1 

> tj

⟨ei, u (s) − um(s)⟩ ds

!2

dt. (33) The third summand in (33), omitting the constant factor, can be estimated by H¨ older’s inequality regarding temporal integration in s by ∆−1

> mm

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

ˆ tj+1 

> tj

|⟨ ei, u m(s) − um(s)⟩| 2 ds dt, 

which by Bessel’s inequality and ∆ −1

> m

´ tj+1  

> tj

dt = 1 is bounded by 

> m

X

> j=1

ˆ tj+1 

> tj

∥u(s) − um(s)∥2 

> L2(Ω)

ds = ∥um − u∥2 

> Y

(34) and converges to zero as m → ∞ due to V ,→→ Y . Convergence of the first summand in (33) to zero can be argued analogously. It remains to show that lim  

> m→∞

∆−2

> mm

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

ˆ tj+1 

> tj

⟨ei, u (t) − u(s)⟩ ds

!2

dt = 0 .

33 Applying the integration-by-parts formula of [79, Lemma 7.3] with H = V = L2(Ω) (which is justified since V attains at least H1(Ω) spatial regularity), we infer that, 

⟨ei, u (t) − u(s)⟩ =

ˆ ts

⟨ei, ∂ tu(z)⟩ dz, 

for every i ∈ N. With this, Bessel’s and twice H¨ older’s inequality we derive that the second summand in (33) can be estimated by ∆−1

> mm

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

ˆ tj+1 

> tj

|⟨ ei, u (t) − u(s)⟩| 2 ds dt

≤ ∆−1

> mm

X

> j=1

ˆ tj+1 

> tj
> m

X

> i=1

ˆ tj+1 

> tj

|t − s|

ˆ ts

|⟨ ei, ∂ tu(z)⟩| 2 dz ds dt

≤ ∆−1

> mm

X

> j=1

ˆ tj+1 

> tj

ˆ tj+1 

> tj

|t − s|

ˆ ts

∥∂tu(z)∥2 dz ds dt. 

Using that |t − s| ≤ ∆m we can estimate this term by 

> m

X

> j=1

ˆ tj+1 

> tj

ˆ tj+1 

> tj

ds dt

!

∥∂tu∥2 

> Y

= ∆ mT ∥∂tu∥2

> Y

which converges to zero as m → ∞ (since ∆ m = T /m does). This finally con-cludes the regularity property (5) for K† as in (29) and the reduced measurement operators Km as in (30). 

# References 

[1] Christian Aarset, Martin Holler, and Tram Thi Ngoc Nguyen. Learning-informed parameter identification in nonlinear time-dependent PDEs. Applied Mathematics & Optimization ,88(3), August 2023. doi:10.1007/s00245-023-10044-y .[2] Robert Acar. Identification of the coefficient in elliptic equations. SIAM Journal on Control and Optimization , 31(5):1221–1244, September 1993. doi:10.1137/0331058 .[3] Robert A. Adams and John J. F. Fournier. Sobolev Spaces . Elsevier, Amsterdam, 2003. [4] Giovanni Alessandrini. An identification problem for an elliptic equation in two variables. 

Annali di Matematica Pura ed Applicata , 145(1):265–295, December 1986. doi:10.1007/ bf01790543 .[5] Dimitrios Angelis, Filippos Sofos, and Theodoros E. Karakasidis. Artificial intelligence in physical sciences: Symbolic regression trends and perspectives. Archives of Computational Methods in Engineering , 30(6):3845–3865, Jul 2023. doi:10.1007/s11831-023-09922-z .

34 [6] D.A. Augusto and H.J.C. Barbosa. Symbolic regression via genetic programming. In Pro-ceedings. Vol.1. Sixth Brazilian Symposium on Neural Networks , SBRN-00. IEEE Comput. Soc, 2000. doi:10.1109/sbrn.2000.889734 .[7] Kamyar Azizzadenesheli, Nikola Kovachki, Zongyi Li, Miguel Liu-Schiaffini, Jean Kossaifi, and Anima Anandkumar. Neural operators for accelerating scientific simulations and design. 

Nature Reviews Physics , 6(5):320–328, April 2024. doi:10.1038/s42254-024-00712-5 .[8] H. T. Banks and K. Kunisch. Estimation Techniques for Distributed Parameter Systems .Birkh¨ auser Boston, 1989. doi:10.1007/978-1-4612-3700-6 .[9] G. K. Batchelor. An Introduction to Fluid Dynamics . Cambridge Mathematical Library. Cambridge University Press, 2000. doi:10.1017/CBO9780511800955 .[10] R. Bellman and K.J. ˚ Astr¨ om. On structural identifiability. Mathematical Biosciences ,7(3–4):329–339, April 1970. doi:10.1016/0025-5564(70)90132-x .[11] Peter Benner, Serkan Gugercin, and Karen Willcox. A survey of projection-based model re-duction methods for parametric dynamical systems. SIAM Review , 57(4):483–531, January 2015. doi:10.1137/130932715 .[12] Kaushik Bhattacharya, Bamdad Hosseini, Nikola B. Kovachki, and Andrew M. Stuart. Model reduction and neural networks for parametric PDEs. The SMAI Journal of compu-tational mathematics , 7:121–157, July 2021. doi:10.5802/smai-jcm.74 .[13] L. Biggio, T. Bendinelli, A. Neitz, A. Lucchi, and G. Parascandolo. Neural symbolic re-gression that scales. In Proceedings of 38th International Conference on Machine Learning (ICML 2021) , volume 139 of Proceedings of Machine Learning Research , pages 936–945. PMLR, July 2021. URL: https://proceedings.mlr.press/v139/biggio21a.html .[14] Nicolas Boull´ e, Yuji Nakatsukasa, and Alex Townsend. Rational neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages 14243–14253. Curran Associates, Inc., 2020. URL: https://proceedings.neurips.cc/paper_files/paper/2020/file/ a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf .[15] Nicolas Boull´ e and Alex Townsend. Chapter 3 - A mathematical guide to operator learning. In Siddhartha Mishra and Alex Townsend, editors, Numerical Analysis Meets Machine Learning , volume 25 of Handbook of Numerical Analysis , pages 83–125. Elsevier, 2024. 

doi:10.1016/bs.hna.2024.05.003 .[16] Steven L. Brunton and J. Nathan Kutz. Promising directions of machine learning for partial differential equations. Nature Computational Science , 4(7):483–494, June 2024. doi: 10.1038/s43588-024-00643-2 .[17] Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equa-tions from data by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of Sciences , 113(15):3932–3937, 2016. doi:10.1073/pnas.1517384113 .[18] Alexander L. Bukhgeim and Gunther Uhlmann. Recovering a potential from partial Cauchy data. Communications in Partial Differential Equations , 27(3–4):653–668, January 2002. 

doi:10.1081/pde-120002868 .[19] J. R. Cannon and Paul DuChateau. An inverse problem for a nonlinear diffusion equa-tion. SIAM Journal on Applied Mathematics , 39(2):272–289, October 1980. doi:10.1137/ 0139024 .

35 [20] Cecilia Casolo, S¨ oren Becker, and Niki Kilbertus. Identifiability challenges in sparse linear ordinary differential equations. ArXiv preprint arXiv:2506.09816 , 2025. doi:10.48550/ arXiv.2506.09816 .[21] Constantin Christof and Julia Kowalczyk. On the identification and optimization of nons-mooth superposition operators in semilinear elliptic PDEs. ESAIM: Control, Optimisation and Calculus of Variations , 30:16, 2024. doi:10.1051/cocv/2023091 .[22] C. Cobelli and J. J. DiStefano. Parameter and structural identifiability concepts and am-biguities: a critical review and analysis. American Journal of Physiology-Regulatory, In-tegrative and Comparative Physiology , 239(1):R7–R24, July 1980. doi:10.1152/ajpregu. 1980.239.1.r7 .[23] Cristina Cornelio, Sanjeeb Dash, Vernon Austel, Tyler R. Josephson, Joao Goncalves, Ken-neth L. Clarkson, Nimrod Megiddo, Bachir El Khadir, and Lior Horesh. Combining data and theory for derivable scientific discovery with AI-Descartes. Nature Communications ,14(1), April 2023. doi:10.1038/s41467-023-37236-y .[24] S´ ebastien Court and Karl Kunisch. Design of the monodomain model by artificial neural networks. Discrete and Continuous Dynamical Systems , 42(12):6031–6061, 2022. doi: 10.3934/dcds.2022137 .[25] Miles Cranmer. Interpretable machine learning for science with PySR and SymbolicRegres-sion.jl. ArXiv preprint arXiv:2305.01582 , 2023. doi:10.48550/arXiv.2305.01582 .[26] Tim De Ryck and Siddhartha Mishra. Numerical analysis of physics-informed neural net-works and related models in physics-informed machine learning. Acta Numerica , 33:633–713, July 2024. doi:10.1017/s0962492923000089 .[27] J. DiStefano and C. Cobelli. On parameter and structural identifiability: Nonunique observability/reconstructibility for identifiable systems, other ambiguities, and new defi-nitions. IEEE Transactions on Automatic Control , 25(4):830–833, August 1980. doi: 10.1109/tac.1980.1102439 .[28] Guozhi Dong, Michael Hinterm¨ uller, and Kostas Papafitsoros. Optimization with learning-informed differential equation constraints and its applications. ESAIM: Control, Optimisa-tion and Calculus of Variations , 28:3, 2022. doi:10.1051/cocv/2021100 .[29] Guozhi Dong, Michael Hinterm¨ uller, and Kostas Papafitsoros. A descent algorithm for the optimal control of ReLU neural network informed PDEs based on approximate directional derivatives. SIAM Journal on Optimization , 34(3):2314–2349, July 2024. doi:10.1137/ 22m1534420 .[30] Guozhi Dong, Michael Hinterm¨ uller, Kostas Papafitsoros, and Kathrin V¨ olkner. First-order conditions for the optimal control of learning-informed nonsmooth PDEs. Numerical Functional Analysis and Optimization , 46(7):505–539, April 2025. doi:10.1080/01630563. 2025.2488796 .[31] Paul DuChateau and William Rundell. Unicity in an inverse problem for an unknown reac-tion term in a reaction-diffusion equation. Journal of Differential Equations , 59(2):155–164, September 1985. doi:10.1016/0022-0396(85)90152-4 .[32] Herbert Egger, Jan-Frederik Pietschmann, and Matthias Schlottbom. Identification of non-linear heat conduction laws. Journal of Inverse and Ill-posed Problems , 23(5):429–437, December 2014. doi:10.1515/jiip-2014-0030 .

36 [33] Heinz W Engl, Martin Hanke, and Gunther Neubauer. Regularization of Inverse Problems .Mathematics and Its Applications. Springer, Dordrecht, Netherlands, 1996. [34] Lawrence C. Evans. Partial Differential Equations . American Mathematical Society, Hei-delberg, 2010. [35] Ali Feizmohammadi, Yavar Kian, and Gunther Uhlmann. Partial data inverse problems for reaction-diffusion and heat equations. ArXiv preprint arXiv:2406.01387 , 2024. doi: 10.48550/ARXIV.2406.01387 .[36] Craig R. Gin, Daniel E. Shea, Steven L. Brunton, and J. Nathan Kutz. DeepGreen: Deep learning of Green’s functions for nonlinear boundary value problems. Scientific Reports ,11(1), November 2021. doi:10.1038/s41598-021-00773-x .[37] David J. Griffiths and Darrell F. Schroeter. Introduction to Quantum Mechanics . Cambridge University Press, Cambridge, 2018. doi:10.1017/9781316995433 .[38] Hillary Hauger, Philipp Scholl, and Gitta Kutyniok. Robust identifiability for symbolic recovery of differential equations. In ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 1–5, 2025. doi:10.1109/ ICASSP49660.2025.10887720 .[39] Niklas Heim, Tom´ aˇ s Pevn´ y, and V´ aclav ˇSm´ ıdl. Neural power units. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neu-ral Information Processing Systems , volume 33, pages 6573–6583. Curran Associates, Inc., 2020. URL: https://proceedings.neurips.cc/paper_files/paper/2020/file/ 48e59000d7dfcf6c1d96ce4a603ed738-Paper.pdf .[40] Michael Hinze, Rene Pinnau, Michael Ulbrich, and Stefan Ulbrich. Optimization with PDE Constraints . Springer Dordrecht, 2009. doi:10.1007/978-1-4020-8839-1 .[41] Martin Holler and Erion Morina. On the growth of the parameters of a class of approximat-ing ReLU neural networks. ArXiv preprint arXiv:2406.14936 , 2024. doi:10.48550/arXiv. 2406.14936 .[42] Martin Holler and Erion Morina. On uniqueness in structured model learning. ArXiv preprint arXiv:2410.22009 , 2024. doi:10.48550/arXiv.2410.22009 .[43] Martin Holler and Erion Morina. C1-approximation with rational functions and rational neural networks. ArXiv preprint arXiv:2508.19672 , 2025. doi:10.48550/arXiv.2508. 19672 .[44] Martin Holler and Erion Morina. Physically consistent model learning for reaction-diffusion systems. ArXiv preprint arXiv:2512.14240 , 2025. doi:10.48550/arXiv.2512.14240 .[45] Samuel Holt, Zhaozhi Qian, and Mihaela van der Schaar. Deep generative symbolic regres-sion. In The Eleventh International Conference on Learning Representations , 2023. URL: 

https://arxiv.org/abs/2401.00282 .[46] V. Isakov. On uniqueness in inverse problems for semilinear parabolic equations. Archive for Rational Mechanics and Analysis , 124(1):1–12, 1993. doi:10.1007/bf00392201 .[47] Nathan Jacobson. Basic Algebra I - Second Edition . Courier Corporation, New York, 2009. [48] Daijun Jiang, Yikan Liu, and Masahiro Yamamoto. Inverse source problem for the hy-perbolic equation with a time-dependent principal part. Journal of Differential Equations ,262(1):653–681, January 2017. doi:10.1016/j.jde.2016.09.036 .

37 [49] Jari P. Kaipio and Erkki Somersalo. Statistical and Computational Inverse Problems .Springer New York, 2005. doi:10.1007/b138659 .[50] Barbara Kaltenbacher. Regularization based on all-at-once formulations for inverse prob-lems. SIAM Journal on Numerical Analysis , 54(4):2594–2618, 2016. doi:10.1137/ 16M1060984 .[51] Barbara Kaltenbacher and Tram T. N. Nguyen. Discretization of parameter identification in PDEs using neural networks. Inverse Problems , 38(12):124007, 2022. doi:10.1088/ 1361-6420/ac9c25 .[52] Barbara Kaltenbacher and William Rundell. The inverse problem of reconstructing re-action–diffusion systems. Inverse Problems , 36(6):065011, May 2020. doi:10.1088/ 1361-6420/ab8483 .[53] Barbara Kaltenbacher and William Rundell. On the simultaneous recovery of the conductiv-ity and the nonlinear reaction term in a parabolic equation. Inverse Problems and Imaging ,14(5):939–966, 2020. doi:10.3934/ipi.2020043 .[54] Barbara Kaltenbacher and William Rundell. On uniqueness and reconstruction of a non-linear diffusion term in a parabolic equation. Journal of Mathematical Analysis and Appli-cations , 500(2):125145, August 2021. doi:10.1016/j.jmaa.2021.125145 .[55] Barbara Kaltenbacher and William Rundell. Reconstruction of space-dependence and non-linearity of a reaction term in a subdiffusion equation. Inverse Problems , 41(5):055008, April 2025. doi:10.1088/1361-6420/adcb67 .[56] Pierre-alexandre Kamienny, St´ ephane d 'Ascoli, Guillaume Lample, and Francois Char-ton. End-to-end symbolic regression with transformers. In S. Koyejo, S. Mo-hamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neu-ral Information Processing Systems , volume 35, pages 10269–10281. Curran Associates, Inc., 2022. URL: https://proceedings.neurips.cc/paper_files/paper/2022/file/ 42eb37cdbefd7abae0835f4b67548c39-Paper-Conference.pdf .[57] Yavar Kian. Lipschitz and H¨ older stable determination of nonlinear terms for elliptic equa-tions. Nonlinearity , 36(2):1302–1322, January 2023. doi:10.1088/1361-6544/acafcd .[58] Yavar Kian and Gunther Uhlmann. Recovery of nonlinear terms for reaction diffusion equations from boundary measurements. Archive for Rational Mechanics and Analysis ,247(1), January 2023. doi:10.1007/s00205-022-01831-y .[59] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 

Proceedings of the 3rd International Conference on Learning Representations (ICLR) , 2015. URL: https://arxiv.org/abs/1412.6980 .[60] Michael V. Klibanov. Carleman estimates for global uniqueness, stability and numerical methods for coefficient inverse problems. jiip , 21(4):477–560, August 2013. doi:10.1515/ jip-2012-0072 .[61] Ian Knowles. Parameter identification for elliptic problems. Journal of Computational and Applied Mathematics , 131(1–2):175–194, June 2001. doi:10.1016/s0377-0427(00) 00275-2 .[62] William La Cava, Bogdan Burlacu, Marco Virgolin, Michael Kommenda, Pa-tryk Orzechowski, Fabr´ ıcio Olivetti de Fran¸ ca, Ying Jin, and Jason H Moore. 

38 Contemporary symbolic regression methods and their relative performance. Ad-vances in Neural Information Processing Systems , 2021(DB1):1–16, 2021. URL: 

https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/ file/c0c7c76d30bd3dcaefc96f40275bdc0a-Paper-round1.pdf .[63] Christoph Lampert and Georg Martius. Extrapolation and learning equations. In 5th International Conference on Learning Representations , ICLR 2017 - Workshop Track Pro-ceedings. International Conference on Learning Representations, 24–26 Apr 2017. URL: 

https://arxiv.org/pdf/1610.02995 .[64] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. ArXiv preprint arXiv:2010.08895 , 2020. doi:10.48550/arXiv.2010. 08895 .[65] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via DeepONet based on the universal approximation theo-rem of operators. Nature Machine Intelligence , 3(3):218–229, March 2021. doi:10.1038/ s42256-021-00302-5 .[66] Hongyu Miao, Xiaohua Xia, Alan S. Perelson, and Hulin Wu. On identifiability of nonlinear ode models and applications in viral dynamics. SIAM Review , 53(1):3–39, January 2011. 

doi:10.1137/090757009 .[67] Terrell Mundhenk, Mikel Landajuela, Ruben Glatt, Claudio P Santiago, Daniel faissol, and Brenden K Petersen. Symbolic regression via deep reinforcement learning enhanced genetic programming seeding. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wort-man Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 24912–24923. Curran Associates, Inc., 2021. URL: https://proceedings.neurips. cc/paper_files/paper/2021/file/d073bb8d0c47f317dd39de9c9f004e9d-Paper.pdf .[68] J.D. Murray. Mathematical Biology: I. An Introduction . Springer New York, 2002. doi: 10.1007/b98868 .[69] J.D. Murray. Mathematical Biology: II: Spatial Models and Biomedical Applications .Springer New York, 2003. doi:10.1007/b98869 .[70] Adrian I. Nachman. Global uniqueness for a two-dimensional inverse boundary value prob-lem. The Annals of Mathematics , 143(1):71, January 1996. doi:10.2307/2118653 .[71] Derick Nganyu Tanyu, Jianfeng Ning, Tom Freudenberg, Nick Heilenk¨ otter, Andreas Rademacher, Uwe Iben, and Peter Maass. Deep learning methods for partial differential equations and related parameter identification problems. Inverse Problems , 39(10):103001, August 2023. doi:10.1088/1361-6420/ace9d4 .[72] Tram Thi Ngoc Nguyen. Sequential bi-level regularized inversion with application to hid-den reaction law discovery. Inverse Problems , 41(6):065015, June 2025. doi:10.1088/ 1361-6420/addf73 .[73] Benoˆ ıt Perthame. Parabolic Equations in Biology - Growth, reaction, movement and diffu-sion . Springer, Berlin, Heidelberg, 2015. doi:10.1007/978-3-319-19500-1 .[74] Brenden K. Petersen, Mikel Landajuela, T. Nathan Mundhenk, Claudio P. Santiago, Soo K. Kim, and Joanne T. Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. In The Ninth International Conference on Learning Representations , 2021. URL: https://arxiv.org/pdf/1912.04871 .

39 [75] P. P. Petrushev and Vasil Atanasov Popov. Rational Approximation of Real Functions .Encyclopedia of Mathematics and its Applications. Cambridge University Press, 1988. doi: 10.1017/CBO9781107340756 .[76] Markus Quade, Markus Abel, Kamran Shafi, Robert K. Niven, and Bernd R. Noack. Pre-diction of dynamical systems by symbolic regression. Phys. Rev. E , 94:012214, Jul 2016. 

doi:10.1103/PhysRevE.94.012214 .[77] Konstantin Riedl, Justin Sirignano, and Konstantinos Spiliopoulos. Global convergence of adjoint-optimized neural PDEs. ArXiv preprint arXiv:2506.13633 , 2025. doi:10.48550/ arXiv.2506.13633 .[78] Arnd R¨ osch. Stability estimates for the identification of nonlinear heat transfer laws. Inverse Problems , 12(5):743–756, October 1996. doi:10.1088/0266-5611/12/5/015 .[79] Tom´ aˇ s Roub´ ıˇ cek. Nonlinear Partial Differential Equations with Applications . Springer Basel, December 2012. doi:10.1007/978-3-0348-0513-1 .[80] Samuel H. Rudy, Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Data-driven discovery of partial differential equations. Science Advances , 3(4):e1602614, 2017. doi: 10.1126/sciadv.1602614 .[81] Subham Sahoo, Christoph Lampert, and Georg Martius. Learning equations for extrap-olation and control. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine Learn-ing Research , pages 4442–4450. PMLR, 10–15 Jul 2018. URL: https://proceedings.mlr. press/v80/sahoo18a.html .[82] Peter J. Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of Fluid Mechanics , 656:5–28, July 2010. doi:10.1017/s0022112010001217 .[83] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. 

Science , 324(5923):81–85, 2009. doi:10.1126/science.1165893 .[84] Michael D. Schmidt and Hod Lipson. Age-fitness Pareto optimization. In Proceedings of the 12th annual conference on Genetic and evolutionary computation , GECCO ’10, page 543–544. ACM, July 2010. doi:10.1145/1830483.1830584 .[85] Philipp Scholl, Aras Bacho, Holger Boche, and Gitta Kutyniok. Symbolic recovery of dif-ferential equations: The identifiability problem. ArXiv preprint arXiv:2210.08342 , 2023. 

doi:10.48550/arXiv.2210.08342 .[86] Philipp Scholl, Aras Bacho, Holger Boche, and Gitta Kutyniok. The uniqueness prob-lem of physical law learning. In ICASSP 2023 - 2023 IEEE International Confer-ence on Acoustics, Speech and Signal Processing (ICASSP) , page 1–5. IEEE, June 2023. 

doi:10.1109/icassp49357.2023.10095017 .[87] Philipp Scholl, Katharina Bieker, Hillary Hauger, and Gitta Kutyniok. ParFam-(neural guided) symbolic regression via continuous global optimiza-tion. In The Thirteenth International Conference on Learning Representations ,2025. URL: https://proceedings.iclr.cc/paper_files/paper/2025/file/ ca98452d4e9ecbc18c40da2aa0da8b98-Paper-Conference.pdf .[88] Zakhar Shumaylov, Peter Zaika, Philipp Scholl, Gitta Kutyniok, Lior Horesh, and Carola-Bibiane Sch¨ onlieb. When is a system discoverable from data? Discovery requires chaos. 

ArXiv preprint arXiv:2511.08860 , 2025. doi:10.48550/arXiv.2511.08860 .

40 [89] Justin Sirignano, Jonathan MacArt, and Konstantinos Spiliopoulos. PDE-constrained mod-els with neural network terms: Optimization and global convergence. Journal of Computa-tional Physics , 481:112016, May 2023. doi:10.1016/j.jcp.2023.112016 .[90] A. M. Stuart. Inverse problems: A Bayesian perspective. Acta Numerica , 19:451–559, May 2010. doi:10.1017/s0962492910000061 .[91] Fangzheng Sun, Yang Liu, Jian-Xun Wang, and Hao Sun. Symbolic physics learner: Discov-ering governing equations via Monte Carlo tree search. In The Twelfth International Con-ference on Learning Representations , 2023. URL: https://arxiv.org/abs/2205.13134 .[92] Matus Telgarsky. Neural networks and rational functions. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning , vol-ume 70 of Proceedings of Machine Learning Research , pages 3387–3393. PMLR, 06–11 Aug 2017. URL: https://proceedings.mlr.press/v70/telgarsky17a.html .[93] Alan M. Turing. The chemical basis of morphogenesis. Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences , 237(641):37–72, 1952. doi:10.1007/ BF02459572 .[94] Silviu-Marian Udrescu and Max Tegmark. AI Feynman: A physics-inspired method for symbolic regression. Science Advances , 6(16), April 2020. doi:10.1126/sciadv.aay2631 .[95] Geoffrey K. Vallis. Atmospheric and Oceanic Fluid Dynamics . Cambridge University Press, Cambridge, 2017. doi:10.1017/9781107588417 .[96] David J. Wales and Jonathan P. K. Doye. Global optimization by basin-hopping and the lowest energy structures of Lennard-Jones clusters containing up to 110 atoms. The Journal of Physical Chemistry A , 101(28):5111–5116, July 1997. doi:10.1021/jp970984n .[97] M Yamamoto. Stability, reconstruction formula and regularization for an inverse source hyperbolic problem by a control method. Inverse Problems , 11(2):481–496, April 1995. 

doi:10.1088/0266-5611/11/2/013 .

41