Title: FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery

URL Source: https://arxiv.org/pdf/2602.14670v1

Published Time: Tue, 17 Feb 2026 02:51:29 GMT

Number of Pages: 20

Markdown Content:
# FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery 

Yanlong Wang 

Tsinghua University China wangyanl21@mails.tsinghua.edu.cn 

Jian Xu 

Tsinghua University China xujian20@mails.tsinghua.edu.cn 

Hongkang Zhang 

Tsinghua University China zhanghk21@mails.tsinghua.edu.cn 

Shao-Lun Huang 

Tsinghua University China shaolun.huang@sz.tsinghua.edu.cn 

Danny Dongning Sun 

Peng Cheng Laboratory China ds316@columbia.edu 

Xiao-Ping Zhang 

Tsinghua University China xpzhang@ieee.org 

Abstract 

Formulaic alpha factor mining is a critical yet challenging task in quantitative investment, characterized by a vast search space and the need for domain-informed, interpretable signals. However, find-ing novel signals becomes increasingly difficult as the library grows due to high redundancy. We propose FactorMiner , a lightweight and flexible self-evolving agent framework designed to navigate this complex landscape through continuous knowledge accumula-tion. FactorMiner combines a Modular Skill Architecture that encapsulates systematic financial evaluation into executable tools with a structured Experience Memory that distills historical min-ing trials into actionable insights (successful patterns and failure constraints). By instantiating the Ralph Loop paradigm‚Äîretrieve, generate, evaluate, and distill‚ÄîFactorMiner iteratively uses mem-ory priors to guide exploration, reducing redundant search while focusing on promising directions. Experiments on multiple datasets across different assets and markets show that FactorMiner con-structs a diverse library of high-quality factors with competitive performance, while maintaining low redundancy among factors as the library scales. Overall, FactorMiner provides a practical ap-proach to scalable discovery of interpretable formulaic alpha factors under the "Correlation Red Sea" constraint. 

CCS Concepts 

‚Ä¢ Applied computing ‚Üí Finance ; ‚Ä¢ Computing methodologies 

‚Üí Learning from demonstrations ; ‚Ä¢ Human-centered computing 

‚Üí Natural language interfaces .

Keywords 

Quantitative Investment, Formulaic Alpha, Factor Mining Agent Skill, Experience Memory, Parallel Evaluation, Intraday Prediction  

> Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
> Preprint,
> ¬©2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. https://doi.org/XXXXXXX.XXXXXXX

ACM Reference Format: 

Yanlong Wang, Jian Xu, Hongkang Zhang, Shao-Lun Huang, Danny Dongn-ing Sun, and Xiao-Ping Zhang. 2026. FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery. In 

Preprint. ACM, New York, NY, USA, 20 pages. https://doi.org/XXXXXXX. XXXXXXX 

1 Introduction 

Discovering predictive alpha factors is central to quantitative trad-ing and portfolio construction. In practice, automated factor dis-covery faces three fundamental challenges: (i) vast search com-plexity ‚Äîthe space of formulaic expressions grows combinatorially with operator compositions and parameters; (ii) poor knowledge accumulation ‚Äîtraditional search methods (genetic programming, reinforcement learning) fail to retain and reuse insights across ex-ploration sessions, leading to repetitive trials; (iii) interpretability constraints ‚Äîunlike black-box neural predictors, financial practition-ers require transparent, auditable formulas with explicit financial logic for regulatory compliance and risk management. Traditional approaches rely heavily on domain experts who man-ually craft factors based on financial intuition [ 3, 8]. More recently, machine learning methods have been applied to asset pricing [ 4, 6 ], demonstrating strong predictive power but often sacrificing inter-pretability. Genetic programming [ 10 ] and reinforcement learn-ing [ 31 , 32 ] offer automated search but suffer from knowledge forgetting: as the factor library grows and correlation constraints tighten, these methods lack mechanisms to accumulate structural knowledge about what works and what fails. Moreover, existing methods treat each mined factor in isolation without considering the global factor library perspective, they optimize individual factor quality but ignore how new factors interact with the existing library, leading to redundant discoveries and inefficient exploration. We study formulaic factor mining as a task for self-evolving AI agents [ 26 , 30 ]. Each factor is an explicit expression over market fields (e.g., close, volume, VWAP) composed from a library of 60+ operators. Unlike end-to-end neural approaches, formulaic factors enable human-in-the-loop auditing and compositional generaliza-tion across market regimes. The key research question is: how can an agent efficiently explore this vast program space while maintaining a global view of the factor library and autonomously accumulating structural knowledge across exploration sessions?  

> arXiv:2602.14670v1 [q-fin.TR] 16 Feb 2026 Preprint, , Wang et al.

We propose FactorMiner , a self-evolving agent framework that addresses these challenges through two synergistic mechanisms: a compositional skill architecture and experience memory. First, we design factor mining as a reusable agent skill that can be in-voked on-demand by a language model agent [ 23 , 30 ]. The skill encapsulates domain knowledge‚Äîa curated operator library with 60+ financial operators, a multi-stage validation pipeline with IC thresholds and correlation checks, and standardized evaluation protocols‚Äîenabling flexible task decomposition and independent upgrades without retraining the agent. Second, we introduce expe-rience memory that enables the agent to self-evolve through accu-mulated knowledge [ 18 , 26 ]. The memory stores distilled structural patterns from historical mining sessions: successful patterns (factor templates that consistently pass quality thresholds) and forbidden regions (factor families with high mutual correlation to existing library members). Crucially, this memory maintains a global factor library perspective: the agent decides mining directions by consid-ering how candidate factors complement the existing library, rather than optimizing individual factors in isolation. The agent adopts the Ralph Loop paradigm for self-evolution: retrieve relevant patterns from experience memory, generate can-didate factors by invoking the mining skill with retrieved priors, evaluate candidates through parallel validation, and distill outcomes back into memory. This creates a positive feedback cycle where each mining session improves future exploration efficiency, enabling the agent to continuously refine its search strategy. To ensure computational efficiency, we build a lightweight yet high-performance system leveraging GPU-accelerated operators, multi-process parallelization, and C-compiled efficient numerical operations. This yields significant speedups over standard Python implementations (e.g., NumPy/Pandas), making large-scale itera-tive evaluation computationally feasible. We summarize our main contributions as follows: 

‚Ä¢ Experience memory for agent-based factor mining. We in-troduce structured experience memory into agent-based factor discovery, enabling self-evolution through accumulated knowl-edge. The memory stores reusable patterns that guide explo-ration and reduce redundant search compared to memoryless baselines. 

‚Ä¢ Modular skill architecture for on-demand invocation. We design factor mining as a compositional, reusable skill that en-capsulates domain knowledge in a standalone module. This enables flexible agent invocation, independent skill upgrades, and clear separation between agent reasoning and skill execu-tion. 

‚Ä¢ Lightweight and efficient mining system. We build a high-performance factor evaluation engine using GPU accelerated operators, multi-process parallelization, and C-compiled effi-cient numerical operations, enabling large-scale factor mining. This supports rapid iteration and large-scale factor exploration while adapting to different server configurations and compute budgets. 

‚Ä¢ Global factor library perspective. We incorporate factor li-brary admission mechanisms into the mining loop, enabling the agent to make mining decisions from a global library perspec-tive. The agent considers how candidate factors complement the existing library, rather than optimizing individual factors in isolation. 

‚Ä¢ Open factor library with research and practical value. We provide 110 A-share equity factors with explicit formulaic ex-pressions, validated on real market data. These interpretable factors serve as diagnostic tools to understand cross-market behavioral anomalies and market microstructure inefficiencies, offering both research insights and practical value for quantita-tive trading. 

2 Related Work 

Our work sits at the intersection of two research directions: auto-mated alpha factor discovery in quantitative finance, and AI agents with skills and memory for autonomous task execution. We review key developments in both areas and position our contributions. 

2.1 Automated Alpha Factor Discovery 

Traditional factor discovery relies on domain expertise to manually craft interpretable signals. Kakushadze [8] presents 101 formulaic al-phas with explicit expressions and low mutual correlations (15.9%), demonstrating that hand-crafted factors can be both diverse and effective; Alpha191 extends this with 191 formulas covering addi-tional microstructure patterns. However, these classical approaches are limited to daily-frequency data, and despite explicit formulaic expressions, their economic interpretability remains opaque for complex nested structures. Consequently, high-frequency markets currently lack comparable curated factor libraries for systematic research. To automate factor discovery, evolutionary program search meth-ods such as genetic programming represent factor formulas as executable programs and evolve them via crossover and muta-tion [ 1, 10 , 14 ]. In practice, vanilla genetic programming can explore the expression space inefficiently, exhibiting slow convergence and limited semantic guidance because genetic operators primarily act on program syntax rather than program behavior [ 13 , 20 ]. More recently, machine learning methods have been applied to empiri-cal asset pricing and high-dimensional model selection [ 4 , 6, 11 ]. While these methods achieve strong predictive performance, they are often less transparent than formulaic signals and can be difficult to interpret or audit in high-stakes settings [22]. Reinforcement learning has been used to navigate the discrete space of formulaic factor expressions by treating evaluation met-rics (e.g., IC/ICIR) as rewards [ 31 , 32 ], often at the cost of addi-tional training and repeated evaluation overhead. In parallel, neural and LLM-driven frameworks generate and refine formulaic fac-tors with exploration strategies designed to mitigate decay and redundancy [ 25 , 28 ]. Despite these advances, these approaches still face the challenge of explicitly and persistently reusing structural patterns across mining sessions; as the factor library grows and re-dundancy/correlation constraints tighten, exploration can become increasingly repetitive [4, 11]. FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery Preprint, , DISTRILLATION (ùöø )

1.MEMORY 

RETRIEVAL 

5.MEMORY 

EVALUATION 

4.LIBRARY 

UPDATE 

RALPH LOOP: SELF -EVOLVING DISCOVERY 

CURRENT 

LIBRARY 

OPERATOR LIBRARY     

> Add TsRank IfElse Skew
> Corr EMA Delta

‚Ä¶            

> ùõº 1=ùëÅùëíùëî (ùëáùë†ùëÖùëéùëõùëò (ùëÜùëñùëîùëõùëíùëëùëÉùëúùë§ùëíùëü
> Ôºàùê∑ùëíùëôùë°ùëé ($ùëêùëôùëúùë†ùëí ,6),0.5),24 ))
> ùõº 2=ùëÅùëíùëî (ùëÄùë¢ùëô (ùê∂ùë†ùëÖùëéùëõùëò ($ùëüùëíùë°ùë¢ùëüùëõùë† ),
> ùê∂ùë†ùëÖùëéùëõùëò (ùëÜùë°ùëë ($ùëüùëíùë°ùë¢ùëüùëõùë† ,12 ))))
> ùõº 3=ùëÄùë¢ùëô (ùê∂ùë†ùëÖùëéùëõùëò (ùëÖùë†ùëûùë¢ùëéùëüùëí ($ùëêùëôùëúùë†ùëí ,
> 24 )),ùê∂ùë†ùëÖùëéùëõùëò (ùê∑ùëíùëôùë°ùëé ($ùëêùëôùëúùë†ùëí ,3)))

‚Ä¶  

> Stage 1:
> Fast IC
> Screening

ùêºùê∂ ‚â• ùúè ùëñùëê  max ùëù ùëê ùëñ , ùëÄ ‚â§ ùúÉ  int ùëíùëü ùëèùëéùë°ùëê‚Ñé ùúå ‚â§ ùúÉ             

> Stage 2:
> Correlation
> Check Stage 3:
> Batch
> Deduplication
> Stage 4:
> Full Validation
> ùëÖùëúùëèùë¢ùë†ùë°ùëõùëíùë†ùë†
> &ùëÇùëÇùëÜ ùëáùëíùë†ùë°
> ùê¥ùëôùëô ùëÜ ùëù ùê¥ùë†ùë†ùëíùë°ùë†
> (ùë£ùë† ‚Ñí1)
> ‚Ñ≥s‚äÜùëÜ ùëù
> ùê¥ùë†ùë†ùëíùë°ùë†
> ùê¥ùëùùëùùëüùëúùë£ùëíùëë

ùê∂ 1

> ùê¥ùëùùëùùëüùëúùë£ùëíùëë

ùê∂ 2  

> Candidate Factors ùê∂
> Batch
> Factors
> Replacement
> Check
> Discard
> Discard
> OPERATOR
> SKILL
> DEFINITION
> (Œ©)
> 1.CSI500
> 2.HS300
> 3.Crypto

MARKET 

DATA (ùíü )

2.AGENT SKILL: FACTORS GENERATION 

3.AGENT SKILL: MULTI -STAGE EVALUATION 

Replacement Check 

ùêº ùê∂ ‚â• ùúè ùëñ ùëê ùëñ 

> ‚Ñéùëñùëî‚Ñé

& ùêºùê∂ ‚â• ùëò ¬∑ IC ùëñ   

> Single Correlated Factor

‚Ä¶

‚Ä¶

Successful Patterns (ùìü ùíîùíñùíÑùíÜ ) Forbidden Regions (ùìü ùíáùíÇùíçùíç )

‚Ä¶

AGENT SKILL: EXPERIENCE MEMORY (ùìú )

> Validated
> Factors
> Combination

Selection        

> Trading
> Test
> DOWNSTREAM
> TASKS
> GPU
> GPU
> ACCELERATION
> CCOMPILATION
> &MULTIPROCESS
> COMPUTATION OPTIMIZATION
> High -moment
> Regimes (ùëÜùëòùëíùë§ /ùêæùë¢ùëüùë° )
> Trend Regression
> Adaptive (ùëÖ 2/ùëÜùëôùëúùëùùëí )
> Amount Efficiency
> Interaction

Expanded failed insights from memory file Expanded success insights from memory file       

> VWAP Deviation
> Variants
> Standardized
> Returns/Amount
> High Correlation
> Clusters (ùúå >ùúÉ ùëé ùëñ )

Memory Priors (ùìÇ )

Added to 

Memory 

Library State 

Validation Trajectories( ùúè )

LLM AGENT  

> Rejected Rejected

Figure 1: FactorMiner System Architecture. The Ralph Loop framework integrates three key components: (1) Experience Memory that stores successful patterns and forbidden regions from past mining sessions; (2) Agent Skill that encapsulates the multi-stage validation pipeline (IC screening, correlation checking, deduplication, and full validation); (3) Factor Library that grows dynamically while maintaining orthogonality constraints. The agent iteratively retrieves memory priors, generates candidates through the skill, and distills outcomes back into memory for improved future exploration. 

2.2 AI Agents with Skills and Memory 

Recent language-model agents shift from one-shot text generation to closed-loop task execution via tool calls and feedback. Tool-former [ 23 ] trains LLMs to insert API calls in a self-supervised man-ner, while ReAct [ 30 ] interleaves reasoning traces with actions‚Äîthe agent plans, invokes tools, observes outcomes, and iterates. This tool-augmented view motivates our skill-based design: factor min-ing is packaged as an executable skill that an agent can invoke on demand. Memory and self-improvement mechanisms further let agents ac-cumulate experience over time. Reflexion [ 26 ] uses language-based self-reflection to store lessons in natural language, and generative agents [ 18 ] maintain memory streams (e.g., observations, reflec-tions, plans) to condition future behavior; more broadly, continual learning studies how to retain knowledge without catastrophic forgetting [ 17 ]. In our setting, we instantiate memory for symbolic program synthesis: instead of storing dense representations or re-play buffers [ 2], we retain reusable symbolic rules and structural patterns discovered during factor mining, together with summary statistics that help avoid redundant regions of the search space. From a theoretical perspective, meta-learning and meta-RL for-malize "learning to learn" by extracting transferable learning biases from prior tasks or episodes [ 5, 7, 15 ]. Analogously, a memory-guided mining process can be seen as acquiring search priors: be-yond producing individual factors, the system accumulates struc-tural knowledge that shapes future exploration. Beyond learning to call tools, systems work has advocated modu-lar agent architectures that route subproblems to specialized models, external knowledge sources, and discrete reasoning modules (e.g., MRKL [ 9]; HuggingGPT [ 24 ]). In parallel, recent benchmarks and training pipelines study how to ground LLM outputs into executable API calls at scale and reduce tool hallucination, including API-Bank [ 12 ], ToolLLM/ToolBench [ 21 ], and Gorilla/APIBench [ 19 ]. These works collectively emphasize the importance of separating high-level planning from reliable execution, and of representing tools/skills in a form that supports retrieval and continual updates. Another line of research addresses long-horizon agent behavior under limited context windows by introducing explicit long-term memory managers. MemGPT [ 16 ] proposes OS-inspired hierarchi-cal memory with controlled paging between fast and slow memory, while Voyager [ 29 ] demonstrates an open-ended embodied agent that accumulates an executable skill library and reuses it to general-ize to new tasks. Such settings are often effectively non-stationary: as an agent acquires new tools/skills (or as an external library grows), the feasible action space and redundancy patterns evolve, motivating experience summarization mechanisms that capture reusable patterns and avoid repeatedly exploring known dead ends. Preprint, , Wang et al. 

3 Methodology 3.1 Problem Formulation 

We define the alpha factor discovery task over a universe of ùëÄ 

assets and a time horizon ùëá . The raw market input is represented by a tensor D ‚àà RùëÄ √óùëá √óùêπ , where each entry ùëë ( ùëó ) 

> ùëö,ùë°

denotes the value of feature ùëó (e.g., price, volume) for asset ùëö at time ùë° . Symbolic factor space. A symbolic alpha factor ùõº is a computational program composed from an operator set Œ© that transforms market states into a cross-sectional predictive signal sùë° ‚àà RùëÄ :

ùõº : D:,:ùë°, : ‚Üí sùë° , ùë† (ùëö ) 

> ùë°

= ùõº (dùëö, :ùë° ) (1) where ùë† (ùëö ) 

> ùë°

represents the predictive score for asset ùëö relative to its peers. Each operator ùëú ‚àà Œ© has a fixed arity and a typed signature (e.g., time-series ‚Üí time-series, cross-section ‚Üí cross-section), and programs ùõº ‚àà P ( Œ©) correspond to expression trees composed from 

Œ© with admissible parameters; detailed definitions and categories of Œ© are provided in Appendix A and Table 2. The effectiveness of ùõº is quantified by the Information Coeffi-cient (IC), defined as the cross-sectional Spearman rank correlation between the signal sùë° and subsequent returns rùë° +1 ‚àà RùëÄ :IC ùë° (ùõº ) = Corr rank 

 sùë° , rùë° +1

 (2) Consistency over time is measured by the Information Ratio (ICIR): ICIR (ùõº ) = ùúá (IC ùë° )/ ùúé (IC ùë° ).

Correlation metric. To enforce library diversity, we measure re-dundancy between two factors ùõº and ùõΩ by the time-average cross-sectional Spearman correlation of their realized signals: 

ùúå (ùõº, ùõΩ ) = 1

|T | ‚àëÔ∏Å ùë° ‚àà T 

Corr rank 

 sùë° (ùõº ), sùë° (ùõΩ ), (3) where T denotes the set of evaluation timestamps. This defini-tion matches our current implementation, but the framework can accommodate alternative dependence measures (e.g., time-series correlation, partial correlation, or non-linear dependence metrics) by replacing ùúå (¬∑ , ¬∑) accordingly. 

Objective: Orthogonal Library Synthesis. FactorMiner treats the problem as the iterative construction of a diverse factor library 

L = {ùõº 1, . . . , ùõº ùêæ }. The goal is to maximize the aggregate predictive quality of the library subject to a global redundancy constraint: 

L‚àó = arg max 

> L ‚äÇ P

‚àëÔ∏Å ùõº ‚àà L 

Œ¶(ùõº ) s.t. ‚àÄùõº ùëñ ‚â† ùõº ùëó ‚àà L : |ùúå (ùõº ùëñ , ùõº ùëó )| < ùúÉ 

(4) where P is the infinite space of constructible programs, Œ¶(¬∑) is a fitness metric, and ùúÉ is the correlation budget. 

The Correlation Red Sea. As the library L populates, the feasible region for new orthogonal factors‚Äî Porth = {ùõº ‚àà P :max ùëî ‚àà L |ùúå (ùõº, ùëî )| < ùúÉ }‚Äîshrinks rapidly as diversity constraints tighten. Standard search methods (e.g., GP or RL) often get trapped in this correlation red sea because they lack mechanisms to track explored regions or failure patterns. 

Decision-Theoretic Memory Formulation. To navigate the correlation red sea, we reformulate the discovery process as a se-quential decision task over an evolving internal knowledge state 

ùëÜ ùë° = (L ùë° , Mùë° ), where Mùë° represents the persistent experience memory. The agent first retrieves a context-dependent memory signal ùëö ùë° from Mùë° and Lùë° , and then samples candidates from a memory-conditioned policy: 

ùõº ‚àº ùúã (ùõº | ùëö ùë° ), ùëö ùë° = ùëÖ (M ùë° , Lùë° ) (5) The role of M is to induce a probability measure contraction over the program space P. By distilling historical trajectories ùúè =

{( ùõº ùëñ , ùëÖ ùëñ )} ùêµ ùëñ =1 into structured patterns, M shifts the sampling mass toward the orthogonal manifold Porth . Mathematically, the evolu-tion of memory is governed by a distillation operator Œ®:

Mùë° +1 = Œ®(M ùë° , ùúè ùë° ) (6) which updates the agent‚Äôs belief distribution so that future ex-ploration is steered toward higher-utility and lower-redundancy regions of P.

3.2 Factor Mining Skill Architecture 

Unlike traditional RL or monolithic agent approaches where domain logic is often hard-coded or entangled with the agent‚Äôs reasoning loop, FactorMiner adopts a modular skill-based architecture. In-spired by the tool-augmented paradigm [ 23 ], we encapsulate the entire factor mining process as a standalone, reusable Agent Skill‚Äîa standardized interface that exposes high-level capabilities to the LLM while abstracting away low-level execution details. The overall interaction pattern is illustrated in Figure 1. 

Compositional Design. The skill is structured as a hierarchical library of tools: 

‚Ä¢ Operator Layer : A curated set of 60+ financial operators (e.g., 

TsRank , Rsquare ) implemented with GPU-accelerated backends. This ensures that the agent‚Äôs symbolic proposals are executable and computationally efficient. 

‚Ä¢ Validation Pipeline : A rigorous, standardized protocol for fac-tor assessment ( check_ic ‚Üí check_correlation ‚Üí admit ). By decoupling validation from generation, we ensure that the agent‚Äôs "creativity" is bounded by strict quantitative constraints, preventing hallucinated or scientifically invalid discoveries. 

Advantages of the Skill-Based Approach. This modular de-sign offers three distinct advantages over monolithic architectures: (1) Prevention of Calculation Hallucination : Large language models often struggle with precise arithmetic and algorith-mic execution. By offloading the evaluation to a determin-istic, code-based skill, FactorMiner eliminates the risk of "hallucinated metrics," ensuring that every reported IC and Sharpe ratio is mathematically rigorous. (2) Cross-Domain Transferability : The skill is parameterized to support multiple markets (e.g., A-shares vs. Crypto) via configuration files. The same high-level agent reasoning logic can be applied to different financial domains simply by switching the underlying skill context, as demonstrated in our cross-market experiments (Section 4.2.2). (3) Independent Optimization : The skill‚Äôs execution backends and evaluation pipeline can be optimized independently of the agent‚Äôs reasoning model, improving throughput without retraining the LLM backbone. FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery Preprint, , 

3.3 Experience Memory 

A key component of FactorMiner is the experience memory M, a structured knowledge base that accumulates insights from historical mining sessions. We formalize the dynamics of the memory system through three conceptual operators: Formation, Evolution, and Retrieval. 

Memory Formation. At the end of each mining batch ùë° , the agent analyzes the mining trajectory ùúè ùë° = {( ùõº ùëñ , ùëÖ ùëñ )} ùêµ ùëñ =1, where ùëÖ ùëñ 

represents the evaluation feedback (IC, correlation, etc.). A forma-tion operator ùêπ selectively extracts informational artifacts: 

Mform  

> ùë° +1

= ùêπ (M ùë° , ùúè ùë° ) (7) This process distills raw data into symbolic patterns, categorizing them into successful patterns Psucc (those that pass admission) and forbidden regions Pfail (those rejected due to high correla-tion). 

Memory Evolution. Formed memory candidates are integrated into the existing knowledge base through an evolution operator ùê∏ :

Mùë° +1 = ùê∏ (M ùë° , Mform  

> ùë° +1

) (8) This operator consolidates redundant entries and discards low-utility information. For instance, if a specific VWAP_Deviation vari-ant is admitted but shows 0.82 correlation with existing factors, it is reclassified into Pfail to prevent further redundant exploration. 

Memory Retrieval. During the factor generation phase, the agent retrieves a context-dependent memory signal ùëö ùë° via a re-trieval operator ùëÖ :

ùëö ùë° = ùëÖ (M ùë° , Lùë° ) (9) The signal ùëö ùë° serves as a prompt-level constraint for the LLM pol-icy, effectively shaping the sampling distribution ùúã (ùõº | ùëö ùë° ). In practice, we store experience as compact natural-language tem-plates with canonical examples (e.g., "recommended directions" and "forbidden directions"), and retrieve them by matching against the current library diagnostics and recent rejection reasons; examples are provided in Appendices F and G. 

Memory Content. The resulting memory M maintains a per-sistent record of the evolving mining landscape: 

(1) Mining State ( S): Tracks the global evolution of the factor library, including current library size |L| , recent admission logs, and saturation metrics that signal when specific logical domains (e.g., price-volume reversal) are becoming overpopulated. 

(2) Structural Experience ( P): This is the core of the agent‚Äôs guidance system, categorized into: 

‚Ä¢ Recommended Directions ( Psucc ): High-success logical tem-plates distilled from recent batches, such as higher moment regimes (using Skew/Kurt for environment switching) and ro-bust efficiency interaction. 

‚Ä¢ Forbidden Directions ( Pfail ): Regions identified as "Red Seas" due to persistent high correlation with the existing library, such as simple VWAP Deviations or standardized returns. 

(3) Strategic Insights ( I): High-level lessons learned from the mining process, such as the observation that non-linear combina-tion strategies (e.g., XGBoost-based synthesis) significantly outper-form linear ones, or specific operator warnings (e.g., the instability of high-order moments in high-frequency data). 

Algorithm 1 Ralph Loop: Self-Evolving Factor Discovery 

Input: Operator library Œ©, experience memory M, target library size ùêæ 

Output: Factor library L

Initialize L ‚Üê ‚àÖ 

repeat Step 1: Memory Retrieval 

Retrieve memory signal ùëö ‚Üê ùëÖ (M , L) 

Step 2: Guided Generation 

Sample batch C ‚àº ùúã (ùõº | ùëö ) using Œ©

Step 3: Multi-Stage Evaluation Stage 1 : Fast IC screening ( ùëÄ fast assets) 

C1 ‚Üê { ùõº ‚àà C : |IC (ùõº )| ‚â• ùúè IC }

Stage 2 : Correlation check against LC2 ‚Üê { ùõº ‚àà C 1 : max ùëî ‚àà L |ùúå (ùõº, ùëî )| < ùúÉ }

Stage 2.5 : Replacement check For ùõº ‚àà C 1 \ C 2, let ùëî ‚òÖ = arg max ùëî ‚àà L |ùúå (ùõº, ùëî )| 

Replace ùëî ‚òÖ with ùõº if |ùúå (ùõº, ùëî ‚òÖ)| ‚â• ùúÉ ,

max ùëî ‚àà L\{ ùëî ‚òÖ } |ùúå (ùõº, ùëî )| < ùúÉ , and Œ¶(ùõº ) ‚â• Œ¶(ùëî ‚òÖ) + Œî

Stage 3 : Batch deduplication (intra-batch ùúå < ùúÉ )

Stage 4 : Full validation ( ùëÄ full assets) and trajectory ùúè collec-tion 

Step 4: Library Update 

Admit validated factors: L ‚Üê L ‚à™ C admitted 

Step 5: Memory Evolution 

Update strategy: M ‚Üê ùê∏ (M , ùêπ (ùúè )) 

until |L| ‚â• ùêæ or budget exhausted 

3.4 Ralph Loop: Self-Evolving Factor Discovery 

FactorMiner adopts the Ralph Loop paradigm‚Äîan iterative re-finement philosophy where agents accumulate experience and self-evolve through repeated interaction. We instantiate this paradigm for factor mining by integrating experience memory into the search process (Algorithm 1). The key insight is that memory enables the agent to learn how to search‚Äîavoiding redundant exploration while focusing on promising regions. Our instantiation of the Ralph Loop for factor mining has four key properties: 

Global library perspective. Unlike methods that optimize indi-vidual factors in isolation, our approach considers how each candi-date complements the existing library L. The correlation constraint (Stage 2) ensures diversity, while the replacement mechanism (Stage 2.5) allows high-quality factors to replace inferior ones. 

Memory-guided exploration. By maintaining Psucc and Pfail ,the agent avoids redundant exploration of known failure regions while focusing on promising structural patterns. 

Multi-stage evaluation. The multi-stage pipeline balances ef-ficiency and accuracy: Stage 1 uses a small asset subset for fast screening, Stages 2‚Äì3 enforce correlation constraints (inter-factor and intra-batch), and Stage 4 performs full validation only on sur-viving candidates. 

Self-evolution. After each iteration, the memory is updated via the evolution operator ùê∏ (integrating insights from ùêπ ), creating a feedback loop where the agent continuously improves its search Preprint, , Wang et al. 

strategy. This enables continual learning: knowledge accumulated in early sessions benefits later exploration. 

Trajectory semantics. The trajectory ùúè ùë° records, for each evalu-ated candidate, its formula ùõº , quality statistics (e.g., IC/ICIR), redun-dancy diagnostics (e.g., max ùëî ‚àà L |ùúå (ùõº, ùëî )| ), and the rejection/admission outcome (including whether a replacement was triggered). These fields are the inputs to the formation operator ùêπ and support expe-rience distillation across batches. Detailed specifications of the operator library, admission criteria, computational efficiency optimizations, and factor combination methods are provided in Appendices A, B, D, and E. To illustrate the diversity structure of the resulting library, we visualize the correlation structure of the released full A-share factor library (110 admitted factors) in Figure 2. The heatmap is computed using Spearman correlations on cross-sectionally standardized real-ized factor signals over the common time‚Äìasset panel, and indicates that most factor pairs are weakly to moderately correlated, with only a few localized clusters of higher dependence. 

4 Experiments 4.1 Experimental Setup 

Datasets. We evaluate FactorMiner across A-share equities and the cryptocurrency market to assess its discovery efficiency and cross-asset generalization. For A-share equities, we utilize intraday 10-minute bars of three index universes: the CSI 500 and CSI 1000 index constituents for mid-cap and broader small-/mid-cap coverage, and the HS300 index constituents for large-cap representation, with over 25 million data points in aggregate. For the Cryptocurrency market, we use 10-minute bars of 64 major assets from Binance. All datasets cover a training period from 2024-Q1 to 2024-Q4 and a held-out test period in 2025. The prediction target is the next 10-minute open-to-close price change ratio. 

Baselines and Metrics. We compare against five representa-tive methods: (1) Alpha101 (Classic) [8], a static library of hand-crafted formulas; (2) Alpha101 (Adapted) , with parameters tuned for high-frequency data; (3) Random Formula Exploration (RF) , ran-domly sampling type-correct expression trees from P ( Œ©) under a bounded depth/size distribution; (4) GPLearn [ 27 ], an enhanced genetic programming approach; and (5) AlphaAgent [ 28 ], an LLM-driven proposal-refinement framework. For a fair comparison, we apply the same admission rules to each method and evaluate an equal-sized factor set; if a method admits fewer factors, we com-plete the set by selecting the remaining candidates with the best IC. We also include a No Memory variant as an internal baseline in Section 4.3. Performance is quantified using standard predictive metrics: Rank Information Coefficient for forecasting precision, and its ratio (ICIR) for stability across time. Unless otherwise stated, we use IC to denote this Spearman correlation based IC. When applicable, all baselines share the same operator library Œ©, data fields, and evaluation/admission protocol, and are scored with a unified evaluation engine. For methods that require LLM-based proposal generation, we use Gemini 3.0 Flash unless otherwise stated. Implementation details are provided in Section C. 

4.2 Main Results 001_Intraday R 

> 002_EMA Deviat
> 003_Vol-VWAP D
> 004_High-Volum
> 005_Range-Posi
> 006_VWAP Devia
> 007_Price-Volu
> 008_Normalized
> 009_Volatility
> 010_Price-Volu
> 011_Price-VWAP
> 012_VWAP_Vol_H
> 013_Illiquidit
> 014_Resilience
> 015_Volatility
> 016_VWAP_Vol_S
> 017_VWAP-Accel
> 018_Range-Posi
> 019_VWAP-Gap R
> 020_Volume-Pri
> 021_Volume-Sho
> 022_Lower-Shad
> 023_Normalized
> 024_Open-Close
> 025_Open-Close
> 026_Price-Volu
> 027_Open-Volum
> 028_Close-Low
> 029_High-Close
> 030_Delta(High
> 031_Price Tren
> 032_Returns-Vo
> 033_Volume-Ran
> 034_Price Tren
> 035_High-Low E
> 036_Volatility
> 037_Price-Volu
> 038_Vol-Regime
> 039_Kurtosis-F
> 040_PricePos_S
> 041_Price Rang
> 042_Regime-Swi
> 043_Skewed Vol
> 044_Kurtotic V
> 045_Kurtosis-R
> 046_Volatility
> 047_Volume-wei
> 048_Volatility
> 049_Open_Close
> 050_Regime_C0_
> 051_High_Med_V
> 052_Regime_C1_
> 053_Alpha101_1
> 054_Amount_Reg
> 055_Skew_Open_
> 056_Kurtosis_R
> 057_TsRank_Pri
> 058_Regime_Amt
> 059_Regime_Tri
> 060_Regime_Amt
> 061_Alpha101_1
> 062_Amount_Sta
> 063_Triple_Ran
> 064_Regime_Pri
> 065_Regime_Vol
> 066_Regime_Kur
> 067_Alpha101_5
> 068_Skewness_R
> 069_Regime_Kur
> 070_Price_Pos_
> 071_Range_Posi
> 072_Kurt_Regim
> 073_Amt_Veloci
> 074_Range_Pos_
> 075_Amt_Effici
> 076_Amt_Veloci
> 077_Vol_of_Vol
> 078_Amt_Effici
> 079_Regime_Vol
> 080_Rsquare_Re
> 081_Regime_Tre
> 082_Regime_Log
> 083_Rsquare_St
> 084_Volatility
> 085_Rsquare_St
> 086_Resi_Accel
> 087_Kurt_Regim
> 088_Amt_Effici
> 089_Extreme_Di
> 090_Trend_Reli
> 091_Amt_Effici
> 092_Amt_Effici
> 093_Amt_Effici
> 094_And_HighVo
> 095_Higher_Mom
> 096_Amt_Effici
> 097_Amt_Effici
> 098_Smoothed_E
> 099_Amt_Effici
> 100_Residual_o
> 101_Median_Ret
> 102_High_Log_R
> 103_Open_Resi_
> 104_Median_Log
> 105_Median_Ske
> 106_Median_Vol
> 107_PricePos_S
> 108_PricePos_S
> 109_PricePos_V
> 110_VWAP_Accel

001_Intraday R 

002_EMA Deviat 

003_Vol-VWAP D 

004_High-Volum 

005_Range-Posi 

006_VWAP Devia 

007_Price-Volu 

008_Normalized 

009_Volatility 

010_Price-Volu 

011_Price-VWAP 

012_VWAP_Vol_H 

013_Illiquidit 

014_Resilience 

015_Volatility 

016_VWAP_Vol_S 

017_VWAP-Accel 

018_Range-Posi 

019_VWAP-Gap R 

020_Volume-Pri 

021_Volume-Sho 

022_Lower-Shad 

023_Normalized 

024_Open-Close 

025_Open-Close 

026_Price-Volu 

027_Open-Volum 

028_Close-Low 

029_High-Close 

030_Delta(High 

031_Price Tren 

032_Returns-Vo 

033_Volume-Ran 

034_Price Tren 

035_High-Low E 

036_Volatility 

037_Price-Volu 

038_Vol-Regime 

039_Kurtosis-F 

040_PricePos_S 

041_Price Rang 

042_Regime-Swi 

043_Skewed Vol 

044_Kurtotic V 

045_Kurtosis-R 

046_Volatility 

047_Volume-wei 

048_Volatility 

049_Open_Close 

050_Regime_C0_ 

051_High_Med_V 

052_Regime_C1_ 

053_Alpha101_1 

054_Amount_Reg 

055_Skew_Open_ 

056_Kurtosis_R 

057_TsRank_Pri 

058_Regime_Amt 

059_Regime_Tri 

060_Regime_Amt 

061_Alpha101_1 

062_Amount_Sta 

063_Triple_Ran 

064_Regime_Pri 

065_Regime_Vol 

066_Regime_Kur 

067_Alpha101_5 

068_Skewness_R 

069_Regime_Kur 

070_Price_Pos_ 

071_Range_Posi 

072_Kurt_Regim 

073_Amt_Veloci 

074_Range_Pos_ 

075_Amt_Effici 

076_Amt_Veloci 

077_Vol_of_Vol 

078_Amt_Effici 

079_Regime_Vol 

080_Rsquare_Re 

081_Regime_Tre 

082_Regime_Log 

083_Rsquare_St 

084_Volatility 

085_Rsquare_St 

086_Resi_Accel 

087_Kurt_Regim 

088_Amt_Effici 

089_Extreme_Di 

090_Trend_Reli 

091_Amt_Effici 

092_Amt_Effici 

093_Amt_Effici 

094_And_HighVo 

095_Higher_Mom 

096_Amt_Effici 

097_Amt_Effici 

098_Smoothed_E 

099_Amt_Effici 

100_Residual_o 

101_Median_Ret 

102_High_Log_R 

103_Open_Resi_ 

104_Median_Log 

105_Median_Ske 

106_Median_Vol 

107_PricePos_S 

108_PricePos_S 

109_PricePos_V 

110_VWAP_Accel 

1.00 0.49 0.13 0.17 0.15 0.17 0.14 0.20 0.25 0.33 0.06 -0.00 0.09 0.09 0.08 0.18 0.09 0.12 0.50 0.07 0.10 0.07 0.11 0.05 0.06 0.11 0.09 0.15 0.08 0.05 0.13 0.09 0.22 0.17 0.21 0.17 0.14 0.03 0.32 0.20 0.37 0.23 0.13 0.20 0.11 0.31 0.35 0.05 0.06 0.34 0.03 0.37 0.30 -0.00 0.03 0.26 0.09 0.05 0.23 0.16 0.13 0.11 0.09 0.35 0.17 0.17 0.17 0.13 0.05 0.20 0.06 0.20 0.13 0.13 0.19 0.16 0.17 0.16 0.18 0.30 0.11 0.16 -0.00 0.25 0.03 0.21 0.10 0.21 0.26 0.18 0.11 0.25 0.10 0.11 0.07 0.10 0.30 0.10 0.07 0.06 0.14 0.00 0.18 0.14 0.15 0.13 0.15 -0.14 0.16 0.51 

0.49 1.00 0.17 0.25 0.36 0.35 0.15 0.24 0.25 0.36 0.09 0.06 0.16 0.16 0.14 0.20 0.08 0.32 0.45 0.21 0.09 0.03 0.36 0.11 0.15 0.28 0.17 0.22 0.24 0.11 0.24 0.06 0.24 0.37 0.24 0.18 0.21 0.02 0.34 0.20 0.33 0.14 0.19 0.16 0.09 0.35 0.40 0.23 0.07 0.37 0.05 0.44 0.32 -0.03 -0.01 0.27 0.43 0.10 0.23 0.19 0.24 0.11 0.07 0.46 0.18 0.16 0.17 0.02 0.06 0.15 0.08 0.34 0.11 0.11 0.19 0.49 0.23 0.16 0.27 0.37 0.40 0.44 0.13 0.41 0.09 0.46 0.44 0.26 0.50 0.32 0.17 0.33 0.11 0.27 0.21 0.38 0.33 0.12 0.32 0.07 0.25 0.04 0.34 0.34 0.34 0.22 0.14 0.03 0.04 0.48 

0.13 0.17 1.00 0.45 0.18 0.54 0.40 0.11 0.34 0.28 0.35 0.28 0.36 0.39 0.23 0.15 0.18 -0.01 0.19 0.36 0.12 0.26 0.18 0.20 0.20 0.24 0.24 0.17 0.37 0.28 0.15 0.43 0.15 0.11 0.11 0.36 -0.04 0.12 0.11 0.34 0.36 0.19 0.19 0.35 0.19 0.37 -0.01 0.12 0.13 0.27 0.06 0.21 0.14 0.39 0.19 0.13 0.18 0.22 0.12 0.17 -0.04 0.20 0.02 0.14 0.14 0.09 0.20 0.11 0.19 0.26 0.22 0.14 0.14 0.26 0.20 0.05 0.20 0.21 0.27 0.18 0.09 0.09 0.23 0.12 0.16 0.24 0.17 0.19 0.17 0.22 0.17 0.23 0.22 0.22 0.20 0.13 0.14 0.17 0.18 0.13 0.27 0.22 0.21 0.18 0.24 0.18 0.33 0.35 0.33 0.30 

0.17 0.25 0.45 1.00 0.17 0.38 0.42 0.01 0.45 0.38 0.22 0.26 0.17 0.30 0.43 0.03 0.11 -0.01 0.16 0.36 0.06 -0.12 0.17 0.28 0.28 0.13 0.13 0.02 0.43 0.22 0.31 0.39 0.14 0.12 0.16 0.26 0.01 -0.00 0.13 0.14 0.39 0.27 0.21 0.21 0.05 0.25 -0.00 0.21 0.22 0.20 -0.01 0.21 0.19 0.31 0.30 0.09 0.23 0.22 0.15 0.18 0.22 0.26 0.08 0.18 0.16 0.16 0.00 0.14 0.24 0.13 0.26 0.16 0.17 0.16 0.21 -0.02 0.27 0.23 0.21 0.21 0.11 0.15 0.35 0.19 0.14 0.35 0.24 0.22 0.27 0.33 0.19 0.25 0.26 0.31 0.23 0.11 0.13 0.14 0.22 0.19 0.34 0.21 0.23 0.19 0.26 0.19 0.17 0.31 0.12 0.28 

0.15 0.36 0.18 0.17 1.00 0.29 0.13 0.24 0.45 0.24 0.10 -0.01 0.33 0.08 0.09 0.15 0.21 0.50 0.33 0.18 0.08 0.06 0.50 0.16 0.25 0.29 0.24 0.22 0.14 0.13 0.16 0.12 0.23 0.23 0.19 0.18 0.23 0.06 0.28 0.27 0.11 0.12 0.28 0.27 0.12 0.32 0.23 0.11 0.13 0.25 0.11 0.36 0.27 0.00 0.05 0.20 0.35 0.19 0.30 0.25 0.26 -0.06 0.12 0.23 0.19 0.17 0.18 0.02 0.03 0.22 -0.04 0.18 0.17 0.14 0.21 0.19 0.28 0.19 0.28 0.19 0.35 0.26 0.28 0.20 0.25 0.32 0.42 0.15 0.25 0.24 0.21 0.22 0.19 0.30 0.20 0.34 0.23 0.16 0.27 0.01 0.36 -0.02 0.38 0.40 0.40 0.20 0.19 0.13 0.11 0.17 

0.17 0.35 0.54 0.38 0.29 1.00 0.47 0.38 0.40 0.15 0.39 0.37 0.48 0.48 0.28 0.47 0.35 0.28 0.19 0.16 0.43 0.30 0.35 0.41 0.42 0.45 0.44 0.50 0.49 0.46 0.43 0.24 0.17 0.27 0.21 0.39 0.17 0.40 0.12 0.42 0.43 0.22 0.20 0.43 0.24 0.46 0.17 0.15 0.13 0.35 0.14 0.29 0.22 0.26 0.26 0.16 0.18 0.37 0.12 0.14 0.44 0.28 0.21 0.18 0.24 0.26 0.43 0.18 0.25 0.32 0.28 0.29 0.29 0.31 0.25 0.05 0.38 0.26 0.46 0.23 0.18 0.20 0.49 0.27 0.19 0.50 0.35 0.24 0.38 0.46 0.33 0.29 0.43 0.44 0.43 0.24 0.17 0.21 0.33 0.27 0.43 0.37 0.23 0.29 0.43 0.36 0.44 0.43 0.42 0.37 

0.14 0.15 0.40 0.42 0.13 0.47 1.00 0.13 0.33 0.32 0.31 0.22 0.30 0.28 0.21 0.18 0.15 0.04 0.20 0.20 0.24 0.34 0.16 0.20 0.21 0.31 0.32 0.26 0.36 0.28 0.23 0.21 0.13 0.12 0.18 0.38 0.07 0.16 0.07 0.40 0.40 0.17 0.18 0.43 0.28 0.42 0.14 0.10 0.13 0.33 0.03 0.13 0.14 0.34 0.21 0.13 0.20 0.24 0.12 0.09 0.26 0.17 0.12 0.13 0.03 0.14 0.15 0.12 0.20 0.32 0.31 0.14 0.16 0.36 0.20 0.02 0.18 0.24 0.30 0.14 0.06 0.05 0.23 0.12 0.14 0.24 0.16 0.20 0.17 0.21 0.16 0.24 0.24 0.22 0.22 0.10 0.13 0.18 0.18 0.14 0.25 0.24 0.13 0.17 0.19 0.21 0.44 0.38 0.44 0.30 

0.20 0.24 0.11 0.01 0.24 0.38 0.13 1.00 0.45 -0.10 0.24 0.08 0.38 0.23 0.22 0.30 0.17 0.46 0.26 -0.05 0.32 0.15 0.32 0.30 0.31 0.32 0.30 0.48 -0.01 0.17 0.38 0.13 0.23 0.26 0.32 0.44 0.43 0.25 0.17 0.35 0.17 0.25 0.27 0.33 0.26 0.42 0.39 0.16 0.15 0.33 0.22 0.32 0.26 -0.10 0.22 0.27 -0.01 0.31 0.13 0.10 0.49 0.39 0.40 0.23 0.50 0.50 0.55 0.32 0.34 0.27 0.12 0.28 0.47 0.24 0.31 -0.02 0.30 0.34 0.21 0.31 0.16 0.16 0.36 0.21 0.26 0.37 0.28 0.34 0.27 0.39 0.31 0.27 0.36 0.38 0.39 0.24 0.22 0.24 0.21 0.20 0.45 0.07 0.34 0.31 0.38 0.38 0.41 0.18 0.34 0.32 

0.25 0.25 0.34 0.45 0.45 0.40 0.33 0.45 1.00 0.22 0.23 0.00 0.35 0.22 0.16 0.20 0.27 0.20 0.36 0.14 0.21 0.08 0.28 0.28 0.28 0.28 0.27 0.26 0.20 0.22 0.32 0.33 0.24 0.18 0.24 0.39 0.20 0.13 0.24 0.38 0.27 0.32 0.26 0.40 0.18 0.38 0.19 -0.07 0.22 0.31 0.13 0.33 0.21 0.17 0.20 0.19 0.14 0.29 0.25 0.22 0.37 0.27 0.28 0.21 0.31 0.34 0.28 0.27 0.21 0.41 0.12 0.22 0.39 0.25 0.37 -0.01 0.37 0.37 0.38 0.28 0.12 0.14 0.38 0.20 0.22 0.36 0.24 0.35 0.28 0.39 0.32 0.28 0.38 0.33 0.28 0.22 0.21 0.24 0.20 0.20 0.42 0.09 0.31 0.29 0.38 0.28 0.32 0.20 0.28 0.31 

0.33 0.36 0.28 0.38 0.24 0.15 0.32 -0.10 0.22 1.00 0.08 0.10 -0.07 0.03 -0.02 0.02 0.06 0.02 0.40 0.33 -0.02 0.06 0.20 0.05 0.06 0.14 0.10 -0.01 0.20 0.07 0.02 0.20 0.20 0.04 0.16 0.18 0.07 -0.05 0.41 0.17 0.15 0.13 0.23 0.13 0.09 0.28 0.28 0.24 0.02 0.38 -0.13 0.40 0.24 0.20 -0.02 0.26 0.38 0.10 0.24 0.28 0.05 0.15 -0.10 0.33 0.05 0.02 -0.03 -0.08 0.03 0.13 0.04 0.14 0.01 0.13 0.13 0.26 0.13 0.12 0.12 0.27 0.22 0.22 0.05 0.16 0.21 0.11 0.18 0.14 0.14 0.12 0.08 0.24 0.04 0.13 0.10 0.20 0.34 0.11 0.12 -0.06 0.24 -0.09 0.24 0.30 0.23 0.12 0.12 0.03 0.07 0.39 

0.06 0.09 0.35 0.22 0.10 0.39 0.31 0.24 0.23 0.08 1.00 0.19 0.33 0.30 0.18 0.20 0.14 0.10 0.10 0.13 0.19 0.20 0.15 0.40 0.18 0.19 0.18 0.22 0.20 0.19 0.16 0.16 0.12 0.10 0.16 0.27 0.14 0.20 0.09 0.26 0.27 0.13 0.19 0.28 0.18 0.32 0.15 0.24 0.14 0.27 0.11 0.20 0.12 0.18 0.17 0.13 0.11 0.17 0.10 0.11 0.18 0.17 0.14 0.11 0.20 0.17 0.28 0.14 0.16 0.20 0.18 0.10 0.19 0.21 0.15 0.07 0.15 0.16 0.19 0.18 0.11 0.11 0.19 0.11 0.18 0.18 0.12 0.15 0.10 0.17 0.14 0.16 0.17 0.17 0.16 0.10 0.15 0.15 0.14 0.14 0.27 0.19 0.13 0.14 0.14 0.15 0.29 0.28 0.30 0.18 

-0.00 0.06 0.28 0.26 -0.01 0.37 0.22 0.08 0.00 0.10 0.19 1.00 0.15 0.25 0.15 0.17 -0.23 0.13 -0.04 0.11 0.19 0.15 0.11 0.17 0.17 0.12 0.13 0.12 0.26 0.20 0.09 0.15 -0.04 0.02 0.09 0.18 0.04 0.20 -0.00 0.08 0.16 0.05 0.10 0.17 0.09 0.19 0.01 0.25 0.01 0.15 -0.05 0.04 0.10 0.25 0.14 0.04 -0.03 0.14 0.03 0.11 0.16 0.29 0.04 0.07 0.07 0.09 0.14 0.05 0.27 -0.20 0.36 0.05 0.14 0.12 -0.10 -0.01 0.06 0.05 -0.40 0.09 -0.01 0.05 0.20 0.04 0.10 0.16 0.10 0.08 0.09 0.19 0.09 0.10 0.16 0.44 0.20 0.09 0.06 0.09 0.10 0.10 0.18 0.23 0.12 0.11 0.11 0.13 0.17 0.22 0.17 0.13 

0.09 0.16 0.36 0.17 0.33 0.48 0.30 0.38 0.35 -0.07 0.33 0.15 1.00 0.40 0.31 0.20 0.15 0.21 0.11 0.06 0.22 0.20 0.15 0.19 0.20 0.22 0.22 0.32 0.17 0.18 0.25 0.06 0.11 0.18 0.16 0.27 0.20 0.17 0.07 0.31 0.38 0.17 0.12 0.36 0.18 0.34 0.13 0.13 0.18 0.23 0.29 0.16 0.13 0.09 0.23 0.07 0.01 0.09 0.10 0.16 0.22 -0.15 0.20 0.12 0.21 0.24 0.36 0.17 0.13 0.26 0.22 0.10 0.29 0.19 0.21 0.01 0.14 0.19 0.27 0.14 0.08 0.08 0.24 0.14 0.11 0.24 0.17 0.18 0.18 0.17 0.12 0.25 0.18 0.18 0.13 0.10 0.11 0.14 0.21 0.13 0.23 0.19 0.17 0.15 0.16 0.08 0.30 0.35 0.26 0.16 

0.09 0.16 0.39 0.30 0.08 0.48 0.28 0.23 0.22 0.03 0.30 0.25 0.40 1.00 0.36 0.16 0.15 0.10 0.06 0.16 0.23 0.17 0.15 0.19 0.20 0.17 0.17 0.20 0.28 0.28 0.17 0.12 0.11 0.11 0.14 0.26 0.11 0.14 0.04 0.25 0.37 0.17 0.14 0.24 0.10 0.28 0.05 0.18 0.14 0.18 0.22 0.12 0.16 0.16 0.22 0.09 0.06 0.14 0.07 0.08 0.17 0.18 0.17 0.12 0.16 0.16 0.25 0.19 0.21 0.08 0.22 0.11 0.18 0.14 0.16 0.00 0.17 0.14 0.20 0.12 0.06 0.08 0.24 0.12 0.11 0.24 0.16 0.13 0.19 0.17 0.22 0.19 0.24 0.20 0.15 0.09 0.07 0.14 0.17 0.17 0.21 0.24 0.13 0.11 0.15 0.11 0.24 0.35 0.22 0.30 

0.08 0.14 0.23 0.43 0.09 0.28 0.21 0.22 0.16 -0.02 0.18 0.15 0.31 0.36 1.00 0.09 0.18 0.10 0.07 0.21 0.23 -0.06 0.22 0.22 0.22 0.09 0.09 0.12 0.21 0.30 0.20 0.12 0.09 0.14 0.15 0.21 0.16 0.03 0.01 0.12 0.32 0.18 0.23 0.17 0.04 0.21 0.02 0.11 0.23 0.06 0.25 0.10 0.11 0.08 0.21 0.10 0.03 0.18 0.18 0.12 0.23 0.12 0.37 0.16 0.18 0.19 0.11 0.40 0.39 0.08 0.17 0.12 0.20 0.09 0.18 -0.09 0.21 0.19 0.16 0.12 0.03 0.08 0.28 0.13 0.08 0.31 0.17 0.19 0.24 0.24 0.17 0.17 0.44 0.27 0.15 0.07 0.01 0.22 0.17 0.32 0.24 0.19 0.14 0.10 0.18 0.16 0.16 0.26 0.09 0.28 

0.18 0.20 0.15 0.03 0.15 0.47 0.18 0.30 0.20 0.02 0.20 0.17 0.20 0.16 0.09 1.00 0.22 0.22 0.11 -0.04 0.32 0.17 0.21 0.24 0.24 0.27 0.28 0.33 0.16 0.24 0.23 0.09 0.09 0.15 0.13 0.20 0.15 0.34 0.03 0.19 0.25 0.17 0.10 0.22 0.22 0.24 0.15 0.00 0.03 0.27 0.09 0.03 0.14 0.05 0.07 0.12 0.02 0.24 0.07 0.05 0.31 0.15 0.18 0.13 0.18 0.20 0.28 0.42 0.17 0.16 0.09 0.22 0.22 0.17 0.13 0.01 0.23 0.16 0.21 0.16 0.06 0.09 0.25 0.16 0.08 0.26 0.13 0.28 0.21 0.30 0.17 0.12 0.27 0.23 0.29 0.14 0.07 0.14 0.13 0.17 0.23 0.19 0.07 0.14 0.11 0.23 0.20 0.02 0.24 0.26 

0.09 0.08 0.18 0.11 0.21 0.35 0.15 0.17 0.27 0.06 0.14 -0.23 0.15 0.15 0.18 0.22 1.00 0.10 0.15 0.06 0.32 0.07 0.23 0.17 0.18 0.21 0.22 0.25 0.10 0.33 0.21 0.09 0.15 0.18 0.07 0.14 0.12 0.14 0.04 0.23 0.11 0.10 0.14 0.18 0.11 0.19 0.07 -0.16 0.04 0.11 0.20 0.17 0.09 -0.05 0.02 0.11 0.08 0.16 0.11 0.04 0.16 0.02 0.19 0.08 0.10 0.13 0.16 0.22 0.11 0.22 -0.22 0.15 0.11 0.14 0.15 -0.07 0.17 0.15 0.48 0.08 0.06 0.04 0.21 0.15 0.09 0.22 0.15 0.11 0.22 0.20 0.11 0.10 0.34 0.08 0.20 0.07 0.06 0.16 0.10 0.23 0.19 0.04 0.07 0.14 0.25 0.18 0.19 0.11 0.16 0.31 

0.12 0.32 -0.01 -0.01 0.50 0.28 0.04 0.46 0.20 0.02 0.10 0.13 0.21 0.10 0.10 0.22 0.10 1.00 0.24 0.02 0.31 0.06 0.44 0.18 0.25 0.34 0.28 0.52 -0.17 -0.01 0.26 -0.01 0.15 0.28 0.36 0.18 0.36 0.17 0.20 0.18 0.08 0.11 0.24 0.18 0.13 0.29 0.35 0.19 0.07 0.26 0.26 0.28 0.28 -0.27 0.07 0.22 -0.01 0.19 0.17 0.04 0.43 0.23 0.22 0.23 0.30 0.29 0.32 0.15 0.15 0.14 0.07 0.17 0.35 0.14 0.08 0.13 0.20 0.16 0.07 0.21 0.29 0.22 0.28 0.19 0.21 0.32 0.40 0.16 0.24 0.28 0.19 0.20 0.18 0.31 0.25 0.31 0.21 0.16 0.24 0.06 0.34 -0.06 0.37 0.37 0.30 0.23 0.22 0.12 0.13 0.21 

0.50 0.45 0.19 0.16 0.33 0.19 0.20 0.26 0.36 0.40 0.10 -0.04 0.11 0.06 0.07 0.11 0.15 0.24 1.00 0.23 0.08 0.12 0.24 0.07 0.08 0.24 0.17 0.35 -0.10 0.10 0.22 0.13 0.26 0.24 0.23 0.25 0.24 0.02 0.39 0.31 0.21 0.19 0.26 0.25 0.18 0.41 0.36 0.15 0.10 0.42 0.04 0.49 0.24 0.01 -0.01 0.47 0.25 0.24 0.31 0.19 0.14 -0.04 0.07 0.37 0.21 0.16 0.15 0.05 0.04 0.30 0.00 0.22 0.15 0.18 0.22 0.20 0.19 0.18 0.23 0.31 0.18 0.17 0.08 0.25 0.13 0.27 0.26 0.22 0.30 0.18 0.11 0.26 0.12 0.19 0.12 0.23 0.32 0.13 0.14 0.04 0.27 -0.08 0.28 0.34 0.31 0.36 0.21 -0.03 0.15 0.44 

0.07 0.21 0.36 0.36 0.18 0.16 0.20 -0.05 0.14 0.33 0.13 0.11 0.06 0.16 0.21 -0.04 0.06 0.02 0.23 1.00 -0.15 -0.01 0.31 0.10 0.11 0.10 0.08 -0.01 0.22 0.20 0.02 0.17 0.18 0.12 0.18 0.12 -0.09 -0.36 0.19 0.08 0.17 0.09 0.43 0.07 0.03 0.32 -0.11 0.31 0.07 0.20 0.07 0.23 0.21 0.13 0.12 0.33 0.25 0.10 0.27 0.35 0.01 0.09 -0.25 0.39 0.03 -0.20 -0.08 0.00 0.13 0.04 0.10 0.09 -0.08 0.06 0.05 0.10 0.08 0.05 0.10 0.23 0.16 0.13 0.14 0.15 0.26 0.22 0.25 0.05 0.10 0.07 0.04 0.20 0.10 0.14 0.10 0.15 0.13 0.17 0.19 0.15 0.30 -0.00 0.18 0.21 0.16 0.07 0.06 0.17 0.03 0.23 

0.10 0.09 0.12 0.06 0.08 0.43 0.24 0.32 0.21 -0.02 0.19 0.19 0.22 0.23 0.23 0.32 0.32 0.31 0.08 -0.15 1.00 0.16 0.24 0.21 0.21 0.26 0.25 0.36 0.07 0.21 0.20 -0.00 0.11 0.14 0.26 0.20 0.36 0.32 0.05 0.20 0.16 0.12 0.18 0.22 0.13 0.25 0.31 0.01 0.07 0.17 0.15 0.10 0.13 0.02 0.06 0.13 -0.07 0.18 0.11 0.08 0.43 0.17 0.41 0.17 0.16 0.31 0.26 0.28 0.27 0.19 0.20 0.14 0.39 0.20 0.16 -0.10 0.24 0.17 0.13 0.13 0.01 0.06 0.23 0.11 0.11 0.26 0.13 0.17 0.21 0.24 0.21 0.10 0.41 0.24 0.21 0.12 0.06 0.21 0.10 0.27 0.23 0.12 0.07 0.14 0.17 0.21 0.27 0.13 0.22 0.37 

0.07 0.03 0.26 -0.12 0.06 0.30 0.34 0.15 0.08 0.06 0.20 0.15 0.20 0.17 -0.06 0.17 0.07 0.06 0.12 -0.01 0.16 1.00 0.09 -0.01 -0.01 0.27 0.28 0.39 0.04 0.15 -0.11 0.05 0.07 0.01 0.06 0.49 0.07 0.19 0.04 0.38 0.18 0.02 0.07 0.38 0.27 0.32 0.08 0.02 -0.03 0.23 0.07 0.06 0.02 0.16 -0.04 0.11 0.02 0.09 0.04 0.06 0.08 0.06 0.05 0.04 0.05 0.07 0.38 0.04 0.05 0.25 0.12 0.05 0.07 0.29 0.08 0.05 0.07 0.08 0.17 0.05 0.02 -0.01 0.02 -0.01 0.07 0.01 0.02 0.07 0.00 0.04 0.11 0.07 0.07 0.07 0.07 0.07 0.08 0.09 0.03 0.01 0.07 0.10 0.07 0.06 0.09 0.10 0.37 0.19 0.41 0.15 

0.11 0.36 0.18 0.17 0.50 0.35 0.16 0.32 0.28 0.20 0.15 0.11 0.15 0.15 0.22 0.21 0.23 0.44 0.24 0.31 0.24 0.09 1.00 0.32 0.34 0.33 0.31 0.28 0.16 0.30 0.10 0.03 0.26 0.36 0.29 0.23 0.41 0.10 0.20 0.21 0.10 0.13 0.48 0.21 0.16 0.42 0.24 0.18 0.06 0.26 0.10 0.31 0.36 0.00 0.08 0.37 0.31 0.29 0.31 0.32 0.38 0.19 0.23 0.42 0.24 0.22 0.23 0.14 0.27 0.16 0.04 0.27 0.17 0.17 0.16 0.06 0.29 0.18 0.20 0.34 0.35 0.28 0.35 0.38 0.37 0.49 0.46 0.17 0.21 0.33 0.22 0.25 0.40 0.38 0.36 0.37 0.20 0.36 0.34 0.41 0.50 0.05 0.34 0.35 0.39 0.31 0.30 0.12 0.21 0.32 

0.05 0.11 0.20 0.28 0.16 0.41 0.20 0.30 0.28 0.05 0.40 0.17 0.19 0.19 0.22 0.24 0.17 0.18 0.07 0.10 0.21 -0.01 0.32 1.00 0.39 0.21 0.22 0.25 0.23 0.24 0.31 0.18 0.10 0.18 0.14 0.22 0.12 0.21 0.06 0.17 0.19 0.15 0.16 0.19 0.12 0.22 0.12 0.15 0.15 0.17 0.06 0.22 0.14 0.09 0.22 0.10 0.14 0.33 0.08 0.08 0.38 0.21 0.17 0.09 0.22 0.20 0.21 0.15 0.20 0.13 0.13 0.21 0.20 0.14 0.17 -0.04 0.31 0.19 0.20 0.20 0.13 0.18 0.40 0.25 0.19 0.36 0.16 0.18 0.18 0.37 0.25 0.16 0.34 0.36 0.34 0.14 0.12 0.19 0.22 0.34 0.37 0.23 0.06 0.15 0.29 0.31 0.22 0.19 0.23 0.13 

0.06 0.15 0.20 0.28 0.25 0.42 0.21 0.31 0.28 0.06 0.18 0.17 0.20 0.20 0.22 0.24 0.18 0.25 0.08 0.11 0.21 -0.01 0.34 0.39 1.00 0.21 0.24 0.26 0.23 0.24 0.32 0.18 0.11 0.19 0.15 0.21 0.14 0.19 0.07 0.17 0.19 0.15 0.17 0.20 0.13 0.23 0.10 0.13 0.16 0.16 0.06 0.21 0.14 0.07 0.23 0.11 0.12 0.33 0.08 0.08 0.39 0.21 0.16 0.11 0.21 0.21 0.20 0.13 0.20 0.14 0.13 0.23 0.21 0.15 0.17 -0.04 0.32 0.20 0.21 0.18 0.17 0.18 0.42 0.23 0.21 0.36 0.27 0.17 0.21 0.37 0.26 0.18 0.34 0.37 0.35 0.15 0.10 0.13 0.24 0.18 0.38 0.19 0.13 0.23 0.33 0.32 0.23 0.20 0.23 0.14 

0.11 0.28 0.24 0.13 0.29 0.45 0.31 0.32 0.28 0.14 0.19 0.12 0.22 0.17 0.09 0.27 0.21 0.34 0.24 0.10 0.26 0.27 0.33 0.21 0.21 1.00 0.54 0.44 0.13 0.22 0.30 0.16 0.19 0.21 0.22 0.31 0.18 0.22 0.12 0.36 0.16 0.13 0.18 0.32 0.25 0.36 0.19 0.10 0.03 0.31 0.11 0.25 0.16 0.04 0.01 0.19 0.19 0.24 0.15 0.08 0.34 0.17 0.15 0.24 0.22 0.20 0.34 0.11 0.17 0.28 0.06 0.33 0.21 0.27 0.20 0.04 0.27 0.19 0.28 0.15 0.23 0.14 0.30 0.16 0.17 0.34 0.31 0.20 0.23 0.27 0.22 0.15 0.28 0.28 0.32 0.29 0.14 0.16 0.18 0.15 0.30 0.08 0.25 0.31 0.34 0.28 0.33 0.17 0.28 0.27 

0.09 0.17 0.24 0.13 0.24 0.44 0.32 0.30 0.27 0.10 0.18 0.13 0.22 0.17 0.09 0.28 0.22 0.28 0.17 0.08 0.25 0.28 0.31 0.22 0.24 0.54 1.00 0.41 0.14 0.24 0.30 0.17 0.17 0.19 0.20 0.30 0.16 0.23 0.08 0.33 0.16 0.13 0.18 0.32 0.25 0.34 0.14 0.07 0.04 0.28 0.10 0.21 0.13 0.10 0.12 0.16 0.12 0.29 0.11 0.09 0.33 0.18 0.15 0.14 0.20 0.20 0.33 0.12 0.18 0.28 0.09 0.23 0.23 0.27 0.20 0.02 0.26 0.20 0.27 0.17 0.14 0.11 0.30 0.15 0.16 0.31 0.21 0.18 0.19 0.29 0.23 0.21 0.30 0.27 0.31 0.24 0.14 0.22 0.24 0.16 0.31 0.12 0.19 0.22 0.31 0.27 0.34 0.17 0.33 0.22 

0.15 0.22 0.17 0.02 0.22 0.50 0.26 0.48 0.26 -0.01 0.22 0.12 0.32 0.20 0.12 0.33 0.25 0.52 0.35 -0.01 0.36 0.39 0.28 0.25 0.26 0.44 0.41 1.00 -0.28 0.09 0.45 0.08 0.12 0.34 0.29 0.34 0.25 0.28 0.08 0.39 0.19 0.14 0.15 0.38 0.29 0.41 0.22 0.08 0.10 0.26 0.37 0.23 0.14 -0.23 0.17 0.26 -0.14 0.38 0.08 -0.11 0.37 0.02 0.23 0.12 0.28 0.30 0.48 0.25 0.14 0.33 0.01 0.22 0.42 0.28 0.20 0.04 0.22 0.22 0.30 0.18 0.13 0.12 0.30 0.22 0.13 0.31 0.22 0.18 0.21 0.30 0.21 0.19 0.28 0.30 0.27 0.18 0.16 0.17 0.20 0.17 0.30 0.08 0.22 0.24 0.29 0.40 0.42 0.19 0.39 0.23 

0.08 0.24 0.37 0.43 0.14 0.49 0.36 -0.01 0.20 0.20 0.20 0.26 0.17 0.28 0.21 0.16 0.10 -0.17 -0.10 0.22 0.07 0.04 0.16 0.23 0.23 0.13 0.14 -0.28 1.00 0.49 0.08 0.19 0.09 0.01 -0.03 0.17 -0.03 0.14 0.08 0.17 0.38 0.10 0.09 0.17 0.05 0.21 -0.02 0.10 0.08 0.19 -0.19 0.14 0.14 0.47 0.17 -0.05 0.37 0.07 0.09 0.31 0.14 0.31 -0.00 0.12 0.05 0.02 0.07 -0.05 0.15 0.08 0.34 0.14 -0.09 0.13 0.08 0.09 0.21 0.08 0.30 0.11 0.10 0.12 0.24 0.12 0.10 0.30 0.19 0.10 0.24 0.25 0.17 0.14 0.21 0.23 0.24 0.10 0.07 0.07 0.19 0.14 0.20 0.39 0.05 0.11 0.23 0.04 0.15 0.37 0.16 0.19 

0.05 0.11 0.28 0.22 0.13 0.46 0.28 0.17 0.22 0.07 0.19 0.20 0.18 0.28 0.30 0.24 0.33 -0.01 0.10 0.20 0.21 0.15 0.30 0.24 0.24 0.22 0.24 0.09 0.49 1.00 0.12 0.12 0.07 0.11 0.02 0.22 0.07 0.18 0.01 0.20 0.24 0.11 0.18 0.21 0.13 0.25 -0.03 0.04 0.07 0.14 0.09 0.12 0.07 0.26 0.12 0.13 0.17 0.20 0.10 0.21 0.25 0.15 0.14 0.11 0.11 0.04 0.16 0.16 0.30 0.15 0.19 0.16 -0.07 0.15 0.07 -0.04 0.23 0.10 0.24 0.10 0.04 0.06 0.27 0.10 0.10 0.32 0.18 0.12 0.24 0.26 0.18 0.11 0.48 0.26 0.27 0.09 0.03 0.18 0.15 0.34 0.24 0.34 0.06 0.13 0.25 0.23 0.21 0.23 0.23 0.32 

0.13 0.24 0.15 0.31 0.16 0.43 0.23 0.38 0.32 0.02 0.16 0.09 0.25 0.17 0.20 0.23 0.21 0.26 0.22 0.02 0.20 -0.11 0.10 0.31 0.32 0.30 0.30 0.45 0.08 0.12 1.00 0.20 0.11 0.37 0.21 0.20 0.09 0.19 0.04 0.22 0.18 0.19 0.05 0.22 0.16 0.23 0.12 0.06 0.13 0.20 0.15 0.21 0.15 -0.04 0.30 0.10 0.01 0.36 0.03 -0.05 0.36 0.14 0.18 0.06 0.29 0.27 0.25 0.18 0.17 0.22 0.07 0.26 0.37 0.17 0.25 -0.02 0.25 0.25 0.27 0.14 0.09 0.11 0.42 0.24 0.07 0.40 0.23 0.20 0.30 0.38 0.21 0.21 0.34 0.35 0.36 0.12 0.14 0.09 0.23 0.16 0.31 0.15 0.17 0.19 0.35 0.37 0.24 0.19 0.22 0.16 

0.09 0.06 0.43 0.39 0.12 0.24 0.21 0.13 0.33 0.20 0.16 0.15 0.06 0.12 0.12 0.09 0.09 -0.01 0.13 0.17 -0.00 0.05 0.03 0.18 0.18 0.16 0.17 0.08 0.19 0.12 0.20 1.00 0.16 0.09 0.11 0.28 -0.20 0.08 0.04 0.19 0.14 0.17 0.02 0.19 0.12 0.12 -0.10 -0.01 0.10 0.13 -0.07 0.14 0.04 0.25 0.12 -0.00 0.10 0.24 0.02 -0.02 0.16 0.25 -0.02 -0.04 0.16 0.08 0.06 0.10 0.15 0.15 0.08 0.14 0.14 0.15 0.20 -0.05 0.22 0.22 0.12 0.11 0.02 0.05 0.22 0.07 0.12 0.19 0.10 0.21 0.17 0.24 0.18 0.11 0.19 0.23 0.23 0.09 0.06 0.06 0.08 0.07 0.23 0.11 0.16 0.13 0.22 0.21 0.16 0.13 0.20 0.09 

0.22 0.24 0.15 0.14 0.23 0.17 0.13 0.23 0.24 0.20 0.12 -0.04 0.11 0.11 0.09 0.09 0.15 0.15 0.26 0.18 0.11 0.07 0.26 0.10 0.11 0.19 0.17 0.12 0.09 0.07 0.11 0.16 1.00 0.28 0.43 0.19 0.28 0.01 0.24 0.20 0.11 0.14 0.29 0.16 0.11 0.33 0.23 0.13 0.04 0.29 0.07 0.29 0.27 0.02 0.05 0.26 0.17 0.10 0.23 0.25 0.18 0.13 0.13 0.32 0.22 0.18 0.19 0.09 0.13 0.20 -0.05 0.15 0.16 0.12 0.26 0.06 0.23 0.13 0.18 0.33 0.13 0.12 0.13 0.18 0.28 0.28 0.23 0.15 0.18 0.14 0.14 0.18 0.16 0.14 0.16 0.19 0.19 0.17 0.13 0.16 0.33 -0.09 0.20 0.24 0.20 0.11 0.21 0.06 0.15 0.29 

0.17 0.37 0.11 0.12 0.23 0.27 0.12 0.26 0.18 0.04 0.10 0.02 0.18 0.11 0.14 0.15 0.18 0.28 0.24 0.12 0.14 0.01 0.36 0.18 0.19 0.21 0.19 0.34 0.01 0.11 0.37 0.09 0.28 1.00 0.33 0.13 0.21 0.07 0.07 0.15 0.13 0.10 0.18 0.14 0.11 0.26 0.17 0.09 0.07 0.18 0.18 0.19 0.25 -0.12 0.14 0.21 0.05 0.18 0.12 0.06 0.19 0.04 0.14 0.22 0.17 0.17 0.19 0.12 0.12 0.15 -0.00 0.19 0.20 0.10 0.13 0.06 0.14 0.12 0.21 0.23 0.15 0.15 0.20 0.32 0.13 0.44 0.31 0.10 0.28 0.23 0.12 0.22 0.23 0.21 0.23 0.17 0.14 0.17 0.26 0.31 0.27 0.07 0.16 0.17 0.22 0.22 0.20 0.10 0.14 0.22 

0.21 0.24 0.11 0.16 0.19 0.21 0.18 0.32 0.24 0.16 0.16 0.09 0.16 0.14 0.15 0.13 0.07 0.36 0.23 0.18 0.26 0.06 0.29 0.14 0.15 0.22 0.20 0.29 -0.03 0.02 0.21 0.11 0.43 0.33 1.00 0.21 0.38 0.04 0.26 0.20 0.12 0.17 0.36 0.20 0.13 0.38 0.29 0.19 0.07 0.33 0.12 0.28 0.34 -0.05 0.10 0.29 0.04 0.11 0.27 0.26 0.32 0.23 0.20 0.38 0.28 0.25 0.23 0.17 0.19 0.18 0.10 0.14 0.30 0.17 0.18 0.02 0.19 0.18 0.08 0.37 0.12 0.13 0.17 0.22 0.31 0.32 0.26 0.17 0.20 0.19 0.16 0.20 0.20 0.21 0.19 0.21 0.19 0.20 0.15 0.22 0.39 -0.12 0.23 0.27 0.18 0.13 0.25 0.09 0.17 0.32 

0.17 0.18 0.36 0.26 0.18 0.39 0.38 0.44 0.39 0.18 0.27 0.18 0.27 0.26 0.21 0.20 0.14 0.18 0.25 0.12 0.20 0.49 0.23 0.22 0.21 0.31 0.30 0.34 0.17 0.22 0.20 0.28 0.19 0.13 0.21 1.00 0.19 0.29 0.15 0.40 0.26 0.20 0.23 0.40 0.26 0.37 0.19 0.13 0.14 0.33 0.12 0.25 0.20 0.21 0.18 0.22 0.10 0.32 0.20 0.17 0.29 0.33 0.22 0.18 0.49 0.30 0.38 0.21 0.30 0.29 0.16 0.19 0.31 0.30 0.25 0.06 0.25 0.29 0.25 0.24 0.11 0.09 0.18 0.12 0.22 0.26 0.17 0.27 0.16 0.30 0.26 0.24 0.29 0.34 0.28 0.17 0.20 0.22 0.16 0.13 0.34 0.14 0.28 0.23 0.30 0.31 0.39 0.26 0.40 0.33 

0.14 0.21 -0.04 0.01 0.23 0.17 0.07 0.43 0.20 0.07 0.14 0.04 0.20 0.11 0.16 0.15 0.12 0.36 0.24 -0.09 0.36 0.07 0.41 0.12 0.14 0.18 0.16 0.25 -0.03 0.07 0.09 -0.20 0.28 0.21 0.38 0.19 1.00 0.13 0.26 0.17 0.08 0.12 0.52 0.19 0.13 0.43 0.51 0.26 0.06 0.29 0.12 0.27 0.33 -0.12 0.07 0.42 0.07 0.11 0.31 0.38 0.27 0.16 0.48 0.46 0.30 0.45 0.32 0.19 0.21 0.14 0.06 0.13 0.31 0.14 0.14 0.04 0.14 0.15 0.09 0.34 0.17 0.14 0.17 0.18 0.33 0.26 0.28 0.15 0.12 0.17 0.14 0.21 0.21 0.21 0.17 0.23 0.20 0.27 0.16 0.21 0.41 -0.10 0.26 0.30 0.21 0.16 0.30 0.08 0.14 0.31 

0.03 0.02 0.12 -0.00 0.06 0.40 0.16 0.25 0.13 -0.05 0.20 0.20 0.17 0.14 0.03 0.34 0.14 0.17 0.02 -0.36 0.32 0.19 0.10 0.21 0.19 0.22 0.23 0.28 0.14 0.18 0.19 0.08 0.01 0.07 0.04 0.29 0.13 1.00 -0.02 0.19 0.12 0.04 -0.04 0.21 0.18 0.12 0.13 -0.07 0.06 0.10 0.06 0.06 0.04 0.09 0.08 -0.02 -0.03 0.23 0.11 -0.06 0.24 0.14 0.22 -0.08 0.36 0.23 0.26 0.12 0.12 0.11 0.05 0.11 0.17 0.15 0.08 -0.01 0.17 0.12 0.21 0.05 -0.01 0.02 0.11 0.07 0.02 0.14 0.04 0.09 0.10 0.23 0.16 0.03 0.20 0.22 0.19 0.06 0.03 0.05 0.06 0.08 0.12 0.20 0.03 0.07 0.16 0.23 0.22 0.13 0.23 0.10 

0.32 0.34 0.11 0.13 0.28 0.12 0.07 0.17 0.24 0.41 0.09 -0.00 0.07 0.04 0.01 0.03 0.04 0.20 0.39 0.19 0.05 0.04 0.20 0.06 0.07 0.12 0.08 0.08 0.08 0.01 0.04 0.04 0.24 0.07 0.26 0.15 0.26 -0.02 1.00 0.19 0.22 0.18 0.29 0.24 0.04 0.31 0.36 0.28 0.33 0.38 0.01 0.46 0.25 0.00 -0.02 0.24 0.22 0.07 0.34 0.28 0.13 0.09 0.03 0.37 0.19 0.12 0.13 -0.09 -0.13 0.18 0.04 0.09 0.11 0.13 0.14 0.20 0.16 0.14 0.15 0.35 0.21 0.21 0.06 0.20 0.25 0.14 0.18 0.18 0.14 0.14 0.33 0.17 0.04 0.25 -0.18 0.23 0.28 0.10 0.11 -0.04 0.26 -0.14 0.26 0.30 0.20 0.09 0.09 0.00 0.03 0.30 

0.20 0.20 0.34 0.14 0.27 0.42 0.40 0.35 0.38 0.17 0.26 0.08 0.31 0.25 0.12 0.19 0.23 0.18 0.31 0.08 0.20 0.38 0.21 0.17 0.17 0.36 0.33 0.39 0.17 0.20 0.22 0.19 0.20 0.15 0.20 0.40 0.17 0.19 0.19 1.00 0.24 0.15 0.20 0.47 0.30 0.45 0.21 0.02 0.13 0.28 0.15 0.37 0.18 0.14 0.12 0.20 0.11 0.23 0.18 0.13 0.21 0.19 0.18 0.18 0.24 0.24 0.46 0.20 0.13 0.38 0.13 0.18 0.27 0.34 0.27 0.05 0.22 0.25 0.35 0.19 0.10 0.08 0.22 0.13 0.24 0.23 0.17 0.22 0.20 0.17 0.20 0.24 0.24 0.22 0.20 0.18 0.20 0.20 0.14 0.09 0.27 0.10 0.24 0.28 0.33 0.23 0.45 0.32 0.44 0.31 

0.37 0.33 0.36 0.39 0.11 0.43 0.40 0.17 0.27 0.15 0.27 0.16 0.38 0.37 0.32 0.25 0.11 0.08 0.21 0.17 0.16 0.18 0.10 0.19 0.19 0.16 0.16 0.19 0.38 0.24 0.18 0.14 0.11 0.13 0.12 0.26 0.08 0.12 0.22 0.24 1.00 0.40 0.12 0.34 0.17 0.37 0.13 0.14 0.29 0.23 0.16 0.21 0.18 0.16 0.19 0.10 0.08 0.13 0.13 0.10 0.18 0.12 0.10 0.20 0.15 0.15 0.27 0.21 0.07 0.27 0.31 0.19 0.15 0.22 0.19 0.06 0.21 0.17 0.34 0.20 0.06 0.12 0.18 0.20 0.01 0.27 0.13 0.35 0.23 0.23 0.20 0.22 0.17 0.20 0.02 0.11 0.08 0.12 0.19 0.15 0.19 0.29 0.11 0.07 0.17 0.13 0.24 0.02 0.23 0.34 

0.23 0.14 0.19 0.27 0.12 0.22 0.17 0.25 0.32 0.13 0.13 0.05 0.17 0.17 0.18 0.17 0.10 0.11 0.19 0.09 0.12 0.02 0.13 0.15 0.15 0.13 0.13 0.14 0.10 0.11 0.19 0.17 0.14 0.10 0.17 0.20 0.12 0.04 0.18 0.15 0.40 1.00 0.16 0.20 -0.13 0.21 0.16 0.07 -0.00 0.24 0.07 0.23 0.14 0.07 0.11 0.07 0.05 0.17 0.11 0.09 0.21 0.18 0.15 0.16 0.19 0.14 0.16 0.18 0.11 0.18 0.10 0.11 0.23 0.14 0.20 -0.04 0.21 0.20 0.14 0.18 0.05 0.08 0.21 0.12 0.11 0.20 0.14 0.32 0.13 0.21 0.22 0.15 0.20 0.22 0.09 0.13 0.08 0.13 0.11 0.12 0.28 0.06 0.17 0.12 0.12 0.12 0.17 -0.18 0.15 0.26 

0.13 0.19 0.19 0.21 0.28 0.20 0.18 0.27 0.26 0.23 0.19 0.10 0.12 0.14 0.23 0.10 0.14 0.24 0.26 0.43 0.18 0.07 0.48 0.16 0.17 0.18 0.18 0.15 0.09 0.18 0.05 0.02 0.29 0.18 0.36 0.23 0.52 -0.04 0.29 0.20 0.12 0.16 1.00 0.35 0.12 0.47 0.25 0.26 0.11 0.29 0.10 0.29 0.35 0.08 0.10 0.45 0.14 0.18 0.47 0.49 0.22 0.19 0.26 0.50 0.25 0.20 0.20 0.19 0.24 0.16 0.09 0.08 0.21 0.43 0.18 0.03 0.20 0.34 0.11 0.41 0.17 0.14 0.20 0.21 0.41 0.28 0.26 0.17 0.10 0.20 0.20 0.23 0.26 0.24 0.16 0.24 0.20 0.33 0.18 0.24 0.49 -0.05 0.27 0.30 0.23 0.17 0.28 0.12 0.18 0.35 

0.20 0.16 0.35 0.21 0.27 0.43 0.43 0.33 0.40 0.13 0.28 0.17 0.36 0.24 0.17 0.22 0.18 0.18 0.25 0.07 0.22 0.38 0.21 0.19 0.20 0.32 0.32 0.38 0.17 0.21 0.22 0.19 0.16 0.14 0.20 0.40 0.19 0.21 0.24 0.47 0.34 0.20 0.35 1.00 0.26 0.45 0.18 0.07 0.23 0.35 0.12 0.21 0.18 0.22 0.15 0.18 0.07 0.23 0.34 0.17 0.24 0.12 0.20 0.15 0.24 0.21 0.46 0.17 0.14 0.37 0.20 -0.10 0.28 0.52 0.25 0.02 0.22 0.37 0.28 0.21 0.08 0.08 0.23 0.13 0.16 0.22 0.16 0.23 0.14 0.26 0.23 0.23 0.25 0.29 0.10 0.16 0.17 0.21 0.14 0.12 0.30 0.13 0.21 0.22 0.23 0.22 0.45 0.26 0.42 0.28 

0.11 0.09 0.19 0.05 0.12 0.24 0.28 0.26 0.18 0.09 0.18 0.09 0.18 0.10 0.04 0.22 0.11 0.13 0.18 0.03 0.13 0.27 0.16 0.12 0.13 0.25 0.25 0.29 0.05 0.13 0.16 0.12 0.11 0.11 0.13 0.26 0.13 0.18 0.04 0.30 0.17 -0.13 0.12 0.26 1.00 0.33 0.12 0.04 0.25 0.15 0.07 0.10 0.10 0.08 0.09 0.28 0.05 0.17 0.08 0.07 0.14 0.14 0.13 0.09 0.18 0.24 0.31 0.16 0.20 0.21 0.08 0.16 0.17 0.21 0.14 0.04 0.10 0.15 0.16 0.12 0.06 0.04 0.11 0.08 0.12 0.12 0.07 0.12 0.10 0.14 0.06 0.14 0.16 0.09 0.25 0.10 0.12 0.15 0.09 0.06 0.15 0.07 0.14 0.17 0.21 0.23 0.34 0.16 0.33 0.17 

0.31 0.35 0.37 0.25 0.32 0.46 0.42 0.42 0.38 0.28 0.32 0.19 0.34 0.28 0.21 0.24 0.19 0.29 0.41 0.32 0.25 0.32 0.42 0.22 0.23 0.36 0.34 0.41 0.21 0.25 0.23 0.12 0.33 0.26 0.38 0.37 0.43 0.12 0.31 0.45 0.37 0.21 0.47 0.45 0.33 1.00 0.33 0.23 0.12 0.49 0.18 0.39 0.37 0.11 0.17 0.43 0.18 0.26 0.23 0.38 0.28 0.25 0.24 0.48 0.37 0.32 0.47 0.21 0.27 0.33 0.17 0.23 0.32 0.33 0.25 0.11 0.23 0.25 0.29 0.42 0.18 0.19 0.25 0.26 0.33 0.36 0.30 0.27 0.21 0.28 0.21 0.33 0.30 0.33 0.28 0.28 0.29 0.31 0.23 0.22 0.46 0.06 0.34 0.36 0.33 0.27 0.51 0.28 0.42 0.46 

0.35 0.40 -0.01 -0.00 0.23 0.17 0.14 0.39 0.19 0.28 0.15 0.01 0.13 0.05 0.02 0.15 0.07 0.35 0.36 -0.11 0.31 0.08 0.24 0.12 0.10 0.19 0.14 0.22 -0.02 -0.03 0.12 -0.10 0.23 0.17 0.29 0.19 0.51 0.13 0.36 0.21 0.13 0.16 0.25 0.18 0.12 0.33 1.00 0.32 0.02 0.46 0.05 0.47 0.30 -0.10 -0.01 0.29 0.22 0.09 0.24 0.17 0.25 0.18 0.30 0.37 0.23 0.34 0.26 0.07 0.09 0.15 0.04 0.18 0.28 0.15 0.16 0.27 0.13 0.16 0.11 0.30 0.29 0.28 0.10 0.18 0.19 0.19 0.23 0.19 0.19 0.16 0.12 0.25 0.09 0.15 0.13 0.24 0.40 0.12 0.13 -0.02 0.26 -0.13 0.28 0.34 0.23 0.15 0.23 0.01 0.09 0.45 

0.05 0.23 0.12 0.21 0.11 0.15 0.10 0.16 -0.07 0.24 0.24 0.25 0.13 0.18 0.11 0.00 -0.16 0.19 0.15 0.31 0.01 0.02 0.18 0.15 0.13 0.10 0.07 0.08 0.10 0.04 0.06 -0.01 0.13 0.09 0.19 0.13 0.26 -0.07 0.28 0.02 0.14 0.07 0.26 0.07 0.04 0.23 0.32 1.00 0.07 0.40 0.03 0.41 0.20 -0.00 0.14 0.23 0.27 0.05 0.11 0.19 0.11 0.14 -0.03 0.28 0.14 0.09 0.10 -0.08 0.11 -0.09 0.13 0.06 0.07 0.05 0.02 0.23 0.03 0.05 -0.09 0.21 0.29 0.23 0.14 0.10 0.24 0.14 0.28 0.05 0.10 0.08 0.06 0.20 0.01 0.18 0.09 0.16 0.28 0.06 0.18 -0.07 0.30 -0.06 0.23 0.29 0.15 0.07 0.10 0.19 0.01 0.18 

0.06 0.07 0.13 0.22 0.13 0.13 0.13 0.15 0.22 0.02 0.14 0.01 0.18 0.14 0.23 0.03 0.04 0.07 0.10 0.07 0.07 -0.03 0.06 0.15 0.16 0.03 0.04 0.10 0.08 0.07 0.13 0.10 0.04 0.07 0.07 0.14 0.06 0.06 0.33 0.13 0.29 -0.00 0.11 0.23 0.25 0.12 0.02 0.07 1.00 0.01 0.11 0.05 0.08 0.09 0.18 0.13 0.03 0.14 0.13 0.05 0.12 -0.01 0.10 0.02 0.13 0.15 0.08 0.06 -0.12 0.17 0.12 -0.00 0.14 0.11 0.17 0.01 0.16 0.17 0.17 0.07 0.02 0.04 0.12 0.08 0.06 0.13 0.04 0.14 0.09 0.13 0.28 0.12 0.13 0.27 -0.35 0.04 0.04 0.09 0.10 0.07 0.08 0.10 0.08 0.09 0.11 0.15 0.05 0.19 0.07 0.05 

0.34 0.37 0.27 0.20 0.25 0.35 0.33 0.33 0.31 0.38 0.27 0.15 0.23 0.18 0.06 0.27 0.11 0.26 0.42 0.20 0.17 0.23 0.26 0.17 0.16 0.31 0.28 0.26 0.19 0.14 0.20 0.13 0.29 0.18 0.33 0.33 0.29 0.10 0.38 0.28 0.23 0.24 0.29 0.35 0.15 0.49 0.46 0.40 0.01 1.00 0.05 0.48 0.30 0.10 0.10 0.35 0.28 0.14 0.28 0.30 0.24 0.26 0.12 0.40 0.27 0.23 0.35 0.09 0.17 0.29 0.16 0.20 0.24 0.27 0.19 0.24 0.18 0.20 0.20 0.38 0.26 0.24 0.15 0.21 0.23 0.25 0.30 0.24 0.23 0.25 0.16 0.27 0.17 0.22 0.23 0.26 0.39 0.17 0.17 0.00 0.39 -0.05 0.28 0.36 0.16 0.17 0.36 0.18 0.28 0.46 

0.03 0.05 0.06 -0.01 0.11 0.14 0.03 0.22 0.13 -0.13 0.11 -0.05 0.29 0.22 0.25 0.09 0.20 0.26 0.04 0.07 0.15 0.07 0.10 0.06 0.06 0.11 0.10 0.37 -0.19 0.09 0.15 -0.07 0.07 0.18 0.12 0.12 0.12 0.06 0.01 0.15 0.16 0.07 0.10 0.12 0.07 0.18 0.05 0.03 0.11 0.05 1.00 0.06 0.06 -0.37 0.14 0.09 -0.27 0.11 0.09 -0.15 0.06 -0.02 0.17 0.05 0.17 0.12 0.22 0.31 0.10 0.09 -0.18 0.04 0.36 0.01 0.15 0.02 0.03 0.06 0.19 0.06 0.02 -0.00 0.08 0.06 0.05 0.10 0.04 0.06 0.03 0.05 0.07 0.10 0.18 0.03 0.00 0.03 0.02 0.14 0.09 0.09 0.08 -0.01 0.10 0.05 0.04 0.02 0.11 0.16 0.09 0.19 

0.37 0.44 0.21 0.21 0.36 0.29 0.13 0.32 0.33 0.40 0.20 0.04 0.16 0.12 0.10 0.03 0.17 0.28 0.49 0.23 0.10 0.06 0.31 0.22 0.21 0.25 0.21 0.23 0.14 0.12 0.21 0.14 0.29 0.19 0.28 0.25 0.27 0.06 0.46 0.37 0.21 0.23 0.29 0.21 0.10 0.39 0.47 0.41 0.05 0.48 0.06 1.00 0.29 -0.01 0.06 0.37 0.31 0.23 0.32 0.26 0.24 0.18 0.10 0.43 0.31 0.20 0.24 -0.04 0.08 0.21 0.03 0.26 0.18 0.15 0.23 0.26 0.27 0.19 0.24 0.39 0.33 0.33 0.20 0.26 0.27 0.31 0.35 0.20 0.29 0.28 0.23 0.25 0.20 0.30 0.21 0.29 0.40 0.14 0.19 0.01 0.44 -0.09 0.35 0.39 0.51 0.26 0.21 0.07 0.12 0.39 

0.30 0.32 0.14 0.19 0.27 0.22 0.14 0.26 0.21 0.24 0.12 0.10 0.13 0.16 0.11 0.14 0.09 0.28 0.24 0.21 0.13 0.02 0.36 0.14 0.14 0.16 0.13 0.14 0.14 0.07 0.15 0.04 0.27 0.25 0.34 0.20 0.33 0.04 0.25 0.18 0.18 0.14 0.35 0.18 0.10 0.37 0.30 0.20 0.08 0.30 0.06 0.29 1.00 0.02 0.12 0.30 0.15 0.13 0.29 0.31 0.21 0.22 0.11 0.40 0.25 0.22 0.22 0.07 0.12 0.12 0.09 0.16 0.21 0.13 0.19 0.09 0.17 0.18 0.14 0.34 0.18 0.16 0.16 0.22 0.26 0.27 0.26 0.18 0.16 0.20 0.17 0.31 0.13 0.22 0.17 0.23 0.30 0.29 0.20 0.17 0.35 -0.02 0.27 0.31 0.24 0.15 0.22 0.15 0.13 0.35 

-0.00 -0.03 0.39 0.31 0.00 0.26 0.34 -0.10 0.17 0.20 0.18 0.25 0.09 0.16 0.08 0.05 -0.05 -0.27 0.01 0.13 0.02 0.16 0.00 0.09 0.07 0.04 0.10 -0.23 0.47 0.26 -0.04 0.25 0.02 -0.12 -0.05 0.21 -0.12 0.09 0.00 0.14 0.16 0.07 0.08 0.22 0.08 0.11 -0.10 -0.00 0.09 0.10 -0.37 -0.01 0.02 1.00 0.10 -0.02 0.29 0.17 0.04 0.36 0.05 0.16 -0.02 -0.00 -0.03 -0.02 -0.00 -0.07 0.15 0.14 0.33 -0.03 -0.04 0.27 0.06 0.04 0.13 0.15 0.06 -0.01 -0.03 -0.03 0.09 -0.03 0.02 0.05 0.01 0.09 -0.00 0.10 0.11 0.10 0.15 0.12 0.08 0.04 0.03 0.08 0.07 0.03 0.07 0.24 0.02 0.02 0.07 0.07 0.14 0.20 0.20 0.10 

0.03 -0.01 0.19 0.30 0.05 0.26 0.21 0.22 0.20 -0.02 0.17 0.14 0.23 0.22 0.21 0.07 0.02 0.07 -0.01 0.12 0.06 -0.04 0.08 0.22 0.23 0.01 0.12 0.17 0.17 0.12 0.30 0.12 0.05 0.14 0.10 0.18 0.07 0.08 -0.02 0.12 0.19 0.11 0.10 0.15 0.09 0.17 -0.01 0.14 0.18 0.10 0.14 0.06 0.12 0.10 1.00 0.07 -0.06 0.21 0.02 0.07 0.14 0.16 0.10 0.01 0.20 0.17 0.15 0.13 0.16 0.08 0.16 -0.00 0.24 0.10 0.15 -0.00 0.11 0.17 0.14 0.12 0.03 0.04 0.21 0.11 0.10 0.17 0.09 -0.09 0.03 0.19 0.14 0.21 0.18 0.19 0.16 -0.02 0.18 0.10 0.20 0.08 0.20 0.22 0.04 0.03 0.10 0.15 0.19 0.30 0.22 0.06 

0.26 0.27 0.13 0.09 0.20 0.16 0.13 0.27 0.19 0.26 0.13 0.04 0.07 0.09 0.10 0.12 0.11 0.22 0.47 0.33 0.13 0.11 0.37 0.10 0.11 0.19 0.16 0.26 -0.05 0.13 0.10 -0.00 0.26 0.21 0.29 0.22 0.42 -0.02 0.24 0.20 0.10 0.07 0.45 0.18 0.28 0.43 0.29 0.23 0.13 0.35 0.09 0.37 0.30 -0.02 0.07 1.00 0.15 0.22 0.34 0.35 0.16 0.09 0.15 0.50 0.22 0.20 0.19 0.15 0.19 0.17 0.03 0.16 0.16 0.15 0.15 0.12 0.14 0.14 0.11 0.37 0.16 0.15 0.11 0.22 0.26 0.24 0.30 0.13 0.14 0.16 0.11 0.21 0.18 0.19 0.16 0.21 0.24 0.24 0.14 0.17 0.43 -0.07 0.24 0.28 0.22 0.34 0.22 0.05 0.17 0.38 

0.09 0.43 0.18 0.23 0.35 0.18 0.20 -0.01 0.14 0.38 0.11 -0.03 0.01 0.06 0.03 0.02 0.08 -0.01 0.25 0.25 -0.07 0.02 0.31 0.14 0.12 0.19 0.12 -0.14 0.37 0.17 0.01 0.10 0.17 0.05 0.04 0.10 0.07 -0.03 0.22 0.11 0.08 0.05 0.14 0.07 0.05 0.18 0.22 0.27 0.03 0.28 -0.27 0.31 0.15 0.29 -0.06 0.15 1.00 0.08 0.11 0.27 0.10 0.03 -0.07 0.26 -0.03 -0.00 -0.12 -0.21 0.02 0.13 0.05 0.20 -0.17 0.07 0.13 0.40 0.18 0.07 0.22 0.12 0.41 0.34 0.13 0.18 0.16 0.19 0.40 0.11 0.17 0.10 0.08 0.18 0.02 0.14 0.16 0.34 0.25 0.06 0.24 -0.11 0.21 0.00 0.27 0.33 0.30 0.15 0.07 0.08 -0.02 0.18 

0.05 0.10 0.22 0.22 0.19 0.37 0.24 0.31 0.29 0.10 0.17 0.14 0.09 0.14 0.18 0.24 0.16 0.19 0.24 0.10 0.18 0.09 0.29 0.33 0.33 0.24 0.29 0.38 0.07 0.20 0.36 0.24 0.10 0.18 0.11 0.32 0.11 0.23 0.07 0.23 0.13 0.17 0.18 0.23 0.17 0.26 0.09 0.05 0.14 0.14 0.11 0.23 0.13 0.17 0.21 0.22 0.08 1.00 0.08 -0.07 0.35 0.16 0.21 0.11 0.30 0.25 0.24 0.18 0.22 0.19 0.05 0.24 0.35 0.19 0.25 0.04 0.33 0.28 0.23 0.16 0.08 0.10 0.39 0.18 0.17 0.33 0.19 0.24 0.06 0.39 0.34 0.21 0.45 0.41 0.35 0.22 0.12 0.22 0.25 0.18 0.37 0.19 0.19 0.20 0.38 0.51 0.25 0.15 0.25 0.14 

0.23 0.23 0.12 0.15 0.30 0.12 0.12 0.13 0.25 0.24 0.10 0.03 0.10 0.07 0.18 0.07 0.11 0.17 0.31 0.27 0.11 0.04 0.31 0.08 0.08 0.15 0.11 0.08 0.09 0.10 0.03 0.02 0.23 0.12 0.27 0.20 0.31 0.11 0.34 0.18 0.13 0.11 0.47 0.34 0.08 0.23 0.24 0.11 0.13 0.28 0.09 0.32 0.29 0.04 0.02 0.34 0.11 0.08 1.00 0.40 0.14 0.04 0.23 0.41 0.26 0.13 0.12 0.10 0.04 0.12 -0.01 0.03 0.13 0.21 0.15 0.10 0.17 0.25 0.14 0.33 0.13 0.11 0.03 0.17 0.23 0.20 0.17 0.10 0.14 0.15 0.14 0.15 0.15 0.18 0.02 0.17 0.20 0.18 0.09 0.14 0.29 -0.08 0.19 0.22 0.17 0.09 0.12 0.06 0.06 0.31 

0.16 0.19 0.17 0.18 0.25 0.14 0.09 0.10 0.22 0.28 0.11 0.11 0.16 0.08 0.12 0.05 0.04 0.04 0.19 0.35 0.08 0.06 0.32 0.08 0.08 0.08 0.09 -0.11 0.31 0.21 -0.05 -0.02 0.25 0.06 0.26 0.17 0.38 -0.06 0.28 0.13 0.10 0.09 0.49 0.17 0.07 0.38 0.17 0.19 0.05 0.30 -0.15 0.26 0.31 0.36 0.07 0.35 0.27 -0.07 0.40 1.00 0.11 0.16 0.10 0.48 0.15 0.11 0.09 0.03 0.19 0.12 0.18 0.04 0.06 0.15 0.07 0.15 0.12 0.12 0.06 0.32 0.11 0.10 0.08 0.13 0.26 0.19 0.19 0.09 0.10 0.12 0.11 0.18 0.18 0.14 0.11 0.17 0.21 0.22 0.13 0.15 0.30 -0.02 0.18 0.21 0.17 0.00 0.15 0.11 0.11 0.30 

0.13 0.24 -0.04 0.22 0.26 0.44 0.26 0.49 0.37 0.05 0.18 0.16 0.22 0.17 0.23 0.31 0.16 0.43 0.14 0.01 0.43 0.08 0.38 0.38 0.39 0.34 0.33 0.37 0.14 0.25 0.36 0.16 0.18 0.19 0.32 0.29 0.27 0.24 0.13 0.21 0.18 0.21 0.22 0.24 0.14 0.28 0.25 0.11 0.12 0.24 0.06 0.24 0.21 0.05 0.14 0.16 0.10 0.35 0.14 0.11 1.00 0.28 0.29 0.21 0.29 0.30 0.27 0.19 0.26 0.21 0.19 0.28 0.32 0.21 0.25 -0.09 0.46 0.28 0.19 0.22 0.15 0.19 0.44 0.23 0.22 0.44 0.31 0.28 0.31 0.45 0.36 0.17 0.40 0.44 0.40 0.24 0.14 0.17 0.21 0.27 0.44 0.12 0.19 0.28 0.37 0.37 0.26 0.16 0.22 0.24 

0.11 0.11 0.20 0.26 -0.06 0.28 0.17 0.39 0.27 0.15 0.17 0.29 -0.15 0.18 0.12 0.15 0.02 0.23 -0.04 0.09 0.17 0.06 0.19 0.21 0.21 0.17 0.18 0.02 0.31 0.15 0.14 0.25 0.13 0.04 0.23 0.33 0.16 0.14 0.09 0.19 0.12 0.18 0.19 0.12 0.14 0.25 0.18 0.14 -0.01 0.26 -0.02 0.18 0.22 0.16 0.16 0.09 0.03 0.16 0.04 0.16 0.28 1.00 0.18 0.16 0.30 0.29 0.29 0.20 0.39 0.09 0.25 0.16 0.31 0.16 0.09 0.01 0.18 0.19 0.00 0.21 0.09 0.09 0.26 0.06 0.21 0.23 0.16 0.23 0.16 0.30 0.23 0.19 0.28 0.24 0.37 0.17 0.16 0.19 0.14 0.11 0.31 0.07 0.25 0.21 0.25 0.13 0.26 0.19 0.24 0.31 

0.09 0.07 0.02 0.08 0.12 0.21 0.12 0.40 0.28 -0.10 0.14 0.04 0.20 0.17 0.37 0.18 0.19 0.22 0.07 -0.25 0.41 0.05 0.23 0.17 0.16 0.15 0.15 0.23 -0.00 0.14 0.18 -0.02 0.13 0.14 0.20 0.22 0.48 0.22 0.03 0.18 0.10 0.15 0.26 0.20 0.13 0.24 0.30 -0.03 0.10 0.12 0.17 0.10 0.11 -0.02 0.10 0.15 -0.07 0.21 0.23 0.10 0.29 0.18 1.00 0.15 0.29 0.48 0.33 0.42 0.35 0.16 0.06 0.14 0.36 0.20 0.21 -0.09 0.20 0.28 0.12 0.16 0.02 0.04 0.21 0.09 0.12 0.22 0.11 0.21 0.17 0.23 0.19 0.12 0.41 0.21 0.21 0.09 0.05 0.24 0.08 0.24 0.23 0.06 0.16 0.13 0.17 0.19 0.27 0.09 0.20 0.28 

0.35 0.46 0.14 0.18 0.23 0.18 0.13 0.23 0.21 0.33 0.11 0.07 0.12 0.12 0.16 0.13 0.08 0.23 0.37 0.39 0.17 0.04 0.42 0.09 0.11 0.24 0.14 0.12 0.12 0.11 0.06 -0.04 0.32 0.22 0.38 0.18 0.46 -0.08 0.37 0.18 0.20 0.16 0.50 0.15 0.09 0.48 0.37 0.28 0.02 0.40 0.05 0.43 0.40 -0.00 0.01 0.50 0.26 0.11 0.41 0.48 0.21 0.16 0.15 1.00 0.24 0.19 0.17 0.10 0.19 0.13 0.10 0.30 0.15 0.11 0.15 0.16 0.16 0.13 0.11 0.44 0.24 0.25 0.10 0.30 0.29 0.32 0.30 0.19 0.17 0.18 0.10 0.22 0.16 0.19 0.17 0.34 0.28 0.23 0.17 0.18 0.38 -0.09 0.28 0.32 0.24 0.14 0.20 0.02 0.05 0.45 

0.17 0.18 0.14 0.16 0.19 0.24 0.03 0.50 0.31 0.05 0.20 0.07 0.21 0.16 0.18 0.18 0.10 0.30 0.21 0.03 0.16 0.05 0.24 0.22 0.21 0.22 0.20 0.28 0.05 0.11 0.29 0.16 0.22 0.17 0.28 0.49 0.30 0.36 0.19 0.24 0.15 0.19 0.25 0.24 0.18 0.37 0.23 0.14 0.13 0.27 0.17 0.31 0.25 -0.03 0.20 0.22 -0.03 0.30 0.26 0.15 0.29 0.30 0.29 0.24 1.00 0.37 0.53 0.23 0.24 0.17 0.06 0.19 0.34 0.16 0.22 0.04 0.22 0.23 0.17 0.31 0.14 0.13 0.18 0.17 0.24 0.29 0.20 0.22 0.16 0.29 0.22 0.18 0.26 0.31 0.25 0.19 0.19 0.18 0.14 0.14 0.37 0.05 0.29 0.26 0.28 0.26 0.26 0.15 0.21 0.24 

0.17 0.16 0.09 0.16 0.17 0.26 0.14 0.50 0.34 0.02 0.17 0.09 0.24 0.16 0.19 0.20 0.13 0.29 0.16 -0.20 0.31 0.07 0.22 0.20 0.21 0.20 0.20 0.30 0.02 0.04 0.27 0.08 0.18 0.17 0.25 0.30 0.45 0.23 0.12 0.24 0.15 0.14 0.20 0.21 0.24 0.32 0.34 0.09 0.15 0.23 0.12 0.20 0.22 -0.02 0.17 0.20 -0.00 0.25 0.13 0.11 0.30 0.29 0.48 0.19 0.37 1.00 0.39 0.27 0.26 0.19 0.09 0.29 0.43 0.16 0.24 -0.01 0.21 0.24 0.15 0.23 0.09 0.10 0.25 0.14 0.19 0.26 0.17 0.25 0.18 0.26 0.22 0.21 0.27 0.24 0.27 0.17 0.17 0.21 0.15 0.15 0.31 0.04 0.26 0.23 0.27 0.25 0.32 0.15 0.23 0.25 

0.17 0.17 0.20 0.00 0.18 0.43 0.15 0.55 0.28 -0.03 0.28 0.14 0.36 0.25 0.11 0.28 0.16 0.32 0.15 -0.08 0.26 0.38 0.23 0.21 0.20 0.34 0.33 0.48 0.07 0.16 0.25 0.06 0.19 0.19 0.23 0.38 0.32 0.26 0.13 0.46 0.27 0.16 0.20 0.46 0.31 0.47 0.26 0.10 0.08 0.35 0.22 0.24 0.22 -0.00 0.15 0.19 -0.12 0.24 0.12 0.09 0.27 0.29 0.33 0.17 0.53 0.39 1.00 0.23 0.21 0.35 0.16 0.17 0.36 0.32 0.23 0.03 0.19 0.24 0.25 0.23 0.12 0.10 0.23 0.13 0.18 0.22 0.17 0.23 0.15 0.25 0.20 0.24 0.24 0.22 0.25 0.18 0.19 0.22 0.15 0.11 0.30 0.12 0.24 0.23 0.24 0.21 0.48 0.27 0.45 0.28 

0.13 0.02 0.11 0.14 0.02 0.18 0.12 0.32 0.27 -0.08 0.14 0.05 0.17 0.19 0.40 0.42 0.22 0.15 0.05 0.00 0.28 0.04 0.14 0.15 0.13 0.11 0.12 0.25 -0.05 0.16 0.18 0.10 0.09 0.12 0.17 0.21 0.19 0.12 -0.09 0.20 0.21 0.18 0.19 0.17 0.16 0.21 0.07 -0.08 0.06 0.09 0.31 -0.04 0.07 -0.07 0.13 0.15 -0.21 0.18 0.10 0.03 0.19 0.20 0.42 0.10 0.23 0.27 0.23 1.00 0.43 0.12 0.00 0.11 0.34 0.12 0.17 -0.12 0.14 0.20 0.10 0.13 -0.09 -0.03 0.20 0.05 0.07 0.17 0.03 0.29 0.16 0.20 0.11 0.08 0.42 0.13 0.22 0.01 -0.01 0.23 0.03 0.26 0.17 0.08 0.08 0.04 0.03 0.14 0.14 -0.03 0.19 0.32 

0.05 0.06 0.19 0.24 0.03 0.25 0.20 0.34 0.21 0.03 0.16 0.27 0.13 0.21 0.39 0.17 0.11 0.15 0.04 0.13 0.27 0.05 0.27 0.20 0.20 0.17 0.18 0.14 0.15 0.30 0.17 0.15 0.13 0.12 0.19 0.30 0.21 0.12 -0.13 0.13 0.07 0.11 0.24 0.14 0.20 0.27 0.09 0.11 -0.12 0.17 0.10 0.08 0.12 0.15 0.16 0.19 0.02 0.22 0.04 0.19 0.26 0.39 0.35 0.19 0.24 0.26 0.21 0.43 1.00 0.08 0.20 0.16 0.24 0.11 0.11 -0.07 0.16 0.14 -0.04 0.16 0.02 0.05 0.26 0.08 0.17 0.25 0.13 0.19 0.16 0.23 0.04 0.15 0.45 0.15 0.47 0.11 0.06 0.26 0.12 0.26 0.26 0.13 0.18 0.15 0.20 0.21 0.27 0.14 0.24 0.33 

0.20 0.15 0.26 0.13 0.22 0.32 0.32 0.27 0.41 0.13 0.20 -0.20 0.26 0.08 0.08 0.16 0.22 0.14 0.30 0.04 0.19 0.25 0.16 0.13 0.14 0.28 0.28 0.33 0.08 0.15 0.22 0.15 0.20 0.15 0.18 0.29 0.14 0.11 0.18 0.38 0.27 0.18 0.16 0.37 0.21 0.33 0.15 -0.09 0.17 0.29 0.09 0.21 0.12 0.14 0.08 0.17 0.13 0.19 0.12 0.12 0.21 0.09 0.16 0.13 0.17 0.19 0.35 0.12 0.08 1.00 0.14 0.15 0.20 0.28 0.32 0.03 0.36 0.21 0.48 0.17 0.09 0.07 0.16 0.13 0.11 0.18 0.14 0.25 0.14 0.18 0.18 0.18 0.19 0.03 0.08 0.16 0.15 0.16 0.12 0.10 0.21 0.08 0.17 0.17 0.20 0.19 0.34 0.16 0.34 0.25 

0.06 0.08 0.22 0.26 -0.04 0.28 0.31 0.12 0.12 0.04 0.18 0.36 0.22 0.22 0.17 0.09 -0.22 0.07 0.00 0.10 0.20 0.12 0.04 0.13 0.13 0.06 0.09 0.01 0.34 0.19 0.07 0.08 -0.05 -0.00 0.10 0.16 0.06 0.05 0.04 0.13 0.31 0.10 0.09 0.20 0.08 0.17 0.04 0.13 0.12 0.16 -0.18 0.03 0.09 0.33 0.16 0.03 0.05 0.05 -0.01 0.18 0.19 0.25 0.06 0.10 0.06 0.09 0.16 0.00 0.20 0.14 1.00 0.01 0.07 0.14 0.03 -0.01 0.10 0.08 -0.07 0.08 0.01 0.04 0.16 0.04 0.04 0.14 0.09 0.11 0.10 0.14 0.09 0.15 0.11 0.18 0.09 0.07 0.05 0.08 0.12 0.09 0.13 0.26 0.09 0.07 0.05 0.08 0.19 0.29 0.18 0.13 

0.20 0.34 0.14 0.16 0.18 0.29 0.14 0.28 0.22 0.14 0.10 0.05 0.10 0.11 0.12 0.22 0.15 0.17 0.22 0.09 0.14 0.05 0.27 0.21 0.23 0.33 0.23 0.22 0.14 0.16 0.26 0.14 0.15 0.19 0.14 0.19 0.13 0.11 0.09 0.18 0.19 0.11 0.08 -0.10 0.16 0.23 0.18 0.06 -0.00 0.20 0.04 0.26 0.16 -0.03 -0.00 0.16 0.20 0.24 0.03 0.04 0.28 0.16 0.14 0.30 0.19 0.29 0.17 0.11 0.16 0.15 0.01 1.00 0.13 0.03 0.21 0.03 0.25 0.13 0.21 0.17 0.18 0.20 0.26 0.23 0.12 0.33 0.23 0.25 0.24 0.29 0.18 0.14 0.24 0.24 0.38 0.30 0.14 0.12 0.17 0.15 0.26 0.05 0.21 0.23 0.37 0.31 0.18 -0.01 0.09 0.24 

0.13 0.11 0.14 0.17 0.17 0.29 0.16 0.47 0.39 0.01 0.19 0.14 0.29 0.18 0.20 0.22 0.11 0.35 0.15 -0.08 0.39 0.07 0.17 0.20 0.21 0.21 0.23 0.42 -0.09 -0.07 0.37 0.14 0.16 0.20 0.30 0.31 0.31 0.17 0.11 0.27 0.15 0.23 0.21 0.28 0.17 0.32 0.28 0.07 0.14 0.24 0.36 0.18 0.21 -0.04 0.24 0.16 -0.17 0.35 0.13 0.06 0.32 0.31 0.36 0.15 0.34 0.43 0.36 0.34 0.24 0.20 0.07 0.13 1.00 0.20 0.27 0.03 0.18 0.32 0.12 0.21 0.06 0.06 0.30 0.12 0.17 0.24 0.14 0.27 0.11 0.26 0.26 0.27 0.34 0.27 0.24 0.18 0.18 0.25 0.19 0.11 0.31 0.02 0.28 0.22 0.22 0.22 0.29 0.20 0.26 0.23 

0.13 0.11 0.26 0.16 0.14 0.31 0.36 0.24 0.25 0.13 0.21 0.12 0.19 0.14 0.09 0.17 0.14 0.14 0.18 0.06 0.20 0.29 0.17 0.14 0.15 0.27 0.27 0.28 0.13 0.15 0.17 0.15 0.12 0.10 0.17 0.30 0.14 0.15 0.13 0.34 0.22 0.14 0.43 0.52 0.21 0.33 0.15 0.05 0.11 0.27 0.01 0.15 0.13 0.27 0.10 0.15 0.07 0.19 0.21 0.15 0.21 0.16 0.20 0.11 0.16 0.16 0.32 0.12 0.11 0.28 0.14 0.03 0.20 1.00 0.18 0.01 0.18 0.48 0.21 0.15 0.07 0.06 0.17 0.10 0.11 0.16 0.11 0.18 0.09 0.23 0.24 0.17 0.20 0.19 0.14 0.14 0.13 0.16 0.10 0.09 0.22 0.09 0.15 0.18 0.18 0.18 0.36 0.19 0.33 0.24 

0.19 0.19 0.20 0.21 0.21 0.25 0.20 0.31 0.37 0.13 0.15 -0.10 0.21 0.16 0.18 0.13 0.15 0.08 0.22 0.05 0.16 0.08 0.16 0.17 0.17 0.20 0.20 0.20 0.08 0.07 0.25 0.20 0.26 0.13 0.18 0.25 0.14 0.08 0.14 0.27 0.19 0.20 0.18 0.25 0.14 0.25 0.16 0.02 0.17 0.19 0.15 0.23 0.19 0.06 0.15 0.15 0.13 0.25 0.15 0.07 0.25 0.09 0.21 0.15 0.22 0.24 0.23 0.17 0.11 0.32 0.03 0.21 0.27 0.18 1.00 0.01 0.50 0.41 0.32 0.17 0.09 0.09 0.22 0.14 0.17 0.22 0.14 0.33 0.17 0.19 0.30 0.26 0.26 0.19 0.17 0.18 0.21 0.22 0.17 0.10 0.25 0.01 0.21 0.18 0.27 0.22 0.23 0.14 0.20 0.24 

0.16 0.49 0.05 -0.02 0.19 0.05 0.02 -0.02 -0.01 0.26 0.07 -0.01 0.01 0.00 -0.09 0.01 -0.07 0.13 0.20 0.10 -0.10 0.05 0.06 -0.04 -0.04 0.04 0.02 0.04 0.09 -0.04 -0.02 -0.05 0.06 0.06 0.02 0.06 0.04 -0.01 0.20 0.05 0.06 -0.04 0.03 0.02 0.04 0.11 0.27 0.23 0.01 0.24 0.02 0.26 0.09 0.04 -0.00 0.12 0.40 0.04 0.10 0.15 -0.09 0.01 -0.09 0.16 0.04 -0.01 0.03 -0.12 -0.07 0.03 -0.01 0.03 0.03 0.01 0.01 1.00 -0.07 0.00 0.10 0.11 0.37 0.32 -0.17 0.11 0.01 -0.00 0.15 0.00 0.08 -0.03 0.00 0.15 -0.10 -0.04 -0.05 0.16 0.26 -0.01 0.16 -0.33 -0.02 -0.03 0.18 0.16 0.07 -0.02 -0.04 0.02 -0.01 0.16 

0.17 0.23 0.20 0.27 0.28 0.38 0.18 0.30 0.37 0.13 0.15 0.06 0.14 0.17 0.21 0.23 0.17 0.20 0.19 0.08 0.24 0.07 0.29 0.31 0.32 0.27 0.26 0.22 0.21 0.23 0.25 0.22 0.23 0.14 0.19 0.25 0.14 0.17 0.16 0.22 0.21 0.21 0.20 0.22 0.10 0.23 0.13 0.03 0.16 0.18 0.03 0.27 0.17 0.13 0.11 0.14 0.18 0.33 0.17 0.12 0.46 0.18 0.20 0.16 0.22 0.21 0.19 0.14 0.16 0.36 0.10 0.25 0.18 0.18 0.50 -0.07 1.00 0.26 0.27 0.20 0.12 0.16 0.36 0.19 0.19 0.37 0.25 0.29 0.27 0.40 0.42 0.16 0.35 0.38 0.27 0.23 0.13 0.16 0.19 0.23 0.37 0.13 0.18 0.22 0.36 0.29 0.19 0.11 0.17 0.23 

0.16 0.16 0.21 0.23 0.19 0.26 0.24 0.34 0.37 0.12 0.16 0.05 0.19 0.14 0.19 0.16 0.15 0.16 0.18 0.05 0.17 0.08 0.18 0.19 0.20 0.19 0.20 0.22 0.08 0.10 0.25 0.22 0.13 0.12 0.18 0.29 0.15 0.12 0.14 0.25 0.17 0.20 0.34 0.37 0.15 0.25 0.16 0.05 0.17 0.20 0.06 0.19 0.18 0.15 0.17 0.14 0.07 0.28 0.25 0.12 0.28 0.19 0.28 0.13 0.23 0.24 0.24 0.20 0.14 0.21 0.08 0.13 0.32 0.48 0.41 0.00 0.26 1.00 0.21 0.18 0.07 0.09 0.25 0.13 0.15 0.22 0.13 0.33 0.14 0.28 0.31 0.27 0.28 0.26 0.20 0.17 0.20 0.23 0.17 0.12 0.27 0.05 0.21 0.19 0.26 0.24 0.26 0.15 0.22 0.23 

0.18 0.27 0.27 0.21 0.28 0.46 0.30 0.21 0.38 0.12 0.19 -0.40 0.27 0.20 0.16 0.21 0.48 0.07 0.23 0.10 0.13 0.17 0.20 0.20 0.21 0.28 0.27 0.30 0.30 0.24 0.27 0.12 0.18 0.21 0.08 0.25 0.09 0.21 0.15 0.35 0.34 0.14 0.11 0.28 0.16 0.29 0.11 -0.09 0.17 0.20 0.19 0.24 0.14 0.06 0.14 0.11 0.22 0.23 0.14 0.06 0.19 0.00 0.12 0.11 0.17 0.15 0.25 0.10 -0.04 0.48 -0.07 0.21 0.12 0.21 0.32 0.10 0.27 0.21 1.00 0.13 0.18 0.12 0.20 0.20 0.08 0.28 0.21 0.15 0.23 0.23 0.24 0.20 0.23 -0.03 0.16 0.14 0.12 0.13 0.22 0.13 0.21 0.15 0.11 0.16 0.29 0.21 0.25 0.27 0.24 0.22 

0.30 0.37 0.18 0.21 0.19 0.23 0.14 0.31 0.28 0.27 0.18 0.09 0.14 0.12 0.12 0.16 0.08 0.21 0.31 0.23 0.13 0.05 0.34 0.20 0.18 0.15 0.17 0.18 0.11 0.10 0.14 0.11 0.33 0.23 0.37 0.24 0.34 0.05 0.35 0.19 0.20 0.18 0.41 0.21 0.12 0.42 0.30 0.21 0.07 0.38 0.06 0.39 0.34 -0.01 0.12 0.37 0.12 0.16 0.33 0.32 0.22 0.21 0.16 0.44 0.31 0.23 0.23 0.13 0.16 0.17 0.08 0.17 0.21 0.15 0.17 0.11 0.20 0.18 0.13 1.00 0.12 0.21 0.13 0.36 0.38 0.36 0.16 0.21 0.17 0.34 0.16 0.21 0.16 0.22 0.18 0.21 0.25 0.23 0.15 0.24 0.51 0.02 0.21 0.21 0.20 0.19 0.25 0.05 0.17 0.35 

0.11 0.40 0.09 0.11 0.35 0.18 0.06 0.16 0.12 0.22 0.11 -0.01 0.08 0.06 0.03 0.06 0.06 0.29 0.18 0.16 0.01 0.02 0.35 0.13 0.17 0.23 0.14 0.13 0.10 0.04 0.09 0.02 0.13 0.15 0.12 0.11 0.17 -0.01 0.21 0.10 0.06 0.05 0.17 0.08 0.06 0.18 0.29 0.29 0.02 0.26 0.02 0.33 0.18 -0.03 0.03 0.16 0.41 0.08 0.13 0.11 0.15 0.09 0.02 0.24 0.14 0.09 0.12 -0.09 0.02 0.09 0.01 0.18 0.06 0.07 0.09 0.37 0.12 0.07 0.18 0.12 1.00 0.35 -0.00 0.20 0.18 0.18 0.36 0.07 0.14 0.11 0.09 0.19 0.02 0.14 0.15 0.26 0.25 0.07 0.24 -0.11 0.23 -0.03 0.30 0.30 0.24 0.13 0.13 0.11 0.08 0.17 

0.16 0.44 0.09 0.15 0.26 0.20 0.05 0.16 0.14 0.22 0.11 0.05 0.08 0.08 0.08 0.09 0.04 0.22 0.17 0.13 0.06 -0.01 0.28 0.18 0.18 0.14 0.11 0.12 0.12 0.06 0.11 0.05 0.12 0.15 0.13 0.09 0.14 0.02 0.21 0.08 0.12 0.08 0.14 0.08 0.04 0.19 0.28 0.23 0.04 0.24 -0.00 0.33 0.16 -0.03 0.04 0.15 0.34 0.10 0.11 0.10 0.19 0.09 0.04 0.25 0.13 0.10 0.10 -0.03 0.05 0.07 0.04 0.20 0.06 0.06 0.09 0.32 0.16 0.09 0.12 0.21 0.35 1.00 0.16 0.23 0.14 0.18 0.28 0.11 0.16 0.19 0.11 0.15 0.06 0.20 0.16 0.26 0.25 0.06 0.19 -0.03 0.23 -0.01 0.24 0.23 0.24 0.15 0.09 0.04 0.01 0.20 

-0.00 0.13 0.23 0.35 0.28 0.49 0.23 0.36 0.38 0.05 0.19 0.20 0.24 0.24 0.28 0.25 0.21 0.28 0.08 0.14 0.23 0.02 0.35 0.40 0.42 0.30 0.30 0.30 0.24 0.27 0.42 0.22 0.13 0.20 0.17 0.18 0.17 0.11 0.06 0.22 0.18 0.21 0.20 0.23 0.11 0.25 0.10 0.14 0.12 0.15 0.08 0.20 0.16 0.09 0.21 0.11 0.13 0.39 0.03 0.08 0.44 0.26 0.21 0.10 0.18 0.25 0.23 0.20 0.26 0.16 0.16 0.26 0.30 0.17 0.22 -0.17 0.36 0.25 0.20 0.13 -0.00 0.16 1.00 0.18 0.24 0.43 0.34 0.24 0.23 0.42 0.32 0.22 0.43 0.45 0.42 0.24 0.10 0.20 0.29 0.29 0.46 0.18 0.26 0.29 0.42 0.36 0.25 0.24 0.21 0.16 

0.25 0.41 0.12 0.19 0.20 0.27 0.12 0.21 0.20 0.16 0.11 0.04 0.14 0.12 0.13 0.16 0.15 0.19 0.25 0.15 0.11 -0.01 0.38 0.25 0.23 0.16 0.15 0.22 0.12 0.10 0.24 0.07 0.18 0.32 0.22 0.12 0.18 0.07 0.20 0.13 0.20 0.12 0.21 0.13 0.08 0.26 0.18 0.10 0.08 0.21 0.06 0.26 0.22 -0.03 0.11 0.22 0.18 0.18 0.17 0.13 0.23 0.06 0.09 0.30 0.17 0.14 0.13 0.05 0.08 0.13 0.04 0.23 0.12 0.10 0.14 0.11 0.19 0.13 0.20 0.36 0.20 0.23 0.18 1.00 0.23 0.39 0.21 0.16 0.19 0.24 0.13 0.17 0.16 0.23 0.18 0.20 0.15 0.14 0.21 0.23 0.33 0.08 0.15 0.14 0.22 0.23 0.16 0.03 0.11 0.22 

0.03 0.09 0.16 0.14 0.25 0.19 0.14 0.26 0.22 0.21 0.18 0.10 0.11 0.11 0.08 0.08 0.09 0.21 0.13 0.26 0.11 0.07 0.37 0.19 0.21 0.17 0.16 0.13 0.10 0.10 0.07 0.12 0.28 0.13 0.31 0.22 0.33 0.02 0.25 0.24 0.01 0.11 0.41 0.16 0.12 0.33 0.19 0.24 0.06 0.23 0.05 0.27 0.26 0.02 0.10 0.26 0.16 0.17 0.23 0.26 0.22 0.21 0.12 0.29 0.24 0.19 0.18 0.07 0.17 0.11 0.04 0.12 0.17 0.11 0.17 0.01 0.19 0.15 0.08 0.38 0.18 0.14 0.24 0.23 1.00 0.24 0.23 0.13 0.02 -0.01 0.16 0.17 0.17 0.22 0.19 0.19 0.17 0.22 0.14 0.17 0.51 -0.04 0.28 0.28 0.23 0.14 0.26 0.15 0.22 0.16 

0.21 0.46 0.24 0.35 0.32 0.50 0.24 0.37 0.36 0.11 0.18 0.16 0.24 0.24 0.31 0.26 0.22 0.32 0.27 0.22 0.26 0.01 0.49 0.36 0.36 0.34 0.31 0.31 0.30 0.32 0.40 0.19 0.28 0.44 0.32 0.26 0.26 0.14 0.14 0.23 0.27 0.20 0.28 0.22 0.12 0.36 0.19 0.14 0.13 0.25 0.10 0.31 0.27 0.05 0.17 0.24 0.19 0.33 0.20 0.19 0.44 0.23 0.22 0.32 0.29 0.26 0.22 0.17 0.25 0.18 0.14 0.33 0.24 0.16 0.22 -0.00 0.37 0.22 0.28 0.36 0.18 0.18 0.43 0.39 0.24 1.00 0.44 0.24 0.48 0.44 0.31 0.26 0.45 0.45 0.42 0.29 0.16 0.21 0.33 0.44 0.47 0.18 0.25 0.29 0.42 0.36 0.27 0.18 0.18 0.33 

0.10 0.44 0.17 0.24 0.42 0.35 0.16 0.28 0.24 0.18 0.12 0.10 0.17 0.16 0.17 0.13 0.15 0.40 0.26 0.25 0.13 0.02 0.46 0.16 0.27 0.31 0.21 0.22 0.19 0.18 0.23 0.10 0.23 0.31 0.26 0.17 0.28 0.04 0.18 0.17 0.13 0.14 0.26 0.16 0.07 0.30 0.23 0.28 0.04 0.30 0.04 0.35 0.26 0.01 0.09 0.30 0.40 0.19 0.17 0.19 0.31 0.16 0.11 0.30 0.20 0.17 0.17 0.03 0.13 0.14 0.09 0.23 0.14 0.11 0.14 0.15 0.25 0.13 0.21 0.16 0.36 0.28 0.34 0.21 0.23 0.44 1.00 0.14 0.36 0.27 0.20 0.24 0.23 0.35 0.34 0.38 0.23 0.14 0.32 0.14 0.41 0.01 0.39 0.48 0.39 0.26 0.20 0.18 0.06 0.24 

0.21 0.26 0.19 0.22 0.15 0.24 0.20 0.34 0.35 0.14 0.15 0.08 0.18 0.13 0.19 0.28 0.11 0.16 0.22 0.05 0.17 0.07 0.17 0.18 0.17 0.20 0.18 0.18 0.10 0.12 0.20 0.21 0.15 0.10 0.17 0.27 0.15 0.09 0.18 0.22 0.35 0.32 0.17 0.23 0.12 0.27 0.19 0.05 0.14 0.24 0.06 0.20 0.18 0.09 -0.09 0.13 0.11 0.24 0.10 0.09 0.28 0.23 0.21 0.19 0.22 0.25 0.23 0.29 0.19 0.25 0.11 0.25 0.27 0.18 0.33 0.00 0.29 0.33 0.15 0.21 0.07 0.11 0.24 0.16 0.13 0.24 0.14 1.00 0.19 0.27 0.29 0.24 0.27 0.24 0.19 0.26 0.14 0.22 0.14 0.15 0.27 0.06 0.19 0.18 0.22 0.23 0.23 -0.07 0.19 0.29 

0.26 0.50 0.17 0.27 0.25 0.38 0.17 0.27 0.28 0.14 0.10 0.09 0.18 0.19 0.24 0.21 0.22 0.24 0.30 0.10 0.21 0.00 0.21 0.18 0.21 0.23 0.19 0.21 0.24 0.24 0.30 0.17 0.18 0.28 0.20 0.16 0.12 0.10 0.14 0.20 0.23 0.13 0.10 0.14 0.10 0.21 0.19 0.10 0.09 0.23 0.03 0.29 0.16 -0.00 0.03 0.14 0.17 0.06 0.14 0.10 0.31 0.16 0.17 0.17 0.16 0.18 0.15 0.16 0.16 0.14 0.10 0.24 0.11 0.09 0.17 0.08 0.27 0.14 0.23 0.17 0.14 0.16 0.23 0.19 0.02 0.48 0.36 0.19 1.00 0.33 0.18 0.16 0.27 0.29 0.32 0.13 0.12 0.08 0.18 0.22 0.21 0.14 0.17 0.24 0.32 0.26 0.14 0.12 0.12 0.34 

0.18 0.32 0.22 0.33 0.24 0.46 0.21 0.39 0.39 0.12 0.17 0.19 0.17 0.17 0.24 0.30 0.20 0.28 0.18 0.07 0.24 0.04 0.33 0.37 0.37 0.27 0.29 0.30 0.25 0.26 0.38 0.24 0.14 0.23 0.19 0.30 0.17 0.23 0.14 0.17 0.23 0.21 0.20 0.26 0.14 0.28 0.16 0.08 0.13 0.25 0.05 0.28 0.20 0.10 0.19 0.16 0.10 0.39 0.15 0.12 0.45 0.30 0.23 0.18 0.29 0.26 0.25 0.20 0.23 0.18 0.14 0.29 0.26 0.23 0.19 -0.03 0.40 0.28 0.23 0.34 0.11 0.19 0.42 0.24 -0.01 0.44 0.27 0.27 0.33 1.00 0.34 0.20 0.40 0.48 0.40 0.23 0.15 0.19 0.24 0.26 0.42 0.20 0.20 0.22 0.41 0.38 0.24 0.14 0.19 0.27 

0.11 0.17 0.17 0.19 0.21 0.33 0.16 0.31 0.32 0.08 0.14 0.09 0.12 0.22 0.17 0.17 0.11 0.19 0.11 0.04 0.21 0.11 0.22 0.25 0.26 0.22 0.23 0.21 0.17 0.18 0.21 0.18 0.14 0.12 0.16 0.26 0.14 0.16 0.33 0.20 0.20 0.22 0.20 0.23 0.06 0.21 0.12 0.06 0.28 0.16 0.07 0.23 0.17 0.11 0.14 0.11 0.08 0.34 0.14 0.11 0.36 0.23 0.19 0.10 0.22 0.22 0.20 0.11 0.04 0.18 0.09 0.18 0.26 0.24 0.30 0.00 0.42 0.31 0.24 0.16 0.09 0.11 0.32 0.13 0.16 0.31 0.20 0.29 0.18 0.34 1.00 0.17 0.38 0.41 0.11 0.23 0.12 0.19 0.21 0.17 0.33 0.11 0.18 0.20 0.30 0.26 0.18 0.13 0.17 0.18 

0.25 0.33 0.23 0.25 0.22 0.29 0.24 0.27 0.28 0.24 0.16 0.10 0.25 0.19 0.17 0.12 0.10 0.20 0.26 0.20 0.10 0.07 0.25 0.16 0.18 0.15 0.21 0.19 0.14 0.11 0.21 0.11 0.18 0.22 0.20 0.24 0.21 0.03 0.17 0.24 0.22 0.15 0.23 0.23 0.14 0.33 0.25 0.20 0.12 0.27 0.10 0.25 0.31 0.10 0.21 0.21 0.18 0.21 0.15 0.18 0.17 0.19 0.12 0.22 0.18 0.21 0.24 0.08 0.15 0.18 0.15 0.14 0.27 0.17 0.26 0.15 0.16 0.27 0.20 0.21 0.19 0.15 0.22 0.17 0.17 0.26 0.24 0.24 0.16 0.20 0.17 1.00 0.17 0.21 0.20 0.32 0.43 0.29 0.40 0.10 0.27 0.06 0.26 0.28 0.26 0.19 0.25 0.24 0.19 0.32 

0.10 0.11 0.22 0.26 0.19 0.43 0.24 0.36 0.38 0.04 0.17 0.16 0.18 0.24 0.44 0.27 0.34 0.18 0.12 0.10 0.41 0.07 0.40 0.34 0.34 0.28 0.30 0.28 0.21 0.48 0.34 0.19 0.16 0.23 0.20 0.29 0.21 0.20 0.04 0.24 0.17 0.20 0.26 0.25 0.16 0.30 0.09 0.01 0.13 0.17 0.18 0.20 0.13 0.15 0.18 0.18 0.02 0.45 0.15 0.18 0.40 0.28 0.41 0.16 0.26 0.27 0.24 0.42 0.45 0.19 0.11 0.24 0.34 0.20 0.26 -0.10 0.35 0.28 0.23 0.16 0.02 0.06 0.43 0.16 0.17 0.45 0.23 0.27 0.27 0.40 0.38 0.17 1.00 0.41 0.40 0.17 0.07 0.39 0.21 0.44 0.37 0.18 0.16 0.20 0.37 0.36 0.28 0.17 0.26 0.33 

0.11 0.27 0.22 0.31 0.30 0.44 0.22 0.38 0.33 0.13 0.17 0.44 0.18 0.20 0.27 0.23 0.08 0.31 0.19 0.14 0.24 0.07 0.38 0.36 0.37 0.28 0.27 0.30 0.23 0.26 0.35 0.23 0.14 0.21 0.21 0.34 0.21 0.22 0.25 0.22 0.20 0.22 0.24 0.29 0.09 0.33 0.15 0.18 0.27 0.22 0.03 0.30 0.22 0.12 0.19 0.19 0.14 0.41 0.18 0.14 0.44 0.24 0.21 0.19 0.31 0.24 0.22 0.13 0.15 0.03 0.18 0.24 0.27 0.19 0.19 -0.04 0.38 0.26 -0.03 0.22 0.14 0.20 0.45 0.23 0.22 0.45 0.35 0.24 0.29 0.48 0.41 0.21 0.41 1.00 0.26 0.28 0.16 0.20 0.26 0.26 0.49 0.17 0.28 0.30 0.43 0.42 0.23 0.19 0.19 0.22 

0.07 0.21 0.20 0.23 0.20 0.43 0.22 0.39 0.28 0.10 0.16 0.20 0.13 0.15 0.15 0.29 0.20 0.25 0.12 0.10 0.21 0.07 0.36 0.34 0.35 0.32 0.31 0.27 0.24 0.27 0.36 0.23 0.16 0.23 0.19 0.28 0.17 0.19 -0.18 0.20 0.02 0.09 0.16 0.10 0.25 0.28 0.13 0.09 -0.35 0.23 0.00 0.21 0.17 0.08 0.16 0.16 0.16 0.35 0.02 0.11 0.40 0.37 0.21 0.17 0.25 0.27 0.25 0.22 0.47 0.08 0.09 0.38 0.24 0.14 0.17 -0.05 0.27 0.20 0.16 0.18 0.15 0.16 0.42 0.18 0.19 0.42 0.34 0.19 0.32 0.40 0.11 0.20 0.40 0.26 1.00 0.23 0.15 0.18 0.23 0.26 0.36 0.15 0.21 0.27 0.46 0.38 0.32 0.14 0.27 0.22 

0.10 0.38 0.13 0.11 0.34 0.24 0.10 0.24 0.22 0.20 0.10 0.09 0.10 0.09 0.07 0.14 0.07 0.31 0.23 0.15 0.12 0.07 0.37 0.14 0.15 0.29 0.24 0.18 0.10 0.09 0.12 0.09 0.19 0.17 0.21 0.17 0.23 0.06 0.23 0.18 0.11 0.13 0.24 0.16 0.10 0.28 0.24 0.16 0.04 0.26 0.03 0.29 0.23 0.04 -0.02 0.21 0.34 0.22 0.17 0.17 0.24 0.17 0.09 0.34 0.19 0.17 0.18 0.01 0.11 0.16 0.07 0.30 0.18 0.14 0.18 0.16 0.23 0.17 0.14 0.21 0.26 0.26 0.24 0.20 0.19 0.29 0.38 0.26 0.13 0.23 0.23 0.32 0.17 0.28 0.23 1.00 0.25 0.26 0.43 0.08 0.32 -0.03 0.33 0.35 0.34 0.22 0.17 0.05 -0.02 0.22 

0.30 0.33 0.14 0.13 0.23 0.17 0.13 0.22 0.21 0.34 0.15 0.06 0.11 0.07 0.01 0.07 0.06 0.21 0.32 0.13 0.06 0.08 0.20 0.12 0.10 0.14 0.14 0.16 0.07 0.03 0.14 0.06 0.19 0.14 0.19 0.20 0.20 0.03 0.28 0.20 0.08 0.08 0.20 0.17 0.12 0.29 0.40 0.28 0.04 0.39 0.02 0.40 0.30 0.03 0.18 0.24 0.25 0.12 0.20 0.21 0.14 0.16 0.05 0.28 0.19 0.17 0.19 -0.01 0.06 0.15 0.05 0.14 0.18 0.13 0.21 0.26 0.13 0.20 0.12 0.25 0.25 0.25 0.10 0.15 0.17 0.16 0.23 0.14 0.12 0.15 0.12 0.43 0.07 0.16 0.15 0.25 1.00 0.13 0.21 -0.04 0.23 -0.07 0.27 0.30 0.25 0.15 0.18 0.13 0.11 0.32 

0.10 0.12 0.17 0.14 0.16 0.21 0.18 0.24 0.24 0.11 0.15 0.09 0.14 0.14 0.22 0.14 0.16 0.16 0.13 0.17 0.21 0.09 0.36 0.19 0.13 0.16 0.22 0.17 0.07 0.18 0.09 0.06 0.17 0.17 0.20 0.22 0.27 0.05 0.10 0.20 0.12 0.13 0.33 0.21 0.15 0.31 0.12 0.06 0.09 0.17 0.14 0.14 0.29 0.08 0.10 0.24 0.06 0.22 0.18 0.22 0.17 0.19 0.24 0.23 0.18 0.21 0.22 0.23 0.26 0.16 0.08 0.12 0.25 0.16 0.22 -0.01 0.16 0.23 0.13 0.23 0.07 0.06 0.20 0.14 0.22 0.21 0.14 0.22 0.08 0.19 0.19 0.29 0.39 0.20 0.18 0.26 0.13 1.00 0.34 0.26 0.29 0.04 0.20 0.18 0.20 0.19 0.26 0.12 0.22 0.29 

0.07 0.32 0.18 0.22 0.27 0.33 0.18 0.21 0.20 0.12 0.14 0.10 0.21 0.17 0.17 0.13 0.10 0.24 0.14 0.19 0.10 0.03 0.34 0.22 0.24 0.18 0.24 0.20 0.19 0.15 0.23 0.08 0.13 0.26 0.15 0.16 0.16 0.06 0.11 0.14 0.19 0.11 0.18 0.14 0.09 0.23 0.13 0.18 0.10 0.17 0.09 0.19 0.20 0.07 0.20 0.14 0.24 0.25 0.09 0.13 0.21 0.14 0.08 0.17 0.14 0.15 0.15 0.03 0.12 0.12 0.12 0.17 0.19 0.10 0.17 0.16 0.19 0.17 0.22 0.15 0.24 0.19 0.29 0.21 0.14 0.33 0.32 0.14 0.18 0.24 0.21 0.40 0.21 0.26 0.23 0.43 0.21 0.34 1.00 0.14 0.28 0.13 0.22 0.22 0.28 0.22 0.16 0.19 0.12 0.17 

0.06 0.07 0.13 0.19 0.01 0.27 0.14 0.20 0.20 -0.06 0.14 0.10 0.13 0.17 0.32 0.17 0.23 0.06 0.04 0.15 0.27 0.01 0.41 0.34 0.18 0.15 0.16 0.17 0.14 0.34 0.16 0.07 0.16 0.31 0.22 0.13 0.21 0.08 -0.04 0.09 0.15 0.12 0.24 0.12 0.06 0.22 -0.02 -0.07 0.07 0.00 0.09 0.01 0.17 0.03 0.08 0.17 -0.11 0.18 0.14 0.15 0.27 0.11 0.24 0.18 0.14 0.15 0.11 0.26 0.26 0.10 0.09 0.15 0.11 0.09 0.10 -0.33 0.23 0.12 0.13 0.24 -0.11 -0.03 0.29 0.23 0.17 0.44 0.14 0.15 0.22 0.26 0.17 0.10 0.44 0.26 0.26 0.08 -0.04 0.26 0.14 1.00 0.29 0.19 -0.01 0.04 0.16 0.21 0.20 0.08 0.15 0.21 

0.14 0.25 0.27 0.34 0.36 0.43 0.25 0.45 0.42 0.24 0.27 0.18 0.23 0.21 0.24 0.23 0.19 0.34 0.27 0.30 0.23 0.07 0.50 0.37 0.38 0.30 0.31 0.30 0.20 0.24 0.31 0.23 0.33 0.27 0.39 0.34 0.41 0.12 0.26 0.27 0.19 0.28 0.49 0.30 0.15 0.46 0.26 0.30 0.08 0.39 0.08 0.44 0.35 0.07 0.20 0.43 0.21 0.37 0.29 0.30 0.44 0.31 0.23 0.38 0.37 0.31 0.30 0.17 0.26 0.21 0.13 0.26 0.31 0.22 0.25 -0.02 0.37 0.27 0.21 0.51 0.23 0.23 0.46 0.33 0.51 0.47 0.41 0.27 0.21 0.42 0.33 0.27 0.37 0.49 0.36 0.32 0.23 0.29 0.28 0.29 1.00 0.06 0.35 0.38 0.40 0.35 0.35 0.20 0.28 0.30 

0.00 0.04 0.22 0.21 -0.02 0.37 0.24 0.07 0.09 -0.09 0.19 0.23 0.19 0.24 0.19 0.19 0.04 -0.06 -0.08 -0.00 0.12 0.10 0.05 0.23 0.19 0.08 0.12 0.08 0.39 0.34 0.15 0.11 -0.09 0.07 -0.12 0.14 -0.10 0.20 -0.14 0.10 0.29 0.06 -0.05 0.13 0.07 0.06 -0.13 -0.06 0.10 -0.05 -0.01 -0.09 -0.02 0.24 0.22 -0.07 0.00 0.19 -0.08 -0.02 0.12 0.07 0.06 -0.09 0.05 0.04 0.12 0.08 0.13 0.08 0.26 0.05 0.02 0.09 0.01 -0.03 0.13 0.05 0.15 0.02 -0.03 -0.01 0.18 0.08 -0.04 0.18 0.01 0.06 0.14 0.20 0.11 0.06 0.18 0.17 0.15 -0.03 -0.07 0.04 0.13 0.19 0.06 1.00 -0.09 -0.10 0.05 0.16 0.12 0.24 0.20 0.03 

0.18 0.34 0.21 0.23 0.38 0.23 0.13 0.34 0.31 0.24 0.13 0.12 0.17 0.13 0.14 0.07 0.07 0.37 0.28 0.18 0.07 0.07 0.34 0.06 0.13 0.25 0.19 0.22 0.05 0.06 0.17 0.16 0.20 0.16 0.23 0.28 0.26 0.03 0.26 0.24 0.11 0.17 0.27 0.21 0.14 0.34 0.28 0.23 0.08 0.28 0.10 0.35 0.27 0.02 0.04 0.24 0.27 0.19 0.19 0.18 0.19 0.25 0.16 0.28 0.29 0.26 0.24 0.08 0.18 0.17 0.09 0.21 0.28 0.15 0.21 0.18 0.18 0.21 0.11 0.21 0.30 0.24 0.26 0.15 0.28 0.25 0.39 0.19 0.17 0.20 0.18 0.26 0.16 0.28 0.21 0.33 0.27 0.20 0.22 -0.01 0.35 -0.09 1.00 0.41 0.38 0.22 0.23 0.16 0.12 0.26 

0.14 0.34 0.18 0.19 0.40 0.29 0.17 0.31 0.29 0.30 0.14 0.11 0.15 0.11 0.10 0.14 0.14 0.37 0.34 0.21 0.14 0.06 0.35 0.15 0.23 0.31 0.22 0.24 0.11 0.13 0.19 0.13 0.24 0.17 0.27 0.23 0.30 0.07 0.30 0.28 0.07 0.12 0.30 0.22 0.17 0.36 0.34 0.29 0.09 0.36 0.05 0.39 0.31 0.02 0.03 0.28 0.33 0.20 0.22 0.21 0.28 0.21 0.13 0.32 0.26 0.23 0.23 0.04 0.15 0.17 0.07 0.23 0.22 0.18 0.18 0.16 0.22 0.19 0.16 0.21 0.30 0.23 0.29 0.14 0.28 0.29 0.48 0.18 0.24 0.22 0.20 0.28 0.20 0.30 0.27 0.35 0.30 0.18 0.22 0.04 0.38 -0.10 0.41 1.00 0.45 0.26 0.24 0.18 0.09 0.30 

0.15 0.34 0.24 0.26 0.40 0.43 0.19 0.38 0.38 0.23 0.14 0.11 0.16 0.15 0.18 0.11 0.25 0.30 0.31 0.16 0.17 0.09 0.39 0.29 0.33 0.34 0.31 0.29 0.23 0.25 0.35 0.22 0.20 0.22 0.18 0.30 0.21 0.16 0.20 0.33 0.17 0.12 0.23 0.23 0.21 0.33 0.23 0.15 0.11 0.16 0.04 0.51 0.24 0.07 0.10 0.22 0.30 0.38 0.17 0.17 0.37 0.25 0.17 0.24 0.28 0.27 0.24 0.03 0.20 0.20 0.05 0.37 0.22 0.18 0.27 0.07 0.36 0.26 0.29 0.20 0.24 0.24 0.42 0.22 0.23 0.42 0.39 0.22 0.32 0.41 0.30 0.26 0.37 0.43 0.46 0.34 0.25 0.20 0.28 0.16 0.40 0.05 0.38 0.45 1.00 0.43 0.24 0.15 0.15 0.25 

0.13 0.22 0.18 0.19 0.20 0.36 0.21 0.38 0.28 0.12 0.15 0.13 0.08 0.11 0.16 0.23 0.18 0.23 0.36 0.07 0.21 0.10 0.31 0.31 0.32 0.28 0.27 0.40 0.04 0.23 0.37 0.21 0.11 0.22 0.13 0.31 0.16 0.23 0.09 0.23 0.13 0.12 0.17 0.22 0.23 0.27 0.15 0.07 0.15 0.17 0.02 0.26 0.15 0.07 0.15 0.34 0.15 0.51 0.09 0.00 0.37 0.13 0.19 0.14 0.26 0.25 0.21 0.14 0.21 0.19 0.08 0.31 0.22 0.18 0.22 -0.02 0.29 0.24 0.21 0.19 0.13 0.15 0.36 0.23 0.14 0.36 0.26 0.23 0.26 0.38 0.26 0.19 0.36 0.42 0.38 0.22 0.15 0.19 0.22 0.21 0.35 0.16 0.22 0.26 0.43 1.00 0.25 0.09 0.24 0.21 

0.15 0.14 0.33 0.17 0.19 0.44 0.44 0.41 0.32 0.12 0.29 0.17 0.30 0.24 0.16 0.20 0.19 0.22 0.21 0.06 0.27 0.37 0.30 0.22 0.23 0.33 0.34 0.42 0.15 0.21 0.24 0.16 0.21 0.20 0.25 0.39 0.30 0.22 0.09 0.45 0.24 0.17 0.28 0.45 0.34 0.51 0.23 0.10 0.05 0.36 0.11 0.21 0.22 0.14 0.19 0.22 0.07 0.25 0.12 0.15 0.26 0.26 0.27 0.20 0.26 0.32 0.48 0.14 0.27 0.34 0.19 0.18 0.29 0.36 0.23 -0.04 0.19 0.26 0.25 0.25 0.13 0.09 0.25 0.16 0.26 0.27 0.20 0.23 0.14 0.24 0.18 0.25 0.28 0.23 0.32 0.17 0.18 0.26 0.16 0.20 0.35 0.12 0.23 0.24 0.24 0.25 1.00 0.28 0.52 0.30 

-0.14 0.03 0.35 0.31 0.13 0.43 0.38 0.18 0.20 0.03 0.28 0.22 0.35 0.35 0.26 0.02 0.11 0.12 -0.03 0.17 0.13 0.19 0.12 0.19 0.20 0.17 0.17 0.19 0.37 0.23 0.19 0.13 0.06 0.10 0.09 0.26 0.08 0.13 0.00 0.32 0.02 -0.18 0.12 0.26 0.16 0.28 0.01 0.19 0.19 0.18 0.16 0.07 0.15 0.20 0.30 0.05 0.08 0.15 0.06 0.11 0.16 0.19 0.09 0.02 0.15 0.15 0.27 -0.03 0.14 0.16 0.29 -0.01 0.20 0.19 0.14 0.02 0.11 0.15 0.27 0.05 0.11 0.04 0.24 0.03 0.15 0.18 0.18 -0.07 0.12 0.14 0.13 0.24 0.17 0.19 0.14 0.05 0.13 0.12 0.19 0.08 0.20 0.24 0.16 0.18 0.15 0.09 0.28 1.00 0.27 0.08 

0.16 0.04 0.33 0.12 0.11 0.42 0.44 0.34 0.28 0.07 0.30 0.17 0.26 0.22 0.09 0.24 0.16 0.13 0.15 0.03 0.22 0.41 0.21 0.23 0.23 0.28 0.33 0.39 0.16 0.23 0.22 0.20 0.15 0.14 0.17 0.40 0.14 0.23 0.03 0.44 0.23 0.15 0.18 0.42 0.33 0.42 0.09 0.01 0.07 0.28 0.09 0.12 0.13 0.20 0.22 0.17 -0.02 0.25 0.06 0.11 0.22 0.24 0.20 0.05 0.21 0.23 0.45 0.19 0.24 0.34 0.18 0.09 0.26 0.33 0.20 -0.01 0.17 0.22 0.24 0.17 0.08 0.01 0.21 0.11 0.22 0.18 0.06 0.19 0.12 0.19 0.17 0.19 0.26 0.19 0.27 -0.02 0.11 0.22 0.12 0.15 0.28 0.20 0.12 0.09 0.15 0.24 0.52 0.27 1.00 0.22 

0.51 0.48 0.30 0.28 0.17 0.37 0.30 0.32 0.31 0.39 0.18 0.13 0.16 0.30 0.28 0.26 0.31 0.21 0.44 0.23 0.37 0.15 0.32 0.13 0.14 0.27 0.22 0.23 0.19 0.32 0.16 0.09 0.29 0.22 0.32 0.33 0.31 0.10 0.30 0.31 0.34 0.26 0.35 0.28 0.17 0.46 0.45 0.18 0.05 0.46 0.19 0.39 0.35 0.10 0.06 0.38 0.18 0.14 0.31 0.30 0.24 0.31 0.28 0.45 0.24 0.25 0.28 0.32 0.33 0.25 0.13 0.24 0.23 0.24 0.24 0.16 0.23 0.23 0.22 0.35 0.17 0.20 0.16 0.22 0.16 0.33 0.24 0.29 0.34 0.27 0.18 0.32 0.33 0.22 0.22 0.22 0.32 0.29 0.17 0.21 0.30 0.03 0.26 0.30 0.25 0.21 0.30 0.08 0.22 1.00 

Factor Library Correlation Heatmap (110 factors) 

0.2 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

Figure 2: Pairwise Spearman correlation heatmap of the re-leased A-share factor library (110 admitted factors), com-puted from cross-sectionally standardized realized factor signals over the common time‚Äìasset panel. The average off-diagonal absolute correlation is Avg |ùúå | = 0.203. 

4.2.1 Factor Quality and Diversity. Table 1 reports 2025 out-of-sample results under a strict protocol: Top-40 factors are selected once on CSI500 (2024) and then frozen for evaluation on multiple datasets. Under this protocol, FactorMiner performs best among the compared methods across all four markets. In the Factor Library setting, it achieves IC/ICIR of 8.25% /0.77 on CSI500, with similar competitive improvements on the other markets (see Table 1). In terms of redundancy, FactorMiner‚Äôs selected factor set ex-hibits moderate pairwise dependence. As summarized by Avg |ùúå |

in Table 1, the average pairwise absolute correlation is 0.30‚Äì0.31 on A-shares and 0.25 on Crypto. The correlation distribution fur-ther shows a controlled tail ( |ùúå | ‚âà 0.44‚Äì0.45 on A-shares and 0.42 on Crypto), suggesting that the performance gains are not driven by a few near-duplicate signals. We further visualize the correla-tion structure of the full admitted factor library in Figure 2. The complete factor formulas and additional downstream analyses are provided in Appendix Sections I to K and P. 

4.2.2 Robustness Across Heterogeneous Markets. The cross-market evaluation reveals the generalization capability of the discovered factors. While the Cryptocurrency market represents a fundamen-tally different microstructure (24/7 trading, no price limits) com-pared to A-shares, FactorMiner maintains competitive performance on Crypto, with IC/ICIR of 3.82%/0.28 in the single-factor library evaluation and 9.48%/0.62 under a simple IC-weighted combination (see Table 1). This robustness suggests that FactorMiner captures fundamental price-volume dynamics (e.g., liquidity-constrained reversals, volatil-ity clustering) that are invariant across asset classes, rather than overfitting to the idiosyncrasies of a single market. FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery Preprint, , 

Table 1: Out-of-Sample performance comparison with a stricter protocol. Top-40 factors are selected on CSI500 (2024) and evaluated on 2025 across datasets. For Alpha101, Classic is restricted to the same candidate set as Adapted. All reported IC and ICIR use the paper‚Äôs absolute-IC summary: |E[ùêºùê∂ ùë° ]| and |E[ùêºùê∂ ùë° ]|/ std (ùêºùê∂ ùë° ). Factor Combination uses the frozen Top-40 with EW and ICW (weights/signs determined on 2024). Factor Selection trains on 2024 and tests on 2025 using Lasso and XGBoost.        

> Dataset Method Factor Library (Top-40) Factor Combination (Top-40) Factor Selection (Train‚Äô24/Test‚Äô25)
> IC (%) ICIR Avg

|ùúå | EW IC EW ICIR ICW IC ICW ICIR Las. IC Las. ICIR XGB. IC XGB. ICIR                                                                                                                                                                                                                                                                                                                      

> CSI500
> RF ‚Ä°2.68 0.25 0.13 6.98 0.41 12.02 0.90 13.81 1.15 8.99 0.90 Alpha101 (Classic) 4.49 0.42 0.19 10.85 0.88 12.89 0.99 14.09 1.27 12.07 1.21 Alpha101 (Adapted) 5.06 0.43 0.21 11.53 0.86 14.71 1.13 13.70 1.08 13.76 1.20 GPLearn 6.04 0.43 0.44 10.30 0.62 13.38 1.00 12.44 1.17 9.86 0.95 AlphaForge 4.48 0.38 0.36 7.12 0.60 11.13 0.93 10.49 0.87 11.30 1.25 AlphaAgent 5.90 0.46 0.32 10.99 0.92 11.86 0.99 13.87 1.19 11.93 1.24
> FactorMiner (Ours) 8.25 0.77 0.31 14.95 1.29 15.11 1.31 14.59 1.21 14.03 1.29 CSI1000
> RF ‚Ä°2.88 0.30 0.13 7.48 0.49 12.28 1.00 13.72 1.24 9.54 1.03 Alpha101 (Classic) 4.86 0.50 0.19 11.37 1.02 13.14 1.08 14.64 1.42 11.11 1.17 Alpha101 (Adapted) 5.32 0.49 0.21 11.95 0.98 14.78 1.21 13.08 1.09 13.88 1.26 GPLearn 5.86 0.48 0.44 11.10 0.73 13.66 1.11 12.87 1.23 9.53 0.96 AlphaForge 4.64 0.42 0.35 7.60 0.71 11.25 1.02 10.42 0.93 12.20 1.34
> AlphaAgent 6.21 0.51 0.32 11.17 1.04 12.00 1.12 13.73 1.27 11.42 1.22
> FactorMiner (Ours) 7.78 0.76 0.30 14.62 1.37 14.76 1.39 14.25 1.25 12.42 1.30
> HS300
> RF ‚Ä°1.94 0.15 0.13 5.46 0.30 9.49 0.61 9.98 0.63 5.55 0.46 Alpha101 (Classic) 3.44 0.26 0.18 8.55 0.57 10.31 0.65 11.84 0.85 9.70 0.74 Alpha101 (Adapted) 4.00 0.28 0.20 9.31 0.60 12.03 0.76 12.04 0.77 11.81 0.90 GPLearn 4.12 0.16 0.45 8.01 0.44 10.64 0.66 9.59 0.58 7.92 0.61 AlphaForge 3.53 0.25 0.36 5.19 0.35 8.54 0.57 6.79 0.43 10.89 0.91 AlphaAgent 4.69 0.30 0.33 8.83 0.59 9.65 0.65 12.26 0.86 9.66 0.84
> FactorMiner (Ours) 7.46 0.38 0.31 12.49 0.88 12.66 0.88 11.23 0.82 11.33 0.94
> Crypto
> RF ‚Ä°1.45 0.09 0.07 3.31 0.19 7.91 0.48 8.04 0.51 3.50 0.24 Alpha101 (Classic) 2.11 0.14 0.14 5.91 0.41 8.05 0.53 9.63 0.54 5.61 0.37 Alpha101 (Adapted) 2.40 0.15 0.15 6.09 0.41 9.21 0.62 8.45 0.48 4.51 0.30 GPLearn 2.50 0.15 0.38 4.44 0.25 7.62 0.42 9.07 0.48 4.04 0.27 AlphaForge 2.52 0.16 0.31 4.63 0.29 7.44 0.44 8.39 0.42 5.00 0.33 AlphaAgent 2.86 0.17 0.27 7.94 0.34 8.40 0.36 7.48 0.41 2.35 0.17
> FactorMiner (Ours) 3.82 0.28 0.25 9.48 0.61 9.48 0.62 8.98 0.51 6.82 0.47
> Note: IC is computed as Spearman rank correlation per bar and summarized as

|E[ùêºùê∂ ùë° ] | . Las.=Lasso, XGB.=XGBoost. ‚Ä°RF=Random Exploration. 

4.2.3 Ensembles vs. Learned Selection. Beyond individual factors, we evaluate the utility of the mined libraries for downstream portfo-lio construction via two families of methods: simple factor combina-tions (equal-weight and IC-weighted) and learned selection models (Lasso and XGBoost trained on 2024 and evaluated on 2025). Across most baselines, learned models provide a clear uplift over simple ensembles, consistent with the presence of heterogeneous yet par-tially redundant signals in the candidate libraries. For FactorMiner, however, learned selection offers limited additional gains and can be slightly negative on some datasets (e.g., CSI500: EW and ICW ICIR = 1.52/1.54 vs. Lasso and XGBoost = 1.42/1.52), indicating that simple ensemble signals already capture most of the exploitable predictive power under the Train‚Äô24/Test‚Äô25 protocol (see Table 1). 

4.3 Effect of Experience Memory 

To isolate the contribution of the Experience Memory ( M), we conducted an ablation study comparing FactorMiner against a "No Memory" variant where operators ùêπ, ùê∏, ùëÖ were disabled, reducing the system to standard LLM-based evolution (see Figure 3 for a summary). Since this experiment is designed as a controlled mech-anism ablation, we adopt relatively relaxed screening thresholds to ensure sufficient samples for comparison (IC threshold |IC | > 0.02 ;redundancy threshold ùúÉ = 0.85 for this ablation setting).Figure 3 highlights the impact of memory guidance: 

‚Ä¢ Precise Navigation (High Yield) : The Have Memory variant generates 96 high-quality candidates (60.0% yield), whereas the No Memory baseline produces only 32 (20.0% yield). This demon-strates that the Retrieval operator effectively maps the search space, allowing the agent to guiding exploration toward regions with higher expected yield rather than exploring randomly. 

‚Ä¢ Aggressive Filtration (High Diversity) : Despite generating 

3.0√ó more valid signals, the Have Memory variant actively rejects a higher proportion of them for redundancy (55.2% vs 43.8%). This confirms that the Evolution operator ( ùê∏ ) functions as a strategic filter, prioritizing unique signal discovery over mere quantity. 

4.4 Mining Efficiency Analysis 

To demonstrate the practical advantage of the factor mining skill, we conduct rigorous benchmarks comparing three execution backends supported by our framework: (1) standard Python (Pandas) , (2) C-compiled (Bottleneck) for efficient CPU execution, and (3) GPU-accelerated for massive parallelism. This multi-backend support allows the agent to remain lightweight while achieving industrial-grade efficiency through on-demand acceleration. 

Results. Figure 4 presents both operator-level and factor-level benchmarks on the real CSI500 dataset (12,610 √ó 500), measuring 

pure computation time excluding I/O. At the operator level, the GPU backend achieves 8‚Äì59 √ó speedups over Pandas and 2‚Äì13 √ó over the optimized C backend. The most dramatic improvement is on 

TsRank : 1,843ms (Pandas) ‚Üí 393ms (C) ‚Üí 31ms (GPU). Preprint, , Wang et al. High-quality Rejected Admitted     

> 0
> 20
> 40
> 60
> 80
> 100
> Count
> 32
> 14 18
> 96
> 53
> 43
> Counts
> Yield rate Rejection rate
> 0
> 20
> 40
> 60
> 80
> Rate (%)
> 20.0%
> 43.8%
> 60.0% 55.2%
> Rates
> No Memory Have Memory

Figure 3: Ablation comparison between Have Memory and No Memory. High-quality candidates are defined as those passing the IC threshold ( |IC | > 0.02 ). The bar chart reports the counts (high-quality / rejected / admitted) and the corre-sponding yield and rejection rates. 

For end-to-end factor evaluation, rank-intensive factors (F43, F48, F53) show substantial 23‚Äì27 √ó speedups. Notably, even the highly-optimized C implementation is 5.4 √ó slower than the GPU backend on average (1,092ms vs. 202ms). This hybrid acceleration strategy demonstrates that our factor mining skill is both flexible and high-performance: by offloading computationally intensive primitives to GPU/C while leveraging multi-process parallelism for batch evaluation, FactorMiner makes large-scale iterative discovery feasible on commodity hardware. This efficiency is a qualitative enabler: evaluating 1,000 candi-date factors takes ‚àº6 minutes with FactorMiner vs. ‚àº70 minutes with Pandas, making the iterative "generate-evaluate-refine" loop computationally feasible. 

5 Discussion 

High-frequency markets can be treated as a data-rich complex adap-tive system, and each formulaic factor can be viewed as an explicit, falsifiable hypothesis about market microstructure. Beyond pre-dictive performance, FactorMiner yields a curated library of 110 interpretable high-frequency alpha factors together with a standard-ized evaluation protocol, enabling reproducible hypothesis testing, mechanistic inspection, and cross-market transfer studies. 

Experience memory as continual learning. Our results confirm that memory-guided, skill-based agents can efficiently scale neural-symbolic program synthesis while retaining interpretability. The experience memory serves as a form of institutional knowledge that accumulates across mining sessions, enabling meta-learning: the system learns not just individual factors, but how to search more effectively. Key insights extracted from our memory (Appendix F) in-clude: successful patterns , such as higher-order moment regimes via Skew/Kurt, trend-regression adaptivity via Rsquare/Slope/Resi, TsRank       

> CsRank
> TsCorr
> TsStd
> 10 2
> 10 3
> Time (ms, log scale)
> 1,843
> 445
> 914
> 238
> 393
> 49
> 712
> 133
> 31 26
> 64
> 30
> Operator-level
> F43
> F48
> F53
> Avg
> 10 2
> 10 3
> 10 4
> Time (ms, log scale)
> 2,997 3,143
> 8,834
> 4,991
> 1,457 1,268
> 552
> 1,092
> 115 131
> 361
> 202
> Factor-level
> Python CGPU

Figure 4: Grouped bar chart of computation time on a log scale for operator-level and factor-level benchmarks. Lower is better; GPU shows consistent order-of-magnitude gains. 

and amount-efficiency interactions, that consistently yield high-IC yet low-correlation factors; failure patterns , such as VWAP-deviation variants, standardized returns, and simple Delta reversals, that tend to be highly correlated with existing factors and should be avoided; and the Correlation Red Sea phenomenon, where discovering new orthogonal factors becomes increasingly difficult as the library grows without memory guidance. 

6 Conclusion 

FactorMiner provides a lightweight self-evolving agent framework for interpretable high-frequency alpha discovery by combining a modular mining skill with experience memory. The resulting library of 110 formulaic factors and a standardized evaluation protocol form a reproducible discovery artifact for hypothesis-driven analysis of market microstructure. Future work will incorporate transaction-cost-aware backtesting, extend to broader assets and frequencies, and develop online memory updates for non-stationary markets. 

7 Limitations and Ethical Considerations 

We focus on internal evaluation, library construction, and analy-sis; we do not yet provide controlled comparisons to end-to-end forecasting models. Although this work uses aggregated market data to improve the productivity and reproducibility of formulaic alpha discovery, the discovered factors could be misused in spec-ulative or manipulative strategies; deployment should therefore follow compliance requirements and incorporate appropriate risk controls. FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery Preprint, , 

References 

[1] Franklin Allen and Risto Karjalainen. 1999. Using Genetic Algorithms to Find Technical Trading Rules. Journal of Financial Economics 51, 2 (1999), 245‚Äì271. doi:10.1016/S0304-405X(98)00052-X [2] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis. 2016. Model-Free Episodic Control. arXiv preprint arXiv:1606.04460 (2016). doi:10.48550/arXiv.1606. 04460 [3] Eugene F. Fama and Kenneth R. French. 1993. Common Risk Factors in the Returns on Stocks and Bonds. Journal of Financial Economics 33, 1 (1993), 3‚Äì56. doi:10.1016/0304-405X(93)90023-5 [4] Guanhao Feng, Stefano Giglio, and Dacheng Xiu. 2020. Taming the Factor Zoo: A Test of New Factors. The Journal of Finance 75, 3 (2020), 1327‚Äì1370. doi:10.1111/jofi.12883 [5] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In International Conference on Machine Learning (ICML) . PMLR, 1126‚Äì1135. https://proceedings.mlr.press/v70/ finn17a.html [6] Shihao Gu, Bryan Kelly, and Dacheng Xiu. 2020. Empirical Asset Pricing via Machine Learning. The Review of Financial Studies 33, 5 (2020), 2223‚Äì2273. doi:10.1093/rfs/hhaa009 [7] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. 2018. Meta-Reinforcement Learning of Structured Exploration Strategies. 

arXiv preprint arXiv:1802.07245 (2018). doi:10.48550/arXiv.1802.07245 [8] Zura Kakushadze. 2016. 101 Formulaic Alphas. Wilmott 2016, 84 (2016), 72‚Äì81. doi:10.1002/wilm.10525 arXiv:1601.00991. [9] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, et al . 2022. MRKL Systems: A Modular, Neuro-Symbolic Architecture that Combines Large Language Models, External Knowledge Sources and Discrete Reasoning. arXiv preprint arXiv:2205.00445 (2022). doi:10.48550/arXiv.2205.00445 [10] John R. Koza. 1992. Genetic Programming: On the Programming of Computers by Means of Natural Selection . MIT Press, Cambridge, MA. [11] Serhiy Kozak, Stefan Nagel, and Shrihari Santosh. 2020. Shrinking the Cross-Section. Journal of Financial Economics 135, 2 (2020), 271‚Äì292. doi:10.1016/j. jfineco.2019.06.008 [12] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs. arXiv preprint arXiv:2304.08244 (2023). doi:10.48550/arXiv.2304.08244 EMNLP 2023. [13] Alberto Moraglio, Krzysztof Krawiec, and Colin G. Johnson. 2012. Geometric Semantic Genetic Programming. In Parallel Problem Solving from Nature ‚Äì PPSN XII . Lecture Notes in Computer Science, Vol. 7491. Springer, 21‚Äì31. doi:10.1007/ 978-3-642-32937-1_3 [14] Christopher J. Neely, Paul A. Weller, and Rob Dittmar. 1997. Is Technical Analysis in the Foreign Exchange Market Profitable? A Genetic Programming Approach. 

Journal of Financial and Quantitative Analysis 32, 4 (1997). doi:10.2307/2331231 JSTOR:2331231. [15] Alex Nichol, Joshua Achiam, and John Schulman. 2018. On First-Order Meta-Learning Algorithms. arXiv preprint arXiv:1803.02999 (2018). doi:10.48550/arXiv. 1803.02999 [16] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. MemGPT: Towards LLMs as Operating Systems. arXiv preprint arXiv:2310.08560 (2024). doi:10.48550/arXiv.2310.08560 [17] German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. 2019. Continual Lifelong Learning with Neural Networks: A Review. 

Neural Networks 113 (2019), 54‚Äì71. doi:10.1016/j.neunet.2019.01.012 [18] Joon Sung Park, Joseph C. O‚ÄôBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of Human Behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST ‚Äô23) . 1‚Äì22. doi:10.1145/3586183.3606763 [19] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Go-rilla: Large Language Model Connected with Massive APIs. arXiv preprint arXiv:2305.15334 (2023). doi:10.48550/arXiv.2305.15334 [20] Riccardo Poli, William B. Langdon, and Nicholas F. McPhee. 2008. A Field Guide to Genetic Programming . Lulu.com. http://www.gp-field-guide.org.uk/ [21] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al . 2023. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. arXiv preprint arXiv:2307.16789 (2023). doi:10.48550/arXiv.2307.16789 [22] Cynthia Rudin. 2019. Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead. Nature Machine Intelligence 1, 5 (2019), 206‚Äì215. doi:10.1038/s42256-019-0048-x [23] Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Lan-guage Models Can Teach Themselves to Use Tools. arXiv preprint arXiv:2302.04761 

(2023). doi:10.48550/arXiv.2302.04761 [24] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. arXiv preprint arXiv:2303.17580 (2023). doi:10.48550/arXiv.2303. 17580 [25] Hao Shi, Weili Song, Xinting Zhang, Jiahe Shi, Cuicui Luo, Xiang Ao, Hamid Arian, and Luis Seco. 2025. AlphaForge: A Framework to Mine and Dynami-cally Combine Formulaic Alpha Factors. Proceedings of the AAAI Conference on Artificial Intelligence 39, 12 (2025), 12524‚Äì12532. doi:10.1609/aaai.v39i12.33365 [26] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv preprint arXiv:2303.11366 (2023). doi:10.48550/ arXiv.2303.11366 [27] Trevor Stephens and contributors. [n. d.]. gplearn: Genetic Programming in Python. GitHub repository. https://github.com/trevorstephens/gplearn Accessed 2026-02-08. [28] Ziyi Tang, Zechuan Chen, Jiarui Yang, Jiayao Mai, Yongsen Zheng, Keze Wang, Jinrui Chen, and Liang Lin. 2025. AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining . ACM, 2813‚Äì 2822. doi:10.1145/3711896.3736838 [29] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied Agent with Large Language Models. arXiv preprint arXiv:2305.16291 (2023). doi:10.48550/arXiv.2305.16291 [30] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv preprint arXiv:2210.03629 (2022). doi:10.48550/arXiv.2210.03629 ICLR 2023. [31] Shuo Yu, Hongyan Xue, Xiang Ao, Feiyang Pan, Jia He, Dandan Tu, and Qing He. 2023. Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . ACM, 5476‚Äì5486. doi:10.1145/3580305.3599831 [32] Junjie Zhao, Chengxi Zhang, Min Qin, and Peng Yang. 2025. QuantFactor RE-INFORCE: Mining Steady Formulaic Alpha Factors with Variance-Bounded REINFORCE. IEEE Transactions on Signal Processing 73 (2025), 2448‚Äì2463. doi:10.1109/TSP.2025.3576781 Preprint, , Wang et al. 

A Factor Expression and Operator Library 

Each symbolic factor ùõº ‚àà P is represented as a symbolic expression tree ùëá (ùõº ) constructed from an operator library Œ©. Following Kakushadze [8], we define factors using a domain-specific language where: 

‚Ä¢ Leaf nodes are raw feature references: $open , $high , $low , $close , $volume , $amt , $vwap , $returns 

‚Ä¢ Internal nodes are operators from Œ© with specified parameters We curate a library of 60+ operators organized into categories (Table 2): (1) Arithmetic : Add, Sub, Mul, Div, Neg, Log, SignedPower; (2) 

Statistical : Mean, Std, Skew, Kurt over rolling windows; (3) Time-series : Delta, TsRank, TsMax, TsMin, Delay; (4) Cross-sectional : CsRank (percentile rank across assets at each time); (5) Smoothing : SMA, EMA, WMA (moving averages); (6) Trend regression : Slope, Rsquare, Resi (linear regression statistics); (7) Logical : IfElse, Greater, And, Or (conditional branching). 

Table 2: Operator categories in the factor mining skill (representative operators).                  

> Category Operators Description Arithmetic Add, Sub, Mul, Div, Neg, Abs, Log, SignedPower, Power, Inv, Sqrt, Square, Exp, Tanh Element-wise transforma-tions Statistical Mean, Std, Var, Skew, Kurt, Med, Sum, Product Rolling window statistics Time-series Delay, Delta, TsRank, TsMax, TsMin, TsArgMax, TsArgMin, TsDecay Temporal pattern capture Cross-sectional CsRank, Scale Cross-asset transforms Smoothing SMA, EMA, WMA Trend extraction Regression Slope, Rsquare, Resi Trend strength and residuals Logical IfElse, Greater, Less, GreaterEqual, LessEqual, And, Or, Eq, Ne Conditional regime switch-ing

Note. Table 2 lists representative operators for readability; the full operator registry in the FactorMiner skill contains 60+ typed operators. 

B Admission Criteria and Factor Replacement 

A symbolic factor ùõº is admitted to the library L if it satisfies: IC (ùõº ) ‚â• ùúè IC ‚àß max  

> ùëî ‚àà L

|ùúå (ùõº, ùëî )| < ùúÉ (10) where ùúè IC and ùúÉ are the IC and correlation thresholds (default: ùúè IC = 0.04 and ùúÉ = 0.5 for A-share mining unless otherwise specified). Additionally, we introduce a factor replacement mechanism for high-quality factors that would otherwise be rejected due to correlation. If a new factor ùõº satisfies: IC (ùõº ) ‚â• 0.10 ‚àß IC (ùõº ) ‚â• 1.3 √ó IC (ùëî ) ‚àß |{ ùëî ‚àà L : ùúå (ùõº, ùëî ) > ùúÉ }| = 1 (11) then ùõº replaces the single correlated factor ùëî in L. This mechanism allows the library to evolve toward higher quality while maintaining the correlation constraint. 

C Implementation Details 

For each method, we first construct a fixed Top-40 factor set by ranking candidates on CSI500 using the training year 2024 : we filter admitted factors by thresholding |IC | and |ICIR | (stock thresholds: |IC | ‚â• 0.05 , |ICIR | ‚â• 0.5), select up to 40 from the admitted set by descending 

|IC |, and if fewer than 40 are admitted, fill the remaining slots with the next-best valid candidates by |IC |. We then freeze this Top-40 set (selected once on CSI500) and evaluate it on CSI500/CSI1000/HS300/Crypto for the full-year 2025 out-of-sample comparison in Table 1, to test generalization across time (2024 ‚Üí 2025), universes, and markets. Alpha101 factors were originally designed for daily frequency data, we adapt them for 10-minute frequency by optimizing window parameters. For each of the Alpha101 factors, we generate up to 10 parameter variants and select the best-performing configuration. FactorMiner is implemented in Python, utilizing NumPy and CuPy for GPU-accelerated operator computation. This achieves up to 26 √ó

speedup; see Section D. A 40-worker multiprocessing pool handles parallel candidate evaluation. The experience memory is maintained as a structured, human-readable knowledge base. We employ Gemini 3.0 Flash as the LLM backbone for symbolic program synthesis. The mining process follows an iterative curriculum using the default admission thresholds in Section B, adjusted when needed. 

D Computational Efficiency 

To enable rapid iteration, FactorMiner leverages three acceleration techniques: 

GPU-accelerated operators. Core operators (CsRank, TsRank, rolling statistics) are implemented using PyTorch, achieving 6‚Äì26 √ó

speedup over CPU implementations (Table 3). 

Multi-process parallelization. Factor evaluation is inherently parallel across candidates. We use a worker pool to evaluate batches concurrently, with each worker handling one candidate factor. FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery Preprint, , 

C-compiled numerical operations. Low-level numerical operations use optimized C implementations (e.g., bottleneck library) for efficient rolling window computations. 

Table 3: GPU acceleration for core operators (single A100 GPU). 

Operator CPU (ms) GPU (ms) Speedup CsRank 93 3.6 26 √ó

TsRank 97 6.0 17 √ó

Rolling Corr 76 11 6.8 √ó

Rolling Std 13 3.4 3.7 √ó

TsDecay 45 5.0 9√ó

E Factor Combination and Selection 

After library construction, we provide three combination strategies and three selection methods: 

Combination strategies. 

‚Ä¢ Equal-weight : Simple average of all factor values. 

‚Ä¢ IC-weighted : Weight factors by their historical IC. 

‚Ä¢ Orthogonal : Gram-Schmidt orthogonalization before averaging. 

Selection methods. 

‚Ä¢ Lasso : L1-regularized linear regression to identify sparse factor subsets. 

‚Ä¢ Forward stepwise : Greedy selection maximizing combined ICIR. 

‚Ä¢ XGBoost : Gradient boosting to capture nonlinear factor interactions. 

F Experience Memory: Recommended Directions 

The following table summarizes successful mining patterns extracted from our experience memory. These patterns have consistently yielded factors that pass the default admission criteria (Section B). 

Table 4: Recommended mining directions from experience memory. 

Pattern Description Success Rate Higher Moment Regimes Use Skew/Kurt as IfElse conditions to identify extreme asymmetric or fat-tail environments for reversal signals. High PV Corr Interaction Combine price-volume correlation (Corr) with amount efficiency or trend operators to capture volume-price coordination. High Robust Efficiency Use median (Med) and other robust statistics to smooth amount effi-ciency, filtering extreme noise. High Smoothed Efficiency Rank Apply time-series smoothing (EMA) to amount efficiency before cross-sectional ranking. High Trend Regression Adaptive Use Rsquare/Slope/Resi operators for adaptive trend regression. High R2 ‚Üí slope reversal; Low R 2 ‚Üí residual reversal. High Logical ‚ÄôOr‚Äô Extreme Regimes Use Or operator to integrate multiple extreme indicators (volume/price) as environment switching conditions. High Kurtosis Regime Use kurtosis to identify fat-tail environments and adaptively adjust reversal windows. High Amt Efficiency Rank Interac-tion Combine amount efficiency time-series rank with other statistical fea-tures (e.g., kurtosis). Medium 

The recommended directions reveal several key insights about effective factor design in high-frequency intraday settings: 

(1) Higher-order moments as regime indicators. Skewness and kurtosis emerge as powerful tools for identifying market regimes. When used as conditions in IfElse branching, they enable factors to adapt their logic based on distributional characteristics. For example, Factor 095 (Higher_Moment_Regime_Switch) achieves IC = 0.062 by switching between amount efficiency and slope reversal based on skewness thresholds. This pattern suggests that extreme distributional environments (high skew or fat tails) signal different underlying market dynamics requiring distinct trading logic. Preprint, , Wang et al. 

(2) Trend regression operators provide orthogonal signals. The Rsquare/Slope/Resi operator family, introduced in our expanded operator library, enables capturing trend reliability and deviations. Factors using these operators (080‚Äì086) consistently achieve low correlation with existing VWAP-based factors while maintaining strong IC. The adaptive logic‚Äîusing slope reversal when R 2 is high (indicating reliable trend) versus residual reversal when R 2 is low‚Äîaligns with financial intuition about trend-following vs. mean-reversion regimes. 

(3) Amount efficiency as an underexplored dimension. Combining returns with transaction amount ( Returns /Amount ) produces signals orthogonal to pure price-based factors. The success of factors 092‚Äì099 demonstrates that this dimension, when properly smoothed and ranked, captures liquidity-adjusted momentum that complements traditional price signals. 

G Experience Memory: Forbidden Directions 

The following table summarizes patterns that consistently lead to high correlation with existing factors and should be avoided. 

Table 5: Forbidden mining directions (high correlation risk).                      

> Direction Correlated Factors Correlation Standardized Returns/Amount 006, 008, 009 >0.6 VWAP Deviation variants 006, 009, 012, 013, 016 >0.5 Mean Reversion / Price Deviation 001, 002 >0.5 Simple Delta Reversal 023, 024 >0.5 Close-Position Location 028, 044 0.87+ Volatility + Price Position Branch 046, 064 >0.6 High R 2Trend Following 081, 083 >0.6 Rsquare Weighted Momentum 002, 023 >0.7 WMA/EMA Smoothed Efficiency 092 >0.9

The forbidden directions encode critical "negative knowledge" that prevents wasteful exploration: 

(1) The VWAP cluster dominance. Factor 006 (VWAP Deviation) and its variants form the most densely populated region of our correlation space. Any new factor involving close-to-VWAP distances, standardized by volatility or transaction volume, will almost certainly correlate > 0.5 with this cluster. This explains why 40+ candidate factors were rejected during mining‚Äîthey inadvertently rediscovered VWAP logic in different algebraic forms. 

(2) The "Correlation Red Sea" phenomenon. As the library grew beyond 70 factors, we observed that high-IC candidates (IC > 0.08) increasingly correlated with existing factors. For example, in Batch 122, factors like SignedPower_Returns (IC = 0.10) and High_Log_Ratio (IC = 0.105) were rejected due to correlations of 0.69‚Äì0.74 with factors 083 and 029. This suggests that the "easy" signal space has been exhausted, and future mining must target genuinely novel dimensions. 

(3) Mathematical equivalence traps. Several forbidden directions represent mathematically equivalent or highly similar computations. For instance, Neg (TsRank (Div (Sub ($close , $vwap ), $vwap ), 24 )) and Neg (CsRank (Delta (Sub ($close , $vwap ), 3))) appear structurally dif-ferent but capture essentially the same VWAP deviation dynamic. Recording these equivalences prevents the agent from "rediscovering" the same signal. 

H Mining Log Analysis 

This section provides detailed analysis of representative mining batches, illustrating the Ralph Loop in action. 

H.1 Batch 126: Median/Log/Resi Exploration (40 Workers) 

Setup. 40 parallel workers evaluated 40 candidate factors generated from Median/Log/Resi combinations. IC threshold: 0.05; correlation threshold: 0.5. 

Stage 1 Results. 7 candidates passed IC screening: 

‚Ä¢ Open_Close_ResiCross (IC = 0.071) 

‚Ä¢ Median_Adjusted_Returns (IC = 0.068) 

‚Ä¢ High_Log_CloseSwitch (IC = 0.062) 

‚Ä¢ Open_Resi_Skew_Hybrid (IC = 0.062) 

‚Ä¢ Median_LogSwap (IC = 0.057) 

‚Ä¢ Low_Log_RangeSwitch (IC = 0.089) 

‚Ä¢ Low_Log_SignedPower (IC = 0.101) FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery Preprint, , 

Stage 2 Bottleneck. 5 of 7 candidates were blocked by correlation with existing factors: 

‚Ä¢ Low_Log_SignedPower: corr = 0.74 with Factor 028 (Close-Low √ó Volume) 

‚Ä¢ Median_Adjusted_Returns: corr = 0.68 with Factor 083 (Rsquare Filtered Momentum) 

‚Ä¢ High_Log_CloseSwitch: corr = 0.61 with Factor 101 (Median Returns Switch) 

‚Ä¢ Open_Close_ResiCross: corr = 0.58 with Factor 006 (VWAP Deviation) 

‚Ä¢ Low_Log_RangeSwitch: corr = 0.52 with Factor 028 

Final Admission. 2 factors admitted: 

‚Ä¢ Factor 103 (Open_Resi_Skew_Hybrid): IC = 0.062, ICIR = 0.62, max_corr = 0.41 

‚Ä¢ Factor 104 (Median_LogSwap): IC = 0.057, ICIR = 0.59, max_corr = 0.49 

Lessons Learned. 

(1) High IC ‚â† admission : The highest-IC candidate (Low_Log_SignedPower, IC = 0.101) was rejected due to correlation. (2) Residual combinations effective : The Resi operator combined with Skew branching produced orthogonal signals. (3) Memory update : Added "Low-level log/close combinations" to forbidden directions (correlated with 028). 

I Factor Combination Detailed Analysis 

This section provides in-depth analysis of the three factor combination strategies. 

Table 6: Detailed combination analysis (110 factors).                               

> Metric Equal-Weight IC-Weighted Orthogonal IC Mean 0.1451 0.1496 0.1400 ICIR 1.2053 1.2430 1.1933 IC Win Rate 85.0% 85.8% 84.8% Q1 Return ‚àí0.0422% ‚àí0.0441% ‚àí0.0406% Q5 Return 0.0603% 0.0619% 0.0564% L-S Return 0.0513% 0.0531% 0.0486% L-S Cumulative 23.72 26.67 19.84 Monotonicity 1.0 1.0 1.0 Avg Turnover 20.14% 20.43% 19.67%

Figures Figures 5 and 6 visualize the temporal IC stability and return profiles of the three combination strategies. 

(1) IC-weighted combination achieves highest cumulative returns. By weighting factors proportionally to their historical IC, the IC-weighted method concentrates exposure on the most predictive signals. The 12.4% improvement in cumulative returns (26.67 vs. 23.72) over equal-weight comes at minimal cost in turnover (+1.4%). 

(2) Orthogonal combination underperforms expectations. Despite its theoretical appeal (removing redundant information), orthogo-nalization actually reduces both IC and cumulative returns. This counterintuitive result suggests that the correlation structure among factors contains useful information‚Äîhighly correlated factors may be capturing the same underlying signal with different noise characteristics, and averaging them provides noise reduction benefits. 

(3) Perfect monotonicity across all methods. All three methods achieve monotonicity = 1.0, indicating perfect rank ordering of quintile returns. This confirms that the combined factors produce consistent cross-sectional rankings, essential for practical portfolio construction. 

J Factor Selection Detailed Analysis 

This section provides comprehensive analysis of the three factor selection methods. 

Interpretation. Figures Figures 7 and 8 summarize the IC stability and return profiles for the three selection methods. 

(1) XGBoost achieves superior performance through nonlinear interactions. The 24% improvement in ICIR (1.49 vs. 1.20) and 51% improvement in cumulative returns (82.6 vs. 54.7) over Lasso demonstrates the value of capturing nonlinear factor interactions. The regime-switching factors (IfElse-based) likely contribute significantly here, as tree-based models can naturally exploit their conditional logic. 

(2) Lasso achieves extreme sparsity. With only 8 factors, Lasso captures 95% of the IC improvement achievable by the full library. This suggests that most predictive information is concentrated in a small subset of factors, consistent with the Pareto principle observed in many financial applications. 

(3) Stepwise provides interpretable middle ground. Forward stepwise selection balances sparsity (18 factors) with performance (ICIR = 1.38). The greedy selection trajectory (Table 9) reveals the diminishing returns of adding factors beyond the first 10. Preprint, , Wang et al. 2024-01 

> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01

Date 

0.00 

0.05 

0.10 

0.15 

0.20 

> Rank IC

Library_Comb - Daily Mean Rank IC 

Mean: 0.1451 

-0.4 -0.2 0.0 0.2 0.4 0.6 

Rank IC 

0

200 

400 

600 

800 

> Frequency

IC Mean: 0.1451 

IC Std: 0.1204 

ICIR: 1.21 

Win Rate: 85.0% 

Library_Comb - Rank IC Distribution 

Mean: 0.1451 

> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01

Date 

0.000 

0.025 

0.050 

0.075 

0.100 

0.125 

0.150 

0.175 

> Rolling IC

Library_Comb - 21-Day Rolling IC (Daily Aggregated) 

> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01

Date 

0

5

10 

15 

20 

25 

30 

35 

> Cumulative IC

Library_Comb - Cumulative IC Comparison 

(a) Equal-weight IC 2024-01 

> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01

Date 

0.00 

0.05 

0.10 

0.15 

0.20 

> Rank IC

Library_Comb - Daily Mean Rank IC 

Mean: 0.1496 

-0.4 -0.2 0.0 0.2 0.4 0.6 

Rank IC 

0

200 

400 

600 

800 

> Frequency

IC Mean: 0.1496 

IC Std: 0.1203 

ICIR: 1.24 

Win Rate: 85.8% 

Library_Comb - Rank IC Distribution 

Mean: 0.1496 

> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01

Date 

0.000 

0.025 

0.050 

0.075 

0.100 

0.125 

0.150 

0.175 

> Rolling IC

Library_Comb - 21-Day Rolling IC (Daily Aggregated) 

> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01

Date 

0

5

10 

15 

20 

25 

30 

35 

> Cumulative IC

Library_Comb - Cumulative IC Comparison (b) IC-weighted IC 2024-01 

> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01

Date 

0.00 

0.05 

0.10 

0.15 

0.20 

> Rank IC

Library_Comb - Daily Mean Rank IC 

Mean: 0.1400 

-0.4 -0.2 0.0 0.2 0.4 0.6 

Rank IC 

0

200 

400 

600 

800 

> Frequency

IC Mean: 0.1400 

IC Std: 0.1173 

ICIR: 1.19 

Win Rate: 84.8% 

Library_Comb - Rank IC Distribution 

Mean: 0.1400 

> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01

Date 

0.000 

0.025 

0.050 

0.075 

0.100 

0.125 

0.150 

0.175 

> Rolling IC

Library_Comb - 21-Day Rolling IC (Daily Aggregated) 

> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01

Date 

0

5

10 

15 

20 

25 

30 

35 

> Cumulative IC

Library_Comb - Cumulative IC Comparison (c) Orthogonal IC 

Figure 5: IC time-series analysis for three combination methods. All methods show stable positive IC throughout the evaluation period, with IC-weighted exhibiting slightly higher peaks. Q1 Q2 Q3 Q4 Q5 

Quantile 

-0.04 

-0.02 

0.00 

0.02 

0.04 

0.06 

> Mean Return (%)

-0.042% 

-0.018% 

0.001% 

0.024% 

0.060% 

Optimized: Library_Comb - Mean Returns 

> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01

Date 

0

1000 

2000 

3000 

4000 

> Cum Return (%)

Optimized: Library_Comb - Cumulative Returns (Daily) 

> Q1
> Q2
> Q3
> Q4
> Q5
> L-S

(a) Equal-weight quintiles Q1 Q2 Q3 Q4 Q5 

Quantile 

-0.04 

-0.02 

0.00 

0.02 

0.04 

0.06 

> Mean Return (%)

-0.044% 

-0.019% 

0.002% 

0.024% 

0.062% 

Optimized: Library_Comb - Mean Returns 

> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01

Date 

0

1000 

2000 

3000 

4000 

> Cum Return (%)

Optimized: Library_Comb - Cumulative Returns (Daily)  

> Q1
> Q2
> Q3
> Q4
> Q5
> L-S

(b) IC-weighted quintiles Q1 Q2 Q3 Q4 Q5 

Quantile 

-0.04 

-0.02 

0.00 

0.02 

0.04 

0.06 

> Mean Return (%)

-0.041% 

-0.018% 

0.002% 

0.025% 

0.056% 

Optimized: Library_Comb - Mean Returns 

> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01

Date 

0

500 

1000 

1500 

2000 

2500 

3000 

> Cum Return (%)

Optimized: Library_Comb - Cumulative Returns (Daily)  

> Q1
> Q2
> Q3
> Q4
> Q5
> L-S

(c) Orthogonal quintiles 

Figure 6: Quantile returns for three combination methods. All methods show perfect monotonicity with Q5 (highest factor value) consistently outperforming Q1 (lowest). Table 7: Detailed selection analysis. 

Metric Lasso Stepwise XGBoost # Selected Factors 8 18 110 (all) IC Mean 0.1562 0.1556 0.1633 ICIR 1.2039 1.3827 1.4929 IC Win Rate 87.2% 88.5% 92.6% Q1 Return ‚àí0.0604% ‚àí0.0485% ‚àí0.0609% Q5 Return 0.0678% 0.0625% 0.0804% L-S Return 0.0642% 0.0556% 0.0708% L-S Cumulative 54.69 31.51 82.63 Monotonicity 1.0 1.0 1.0 Avg Turnover 19.92% 20.02% 19.32% 

K Cost Pressure Stress Test 

We further evaluate the robustness of downstream combination and selection methods under transaction cost pressure. Specifically, we test five transaction cost settings (1, 4, 7, 10, and 11 bps) and report the resulting net cumulative performance trajectories. Figure 9 summarizes the cost-pressure analyses for three factor combination strategies and three factor selection strategies. For ease of comparison across cost regimes and performance scales, each panel is shown with both a linear y-axis and a log-scale y-axis. 

L Lasso Selected Factors Analysis 

Interpretation. The Lasso-selected factors represent a diverse minimal spanning set of the factor space: 

Factor 006 (VWAP Deviation) dominates with coefficient 4.5x larger than the second factor. This confirms VWAP as the most powerful single signal for intraday reversal prediction. 

Factor 002 (EMA Deviation) provides complementary trend information using exponential smoothing rather than volume-weighted averaging. 

Factors 079, 045 (Regime-switching) introduce conditional logic that adapts to market state, providing nonlinear signal that linear Lasso partially captures through coefficient weighting. FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery Preprint, , 2024-01 

> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0.00
> 0.05
> 0.10
> 0.15
> 0.20
> Rank IC

Lasso_Select - Daily Mean Rank IC      

> Mean: 0.1563
> -0.4 -0.2 0.0 0.2 0.4 0.6
> Rank IC
> 0
> 200
> 400
> 600
> 800
> Frequency
> IC Mean: 0.1562
> IC Std: 0.1298
> ICIR: 1.20
> Win Rate: 87.2%

Lasso_Select - Rank IC Distribution 

> Mean: 0.1562
> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0.000
> 0.025
> 0.050
> 0.075
> 0.100
> 0.125
> 0.150
> 0.175
> 0.200
> Rolling IC

Lasso_Select - 21-Day Rolling IC (Daily Aggregated) 

> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0
> 5
> 10
> 15
> 20
> 25
> 30
> 35
> Cumulative IC

Lasso_Select - Cumulative IC Comparison 

(a) Lasso IC analysis 2024-01 

> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0.00
> 0.05
> 0.10
> 0.15
> 0.20
> Rank IC

Stepwise_Sel - Daily Mean Rank IC      

> Mean: 0.1558
> -0.4 -0.2 0.0 0.2 0.4 0.6
> Rank IC
> 0
> 100
> 200
> 300
> 400
> 500
> 600
> 700
> 800
> Frequency
> IC Mean: 0.1556
> IC Std: 0.1125
> ICIR: 1.38
> Win Rate: 88.5%

Stepwise_Sel - Rank IC Distribution 

> Mean: 0.1556
> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0.000
> 0.025
> 0.050
> 0.075
> 0.100
> 0.125
> 0.150
> 0.175
> Rolling IC

Stepwise_Sel - 21-Day Rolling IC (Daily Aggregated) 

> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0
> 5
> 10
> 15
> 20
> 25
> 30
> 35
> Cumulative IC

Stepwise_Sel - Cumulative IC Comparison (b) Stepwise IC analysis 2024-01 

> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0.00
> 0.05
> 0.10
> 0.15
> 0.20
> 0.25
> Rank IC

XGBoost_Comb - Daily Mean Rank IC     

> Mean: 0.1633
> -0.2 0.0 0.2 0.4 0.6
> Rank IC
> 0
> 100
> 200
> 300
> 400
> 500
> 600
> 700
> 800
> Frequency
> IC Mean: 0.1633
> IC Std: 0.1094
> ICIR: 1.49
> Win Rate: 92.6%

XGBoost_Comb - Rank IC Distribution 

> Mean: 0.1633
> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0.000
> 0.025
> 0.050
> 0.075
> 0.100
> 0.125
> 0.150
> 0.175
> 0.200
> Rolling IC

XGBoost_Comb - 21-Day Rolling IC (Daily Aggregated) 

> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0
> 5
> 10
> 15
> 20
> 25
> 30
> 35
> 40
> Cumulative IC

XGBoost_Comb - Cumulative IC Comparison (c) XGBoost IC analysis 

Figure 7: IC time-series analysis for three selection methods. XGBoost shows the most stable IC with highest win rate (92.6%). Q1 Q2 Q3 Q4 Q5 

> Quantile
> -0.06
> -0.04
> -0.02
> 0.00
> 0.02
> 0.04
> 0.06
> Mean Return (%)
> -0.060%
> -0.014%
> 0.003%
> 0.018%
> 0.068%

Optimized: Lasso_Select - Mean Returns 

> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 0
> 1000
> 2000
> 3000
> 4000
> 5000
> 6000
> Cum Return (%)

Optimized: Lasso_Select - Cumulative Returns (Daily) 

> Q1
> Q2
> Q3
> Q4
> Q5
> L-S

(a) Lasso quintile returns Q1 Q2 Q3 Q4 Q5 

> Quantile
> -0.04
> -0.02
> 0.00
> 0.02
> 0.04
> 0.06
> Mean Return (%)
> -0.048%
> -0.019%
> 0.004%
> 0.026%
> 0.062%

Optimized: Stepwise_Sel - Mean Returns 

> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 0
> 1000
> 2000
> 3000
> 4000
> Cum Return (%)

Optimized: Stepwise_Sel - Cumulative Returns (Daily)  

> Q1
> Q2
> Q3
> Q4
> Q5
> L-S

(b) Stepwise quintile returns Q1 Q2 Q3 Q4 Q5 

> Quantile
> -0.06
> -0.04
> -0.02
> 0.00
> 0.02
> 0.04
> 0.06
> 0.08
> Mean Return (%)
> -0.061%
> -0.013%
> 0.003%
> 0.020%
> 0.080%

Optimized: XGBoost_Comb - Mean Returns 

> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 0
> 2000
> 4000
> 6000
> 8000
> 10000
> 12000
> 14000
> Cum Return (%)

Optimized: XGBoost_Comb - Cumulative Returns (Daily)  

> Q1
> Q2
> Q3
> Q4
> Q5
> L-S

(c) XGBoost quintile returns 

Figure 8: Quantile returns for three selection methods. XGBoost shows the widest Q5-Q1 spread, indicating strongest discrimi-native power. Table 8: Factors selected by Lasso (8 factors, sorted by coefficient magnitude). 

ID Name Coefficient Role 006 VWAP Deviation 3.23 √ó 10 ‚àí4 Core anchor 002 EMA Deviation 7.23 √ó 10 ‚àí5 Trend baseline 079 Regime_Vol_Range_Pos 2.58 √ó 10 ‚àí5 Regime switch 040 PricePos_Skew_Slope 1.14 √ó 10 ‚àí5 Distribution 011 Price-VWAP Momentum 8.18 √ó 10 ‚àí6 VWAP variant 045 Kurtosis-Regime Range 7.18 √ó 10 ‚àí6 Higher moment 009 Vol-Filtered Return 4.28 √ó 10 ‚àí6 Risk filter 022 Lower-Shadow Ratio 2.59 √ó 10 ‚àí6 Candlestick 

Factors 040, 022 (Distribution/Candlestick) represent orthogonal signal sources‚Äîhigher-order moments and price action patterns‚Äîthat Lasso identifies as non-redundant. 

M Stepwise Selection Trajectory 

Interpretation. The stepwise trajectory reveals the structure of marginal contribution: 

Steps 1‚Äì5: Rapid ICIR improvement (+0.133). The first five factors (006, 046, 079, 044, 041) provide the bulk of predictive improvement. These factors span different logical categories (VWAP, regime-switching, higher moments), confirming the value of diversity. 

Steps 6‚Äì11: Diminishing returns. Adding factors 6‚Äì11 improves ICIR by only +0.020 total, despite each factor having respectable individual IC ( > 0.07). This plateau indicates that the marginal information content decreases rapidly after the core signal space is covered. 

Step 12: Second-wave improvement. Factor 026 (PV Covariance Long) provides an unexpected +0.041 ICIR boost, suggesting that price-volume covariance captures a distinct signal dimension not well-represented by the first 11 factors. 

Steps 13‚Äì18: Final refinement. The last six factors add only +0.026 ICIR total, confirming convergence to the ICIR ceiling. 

N XGBoost Feature Importance Analysis 

Interpretation. XGBoost importance reveals the nonlinear signal structure: 

(1) VWAP remains dominant but not overwhelming. Factor 006 contributes 6.04% importance‚Äîsignificant but not dominant. In contrast, Lasso assigns 006 a coefficient 4.5x larger than the second factor. This suggests XGBoost extracts value from many factors simultaneously rather than relying on a single anchor. Preprint, , Wang et al. 01-01  

> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 0
> 200
> 400
> 600
> 800
> 1000
> 1200
> 1400
> Net Cum Return (%)
> Optimized (Linear Scale)
> 1bps
> 4bps
> 7bps
> 10bps
> 11bps
> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 10 1
> 10 0
> 10 1
> Net Cum Value
> Optimized (Log Scale)
> 1bps
> 4bps
> 7bps
> 10bps
> 11bps

(a) Equal-weight (combination) 01-01   

> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 0
> 250
> 500
> 750
> 1000
> 1250
> 1500
> Net Cum Return (%)
> Optimized (Linear Scale)
> 1bps
> 4bps
> 7bps
> 10bps
> 11bps
> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 10 1
> 10 0
> 10 1
> Net Cum Value
> Optimized (Log Scale)
> 1bps
> 4bps
> 7bps
> 10bps
> 11bps

(b) IC-weighted (combination) 01-01   

> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 0
> 200
> 400
> 600
> 800
> 1000
> 1200
> Net Cum Return (%)
> Optimized (Linear Scale)
> 1bps
> 4bps
> 7bps
> 10bps
> 11bps
> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 10 1
> 10 0
> 10 1
> Net Cum Value
> Optimized (Log Scale)
> 1bps
> 4bps
> 7bps
> 10bps
> 11bps

(c) Orthogonal (combination) 01-01 

> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 0
> 500
> 1000
> 1500
> 2000
> 2500
> 3000
> Net Cum Return (%)
> Optimized (Linear Scale)
> 1bps
> 4bps
> 7bps
> 10bps
> 11bps
> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 10 0
> 10 1
> Net Cum Value
> Optimized (Log Scale)
> 1bps
> 4bps
> 7bps
> 10bps
> 11bps

(d) Lasso (selection) 01-01  

> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 0
> 250
> 500
> 750
> 1000
> 1250
> 1500
> 1750
> Net Cum Return (%)
> Optimized (Linear Scale)
> 1bps
> 4bps
> 7bps
> 10bps
> 11bps
> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 10 0
> 10 1
> Net Cum Value
> Optimized (Log Scale)
> 1bps
> 4bps
> 7bps
> 10bps
> 11bps

(e) Stepwise (selection) 01-01  

> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 0
> 1000
> 2000
> 3000
> 4000
> 5000
> Net Cum Return (%)
> Optimized (Linear Scale)
> 1bps
> 4bps
> 7bps
> 10bps
> 11bps
> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 10 0
> 10 1
> Net Cum Value
> Optimized (Log Scale)
> 1bps
> 4bps
> 7bps
> 10bps
> 11bps

(f) XGBoost (selection) 

Figure 9: Cost pressure stress tests for three factor combination methods (Equal-weight, IC-weighted, Orthogonal; top row) and three factor selection methods (Lasso, Stepwise, XGBoost; bottom row). For each method, we evaluate performance under five transaction cost settings: 1, 4, 7, 10, and 11 bps. Each panel reports the net cumulative performance over time using both a linear and a log-scale y-axis to facilitate comparison across cost regimes and performance scales. Table 9: Forward stepwise selection trajectory (all 18 steps). 

Step Added Factor Indiv. IC Comb. IC ICIR ŒîICIR 1 006 VWAP Deviation 0.129 0.150 1.163 ‚Äì2 046 Vol-Regime Reversal 0.109 0.145 1.184 +0.021 3 079 Regime Vol-Range 0.103 0.145 1.234 +0.050 4 044 Kurtotic Vol Intensity 0.100 0.145 1.271 +0.037 5 041 Price Range Skew 0.097 0.145 1.296 +0.025 6 040 PricePos Skew Slope 0.095 0.145 1.299 +0.003 7 013 Illiquidity VWAP 0.092 0.148 1.304 +0.005 8 107 PricePos Skew VolDelta 0.090 0.147 1.304 +0.000 9 004 High-Vol Weakness 0.082 0.150 1.308 +0.004 10 011 Price-VWAP Momentum 0.078 0.151 1.315 +0.007 11 074 Range Pos Vol Skew 0.073 0.150 1.316 +0.001 12 026 PV Covariance Long 0.066 0.153 1.357 +0.041 13 054 Amount Regime 0.063 0.153 1.366 +0.009 14 016 VWAP Vol Skew Switch 0.063 0.154 1.370 +0.004 15 005 Range-Pos Vol-Cov 0.055 0.154 1.370 +0.000 16 022 Lower-Shadow Ratio 0.055 0.154 1.378 +0.008 17 051 High Med Volume Switch 0.049 0.155 1.381 +0.003 18 076 Amt Velocity Volatility 0.047 0.156 1.383 +0.002 

(2) Regime-switching factors are highly valued. Factors 068 (Skewness Regime) and 070 (Price-Vol Interaction) rank 4th and 6th despite lower individual IC than factors 046 or 079. This indicates XGBoost effectively exploits their conditional logic structure. 

(3) Classic Alpha101 factors remain relevant. Factors 061 (Alpha101_12_Modern) and 053 (Alpha101_1_V) rank 2nd and 13th, demonstrating that modernized versions of classic formulas contribute unique signals even in a 110-factor library. FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery Preprint, , 

Table 10: XGBoost feature importance (top 20 factors). 

ID Name Import. Category 006 VWAP Deviation 6.04% VWAP 061 Alpha101_12_Modern 4.06% Classic 023 Normalized-Momentum TsRank 3.59% Momentum 068 Skewness_Regime_PV_Div 3.55% Regime 028 Close-Low √ó Volume 3.03% Price range 070 Price_Pos_Vol_Interaction 2.27% Interaction 057 TsRank_PV_Divergence 2.26% Divergence 029 High-Close √ó Volume 2.15% Price range 018 Range-Position Vol Regime 1.62% Range 045 Kurtosis-Regime Range 1.57% Higher moment 048 Vol-Price Rank Divergence 1.47% Divergence 104 Median_LogSwap 1.46% Median 053 Alpha101_1_V 1.44% Classic 101 Median_Returns_Switch 1.41% Median 004 High-Vol Relative Weakness 1.34% Volume 055 Skew_Open_Close_Product 1.24% Distribution 019 VWAP-Gap Regime Range 1.20% VWAP 092 Amt_Efficiency_EMA_Smooth 1.16% Efficiency 073 Amt_Velocity_Regime 1.16% Efficiency 043 Skewed Volume Momentum 1.11% Distribution 

(4) Long-tail distribution of importance. The top 20 factors contribute 43.8% of total importance, while the remaining 90 factors contribute 56.2%. This confirms that XGBoost extracts value broadly across the library, justifying the construction of a diverse factor set. 

O Individual Factor Tear-Sheet Example 

We present the complete evaluation report for Factor 046 (Volatility-Regime Reversal Divergence), one of the top-performing factors (see Figure 10). 

Factor Profile. 

‚Ä¢ ID : 046 

‚Ä¢ Name : Volatility-Regime Reversal Divergence 

‚Ä¢ Formula : IfElse(Greater(Std($returns, 12), Mean(Std($returns, 12), 48)), Neg(CsRank(Delta($close, 3))), Neg(CsRank(Div(Sub($close, $low), Add(Sub($high, $low), 0.0001))))) 

‚Ä¢ Category : Regime-switching 

Metric Value IC Mean 0.1087 ICIR 0.9422 IC Win Rate (daily) 80.1% Q1 Return ‚àí0.0380% Q5 Return 0.0402% L-S Return 0.0390% L-S Cumulative 10.57 Monotonicity 1.0 Avg Turnover 15.69% 

Performance Metrics. Note. IC is computed at 10-minute frequency. The win rate is reported at daily granularity by aggregating the 10-minute IC values within each day (daily mean) and counting the fraction of days with positive daily IC. 

Financial Logic. This factor implements adaptive reversal logic based on volatility regime: 

‚Ä¢ High volatility regime (Std > 48-period mean): Use volume-price divergence signal. In turbulent markets, divergence between volume momentum and price momentum signals exhaustion. 

‚Ä¢ Low volatility regime : Use VWAP deviation normalized by volatility. In calm markets, simple mean-reversion to VWAP is more reliable. Preprint, , Wang et al. 2024-01      

> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0.000
> 0.025
> 0.050
> 0.075
> 0.100
> 0.125
> 0.150
> 0.175
> Rank IC
> Volatility-R - Daily Mean Rank IC
> Mean: 0.1087
> -0.4 -0.2 0.0 0.2 0.4 0.6
> Rank IC
> 0
> 200
> 400
> 600
> 800
> Frequency
> IC Mean: 0.1087
> IC Std: 0.1153
> ICIR: 0.94
> Win Rate: 80.1%
> Volatility-R - Rank IC Distribution
> Mean: 0.1087
> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0.00
> 0.02
> 0.04
> 0.06
> 0.08
> 0.10
> 0.12
> Rolling IC
> Volatility-R - 21-Day Rolling IC (Daily Aggregated)
> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0
> 5
> 10
> 15
> 20
> 25
> Cumulative IC
> Volatility-R - Cumulative IC Comparison

(a) IC time-series analysis Q1 Q2 Q3 Q4 Q5  

> Quantile
> -0.04
> -0.03
> -0.02
> -0.01
> 0.00
> 0.01
> 0.02
> 0.03
> 0.04
> Mean Return (%)
> -0.038%
> -0.017%
> 0.004%
> 0.021%
> 0.040%
> Optimized: Volatility-R - Mean Returns
> 01-01
> 03-01
> 05-01
> 07-01
> 09-01
> 11-01
> 01-01
> Date
> 0
> 200
> 400
> 600
> 800
> 1000
> Cum Return (%)
> Optimized: Volatility-R - Cumulative Returns (Daily)
> Q1
> Q2
> Q3
> Q4
> Q5
> L-S

(b) Quantile returns -1.0 -0.8 -0.6 -0.4 -0.2 0.0 

> Factor Value
> 0
> 10000
> 20000
> 30000
> 40000
> 50000
> 60000
> Frequency
> Volatility-R - Factor Value Distribution
> Mean: -0.5034
> 2024-01
> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> -0.9
> -0.8
> -0.7
> -0.6
> -0.5
> -0.4
> -0.3
> -0.2
> -0.1
> Volatility-R - CS Statistics

(c) Factor value distribution 2024-01  

> 2024-03
> 2024-05
> 2024-07
> 2024-09
> 2024-11
> 2025-01
> Date
> 0
> 250
> 500
> 750
> 1000
> 1250
> 1500
> 1750
> Turnover (%)
> Volatility-R - Daily Total Turnover
> Avg Daily Turnover: 1569.1%

(d) Turnover analysis 

Figure 10: Complete tear-sheet for Factor 046 (Volatility-Regime Reversal Divergence). The factor shows stable IC with 80% daily win rate, perfect monotonicity in quintile returns, and moderate turnover (15.7%). 

P Full Factor Library (110 factors) 

We provide the complete factor library (ID, name, and formula). 

ID Name Formula 001 Intraday Range Position Neg(CsRank(Div(Sub($close, Min($close, 48)), Add(Sub(Max($close, 48), Min($close, 48)), 1e-8)))) 

002 EMA Deviation Neg(Div(Sub($close, EMA($close, 10)), EMA($close, 10))) 

003 Vol-VWAP Divergence Sub(CsRank(Delta($volume, 1)), CsRank(Div(Sub($close, $vwap), $vwap))) 

004 High-Volume Relative Weakness Mul(CsRank(Div($volume, Mean($volume, 24))), CsRank(Neg($returns))) 

005 Range-Position Vol-Volume Cov Reversal Neg(Mul(TsRank(Div(Sub($close, TsMin($close, 12)), Add(Sub(TsMax($close, 12), TsMin($close, 12)), 1e-6)), 12), CsRank(Abs(Cov($returns, $volume, 12))))) 

006 VWAP Deviation Neg(Div(Sub($close, $vwap), $vwap)) 

007 Price-Volume Mean Reversal Neg(Sub(CsRank(Div(Sub($close, $low), Add(Sub($high, $low), 0.0001))), CsRank(Delta(EMA($volume, 5), 5)))) 

008 Normalized Money Strength Reversal Neg(Mul(CsRank(Div($returns, Add(Std($returns, 12), 0.0001))), CsRank(Div($volume, Mean($volume, 12))))) 

009 Volatility-Filtered Return Reversal Neg(Mul(CsRank($returns), CsRank(Std($returns, 12)))) 

010 Price-Volume Divergence Reversal Neg(CsRank(Sub(CsRank(Div($close, Delay($close, 5))), CsRank(Div($volume, Delay($volume, 5)))))) 

011 Price-VWAP Momentum Overextension Neg(Sub(CsRank(Delta($close, 6)), CsRank(Delta($vwap, 6)))) 

012 Volume-Augmented VWAP Reversal Mul(CsRank(Neg(Div(Sub($close, $vwap), $vwap))), CsRank(Div($volume, EMA($volume, 12)))) 

013 Illiquidity-Augmented VWAP Reversal Mul(CsRank(Neg(Div(Sub($close, $vwap), $vwap))), Sub(1, CsRank($volume))) 

014 Resilience-Momentum Blend Reversal Mul(Add(Mul(CsRank(Neg(Div(Sub($close, $vwap), $vwap))), 0.6), Mul(CsRank(Delta(Neg(Div(Sub($close, $vwap), $vwap)), 3)), 0.4)), CsRank(Delta(Div(Mean(Div(Abs($returns), Add($volume, 1)), 24), Add(Div(Abs($returns), Add($volume, 1)), 1e-6)), 3))) 

015 Volatility-Balanced Momentum Reversal Mul(CsRank(Neg(Delta($returns, 3))), Mul(Sub(1, CsRank(Std($returns, 12))), CsRank(Std($returns, 12)))) 

016 VWAP-Deviation Acceleration Reversal Neg(Mul(CsRank(Delta(Div(Sub($close, $vwap), $vwap), 3)), CsRank(Div($volume, EMA($volume, 12))))) 

017 VWAP-Acceleration Volatility Reversal Neg(Mul(TsRank(Delta(Div(Sub($close, $vwap), $vwap), 3), 12), TsRank(Std($returns, 12), 12))) 

018 Range-Position Volume Regime Reversal Neg(Mul(TsRank(Div(Sub($close, TsMin($close, 12)), Add(Sub(TsMax($close, 12), TsMin($close, 12)), 1e-6)), 12), TsRank(Div($volume, EMA($volume, 12)), 12))) 

019 VWAP-Gap Regime Range Reversal IfElse(Greater(Abs(Div(Sub($close, $vwap), $vwap)), 0.01), Neg(CsRank(Sub($close, TsMax($close, 12)))), Neg(CsRank(Sub($close, TsMin($close, 12))))) 

020 Volume-Price Delta Divergence TsRank TsRank(Sub(CsRank(Delta($volume, 3)), CsRank(Delta($close, 3))), 18) 

021 Volume-Shock VWAP-Momentum Reversal Neg(Mul(TsRank(Delta(Div($volume, EMA($volume, 12)), 3), 12), TsRank(Delta(Div(Sub($close, $vwap), $vwap), 3), 12))) 

022 Lower-Shadow Ratio Reversal Neg(CsRank(Div(Sub(Min2($open, $close), $low), Add(Sub($high, $low), 0.0001)))) 

023 Normalized-Momentum TsRank Reversal Neg(TsRank(Div(Delta($close, 3), Mean(Abs(Delta($close, 3)), 12)), 12)) 

024 Open-Close Momentum Divergence Sub(TsRank(Delta($open, 6), 12), TsRank(Delta($close, 6), 12)) 

025 Open-Close Momentum Divergence V2 Sub(TsRank(Delta($open, 9), 18), TsRank(Delta($close, 9), 18)) 

026 Price-Volume Covariance Position Long Neg(Mul(TsRank(Cov(TsRank($close, 18), TsRank($volume, 18), 10), 18), TsRank(Div(Sub($close, $low), Sub($high, $low)), 18))) 

(continued) FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery Preprint, , 

ID Name Formula 027 Open-Volume Covariance Position Neg(Mul(TsRank(Cov(TsRank($open, 12), TsRank($volume, 12), 5), 12), TsRank(Div(Sub($close, $low), Sub($high, $low)), 12))) 

028 Close-Low √ó Volume Ratio Neg(Mul(Sub($close, $low), Div($volume, Mean($volume, 12)))) 

029 High-Close √ó Volume Ratio Mul(Sub($high, $close), Div($volume, Mean($volume, 12))) 

030 Delta(High-Close)3 √ó Volume Ratio Mul(Delta(Sub($high, $close), 3), Div($volume, Mean($volume, 12))) 

031 Price Trend Conditional Strategy IfElse(Greater(Delta($close, 1), 0), Neg(TsRank(Delta($close, 3), 12)), TsRank(Delta($close, 3), 12)) 

032 Returns-Volume Delta Divergence Neg(TsRank(Sub(CsRank(Delta($returns, 2)), CsRank(Delta($volume, 2))), 12)) 

033 Volume-Range Correlation Shock Neg(Mul(TsRank(Corr(TsRank(Div($volume, Mean($volume, 24)), 12), TsRank(Sub($high, $low), 12), 12), 24), CsRank(Delta($close, 2)))) 

034 Price Trend Conditional Momentum V2 IfElse(Greater(Delta($close, 2), 0), Neg(TsRank(Delta($close, 5), 15)), TsRank(Delta($close, 5), 15)) 

035 High-Low Expansion vs Close Accel Neg(Mul(TsRank(Delta(Sub($high, $low), 2), 12), CsRank(Delta($close, 2)))) 

036 Volatility Switch Candle Reversal IfElse(Greater(Std($returns, 12), Mean(Std($returns, 12), 48)), Neg(CsRank(Div(Sub(Min2($open, $close), $low), Add(Sub($high, $low), 0.0001)))), Neg(CsRank($returns))) 

037 Price-Volume Momentum Synchrony Neg(TsRank(Mul(CsRank(Delta($close, 3)), CsRank(Delta($volume, 3))), 18)) 

038 Vol-Regime Conditional Divergence V2 IfElse(Greater(Std($returns, 12), Mean(Std($returns, 12), 48)), Neg(TsRank(Sub(CsRank(Delta($volume, 3)), CsRank(Delta($close, 3))), 18)), Neg(TsRank(Div(Sub($close, $vwap), Add(Std($returns, 24), 0.0001)), 12))) 

039 Kurtosis-Filtered Momentum Neg(Mul(CsRank(Delta($close, 5)), CsRank(Kurt($returns, 24)))) 

040 Price Intensity Skewness Blend Neg(Mul(CsRank(Div(Sub($close, $low), Add(Sub($high, $low), 0.0001))), CsRank(Skew($returns, 24)))) 

041 Price Range Skewness Interaction Mul(CsRank(Div(Sub($high, $close), Add(Sub($high, $low), 1e-8))), CsRank(Neg(Skew($returns, 24)))) 

042 Regime-Switching Skew Factor IfElse(Greater(Abs(Skew($returns, 24)), 1.0), Neg(CsRank($returns)), Neg(CsRank(Skew($returns, 24)))) 

043 Skewed Volume Momentum Neg(TsRank(Mul(CsRank(Skew($volume, 24)), CsRank(Delta($close, 3))), 12)) 

044 Kurtotic Volume Intensity Neg(Mul(CsRank(Kurt($volume, 24)), CsRank(Div(Sub($close, $low), Add(Sub($high, $low), 0.0001))))) 

045 Kurtosis-Regime Range Reversal IfElse(Greater(Kurt($returns, 24), 3.0), Neg(CsRank(Div(Sub($high, $close), Add(Sub($high, $low), 0.0001)))), Neg(CsRank(Div(Sub($close, $low), Add(Sub($high, $low), 0.0001))))) 

046 Volatility-Regime Reversal Divergence IfElse(Greater(Std($returns, 12), Mean(Std($returns, 12), 48)), Neg(CsRank(Delta($close, 3))), Neg(CsRank(Div(Sub($close, $low), Add(Sub($high, $low), 0.0001))))) 

047 Volume-weighted Alpha101-1 variant V2 Neg(Mul(CsRank(TsRank(Delta(Log(Add($volume, 1)), 3), 6)), CsRank(Div(Delta($close, 6), $close)))) 

048 Volatility-Price Rank Divergence TsRank V2 TsRank(Sub(CsRank(Std($returns, 12)), CsRank(Delta($close, 6))), 18) 

049 Regime_C0_F10_F21 IfElse(Greater(Skew($returns, 24), 0.5), CsRank(Delta($volume, 3)), Neg(CsRank(Std($returns, 12)))) 

050 Regime_C0_F11_F20 IfElse(Greater(Skew($returns, 24), 0.5), Neg(CsRank(Div(Sub($close, $low), Add(Sub($high, $low), 1e-6)))), Neg(CsRank(Delta($close, 6)))) 

051 Regime_C1_F11_F21 IfElse(Less(Skew($returns, 24), -0.5), CsRank(Delta($close, 5)), Neg(CsRank(Std($returns, 12)))) 

052 Regime_C1_F12_F20 IfElse(Less(Skew($returns, 24), -0.5), Neg(TsRank($returns, 12)), Neg(CsRank(Delta($close, 6)))) 

053 Alpha101_1_V Neg(CsRank(TsArgMax(SignedPower(IfElse(Less($returns, 0), Std($returns, 20), $close), 2), 5))) 

054 Amount_Regime_Reversal IfElse(Greater($amt, Mean($amt, 24)), CsRank(Delta($close, 3)), Neg(CsRank(Div(Sub($close, $vwap), $vwap)))) 

055 Return_Correlation_Regime IfElse(Greater(Corr($close, $volume, 12), 0.5), Neg(CsRank($returns)), CsRank(Delta($volume, 3))) 

056 Kurtosis_Regime_Amount_Efficiency IfElse(Greater(Kurt($returns, 24), 3.0), Neg(CsRank(Div($amt, Add($volume, 1e-6)))), Neg(CsRank(Delta($close, 3)))) 

057 TsRank_Price_Volume_Momentum_Divergence Neg(Sub(TsRank(Delta($close, 6), 24), TsRank(Delta($volume, 6), 24))) 

058 Regime_Amt_Efficiency_Switch_V2 IfElse(Greater($amt, Mean($amt, 48)), Neg(CsRank(Div($amt, Add($volume, 1e-6)))), Neg(TsRank($returns, 12))) 

059 Regime_Triple_Vol_Amt_Skew IfElse(Greater(Std($returns, 12), Mean(Std($returns, 12), 24)), Neg(CsRank(Skew($amt, 12))), Neg(CsRank(Delta($close, 3)))) 

060 Regime_Amt_Vol_Divergence_Switch IfElse(Greater(Div($amt, Mean($amt, 24)), 1.2), Sub(CsRank($close), CsRank($volume)), Neg(CsRank(Delta($close, 3)))) 

061 Alpha101_12_Modern Neg(Mul(TsRank(Delta($volume, 1), 12), TsRank(Delta($close, 1), 12))) 

062 Amount_Stability_Adjusted_Returns Neg(Div(CsRank($returns), Add(Std($amt, 12), 1e-6))) 

063 Triple_Rank_Synchrony_V8 Neg(TsRank(Add(Add(CsRank(Delta($returns, 3)), CsRank(Delta($amt, 3))), CsRank(Kurt($volume, 12))), 12)) 

064 Regime_Price_Volume_Corr_Switch_V3 IfElse(Less(Corr($close, $volume, 12), -0.5), CsRank($returns), Neg(CsRank(Delta($close, 3)))) 

065 Regime_Volatility_Regime_Switch_V2 IfElse(Greater(Std($returns, 12), Mean(Std($returns, 12), 48)), Neg(CsRank(Delta($amt, 6))), Neg(CsRank(Delta($close, 1)))) 

066 Regime_Kurt_Amt_Flow_Switch IfElse(Greater(Kurt($volume, 24), 3), Neg(CsRank(Delta($amt, 3))), Neg(CsRank($returns))) 

067 Alpha101_54_Amt_Divergence Neg(Mul(CsRank(Div(Sub($low, $close), Add(Sub($low, $high), 1e-6))), CsRank(Delta($amt, 6)))) 

068 Skewness_Regime_PV_Divergence_Fixed Neg(IfElse(Less(Skew($returns, 24), -0.5), Neg(Sub(TsRank($close, 12), TsRank($volume, 12))), CsRank(Delta($returns, 3)))) 

069 Regime_Kurt_Volatility_Switch_Fixed Neg(IfElse(Greater(Kurt($returns, 12), 3.0), Neg(CsRank(Std($returns, 6))), CsRank(Delta($returns, 3)))) 

070 Price_Pos_Vol_Interaction Neg(Mul(CsRank(Div(Sub($close, $low), Add(Sub($high, $low), 0.0001))), TsRank(Std($returns, 24), 24))) 

071 Range_Position_Vol_Stability_Interaction_Fixed Mul(CsRank(Div(Sub($high, $close), Add(Sub($high, $low), 1e-6))), TsRank(Std($volume, 12), 12)) 

072 Kurt_Regime_PV_Corr_Switch IfElse(Greater(Kurt($volume, 24), 3.0), Neg(Corr($close, $volume, 12)), Neg(TsRank($returns, 24))) 

073 Amt_Velocity_Regime_Switch_V2 IfElse(Greater($amt, EMA($amt, 12)), Neg(TsRank(Delta($volume, 3), 12)), Neg(CsRank($returns))) 

074 Range_Pos_Vol_Skew_Interaction_Fixed Neg(Mul(CsRank(Div(Sub($close, $low), Add(Sub($high, $low), 1e-6))), TsRank(Skew($volume, 24), 12))) 

075 Amt_Efficiency_Rank_Interaction Neg(Mul(CsRank(Div($returns, $amt)), TsRank(Std($volume, 20), 20))) 

076 Amt_Velocity_Volatility_Reversal Neg(IfElse(Greater(Div($amt, Mean($amt, 24)), 2.0), Neg(Delta($close, 1)), SMA($returns, 6))) 

077 Vol_of_Volume_Price_Reversal Neg(Mul(TsRank(Std(Div($volume, Mean($volume, 24)), 24), 24), TsRank($returns, 12))) 

078 Amt_Efficiency_Kurtosis_Interaction Neg(Mul(CsRank(Div($returns, $amt)), TsRank(Kurt($volume, 20), 20))) 

079 Regime_Vol_Range_Pos_Switch_Fixed IfElse(Greater(Std($returns, 12), EMA(Std($returns, 12), 48)), Neg(Div(Sub($close, $low), Add(Sub($high, $low), 1e-6))), Div(Sub($high, $close), Add(Sub($high, $low), 1e-6))) 

080 Rsquare_Resi_Adaptive IfElse(Greater(Rsquare($close, 24), 0.7), Neg(CsRank(Slope($close, 24))), Neg(CsRank(Resi($close, 12)))) 

081 Regime_Trend_Vol_Corrected IfElse(And(Greater(Rsquare($close, 24), 0.6), Greater(Std($returns, 12), Mean(Std($returns, 12), 48))), Slope($close, 12), Neg(SMA($returns, 6))) 

082 Regime_Logical_Shadow_Slope_Follow IfElse(Or(Greater(Div(Sub($high, Max2($open, $close)), Add(Sub($high, $low), 1e-6)), 0.6), Greater(Div(Sub(Min2($open, $close), $low), Add(Sub($high, $low), 1e-6)), 0.6)), Slope($close, 12), Neg(SMA($returns, 6))) 

083 Rsquare_Stability_Filtered_Momentum IfElse(And(Greater(Rsquare($close, 24), 0.5), Less(Std($returns, 12), Mean(Std($returns, 12), 48))), Slope($close, 12), Neg($returns)) 

084 Volatility_Sign_Agreement_Resi_Switch IfElse(Eq(Sign(Delta(Std($returns, 12), 1)), Sign(Delta($returns, 1))), Neg(Resi($close, 12)), Neg(Slope($close, 24))) 

085 Rsquare_Stability_Resi_Interaction Neg(Mul(CsRank(Rsquare($close, 24)), CsRank(Resi($close, 12)))) 

086 Resi_Acceleration_Sign_Logic_Fixed IfElse(Eq(Sign(Delta(Resi($close, 12), 1)), Sign(Resi($close, 12))), Neg(Resi($close, 6)), Neg(Slope($close, 12))) 

087 Kurt_Regime_Resi_Momentum IfElse(Greater(Kurt($returns, 24), 3.0), Neg(Resi($close, 6)), Neg(Resi($close, 24))) 

088 Amt_Efficiency_Skew_Interaction Neg(Mul(CsRank(Div($returns, Add($amt, 1e-6))), TsRank(Skew($returns, 24), 24))) 

089 Extreme_Divergence_Or_Logic IfElse(Or(Greater($volume, Mean($volume, 24)), Greater(Abs($returns), Std($returns, 24))), Neg(Resi($close, 6)), Neg(Slope($close, 12))) 

(continued) Preprint, , Wang et al. 

ID Name Formula 090 Trend_Reliability_Switch_Logic_V2 IfElse(Greater(Rsquare($close, 24), 0.75), Neg(Slope($close, 12)), Neg(TsRank($returns, 12))) 

091 Amt_Efficiency_TsRank_Kurt_Interaction Neg(Mul(TsRank(Div($returns, Add($amt, 1e-6)), 24), TsRank(Kurt($returns, 24), 24))) 

092 Amt_Efficiency_EMA_Smooth_Rank Neg(CsRank(EMA(Div($returns, Add($amt, 1e-6)), 6))) 

093 Amt_Efficiency_Delta_TsRank Neg(TsRank(Delta(Div($returns, Add($amt, 1e-6)), 3), 24)) 

094 And_HighVol_LowKurt_Switch IfElse(And(Greater(Std($returns, 12), Mean(Std($returns, 12), 60)), Less(Kurt($returns, 24), 2)), Neg(Delta($close, 3)), Neg(TsRank($returns, 24))) 

095 Higher_Moment_Regime_Switch IfElse(Or(Greater(Abs(Skew($returns, 24)), 1.5), Greater(Kurt($returns, 24), 4.0)), Neg(Resi($close, 6)), Neg(TsRank($returns, 24))) 

096 Amt_Efficiency_PV_Corr_Interaction Neg(Mul(TsRank(WMA(Div($returns, Add($amt, 1e-6)), 6), 24), TsRank(Corr($close, $volume, 24), 24))) 

097 Amt_Efficiency_Med_Smooth_Rank Neg(CsRank(Med(Div($returns, Add($amt, 1e-6)), 6))) 

098 Smoothed_Efficiency_Acceleration Neg(CsRank(Delta(WMA(Div($returns, Add($amt, 1e-6)), 6), 3))) 

099 Amt_Efficiency_Double_EMA_Cross Neg(Sub(EMA(Div($returns, Add($amt, 1e-6)), 6), EMA(Div($returns, Add($amt, 1e-6)), 24))) 

100 Residual_of_Residual_Acceleration Neg(Resi(Delta(Resi($close, 24), 3), 12)) 

101 Median_Returns_Switch Neg(IfElse(Greater(Abs(Skew($returns, 24)), 1.0), TsRank(Div(Sub($returns, Med($returns, 24)), Add(Std($returns, 24), 1e-6)), 24), CsRank(Resi($close, 12)))) 

102 High_Log_Resi_Divergence IfElse(Greater(Skew($high, 24), 0), TsRank(Log(Div($high, Add($close, 1e-6))), 24), CsRank(Resi($low, 24))) 

103 Open_Resi_Skew_Hybrid Neg(IfElse(Greater(Skew($open, 24), 0), TsRank(Resi($open, 24), 24), CsRank(SignedPower($returns, 0.5)))) 

104 Median_LogSwap Neg(IfElse(Greater(Med($returns, 24), 0), TsRank(Log(Div($close, Add($open, 1e-6))), 24), CsRank(Sign(Resi($close, 24))))) 

105 Median_Skew_Open Neg(IfElse(Greater(Skew($returns,24),0.4), CsRank(Sign(Resi($open,24))), TsRank(SignedPower($returns,0.6),24))) 

106 Median_Volatility_Extreme Neg(IfElse(Greater(Std($volume,24),Mean(Std($volume,24),48)), CsRank(Div($amt, Add($volume,1e-6))), TsRank(Div($returns, Add(Std($returns,24),1e-6)),24))) 

107 VWAP_Vol_HighVol_Switch IfElse(Greater(Std($returns, 12), Med(Std($returns, 12), 48)), Mul(CsRank(Neg(Div(Sub($close, $vwap), $vwap))), CsRank(Div($volume, EMA($volume, 12)))), Neg(TsRank(Div(Sub($close, $vwap), $vwap), 24))) 

108 VWAP_Vol_Skew_Switch Neg(IfElse(Less(Skew($returns, 24), -0.3), Mul(CsRank(Neg(Div(Sub($close, $vwap), $vwap))), CsRank(Div($volume, EMA($volume, 12)))), TsRank(Div(Sub($close, $vwap), $vwap), 12))) 

109 VWAP_Vol_Rsquare_Switch IfElse(Greater(Rsquare($close, 24), 0.6), Neg(CsRank(Slope($close, 24))), Mul(CsRank(Neg(Div(Sub($close, $vwap), $vwap))), CsRank(Div($volume, EMA($volume, 12))))) 

110 PricePos_Skew_Slope Neg(Mul(CsRank(Div(Sub($close, $low), Add(Sub($high, $low), 1e-6))), CsRank(Mul(Skew($returns, 24), Slope($close, 12)))))