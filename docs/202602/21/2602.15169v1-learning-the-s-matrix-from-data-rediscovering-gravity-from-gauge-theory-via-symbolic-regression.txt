Title: Learning the S-matrix from data: Rediscovering gravity from gauge theory via symbolic regression

URL Source: https://arxiv.org/pdf/2602.15169v1

Published Time: Wed, 18 Feb 2026 01:08:40 GMT

Number of Pages: 13

Markdown Content:
# Learning the S-matrix from data: Rediscovering gravity from gauge theory via symbolic regression 

Nathan Moynihan 1, ∗

> 1

Queen Mary, University of London, Mile End Road, London E1 4NS, UK 

(Dated: February 18, 2026) We demonstrate that modern machine-learning methods can autonomously reconstruct several flagship analytic structures in scattering amplitudes directly from numerical on-shell data. In par-ticular, we show that the Kawai–Lewellen–Tye (KLT) relations can be rediscovered using symbolic regression applied to colour-ordered Yang–Mills amplitudes with Mandelstam invariants as input features. Using standard feature-selection techniques, specifically column-pivoted QR factorisation, we simultaneously recover the Kleiss–Kuijf and Bern–Carrasco–Johansson (BCJ) relations, iden-tifying a minimal basis of partial amplitudes without any group-theoretic input. We obtain the tree-level KLT relations with high numerical accuracy up to five external legs, using only minimal theoretical priors, and we comment on the obstacles to generalising the method to higher multiplic-ity. Our results establish symbolic regression as a practical tool for exploring the analytic structure of the scattering-amplitude landscape, and suggests a general data-driven strategy for uncovering hidden relations in general theories. For comparison, we benchmark this general approach with a recently introduced neural-network based method. 

INTRODUCTION 

Machine learning has become an increasingly useful tool in the natural sciences, and recently there has been a growing interest in applying ML techniques to problems in formal theoretical physics [1]. Outside of physics, the development of deep neural networks (DNNs) has been extremely rapid, and these models have been shown to be capable of performing a variety of tasks, from natural language processing to image recognition [1]. That being said, the progress in applying these models to problems in theoretical physics has been somewhat slower, and this is in part due to the fact that deep neural networks have an interpretability problem : they are excellent at making predictions, but the underlying mechanism by which they do so is often opaque. While it would undoubtedly be useful to have a DNN that could predict the numerical result of some experiment with better accuracy than our existing models, it’s hard to see how this would constitute a “discovery” in the sense that we usually mean, since we would have no better understanding of the underlying physics than we did before [2, 3]. Instead of simply outputting a prediction, symbolic 

machine learning seeks to express learned relationships in terms of physically interpretable expressions. In par-ticular, symbolic regression (SR) is a method for inferring algebraic, human-readable symbolic expressions from nu-merical data, making it a particularly appealing tool for theoretical physics. There have been several notable ap-plications of SR in discovering physical laws of nature, for example AI-Feynman [4] was able to rediscover O(100) well-known physical laws from synthetic data, such as the equations of motion for a double pendulum and the Hamiltonian for a two-body gravitational system. A re-cent application of SR was to use astronomical data from NASA to rediscover Newton’s law of gravitation and all the planets’ masses [5]. While these examples involve re-discovering empirical laws governing real, observable phe-nomena, it is interesting to consider whether SR can be used to probe more formal algebraic structures in quan-tum field theory. Scattering amplitudes are not themselves directly ob-servable, and therefore do not obey “natural” laws in the usual sense. That said, they are highly constrained by general physical principles, for example locality, uni-tarity, gauge invariance, little-group scaling, dimensional analysis etc. This makes them an especially attractive target for interpretable machine learning: exact analytic results are available as benchmarks, the underlying ex-pressions exhibit striking hidden simplicity, and they play a dual role as both formal theoretical objects and build-ing blocks for collider phenomenology and gravitational wave physics. The rich analytic structure of scattering amplitudes has been studied extensively over the past few decades, leading to a number of important discoveries, such as the BCFW recursion relations [6], the CHY for-malism [7], and the double-copy relations between gauge and gravity theories [8]. Very recent work used a large language model to help reveal subtle structure in gauge theory, conjecturing a closed-form expression for single-minus tree-level gluon amplitudes and showing that these can be non-zero in restricted kinematic regions [9]. In this work, we take the Kawai–Lewellen–Tye (KLT) relations as a guiding target, and ask whether symbolic regression (specifically pysr [10]) can rediscover them directly from numerical amplitude data alone. The first step in any data-driven analysis is a completely standard feature-selection pass, and for us this will take the form of column-subset selection via column pivoted QR (CPQR) factorisation [11]. This is an interpretable form of fea-ture selection to prune redundant colour orderings and kinematic invariants, and as we will see the familiar KK 

> arXiv:2602.15169v1 [hep-th] 16 Feb 2026

2and BCJ relations then emerge automatically as the lin-ear redundancies removed by this compression step. We then ask the important questions: what physical priors are necessary, and where do the limitations arise? 

> SETUP: AMPLITUDES, RELATIONS AND DATA Scattering amplitudes of massless particles

We will consider massless boson scattering amplitudes in four spacetime dimensions, where the external states are characterized by their four-momenta pμi and helicities 

hi. The momenta are null vectors, p2 

> i

= 0, and satisfy momentum conservation, Pni=1 pμi = 0. There are several ways to encode the kinematic data of massless particles (spinor-helicity variables, polarization vectors etc), but the precise construction will not concern us here. What will be important are the general properties of amplitudes in four-dimensions. For massless particles, little-group scaling implies that amplitudes transform as 

An(. . . , t iλi, t −1 

> i

˜λi . . . ) = t−2hi 

> i

An(. . . , λ i, ˜λi, . . . ), (1) for each external leg i, under a little group transforma-tion of the form Λ μν pν = pμ. This means that ampli-tudes have a definite helicity weight, along with a defi-nite mass dimension, which in four-dimensions is given by [ An] = 4 − n, where n is the number of external legs. Finally, massless amplitudes typically depend on Man-delstam invariants sij = 2 pi · pj . These invariants are not all independent, however, since they satisfy momen-tum conservation, P  

> j̸=i

sij = 0, for each i. For n-point scattering, there are 12 n(n − 3) independent Mandelstam invariants. There are also dimensionally-dependent iden-tities among the invariants arising from the vanishing of the Gram determinant for n ≥ 6 in four dimensions. We will consider only the dimension-agnostic form in what follows. 

> Scattering Amplitudes in Yang-Mills

At tree-level, scattering amplitudes of pure gluons in Yang-Mills can be computed using Feynman diagrams, BCFW recursion relations [6], or the CHY formalism [7]. What matters for us is that gluon amplitudes can always be decomposed into colour-ordered partial am-plitudes An, which are gauge-invariant functions of the external momenta and helicities, 

An = X

> σ∈Sn/Z n

Tr( T aσ(1) · · · T aσ(n) )An(σ(1) , · · · , σ (n)) ,

(2) where the colour generators obey [T a, T b] = if abc T c, Tr( T aT b) = δab , (3) and the An(σ(1) , · · · , σ (n)) are the partial amplitudes which contain all of the kinematic information. Naively, for n legs there are n! partial amplitudes, however there are a number of important redundancies to con-sider. Firstly, the colour traces satisfy cyclic symme-try, Tr( T a1 T a2 · · · T an ) = Tr( T a2 · · · T an T a1 ), and there-fore the partial amplitudes are cyclically symmetric, 

An(1 , 2, . . . , n ) = An(2 , . . . , n, 1), reducing the number of partials to ( n − 1)!. They also obey reflection symmetry, 

An(1 , 2, . . . , n ) = ( −1) nAn(n, n −1, . . . , 1), further reduc-ing their number to 12 (n − 1)!. The partial amplitudes themselves also obey important kinematic relations, con-straining the number of independent partials even more. Firstly, the Kleiss-Kuijf (KK) relations [12] 

An(1 , α, n, β ) = ( −1) |β| X

> σ∈αβT

An(1 , σ, n ), (4) where α and β are ordered subsets of {2, 3, . . . , n − 1},and βT is the reverse ordering of β, while σ runs over all shuffles of α with βT . This reduces the number of independent colour-ordered amplitudes from 12 (n − 1)! to ( n − 2)!. Secondly, the fundamental Bern-Carrasco-Johansson (BCJ) [8] relations 

> n−1

X

> i=2



> n

X

> j=i+1

s2j

 An(1 , 3, . . . , i, 2, i + 1 , . . . , n − 1, n ) = 0 .

(5) takes the number of independent partials from ( n − 2)! to ( n − 3)! . Importantly, these relations are all linear in the partial amplitudes, and the relations above allow us to fix 3 legs, say legs 1, n − 1 and n, and express all other partial amplitudes in terms of the ( n − 3)! independent amplitudes of the form An(1 , α, n − 1, n ), where α runs over all permutations of {2, 3, . . . , n − 2}.

> Gravity via the KLT Relations

Tree-level scattering amplitudes of pure gravitons in General Relativity can also be computed using Feynman diagrams, BCFW recursion relations [6], or the CHY for-malism [7]. The important properties for us are that graviton amplitudes are fully permutation symmetric, and that they can be expressed in terms of gluon ampli-tudes via the Kawai-Lewellen-Tye (KLT) relations [13]. The KLT relations state that tree-level graviton ampli-tudes can be expressed as a sum over products of colour-ordered gluon amplitudes, weighted by a kinematic kernel 

S[α|β], a pure function of Mandelstam invariants. For n-point tree-amplitudes, the KLT relations are 3

Mn = ( −1) n+1 X

> α,β ∈Sn−3

An(1 , α, n − 1, n )S[α|β]An(1 , β, n, n − 1) , (6) where α and β run over all permutations of the set 

{2, 3, . . . , n − 2}, and the KLT kernel S[α|β] is given by 

S[α|β] = 

> n−2

Y

> k=2

s1,α k +

> k−1

X

> ℓ=2

θ(αk, α ℓ|β) sαk αℓ

!

, (7) where θ(iα, i j |β) = 1 iff iα appears before ij in the or-dering β, and zero otherwise. This is best understood by way of some examples, which will also serve as use-ful benchmarks later on. At 4-point, α = β = 2, and therefore S[2 |2] = s12 , and the KLT relation is simply 

M4 = −s12 A4(1 , 2, 3, 4) A4(1 , 2, 4, 3) . (8) At 5-point, α, β ∈ { 2, 3}, and we have to consider all permutations. The KLT kernel is given by 

S[2 , 3|2, 3] = s12 s13 

S[2 , 3|3, 2] = s12 (s13 + s23 )

S[3 , 2|2, 3] = s13 (s12 + s23 )

S[3 , 2|3, 2] = s13 s12 ,

(9) and so the corresponding gravity amplitude is 

M5 = s12 s13 A5(1 , 2, 3, 4, 5) A5(1 , 2, 3, 5, 4) + s12 (s13 + s23 )A5(1 , 2, 3, 4, 5) A5(1 , 3, 2, 5, 4) + s13 (s12 + s23 )A5(1 , 3, 2, 4, 5) A5(1 , 2, 3, 5, 4) + s13 s12 A5(1 , 3, 2, 4, 5) A5(1 , 3, 2, 5, 4) .

(10) With these examples in mind, we now turn to the task at hand: can we discover the KLT relations from numerical data alone? 

> LINEAR STRUCTURE FROM DATA

Our overall strategy is to treat the KLT relation as the destination, assuming only the 12 (n − 1)! colour-ordered Yang–Mills amplitudes, along with Mandelstam invari-ants, as features. We then compress the raw feature space using off-the-shelf linear feature selection, and then use symbolic regression on our reduced feature set to re-construct the gravity amplitude (which we compute via Hodges formula [14, 15]), with the KK and BCJ relations appearing along the way as the linear relations exposed by the feature reduction step. 

> Data and Kinematics

We work at tree-level, in four dimensions, with mass-less external states satisfying all-outgoing kinematics. Random on-shell kinematics are generated in the centre-of-mass frame by sampling 2 → (n − 2) momenta, satis-fying momentum conservation. We work with exact ra-tional kinematics initially, although for the symbolic re-gression step we convert to 64-bit floating-point numbers, and we impose the on-shell and transversality conditions to within a numerical tolerance of O(10 −16 ). Spinor–helicity variables are constructed from the sam-pled four-vectors, and colour-ordered MHV gluon ampli-tudes are computed using the Parke-Taylor formula [16] for the appropriate orderings, with the MHV gravita-tional targets computed similarly. Since generic spinor-helicity variables in (3 , 1) signature are complex, the am-plitudes are complex-valued. To avoid doubling the fea-ture space by treating real and imaginary parts sepa-rately, we analytically continue to (2 , 2) signature, where spinor products and amplitudes are real. For an n-point tree amplitude of massless gluons in a fixed helicity configuration, we take O1, . . . , ON to de-note a set of N colour orderings. Evaluating each or-dering at M random kinematic points yields an M × N

amplitude data matrix 

A =



A(1) 1 A(2) 1 · · · A(N )1

A(1) 2 A(2) 2 · · · A(N )2

... ... ...

A(1)  

> M

A(2)  

> M

· · · A(N )

> M

 (11) where A(i) 

> k

≡ An(Oi) evaluated at the k-th phase space point. In parallel, we construct an M × P matrix of Mandelstam invariants, 

S =



s(1) 12 s(1) 13 · · · s(1) 

> n−1,n

s(2) 12 s(2) 13 · · · s(2) 

> n−1,n

... ... ...

s(M )12 s(M )13 · · · s(M )

> n−1,n

 , (12) and an M × 1 vector of gravitational targets, 

M =



M1

M2

...

MM

 . (13) For numerical stability we discard phase-space points close to singular kinematics; concretely, we reject any point for which some Mandelstam invariant satisfies 

|sij | < δ , for some small cutoff δ of order O(10 −10 ), say. We also rescale all invariants and amplitudes by some 4values in the generated sample, ˜sij = sij 

S , ˜An = An

A , ˜Mn = Mn

M , (14) where we either take S = A = M = 10 k to be some fixed scale with integer k ≥ 0, or we take S = median( {sij }), 

A = median( {| An|} ) and M = median( {| Mn|} ). This ensures that all features and targets are dimensionless 

O(1) numbers, which again improves numerical stability. The final dataset will eventually be a matrix of random phase-space points, with columns corresponding to the colour-ordered gluon amplitudes, the Mandelstam invari-ants, and the target gravitational amplitude 

X = ( S′ | A′) , y = M (15) where S′ and A′ are the potentially pruned sets of in-variants and amplitudes after feature selection. 

> Structure discovery via CPQR

From a physics perspective, the space of colour-ordered amplitudes and Mandelstam invariants is highly redun-dant. At tree level, this redundancy is encoded in famil-iar relations such as Kleiss–Kuijf (KK) [12] and Bern– Carrasco–Johansson (BCJ) [8] relations among ampli-tudes, as well as momentum-conservation identities for invariants. In this section, however, we deliberately avoid imposing any of these structures a priori . Instead, we will treat amplitudes and invariants as raw numerical features and ask what linear relations can be inferred using stan-dard, interpretable linear-algebraic diagnostics. The mo-tivation here is purely pragmatic: for symbolic regression of the KLT relations, we need a compact, non-redundant set of inputs, and CPQR provides an interpretable way to obtain it. Identifying KK/BCJ is simply a check on what was removed. The guiding observation is simple: if a set of sam-pled features obeys exact linear relations, then the cor-responding data matrix is rank-deficient, up to numeri-cal tolerance, and we should perform a dimensional re-duction. To extract these relations, we will use column-pivoted QR (CPQR) factorisation to select a minimal, physically interpretable subset of original columns that spans the data. 

> Column subset selection and the Kleiss–Kuijf relations

The gold standard in dimensional reduction in machine learning is principal component analysis (PCA), which ef-ficiently reveals low-dimensional structure in a numerical dataset. However, the basis vectors in the set are typi-cally linear combinations of the original features, which doesn’t lend itself to interpretability, for example a PCA mode could mix objects with different mass dimension, say an amplitude added to a Mandelstam variable, which has no sensible physical meaning. Since our goal is inter-pretable structure discovery, we will instead use CPQR factorisation, which selects a subset of existing columns that approximates the full column space to within a cho-sen tolerance. We apply this procedure to the amplitude and invari-ant matrices A and S defined in the previous subsection. Here, rows correspond to random phase-space points and columns correspond to distinct features. In this lan-guage, the Kleiss–Kuijf statement that only ( n − 2)! colour-ordered amplitudes are independent becomes the statement that the columns of A span an approximately (n − 2)!-dimensional subspace of RM .Concretely we compute 

AP = QR , (16) where P is a permutation matrix which performs the piv-ots, Q ∈ RM ×N is a semi-unitary matrix (with orthonor-mal columns), and R is an upper-triangular matrix of the form 

R =

R11 R12 

0 ϵR 22 



=

R11 R12 

0 0



+ O(ϵ), (17) A sharp drop in the diagonal entries of R identifies an ef-fective numerical rank k, in which case the first k columns of AP provide a basis for the column space of A to within our chosen tolerance ϵ. This in turn implies that there are several linear relations among the columns of A.To simply perform symbolic regression, it is not a re-quirement that we know how the columns are related – we only need a reduced set of independent features to feed into our regression algorithm. However, it is inter-esting to see what relations are actually being uncovered by CPQR, and so we will extract from the R matrix by explicitly constructing the nullspace basis N as 

N = P

−R−111 R12 

I



, so that AN ≃ 0. (18) The columns of N therefore give rise to null relations among the sampled features. In algebraic language, such relations are syzygies [17] 1: nontrivial coefficient vectors 

c(sij ) (viewed as polynomials/rational functions of the kinematic invariants) for which a linear combination of candidate features vanishes identically on the kinematic support. In the present subsection the extracted syzygies are degree-0 in the invariants, since they have (approx-imately) constant coefficients, and they reproduce the Kleiss–Kuijf relations [12].  

> 1We thank David Kosower for bringing our attention to this work, and for useful conversations on the topic.

5For example, using O(n3) random phase space points we find that the effective rank of A is k = ( n − 2)!, with a pronounced multi-order-of-magnitude gap in the diagonal of R after the first k pivots. The resulting null relations (syzygies) AN ≃ 0 con-tain the celebrated KK relations [12] up to numerical er-ror. We emphasise that this extraction requires no group-theoretic input: the relations emerge from numerical data and linear algebra alone. Applying the same CPQR analysis to S similarly recovers the expected momentum-conservation identities among Mandelstam invariants. 

> Joint feature-space structure and the Bern–Carrasco–Johansson relations

After pruning A and S using CPQR, the simplest nat-ural input set for symbolic regression is the concatenated matrix 

X = ( S′ | A′) , (19) where S′ and A′ denote the surviving invariant and am-plitude columns. Although this feature set is sufficient in principle to express the gravitational targets, it re-quires the symbolic regression algorithm to discover apriori that amplitudes and Mandelstam invariants must first be multiplied to form objects of the appropriate mass dimension and little-group weight. This is inefficient, as it greatly enlarges the search space. A standard remedy in symbolic regression is to include composite features suggested by our knowledge of the problem. In our case, we use only minimal physical in-formation about the target sector (mass dimension and little-group weights) to build a composite feature library, for example 

{ sij Aα }, { sij skl Aα }, { sij AαAβ }, . . . (20) where Aα denotes a surviving colour-ordered amplitude column. After forming these products, we apply CPQR again to prune the enlarged feature set. Since the number of candidate composites grows rapidly, the precise trunca-tion of the library is ultimately a practical choice. Im-portantly, the subsequent pruning and regression remain fully data-driven; our only theoretical input is the choice to include composites compatible with the target mass dimension and little-group weights. Applying CPQR to the composite feature matrix also provides a natural interpretation in terms of degree-1 syzygies [17]. Concretely, consider the set of features with columns of the form ( sij Aα), arranged into a ma-trix A(1) . A null vector N satisfying 

A(1) N ≃ 0 ⇐⇒ X

> α, ij

cα,ij sij Aα ≃ 0 (21) can be regrouped as 

X

> α

 X 

> ij

cα,ij sij 



Aα ≃ 0. (22) Thus, a constant-coefficient null relation in the composite space corresponds to a relation among amplitudes whose coefficients are linear polynomials in the Mandelstams, i.e. a degree-1 syzygy in the invariants, which of course contain the Bern–Carrasco–Johansson relations [8] di-rectly from numerical data and linear algebra, without imposing amplitude identities by hand. In other words, CPQR automatically discovers that certain linear com-binations of amplitudes, weighted by Mandelstam poly-nomials, vanish identically—precisely the BCJ relations. However, it should be noted that the nullspace basis returned by CPQR is not unique, and the resulting rela-tions are often not presented in the canonical BCJ form, but rather as generic linear combinations of a BCJ ba-sis (and often overcomplete). At low points this can be massaged into the familiar representation, but at higher points it becomes impractical, and it would be interesting to explore algorithmic ways to extract the canonical BCJ basis directly. We can, in principle, extend this to higher-degree syzygies by including higher-order composites, e.g. 

{sij skl Aα} (see [17] where this has been done). However, for purely symbolic regression purposes this tends to re-duce the flexibility of the feature set: the library blows up and is full of near-duplicates, and pruning then leaves you with “pre-multiplied” high-degree features that are less convenient for building simple formulas. 

> SYMBOLIC RECONSTRUCTION

We now turn to learning about amplitudes using sym-bolic regression. Given a target y, the two central de-sign choices are the input feature set xi and the oper-ator/function class over which the search is performed. For tree-level amplitudes, the relevant building blocks are Lorentz scalars (which could be spinor-helicity ob-jects) and rational functions thereof. We therefore con-sider polynomial/rational expression spaces generated by 

{+, −, ×, / }, (23) and in practice often implement division via a unary inv 

operator, x 7 → 1/x , for improved search stability. A crucial practical point is that the symbolic search space grows rapidly with both the number of input fea-tures and the allowed operator set. The CPQR pipeline therefore serves a dual purpose: it is not only a discovery tool for linear amplitude identities, but also a mecha-nism for reducing the combinatorial burden on symbolic regression. Finally, we note that further performance gains can be achieved by modest, physics-motivated feature engineer-6         

> Generate data
> Evaluate amplitudes An(σ), ˜An(˜ σ), and invariants
> {sij }, on random on-shell phase space points. Split into train/test.
> ⇓
> Build feature sets
> Either individual amplitudes A(σa) or bilinears
> Bab =A(σa) ˜A(˜ σb)).
> ⇓
> Feature selection
> Build a feature matrix on the training set and apply CPQR at tolerance εto expose rank structure and prune redundant features.
> ⇓
> Basis selection
> Scan left/right amplitude bases and choose a basis that minimises a simple proxy objective (e.g. validation error / model size / spurious-pole indicators).
> ⇓
> Symbolic regression + validation
> Run SR on the engineered/pruned library to learn the target. Select expressions that generalise on held-out points and simplify to compact analytic form. FIG. 1. Schematic of the pipeline used to rediscover KLT-type relations from numerical samples.

ing. We can use our prior amplitude knowledge, for ex-ample that they have definite mass dimension and little-group weights, and use this to build composite physics-aware inputs with the correct overall scaling for the target sector. For instance, at four points in the MHV sector, [M4] = 0 and the little-group weights are (−2h1, −2h2, −2h3, −2h4) = (+2 , +2 , −2, −2) , (24) so it is natural to prioritise feature combinations that re-spect these constraints. In practice, the most effective strategy balances two competing considerations: the ex-pressivity of the engineered inputs against the complex-ity of the resulting symbolic search space. In practice, the feature set we pick is bounded: beyond bilinear in the gluon amplitudes, any resulting expression will have the wrong little-group scaling, and beyond sn−3 

> ij

in Man-delstam invariants the mass dimension will be incorrect. The full pipeline is summarised in Fig. 1, which we will now apply to the Parke-Taylor and KLT problems in turn. 

> Parke-Taylor Amplitudes

As a warm-up, and as a benchmark en route to the KLT problem, we consider a landmark result in the study of scattering amplitudes: the Parke-Taylor formula [16] for tree-level MHV gluon amplitudes. The Parke-Taylor formula gives a remarkably simple closed-form expres-sion for the n-point MHV amplitude with two negative-helicity gluons (say legs 1 and 2) and ( n − 2) positive-helicity gluons 

An[1 −, 2−, 3+, . . . , n +] = ⟨12 ⟩4

⟨12 ⟩ ⟨ 23 ⟩ · · · ⟨ n1⟩ (25) This should be contrasted with the complexity of the Feynman-diagram expansion, which grows factorially with n. We ask: can symbolic regression rediscover the Parke-Taylor formula directly from numerical data? While this may seem like a simple exercise, it has enough non-trivial aspects to serve as a good test of the method. The Parke-Taylor formula is a rational function of spinor products, and cannot be expressed as a polynomial in Mandelstam invariants. At the same time, it is a rela-tively simple expression, and therefore should be within reach of symbolic regression. It therefore serves as a useful benchmark to set our expectations: it has sev-eral known physical properties that can be used to guide our exploration, and so we will start with minimal priors and gradually add structure to gauge their effect. Our target is the n-point MHV amplitude 

An[1 −, 2−, 3+, . . . , n +], evaluated on M random phase space points, forming an M × 1 target vector Atarget ,perhaps generated by Feynman diagrams, with the full set of spinor brackets as features: 

{⟨ 12 ⟩ , [12] , ⟨13 ⟩ , [13] , · · · , | Atarget }. (26) For n particles, there are  n

> 2

 independent angle brackets and  n

> 2

 independent square brackets, and so the feature space grows quadratically with n. We then run symbolic regression (SR) on this dataset, searching for an expres-sion that fits the target amplitude. For n = 4, SR is able to rediscover the Parke-Taylor formula with minimal dif-ficulty, in O(10 2) seconds, although often in a peculiar form 2. However, as we increase n, the search space grows 

rapidly , and so SR struggles to find the correct expression within a reasonable time-frame — n = 5 requires O(10 4)seconds, for example. The best way to improve this sit-uation is to include additional priors, either to shrink the feature set, including only a subset of spinor-helicity variables for example, or the search space itself, by re-stricting to a particular set of operators/functions. We must be careful here, however, to not make assumptions based on the known Parke-Taylor form itself, since this would defeat the purpose of rediscovery.        

> 2For example
> A4=−[34] 2⟨12 ⟩⟨23 ⟩[23][12] (27)

7At tree-level, we know that amplitudes are polynomi-als in spinor products along with poles corresponding to factorisation channels. We can’t therefore really do bet-ter than {+, ×, inv } without prior knowledge that Parke-Taylor happens to be a single fraction, which would be cheating. We can help the algorithm along by perform-ing some physics-inspired feature engineering, i.e. using known physical properties of the target amplitudes to bias the search space. Firstly, we turn to dimensional analysis, knowing that tree-level amplitudes have dimen-sion [ An] = 4 − n, we can enforce that this is as a con-straint (this is built-in to pySR [10]), taking [ ⟨ij ⟩] = 1 and a dimensionless coupling. Secondly, it is well known that (tree-level) MHV gluon amplitudes in four dimen-sions admit a chiral representation: they can be writ-ten purely in terms of the undotted spinors λi, therefore depending only on angle brackets ⟨ij ⟩, with the parity-conjugate MHV sector depending only on [ ij ]. This holomorphy is natural when Yang–Mills is treated per-turbatively as an expansion around its self-dual sector, and it is manifest in twistor-space and light-cone/MHV-Lagrangian formulations where the relevant MHV ver-tices are explicitly holomorphic [18–24]. Motivated by this known chiral structure of the tree-level MHV sec-tor, we can optionally restrict ourselves to angle brackets only. This reduces the search space without specifying the cyclic denominator structure, resulting in the dataset 

{⟨ 12 ⟩ , ⟨13 ⟩ , . . . | Atarget }, (28) With these considerations alone, we find that SR redis-covers the Parke-Taylor formula at n = 4 , 5, 6 in a rea-sonable time, i.e. under O(10 3) seconds, to O(10 −15 )maximum relative error on a held out test set. This is an orders-of-magnitude speed up over the full feature set and unrestricted search space. Stronger priors, such as re-stricting to a single fraction or only using adjacent spinor brackets, speed the search up further, but are not phys-ically well-motivated and are unnecessary up to n = 6 anyway. 

> Learning The KLT Relations

Having benchmarked the method on the Parke-Taylor formula, we now turn to our main target: the KLT rela-tions between tree-level gluon and graviton amplitudes, a clean target for bootstrap attempts [25]. Our core as-sumption is that the graviton amplitude can be expressed as a function of gluon amplitudes and Mandelstam invari-ants, i.e. Mn = f (An, s ij ), with no assumption about the particular structure beyond what we can glean from physics. We continue to work in the MHV sector; at four and five points this captures all helicity configurations, while NMHV first appears at n ≥ 6. Our first task will be to construct a feature set suit-able for symbolic regression. The most straightforward choice would be to simply take all colour-ordered gluon amplitudes An(Oi) and all Mandelstam invariants sij as features. However, this leads to a combinatorial explo-sion in the size of the feature space, since the number of colour-ordered amplitudes grows as ( n − 1)!. That being said, we have already seen that our naive features are highly redundant, and so we can use the CPQR pipeline to prune both sets to a basis of independent features, and so we will end up with ( n − 3)! independent amplitudes and n(n − 3) /2 Mandelstams after CPQR, which should be manageable at least for small n. Again using the poly-nomial/rational function space generated by {+, ×, / }, it is obvious from the last section that the size of this fea-ture space is a problem for symbolic regression, so we again use physics-motivated feature engineering to build composite features with the correct overall mass dimen-sion and little-group scaling for the gravitational target. Firstly, since Mandelstams carry little-group weight zero, it must be the case that graviton amplitudes are bilinear in gluon amplitudes to get the correct little-group scaling. In D = 4, scattering amplitudes have mass dimension [An] = 4 − n = [ κn−2Mn], and [ κ] = −1. The stripped graviton amplitude has mass dimension [ Mn] = 2, and so the kinematic function multiplying An ˜An must have dimension 2( n − 3), i.e. be a homogeneous polynomial of degree n − 3 in Mandelstams. In other words, the amplitude must be of the form 

Mn = X

> a,b

fab (sn−3 

> ij

)An(σa) ˜An(˜ σb). (29) The choice of orderings σa and ˜ σb has a significant effect on the complexity of the resulting expression, and espe-cially the function fab (sn−3 

> ij

). Colour-ordered gluon am-plitudes are not manifestly Bose-symmetric, because the colour-ordering selects planar singularity channels only, having poles only in adjacent momenta. For example, the four-point amplitude A4(1 , 2, 3, 4) has simple poles in 1 /s 12 and 1 /s 23 , but not in 1 /s 13 , so symmetry under 1 ↔ 3 exchange is not manifest. Graviton amplitudes, on the other hand, must be fully Bose-symmetric, since gravitons are colourless and indistinguishable, and so we should expect graviton amplitudes to have poles in all channels, not just adjacent ones. Importantly, if the pair of chosen orderings σa and ˜ σb do not together cover all possible channels, then the functions fab (sn−3 

> ij

) must in-troduce additional poles in Mandelstam invariants to en-sure that the full gravitational amplitude is permutation-invariant. It should be noted, however, that even if the pair of orderings do cover all channels, there is no guar-antee that fab (sn−3 

> ij

) will be free of spurious poles, since they can still appear provided they cancel in the full ex-pression. This is easy to see with an example at four-point. Choose the orderings σ = (1 , 2, 3, 4) and ˜ σ = (1 , 2, 4, 3), 8and consider the ansatz 

M4 = f (sn−3 

> ij

)A4(1 , 2, 3, 4) ˜A4(1 , 2, 4, 3) , (30) where f (sij ) is taken, for the moment, to be a pole-free polynomial function. This choice is at least compatible with locality and factorisation: A4(1 , 2, 3, 4) has poles in 1/s 12 and 1 /s 23 , while A4(1 , 2, 4, 3) has poles in 1 /s 12 and 1/s 24 . Their product therefore has simple poles in 1 /s 12 ,1/s 23 and 1 /s 24 , covering all possible channels at four-point, and eliminating the need for f (sij ) to introduce any additional poles. However, the absence of a need for poles does not mean poles cannot be shifted into f (sij )by changing the partial-amplitude basis. Indeed, using the four-point BCJ relation, we can write 

M4 = X

> a,b

fab (sn−3 

> ij

)s23 

s13 

A4(1 , 2, 3, 4) ˜A4(1 , 2, 3, 4) = X

> a,b

f ′

> ab

(sn−3 

> ij

)A4(1 , 2, 3, 4) ˜A4(1 , 2, 3, 4) .

(31) All of this raises the question: which orderings should we choose when building our feature set? There is no obvi-ous physical reason to choose one particular ordering over another, since they are all related by amplitude relations. We know from the CPQR/BCJ that there are ( n − 3)! independent amplitudes at n-point, so many equivalent orderings exist. Since our goal is to rediscover compact analytic expressions, we would like the chosen orderings to bias the search toward minimal complexity, ideally avoiding spurious poles in fab (sn−3 

> ij

) if possible. In partic-ular, using the same ordering in both copies tends to force 

f (sij ) to introduce poles to reproduce missing channels, whereas using distinct orderings can distribute the phys-ical poles across the two gauge-theory factors and permit simpler coefficient functions. Unfortunately, there is no physical principle that tells us which orderings are best in this regard, and so we must again let the data guide us. Our strategy in this case is to use a simple, computa-tionally cheap decision-tree model to guide our choice of orderings. We construct candidate feature sets from dif-ferent left-right ordering choices and use a fast decision-tree-based model to test how well each feature set ap-proximates the gravitational target, ranking candidates by mean-squared error (MSE). If a particular ordering choice allows the target to be fit accurately using only a few features, this is strong evidence that the underlying analytic relation is comparatively simple in that basis, and so we can then use that ordering choice to build our feature set for symbolic regression, i.e. we build bilinears of the form 

Bab = An(σa) ˜An(˜ σb), (32) together with an appropriate set of Mandelstam invari-ants {sij }, chosen in the same way. 

> Results and Scaling

We now have a pipeline for rediscovering the KLT re-lations, and we can apply it at four, five and six points to see how it performs. Success is counted as the rediscov-ery of a KLT-type relation within a fixed compute budget (we take 8 hours in the runs below), with success defined as matching the gravitational target to within numer-ical tolerance on a held-out test set (within O(10 −16 ), and ideally producing a compact expression in terms of gluon amplitudes and Mandelstam invariants. The grav-itational target is generated independently using Hodges’ formula [14, 15], so as to avoid biasing the search toward the KLT form. At four points the problem is essentially one-dimensional from the BCJ point of view: there is only (n − 3)! = 1 independent colour-ordered amplitude per copy, so the symbolic regression task is relatively straightforward. The pipeline rediscovers the expected relation with minimal difficulty, typically finding several equivalent expressions that agree within numerical toler-ance, e.g. 

M4 = −s12 A4(1 , 2, 3, 4) ˜A4(1 , 2, 4, 3) . (33) (As discussed above, different ordering choices can shift kinematic factors between the amplitudes and the coef-ficient function, so the same M4 can look polynomial or rational depending on basis.) At five points the situation is more interesting, since there are now multiple BCJ-independent orderings in each copy. Using the tree-based ordering-selection proce-dure outlined above, discovery within the allotted time-frame is frequently achieved, but the time-to-solution is highly sensitive to the particular left-right basis chosen. In some cases the correct relation is recovered in O(10 3)seconds, while in others it takes O(10 4) seconds, and for many basis choices the algorithm fails to converge within the 8-hour limit. Empirically, pairs of bases whose pole structure covers the physical factorization channels tend to yield lower MSE in the decision-tree analysis step, and then tend to enjoy substantially faster symbolic discov-ery. Conversely, “unfortunate” bases often lead to more complicated rational kernels with spurious poles and del-icate cancellations, which dramatically slows the search, often failing in the allotted time. For example, one partic-ularly simple such attempt (reproduced using seed 1096) yields the expression 

M5 = ( s35 + s23 − s14 )s13 A5(1 , 2, 5, 4, 3) A5(1 , 4, 2, 5, 3) 

− s15 s23 A5(1 , 2, 3, 4, 5) A5(1 , 4, 2, 3, 5) (34) in under a minute, which evaluates on a held-out test set with max error O(10 −16 ). At six points the problem becomes distinctly more challenging. The BCJ basis size grows to ( n − 3)! = 6, 9so even before introducing any Mandelstam dependence there are 36 bilinears A(σa) ˜A(˜ σb) available. Dimensional analysis suggests that the corresponding kernel should involve a homogeneous polynomial of degree n − 3 = 3 in Mandelstams (mass dimension 6). Even if we restrict to a minimal independent set of nine invariants, the num-ber of plausible composite features grows rapidly, and the resulting search space becomes prohibitively large. This is a simple result of combinatorics: the number of pos-sible expressions already has Catalan-number growth in the expression size (counting distinct binary-trees), and once we also account for the allowed operators and num-ber of features, the total number of candidate expressions blows up extremely fast. For an expression of complexity 

m with K features and 4 operators {+, −, ∗, / }, a back of the envelope estimate of the size of the search space is [26, 27] Cat m4mKm+1 ∼ 16 m

m3/2√π Km+1 . (35) This means that even a modest increase in the number of features K can lead to an explosion in the search space, which is then further exacerbated by the fact that many of the features are strongly correlated, and that the 6pt KLT relation could have high complexity, on the order of m ∼ 500 depending on the feature set used and how expanded the expression is. There are several physically motivated biases we could introduce to try and tame this explosion. For exam-ple, we could use the 6pt syzygy relations discovered in [17, 28] to reduce the number of independent ampli-tudes (introducing spurious poles into f (sij ), as discussed above), and we could also further restrict the space of Mandelstams by using the non-linear dimensionally-dependent identities in D = 4. However, in practice this would still leave the search space far too large, and still we would find that regression fails to converge within the allotted time, even with aggressive feature engineering and basis selection. Concretely, the tree-based scan can still identify “better” bases according to our proxy objec-tives, but the subsequent symbolic regression step does not reliably rediscover the six-point KLT relation within the compute budget. In short, the bottleneck at six points and beyond is a genuine combinatorial explosion in the effective feature space, made worse by the strong correlations among in-variants and by basis choices that push rational structure (and spurious cancellations) into the learned function f .Put slightly differently: at five points we can often “get away with” a basis in which the kernel looks like a short polynomial in a small set of invariants, while at six points many perfectly valid bases make the same object look like a delicate rational function whose simplicity is only vis-ible after cancellations that SR has no reason to guess early. Further improvements are therefore necessary to make progress at six points and beyond. 

> COMPARISON WITH NEURAL NETWORKS

As we have seen, symbolic regression is a powerful tool for discovering analytic relations in scattering am-plitudes, especially when the answer is simple . In this sense, there is an obvious use-case for symbolic regres-sion as a simplification tool for amplitudes: given an un-wieldy expression for some amplitude, we can attempt to discover an equivalent analytic expression with a much lower complexity score. However, there are several other machine learning paradigms on the market [29–32], and it is natural to ask how these might perform at the same task. Thankfully, there has been a recent attempt to do just this with transformer based neural networks, where a network is trained on pairs of equivalent expressions, one complex and one simple, with the goal of inferring simple expressions from complex ones [33]. There is an immediate conceptual distinction between the two approaches. The transformer setup is a genuinely 

symbolic-to-symbolic map: the input is an explicit expres-sion (represented as a token sequence) and the output is another explicit expression, with the network implicitly learning which algebraic identities are useful and how to apply them. Symbolic regression, by contrast, is most naturally a numeric-to-symbolic procedure: it is given numerical evaluations of a target function on phase space and attempts to infer a closed-form expression that repro-duces those values. For amplitudes this difference is not cosmetic. The neural network is, in principle, sensitive to the detailed presentation of the expression (how it is bracketed, which sub-expressions appear, etc.), whereas symbolic regression only “sees” the function defined by that expression through its numerical values. This makes the symbolic-regression problem closer in spirit to redis-covering an amplitude from something akin to a measure-ment (or a black-box evaluation), rather than performing algebraic rewriting in the usual sense. To compare the two paradigms on the same phys-ical target, we focus on the benchmark highlighted in [33]: the five-point amplitude for three scalars and two same-helicity gravitons. In that work, a sequential transformer-based network, trained on complex-simple pairs, reduces a 298-term spinor-helicity expression to a compact two-term result (their eq. (4.7)), 

M (ϕϕϕh +h+) = ⟨12 ⟩⟨ 13 ⟩⟨ 23 ⟩⟨24 ⟩⟨ 25 ⟩⟨ 45 ⟩

 [14][35] 

⟨14 ⟩⟨ 35 ⟩ − [15][34] 

⟨15 ⟩⟨ 34 ⟩



.

(36) From the point of view of our pipeline, this is an ideal test case: the “raw” expression is extremely long, while the simplified answer consists of only two rational mono-mials. 10 

> Regression setup and feature design

Our symbolic-regression comparison is deliberately set up so that the algorithm does not receive the simpli-fied form in eq. (36). Instead, we numerically evaluate the full 298-term expression on a set of randomly gener-ated phase-space points, and ask symbolic regression to infer a compact analytic formula consistent with those evaluations. Concretely, we proceed as before and gener-ate exact rational on-shell five-point kinematics in split signature (2 , 2) using spinors, so that ⟨ij ⟩ and [ ij ] are independent real numbers. For each phase-space point we compute the full set of angle and square brackets and store these as features, together with the numerical value of the target 298-term amplitude. The data-driven ap-proach has an important advantage over the neural net-work approach: any candidate expression returned by symbolic regression is immediately verified on held-out data, and since the objects involved are rational func-tions of spinor brackets, agreement on a sufficiently large number of generic phase-space points makes an incor-rect expression incredibly unlikely. The neural network, by contrast, returns a conjectured simplified expression whose correctness must still be validated by numerical testing to rule out hallucinations, which are a common issue in neural symbolic manipulation. A naive symbolic-regression attempt would feed all brackets {⟨ ij ⟩, [ij ]} directly into a generic rational func-tion search space generated by {+, −, ×, / }. As we have seen, however, this is an unnecessarily hard search prob-lem, since the search space is very large. Furthermore, if this is all we feed in, then we are expecting the algorithm to simultaneously discover both the correct little-group weights and the correct overall mass dimension, all the while navigating the vast number of algebraically equiv-alent expressions. As before, then, we will use our knowledge of physics to inject mild priors to constrain the problem, enforc-ing little-group covariance and dimensional consistency. For the ϕϕϕh +h+ amplitude, the scalars carry zero little-group weight while each positive-helicity graviton carries weight −2hi = −4. This means that any valid term in the amplitude must have net little-group exponents (0 , 0, 0, −4, −4) (37) across legs (1 , 2, 3, 4, 5), where each ⟨ij ⟩ contributes +1 to legs i and j, and each [ ij ] contributes −1 to legs i

and j. Since the target is a rational function of brackets, we can enumerate candidate rational monomials built from products of ⟨ij ⟩ and [ ij ] in numerator and denom-inator that satisfy these little-group constraints (subject to modest caps on the total number of bracket factors). Evaluated on data, these monomials form a design ma-trix Φ. We then proceed as before: we apply CPQR to remove redundant features, leaving an independent (and typically better conditioned) feature set. There is no reason a priori to assume that the simplest expression is linear in these features alone: as we have seen, spurious poles can be introduced in intermediate terms assuming they cancel in the final expression, for example. That being said, dimensional consistency and little-group covariance only allows for a very restricted set of terms, and at this order we can simply enumerate all possible permitted terms and add them to the feature set. Operationally, this allows us to restrict the symbolic-regression operator set to {+, −, ×} , effectively making it linear regression, since division is already built into the monomials, substantially shrinking the search space while still containing the true answer. With this setup, symbolic regression recovers eq. (36), or an alternative simple expression equal by schouten or momentum conservation, in O(10 2) seconds, matching the 298-term target to O(10 −16 ) maximum relative error on a held-out test set. The rediscovered expression is algebraically equivalent to the two-term form reported in [33], confirming that SR can simplify a lengthy spinor-helicity expression to its simplest known form without ever seeing that form as input, and without extensive training or a large corpus of examples. 

> Similarities and differences between neural and symbolic regression

This benchmark usefully highlights the different inher-ent biases of the neural and symbolic approaches. On the neural side, the transformer is explicitly learn-ing an algorithm for simplification : it must internalize patterns of identity application (Schouten, momentum conservation, partial fractioning, etc.) and learn how to rewrite a given expression into a shorter but equivalent one. This is powerful when one wants an automated “algebra engine” that can act directly on an input ex-pression, and it naturally supports an iterative approach, i.e. it can simplify long expressions by breaking it into smaller expressions and simplifying those first. On the symbolic-regression side, we are not learning a rewrite algorithm at all. Instead, we exploit the fact that amplitudes are highly constrained functions, and we build a feature basis that respects those constraints from the outset. In cases like eq. (36), where the final answer is extremely simple in a physically natural basis, symbolic regression is well-matched to the task: it does not need to discover how to simplify the 298-term input, only what 

function that input represents. The limitations of both approaches are also comple-mentary. Neural simplification can, in principle, improve with larger and richer training corpora, and can act di-rectly on complicated symbolic inputs, however it can often hallucinate: it returns a conjectured expression whose correctness must still be validated (e.g. by numer-ical testing or symbolic checking). Symbolic regression 11 produces an explicit analytic candidate which is imme-diately verifiable on held-out data, but its performance depends sharply on feature construction: without strong physics-guided bases, the search space quickly becomes a combinatorial nightmare, and the method deteriorates as the number of independent structures grows. This is the same obstruction we saw already in the KLT rediscovery problem at higher multiplicity, and even when the true answer is “simple”, it may only be simple in a basis that is hard to find without additional structure. Taken together, these observations suggest a prag-matic division of labour. Neural models are attractive as general-purpose symbolic manipulators which may dis-cover useful intermediate rearrangements, while symbolic regression is most effective as an equation-discovery and 

final compression tool once a physically meaningful fea-ture set has been identified. With this in mind, it would be interesting to explore a hybrid approach, where a neu-ral simplification step is used to preprocess the input ex-pression into a more compact form, not necessarily the 

most compact form, which is then fed into symbolic re-gression for final compression. For even higher efficiency, the neural step could be used to suggest promising fea-tures for the symbolic regression stage, for example by identifying which combinations of brackets appear most frequently in the simplified expression, or by suggesting which orderings of gluon amplitudes are most likely to yield simple KLT relations. This could potentially com-bine the best of both worlds: the neural network can learn to apply algebraic identities to reduce the complex-ity of the expression, while symbolic regression can then identify a compact analytic form that matches the sim-plified numerical data. 

> CONCLUSION AND OUTLOOK

In this letter, we have presented a data-driven pipeline that rediscovers several textbook flagship analytic struc-tures in scattering amplitudes: the Kleiss–Kuijf and BCJ relations, the Parke–Taylor formula, and the KLT double-copy, all using only numerical on-shell data, stan-dard linear algebra (CPQR), and symbolic regression, with a few well-motivated theoretical priors. The method succeeds at four and five points, where the BCJ basis is small and the KLT kernel is a short polynomial, but en-counters a genuine combinatorial barrier at six points and beyond. A comparison with the neural-network simpli-fication approach of [33] shows that the two approaches are complementary: symbolic regression excels at discov-ering compact expressions from numerical evaluations, while neural rewriting excels at simplifying known sym-bolic inputs. The most pernicious obstacle we have come across in this work is the combinatorial explosion at higher multi-plicity, and perhaps the most natural way to circumvent it is by adopting a factorisation bootstrap: rather than learning the full amplitude in one shot, we could probe the target at phase-space points approaching a chosen channel sI → 0, extract the residue of the scaled quan-tity sI M via SR, and assemble a candidate pole part 

Mpoles . This would leave some remaining contact terms 

Mcontact = M − M poles , but since these are free of phys-ical poles they should be substantially simpler to learn by SR. Initial tests of this idea are promising, but ap-proaching poles cleanly requires care to avoid collinear regions and spurious-pole cancellations, and we leave this idea to future work. Should this approach prove suc-cessful, a natural next step would be to explore the ap-plication of these methods to more complex amplitude sectors where less analytic structure is known, such as at NMHV at six points and above. It would also be in-teresting to explore a hybrid neural–symbolic pipeline, in which a transformer first reduces the complexity of the input expression, perhaps to several smaller expres-sions, which it then hands-off to symbolic regression per-forms the final compression, combining the complemen-tary strengths discussed in the previous section, perhaps undergoing several passes. On the physics side, extension to loop level mean the appearance of transcendental func-tions (logarithms, polylogarithms), which would require enlarging the operator library, although the same CPQR-based feature selection and physics-informed dimensional constraints apply in principle. A simpler target, staying at tree-level, might be to try and learn the string the-ory KLT relations, a recent target for bootstrapping [34], which would also involve enlarging the operator library to include trigonometric functions, but would still be a rational function of the kinematic invariants times the bilinears, and ought to be simple enough to learn. While we have focussed on a particular scattering-amplitudes formulation of the double copy, symbolic regression has a long track record in learning equations of motion, largely because trajectory data providing direct access to the un-derlying dynamics are often easy to obtain. Several non-perturbative formulations of the double copy can, in fact, be expressed at the level of equations of motion [35–42], and it would be interesting to investigate whether sym-bolic regression could help explore these formulations, or even suggest new ones. More generally, we believe that the combination of symbolic regression and neural network-based simplifica-tion has the potential to be a powerful tool for exploring the rich structure of scattering amplitudes and uncov-ering new insights into the underlying physics, and we look forward to investigating this in more detail in fu-ture work. 12 

ACKNOWLEDGMENTS 

It is a pleasure to thank Kymani Armstrong-Williams, David Berman, Miles Cranmer, Ed Hirst, David Kosower, Costis Papageorgakis, and Chris White for use-ful conversations and feedback. This work was sup-ported by the Science and Technology Facilities Coun-cil (STFC) Consolidated Grant ST/X00063X/1 “Ampli-tudes, Strings & Duality”. No new data were gener-ated or analysed during this study. Some of the code used to generate the results in this paper is available at https://github.com/nmoynihan/amplitudes-sr.  

> ∗

n.moynihan@qmul.ac.uk [1] Y.-H. He, Machine Learning in Pure Mathematics and Theoretical Physics 

(WORLD SCIENTIFIC (EUROPE), 2023) https://www.worldscientific.com/doi/pdf/10.1142/q0404. [2] C. Rudin, Stop explaining black box machine learn-ing models for high stakes decisions and use inter-pretable models instead, Nature Machine Intelligence 1,206 (2019). [3] N. Makke and S. Chawla, A Perspective on Symbolic Machine Learning in Physical Sciences, in 38th confer-ence on Neural Information Processing Systems (2025) arXiv:2502.17993 [cs.LG]. [4] S.-M. Udrescu and M. Tegmark, AI Feynman: a Physics-Inspired Method for Symbolic Regression, Sci. Adv. 6,eaay2631 (2020), arXiv:1905.11481 [physics.comp-ph]. [5] P. Lemos, N. Jeffrey, M. Cranmer, S. Ho, and P. Battaglia, Rediscovering orbital mechanics with ma-chine learning, Mach. Learn. Sci. Tech. 4, 045002 (2023), arXiv:2202.02306 [astro-ph.EP]. [6] R. Britto, F. Cachazo, B. Feng, and E. Witten, Di-rect proof of tree-level recursion relation in Yang-Mills theory, Phys. Rev. Lett. 94 , 181602 (2005), arXiv:hep-th/0501052. [7] F. Cachazo, S. He, and E. Y. Yuan, Scattering of Massless Particles in Arbitrary Dimensions, Phys. Rev. Lett. 113 ,171601 (2014), arXiv:1307.2199 [hep-th]. [8] Z. Bern, J. J. M. Carrasco, and H. Johansson, New Re-lations for Gauge-Theory Amplitudes, Phys. Rev. D 78 ,085011 (2008), arXiv:0805.3993 [hep-ph]. [9] A. Guevara, A. Lupsasca, D. Skinner, A. Stro-minger, and K. Weil, Single-minus gluon tree ampli-tudes are nonzero 10.48550/arXiv.2602.12176 (2026), arXiv:2602.12176 [hep-th]. [10] M. Cranmer, Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl, arXiv e-prints , arXiv:2305.01582 (2023), eprint: 2305.01582. [11] T. F. Chan, Rank revealing qr factorizations, Linear Al-gebra and its Applications 88-89 , 67 (1987). [12] R. Kleiss and H. Kuijf, Multi - Gluon Cross-sections and Five Jet Production at Hadron Colliders, Nucl. Phys. B 

312 , 616 (1989). [13] H. Kawai, D. C. Lewellen, and S. H. H. Tye, A Relation Between Tree Amplitudes of Closed and Open Strings, Nucl. Phys. B 269 , 1 (1986). [14] A. Hodges, New expressions for gravitational scattering amplitudes, JHEP 07 , 075, arXiv:1108.2227 [hep-th]. [15] A. Hodges, A simple formula for gravitational MHV am-plitudes, (2012), arXiv:1204.1930 [hep-th]. [16] S. J. Parke and T. R. Taylor, An Amplitude for n Gluon Scattering, Phys. Rev. Lett. 56 , 2459 (1986). [17] D. A. Kosower and S. P¨ ogel, Serendipitous syzygies of scattering amplitudes, Phys. Rev. D 112 , 076030 (2025), arXiv:2505.14857 [hep-th]. [18] G. Chalmers and W. Siegel, The Selfdual sector of QCD amplitudes, Phys. Rev. D 54 , 7628 (1996), arXiv:hep-th/9606061. [19] E. Witten, Perturbative gauge theory as a string theory in twistor space, Commun. Math. Phys. 252 , 189 (2004), arXiv:hep-th/0312171. [20] L. J. Mason, Twistor actions for non-self-dual fields: A Derivation of twistor-string theory, JHEP 10 , 009, arXiv:hep-th/0507269. [21] F. Cachazo, P. Svrcek, and E. Witten, MHV vertices and tree amplitudes in gauge theory, JHEP 09 , 006, arXiv:hep-th/0403047. [22] P. Mansfield, The Lagrangian origin of MHV rules, JHEP 

03 , 037, arXiv:hep-th/0511264. [23] J. H. Ettle and T. R. Morris, Structure of the MHV-rules Lagrangian, JHEP 08 , 003, arXiv:hep-th/0605121. [24] V. P. Nair, A Current Algebra for Some Gauge Theory Amplitudes, Phys. Lett. B 214 , 215 (1988). [25] H.-H. Chi, H. Elvang, A. Herderschee, C. R. T. Jones, and S. Paranjape, Generalizations of the double-copy: the KLT bootstrap, JHEP 03 , 077, arXiv:2106.12600 [hep-th]. [26] W. B. Langdon, Genetic programming convergence, Ge-netic Programming and Evolvable Machines 23 , 71 (2022). [27] T. Aytekin, E. E. Korkmaz, and H. A. G¨ uvenir, An ap-plication of genetic programming to the 4-op problem us-ing map-trees, in Progress in Evolutionary Computation ,edited by X. Yao (Springer Berlin Heidelberg, Berlin, Heidelberg, 1995) pp. 28–40. [28] N. E. J. Bjerrum-Bohr, P. H. Damgaard, H. Johansson, and T. Sondergaard, Monodromy–like Relations for Fi-nite Loop Amplitudes, JHEP 05 , 039, arXiv:1103.6190 [hep-th]. [29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention Is All You Need, in 31st International Confer-ence on Neural Information Processing Systems (2017) arXiv:1706.03762 [cs.CL]. [30] S. Hochreiter and J. Schmidhuber, Long short-term mem-ory, Neural Computation 9, 1735 (1997). [31] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, Neural message passing for quantum chem-istry, in Proceedings of the 34th International Conference on Machine Learning (2017) arXiv:1704.01212 [cs.LG]. [32] T. M. Huy and E. Hirst, Versor: A Geometric Sequence Architecture, (2026), arXiv:2602.10195 [cs.LG]. [33] C. Cheung, A. Dersy, and M. D. Schwartz, Learning the simplicity of scattering amplitudes, SciPost Phys. 18 , 040 (2025), arXiv:2408.04720 [hep-th]. [34] A. S.-K. Chen, H. Elvang, and A. Herderschee, Boot-strapping the String Kawai-Lewellen-Tye Kernel, Phys. Rev. Lett. 131 , 031602 (2023), arXiv:2302.04895 [hep-th]. 13 

[35] N. Moynihan, M. L. R. Ashby, and C. D. White, The double copy as a doppelg¨ anger, (2025), arXiv:2509.22350 [hep-th]. [36] C. Cheung, J. Mangan, J. Parra-Martinez, and N. Shah, Non-perturbative Double Copy in Flatland, Phys. Rev. Lett. 129 , 221602 (2022), arXiv:2204.07130 [hep-th]. [37] N. Moynihan, Massive covariant colour-kinematics in 3D, JHEP 05 , 310, arXiv:2110.02209 [hep-th]. [38] C. Cheung and J. Mangan, Covariant color-kinematics duality, JHEP 11 , 069, arXiv:2108.02276 [hep-th]. [39] W. T. Emond, N. Moynihan, and L. Wei, Quantiza-tion conditions and the double copy, JHEP 09 , 108, arXiv:2109.11531 [hep-th]. [40] N. Moynihan, Scattering Amplitudes and the Double Copy in Topologically Massive Theories, JHEP 12 , 163, arXiv:2006.15957 [hep-th]. [41] K. Armstrong-Williams, C. D. White, and S. Wikeley, Non-perturbative aspects of the self-dual double copy, JHEP 08 , 160, arXiv:2205.02136 [hep-th]. [42] J. J. M. Carrasco, Y. Chen, N. H. Pavao, and A. Seifi, Nonperturbative double copy: worldline instantons, color thermality, and backreaction, (2026), arXiv:2601.17884 [hep-th].