Title: Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

URL Source: https://arxiv.org/pdf/2602.04492v1

Published Time: Thu, 05 Feb 2026 02:04:04 GMT

Number of Pages: 35

Markdown Content:
> 2026-02-05

# Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico 

# Zebrafish 

Jan-Matthis Lueckmann 1, Viren Jain 1 and MichaÅ‚ Januszewski 1

> 1Google Research

Constructing mechanistic models of neural circuits is a fundamental goal of neuroscience, yet verifying such models is limited by the lack of ground truth. To rigorously test model discovery, we establish an in silico testbed using neuromechanical simulations of a larval zebrafish as a transparent ground truth. We find that LLM-based tree search autonomously discovers predictive models that significantly outperform established forecasting baselines. Conditioning on sensory drive is necessary but not sufficient for faithful system identification, as models exploit statistical shortcuts. Structural priors prove essential for enabling robust out-of-distribution generalization and recovery of interpretable mechanistic models. Our insights provide guidance for modeling real-world neural recordings and offer a broader template for AI-driven scientific discovery. 

1. Introduction 

A fundamental goal in neuroscience is to understand how neural circuits process information and generate behavior. The ability to predict neural activity is a key test towards such understanding. Data-driven benchmarks can accelerate progress by providing standardized datasets and evaluation metrics (Schrimpf et al., 2020; Pei et al., 2021; Turishcheva et al., 2023; Lueckmann et al., 2025). However, benchmarks relying on real experimental data face inherent limitations and verification challenges. For instance, limitations include the high cost and technical difficulty of acquiring large-scale, high-resolution neural recordings, the presence of noise and unobserved variables, and restrictions in systematically manipulating neural circuits or environments. Real-world experiments that involve imaging of neural activity are severely restricted in duration and throughput (e.g., due to phototoxicity, molecular reporters, and imaging limitations), and subject to artifacts and preprocessing challenges (e.g., motion, striping, imperfect alignment and segmentation). More fundamentally, much about the underlying biological systems used in such benchmarks is as yet unknown (e.g., the detailed connectivity of underlying circuits, synaptic release probabilities, neuromodulatory interactions). Therefore, since the ground-truth data-generating process is unknown, models are ranked relative to each other rather than compared against an absolute reference. It is thus unclear how far models are from the true mechanisms of computation, or whether they have identified statistical shortcuts. This creates a verification gap: without a known ground truth, it is hard to rigorously evaluate models and model discovery strategies. Neuromechanical simulations provide a powerful avenue to complement real-data benchmarks: Embodied simulations can generate vast amounts of perfectly annotated data where the underlying â€œground truthâ€ of the neural system is known and controllable. They allow for systematic manipulation of physiological and circuit features that are difficult or impossible to achieve in biological experimentsâ€” offering full observability and control over factors such as dataset dimensionality, length, and noise levels, along with the ability to perturb any part of their neural circuitry. simZFish (Liu et al., 2025), an open-source neuromechanical simulation of the larval zebrafish, is one such platform that realistically models the fishâ€™s body, its physical interactions with the environment, visual inputs, and experimentally    

> Corresponding author(s): janmatthis@google.com, mjanusz@google.com
> Â©2026 Google. All rights reserved
> arXiv:2602.04492v1 [q-bio.NC] 4 Feb 2026 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

Figure 1 | Verifiable discovery of neural mechanisms in an in silico testbed. a. The simulation environment consists of a neuromechanical model subject to fluid dynamics, responding to visual stimuli driven by a neural circuit in a closed-loop setting. b. We use LLM-guided tree search to autonomously explore the space of dynamical models, evolving Python code to minimize predictive error on neural activity. c. Despite high predictive in-distribution performance, unconstrained black-box tree search models fail to identify the systemâ€™s mechanisms, as revealed by effective connectivity matrices (excerpt shown; blue and red indicate inhibitory and excitatory interactions, respectively; color intensity represents magnitude). In contrast, a structure-constrained grey-box tree search model successfully identifies the correct signs and magnitudes closely matching the ground truth, from a structural prior that provides information about the existence and absence of connections. constrained neural architectures for visuomotor behaviors. In this work, we use simZFish to generate high-fidelity in silico datasets that serve as a testbed for system identification (Ljung, 2010). By simulating the virtual fishâ€™s response to a range of visual stimuli, we create a ground-truth dataset where every internal state, sensory input, and underlying circuit connectivity is perfectly observable. This allows us to study a critical question: What are the data and modeling prerequisites to recover the mechanisms of a complex neural circuit? While standard forecasting architectures can provide initial baselines, they are rarely optimized for the specific temporal and structural constraints of neural dynamics. A primary bottleneck in modeling neural activity is often the modelerâ€™s own inductive bias and the significant time required to manually explore the architectural landscape. We address this by framing model design as an optimization problem. Specifically, we utilize an automated tree-search approach (AygÃ¼n et al., 2025) to traverse a vast space of code-based model definitions, leveraging Large Language Models (LLMs) to effectively â€œevolveâ€ solutions. We compare two distinct discovery regimes: an unconstrained search for high-accuracy black-box predictors, and a structure-constrained search that uses structural priors (derived from the â€œwiring diagramâ€), and we evaluate the discovered models with respect to their ability to generalize to unseen stimuli, their interpretability, and mechanistic recovery (Figure 1). In summary, our contributions are as follows: 1. An in silico testbed for verifiable neural system identification. Unlike real-data benchmarks where ground truth is unknown, our testbed provides a transparent reference, allowing us to evaluate whether models recover the true mechanisms of a neural circuit. 2. Sensory drive is a prerequisite for identifiability. We demonstrate that in sensory-driven neural circuits, the true transition function is practically non-identifiable using standard autore-gressive history alone.   

> 2Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

Figure 2 | Neural circuit model. a. The neural circuit model defines the information flow, processing retinal input through the early and late pretectum (ePT, lPT) to drive downstream command nuclei (nMLF, aHB) and motor circuits. b. Connectivity diagram of an example late pretectal neuron ( oB 1), illustrating the specific excitatory and inhibitory wiring with ePT neurons resulting in its direction-selectivity. 3. In-distribution accuracy is a poor proxy for system identification. We show that LLM-guided tree search discovers models that achieve state-of-the-art test set accuracy but fail on out-of-distribution (OOD) stimuli. These models prioritize statistical shortcuts over faithful recovery of mechanisms, challenging the practice of ranking neural models by test set error. 4. Structural priors enable mechanistic recovery. We demonstrate that providing structural priors is sufficient to regularize the discovery process. This eliminates statistical shortcuts, enables robust OOD generalization, interpretability, and faithful recovery of static and dynamic mechanismsâ€”as verified by effective connectivity and impulse response analyses, respectively. 5. Recommendations for real-world system identification: Based on our in silico results, we propose concrete guidelines for modeling real neural recordings. We recommend shifting from pure autoregression to connectome-constrained conditional forecasting tasks with OOD evaluations, to bridge the gap between predictive AI and mechanistic neuroscience. We position our work relative to recent advances in automated model discovery, neural system identification, and connectome-constrained modeling in Appendix A. 

2. Methods 

2.1. Neuromechanical Simulations 

We generate synthetic neural activity datasets with simZFish, a detailed neuromechanical simulator of larval zebrafish Liu et al. (2025). It integrates realistic body mechanics, environmental interactions, and neural circuits to reproduce sensorimotor behaviors like the optomotor response (OMR). The OMR is an innate reflex causing fish to orient in the direction of whole-field visual motion to stabilize position, and it is probed in openly available whole-brain activity recordings (e.g., Chen et al., 2018; Lueckmann et al., 2025). We provide a summary of the simulations, and refer to Liu et al. (2025) and Appendix B for full details. 

2.1.1. Simulation Environment 

Simulations are implemented in Webots (Michel, 2004), an open-source robotics simulator. An illustration of the simulation environment is in Figure 1a.   

> 3Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

Body Model. The body approximates a 4 mm larval zebrafish with density slightly less than water. It comprises seven cuboid segmentsâ€”a head with cameras and pectoral fins, followed by six body segments, and a tail finâ€”connected by servomotor-actuated hinge joints. 

Sensory Environment. Visual stimuli (drifting sinusoidal gratings) are presented in a simulated petri dish, locked to the body orientation, mimicking closed-loop setups. 

Fluid Dynamics. To simulate realistic locomotion, the body is subject to buoyancy, viscous resistance, and inertial drag, with parameters tuned to match kinetic recordings. Our simulations employ a 0.05 ms integration time step using Webotsâ€™ default first-order semi-implicit integrator for the body kinematics. 

2.1.2. Neural Circuit Model 

The neural circuit model is based on anatomical, functional, and computational findings from real zebrafish visuomotor pathways. We ported the C-based implementation by Liu et al. (2025) to JAX (Bradbury et al., 2018), for differentiability and ease of experimentation. An overview of the network model is in Figure 2. The circuit is formulated as a dynamical system where the activity of a neural population is deter-mined by a non-linear transition operator. We define a generalized sigmoidal response function, Î¦,parameterized by synaptic weights W, bias thresholds b, and response gains ðŽ :

> Î¦

(x; W, b, ðŽ ) = ðœŽ (ðŽ âŠ™ ( Wx âˆ’ b)) , (1) where x is the input vector, ðœŽ (Â·) is the element-wise sigmoid function, and âŠ™ denotes the Hadamard product. 

Retina. Visual input is captured by two laterally positioned simulated cameras, acting as photore-ceptors driving a layer of simulated Bipolar Cells (BCs) detecting transient luminance changes. BCs project to four types of Direction-Selective Retinal Ganglion Cells (DSGCs), tuned to the four cardinal directions, producing an output vector uret  

> ð‘¡

representing the instantaneous motion across the visual field. 

Early Pretectum. The retinal output projects contralaterally to the Early Pretectum (ePT). Unlike the retina, pretectal responses exhibit significant temporal persistence. The ePT state hept  

> ð‘¡

is defined as a first-order low-pass filter driven by sparse retinotopic projections: 

hept  

> ð‘¡

= (1 âˆ’ ð›¼ ð‘¡ )hept  

> ð‘¡ âˆ’1

+ ð›¼ ð‘¡ Î¦(uret  

> ð‘¡

; Wret , bept , ðŽ ept ), (2) where Wret is a fixed, sparse binary mask enforcing contralateral connectivity to the lower-temporal visual field. Crucially, the update rate ð›¼ ð‘¡ is state-dependent: its value depends on the current motor state (swim bout vs. rest) to dynamically filter out self-generated visual motion artifacts during rapid tail beats. 

Late Pretectum. The ePT neurons drive the Late Pretectum (lPT), a layer responsible for synthesizing complex binocular motion primitives. This layer consists of 12 distinct functional types per hemisphere, whose names reflect three axes of variation: (1) direction (inward/outward/global); (2) ocularity (monocular/binocular); and (3) velocity preference (type 1/type 2; forward- vs. backward-preferring). For example, an oB 1 neuron responds to outward binocular motion and prefers forward velocity. Mathematically, we model the lPT as an instantaneous layer combining linear connectivity with a non-linear binocular gating mechanism. We express this compactly by treating the gating signal as a   

> 4Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

state-dependent threshold modulation: 

hlpt  

> ð‘¡

= Î¦



hept  

> ð‘¡

; Wlpt , blpt âˆ’ Mgate âŠ™ g(hept  

> ð‘¡

), ðŽ lpt 



. (3) Here, Wlpt encodes the subtype-specific excitatory/inhibitory wiring (e.g., Figure 2b). The term 

g(Â·) represents a binocular co-activation signal (AND-gate) implemented via a minimum function that selects specific pairs of ipsilateral and contralateral inputs (see Appendix B for details). This modulation effectively acts as a dynamic bias, controlled by the sparse mask Mgate (active only for two lPT neurons per hemisphere). 

Command Neurons in Mid- and Hindbrain (CMD). The lPT layer projects to two downstream command centers: the Nucleus of the Medial Longitudinal Fasciculus (nMLF) controls swim vigor, and the Anterior Hindbrain (aHB), controlling turning bias. They are modeled as first-order low-pass filters with distinct timescales ð›½ and connectivity: 

hcmd  

> ð‘¡

= (1 âˆ’ ð›½ )hcmd  

> ð‘¡ âˆ’1

+ ð›½ Î¦(hlpt  

> ð‘¡

; Wcmd , bcmd , ðŽ cmd ). (4) While the nMLF integrates inputs to drive forward velocity, aHB integrates bilateral differences for the turning direction. 

Motor Circuits. Locomotion in zebrafish is discrete (burst-and-glide). The â€œBout Gateâ€ is implemented as a stochastic Integrate-and-Fire mechanism. An evidence accumulator ð¼ ð‘¡ integrates the total drive from the nMLF: 

ð¼ ð‘¡ = ð¼ ð‘¡ âˆ’1 + ð›¾ 

> 2

âˆ‘ï¸ 

> ð‘– =1

â„Žcmd  

> ð‘¡,ð‘–

+ ðœ– ð‘¡ , ðœ– ð‘¡ âˆ¼ N ( 0, ðœŽ 2). (5) A swim bout is triggered if ð¼ ð‘¡ exceeds a threshold ðœƒ , resetting the integrator. Upon initiation, the discrete action ð‘Ž ð‘¡ âˆˆ { Forward , Left , Right } is sampled from a categorical distribution determined by the command neuron states, with action probabilities pð‘¡ :

pð‘¡ = sð‘¡ 

âˆ¥sð‘¡ âˆ¥1

with sð‘¡ = Wact hcmd  

> ð‘¡

+ bact . (6) When the â€œBehavior Determinatorâ€ initiates a turn, it triggers greater activity in the ventral spinal neurons (vSPN) on one side, which in turn leads to increased ipsilateral motor neuron activation and the necessary body curvature for the turn. Finally, twelve spinal central pattern generators (CPGs), modeled as coupled oscillators, generate rhythmic tail undulations, and twelve motor neurons determine body curvature by integrating outputs from vSPNs and CPGs. 

2.1.3. Simulated Behavior 

The simulated fish executes discrete bouts consisting of rapid tail undulations followed by a passive glide phase, similar to real zebrafish. When exposed to drifting sinusoidal gratings, it recapitulates the behavioral distribution of swims and turns observed in real zebrafish. 

2.1.4. Dataset Generation 

We ran simZFish simulations under four binocular motion conditions designed to probe the visuomotor circuit: bilateral outward, bilateral inward, and two asymmetric configurations where eyes receive opposing inward/outward motion. At the same time, we recorded the activity, resulting in 400,000 samples at 1 ms resolution in total.   

> 5Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish 1e-6 1e-5 1e-4 1e-3 1e-2 1e-1 Test MAE 1 step ahead 4 steps ahead 16 steps ahead 64 steps ahead 256 steps ahead mean  hnlinear  htide  htide  s+h tsmixer  htsmixer  s+h deepar  hdeepar  s+h gt hgt s+h mean  hnlinear  htide  htide  s+h tsmixer  htsmixer  s+h deepar  hdeepar  s+h gt hgt s+h mean  hnlinear  htide  htide  s+h tsmixer  htsmixer  s+h deepar  hdeepar  s+h gt hgt s+h mean  hnlinear  htide  htide  s+h tsmixer  htsmixer  s+h deepar  hdeepar  s+h gt hgt s+h mean  hnlinear  htide  htide  s+h tsmixer  htsmixer  s+h deepar  hdeepar  s+h gt hgt s+h

Figure 3 | Sensory information is a prerequisite for system identification. Performance (Test MAE, log scale) of baseline models and the gt circuit across prediction horizons. While models marked with 

s+h are conditioned on exogenous sensory drive, models with h only use past history. gt h yields higher error than the naive mean h baseline, suggesting that the true solution is non-identifiable without sensory drive based on standard error metrics. 

2.2. System Identification 

To provide a foundation for verifiable model discovery, we establish a task using the simZFish dataset. 

2.2.1. Task Definition 

We frame our task as a system identification problem (Ljung, 2010). We assume the neural activ-ity evolves according to a ground-truth transition function ð‘“ âˆ— driven by past activity and sensory information: 

að‘¡ = ð‘“ âˆ— (að‘¡ âˆ’1, xð‘¡ ), (7) where að‘¡ âˆˆ â„ð‘ is the activity of the ð‘ = 36 neural units (comprised of ePT, lPT, and nMLF/aHB populations) and xð‘¡ âˆˆ â„ð· represents the exogenous covariates. The covariates include variables that drive the system but are not targets of prediction: specifically the instantaneous sensory drive Î¦(uret  

> ð‘¡

)

and the behavioral bout state. Because the internal bout trigger is stochastic (Equation 5), the future trajectory contains inherent (aleatoric) uncertainty. Conditioning on x resolves this, transforming the task from a probabilistic into a deterministic problem. We seek to discover a parameterized model ð‘“ ðœƒ that approximates the mechanisms of ð‘“ âˆ—. Since the ground-truth mechanisms are unknown in real-world settings, we do not optimize for mechanistic similarity directly. Instead, we employ multi-step forecasting as a proxy task. The identification task is to find the optimal pair ( ð‘“ , ðœƒ ) that minimizes the divergence between the recursively simulated trajectory Ë†a and the ground truth a over a horizon ð» = 256 :

min 

> ðœƒ ð»

âˆ‘ï¸ 

> ð‘¡ =1

âˆ¥Ë†að‘¡ âˆ’ að‘¡ âˆ¥ s.t. Ë†að‘¡ = ð‘“ ðœƒ (Ë†að‘¡ âˆ’1, xð‘¡ ). (8) To test for both interpolation and robust extrapolation, we use three of the conditions described in Section 2.1.4 for the in-distribution regime (split chronologically into 70% training, 10% validation, and 20% test), while the fourth is reserved as an out-of-distribution (OOD) holdout to evaluate whether models generalize.   

> 6Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

2.2.2. Evaluation 

We evaluate candidate models on two axes: their ability to predict neural activity and their success in recovering ground-truth mechanisms. Following ZAPBench (Lueckmann et al., 2025), we quantify trajectory prediction error using the Mean Absolute Error (MAE) averaged across all units and time steps. For a set of test windows W,the error is defined as: MAE W = 1

|W| âˆ‘ï¸ ð‘¤ âˆˆ W 

1

> ð‘
> ð‘

âˆ‘ï¸ 

> ð‘› =1

1

> ð»
> ð»

âˆ‘ï¸ 

> â„Ž=1

|Ë†ð‘Ž ð‘¤ â„Ž,ð‘› âˆ’ ð‘Ž ð‘¤ â„Ž,ð‘› |. (9) We report this metric on two data splits. The Test MAE measures performance on held-out trajectories from the same stimulus conditions used during training. The Holdout MAE measures performance on the OOD stimulus condition. We posit that a low Holdout MAE is a strong proxy for having learned the correct mechanisms. Since we operate in an in silico setting with known, differentiable ground truth ð‘“ âˆ—, we can verify the mechanistic fidelity of a discovered model ð‘“ ðœƒ : We compute the effective connectivity matrix using Jacobian sensitivity analysis and impulse responses (IR) relative to ð‘“ âˆ—. This allows us to evaluate recovery of static and dynamic mechanisms, respectively. Appendix C provides details on these analyses including computation of the error metrics LJac and LIR .

2.3. Baseline Models 

To contextualize the performance of our discovered models, we compare against two distinct categories of baselines. First, to establish the predictive ceiling of the dataset, we employ state-of-the-art time-series forecasting architectures including nlinear (Zeng et al., 2023), tide (Das et al., 2023), and 

tsmixer (Chen et al., 2023), as well as a naive mean baseline. These models map a history window directly to the future horizon without explicitly modeling step-by-step transition dynamics. While they serve as rigorous controls for predictive accuracy, they do not yield an explicit transition function. Second, we evaluate deepar (Salinas et al., 2020), a probabilistic recurrent network. Like our discovered models, deepar recursively predicts the next step via a learned transition function. Details are in Appendix D. 

2.4. Tree Search 

To navigate the combinatorial space of potential neural architectures, we employ an automated code generation approach driven by LLMs and Tree Search: We use the method proposed by AygÃ¼n et al. (2025), which combines a pre-trained LLM with a heuristic search algorithm (Predictor Upper Confidence Bound; Silver et al., 2016) to iteratively refine solutions. The search scores solution using validation set MAE on in-distribution conditions. We utilized Gemini 3 Flash (Google, 2025) as the backbone LLM. All details are in Appendix E. 

2.5. Data and Code 

We will release datasets and code, including the JAX-translated neural circuit model, upon publication.    

> 7Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish 1e-6 1e-5 1e-4 1e-3 1e-2 1e-1 Test MAE 1 step ahead 4 steps ahead 16 steps ahead 64 steps ahead 256 steps ahead nlinear tsmixer deepar ts 001 ts 422 nlinear tsmixer deepar ts 001 ts 422 nlinear tsmixer deepar ts 001 ts 422 nlinear tsmixer deepar ts 001 ts 422 nlinear tsmixer deepar ts 001 ts 422 a. b.
> 001 422
> Score

Figure 4 | Tree search discovers SOTA predictive models . a. Progression of LLM-guided tree search, highlighting first and highest-scoring solutions ( ts 001 , ts 422 ). For visual clarity, this tree is pruned to show only the ancestral path of ts 422 ; the complete, unpruned search tree is in Figure S1. 

b. Performance comparison of discovered models against human-curated baselines. The highest-scoring discovered tree-search architecture ( ts 422 ) significantly outperforms baselines across all prediction horizons. 

3. Results 

3.1. Sensory Drive and Identifiability 

A fundamental challenge in neural modeling is distinguishing intrinsic dynamics from responses to extrinsic drive (Vahidi et al., 2024). We used our in silico testbed to perform this decomposition explicitlyâ€”an intervention virtually impossible to replicate in real-world experiments. We evaluated a model reflecting the ground truth circuit in two regimes: conditioned on instantaneous sensory drive and the history of past activity ( gt s+h ), and a control for which the sensory drive Î¦(uret  

> ð‘¡

) is set to zero ( gt h). Despite using the true transition function ð‘“ âˆ—, gt h yields higher error than the naive mean h baseline (Figure 3). This apparent paradox highlights the dissipative nature of the system: without sensory drive, the underlying circuit dynamics decay to rest. In contrast, the mean h baseline minimizes error by predicting that neural activity persists unchangedâ€”thereby reflecting the average activity maintained by the continuous sensory drive. This shows that in a purely autoregressive setting, the true mechanistic model is practically non-identifiable based on standard error metrics. Only when conditioning on sensory drive in addition to history is the system identification task well-posed: The error for gt s+h drops to the numerical floor, representing the limits of floating-point precision. However, access to sensory drive is necessary but not sufficient; the model architecture must also be capable of effectively utilizing it. While the multivariate tsmixer s+h and deepar s+h baselines improve with the addition of sensory covariates, the univariate tide s+h model does not significantly improve over its history-only counterpart, tide h. This disparity highlights a representational mismatch: the visual stimulus is a global signal that exerts differential effects across the population. Global univariate models like tide â€”which apply a shared mapping across all unitsâ€”lack the expressivity to capture how a single global input drives heterogeneous neural responses. 

3.2. Model Discovery with Tree Search 

The search space of predictive architectures is vast, encompassing a wide variety of structural biases, connectivity patterns, and temporal integration strategies. Traditionally, navigating this space relies on researcher intuition and manual trial-and-error. To evaluate whether automated discovery can identify accurate models of neural dynamics, we utilized an LLM-guided tree search.    

> 8Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish 1e-5 1e-4 1e-3 1e-2 1e-1 1e+0 1e+1 Holdout MAE 1e-5 1e-4 1e-3 1e-2 1e-1 1e+0 1e+1 Test MAE tree search (ts) structure-constrained ts (sts) 1e-6 1e-5 1e-4 1e-3 1e-2 1e-1 Holdout MAE 1 step ahead 4 steps ahead 16 steps ahead 64 steps ahead 256 steps ahead nlinear tsmixer deepar ts 001 ts 422 sts 445 nlinear tsmixer deepar ts 001 ts 422 sts 445 nlinear tsmixer deepar ts 001 ts 422 sts 445 nlinear tsmixer deepar ts 001 ts 422 sts 445 nlinear tsmixer deepar ts 001 ts 422 sts 445 a. b.

Figure 5 | Structural priors enable robust generalization. a. Generalization analysis comparing performance on in-distribution (Test MAE) versus out-of-distribution stimuli (Holdout MAE). Tree search models ( ts , purple) exhibit a significant generalization gap; while they achieve high accuracy on the test set, they fail to generalize to novel stimuli, suggesting they memorize sensory-motor correlations. Structurally-constrained tree search solutions ( sts , green) cluster closer along the diagonal, indicating robust transfer to unseen environments. Dotted purple line marks the convex hull for ts solutions. b. Holdout performance across prediction horizons. The highest-scoring constrained model ( sts 445 ) significantly outperforms its unconstrained counterpart ( ts 422 ) and baselines. In our initial experiments, tree search tended to produce heterogeneous code across various frame-works, making systematic evaluation and integration difficult. We developed a standardized evaluation harness and prompt designed specifically to search within the class of autoregressive models im-plemented in flax (Heek et al., 2024). To ensure the discovery process remained computationally efficient, we implemented a heavily optimized, GPU-resident data loading pipeline. We found that training models on a truncated rollout of 32 timesteps was sufficient to identify architectures that performed well on the full 256-step prediction horizon, significantly reducing the training time required for each candidate solution during the search process. Following these changes, the automated discovery process Figure 4a was highly successful, with many of the identified solutions achieving significantly lower Test MAE than the predictive baselines: Figure 4b highlights the performance of two discovered models, ts 001 (the first AI-generated solution) and ts 422 , compared to nlinear , tsmixer , and deepar . Notably, ts 422 represents the highest-scoring outcome of our search, outperforming state-of-the-art baselines by almost an order of magnitude when averaging over all prediction horizons. 

3.3. Generalization and Interpretability 

Despite the high predictive accuracy of the tree-search models, they have limited scientific utility: they fail to generalize and it is unclear how to map them to the underlying circuit. As shown in Figure 5a, unconstrained tree-search models ( ts ) with low Test MAE exhibit a significant generalization gap. Although they achieve near-perfect MAE on the test set for trained stimulus con-ditions, their performance degrades sharply when evaluated on held-out visual stimuli. This indicates that the search identifies solutions that exploit stimulus-specific correlationsâ€”essentially capturing statistical shortcutsâ€”rather than recovering mechanisms that govern the zebrafishâ€™s response. Furthermore, these solutions offer limited interpretability for the computational neuroscientist. In Appendix F.3, we show the transition function for the highest-scoring model, ts 422 . While the code is mathematically precise, it is mechanistically opaque. The transition function lacks a clear correspondence to the neural circuit and instead makes its predictions in a 256-dimensional latent   

> 9Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

space. This lack of mechanistic transparency makes it difficult to map the modelâ€™s internal operations back to the physical circuit, rendering it of limited value for formulating testable biological hypotheses. 

3.4. Incorporating Structural Priors 

To bridge the gap between predictive accuracy and scientific utility, we modified the discovery process by incorporating structural priors. Instead of an unconstrained search over transition functions, we provided the tree search with prior information derived from the structural connectivity of the neural circuit (Appendix E.3): These hints reveal the systemâ€™s hierarchical organization, the sparse connectivity patterns (including recurrence) and the functional role of covariates. Crucially, the structural prior provided to the search contains no information about connectivity weightsâ€”only about the existence and absence of connections. The specific non-linear functions that govern the interactions must still be discovered. The impact of the structural prior is clearly evident in the modelâ€™s ability to generalize. As shown in Figure 5a, structure-constrained models (green markers, sts ) are closer to the diagonal region of the scatter plot. Unlike the unconstrained models, sts solutions maintain more consistent performance across both training and held-out stimulus conditions, suggesting that they overfit less to correlations present in train and test sets. The highest-scoring model ( sts 445 ) is significantly more robust on the OOD condition, more than an order of magnitude for longer prediction horizons compared to baselines (Figure 5b). In Appendix F.4, we show the transition function for sts 445 : In contrast to the hard-to-interpret code of ts 422 , it reflects the structural organization of the circuit and is composed of semantically meaningful motifs. 

3.5. Mechanistic Recovery 

We analyzed the effective connectivity of the highest-scoring discovered solution to evaluate mecha-nistic recovery (Appendix C.1). In the ground truth circuit (Figure 6a), the lPT population depends on structured inputs from the ePT population (middle-left block). The unconstrained ts 422 model (Fig-ure 6b) fails to recover these upstream dependencies. Instead, it relies on spurious recurrence within the lPT population (central block)â€”effectively substituting internal memory for sensory processing. In contrast, sts 445 recovers the correct effective connectivity. While the structural prior included a binary matrix for ePT â†’ lPT connectivity and indicated absence of recurrent lPT connections, it contained no information about the strengths of the feedforward interactions. Consequently, the optimization process had to discover the nature of these links. We find that sts 445 correctly recovered the excitatory and inhibitory checkerboard pattern of ePT â†’ lPT interactions, matching the ground truth in both sign and relative magnitude (compare Figure 6a versus Figure 6c). To analyze the fidelity of dynamic mechanisms of the discovered models, we performed an impulse response analysis (Appendix C.2), perturbing the system to a saturated state and observing the relaxation dynamics under different behavioral contexts (swimming versus resting): While ts 422 

produces spurious oscillations and fails to capture the correct decay rates, sts 445 closely tracks the ground truth trajectories as shown in Figure S2. We confirm the qualitative differences discussed in this section by quantitative metrics reported in Table S1: We find that, relative to ts 422 , sts 445 achieves a more than 30-fold reduction in effective connectivity error ( LJac ), and a more than 5-fold reduction in impulse response error ( LIR ). Table S2 reports the same metrics for the top 50 highest-scoring solutions of both tree searches, confirming the robustness of these results.       

> 10 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish gt ePT lPT CMD Input (t) ePT lPT CMD Output (t+1) ts 422 ePT lPT CMD Input (t) sts 445 ePT lPT CMD Input (t) âˆ’1.0 âˆ’0.5 0.0 0.5 1.0 Net Influence (dOutput/dInput) a. b. c.

Figure 6 | Structural priors enable mechanistic recovery. a. Effective connectivity matrices derived via Jacobian sensitivity analysis for the ground-truth circuit. Note the distinct excitatory (red) and inhibitory (blue) pattern in the ePT â†’ lPT interactions (middle-left block) and the absence of recurrence in the lPT block (center). b. The unconstrained ts 422 model largely fails to capture the ePT 

â†’ lPT interactions, instead relying on spurious recurrent lPT dynamics. c. The structure-constrained model, which received a boolean mask for ePT â†’ lPT connectivity but no information about strength, faithfully recovers the sign and magnitude pattern of the interactions. Taken together, these analyses show that structural priors enable the discovery of models that abandon statistical shortcuts in favor of recovering true mechanisms, allowing faithful system identification. 

4. Discussion 

Our analysis reveals that sensory drive is a prerequisite for identifiability, while in-distribution accuracy is a poor proxy for system identification. Crucially, we find that structural priors enable mechanistic recovery, eliminating statistical shortcuts that unconstrained models exploit. These insights motivate three concrete recommendations for future modeling of real-world datasets: 

#1: Redefining prediction tasks. Current benchmarks, such as ZAPBench (Lueckmann et al., 2025) which is based on whole-brain neural activity recordings from a larval zebrafish, treat neural activity as a self-contained time series suitable for autoregressive prediction. However, our control experiments show that without access to the driving sensory signals, the true circuit mechanisms may be non-identifiable. In real-world recordings, this issue is exacerbated by additional unobserved drivers (e.g., olfactory, auditory, or mechanosensory inputs). We propose changing the task from predicting future activity from history and stimulus to predicting downstream integration, conditioned on the activity of sensory populations identified via the connectome. 

#2: OOD generalization as the primary metric . We show that unconstrained models can achieve near-perfect accuracy on familiar data by exploiting statistical shortcuts, only to fail completely when the stimulus changes. To ensure models capture true mechanisms rather than dataset-specific correlations, evaluation should prioritize performance on held-out stimulus classes or perturbation studies. 

#3: Use physical wiring as a scaffold. We propose a shift toward structural grey-box modeling. While black-box models currently dominate neural forecasting, our in silico experiments suggest they poorly recover mechanisms and tend to have limited interpretability. The upcoming release of the synapse-resolution connectome for the ZAPBench specimen will make   

> 11 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

these recommendations practical and offer an opportunity to test a key result from our work: that static wiring diagrams can serve as potent scaffold. It is often assumed that recovering mechanisms requires detailed knowledge of synaptic strengths and valences from the ultrastructure or molecular annotations. However, we find that knowing the wiring diagram provides a structural prior that allows an optimization process to infer mechanisms directly from paired neural activity. 

Limitations and Future Work. simZFish is a simplification of the vertebrate brain, focusing on a particular visuomotor circuit and associated behavior. Future work could extend the simulator to include broader behavioral repertoires and associated neural circuits. Additionally, while the tree search is applicable to high-dimensional data, the process is computationally intensive. Finally, we prioritized a fully observable, high-fidelity regime to rigorously validate mechanistic recovery against a known ground truth. Extending this to settings with partial observability or lower signal-to-noise ratios is an area for future research. 

Conclusion. In silico testbeds allow us to better understand the capabilities of AI-driven code search. Our results reveal that treating model discovery purely as an unconstrained error-minimization problem encourages solutions that exploit statistical shortcuts rather than faithful system identification. As we deploy these tools on complex real-world science problems, testbeds with verifiable solutions can guide the design of constraints and metrics needed to align AI-driven discovery with scientific utility and mechanistic understanding. 

Acknowledgements 

We would like to thank Peter H. Li, Franz Rieger, Zhengdao Chen, and the Science AI team at Google Research for discussions and project support. We also thank the authors of open-source software enabling this work, including simZFish (Liu et al., 2025), Webots (Michel, 2004), JAX (Bradbury et al., 2018), Flax (Heek et al., 2024), and the wider Python community. 

References 

E. AygÃ¼n, A. Belyaeva, G. Comanici, M. Coram, H. Cui, J. Garrison, R. J. A. Kast, C. Y. McLean, P. Nor-gaard, Z. Shamsi, D. Smalling, J. Thompson, S. Venugopalan, B. P. Williams, C. He, S. Martinson, M. Plomecka, L. Wei, Y. Zhou, Q.-Z. Zhu, M. Abraham, E. Brand, A. Bulanova, J. A. Cardille, C. Co, S. Ellsworth, G. Joseph, M. Kane, R. Krueger, J. Kartiwa, D. Liebling, J.-M. Lueckmann, P. Raccuglia, X. Wang, K. Chou, J. Manyika, Y. Matias, J. C. Platt, L. Dorfman, S. Mourad, and M. P. Brenner. An AI system to help scientists write expert-level empirical software. arXiv preprint , 2025. URL 

https://doi.org/10.48550/arXiv.2509.06503 .J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-derPlas, S. Wanderman-Milne, and Q. Zhang. JAX: Composable transformations of Python+NumPy programs, 2018. URL https://github.com/jax-ml/jax .S.-A. Chen, C.-L. Li, N. Yoder, S. O. Arik, and T. Pfister. TSMixer: An all-MLP architecture for time series forecasting. Transactions on Machine Learning Research , 2023. URL https://openreview. net/forum?id=wbpxTuXgm0 .X. Chen, Y. Mu, Y. Hu, A. T. Kuan, M. Nikitchenko, O. Randlett, A. B. Chen, J. P. Gavornik, H. Sompolinsky, F. Engert, and M. B. Ahrens. Brain-wide organization of neuronal activity and convergent sensorimotor transformations in larval zebrafish. Neuron , 2018. URL https: //doi.org/10.1016/j.neuron.2018.09.042 .  

> 12 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

A. Das, W. Kong, A. Leach, S. Mathur, R. Sen, and R. Yu. Long-term forecasting with TiDE. Transactions on Machine Learning Research , 2023. URL https://openreview.net/forum?id=pCbC3aQB5W .Google. Gemini 3 Flash: Frontier intelligence built for speed, December 17 2025. URL https: //blog.google/products-and-platforms/products/gemini/gemini-3-flash/ . The Keyword. J. Heek, A. Levskaya, A. Oliver, M. Ritter, B. Rondepierre, A. Steiner, and M. van Zee. Flax: A neural network library and ecosystem for JAX, 2024. URL https://github.com/google/flax .X. Liu, M. D. Loring, L. Zunino, K. E. Fouke, F. A. Longchamp, A. Bernardino, A. J. Ijspeert, and E. A. Naumann. Artificial embodied circuits uncover neural architectures of vertebrate visuomotor behaviors. Science Robotics , 2025. URL https://doi.org/10.1126/scirobotics.adv4408 .L. Ljung. Perspectives on system identification. Annual Reviews in Control , 2010. URL https: //doi.org/10.1016/j.arcontrol.2009.12.001 .J.-M. Lueckmann, A. Immer, A. B.-Y. Chen, P. H. Li, M. D. Petkova, N. A. Iyer, L. W. Hesselink, A. Dev, G. Ihrke, W. Park, A. Petruncio, A. Weigel, W. Korff, F. Engert, J. Lichtman, M. Ahrens, M. Januszewski, and V. Jain. ZAPBench: A benchmark for whole-brain activity prediction in zebrafish. In International Conference on Learning Representations , 2025. URL https://openreview. net/forum?id=oCHsDpyawq .O. Michel. Webots: Professional mobile robot simulation. Journal of Advanced Robotics Systems , 2004. URL https://doi.org/10.5772/5618 .F. Pei, J. Ye, D. Zoltowski, A. Wu, R. H. Chowdhury, H. Sohn, J. E. Oâ€™Doherty, K. V. Shenoy, M. T. Kaufman, M. Churchland, M. Jazayeri, L. E. Miller, J. Pillow, I. M. Park, E. L. Dyer, and C. Pandarinath. Neural Latents Benchmark â€™21: Evaluating latent variable models of neural population activity. In 

NeurIPS , 2021. URL https://doi.org/10.48550/arXiv.2109.04463 .D. Salinas, V. Flunkert, J. Gasthaus, and T. Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting , 2020. URL https://doi. org/10.1016/j.ijforecast.2019.07.001 .M. Schrimpf, J. Kubilius, M. J. Lee, N. A. R. Murty, R. Ajemian, and J. J. DiCarlo. Integrative Benchmarking to Advance Neurally Mechanistic Models of Human Intelligence. Neuron , 2020. URL 

https://doi.org/10.1016/j.neuron.2020.07.040 .D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalch-brenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature , 2016. URL 

https://doi.org/10.1038/nature16961 .C. Sourmpis, C. C. Petersen, W. Gerstner, and G. Bellec. Biologically informed cortical models predict optogenetic perturbations. eLife , 2025. URL https://doi.org/10.7554/eLife.106827.2 .P. Turishcheva, P. G. Fahey, L. Hansel, R. Froebe, K. Ponder, M. VystrÄilovÃ¡, K. F. Willeke, M. Bashiri, E. Wang, Z. Ding, A. S. Tolias, F. H. Sinz, and A. S. Ecker. The dynamic sensorium competition for predicting large-scale mouse visual cortex activity from videos. arXiv preprint , 2023. URL 

https://doi.org/10.48550/arXiv.2305.19654 .P. Vahidi, O. G. Sani, and M. M. Shanechi. Modeling and dissociation of intrinsic and input-driven neural population dynamics underlying behavior. Proceedings of the National Academy of Sciences ,2024. URL https://doi.org/10.1073/pnas.2212887121 .  

> 13 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

A. Zeng, M. Chen, L. Zhang, and Q. Xu. Are transformers effective for time series forecasting? In AAAI Conference on Artificial Intelligence , 2023. URL https://doi.org/10.1609/aaai.v37i9.26317 .  

> 14 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

# Appendices 

A Related Work 2B Neuromechanical Simulations 3C Additional Analyses 5

C.1 Effective Connectivity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5C.2 Impulse Response Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

D Baseline Models 6

D.1 mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6D.2 nlinear . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6D.3 tide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6D.4 tsmixer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6D.5 deepar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

E Tree Search 7

E.1 Search Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7E.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7E.3 Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

F Code Listings 10 

F.1 Initial Code for Tree Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 F.2 Solution ts 001 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 F.3 Solution ts 422 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 F.4 Solution sts 445 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 

G Supplementary Figures 17 H Supplementary Tables 19 References 20   

> 1Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

A. Related Work 

Automated Model Discovery. Our work uses the capability of Large Language Models (LLMs) to perform automated model discovery via code generation. While traditional approaches to model discovery relied on genetic programming (Schmidt and Lipson, 2009) or reinforcement learning for neural architecture search (Zoph and Le, 2017), recent methods treat the search process as an evolution of executable code. We utilize the tree-search methodology proposed by AygÃ¼n et al. (2025). Parallel efforts have explored similar LLM-driven evolutionary strategies for mathematical discovery (Romera-Paredes et al., 2024) and algorithmic evolution (Novikov et al., 2025; Lange et al., 2025; Jiang et al., 2025). In related domains, automated model discovery techniques have for example been applied to search for interpretable cognitive models of human and animal behavior (Castro et al., 2025), and neural tuning curves (Tilbury et al., 2025). Our work distinguishes itself by applying these methods to neural system identification in a setting where we can rigorously benchmark the discovery of mechanisms against a verifiable ground truth. 

System Identification with Ground Truth. The difficulty of inferring ground-truth mechanisms from neural data is a long-standing methodological concern. Jonas and Kording (2017) utilized a microprocessor to demonstrate that standard data analysis techniques can fail to recover the known logic of information processing. More recently, Han et al. (2023) used artificial neural networks (ANNs) as in silico ground truths, revealing that functional similarity (predictive performance) does not imply mechanistic similarity. We advance this line of inquiry by moving beyond artificial proxiesâ€” such as microprocessors or generic ANNsâ€”which differ fundamentally from biological substrates. Instead, we employ a high-fidelity, experimentally constrained emulation of the zebrafish optomotor response to move towards more realistic models with ground truth. 

Neuromechanical Simulations and Connectome Constraints. Our work is situated within the growing field of neuromechanical modeling. While detailed simulations have been established for invertebrates such as the worm C. elegans (Sarma et al., 2018; Zhao et al., 2024) and the fruit fly 

D. melanogaster (Lobato-Rios et al., 2022; Vaxenburg et al., 2025), we utilize a larval zebrafish simulation to capture vertebrate visuomotor dynamics. Concurrently, the increasing availability of connectomes has spurred research into constraining neural models with physical wiring diagrams. The success of our structurally constrained models aligns with the findings of Sourmpis et al. (2025), who demonstrated that biologically informed models are more robust to optogenetic perturbations than generic RNNs. Several studies, including Mi et al. (2021); Achterberg et al. (2023); Lappalainen et al. (2024); Beiran and Litwin-Kumar (2025); Duan et al. (2025), have highlighted the predictive utility of structural constraints. Our work bridges these findings with automated discovery: rather than manually designing constrained architectures, we show that connectomic information can be used with AI-driven search to navigate the vast space of possible models and recover mechanistic solutions.   

> 2Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

B. Neuromechanical Simulations 

We detail the numerical values for the parameters introduced in the neural circuit model (Section 2.1), which are based on the reference implementation Liu et al. (2025). 

Early Pretectum (ePT) . The ePT dynamics described in Equation 2 are governed by the update rate 

ð›¼ ð‘¡ and the response function parameters ðŽ ept , bept :

ð›¼ ð‘¡ =

(

0.002 if swim bout active, 

0.01 otherwise, 

ðŽ ept = [0.068 , 0.100 , 0.068 , 0.100 , 0.068 , 0.100 , 0.068 , 0.100 ]âŠ¤,

bept = [90 .0, 60 .0, 90 .0, 60 .0, 90 .0, 60 .0, 90 .0, 60 .0]âŠ¤.

The ePT state vector is ordered as hept = [ePT SL, ePT AL , ePT IL, ePT PL, ePT SR, ePT AR, ePT IR, ePT PR]âŠ¤, i.e., grouped by hemisphere (Left, Right) and functional tuning (Superior, Anterior, Inferior, Posterior). 

Late Pretectum (lPT) . The lPT activations in Equation 3 are computed using the connectivity matrix 

Wlpt , the gating mask Mgate , and response parameters ðŽ lpt , blpt âˆˆ â„24 :

Wlpt =

ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°                                                                                                                                                                        

> 011âˆ’1.50.81.5âˆ’1âˆ’20âˆ’111.50.80âˆ’10
> âˆ’0.81100.81âˆ’1âˆ’2
> âˆ’0.8âˆ’112.50.80âˆ’10
> âˆ’0.8010.50.800000100.800000.810000âˆ’0.8011âˆ’101.5âˆ’1âˆ’10âˆ’1.511.500âˆ’10.500000.80.30001.51âˆ’10.80.50âˆ’20âˆ’1.511.50.8âˆ’1010.81.5âˆ’1âˆ’2011âˆ’1.50.80âˆ’100âˆ’111.50.81âˆ’1âˆ’2âˆ’0.81100.80âˆ’10âˆ’0.8âˆ’112.50.8000âˆ’0.8010.50.80000010000âˆ’0.800.81001.5âˆ’1âˆ’1011âˆ’100âˆ’10.50âˆ’1.511.50.80.30000000.80.50âˆ’201.51âˆ’10.8âˆ’1010âˆ’1.511.5

ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»

, Mgate =

ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°

> 000000000011000000000011

ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»

, ðŽ lpt =

ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°

> 2.52.52.52.52.52.54.04.04.05.04.04.02.52.52.52.52.52.54.04.04.05.04.04.0

ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»

, blpt =

ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°

> 0.80.80.80.80.80.80.60.60.60.40.60.60.80.80.80.80.80.80.60.60.60.40.60.6

ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»

.

The lPT state vector is ordered as 

hlpt = [lPT oB 1 

> L

, lPT oB 2 

> L

, lPT B1 

> L

, lPT B2 

> L

, lPT iB L , lPT ioB L , lPT iMm L , lPT Mm 1 

> L

, lPT Mm 2 

> L

, lPT oMm L , lPT S1 

> L

, lPT S2 

> L

,

lPT oB 1 

> R

, . . . , lPT S2 

> R

]âŠ¤,

grouping first by hemisphere (Left, Right) and then by functional type. The gating function g(h) referenced in Equation 3 implements a â€œsoftâ€ AND-gate using a minimum operation between specific pairs of inputs. This function selects specific ipsilateral ( hipsi ) and con-tralateral ( hcontra ) components from the ePT state vector hept . Specifically, for â€œS-typeâ€-LPT neurons, the gating signal requires activation from both contralateral Superior and ipsilateral Inferior inputs: 

g(h) = min (0.8 Â· hcontra , hipsi ) = min (0.8 Â· ePT SR, ePT IL).

This ensures the neuron is only active when both specific directional conditions are met, scaled by a factor of 0.8 on the contralateral input to account for asymmetric drive.   

> 3Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

Command Neurons (nMLF/aHB) . The dynamics of the command neurons in Equation 4 are defined by the update rate ð›½ = 0.0005 , connectivity Wcmd , and response parameters ðŽ cmd , bcmd :

W1... 12  

> cmd

=

 0.1 âˆ’0.1 0.32 âˆ’0.08 0.25 âˆ’0.05 0 .6 0.4 0 0 0.3 âˆ’0.10 0 0 0 0 0 0 0 0 âˆ’0.8 0 00.1875 0.3875 0.2165 0.3165 0 0 0 0.0165 0 .4165 0 0.096 0 .096            

> âˆ’0.1665 âˆ’0.1665 âˆ’0.1665 âˆ’0.1665 000âˆ’0.105 âˆ’0.105 0âˆ’0.29 âˆ’0.29



,

W13 ... 24  

> cmd

=

 0 0 0 0 0 0 0 0 0 âˆ’0.8 0 00.1 âˆ’0.1 0.32 âˆ’0.08 0.25 âˆ’0.05 0 .6 0.4 0 0 0.3 âˆ’0.1                    

> âˆ’0.1665 âˆ’0.1665 âˆ’0.1665 âˆ’0.1665 000âˆ’0.105 âˆ’0.105 0âˆ’0.29 âˆ’0.29 0.1875 0.3875 0.2165 0.3165 0000.0165 0 .4165 00.096 0 .096



,

ðŽ cmd = [6.5, 6.5, 4.0, 4.0]âŠ¤, bcmd = [0.4, 0.4, 0.65 , 0.65 ]âŠ¤.

The resulting command state vector is ordered as hcmd  

> ð‘¡

= [nMLF L, nMLF R, aHB L, aHB R]âŠ¤.

Motor circuits. The evidence accumulation dynamics in Equation 5 are defined by the integrator threshold ðœƒ = 436 .0, noise standard deviation ðœŽ = 0.65 , and nMLF input scaling factor ð›¾ = 0.6.The action probabilities defined in Equation 6 are computed using the command state vector hcmd  

> ð‘¡

,the projection matrix Wact , and bact :

Wact =

ï£®ï£¯ï£¯ï£¯ï£¯ï£°

1 1 0 00 0 1 00 0 0 1

ï£¹ï£ºï£ºï£ºï£ºï£»

, bact =

ï£®ï£¯ï£¯ï£¯ï£¯ï£°

0.200 0.066 0.066 

ï£¹ï£ºï£ºï£ºï£ºï£»

.

For an explanation of all other stages in the model, see Liu et al. (2025).   

> 4Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

C. Additional Analyses 

C.1. Effective Connectivity Analysis 

To evaluate whether the discovered models recover the true static mechanisms of the neural circuit, we computed the Effective Connectivity Matrix . This matrix quantifies the sensitivity of the predicted neural state at time ð‘¡ + 1 to perturbations in the state at time ð‘¡ (Figure 6). Formally, we analyze the one-step transition að‘¡ +1 = ð‘“ ðœƒ (að‘¡ , xð‘¡ +1). The effective connectivity matrix 

ð½ âˆˆ â„ð‘ Ã—ð‘ is defined as the Jacobian of the future state with respect to the current state: 

ð½ ð‘– ð‘— = ðœ• (að‘¡ +1)ð‘– 

ðœ• (að‘¡ ) ð‘— 

(S1) where ð½ ð‘– ð‘— represents the causal influence of source neuron ð‘— on target neuron ð‘– .To isolate the intrinsic connectivity of the circuit, we compute ð½ while setting exogenous covariates to zero ( xð‘¡ +1 = 0). To account for non-linearities (e.g., sigmoids) where gradients vanish in saturated regions, we do not evaluate ð½ at a single operating point. Instead, we estimate the expected sensitivity by averaging over ð¾ = 11 homogeneous probe states. We define a set of scalars S = {0.0, 0.1, . . . , 1.0}

and construct input vectors a(ð‘  ) = ð‘  Â· 1, where 1 is the all-ones vector. The reported connectivity is: 

Â¯ð½ = 1

|S| âˆ‘ï¸ ð‘  âˆˆ S 

ð½ |að‘¡ =a(ð‘  ) (S2) We quantify the mechanistic fidelity of a model by comparing its connectivity Â¯ð½ model to the ground truth Â¯ð½ gt using the Frobenius norm: 

Ljac = || Â¯ð½ model âˆ’ Â¯ð½ gt || ð¹ (S3) 

C.2. Impulse Response Analysis 

To assess recovery of dynamic mechanisms, we performed an Impulse Response Analysis . We probe how the system relaxes from perturbed states back to rest, and how this dynamic is modulated by behavioral states. We simulate the system for a horizon of ð» = 256 steps starting from a set of impulse conditions. We test ð‘ + 1 initial conditions: 36 cases where a single neuron is activated ( a0 = eð‘– ) and one case where the entire system is saturated ( a0 = 1). We evolve these states using two fixed covariates xmode to test mechanisms depending on swim state (e.g., the ePT update in Equation 2): 1. Rest: Bout state and sensory drive set to 0. 2. Swim: Bout state set to 1, sensory drive set to 0. We quantify the divergence between the modelâ€™s predicted response traces ( Ë†A) and the ground truth traces ( Aâˆ—) using the Mean Absolute Error (MAE) averaged over all time steps, units, and initial conditions. We report this metric separately for the two regimes: 

Lrest IR = MAE ( Ë†Arest , Aâˆ—rest ), Lswim IR = MAE ( Ë†Aswim , Aâˆ—swim ) (S4)   

> 5Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

D. Baseline Models 

D.1. mean 

The naive mean baseline repeats the per-neuron activity from the last timestep across the entire prediction horizon. 

D.2. nlinear 

For nlinear (Zeng et al., 2023), we use a model with context window size of 2. For optimization, we use AdamW (Loshchilov and Hutter, 2019) with a learning rate of 10 âˆ’4 and weight decay of 10 âˆ’4.

D.3. tide 

For tide (Das et al., 2023), we use a model with a context window size of 2, 2 encoder and decoder layers each, a hidden layer size of 128, a decoder output dimensionality of 32, and reversible instance norm. For optimization, we use AdamW (Loshchilov and Hutter, 2019) with a learning rate of 10 âˆ’3

and weight decay of 10 âˆ’4.

D.4. tsmixer 

For tsmixer (Chen et al., 2023), we use a model with a context window size of 2, 2 blocks, and an MLP dimension of 256, and no instance norm. For optimization, we use AdamW (Loshchilov and Hutter, 2019) with a learning rate of 10 âˆ’3 and weight decay of 10 âˆ’4.

D.5. deepar 

For deepar (Salinas et al., 2020), we use a model with 2 stacked GRU layers, with a hidden size of 128. For optimization, we use AdamW (Loshchilov and Hutter, 2019) with a learning rate of 10 âˆ’3

and weight decay of 10 âˆ’4.  

> 6Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

E. Tree Search 

We formulate the model discovery process as a search problem where the state space consists of executable Python code defining neural transition functions, and the objective is to maximize predictive accuracy on the validation set. 

E.1. Search Algorithm 

We utilize the Predictor Upper Confidence Bound applied to Trees (PUCT) algorithm (Silver et al., 2016; AygÃ¼n et al., 2025) to balance exploration of novel solutions and exploitation (refining high-performing solutions). Unlike standard Monte Carlo Tree Search, where the algorithm traverses from the root to a leaf node for expansion, the space of possible code edits here is effectively infinite. Consequently, the algorithm treats every existing node in the tree as a candidate for expansion. At each step, a node ð‘¢ âˆ— is sampled from the entire tree T by maximizing the PUCT objective:  

> ð‘¢ âˆ—

= argmax 

> ð‘¢ âˆˆ T

Ëœð‘… (ð‘¢ ) + ð‘ puct |T | âˆ’1âˆšï¸ Ãð‘£ âˆˆ T ð‘‰ (ð‘£ )

1 + ð‘‰ (ð‘¢ )

! 

> ,

(S5) where ð‘‰ (ð‘¢ ) is the visit count of node ð‘¢ , ð‘ puct is an exploration constant, and |T | âˆ’1 acts as a flat prior. The term Ëœð‘… (ð‘¢ ) represents the normalized rank score of a solution. Solutions are scored by calculating the negative Mean Absolute Error (MAE) on the validation set, averaged over in-distribution conditions. These raw scores are converted into ranks Rank T (ð‘¢ ) in ascending order (where the best solution with the least error has the highest rank), and normalized as: 

Ëœð‘… (ð‘¢ ) = Rank T (ð‘¢ ) âˆ’ 1

|T | âˆ’ 1 . (S6) Upon selecting a node ð‘¢ âˆ—, we prompt the LLM to generate a child node by modifying the source code of ð‘¢ âˆ—. The LLM acts as a mutation operator, guided by instructions defined in Appendix E.3, and the initial code specified in Appendix F.1. The generated code is immediately executed in a sandboxed environment to compute the validation score on the task defined in the Section 2.2.1. Finally, the visit count ð‘‰ is incremented for the selected node and backpropagated to its ancestors. 

E.2. Implementation Details 

We utilized Gemini 3 Flash (Google, 2025) as the backbone LLM for code generation and mutation. Each tree search was limited to a maximum of 500 nodes. To ensure computational feasibility on a single NVIDIA T4 GPU, we enforced a runtime limit of 12 hours per candidate solution. To further accelerate the discovery process, we employed a truncated training regime: models were trained on a short rollout of 32 timesteps. We found this efficient training proxy sufficient to identify architectures that generalize well when rolled out to the full prediction horizon (256 timesteps) used for scoring and evaluation. 

E.3. Prompts 

The initial prompt for tree search, without prior information about structure: 

> 7

Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish 

# Predicting neural activity ## Challenge 

Your task is to build a forecasting model that **predicts future neural activity traces given past activity and covariates**. **Data Specifications:** - **Inputs & Outputs:** All neural activity and continuous covariates are normalized to the â€˜[0, 1]â€˜ range. - **Covariates (â€˜cov_tâ€˜):** Shape â€˜(Batch, CovFeatures=9)â€˜. - â€˜cov_t[..., 0]â€˜: **Bout State**. A binary flag (0 or 1). This acts as a switch for the systemâ€™s dynamics. - â€˜cov_t[..., 1:]â€˜: **Visual Input**. Continuous signals â€˜[0, 1]â€˜ driving the system. **Key Difficulties:** 1. **Autoregressive Stability:** You will develop a transition model $f(y_{t-1}, c_t) \rightarrow y_t$. This is rolled out for 32 timesteps. Small errors at step 1 can explode by step 32. 2. **OOD Generalization:** The test set contains visual patterns **not seen during training**. Your model must capture the underlying causal rules, not memorize training sequences. 

## Evaluation 

Performance is measured by Mean Absolute Error (MAE) on held-out trajectories. Lower is better. 

## Rules 

1. **Structure:** Keep the sections â€˜CONSTANTSâ€˜, â€˜MODELâ€˜, and â€˜OPTIMIZERâ€˜ in this order. 2. **Interface - Hyperparameters:** You **must** define the following constants in the â€˜CONSTANTSâ€˜ section to control the training harness: * â€˜BATCH_SIZEâ€˜: (int) Default ~512. * â€˜MAX_STEPSâ€˜: (int) Default ~500,000. * â€˜SCHEDULED_SAMPLING_START_PROBâ€˜ & â€˜SCHEDULED_SAMPLING_FINAL_PROBâ€˜: (float) Controls teacher forcing. High start (1.0) helps convergence; low final (<0.1) enforces stability. * â€˜SCHEDULED_SAMPLING_DECAY_STEPSâ€˜: (int) Steps to decay the probability. * â€˜EARLY_STOP_PATIENCEâ€˜: (int) Steps to wait before stopping. * â€˜MIN_DELTAâ€˜: (float) Improvement threshold for early stopping. 3. **Interface - Model:** - Use â€˜flax.linenâ€˜. Class name must be â€˜Candidateâ€˜. - â€˜__call__â€˜ signature: â€˜(self, past_pred, cov_t, train=False)â€˜. - **Shape Contract:** Input â€˜past_predâ€˜ is (B, 36). Output must be (B, 36). 4. **Interface - Optimizer:** - Use â€˜optaxâ€˜. Instance must be named â€˜optimizerâ€˜. 5. **Constraints:** Do not modify code in â€˜NOTEâ€˜ comments. 

## Strategic Approaches & Hints 

To win this competition, you should explore distinct architectural ideas. Do not limit yourself to simple MLPs. 

For the tree search with structural prior, the Strategic Approaches & Hints section in the prompt was extended as follows: 

We have prior knowledge about the neuron hierarchy that is likely critical for a low score. The 36 neurons are split into three groups with specific dependencies: 1. **EPT Neurons (Indices 0-8):** Depend on their past state (â€˜past_pred[..., 0:8]â€˜) and current covariates (â€˜cov_tâ€˜). The bout state â€˜cov_t[..., 0]â€˜ acts as a switch, modulating the neuronsâ€™ update dynamics based on whether a bout is active. Each neuron i in this group (indices 0-7) is driven by a specific visual input: â€˜cov_t[..., 1+i]â€˜ (e.g., neuron 0 depends on â€˜cov_t[..., 1]â€˜, neuron 1 on â€˜cov_t[..., 2]â€˜, and so on). 2. **LPT Neurons (Indices 8:32):** Depend only on the **newly computed EPT activations** (current stepâ€™s neurons 0-7), not on their own past activations from past_pred. The â€˜CONNECTIVITYâ€˜ variable below indicates sparse connections: â€˜CONNECTIVITY[i][j]â€˜ is true if the i-th neuron within the LPT group depends on the j-th neuron within the EPT group. Since i ranges from 0-23 and j from 0-7, â€˜CONNECTIVITY[0][j]â€˜ refers to dependencies of neuron 8 (first LPT neuron), â€˜CONNECTIVITY[1][j]â€˜ refers to dependencies of neuron 9 (second LPT neuron), and so on. 

8Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish 

3. **aHB/nMLF Neurons (Indices 32:36):** Depend on their past state (â€˜past_pred[..., 32:36]â€˜) and the **newly computed** LPT activations (current step). 

... and the following connectivity matrix was included:  

> 1

# NOTE: Connectivity from EPT (cols) to LPT (rows)  

> 2

CONNECTIVITY = [  

> 3

[False , True , True , True , True , True , True , True ],  

> 4

[False , True , True , True , True , False , True , False ],  

> 5

[True , True , True , False , True , True , True , True ],  

> 6

[True , True , True , True , True , False , True , False ],  

> 7

[True , False , True , True , True , False , False , False ],  

> 8

[False , False , True , False , True , False , False , False ],  

> 9

[False , True , True , False , False , False , False , True ],  

> 10

[False , True , True , True , False , True , True , True ],  

> 11

[False , True , True , True , False , False , True , True ],  

> 12

[False , False , False , False , True , True , False , False ],  

> 13

[False , True , True , True , True , True , False , True ],  

> 14

[False , True , True , True , True , True , False , True ],  

> 15

[True , True , True , True , False , True , True , True ],  

> 16

[True , False , True , False , False , True , True , True ],  

> 17

[True , True , True , True , True , True , True , False ],  

> 18

[True , False , True , False , True , True , True , True ],  

> 19

[True , False , False , False , True , False , True , True ],  

> 20

[True , False , False , False , False , False , True , False ],  

> 21

[False , False , False , True , False , True , True , False ],  

> 22

[False , True , True , True , False , True , True , True ],  

> 23

[False , False , True , True , False , True , True , True ],  

> 24

[True , True , False , False , False , False , False , False ],  

> 25

[True , True , False , True , False , True , True , True ],  

> 26

[True , True , False , True , False , True , True , True ]] 

9Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish 

F. Code Listings 

F.1. Initial Code for Tree Search 

The initial code, which the tree search was tasked to modify:  

> 1

# 

> 2

# CONSTANTS  

> 3

# 

> 45

BATCH_SIZE = 512  

> 6

MAX_STEPS = 500_000  

> 7

SCHEDULED_SAMPLING_START_PROB = 1.0  

> 8

SCHEDULED_SAMPLING_FINAL_PROB = 0.01  

> 9

SCHEDULED_SAMPLING_DECAY_STEPS = 100_000  

> 10

EARLY_STOP_PATIENCE = 20  

> 11

MIN_DELTA = 1e-6  

> 12

LEARNING_RATE = 1e-3  

> 13 14 15

# 

> 16

# MODEL  

> 17

# 

> 18 19

class Candidate( nn .Module):  

> 20

"""Candidate transition model for neural activity."""  

> 21 22

@nn.compact  

> 23

def __call__( self , past_pred, cov_t, train= False ):  

> 24

"""  

> 25

Args:  

> 26

past_pred: (Batch, 36) - Previous neural state [0, 1].  

> 27

cov_t: (Batch, 9) - Index 0 is Bout {0,1}, 1-8 are Visual [0,1].  

> 28

train: bool - Training mode flag.  

> 29

Returns:  

> 30

(Batch, 36) - Predicted next state.  

> 31

"""  

> 32

bout_state = cov_t[:, 0:1] # Shape (B, 1)  

> 33

visual_in = cov_t[:, 1:] # Shape (B, 8)  

> 34 35

# TODO: Implement your architecture here.  

> 36 37

# Identity baseline: predicts no change.  

> 38

return past_pred  

> 39 40 41

transition_model = Candidate() # NOTE: Do keep this line as-is.  

> 42 43 44

# 

> 45

# OPTIMIZER  

> 46

# 

> 47 48

optimizer = optax .adamw(learning_rate=LEARNING_RATE, weight_decay=1e-4) 

10 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish 

F.2. Solution ts 001  

> 1

import jax .numpy as jnp  

> 2

import flax .linen as nn  

> 3

import optax  

> 45

# 

> 6

# CONSTANTS  

> 7

# 

> 89

BATCH_SIZE = 512  

> 10

MAX_STEPS = 500_000  

> 11

# High start prob uses ground truth; low final prob enforces autoregressive stability.  

> 12

SCHEDULED_SAMPLING_START_PROB = 1.0  

> 13

SCHEDULED_SAMPLING_FINAL_PROB = 0.02  

> 14

SCHEDULED_SAMPLING_DECAY_STEPS = 150_000  

> 15

EARLY_STOP_PATIENCE = 40  

> 16

MIN_DELTA = 1e-6  

> 17

LEARNING_RATE = 1e-3  

> 18 19 20

# 

> 21

# MODEL  

> 22

# 

> 23 24

class Candidate( nn .Module):  

> 25

"""Residual transition model for neural activity."""  

> 26 27

@nn.compact  

> 28

def __call__( self , past_pred, cov_t, train= False ):  

> 29

"""  

> 30

Args:  

> 31

past_pred: (Batch, 36) - Previous neural state.  

> 32

cov_t: (Batch, 9) - Bout switch and Visual inputs.  

> 33

train: bool - Training mode.  

> 34

Returns:  

> 35

(Batch, 36) - Predicted next state.  

> 36

"""  

> 37

# Combine previous state and external drivers  

> 38

# Shape: (Batch, 36 + 9) = (Batch, 45)  

> 39

x = jnp .concatenate([past_pred, cov_t], axis=-1)  

> 40 41

# MLP for state transition dynamics  

> 42

h = nn .Dense(512)(x)  

> 43

h = nn .LayerNorm()(h)  

> 44

h = nn .silu(h)  

> 45 46

h = nn .Dense(512)(h)  

> 47

h = nn .LayerNorm()(h)  

> 48

h = nn .silu(h)  

> 49 50

# Predict the delta (residual). Initialize with zeros so  

> 51

# the model starts as an identity function baseline.  

> 52

delta = nn .Dense(36, kernel_init= nn .initializers.zeros)(h)  

> 53 54

# Autoregressive update: y_t = y_{t-1} + delta  

> 55

next_state = past_pred + delta  

> 56 57

# Ensure valid range [0, 1]  

> 58

return jnp .clip(next_state, 0.0, 1.0)  

> 59 60 61

transition_model = Candidate() # NOTE: Do keep this line as-is.  

> 62 63 64

# 

> 65

# OPTIMIZER 

11 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish  

> 66

# 

> 67 68

# Learning rate schedule with warmup and cosine decay for stability  

> 69

lr_schedule = optax .warmup_cosine_decay_schedule(  

> 70

init_value=1e-6,  

> 71

peak_value=LEARNING_RATE,  

> 72

warmup_steps=2000,  

> 73

decay_steps=MAX_STEPS,  

> 74

) 

> 75 76

optimizer = optax .adamw(learning_rate=lr_schedule, weight_decay=1e-4) 

F.3. Solution ts 422  

> 1

import jax  

> 2

import jax .numpy as jnp  

> 3

import flax .linen as nn  

> 4

import optax  

> 56

# 

> 7

# CONSTANTS  

> 8

# 

> 910

BATCH_SIZE = 512  

> 11

MAX_STEPS = 500_000  

> 12

# Scheduled sampling enforces autoregressive stability.  

> 13

# We decay slightly slower to allow the model to learn the base rules first.  

> 14

SCHEDULED_SAMPLING_START_PROB = 1.0  

> 15

SCHEDULED_SAMPLING_FINAL_PROB = 0.05  

> 16

SCHEDULED_SAMPLING_DECAY_STEPS = 300_000  

> 17

EARLY_STOP_PATIENCE = 1000  

> 18

MIN_DELTA = 1e-9  

> 19

LEARNING_RATE = 8e-4  

> 20 21

# 

> 22

# MODEL  

> 23

# 

> 24 25

class ResidualBlock( nn .Module):  

> 26

features: int  

> 27 28

@nn.compact  

> 29

def __call__( self , x, context):  

> 30

# Pre-norm architecture for stability  

> 31

norm = nn .LayerNorm()(x)  

> 32 33

# Gated Linear Unit (GEGLU variant)  

> 34

# Use context to modulate the hidden representation  

> 35

h = nn .Dense( self .features * 2, kernel_init= nn .initializers.orthogonal())(norm)  

> 36

h_val, h_gate = jnp .split(h, 2, axis=-1)  

> 37

h = h_val * nn .gelu(h_gate)  

> 38 39

# FiLM modulation using context  

> 40

gamma = nn .Dense( self .features, kernel_init= nn .initializers.zeros)(context)  

> 41

beta = nn .Dense( self .features, kernel_init= nn .initializers.zeros)(context)  

> 42

h = h * (1.0 + gamma) + beta  

> 43 44

h = nn .Dense( self .features, kernel_init= nn .initializers.orthogonal())(h)  

> 45

return x + h  

> 46 47

class Candidate( nn .Module):  

> 48

"""  

> 49

Refined Gated Dynamics Model. 

12 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish  

> 50

Models neural transitions using learned time-constants and target states,  

> 51

heavily conditioned on the behavioral â€™Boutâ€™ switch.  

> 52

"""  

> 53 54

@nn.compact  

> 55

def __call__( self , past_pred, cov_t, train= False ):  

> 56

B, N = past_pred.shape # N=36  

> 57 58

# 1. Input Processing  

> 59

# Split covariates: Bout State (switch) and Visual Input (driving signal)  

> 60

bout = cov_t[:, 0:1]  

> 61

visual = cov_t[:, 1:]  

> 62 63

# High-dimensional visual embedding  

> 64

v_enc = nn .Dense(128)(visual)  

> 65

v_enc = nn .gelu(v_enc)  

> 66 67

# Context incorporates the Bout switch explicitly  

> 68

# We use the bout flag to gate visual information  

> 69

ctx = jnp .concatenate([  

> 70

v_enc,  

> 71

bout,  

> 72

v_enc * bout, # Bout-specific visual processing  

> 73

v_enc * (1.0 - bout) # Non-bout specific visual processing  

> 74

], axis=-1)  

> 75

ctx = nn .Dense(128)(ctx)  

> 76

ctx = nn .gelu(ctx)  

> 77 78

# 2. Latent Projection  

> 79

# Project current neural state to a latent space  

> 80

latent_dim = 256  

> 81

x = nn .Dense(latent_dim, kernel_init= nn .initializers.orthogonal())(past_pred)  

> 82 83

# Apply N residual blocks with conditional modulation  

> 84

for i in range (3):  

> 85

x = ResidualBlock(features=latent_dim, name=f"res_block_{i}")(x, ctx)  

> 86 87

# 3. Liquid-Style Transition  

> 88

# We predict a â€™gateâ€™ (alpha) and a â€™targetâ€™ state.  

> 89

# This ensures the output is always a convex combination of previous state  

> 90

# and a target, guaranteeing stability in [0, 1].  

> 91 92

# Gate: How much the state changes (1/tau)  

> 93

# We initialize it to be small to favor stability early on  

> 94

gate_logit = nn .Dense(N, kernel_init= nn .initializers.zeros)(x)  

> 95

alpha = nn .sigmoid(gate_logit - 3.0)  

> 96 97

# Target: What the neurons are striving towards  

> 98

target = nn .Dense(N, kernel_init= nn .initializers.orthogonal())(x)  

> 99

target = nn .sigmoid(target) # Ensure target is in [0, 1]  

> 100 101

# Direct Delta: Small additive term for fast transient dynamics  

> 102

# This helps capture sudden shifts that a convex combination might miss.  

> 103

delta = nn .Dense(N, kernel_init= nn .initializers.zeros)(x)  

> 104

delta = jnp .tanh(delta) * 0.05  

> 105 106

# Update rule: y_t = (1-alpha)*y_{t-1} + alpha*target + delta  

> 107

next_state = (1.0 - alpha) * past_pred + alpha * target + delta  

> 108 109

return jnp .clip(next_state, 0.0, 1.0)  

> 110 111

transition_model = Candidate() # NOTE: Do keep this line as-is.  

> 112 113

# 

> 114

# OPTIMIZER  

> 115

#

> 116

13 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish  

> 117

lr_schedule = optax .warmup_cosine_decay_schedule(  

> 118

init_value=1e-7,  

> 119

peak_value=LEARNING_RATE,  

> 120

warmup_steps=4000,  

> 121

decay_steps=MAX_STEPS,  

> 122

end_value=1e-6  

> 123

) 

> 124 125

optimizer = optax .chain(  

> 126

optax .clip_by_global_norm(0.5), # Tighter clipping for autoregressive stability  

> 127

optax .adamw(  

> 128

learning_rate=lr_schedule,  

> 129

weight_decay=2e-3, # Increased weight decay for OOD robustness  

> 130

b1=0.9,  

> 131

b2=0.999  

> 132

) 

> 133

)

F.4. Solution sts 445  

> 1

import jax  

> 2

import jax .numpy as jnp  

> 3

import flax .linen as nn  

> 4

import optax  

> 56

# 

> 7

# CONSTANTS  

> 8

# 

> 910

BATCH_SIZE = 512  

> 11

MAX_STEPS = 500_000  

> 12

SCHEDULED_SAMPLING_START_PROB = 1.0  

> 13

SCHEDULED_SAMPLING_FINAL_PROB = 0.0  

> 14

SCHEDULED_SAMPLING_DECAY_STEPS = 400_000  

> 15

EARLY_STOP_PATIENCE = 1500  

> 16

MIN_DELTA = 1e-10  

> 17

LEARNING_RATE = 1e-3  

> 18 19

# NOTE: Connectivity from EPT (cols) to LPT (rows)  

> 20

CONNECTIVITY = [  

> 21

[False , True , True , True , True , True , True , True ],  

> 22

[False , True , True , True , True , False , True , False ],  

> 23

[True , True , True , False , True , True , True , True ],  

> 24

[True , True , True , True , True , False , True , False ],  

> 25

[True , False , True , True , True , False , False , False ],  

> 26

[False , False , True , False , True , False , False , False ],  

> 27

[False , True , True , False , False , False , False , True ],  

> 28

[False , True , True , True , False , True , True , True ],  

> 29

[False , True , True , True , False , False , True , True ],  

> 30

[False , False , False , False , True , True , False , False ],  

> 31

[False , True , True , True , True , True , False , True ],  

> 32

[False , True , True , True , True , True , False , True ],  

> 33

[True , True , True , True , False , True , True , True ],  

> 34

[True , False , True , False , False , True , True , True ],  

> 35

[True , True , True , True , True , True , True , False ],  

> 36

[True , False , True , False , True , True , True , True ],  

> 37

[True , False , False , False , True , False , True , True ],  

> 38

[True , False , False , False , False , False , True , False ],  

> 39

[False , False , False , True , False , True , True , False ],  

> 40

[False , True , True , True , False , True , True , True ],  

> 41

[False , False , True , True , False , True , True , True ],  

> 42

[True , True , False , False , False , False , False , False ],  

> 43

[True , True , False , True , False , True , True , True ], 

14 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish  

> 44

[True , True , False , True , False , True , True , True ]]  

> 45 46

# 

> 47

# MODEL  

> 48

# 

> 49 50

class Candidate( nn .Module):  

> 51

"""Hierarchical Neural Dynamic Model with Sparse Connectivity."""  

> 52 53

@nn.compact  

> 54

def __call__( self , past_pred, cov_t, train= False ):  

> 55

B = past_pred.shape[0]  

> 56 57

# 1. Decomposition  

> 58

ept_past = past_pred[:, 0:8] # (B, 8)  

> 59

ahb_past = past_pred[:, 32:36] # (B, 4)  

> 60

bout_state = cov_t[:, 0:1] # (B, 1)  

> 61

visual_in = cov_t[:, 1:9] # (B, 8)  

> 62 63

# 2. EPT Update (Neurons 0-7)  

> 64

# Shared dynamics rule across all 8 neurons for OOD robustness.  

> 65

# Inputs: past state, specific visual channel, and global bout state.  

> 66

ept_combined = jnp .stack([ept_past, visual_in, jnp .broadcast_to(bout_state, (B, 8))], axis=-1)  

> 67 68

# Capture second-order interactions [y, v, b, y*v, y*b, v*b]  

> 69

ept_feat = jnp .concatenate([  

> 70

ept_combined,  

> 71

ept_combined[..., [0]] * ept_combined[..., [1]],  

> 72

ept_combined[..., [0]] * ept_combined[..., [2]],  

> 73

ept_combined[..., [1]] * ept_combined[..., [2]]  

> 74

], axis=-1) # (B, 8, 6)  

> 75 76

# Shared MLP rule  

> 77

e = nn .Dense(128)(ept_feat)  

> 78

e = nn .leaky_relu(e)  

> 79

e = nn .Dense(128)(e)  

> 80

e = nn .leaky_relu(e)  

> 81 82

# Gated leaky integration for autoregressive stability  

> 83

ept_gate = nn .sigmoid( nn .Dense(1)(e)).squeeze(-1)  

> 84

ept_candidate = nn .sigmoid( nn .Dense(1)(e)).squeeze(-1)  

> 85 86

# Per-neuron individual gain/bias to capture phenotypic variation  

> 87

ept_gain = self .param(â€™ept_gainâ€™, nn .initializers.ones, (8,))  

> 88

ept_out = (1.0 - ept_gate) * ept_past + ept_gate * (ept_candidate * ept_gain)  

> 89 90

# 3. LPT Update (Neurons 8-31)  

> 91

# Feed-forward mapping from current EPT, enforcing sparse CONNECTIVITY.  

> 92

mask = jnp .array(CONNECTIVITY) # (24 LPT rows, 8 EPT cols)  

> 93 94

# Two-layer sparse projection  

> 95

lpt_w1 = self .param(â€™lpt_w1â€™, nn .initializers.glorot_uniform(), (24, 8, 32))  

> 96

lpt_b1 = self .param(â€™lpt_b1â€™, nn .initializers.zeros, (24, 32))  

> 97 98

# Each LPT neuron integrates its subset of EPT inputs into a latent space  

> 99

lpt_h = jnp .einsum(â€™bi,nik->bnkâ€™, ept_out, lpt_w1 * mask[..., None ]) + lpt_b1  

> 100

lpt_h = nn .leaky_relu(lpt_h)  

> 101 102

lpt_w2 = self .param(â€™lpt_w2â€™, nn .initializers.glorot_uniform(), (24, 32, 1))  

> 103

lpt_b2 = self .param(â€™lpt_b2â€™, nn .initializers.zeros, (24,))  

> 104

lpt_out = jnp .einsum(â€™bnk,nko->bnâ€™, lpt_h, lpt_w2) + lpt_b2  

> 105

lpt_out = nn .sigmoid(lpt_out)  

> 106 107

# 4. aHB/nMLF Update (Neurons 32-35)  

> 108

# Temporal integrators depending on past state and newly computed LPT.  

> 109

ahb_in = jnp .concatenate([ahb_past, lpt_out], axis=-1) # (B, 4 + 24) 

> 110

15 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish  

> 111

a = nn .Dense(256)(ahb_in)  

> 112

a = nn .leaky_relu(a)  

> 113

a = nn .Dense(256)(a)  

> 114

a = nn .leaky_relu(a)  

> 115 116

ahb_gate = nn .sigmoid( nn .Dense(4)(a))  

> 117

ahb_candidate = nn .sigmoid( nn .Dense(4)(a))  

> 118

ahb_out = (1.0 - ahb_gate) * ahb_past + ahb_gate * ahb_candidate  

> 119 120

return jnp .concatenate([ept_out, lpt_out, ahb_out], axis=-1)  

> 121 122

transition_model = Candidate()  

> 123 124

# 

> 125

# OPTIMIZER  

> 126

# 

> 127 128

lr_schedule = optax .cosine_onecycle_schedule(  

> 129

transition_steps=MAX_STEPS,  

> 130

peak_value=LEARNING_RATE,  

> 131

pct_start=0.1,  

> 132

div_factor=10,  

> 133

final_div_factor=100  

> 134

) 

> 135 136

optimizer = optax .adamw(  

> 137

learning_rate=lr_schedule,  

> 138

weight_decay=1e-5  

> 139

)

16 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish 

G. Supplementary Figures a. 

> b.
> Score
> Score

Figure S1 | Full search trees for automated model discovery. Visualization of the complete evolutionary lineage explored by the LLM-guided tree search. Nodes represent executable Python code (candidate models), and edges represent mutations generated by the LLM. Node color indicates the score, with lighter colors representing lower error. a. The unconstrained search tree ( ts ). b. The structure-constrained search tree ( sts ). 

> 17

Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish ePTSLePTALePTILePTPLePTSRePTARePTIRePTPRlPToLB1lPToLB2lPTBL1lPTBL2lPTiLBlPTiLoBlPTiLMmlPTMLm1lPTMLm2lPToLMmlPTSL1lPTSL2lPToRB1lPToRB2lPTBR1lPTBR2lPTiRBlPTiRoBlPTiRMmlPTMRm1lPTMRm2lPToRMmlPTSR1lPTSR2aHBLaHBRnMLFLnMLFR32 steps Not Swimming gt ts 422 32 steps Swimming gt ts 422 32 steps Not Swimming gt sts 445 32 steps Swimming gt sts 445 a. b. c. d. 

Figure S2 | Impulse response dynamics. We probe the recovery of dynamic mechanisms by the discovered models by initializing all neurons to a saturated state ( a0 = 1) and evolving the system for 32 steps under â€œNot Swimmingâ€ (Rest) and â€œSwimmingâ€ conditions. a, b. ts 422 (purple) exhibits erroneous oscillatory behavior, deviating significantly from the ground truth (blue). c, d. sts 445 (green) faithfully reproduces the ground truth state-dependent decay profiles that distinguish swimming from resting dynamics. 

18 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish 

H. Supplementary Tables 

Table S1 | Performance comparison between best-scoring tree search solutions. In addition to predictive accuracy (MAE), we report mechanistic verification metrics defined in Appendix C: the Frobenius norm of the effective connectivity mismatch ( LJac ) and the impulse response error ( LIR )for both resting and swimming states. The structure-constrained solution ( sts 445 ) demonstrates superior mechanistic fidelity compared to the unconstrained one ( ts 422 ). 

ts 422 sts 445 

Test MAE â†“ 0.00005 0.00001 

Holdout MAE â†“ 0.13995 0.00027 

LJac â†“ 8.33 0.26 

Lrest IR â†“ 0.27 0.04 

Lswim IR â†“ 0.29 0.04 

Table S2 | Population statistics (median [IQR]) for the top 50 highest-scoring models of each tree search. Aggregated metrics across models discovered during the unconstrained ( ts ) and structure-constrained ( sts ) tree searches. The constrained search consistently yields models with lower Jacobian error ( Ljac ) and more accurate dynamics ( LIR ), indicating that the structural prior effectively regularizes the search space towards higher mechanistic fidelity. 

ts sts 

Test MAE â†“ 0.00009 [0.00008, 0.00009] 0.00004 [0.00002, 0.00005] 

Holdout MAE â†“ 0.00833 [0.00636, 0.02349] 0.00030 [0.00025, 0.00073] 

LJac â†“ 7.58 [7.55, 7.73] 0.34 [0.23, 0.45] 

Lrest IR â†“ 0.14 [0.10, 0.21] 0.03 [0.02, 0.04] 

Lswim IR â†“ 0.12 [0.09, 0.18] 0.03 [0.02, 0.04]   

> 19 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

References 

J. Achterberg, D. Akarca, D. Strouse, J. Duncan, and D. E. Astle. Spatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings. Nature Machine Intelligence , 2023. URL https://doi.org/10.1038/s42256-023-00748-9 .E. AygÃ¼n, A. Belyaeva, G. Comanici, M. Coram, H. Cui, J. Garrison, R. J. A. Kast, C. Y. McLean, P. Nor-gaard, Z. Shamsi, D. Smalling, J. Thompson, S. Venugopalan, B. P. Williams, C. He, S. Martinson, M. Plomecka, L. Wei, Y. Zhou, Q.-Z. Zhu, M. Abraham, E. Brand, A. Bulanova, J. A. Cardille, C. Co, S. Ellsworth, G. Joseph, M. Kane, R. Krueger, J. Kartiwa, D. Liebling, J.-M. Lueckmann, P. Raccuglia, X. Wang, K. Chou, J. Manyika, Y. Matias, J. C. Platt, L. Dorfman, S. Mourad, and M. P. Brenner. An AI system to help scientists write expert-level empirical software. arXiv preprint , 2025. URL 

https://doi.org/10.48550/arXiv.2509.06503 .M. Beiran and A. Litwin-Kumar. Prediction of neural activity in connectome-constrained recurrent networks. Nature Neuroscience , 2025. URL https://doi.org/10.1038/s41593-025-02080-4 .P. S. Castro, N. Tomasev, A. Anand, N. Sharma, R. Mohanta, A. Dev, K. Perlin, S. Jain, K. Levin, N. Elteto, W. Dabney, A. Novikov, G. C. Turner, M. K. Eckstein, N. D. Daw, K. J. Miller, and K. Stachenfeld. Discovering symbolic cognitive models from human and animal behavior. In International Conference on Machine Learning , 2025. URL https://openreview.net/forum?id=dhRXGWJ027 .S.-A. Chen, C.-L. Li, N. Yoder, S. O. Arik, and T. Pfister. TSMixer: An all-MLP architecture for time series forecasting. Transactions on Machine Learning Research , 2023. URL https://openreview. net/forum?id=wbpxTuXgm0 .A. Das, W. Kong, A. Leach, S. Mathur, R. Sen, and R. Yu. Long-term forecasting with TiDE. Transactions on Machine Learning Research , 2023. URL https://openreview.net/forum?id=pCbC3aQB5W .S. Duan, L. L. Dong, and I. Fiete. From synapses to dynamics: Obtaining function from structure in a connectome constrained model of the head direction circuit. bioRxiv preprint , 2025. URL 

https://doi.org/10.1101/2025.05.26.655406 .Google. Gemini 3 Flash: Frontier intelligence built for speed, December 17 2025. URL https: //blog.google/products-and-platforms/products/gemini/gemini-3-flash/ . The Keyword. Y. Han, T. A. Poggio, and B. Cheung. System identification of neural systems: If we got it right, would we know? In International Conference on Machine Learning , 2023. URL https://openreview.net/ forum?id=NkTEhPQCjg .Z. Jiang, D. Schmidt, D. Srikanth, D. Xu, I. Kaplan, D. Jacenko, and Y. Wu. Aide: Ai-driven exploration in the space of code. arXiv preprint , 2025. URL https://doi.org/10.48550/arXiv.2502.13138 .E. Jonas and K. P. Kording. Could a neuroscientist understand a microprocessor? PLoS computational biology , 2017. URL https://doi.org/10.1371/journal.pcbi.1005268 .R. T. Lange, Y. Imajuku, and E. Cetin. ShinkaEvolve: Towards open-ended and sample-efficient program evolution. arXiv preprint , 2025. URL https://doi.org/10.48550/arXiv.2509.19349 .J. K. Lappalainen, F. D. Tschopp, S. Prakhya, M. McGill, A. Nern, K. Shinomiya, S.-y. Takemura, E. Grunt-man, J. H. Macke, and S. C. Turaga. Connectome-constrained networks predict neural activity across the fly visual system. Nature , 2024. URL https://doi.org/10.1038/s41586-024-07939-3 .  

> 20 Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish

V. Lobato-Rios, S. T. Ramalingasetty, P. G. Ã–zdil, J. Arreguit, A. J. Ijspeert, and P. Ramdya. Neu-roMechFly, a neuromechanical model of adult drosophila melanogaster. Nature Methods , 2022. URL https://doi.org/10.1038/s41592-022-01466-7 .I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7 .L. Mi, R. Xu, S. Prakhya, A. Lin, N. Shavit, A. Samuel, and S. C. Turaga. Connectome-constrained latent variable model of whole-brain neural activity. In International Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=CJzi3dRlJE-.A. Novikov, N. V Ëœu, M. Eisenberger, E. Dupont, P.-S. Huang, A. Z. Wagner, S. Shirobokov, B. Kozlovskii, F. J. R. Ruiz, A. Mehrabian, M. P. Kumar, A. See, S. Chaudhuri, G. Holland, A. Davies, S. Nowozin, P. Kohli, and M. Balog. Alphaevolve: A coding agent for scientific and algorithmic discovery. arXiv preprint , 2025. URL https://doi.org/10.48550/arXiv.2506.13131 .B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar, E. Dupont, F. J. R. Ruiz, J. S. Ellenberg, P. Wang, O. Fawzi, P. Kohli, and A. Fawzi. Mathematical discoveries from program search with large language models. Nature , 2024. URL https://doi.org/10.1038/s41586-023-06924-6 .D. Salinas, V. Flunkert, J. Gasthaus, and T. Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting , 2020. URL https://doi. org/10.1016/j.ijforecast.2019.07.001 .G. P. Sarma, C. W. Lee, T. Portegys, V. Ghayoomie, T. Jacobs, B. Alicea, M. Cantarelli, M. Currie, R. C. Gerkin, S. Gingell, P. Gleeson, R. Gordon, R. M. Hasani, G. Idili, S. Khayrulin, D. Lung, A. Palyanov, M. Watts, and S. D. Larson. OpenWorm: Overview and recent advances in integrative biological simulation of caenorhabditis elegans. Philosophical Transactions of the Royal Society B , 2018. URL 

https://doi.org/10.1098/rstb.2017.0382 .M. Schmidt and H. Lipson. Distilling free-form natural laws from experimental data. Science , 324, 2009. URL https://doi.org/10.1126/science.1165893 .R. Tilbury, D. Kwon, A. Haydaroglu, J. Ratliff, V. Schmutz, M. Carandini, K. Miller, K. Stachenfeld, and K. D. Harris. Characterizing neuronal population geometry with AI equation discovery. bioRxiv preprint , 2025. URL https://doi.org/10.1101/2025.11.12.688086 .R. Vaxenburg, I. Siwanowicz, J. Merel, A. A. Robie, C. Morrow, G. Novati, Z. Stefanidi, G.-J. Both, G. M. Card, M. B. Reiser, M. M. Botvinick, K. M. Branson, Y. Tassa, and S. C. Turaga. Whole-body physics simulation of fruit fly locomotion. Nature , 2025. URL https://doi.org/10.1038/ s41586-025-09029-4 .A. Zeng, M. Chen, L. Zhang, and Q. Xu. Are transformers effective for time series forecasting? In AAAI Conference on Artificial Intelligence , 2023. URL https://doi.org/10.1609/aaai.v37i9.26317 .M. Zhao, N. Wang, X. Jiang, X. Ma, H. Ma, G. He, K. Du, L. Ma, and T. Huang. An integrative data-driven model simulating c. elegans brain, body and environment interactions. Nature Computational Science , 2024. URL https://doi.org/10.1038/s43588-024-00738-w .B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations , 2017. URL https://openreview.net/forum?id=r1Ue8Hcxg .