Title: Rational ANOVA Networks

URL Source: https://arxiv.org/pdf/2602.04006v1

Published Time: Thu, 05 Feb 2026 01:13:31 GMT

Number of Pages: 27

Markdown Content:
# Rational ANOVA Networks 

Jusheng Zhang 1 Ningyuan Liu 1 Qinhan Lyu 1 Jing Yang 1 Keze Wang 1

## Abstract 

Deep neural networks typically treat nonlinear-ities as fixed primitives (e.g., ReLU), limiting both interpretability and the granularity of control over the induced function class. While recent additive models (like KANs) attempt to address this using splines, they often suffer from com-putational inefficiency and boundary instability. We propose the Rational-ANOVA Network (RAN), a foundational architecture grounded in functional ANOVA decomposition and Pad ´e-style rational approximation. RAN models f (x) as a composition of main effects and sparse pairwise interactions, where each component is parameter-ized by a stable, learnable rational unit. Crucially, we enforce a strictly positive denominator, which avoids poles and numerical instability while capturing sharp transitions and near-singular behaviors more efficiently than polynomial bases. This ANOVA structure provides an explicit low-order interaction bias for data efficiency and interpretability, while the rational parame-terization significantly improves extrapolation. Across controlled function benchmarks and vision classification tasks (e.g., CIFAR-10) under matched parameter and compute budgets, RAN matches or surpasses parameter-matched MLPs and learnable-activation baselines, with better stability and throughput.Code is available at 

https://github.com/jushengzhang/ Rational-ANOVA-Networks.git .

## 1. Introduction 

Deep neural networks have become the default function approximators across domains(Schmidhuber, 2015; DeVore et al., 2020; Lin et al., 2021; Kovachki et al., 2023), from vision and language to scientific computing. Yet a core in-gredient of this success, i.e., nonlinearity, is still treated as a  

> 1Sun Yat-sen University, China. Correspondence to: Jusheng Zhang <zhangjs@mail.sysu.edu.cn >.
> Preprint. February 5, 2026.

surprisingly fixed and coarse design choice in most archi-tectures. In practice, widely used backbones rely on low-parameter activations (e.g., ReLU (Agarap, 2018; Xu et al., 2020), GELU (Hendrycks & Gimpel, 2016), SiLU (Elfwing et al., 2017)) whose role is largely to inject generic curva-ture, while the burden of expressivity is delegated to scaling width and depth. This design is robust and efficient but exposes a persistent limitation: when the target mapping in-volves sharp transitions, near-singular regimes, or long-tail behaviors, fixed activations often require substantial over-parameterization to fit the shape (Glorot & Bengio, 2010; Klambauer et al., 2017). Furthermore, deep composition can amplify activation outliers (e.g., heavy-tailed feature excursions) into unstable optimization dynamics. A natural response is to make the nonlinearity itself learnable. Recent progress, exemplified by spline-parameterized networks such as Kolmogorov–Arnold Net-works (KANs) (Liu et al., 2024b;a), suggests that learn-able nonlinearities can improve interpretability and alter the accuracy–efficiency trade-off. However, turning nonlinear-ity into a high-capacity object raises a practical challenge: deep networks are not merely function classes but carefully tuned parameterizations whose stability depends on activa-tion statistics and residual pathways. A highly expressive nonlinearity may inadvertently degrade conditioning or in-troduce numerical fragility when composed repeatedly in depth. This tension is especially salient if we seek a base architecture, i.e., a drop-in primitive that can be trained deeply and compared fairly against MLPs and KANs under matched budgets. In this work, we propose Rational-ANOVA Networks (RAN) , a base architecture that makes nonlinearity learn-able while maintaining stable and controllable deep training. RAN is motivated by two classical ideas: Pad ´e-style rational approximation (Molina et al., 2019; Boull ´e et al., 2020; Palm et al., 2018) and functional ANOVA decomposition. Instead of relying on a fixed pointwise activation, RAN models a multivariate function via a low-order additive structure: 

f (x) ≈

> d

X

> i=1

ri(xi) + X

> (i,j )∈S

rij (xi, x j ), (1) where the first term captures main effects and the second term captures sparse pairwise interactions over a controlled set S. This set S can be chosen to match a target compute 

Preprint. Under review.           

> arXiv:2602.04006v1 [cs.LG] 3 Feb 2026 Rational ANOVA Networks
> Figure 1. Comparison of RAN with MLPs and KANs. MLPs use fixed activations; KANs learn edge splines. RAN employs learnable rational units in a Functional ANOVA topology, decomposing finto main effects ( Pi/Q i) and sparse interactions ( Pij /Q ij ).

or parameter budget, yielding a controllable interaction topology. As shown in Figure 1, each component ri and 

rij is parameterized by a learnable rational unit—a quotient of low-degree polynomials. Figure 1 contrasts this design with MLPs and KANs: while MLPs place fixed nonlineari-ties on nodes and KANs place learnable splines on edges, RAN employs learnable rational units within an explicit low-order interaction topology. A key feature of RAN is its deep compatibility. To ensure numerical stability, we param-eterize denominators to be strictly positive (specifically via 

1 + softplus (·)), effectively avoiding poles while retaining the expressive “Pad ´e-like” modeling capability (Trefethen, 2019; Telgarsky, 2017a). Furthermore, we employ residual-style gating (shown in Figure 2), allowing each rational unit to initialize near an identity map ( y ≈ x) and gradually increase its effective nonlinearity during training. This pre-vents the early-stage instability often caused by aggressive curvature in rational functions. We evaluate RAN on controlled function benchmarks and physics-inspired problems under matched parameter and compute budgets against MLPs and KANs . Our results show that rational parameterization and sparse ANOVA structure are complementary: rational units handle non-smooth or near-singular regimes, while sparse interactions balance expressivity and generalization. 

## 2. Rational-ANOVA Networks 

Setup and notation. Let x ∈ Rd denote the input and 

fθ (x) the model output (logits for classification or a scalar for regression) (Rumelhart et al., 1986). We design Rational-ANOVA Networks (RAN) as a base network primitive with two design goals: (i) to enable strictly fair comparisons with parameter-matched MLPs and KANs, and (ii) to serve as a drop-in replacement for FFNs in pretrained backbones by modifying only the nonlinearity parameterization while preserving linear projections and activation statistics. 

2.1. ANOVA-Induced Architecture Low-order functional decomposition. RAN parameter-izes multivariate mappings via an explicit low-order additive structure(conceptually contrasted with MLPs and KANs in Figure 1):         

> fθ(x) =
> dX
> i=1
> ri(xi)
> |{z }
> main effects
> +X
> (i,j )∈S
> rij (xi, x j)
> |{z }
> sparse pairwise effects
> (optionally followed by a linear readout). (2)

Here, S ⊆ { (i, j ) : 1 ≤ i < j ≤ d} is a controlled interac-tion set . This design makes the interaction topology explicit and budgetable : increasing |S| adds second-order capac-ity without the combinatorial explosion of dense pairwise grids. Beyond efficiency, the sparsity of S also induces a structured influence geometry (as analyzed in Sec. 3), al-lowing cross-sample coupling to be explicitly analyzed and 2Rational ANOVA Networks 

Figure 2. Deep Rational-ANOVA Network (RAN) architecture. Left: A deep backbone of stacked residual blocks; each block performs sparse pairwise message passing (interactions) then node-wise updates. Right: Learnable rational units. R1D and R2D use residual gating 

y = x + α(R(x) − x) for identity initialization. Denominators are positive ( 1 + softplus (·)) for stability and pole-free composition. 

controlled through the interaction topology. 

Instantiation for supervised learning. For C-class clas-sification, we construct a feature vector by concatenating all main and pairwise outputs: 

ϕθ (x) = r1(x1), . . . , r d(xd),ri1j1 (xi1 , x j1 ), . . . , r iK jK (xiK , x jK ) ∈ Rd+K ,

(3) 

where K = |S| . Logits are produced by a linear head 

z(x) = W ϕ θ (x) + b. This yields a robust baseline where 

all nonlinearity lives inside learnable rational units , while global mixing remains linear (hence stable, interpretable, and compatible with pretrained linear projections if inserted into FFNs). Choosing the interaction set S. To ensure fair comparisons and avoid task-specific engineering, our default setting uses a fixed random S sampled once with a global seed and reused across runs. This isolates the inductive bias of rational parameterization combined with sparse low-order structure. We evaluate data-driven choices (e.g., correlation-based or gradient-based selection) only in ablations (see Appendix J). 

2.2. Learnable Rational Units 1D rational unit (main effects). Each main-effect func-tion ri : R → R is parameterized as a Pad ´e-style rational map: 

˜ri(x) = pi(x)

di(x) , pi(x) = 

> m

X

> a=0

αia xa,di(x) = 1 + softplus 

 nX

> b=1

βib xb

+ ε, 

(4) 

where (m, n ) are degrees and ε > 0. The constraint di(x) ≥

1 + ε prevents poles while retaining the expressive quotient form (see Appendix K for regularity proofs). In practice, low degrees are sufficient to capture sharp transitions and saturation effects that fixed activations often model only via increased width or depth. 

2D rational unit (pairwise effects). Each pairwise func-tion rij : R2 → R uses a low-degree bivariate rational form: 

˜rij (x, y ) = pij (x, y )

dij (x, y ) , pij (x, y ) = 

> T

X

> t=1

γij,t ψt(x, y ),dij (x, y ) = 1 + softplus 

 SX

> s=1

δij,s φs(x, y )



+ ε, 

(5) 

where {ψt} and {φs} are simple basis monomials (e.g., 

1, x, y, x 2, xy, y 2). Restricting the basis to low order is cru-cial for deep composition: it controls curvature and reduces optimization instability, while still providing a richer local shaping mechanism than fixed node-wise activations. 3Rational ANOVA Networks (a) MLP: Dense Entanglement 

> Input Space (Slice)
> ∇L

xuxo   

> Unintended Shift
> ∆f(xo)∝Kdense (xo, x u)
> Non-local updates (Global mixing)

(b) RAN: ANOVA Locality 

> Input Space (Slice)
> ∇L

xuxo    

> Stable (Zero Influence)
> Influence restricted by topology
> PKmain +PKpair sparse

(c) Rational Stability     

> Pre-activation x
> Target
> Squeezing
> Poly/Spline
> Rational
> Bounded deriv.
> d′(x)≈0(Eq. 18)
> Sharp transitions w/o poles Prevents gradient explosion

Figure 3. Learning Dynamics Comparison. Similar to how RLHF/DPO dynamics affect probability mass, we visualize how structural choices affect function updates ∆f . (a) MLP Entanglement: An update at xu (red arrow) causes uncontrolled shifts at distant xo

(dashed arrow) due to dense kernel mixing. (b) RAN Locality (Ours): The ANOVA structure (Eq. 17) disentangles interactions; updates at xu leave uncorrelated regions xo stable. (c) Rational Stability: Under strong “squeezing” gradients (steep transitions), polynomials oscillate (Runge’s phenomenon), while RAN’s rational units (Eq. 18) fit smoothly due to denominator-controlled derivatives. 

2.3. Deep Compatibility via Residual Gating Residual-style gating (identity-safe). Directly compos-ing expressive rational maps can be unstable if aggressive curvature is activated too early. We therefore wrap each rational map with a residual gate(as detailed in Figure 2): 

r(x) = x + α ·  ˜r(x) − x, (6) 

and similarly for rij with gate αij . Gates are initialized so that each unit starts near-identity ( r(x) ≈ x), enabling the model to gradually “turn on” rational nonlinearity during training. 

Stable early-stage Jacobian. The Jacobian of the gated unit is ∂r ∂x = (1 − α) + α ∂ ˜r∂x . (7) 

A small initial α keeps the effective sensitivity close to 1.This prevents early training dynamics from being dominated by activation outliers, and ensures deep optimization behav-ior is comparable to stable residual architectures (proven in Appendix L). 

2.4. RAN as a Drop-in FFN Replacement Replacing node-wise activations. A standard Trans-former/ViT FFN is FFN( h) = W2 σ(W1h) + b with σ

(e.g., GELU). Our minimal drop-in variant replaces σ with feature-wise 1D RAN units: 

RAN -FFN( h) = W2 r(W1h) + b, (8) 

where r(·) is the vectorized collection of gated rational units. This preserves the linear projections (and most pretrained weights) and modifies only the nonlinearity parameteriza-tion, while the identity-safe initialization avoids disrupting pretrained activation statistics. 

Optional pairwise augmentation. If explicit second-order structure is desired inside FFNs, we can augment the hidden representation with sparse pairwise rational features over selected channels: 

RAN -FFN +(h) = W2

r(W1h),

{rij (( W1h)i, (W1h)j )}(i,j )∈S h



+ b. 

(9) 

While this formulation enhances expressivity by capturing internal feature correlations, it incurs non-trivial computa-tional overhead. Therefore, we position RAN -FFN + pri-marily as an analytical tool in our ablation studies to in-vestigate the value of explicit second-order information, retaining the 1D variant as the default for efficiency-critical comparisons. 

2.5. Parameter Budgeting and Implementation Details Budget matching. RAN supports exact budget matching with baselines. A 1D unit uses (m + 1) numerator coeffi-cients, n denominator coefficients, and one gate parameter, totaling ≈ m + n + 2 parameters per feature. A 2D unit con-tributes ≈ T +S +1 parameters per selected pair. Excluding the linear head: 

Params ≈ d m + n + 2  + |S| (T + S + 1) . (10) This explicit accounting enables strictly matched compar-isons with MLPs and KANs. Stability-oriented implemen-tation. To ensure robust deep training, we employ the fol-lowing stabilizers: Positive Denominators: As defined in Eq. (4) , we enforce d(·) ≥ 1+ ε via softplus to strictly avoid poles. Initialization: We initialize ˜r(x) near identity by setting p(x) ≈ x and d(x) ≈ 1 (coefficients near zero), and initialize gates α near zero. Optimization: We optionally 4Rational ANOVA Networks 

use a smaller learning rate for denominator/gate parameters and mild weight decay on denominator coefficients (detailed in Appendix I). These stabilizers do not change the hypoth-esis class but significantly improve numerical stability in deep networks (see analysis in Appendix K). 

## 3. Learning Dynamics of Rational-ANOVA Networks 

Learning dynamics. The term “learning dynamics” refers to how training updates affect model behavior. We study a concrete per-step notion: how a single gradient update on one training example changes the model’s prediction on another input . Consider one SGD step at iteration t on 

(xu, y u) with learning rate η:

∆θt ≜ θt+1 − θt = −η∇θ L(fθt (xu), y u) ,

∆ft(xo) ≜ fθt+1 (xo) − fθt (xo). (11) 

The central question is: 

After one SGD update on (xu, y u), how does the prediction on a different input xo change? 

A first-order influence formula. A Taylor expansion yields a model-agnostic first-order approximation: 

∆ft(xo) ≈ − η ∇θ fθt (xo)⊤∇θ L(fθt (xu), y u) . (12) 

Thus, the geometry of influence is governed by gradient alignment between the query point xo and the update point 

xu.

Classification: eNTK-style decomposition. For multi-class classification with logits z = hθ (x) and π =Softmax( z), we track the vector of log-probabilities. Afirst-order expansion gives: 

∆ log πt(· | xo) ≈ − η A t(xo) Kt(xo, x u) Gt(xu, y u) + O(η2),

(13) 

where At(xo) = ∇z log πθt (· | xo) depends only on the cur-rent predictive distribution, Gt(xu, y u) is the loss gradient w.r.t. logits, and 

Kt(xo, x u) ≜ (∇θ zθ (xo)|θt ) ( ∇θ zθ (xu)|θt )⊤ (14) 

is the Empirical Neural Tangent Kernel (eNTK) . Intu-itively, Kt(xo, x u) measures how strongly an update trig-gered by xu projects onto the prediction at xo.

A mild stability assumption. For visualization, we as-sume relative influence stability : for a fixed xu, the relative magnitudes of ∥Kt(xo, x u)∥ across different xo remain ap-proximately stable over short windows. This is weaker than lazy training and is empirically verified in our experiments. 

Structured kernel decomposition in RAN. As defined in Sec. 2, RAN parameterizes a multivariate function via a low-order interaction topology (Eq. (2) ). With logits z(x) = 

W ϕ θ (x) + b, the eNTK admits an additive decomposition: 

Kt(xo, x u) = Krat  

> t

(xo, x u) + Khead  

> t

(xo, x u). (15) 

The head-side term Khead  

> t

behaves like a linear kernel on features. 

Khead  

> t

(xo, x u) = ( ∇W z(xo))( ∇W z(xu)) ⊤

+ ( ∇bz(xo))( ∇bz(xu)) ⊤, (16) 

which is low-rank and explicitly controlled by feature sim-ilarities. Crucially, owing to the ANOVA topology intro-duced in Eq. (2) , the rational-unit-side term decomposes into interpretable groups: 

Krat  

> t

(xo, x u) = 

> d

X

> i=1

Kmain  

> t,i

(xo, x u) + X

> (i,j )∈S

Kpair  

> t,ij

(xo, x u).

(17) 

Key insight. Unlike dense feature mixing in MLPs, RAN makes cross-sample coupling explicitly controllable . As visualized in Figure 3 (a) vs. (b), dense MLPs suffer from global entanglement where an update at xu causes unin-tended shifts at distant xo. In contrast, RAN’s ANOVA topology ensures that only inputs overlapping in specific low-order projections (main coordinates and selected pairs in S) can strongly influence each other. This transparency allows us to trade off expressivity and generalization by designing the interaction set S (analyzed in Appendix D). 

Why rational units are stable in depth. For a 1D unit 

˜r(x) = p(x)/d (x), since d(x) ≥ 1 + ε (as enforced in Eq. (4) ), the quotient rule implies (recalling d(x) = 1 + softplus( q(x)) + ε): 

∂ ˜r∂x = p′(x)d(x) − p(x)d′(x)

d(x)2

≤ | p′(x)| + |p(x)| | d′(x)|.

(18) 

(Detailed Lipschitz bounds provided in Appendix K.) More-over, denominator sensitivity is controlled by 

d′(x) = σsig 

 q(x) q′(x), with σsig (·) ∈ (0 , 1) , (19) 

which prevents the denominator pathway from amplifying gradients excessively. Figure 3 (c) visualizes this bene-fit: while polynomials often oscillate under steep gradi-ent requirements (Runge’s phenomenon), RAN’s rational units maintain smooth and bounded updates due to this denominator-controlled mechanism, thereby enabling stable deep training (proven in Appendix L). 5Rational ANOVA Networks 

Accumulated influence. To study long-term effects, we visualize the accumulated influence from a subset of updates 

U onto a query input xo:     

> Infl( xo;U)≈ − ηX
> t∈U
> At(xo)Kt(xo, x u(t))Gt(xu(t), y u(t)).
> (20)

In Sec. 4, we show how RAN’s sparse interaction topology reshapes these influence patterns compared to MLPs and KANs. 

## 4. Experiments 

Unless otherwise stated, all experiments are conducted un-der the strict premise of matched parameter counts (and matched FLOPs when applicable), and all runs are per-formed on NVIDIA RTX 6000 GPUs. 

Evaluation roadmap and why we start from vision. 

Our experiments are organized to support RAN as a drop-in nonlinear block across regimes. We begin with con-trolled visual benchmarks because they provide a widely adopted and reproducible stress test for both expressivity and optimization stability under strict parameter/FLOPs budgets. Importantly, this suite is also the standard testbed in the Kanbefair protocol, enabling direct comparisons to learnable-nonlinearity baselines under identical pipelines. To avoid conclusions that depend on a single domain, we further include (i) large-scale backbone integration (ViT on ImageNet-1K), 

(ii) real-world restoration (PolyU denoising), and (iii) non-visual evaluations and ablations (e.g., TabArena) to isolate the roles of topology and rational nonlinearities. 

Broader evaluation across domains. We emphasize that this paper does not claim KAN is primarily designed for vision. Instead, we adopt vision benchmarks as a standard-ized, budget-controlled protocol for learnable nonlinearity 

comparisons. To fairly reflect the regimes where KAN is typically most competitive, we additionally evaluate RAN on scientific discovery and extrapolation settings where KAN is often expected to excel: (i) tabular learning with strong efficiency constraints (TabArena; see Supplementary Section A), (ii) symbolic regression / physical law recov-ery (e.g., Lorentzian potential and Feynman equations; Sup-plementary Sections B and M), and (iii) extrapolation and boundary stability via the Runge stress test (Supplemen-tary Section C). These tasks directly evaluate interpretability, structure recovery, and out-of-support generalization, reduc-ing reliance on vision-only evidence. 

4.1. Comprehensive Evaluation on Visual Benchmarks Benchmarks and baselines. We follow the Kanbefair 

evaluation framework (Yu et al., 2024) to benchmark across diverse visual classification datasets, including MNIST (Le-Cun et al., 2002), EMNIST (Cohen et al., 2017), FM-NIST (Xiao et al., 2017), SVHN (Netzer et al., 2011), and CIFAR (Krizhevsky et al., 2009). We compare against: (i) 

MLP with GELU (and ReLU as an additional reference), (ii) the original KAN (Liu et al., 2024b), and (iii) the recent 

KAF (Zhang et al., 2025), in Table 1. 4.1.1. T RAINING PROTOCOL AND FAIRNESS CONTROLS 

Unified training budget and model selection. To avoid confounding factors from different optimization schedules, we train all methods under a unified training budget .Model selection is performed using the validation set (or a held-out split following Kanbefair), and we report the corresponding test accuracy of the selected checkpoint. This prevents inadvertently favoring any method via “test-set best” selection and makes the comparison reproducible. 

Parameter matching. For each dataset and each tar-get budget, we enforce parameter parity across methods (within a tight tolerance). For KAN, we sweep the recom-mended configuration space (grid sizes {3, 5, 10 , 20 } and B-spline degrees {2, 3, 5}) and report the best-performing setting under the same parameter budget. For MLP and KAF, we follow their standard implementations and tune width/depth only to meet the required parameter count. For RAN, parameter control is non-trivial because the sparse interaction set contributes additional learnable components. We therefore use an explicit parameter estimator to guide architecture instantiation: Params ≈ (18 + C)N + (26 + 

C)K + C + 2 , where N denotes the hidden width, K is the rational grid/order size, and C is the number of classes. Crucially, we dynamically adjust the number of sparse interaction pairs |S| so that RAN matches the target budget of each baseline configuration. This ensures that improve-ments cannot be attributed to a larger parameter count. 

Model capacity sweep. To stress-test robustness across regimes, we sweep a wide range of hidden widths (from 2 to 1024 when feasible) and report performance at multiple pa-rameter budgets. Unless otherwise stated, we report results averaged over multiple random seeds (mean and variance in the supplementary), and keep all data preprocessing and augmentation identical across methods. 

4.2. Evaluation in Large-Scale Vision Models 

To assess scalability, we integrate RAN into Vision Trans-formers (Dosovitskiy et al., 2020). Concretely, we adopt ViT-Tiny (DeiT-T) (Touvron et al., 2021) and replace the standard FFN blocks with RAN-FFN blocks while pre-serving the original parameter budget and FLOPs . All models are trained on ImageNet-1K (Deng et al., 2009; Russakovsky et al., 2015) using the same training recipe 6Rational ANOVA Networks 

Table 1. Comparison of Model Accuracy across Different Datasets. RAN (ours) is highlighted with a gray background. Note that RAN achieves consistent improvements across varying parameter scales (50k to 1.0M) and diverse datasets. 

DATASET PARAMS Accuracy (%) DATASET PARAMS Accuracy (%) 

KAF MLP KAN RAN † KAF MLP KAN RAN †

MNIST 

50k 97.45 97.60 96.50 97.55 

EMNIST-Let 

50k 82.50 84.60 70.00 84.55 100k 98.15 98.25 97.10 98.20 100k 86.80 88.55 78.50 88.40 200k 98.35 98.45 97.40 98.40 200k 88.90 90.20 81.20 90.15 300k 98.50 98.60 97.50 98.55 300k 89.15 90.80 81.50 90.85 

400k 98.65 98.70 97.65 98.75 400k 89.40 91.30 82.00 91.25 

FMNIST 

50k 88.00 88.50 86.00 95.39 SVHN 

200k 76.50 79.20 60.50 79.15 100k 89.20 89.00 88.00 96.67 500k 79.80 81.55 63.80 81.65 

200k 89.50 89.20 87.00 97.49 1M 81.20 82.40 62.00 82.35 300k 89.50 89.30 86.50 97.69 1.5M 81.80 83.15 55.00 83.25 

400k 89.50 89.30 86.20 97.79 2M 82.05 83.85 48.00 83.80 

CIFAR-10 0.5M 56.20 54.98 46.81 58.84 CIFAR-100 0.5M 25.62 25.85 17.73 27.86 

1.0M 56.95 56.45 43.32 59.05 1.0M 26.75 27.10 14.80 28.12 

Table 2. Performance on ViT-T/16 (ImageNet-1K). All methods are trained with the same recipe; Params/FLOPs are matched to the baseline when applicable. OOM denotes infeasibility under the budget-matched setting on 48GB GPUs.                             

> METHOD MECHANISM PARAMS FLOP STOP -1 (%) GAIN
> Existing Approaches
> (1) MLP (B ASELINE )LINEAR + GELU 5.7 M 1.08 G 72.3 –(2) KAN B-S PLINE OOM OOM ––(3) KAF KERNEL FUNCTION 5.9 M 1.12 G 73.2 +0.9
> Proposed Method
> (4) RAN (O URS )RATIONAL 5.7 M 1.08 G 74.2 +1.9

(including standard strong augmentations such as Mixup and CutMix, and the same learning-rate schedule) to ensure fair comparison. 

Baselines: (i) the standard MLP-based ViT (GELU FFN), (ii) KAF, and (iii) KAN. For KAN, we attempted to in-stantiate the smallest feasible configurations under the ViT embedding dimension while enforcing the same budget con-straints. However, due to the intrinsic parameter/memory growth induced by the B-spline basis expansion in high-dimensional settings, KAN remains infeasible on our hard-ware budget (48GB GPU memory), resulting in OOM un-der budget-matched conditions. We explicitly mark this outcome as OOM rather than reporting an under-budget or structurally altered KAN that would not be comparable. Overall, RAN improves Top-1 accuracy from 72.3% (MLP baseline) to 74.2% under the same Params/FLOPs. We at-tribute the gain to the combination of (i) learnable rational nonlinearities with stronger local approximation capacity and (ii) near-identity initialization and stabilization con-straints that preserve optimization stability when replacing deep FFN stacks. 

4.3. Real-World Image Denoising on PolyU 

We further evaluate RAN on the PolyU Real-world Noisy Image Dataset (Xu et al., 2018), which contains real noise under varying ISO and shutter speeds. Unlike synthetic 0.05M 0.10M 0.50M 1.00M 

> Parameters (Million, Log Scale)
> 10.0
> 12.5
> 15.0
> 17.5
> 20.0
> 22.5
> 25.0
> 27.5
> 30.0
> PSNR (dB)
> MLP
> 22.85 dB
> CNN
> 24.53 dB
> KAN
> 15.46 dB
> RAN (Ours)
> 29.40 dB
> State-of-the-Art
> & 12x Fewer Params!
> Real-World Denoising Efficiency (PolyU)
> MLP
> CNN
> KAN
> RAN (Ours)

Figure 4. Real-World Denoising Efficiency on PolyU. PSNR (y-axis) vs. parameter count (x-axis, log scale). Each point cor-responds to a budgeted model instance. RAN achieves a strong accuracy–efficiency trade-off and lies on the Pareto frontier. 

Gaussian noise, PolyU requires modeling complex, non-i.i.d. noise patterns under tight budgets. 

Protocol and preprocessing. We split the dataset into train/test following paired Real (noisy) and Mean (clean) folders. During training, we apply random cropping to generate 128 × 128 patches; during evaluation, we use cen-ter crops for deterministic reporting. To apply our non-convolutional model, we use a patch-based strategy: each image is unfolded into non-overlapping patches of size 

P × P (P = 4 ), yielding d = 3 P 2 = 48 input dimen-sions per patch. Model configuration and budgets. We instantiate a lightweight Residual Rational-ANOVA model with 3 layers and a hidden dimension of 16, equipped with both unit-level gating and block-level residual connections for stable optimization. Importantly, PolyU is evaluated as an efficiency study : we report PSNR versus parameter count and compare the Pareto frontier across model fam-ilies rather than enforcing a single identical budget for all methods. As shown in Fig. 4, RAN achieves high PSNR at a small budget (e.g., ∼50k parameters), while the compared 7Rational ANOVA Networks                        

> Table 3. Component Analysis. Dense vs. sparse ANOVA topology and ReLU vs. rational activations under a fixed ∼1M budget.
> Model Topology Activ. TabArena C-10 (%)
> MLP (Base) Dense ReLU 0.82 56.0 MLP-Rat Dense Rational 0.89 56.8 ANOVA-ReLU Sparse ReLU 0.86 55.2
> RAN (Ours) Sparse Rational 0.96 58.3

CNN/KAN baselines are instantiated at larger budgets (e.g., 

∼600k parameters) to reflect their typical operating points. 

Training and metrics. All models use Adam ( 10 −3), batch size 32, and identical epoch budgets. We optimize ℓ1 (MAE) to better preserve sharp edges, and process patches in chunks (4,096 per forward) to save memory. We report PSNR on full-resolution reconstructions after refolding and clamp-ing outputs to [0 , 1] . Baselines. We compare against (i) a ReLU MLP at matched small budgets, (ii) a lightweight 

CNN for fast denoising at larger budgets, and (iii) KAN 

(B-splines) at comparable larger budgets. Fig. 4 summarizes PSNR–Params trade-offs. 

4.4. Ablation Studies 

We conduct ablations on TabArena and CIFAR-10 to isolate the sources of improvement. We fix the parameter budget to ∼1M and decouple the effects of topology (Dense vs. Sparse ANOVA) and activation (ReLU vs. Rational). 

Structure vs. activation. MLP-Rat improves over the dense ReLU baseline, indicating that learnable rational nonlineari-ties are beneficial even without sparsity. However, ANOVA-ReLU degrades CIFAR-10 accuracy, suggesting that sparsity alone can reduce effective capacity for complex visual pat-terns. Combining both yields the best result: RAN achieves the strongest performance across domains, supporting a syn-ergistic effect where rational units compensate for capacity loss induced by sparse ANOVA connectivity. Topology and stability. We further study: (i) interaction set selec-tion for S (random vs. heuristic criteria), and (ii) stabiliza-tion mechanisms . Heuristic selection provides negligible gains ( < 0.3% ) while increasing preprocessing overhead, suggesting that having mixing capacity matters more than precise initial pair choice. Stability constraints are critical: removing the positive denominator constraint can trigger divergence across seeds, and removing residual gating (i.e., 

α = 1 ) substantially reduces final accuracy, confirming that near-identity initialization is important. 

## 5. Related Work 

Learnable nonlinearities and rational parameterizations. 

While most deep networks rely on fixed pointwise activa-tions such as ReLU (Agarap, 2018; Nair & Hinton, 2010) or GELU (Hendrycks & Gimpel, 2016), an increasing body of work argues that the shape of nonlinear transformations should itself be learnable (Goodfellow et al., 2013; Ra-machandran et al., 2017; He et al., 2015a;b). By adapting the nonlinearity to the data distribution, such models can achieve higher expressivity per parameter and improved sample efficiency. Rational function parameterizations are especially appealing in this regard (Telgarsky, 2017a), as low-degree rational forms can approximate functions with sharp transitions, saturation, or near-singular behavior more efficiently than polynomials. Nevertheless, unconstrained rational compositions may introduce poles or unstable gra-dients, which has motivated recent designs that enforce pos-itivity or normalization in the denominator and emphasize stability under deep composition. 

Spline-based models and Kolmogorov–Arnold Networks. 

Spline-based neural models, most prominently Kolmogorov– Arnold Networks (KANs) (Yu et al., 2024; Zhang et al., 2025), revisit classical representation theorems by plac-ing learnable univariate functions on network edges. This design offers strong interpretability and flexible function approximation, and has shown promising empirical perfor-mance (Scardapane et al., 2018; Bohra et al., 2020). How-ever, spline discretization typically requires maintaining knot grids, which can lead to non-negligible memory and computation overhead, as well as boundary artifacts and sensitivity to grid resolution (Li, 2024). These challenges become more pronounced in high-dimensional or resource-constrained settings, motivating alternative function parame-terizations that preserve the spirit of learnable nonlinearities while offering better numerical robustness and scalability. 

Additive models and ANOVA-style inductive bias. Ad-ditive models and functional ANOVA decompositions pro-vide a principled way to control interaction order by de-composing a function into main effects and a limited set of low-order interaction terms. Such structures have a long his-tory in statistics and machine learning (Hastie & Tibshirani, 1986; Hoeffding, 1948), where they are valued for inter-pretability, data efficiency, and robustness. Recent neural architectures have revisited ANOVA-style designs to impose structured inductive bias, explicitly restricting how features interact during training (Yang et al., 2021; Hu et al., 2023; Enouen & Liu, 2023). By limiting uncontrolled high-order entanglement, these models often exhibit more stable op-timization dynamics and generalization, particularly when operating under fixed parameter or compute budgets. 

## 6. Conclusion 

We introduced Rational-ANOVA Networks (RAN) , a base architecture that makes nonlinearities learnable while re-maining compatible with deep, budget-controlled training. RAN parameterizes multivariate functions with an explicit functional-ANOVA topology—main effects plus a con-trolled set of sparse pairwise interactions—and equips each 8Rational ANOVA Networks 

component with low-degree Pad ´e-style rational units. To ensure stable deep composition, we enforce strictly positive denominators to avoid poles and adopt residual-style gating to initialize each rational unit near an identity map, thereby preserving early-stage activation statistics and optimization behavior. Across controlled function benchmarks and real-world vision tasks under matched parameter (and when applicable, matched compute) budgets, RAN consistently improves the accuracy–efficiency trade-off over parameter-equivalent MLPs and strong learnable-nonlinearity base-lines. Moreover, RAN is a drop-in FFN nonlinearity, im-proving Vision Transformers under matched params/FLOPs. 

## Impact Statement 

This work proposes Rational-ANOVA Networks (RAN), a stable and budget-controlled architecture that combines an explicit low-order interaction structure with learnable rational nonlinearities. By improving accuracy–efficiency trade-offs and enabling drop-in upgrades to common back-bones (e.g., transformers) without increasing parameters or FLOPs, RAN may reduce the computational and energy cost required to reach a target performance, benefiting sus-tainable deployment and broader accessibility. 

Limitations . Our experiments are constrained by limited computational resources, which restricts the scale of mod-els and the breadth of pretraining regimes we can explore. While we demonstrate encouraging results on representative vision backbones and benchmarks, we do not claim exhaus-tive validation on the full spectrum of modern foundation models (e.g., very large language or multimodal models) or at frontier training scales. We hope future work, e.g., poten-tially by groups with greater compute, will further evaluate RAN on larger, more diverse foundation models, investigate stronger scaling behavior, and stress-test reliability under distribution shifts and safety-critical settings. Potential negative impacts are also possible. More capa-ble models can amplify downstream misuse (e.g., intrusive surveillance, manipulation, or automated decision systems deployed without adequate oversight). Moreover, inductive biases from low-order decompositions and rational parame-terizations may interact with dataset bias, potentially wors-ening performance disparities in underrepresented groups or rare regimes. We therefore encourage responsible use, trans-parent reporting, and thorough robustness auditing before deploying RAN-based systems in high-stakes applications. 

## References 

Agarap, A. F. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375 , 2018. Bohra, P., Campos, J., Gupta, H., Aziznejad, S., and Unser, M. Learning activation functions in deep (spline) neural networks. IEEE Open Journal of Signal Processing , 1: 295–309, 2020. doi: 10.1109/OJSP.2020.3039379. Boull ´e, N., Nakatsukasa, Y., and Townsend, A. Rational neural networks. CoRR , abs/2004.01902, 2020. URL 

https://arxiv.org/abs/2004.01902 .Brunton, S. L., Proctor, J. L., and Kutz, J. N. Discov-ering governing equations from data by sparse iden-tification of nonlinear dynamical systems. Proceed-ings of the National Academy of Sciences , 113(15): 3932–3937, March 2016. ISSN 1091-6490. doi: 10.1073/ pnas.1517384113. URL http://dx.doi.org/10. 1073/pnas.1517384113 .Cohen, G., Afshar, S., Tapson, J., and Van Schaik, A. Em-nist: Extending mnist to handwritten letters. In 2017 in-ternational joint conference on neural networks (IJCNN) ,pp. 2921–2926. IEEE, 2017. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248–255. Ieee, 2009. DeVore, R., Hanin, B., and Petrova, G. Neural network approximation, 2020. URL https://arxiv.org/ abs/2012.14501 .Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR , abs/2010.11929, 2020. URL 

https://arxiv.org/abs/2010.11929 .Elfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. CoRR , abs/1702.03118, 2017. URL http://arxiv.org/abs/1702.03118 .Enouen, J. and Liu, Y. Sparse interaction additive net-works via feature interaction detection and sparse selec-tion, 2023. URL https://arxiv.org/abs/2209. 09326 .Erickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., and Smola, A. Autogluon-tabular: Robust and accurate automl for structured data. arXiv preprint arXiv:2003.06505 , 2020. Erickson, N., Purucker, L., Tschalzev, A., Holzm ¨uller, D., Desai, P. M., Salinas, D., and Hutter, F. Tabarena: A liv-ing benchmark for machine learning on tabular data, 2025. URL https://arxiv.org/abs/2506.16791 .9Rational ANOVA Networks 

Garg, A., Ali, M., Hollmann, N., Purucker, L., M ¨uller, S., and Hutter, F. Real-tabpfn: Improving tabular founda-tion models via continued pre-training with real-world data, 2025. URL https://arxiv.org/abs/2507. 03971 .Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Pro-ceedings of the thirteenth international conference on artificial intelligence and statistics , pp. 249–256. JMLR Workshop and Conference Proceedings, 2010. Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. Maxout networks. In International conference on machine learning , pp. 1319–1327. PMLR, 2013. Hastie, T. and Tibshirani, R. Generalized additive models. 

Statistical science , 1(3):297–310, 1986. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE inter-national conference on computer vision , pp. 1026–1034, 2015a. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition, 2015b. URL https: //arxiv.org/abs/1512.03385 .Hendrycks, D. and Gimpel, K. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. 

CoRR , abs/1606.08415, 2016. URL http://arxiv. org/abs/1606.08415 .Hoeffding, W. A class of statistics with asymptotically nor-mal distribution. The Annals of Mathematical Statistics ,19(3):293–325, 1948. URL https://www.jstor. org/stable/2235637 .Hu, L., Nair, V. N., Sudjianto, A., Zhang, A., and Chen, J. Interpretable machine learning based on functional anova framework: Algorithms and comparisons, 2023. URL 

https://arxiv.org/abs/2305.15670 .Klambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S. Self-normalizing neural networks. Advances in neural information processing systems , 30, 2017. Kovachki, N., Li, Z., Liu, B., Azizzadenesheli, K., Bhat-tacharya, K., Stuart, A., and Anandkumar, A. Neural operator: Learning maps between function spaces with applications to pdes. Journal of Machine Learning Re-search , 24(89):1–97, 2023. URL http://jmlr.org/ papers/v24/21-1524.html .Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images.(2009), 2009. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceed-ings of the IEEE , 86(11):2278–2324, 2002. Li, Z. Kolmogorov-arnold networks are radial basis function networks. arXiv preprint arXiv:2405.06721 , 2024. Lin, T., Wang, Y., Liu, X., and Qiu, X. A survey of trans-formers, 2021. URL https://arxiv.org/abs/ 2106.04554 .Liu, Z., Ma, P., Wang, Y., Matusik, W., and Tegmark, M. Kan 2.0: Kolmogorov-arnold networks meet sci-ence, 2024a. URL https://arxiv.org/abs/ 2408.10205 .Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halver-son, J., Solja ˇci ´c, M., Hou, T. Y., and Tegmark, M. Kan: Kolmogorov-arnold networks. arXiv preprint arXiv:2404.19756 , 2024b. Lou, Y., Caruana, R., Gehrke, J., and Hooker, G. Accurate intelligible models with pairwise interactions. In Pro-ceedings of the 19th ACM SIGKDD International Confer-ence on Knowledge Discovery and Data Mining , KDD ’13, pp. 623–631, New York, NY, USA, 2013. Associa-tion for Computing Machinery. ISBN 9781450321747. doi: 10.1145/2487575.2487579. URL https://doi. org/10.1145/2487575.2487579 .Louizos, C., Welling, M., and Kingma, D. P. Learning sparse neural networks through l0 regularization, 2018. URL https://arxiv.org/abs/1712.01312 .Molina, A., Schramowski, P., and Kersting, K. Pad ´e acti-vation units: End-to-end learning of flexible activation functions in deep networks. CoRR , abs/1907.06732, 2019. URL http://arxiv.org/abs/1907.06732 .Nair, V. and Hinton, G. E. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10) ,pp. 807–814, 2010. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A. Y., et al. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning , volume 2011, pp. 7. Granada, 2011. Palm, R. B., Paquet, U., and Winther, O. Recurrent rela-tional networks, 2018. URL https://arxiv.org/ abs/1711.08028 .Petersen, B. K. Deep symbolic regression: Recovering mathematical expressions from data via policy gradients. 

CoRR , abs/1912.04871, 2019. URL http://arxiv. org/abs/1912.04871 .10 Rational ANOVA Networks 

Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F. A., Bengio, Y., and Courville, A. On the spectral bias of neural networks, 2019. URL https: //arxiv.org/abs/1806.08734 .Ramachandran, P., Zoph, B., and Le, Q. V. Searching for activation functions. arXiv preprint arXiv:1710.05941 ,2017. Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learn-ing representations by back-propagating errors. Nature ,323:533–536, 1986. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition chal-lenge. International journal of computer vision , 115(3): 211–252, 2015. Scardapane, S., Scarpiniti, M., Comminiello, D., and Uncini, A. Learning Activation Functions from Data Using Cubic Spline Interpolation , pp. 73–83. Springer International Publishing, July 2018. ISBN 9783319950983. doi: 10. 1007/978-3-319-95098-3 7. URL http://dx.doi. org/10.1007/978-3-319-95098-3_7 .Schmidhuber, J. Deep learning in neural networks: An overview. Neural Networks , 61:85–117, January 2015. ISSN 0893-6080. doi: 10.1016/j.neunet.2014. 09.003. URL http://dx.doi.org/10.1016/j. neunet.2014.09.003 .Telgarsky, M. Neural networks and rational functions. In 

International Conference on Machine Learning , pp. 3387– 3393. PMLR, 2017a. Telgarsky, M. Neural networks and rational functions, 2017b. URL https://arxiv.org/abs/1706. 03301 .Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image transform-ers & distillation through attention. In International Con-ference on Machine Learning , volume 139, pp. 10347– 10357, July 2021. Trefethen, L. N. Approximation theory and approximation practice, extended edition . SIAM, 2019. Udrescu, S.-M. and Tegmark, M. Ai feynman: A physics-inspired method for symbolic regression. Science Ad-vances , 6(16):eaay2631, 2020. Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017. Xu, J., Li, Z., Du, B., Zhang, M., and Liu, J. Reluplex made more practical: Leaky relu. In 2020 IEEE Symposium on Computers and communications (ISCC) , pp. 1–7. IEEE, 2020. Yang, Z., Zhang, A., and Sudjianto, A. Gami-net: An explainable neural network based on generalized additive models with structured interactions, 2021. URL https: //arxiv.org/abs/2003.07132 .Yoshida, Y. and Miyato, T. Spectral norm regularization for improving the generalizability of deep learning, 2017. URL https://arxiv.org/abs/1705.10941 .Yu, R., Yu, W., and Wang, X. Kan or mlp: A fairer compar-ison. arXiv preprint arXiv:2407.16674 , 2024. Zhang, J., Fan, Y., Cai, K., and Wang, K. Kolmogorov-arnold fourier networks. arXiv preprint arXiv:2502.06018 , 2025. 11 Rational ANOVA Networks 

# Supplementary Material 

## Overview and Roadmap 

This document provides a comprehensive extension of the empirical validation, theoretical analysis, and mechanistic investigation presented in the main text. While the main paper establishes Rational-ANOVA Networks (RAN) as a general-purpose architecture, this supplementary material deepens the analysis into specialized domains requiring high interpretability, structural discovery, and extrapolation capability. The material is organized into five thematic parts: 

Part I: Extended Benchmarks on Tabular & Scientific Data 

• Section A (Tabular Efficiency): Evaluates RAN on the TabArena benchmark, showing that RAN achieves SOTA win rates ( > 0.95 ) with orders of magnitude less training time than AutoML ensembles. • Section M (Feynman Benchmark): Extends the evaluation to 14 equations from the Feynman Symbolic Regression dataset. We demonstrate that RAN consistently recovers exact physical laws (RMSE ∼ 10 −8) where baselines struggle with rational structures (singularities and ratios). 

Part II: Physics-Informed Case Studies & Symbolic Discovery 

• Section B (Lorentzian Potential): A “Davids vs. Goliaths” study showing RAN (72 params) outperforming dense MLPs/KANs (5k params) by 100 × in precision, overcoming spectral bias. • Section C (Extrapolation): Stress-tests boundary stability on the Runge function, confirming that RAN eliminates the boundary oscillations (Runge phenomenon) inherent to spline-based KANs. • Section E & G (Automated Discovery): Demonstrates RAN’s “One-Shot” symbolic discovery capability on Van der Waals, Enzyme Kinetics, and Breit-Wigner resonance systems, avoiding the iterative manual pruning required by KANs. 

Part III: Mechanistic Visualization & Ablation 

• Section D (Visualizing Topology): Qualitatively visualizes the training dynamics, contrasting the “Spectral Leakage” of dense baselines with the “Structural Resonance” mechanism of RAN. • Section J (Topology Ablation): Quantitatively validates the “Smart Sparse” selection strategy, proving that finding the right connections is more critical than adding more connections. 

Part IV: Theoretical Foundations 

• Section K (Regularity): Provides proofs for Global Holomorphy and Explicit Lipschitz Bounds, certifying that RAN units are pole-free and smooth. • Section L (Deep Stability): Proves that the Residual Gating mechanism guarantees ϵ-isometry at initialization, enabling stable training for deep architectures. 

Part V: Reproducibility & Usability 

• Section F (Workflow): Compares the API design, highlighting RAN’s automated pipeline versus KAN’s manual “train-prune-fix” cycle. • Section I (Hyperparameters): Lists detailed experimental configurations for reproducibility. 12 Rational ANOVA Networks 

## A. Efficiency and Performance on Tabular Benchmarks 10 1 10 2 10 3 10 4 10 5 

> Training Time (seconds) [Log Scale]
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Win Rate (0.0 - 1.0)
> RAN(Ours) AutoGluon
> RealTabPFN
> TabArena Benchmark: Model Performance vs. Efficiency
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> Win Rate Strength

Figure 5. Performance vs. Efficiency on TabArena. The plot compares the Win Rate (y-axis) against Training Time (x-axis, log scale) of various models. RAN (Ours, marked with a red star) achieves the highest win rate while maintaining a training time orders of magnitude lower than top-tier baselines like AutoGluon and RealTabPFN. 

Beyond high-dimensional perception tasks, we further evaluated the versatility of RAN on the TabArena (Erickson et al., 2025) benchmark, a rigorous testbed designed to assess model performance across diverse tabular datasets. Figure 5 illustrates the trade-off between predictive capability (measured by Win Rate) and computational cost (Training Time in seconds, log scale). As visualized in the scatter plot, traditional AutoML frameworks such as AutoGluon (Erickson et al., 2020), and specialized tabular architectures like RealTabPFN (Garg et al., 2025) (represented by yellow and light-green markers) demonstrate strong performance, achieving win rates between 0.8 and 0.9. However, these methods typically incur high computational overhead, with training times often exceeding 10 4 seconds due to ensemble overhead or complex prior fitting. In contrast, RAN (Ours) , marked by the red star, establishes a new state-of-the-art on this benchmark. It achieves a normalized Win Rate approaching 1.0 , significantly outperforming the closest competitors. Crucially, RAN achieves this superior performance with substantially reduced computational resources, requiring approximately 10 3 seconds for training. This places RAN in the optimal upper-left quadrant of the efficiency-performance landscape, demonstrating that the learnable rational activations can effectively capture complex tabular feature interactions significantly faster than heavy ensemble methods or Transformer-based tabular baselines. 

## B. Case Study: Structural Resonance in Physical Discovery 

To investigate the limits of parameter efficiency and interpretability, we challenge the models to learn a fundamental physical structure: the Lorentzian potential, defined as f (x, y ) = (1 + x2 + y2)−1. This function is ubiquitous in physics (describing resonance and decay) but poses a dual challenge for standard deep learning architectures: it possesses an infinite domain with algebraically heavy tails, which are notoriously difficult to approximate using exponentially decaying (e.g., Sigmoid/Tanh) or compactly supported (e.g., B-Splines) primitives. 

B.1. Experimental Setup 

We compare Rational-ANOVA (RAN) against two strong baselines: a standard Multi-Layer Perceptron (MLP) with GELU activations and the Kolmogorov-Arnold Network (KAN). To highlight the efficiency gap, we operate under a strict “Davids vs. Goliaths” regime: • Baselines (Goliaths): We configure the MLP and KAN with sufficient depth and width ( ∼ 5, 300 parameters) to ensure they have ample capacity to fit the surface. • Ours (David): We constrain RAN to a minimal topology, resulting in only 72 learnable parameters .13 Rational ANOVA Networks MLP (~5.3k Params)                         

> Fail at Heavy Tails
> KAN (~5.2k Params)
> Spline Oscillations
> RAN (Ours, 72 Params)
> Structural Resonance
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> Absolute Error  |f f| (Log Scale)
> 3210123
> Spatial Coordinate x(y= 0 )
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> 1.2
> Potential  f(x)
> (a) Cross-Section Analysis: Capturing Global Topology
> Ground Truth
> MLP (Tail Bias)
> KAN (Ripple)
> RAN (Ours)
> Zoom: Peak Fidelity
> MLP KAN RAN
> 10 1
> 10 2
> 10 3
> 10 4
> Parameter Count (Bars)
> (b) The Efficiency Gap: Params vs. Precision
> 72 params beat 5k+ params
> (100× lower MSE vs. KAN)
> 5300 5200
> 72
> 72
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> MSE Loss (Line, Lower is Better)

Figure 6. Structural Resonance in Action. We task models to discover the Lorentzian potential. (Top) Error heatmaps (log scale) reveal that MLPs suffer from tail bias (red halos) and KANs from spline oscillations (ripples), while RAN achieves analytical precision. (Bottom Left) Cross-section analysis at y = 0 shows RAN capturing the heavy tail perfectly. (Bottom Right) RAN outperforms baselines by two orders of magnitude in precision while using < 1.5% of their parameter budget. 

All models are trained to minimize the Mean Squared Error (MSE) on a 200 × 200 grid over [−4, 4] 2.

B.2. Results: The Efficiency Gap 

The quantitative results, summarized in Table 4 and visualized in Figure 6, demonstrate a decisive advantage for the proposed architecture. 

Breaking the Scaling Law. Despite having ∼ 75 × fewer parameters than the baselines, RAN achieves an MSE of 

1.3 × 10 −7, surpassing the MLP ( 1.2 × 10 −5) and KAN ( 1.5 × 10 −4) by two orders of magnitude. This result challenges the prevailing scaling law assumption that higher precision strictly requires larger models. From the perspective of Minimum Description Length (MDL), RAN demonstrates that structural alignment —matching the network’s inductive bias to the target function—can yield O(1) complexity solutions for problems that typically demand O(N ) over-parameterization. 

Failure Mode Analysis: Why Baselines Struggle? The error heatmaps in Figure 6 (Top) reveal distinct failure modes rooted in the mathematical properties of the baselines: • MLP (Spectral Bias & Decay Mismatch): The MLP exhibits significant “tail bias” (red halos). Mathematically, approximating an algebraically decaying function ( 1/r 2) using a composition of exponentially decaying or piecewise linear (ReLU/GELU) activations is inefficient. The model struggles to balance the high curvature at the peak with the slow decay at the tails, resulting in systematic bias in the far-field. • KAN (Local Basis Artifacts): KAN exhibits “spline ripples.” Since B-splines have compact support, they lack global constraints. To approximate the smooth, infinite-domain Lorentzian function, independent spline coefficients must be perfectly synchronized. Small optimization errors between adjacent intervals manifest as high-frequency oscillations, confirming that local bases are suboptimal for capturing global algebraic laws. 14 Rational ANOVA Networks 

B.3. Interpretability: Symbolic Discovery 

Beyond numerical accuracy, RAN demonstrates the capacity for symbolic discovery. By inspecting the learned coefficients of the rational units, we observe that the network effectively “re-discovered” the analytical form of the physical law. 

Coefficient Recovery. As shown in Table 4, RAN recovers the denominator coefficients with near-perfect accuracy. For instance, the learned coefficient for the quadratic term is 1.000 ± 0.001 , while the coefficients for non-existent terms (e.g., linear terms or cross-interaction terms in the denominator) are correctly suppressed to ≈ 0.

Implicit Symbolic Regression. This implies that RAN acts as a differentiable symbolic regression engine. Unlike traditional symbolic regression, which searches through a discrete space of expressions, RAN finds the optimal functional form purely through gradient descent. The explicit separation of P (x) and Q(x) allows the model to “lock on” to the physical poles and zeros, providing a white-box explanation that is mathematically isomorphic to the ground truth.   

> Table 4. Comparison on Lorentzian Potential Fitting. RAN achieves the lowest error with significantly fewer parameters and is the only model capable of recovering the exact symbolic form of the physical law.

Model Params Training Time MSE (Log) Main Failure Mode Symbolic Recovery 

MLP (GELU) ∼ 5, 300 1× −4.9 Tail Bias (Far-field) ✗

KAN (Spline) ∼ 5, 200 2.5× −3.8 High-Freq Ripples ✗

RAN (Ours) 72 0.8× −6.9 None (Noise Floor) ✓

Conclusion. This experiment underscores that for scientific modeling tasks, the quality of parameters (inductive bias) is far more critical than their quantity . RAN provides a “white-box” alternative that is not only more efficient but also physically interpretable. 

## C. Analysis on Extrapolation and Boundary Stability 

While universal approximation theorems suggest that MLPs, KANs, and RANs can fit any continuous function given infinite capacity, their behavior under finite budgets differs fundamentally due to their respective inductive biases. To investigate this, we conduct a controlled stress test using the Runge function f (x) = (1 + 25 x2)−1, a classic benchmark for evaluating interpolation stability and extrapolation capability. 

C.1. Experimental Setup 

We train all models on the interval x ∈ [−1, 1] and evaluate them on an extended domain [−2.5, 2.5] . This setup tests the models’ ability to capture the underlying physical law (asymptotic decay) rather than merely memorizing training points. 

C.2. Results: The Generalization Gap 

As summarized in Table 5, we observe a distinct performance dichotomy between local/piecewise approximations and our global rational approach.      

> Table 5. Stress Test on the Runge Function. While all models achieve low error within the training range ( |x| ≤ 1), only RAN generalizes to the extrapolation regime ( |x|>1). Baselines suffer from orders-of-magnitude degradation due to inductive bias mismatch.

(Rahaman et al., 2019) 

Model Inductive Bias Interp. MSE Extrap. MSE Failure Mode 

(|x| ≤ 1) (|x| ∈ [1 , 2.5] ) (Qualitative Behavior) 

MLP (ReLU) Piecewise Linear 2.5 × 10 −4 1.1 × 10 −1 Linear Bias (Fails to decay) KAN (Spline) Local Polynomial 1.8 × 10 −5 4.5 × 10 −1 Runge Oscillation & Divergence 

RAN (Ours) Global Rational 1.2 × 10 −7 1.5 × 10 −7 None (Perfect Asymptotic Fit) 

The Spline Trap (KAN). Although KAN achieves a competitive interpolation error ( ∼ 10 −5), it exhibits the highest 15 Rational ANOVA Networks 

extrapolation error ( 4.5 × 10 −1). This confirms that spline bases, which lack global constraints, suffer from the Runge Phenomenon . To minimize residual error within the training range, the high-degree polynomials are forced to oscillate violently near the boundaries, leading to divergent behavior outside the training support. 

The Linear Trap (MLP). The MLP’s extrapolation error remains high ( ∼ 10 −1) because ReLU networks inherently extrapolate linearly based on the slope of the final activation region. This physically violates the asymptotic condition 

lim x→∞ f (x) = 0 , rendering standard MLPs unsuitable for modeling potential fields with infinite support (Telgarsky, 2017b). 

The Rational Advantage (RAN). RAN is the only architecture where extrapolation performance matches interpolation performance, maintaining an MSE of ∼ 10 −7 across the entire domain. This “flat generalization profile” indicates that RAN acts as a symbolic discovery engine: it does not merely approximate the geometry but successfully identifies the functional form of the physical law. 

## D. Visualizing Structural Resonance: The Dynamics of Topology Learning 

quantified the efficiency gains of the Rational-ANOVA architecture, this section provides a qualitative analysis of how the model discovers the underlying topology. We visualize the training dynamics on the “Needle in a Haystack” benchmark to contrast the learning behaviors of dense baselines versus our sparse approach. 

D.1. The Geometry of Interaction 

To understand the visualization in Figure 7 (Row 1), we first define the geometric signature of the target interaction. The multiplicative term γx 1x2 defines a hyperbolic paraboloid surface. In the plotted 2D projection, this manifests as a characteristic “checkerboard” saddle geometry, where the gradient sign flips across the axes: sgn (∇f ) ∝ sgn (x1) · sgn (x2) (21) This geometric signature serves as the “ground truth” pattern that the models must reconstruct from the high-dimensional input. 

D.2. Failure Mode: Spectral Leakage in Dense Baselines 

The second row of Figure 7 reveals the fundamental limitation of Dense (MLP/KAN) baselines, which we term Spectral Leakage .• Entangled Initialization: Dense models initialize with a fully connected graph ( |S| ≈ d2/2). Consequently, the gradient signal for the specific pair (x1, x 2) is mechanically dispersed across all spurious connections (xi, x j ).• Visual Artifacts: This dispersion is visible in the heatmaps (Epoch 10-50) as high-frequency “salt-and-pepper” noise. The model attempts to approximate the smooth saddle surface by superimposing thousands of misaligned, small-magnitude functions. • Noisy Topology: The resulting interaction matrix (Right Column) exhibits a high-entropy distribution. The “red squares” scattered across the matrix indicate that the model has “memorized” the data via distributed spurious correlations rather than “learning” the physical law. 

D.3. Success Mode: Structural Resonance in RAN 

The third row of Figure 7 demonstrates the phenomenon of Structural Resonance unique to RAN. • Topology Locking: Unlike the dense baseline, RAN’s sparse update mechanism (driven by the gradient coupling strength Cij ) acts as a denoising filter. By Epoch 50, the surface reconstruction is visually indistinguishable from the ground truth. • Zero-Error Convergence: The “Final Error” map (Row 3, Col 4) is uniformly black, implying pointwise error 

ϵ(x) ≈ 0 across the domain. This confirms that the learned rational unit ϕ1,2(x1, x 2) has converged to the exact analytical form of the target interaction. 16 Rational ANOVA Networks 

• Symbolic Identification: Most critically, the learned topology matrix S collapses to a single active entry at index (1 , 2) .This indicates that RAN has effectively performed Implicit Symbolic Regression , discarding the O(d2) “haystack” terms to isolate the single “needle” of physical causality. x1 x2 x3 x4         

> Interaction Topology
> (Sparse ANOVA Mask)
> Rational Unit
> (u) = P(u)1 + sp( Q(u))
> Pole-free parameterization
> f(x)Physics
> (a) Rational-ANOVA Filtering Mechanism
> Inductive Bias
> (Rational Polynomial)
> 3210123
> Interaction Complexity
> 3
> 2
> 1
> 0
> 1
> 2
> 3
> Loss / Error
> Rational Regime (RAN)
> Polynomial / Spline Regime
> (MLP / KAN)
> (b) Landscape of Structural Resonance
> RAN (Structural Resonance)
> Physical Truth
> KAN (Spline Oscillation)

Figure 7. Visualizing Structural Resonance. (Top Table) Quantitative comparison of topology size vs. accuracy. (Bottom Visuals) 

Evolution of the learned manifold f (x) for the interaction term γx 1x2. Row 1: Ground Truth showing the characteristic saddle geometry (“checkerboard”). Row 2 (Dense Baseline): Suffers from Spectral Leakage . The gradient signal disperses into spurious connections, resulting in noisy heatmaps and an entangled interaction matrix (Right). Row 3 (RAN): Demonstrates Structural Resonance . The model rapidly locks onto the target geometry (Epoch 50). The learned topology matrix S (Right) is perfectly sparse, isolating the single causal pair (x1, x 2).

## E. Automated Discovery of Rational Physical Laws 

A critical bottleneck in scientific machine learning is the “Human-in-the-Loop” requirement. As illustrated in the KAN framework (Liu et al., 2024), extracting clean physical formulas often requires an iterative manual process: training a large model, pruning sparse edges, fixing symbolic forms, and re-training (Steps 1-3). To evaluate whether RAN can automate this pipeline, we task models with discovering two fundamental laws: the Van der Waals Equation (Thermodynamics) and Michaelis-Menten Kinetics (Biochemistry ) (Petersen, 2019). Both systems are governed by rational dynamics ( P/Q ), which challenge standard polynomial or exponential bases. 

Results: The “One-Shot” Advantage. Table 6 presents a comparative analysis of the discovery process. 

1. The Taylor Trap (Symbolic Regression). Genetic algorithms (PySR) tend to rediscover Taylor series expansions rather than the closed-form laws. For the VdW gas, PySR approximates the interaction term (V − b)−1 as a power series 

V −1 + bV −2 + . . . . While mathematically valid near large volumes, this approximation fails to capture the singularity at 

V = b (the excluded volume limit), leading to poor extrapolation. 

2. The Refinement Burden (KAN). KANs show promise but suffer from basis mismatch. In the “Auto” phase, KANs approximate the rational curve using spline-like shapes (often combinations of exp and sin ), resulting in complex, uninter-pretable formulas (18 ops). Achieving a clean physical law requires manual intervention (“Manual Step 3”) to prune the network and enforce specific symbolic priors. 

3. Automated Rational Discovery (RAN). RAN achieves “One-Shot Discovery.” Without any manual pruning or iterative refinement, the “Auto” RAN converges directly to the exact symbolic form. • System I: It identifies the VdW interaction term aV 2 and the excluded volume term 1  

> V−b

as inherent components of its rational units, achieving an RMSE of 10 −6 (machine precision noise). • System II: It recovers the Michaelis-Menten constant Km as a learnable coefficient in the denominator, identifying the 17 Rational ANOVA Networks 

Table 6. Symbolic Discovery of Rational Physical Laws. Comparison of discovered formulas for two classic systems: (I) Van der Waals Equation of State and (II) Michaelis-Menten Enzyme Kinetics. While Symbolic Regression (PySR) produces Taylor-series approximations and KANs require multi-step manual refinement to escape local spline minima, RAN identifies the exact governing laws in a single automated pass (Auto).                                                                                                            

> System Method / Origin Discovered Symbolic Formula RMSE Complexity (I) VdW Gas Theory (Ground Truth) P=RT V−b−aV20.00 5 ops
> PySR (Genetic Algo) P≈RT V+RT b −aV2+RT b 2
> V3(Taylor Expansion) 3.2×10 −212 ops
> KAN (Auto) 0.98 RT ·V−1.02 −1.12 a·exp( −0.8V) + 0 .05 sin( V)1.5×10 −218 ops
> KAN (Manual Step 3) RT V−0.98 b−0.95 aV2+ϵ4.1×10 −37 ops
> RAN (Auto) 1.00 RT ·V2−1.00 a(V−1.00 b)
> V2(V−1.00 b)1.2×10 −65 ops (II) Enzyme Theory (Ground Truth) v=Vmax [S]
> Km+ [ S]0.00 3 ops
> PySR (Genetic Algo) v≈0.82 Vmax tanh(2 .1[ S]) + 0 .05 5.5×10 −28 ops
> KAN (Auto) Vmax ·(1 −exp( −1.2[ S])) + 0 .03[ S]22.8×10 −29 ops
> KAN (Manual Step 2) Vmax [S]1.12 Km+ 0 .95[ S]1.5×10 −34 ops
> RAN (Auto) 1.00 Vmax [S]1.00 Km+ 1 .00[ S]8.5×10 −83 ops

saturation mechanism v → Vmax exactly. 

Conclusion. By embedding rationality into the architecture itself, RAN eliminates the need for the tedious “train-prune-fix” cycle, offering a fully automated path from data to physical law (Brunton et al., 2016). 

## F. Functionality and Workflow Analysis 

To understand the practical advantages of RAN, we compare its functional workflow against the KAN framework in Table 7. The KAN methodology (Liu et al., 2024) is inherently procedural , treating training, pruning, and symbolic identification as distinct phases requiring user intervention. For example, discovering a physical law in KAN involves a loop of suggest symbolic (viewing potential functions) and fix symbolic (locking functions manually), which scales poorly with problem complexity. 

The RAN Automation Pipeline. In contrast, RAN is designed as an end-to-end discovery engine. • Implicit Pruning: Instead of a manual model.prune() step, RAN incorporates sparsity regularization directly into the optimization objective (Louizos et al., 2018), automatically silencing irrelevant interaction terms during 

model.fit() .• Rational Projection: While KAN requires the user to select basis functions (e.g., choosing between sin , exp , or 

x2), RAN’s model.auto rational() method automatically “snaps” the learned continuous coefficients to their nearest rational counterparts (e.g., 1.00003 → 1). This allows for the exact recovery of integer-based physical laws without manual guessing. • ANOVA Visualization: Beyond standard plotting, RAN introduces model.plot anova() , which explicitly visualizes the coupling strength between variables, offering immediate insight into the physical topology (e.g., “Does variable x interact with y?”). This streamlined API reduces the “Scientist-CPU time” from hours of manual tuning to seconds of automated computation. 18 Rational ANOVA Networks 

Table 7. Workflow and Functionality Comparison: KAN vs. RAN. We contrast the API functionalities required to discover a physical law. KAN relies on an iterative “Train-Prune-Fix-Retrain” cycle involving manual inspection ( suggest symbolic , fix symbolic ). In contrast, RAN consolidates these steps into a unified automated pipeline, leveraging its rational inductive bias to perform training, sparsification, and symbolic identification simultaneously.                                     

> Phase Functionality KAN API (Iterative / Manual) RAN API (Ours / Automated) 1. Training Optimization model.fit(dataset, opt=’LBFGS’) model.fit(dataset, opt=’Adam’)
> Mechanism Requires grid extension & heavy optimizers. Standard backprop , highly stable.
> 2. Pruning Sparsification model.prune(threshold=1e-2) (Implicit during fit via Group Lasso) Mechanism Manual removal of inactive nodes. Auto-sparsification of interaction terms.
> 3. Discovery
> Suggestion model.suggest symbolic(l,i,j) model.auto rational(precision=1e-5)
> Decision model.fix symbolic(l,i,j, fun=...)
> Refinement model.fit(update grid=False) (Not needed, coefficients are learned) Mechanism User must manually inspect & lock nodes. Snap-to-Rational: Auto-projects weights to Q.
> 4. Analysis Visualization model.plot(beta=100) model.plot anova(mode=’interaction’)
> Output Visualizes 1D splines. Visualizes 2D Interaction Heatmaps & Poles.
> Final Output Formula model.symbolic formula() model.symbolic formula(simplify=True)

## G. Symbolic Discovery of Physical Laws 

A central promise of scientific machine learning is the ability to distill interpretable laws from observational data, moving beyond “black-box” prediction to “white-box” understanding. To evaluate this, we task the models with a Symbolic Discovery problem governed by the Breit-Wigner formula, f (E) ∝ (( E2 − M 2)2 + Γ 2)−1, which describes particle resonance and features a characteristic rational pole structure. Table 8 contrasts the analytical formulas discovered by different architectures. 

The Trap of Functional Misalignment. As demonstrated in Rows B and C of Table 8, traditional methods struggle not due to a lack of fitting power, but due to a fundamental mismatch in the hypothesis space. Symbolic regression (PySR), typically driven by genetic algorithms over a library of standard primitives ( exp , cos , sin ), attempts to approximate the resonance peak using a Taylor-like expansion. While numerically acceptable ( R2 = 0 .912 ), the resulting Formula B interprets the resonance as a Gaussian decay modulated by a cosine term. This is physically erroneous: resonance is an algebraic phenomenon (Lorentzian distribution) with heavy tails, whereas Gaussian approximations decay exponentially fast, leading to significant extrapolation errors. Similarly, KAN (Formula C) relies on B-splines, which act as localized basis functions. Consequently, KAN interprets the pole as a local “bump” (approximated by a Gaussian bell curve) rather than a global singularity structure. This confirms that while KANs are excellent interpolators, their spline-based inductive bias limits their ability to discover laws governed by rational dynamics. 

Structural Resonance and Exact Identification. In contrast, RAN (Row D) operates in a regime of “Structural Resonance.” Because the network is composed of rational units, its optimization landscape naturally aligns with the meromorphic nature of physical potentials. The discovered raw formula 1 

> E4−4.82 E2+6 .05

is not an approximation but an algebraic transformation of the ground truth (E2 − M 2)2 + Γ 2. The model spontaneously allocated its denominator coefficients to match the polynomial expansion of the physical law, achieving an R2 of 0.998 without requiring any auxiliary regularization terms to force rationality. This suggests that RAN performs symbolic regression implicitly via gradient descent, finding the exact analytical form that minimizes the loss landscape. 

Parsimony and Occam’s Razor. From the perspective of model complexity, RAN exhibits superior parsimony. As shown in the Complexity column, the MLP and KAN solutions require 18 and 11 operations, respectively, to construct their approximations. These overly complex formulas are symptomatic of “over-fitting to geometry” rather than “learning the physics.” RAN, conversely, solves the problem with only 5-7 operations. By adhering to the principle of Occam’s Razor, RAN provides the simplest explanation that fits the data. This parsimony is critical for scientific trust; a physicist can immediately inspect Formula E and recognize the Breit-Wigner form, whereas the “black-box” expressions of Baselines B 19 Rational ANOVA Networks         

> Table 8. Symbolic Formula Discovery on Resonance Physics. We compare the analytical formulas discovered by different methods for the scattering amplitude task. A: Human ground truth. B-C: Symbolic Regression (PySR) and KANs struggle to capture the rational pole structure, approximating the resonance peak with exponential gaussians or trigonometric expansions. D-E: RAN (Ours) successfully identifies the exact rational structure with minimal complexity, effectively recovering the governing physical law.

ID Discovered Symbolic Formula Method Test R2 Complexity Physicality A 1(E2 − M 2)2 + M 2Γ2 Human (Truth) 1.000 5 ops ✓

B 0.85 e−2.1( E−M )2

+ 0 .12 cos(3 .5E) − 0.04 PySR (MLP) 0.912 18 ops ✗

C 1.02 exp 



− (E − 2.4) 2

0.35 



+ 0 .05 tanh( E − 1) [2,5,1] KAN 0.945 11 ops ✗

D 1.00 

E4 − 4.82 E2 + 6 .05 RAN (Raw) 0.998 7 ops ✓

E 1(E2 − M 2)2 + Γ 2 RAN (Pruned) 0.999 5 ops ✓

and C offer no such physical insight. 

## H. Hyperparameter Robustness and Structural Resonance 

A central hypothesis of this work is that Rational ANOVA Networks (RAN) achieve superior performance not merely through increased capacity, but through structural resonance —matching the network’s inductive bias to the underlying mathematical form of the data. To validate this, we conduct a comprehensive grid search comparing RAN against MLPs and KANs on a task designed to stress-test their approximation capabilities: the Sharpened Lorentzian function, defined as 

f (x, y ) = (1 + 10( x2 + y2)) −1.This function features two challenging characteristics ubiquitous in physics: a sharp, high-gradient peak at the origin and a heavy-tailed asymptotic decay. We systematically vary model depth ( D ∈ { 2, 3, 4}), width ( W ∈ { 10 , 100 }), and learning rate ( lr ∈ { 10 −4, 10 −3, 10 −2}) to analyze the scaling laws of each architecture. 

H.1. Analysis: Approximation vs. Identification 

The results, summarized in Table 9, reveal a striking dichotomy in how different architectures solve the task: 

1. The Brute Force Regime (MLP). The MLP results exhibit standard scaling behavior: performance improves monotoni-cally with width and depth. However, the convergence is inefficient. Even with its largest configuration ( D = 4 , W = 100 ), the MLP error plateaus at ∼ 2 × 10 −3. This limitation stems from the piecewise-linear nature of ReLU, which requires exponentially many neurons to approximate the smooth, high-curvature peak of the Lorentzian function. 

2. The Spline Precision Floor (KAN). KANs outperform MLPs by an order of magnitude ( ∼ 10 −4 MSE), confirming the benefit of learnable activation functions. However, they hit a distinct “precision floor.” Spline bases are local and polynomial; fitting the sharp peak at x = 0 induces high-frequency oscillations (Gibbs-like phenomena) unless the grid size is prohibitively large. Furthermore, KANs show higher sensitivity to learning rates, with divergence risks at lr = 10 −2 in deeper configurations. 

3. Structural Resonance (RAN). RAN operates in a fundamentally different regime. Crucially, RAN achieves optimal performance ( ∼ 10 −7 MSE) even with the minimal configuration ( D = 2 , W = 10 ). Increasing depth or width yields diminishing returns because the model has already effectively identified the analytical solution. This confirms that when the architecture’s inductive bias aligns with the physical law (i.e., rationality), the problem complexity collapses from O(N ) to 

O(1) .

Takeaway. This experiment demonstrates that RAN is not just “another approximator” but a specialized tool for scientific discovery. It removes the need for the extensive hyperparameter tuning typically required to force generic networks (MLPs/KANs) to fit physical geometries. 20 Rational ANOVA Networks                                                                                                                                                                                                                                                                                                                                                                                            

> lr = 10 −4lr = 10 −3lr = 10 −2
> Method DWTrain ↓Test ↓Train ↓Test ↓Train ↓Test ↓
> MLP 210 4.2×10 −24.5×10 −23.8×10 −24.1×10 −23.5×10 −23.9×10 −2
> MLP 2100 1.2×10 −21.4×10 −28.5×10 −39.1×10 −38.1×10 −38.9×10 −3
> MLP 310 9.5×10 −39.8×10 −37.2×10 −37.6×10 −36.8×10 −37.4×10 −3
> MLP 3100 5.1×10 −35.5×10 −34.2×10 −34.8×10 −33.9×10 −34.5×10 −3
> MLP 410 6.5×10 −36.9×10 −35.1×10 −35.5×10 −34.8×10 −35.2×10 −3
> MLP 4100 2.1×10 −32.4×10 −31.8×10 −32.1×10 −31.5×10 −31.9×10 −3
> KAN 210 5.2×10 −46.1×10 −44.1×10 −44.8×10 −43.8×10 −44.5×10 −4
> KAN 2100 2.1×10 −43.5×10 −41.8×10 −42.9×10 −41.5×10 −42.5×10 −4
> KAN 310 3.5×10 −44.2×10 −42.2×10 −43.1×10 −41.9×10 −42.8×10 −4
> KAN 3100 1.2×10 −42.1×10 −48.5×10 −51.5×10 −49.2×10 −51.8×10 −4
> KAN 410 2.8×10 −43.5×10 −41.9×10 −42.6×10 −41.6×10 −42.4×10 −4
> KAN 4100 9.5×10 −51.8×10 −47.2×10 −51.2×10 −46.8×10 −51.4×10 −4
> RAN 210 1.5×10 −71.8×10 −71.2×10 −71.5×10 −71.1×10 −71.4×10 −7
> RAN 2100 1.2×10 −71.6×10 −78.5×10 −81.2×10 −77.2×10 −81.1×10 −7
> RAN 310 1.1×10 −71.4×10 −78.2×10 −81.1×10 −76.8×10 −89.5×10 −8
> RAN 3100 8.5×10 −81.1×10 −76.5×10 −88.9×10 −85.2×10 −87.8×10 −8
> RAN 410 9.5×10 −81.2×10 −77.5×10 −89.8×10 −86.2×10 −88.5×10 −8
> RAN 4100 7.2×10 −89.5×10 −85.5×10 −87.2×10 −84.8×10 −86.5×10 −8

Table 9. Robustness Analysis on the Sharpened Lorentzian. We compare test MSE across varying Depth ( D), Width ( W ), and Learning Rates ( lr ). While MLPs and KANs rely on scaling depth/width to improve performance (stagnating at ∼ 10 −3 and ∼ 10 −4 respectively), 

RAN achieves near-machine precision ( ∼ 10 −7) even with the most minimal architecture ( D = 2 , W = 10 ), demonstrating that correct inductive bias eliminates the need for massive over-parameterization. 

## I. Reproducibility and Hyperparameter Configuration 

To ensure the reproducibility of our results and facilitate future research, we provide a comprehensive breakdown of the experimental configurations used across all benchmarks. Table 10 details the specific hyperparameter settings, including optimization schedules, rational unit topology, and interaction budgets. 

Table 10. Detailed Hyperparameter Configuration. We list the exact settings used to reproduce the main results in Table 1 and Table 2. Note that for RAN, the Rational Degree (P, Q ) and Interaction Budget |S| are structural hyperparameters that replace the standard “activation function” choice in baselines. 

Dataset Backbone / Task Optimizer Batch Size Init. LR Weight Decay Rational (P, Q ) Interaction |S|

Computer Vision Benchmarks 

ImageNet-1K ViT-Tiny (RAN-FFN) AdamW 1024 5 × 10 −4 0.05 (3 , 2) Dense (Token) CIFAR-10 RAN-ConvNet AdamW 128 1 × 10 −3 1 × 10 −4 (3 , 2) d log d (Sparse) MNIST / FMNIST RAN-MLP Adam 256 1 × 10 −3 0.0 (2 , 2) Full Pairwise 

Tabular Learning (TabArena) 

Tabular (Avg.) RAN-Tabular Adam 512 2 × 10 −3 1 × 10 −5 (2 , 2) 0.5d (Learned) Higgs Boson RAN-Deep Adam 1024 1 × 10 −3 1 × 10 −5 (2 , 2) 100 (Top-K) 

Scientific Discovery (Physics) 

Lorentzian 2D Symbolic Discovery L-BFGS Full Batch 1.0 0.0 (2 , 2) Auto-Pruned Many-Body Interaction Search Adam 200 5 × 10 −3 0.0 (3 , 2) d(d − 1) /2

Runge Function Stress Test L-BFGS Full Batch 0.1 0.0 (4 , 3) N/A (1D) 

## J. Ablation Study: Interaction Topology and Selection Efficiency 

To rigorously quantify the contribution of the ANOVA decomposition, we conduct an ablation study focusing on two key dimensions: the density of pairwise interactions ( |S|) and the strategy used to select them. 

J.1. Setup: The Interaction “Needle in a Haystack” 

We construct a synthetic 4D benchmark f (x) = P sin( xi) + γ · x1x2, designed to decouple additive separability from multiplicative interactions. A standard GAM (Generalized Additive Model) (Lou et al., 2013) or a RAN with |S| = 0 is mathematically incapable of solving this task perfectly, regardless of depth. 21 Rational ANOVA Networks 

Table 11. Interaction topology ablation. Smart Sparse achieves a strong accuracy–efficiency trade-off. 

Topology |S| Params Test MSE ↓ Time Mean Std 

Main effects 0 40 0.2450 1.0x 

Random sparse 0.5d 52 0.0820 0.02 1.1x Full pairwise d(d − 1) /2 85 0.0012 1.4x 

Smart Sparse (Ours) 0.25d 48 0.0015 1.2x Ground Truth                

> Main Effects ri(xi)Interaction rij (xi,xj)Full Function f(x)
> Dense Baseline
> (Standard MLP/KAN)  Epoch 10 Epoch 50 Epoch 100 Final Error ||
> Input Dimension i
> Input Dimension  j
> Dense/Noisy S
> RAN (Ours)
> Fast Convergence
> Converged Early!
> Input Dimension i
> Input Dimension  j
> Sparse/Learned S
> 2.0
> 1.5
> 1.0
> 0.5
> 0.0
> 0.5
> 1.0
> 1.5
> 2.0
> Function Output  y
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Abs Error  | |

+ =

> Visualizing Efficiency: RAN's Topology Learning vs. Dense Baselines

Figure 8. Evolution of Model Capacity via Interaction Topology. (Row 1) Decomposition of the target function into main effects and interactions. (Row 2) Random Selection : As the interaction budget |S| increases (left to right), the model gradually fits the complex surface. (Row 3) Smart Selection : By prioritizing pairs with high gradient coupling, RAN converges to the true form even in the highly sparse regime (Col 2), demonstrating that finding the right connections is more important than adding more connections. 

J.2. Visual Analysis of Structural Growth 

Figure 8 visualizes the “growth” of the model’s capacity as we relax the topological constraints. Main Effects Only (|S| = 0 ): As shown in the first column of Figure 8, the model learns the additive components but completely fails to capture the “checkerboard” pattern of the interaction term, resulting in high residual error. Random vs. Smart Selection: The second and third rows compare two topology selection strategies. While Random Selection (Row 2) eventually converges as the budget increases, Gradient-based Selection (Row 3) identifies the critical (x1, x 2) coupling immediately. Notably, the “Smart” sparse model achieves near-optimal performance with only 25% of the interaction budget, validating the efficiency of our sparse ANOVA prior. 

J.3. Quantitative Trade-offs 

Table 11 details the trade-off between topology size, accuracy, and training cost. Adding pairwise terms ( |S| = d) introduces a minimal computational overhead ( 1.2× time) but yields a massive performance gain ( 98% error reduction) compared to the main-effects-only baseline. 

Conclusion: The ablation confirms that the ANOVA structure is not merely a stylistic choice but a functional necessity for capturing non-additive dependencies. The “Smart Sparse” regime represents the sweet spot, offering the interpretability of simple models with the expressivity of dense networks. 22 Rational ANOVA Networks 

## K. Theoretical Analysis: Global Regularity and Structural Stability 

In this section, we provide a rigorous theoretical characterization of the Rational ANOVA Unit. Unlike standard rational approximations (e.g., Pad ´e approximants), which often suffer from poles and unbounded derivatives, we prove that our 

Strictly Positive Denominator formulation guarantees: 1. Global Holomorphy: The function is well-defined and smooth everywhere on R.2. Explicit Lipschitz Bounds: The local sensitivity is strictly controlled by the weight norms and polynomial degrees, preventing gradient explosion. 3. Deep Compositional Stability: These stability properties are preserved across deep architectures. 

K.1. Formulation and Global Smoothness 

Consider the scalar Rational Unit ϕ : R → R defined as: 

ϕ(x; θ) := P (x)

D(x) , where D(x) := 1 + softplus( Q(x)) . (22) Here, P (x) and Q(x) are polynomial parameterizations of degrees m and n, respectively, parameterized by weights 

wP ∈ Rm+1 and wQ ∈ Rn+1 :

P (x) = 

> m

X

> i=0

wPi xi, Q(x) = 

> n

X

> j=0

wQj xj . (23) 

Lemma K.1 (Global Regularity and Pole-Freeness) . For any finite weight configuration {wP , wQ}, the function ϕ(x) is 

C∞-smooth on the entire domain R. Specifically, ϕ(x) admits no poles. Proof. The function softplus( u) = log(1 + eu) satisfies softplus( u) > 0 for all u ∈ R. Consequently, the denominator satisfies the strict lower bound: 

D(x) = 1 + softplus( Q(x)) ≥ 1, ∀x ∈ R. (24) Since D(x)̸ = 0 everywhere, the quotient P (x) 

> D(x)

is defined for all x. Furthermore, as compositions of polynomials and C∞

functions (softplus) are C∞, and the quotient of smooth functions (with a non-zero denominator) is smooth, ϕ(x) is C∞ on 

R.

Remark 1 (Comparison with Standard Rationals). Standard rational functions R(x) = P (x) 

> Q(x)

inevitably possess poles at the roots of Q(x). Near a root x0, the activation magnitude |ϕ(x)| → ∞ and the gradient |ϕ′(x)| ∼ 1 

> |x−x0|2

→ ∞ , causing catastrophic numerical instability. Lemma K.1 guarantees that our architecture structurally forbids this failure mode. 

K.2. Traceable Lipschitz Constants 

To certify training stability, we must show that the gradient of the unit does not explode. We derive an explicit upper bound for the Lipschitz constant of ϕ(x) as a function of the input radius B, polynomial degrees (m, n ), and weight norms. 

Assumption K.2 (Compact Domain and Bounded Weights) . 1. Input Domain: The input x is confined to a compact interval X = [ −B, B ]. (This is typically enforced by Layer Normalization, e.g., B ≈ 3). 2. Weight Boundedness: The polynomial coefficients satisfy an L1-norm bound: ∥wP ∥1 ≤ WP and ∥wQ∥1 ≤ WQ.First, we establish auxiliary growth bounds for the polynomial components. 

Lemma K.3 (Polynomial Derivative Bounds) . Under Assumption K.2, for any x ∈ [−B, B ], the polynomials and their derivatives are uniformly bounded by: 

|P (x)| ≤ WP · S 0(m, B ) := MP , (25) 

|P ′(x)| ≤ WP · S 1(m, B ) := MP ′ , (26) 

|Q′(x)| ≤ WQ · S 1(n, B ) := MQ′ , (27) 23 Rational ANOVA Networks 

where Sk(d, B ) = Pdi=k i(i − 1) · · · (i − k + 1) Bi−k is the geometric scaling factor determined solely by the degree d and radius B. Specifically, S0(m, B ) = Pmi=0 Bi and S1(m, B ) = Pmi=1 iB i−1.Proof. We prove the bound for |P ′(x)|. By triangle inequality: 

|P ′(x)| =

> m

X

> i=1

iw Pi xi−1 ≤

> m

X

> i=1

|wPi | · i|x|i−1 ≤

> m

X

> i=1

|wPi | · iB i−1 (28) 

≤

> m

X

> i=1

|wPi |

!

· max 

> 1≤k≤m

(kB k−1) (Loose bound ) (29) 

≤ ∥ wP ∥1 ·

> m

X

> i=1

iB i−1

!

(Strict bound ) (30) 

= WP · S 1(m, B ). (31) The proofs for |P (x)| and |Q′(x)| follow identical logic. 

Theorem K.4 (Explicit Lipschitz Constant) . Under Assumption K.2, the Rational Unit ϕ(x) is Lipschitz continuous on 

[−B, B ]. The Lipschitz constant Kϕ := sup x∈X |ϕ′(x)| is explicitly bounded by: 

Kϕ ≤ M P ′ + MP · M Q′ . (32) 

Substituting the structural parameters from Lemma K.3, we obtain the full expansion: 

Kϕ ≤ WP S1(m, B )

| {z }

> Linear Sensitivity

+ ( WP S0(m, B )) · (WQS1(n, B )) 

| {z }

> Interaction Sensitivity

. (33) 

Proof. We differentiate ϕ(x) using the quotient rule: 

ϕ′(x) = P ′(x)D(x) − P (x)D′(x)

D(x)2 . (34) Recall that D(x) ≥ 1, which implies 1 

> D(x)

≤ 1 and 1 

> D(x)2

≤ 1. The derivative of the denominator is D′(x) = softplus ′(Q(x)) · Q′(x). Note that softplus ′(u) = σ(u) (the sigmoid function), which satisfies 0 < σ (u) < 1. Thus, we have the contraction property: 

|D′(x)| = |σ(Q(x)) | · | Q′(x)| ≤ | Q′(x)|. (35) Now, applying the triangle inequality to the quotient rule: 

|ϕ′(x)| = P ′(x)

D(x) − P (x)D′(x)

D(x)2 (36) 

≤ |P ′(x)||D(x)| + |P (x)| · | D′(x)||D(x)|2 (37) 

≤ | P ′(x)| · 1 + |P (x)| · | Q′(x)| · 1. (38) Substituting the uniform bounds MP , MP ′ , MQ′ from Lemma K.3 yields Eq. (32). 

Physical Interpretation of Stability. Equation (33) provides a direct theoretical justification for the use of weight decay in RAN (Yoshida & Miyato, 2017). The bound shows that the unit’s sensitivity ( Kϕ) scales linearly with WP and quadratically with the product WP WQ. Standard L2 regularization minimizes ∥w∥, which directly suppresses WP and WQ.Consequently, this compresses the Lipschitz constant of the network, preventing the chaotic dynamics typically associated with high-degree polynomials. 24 Rational ANOVA Networks 

K.3. Deep Composition and Jacobian Control 

The stability of a single unit implies the stability of the deep network constructed from them. 

Corollary K.5 (Network-Level Jacobian Bound) . Consider a deep network F composed of L layers, where each layer 

l consists of a linear map with norm ∥Wl∥ followed by Rational Units with Lipschitz constant K(l) 

> ϕ

. The Jacobian of the entire network is bounded by: 

∥J F (x)∥ ≤ 

> L

Y

> l=1



∥Wl∥ · K(l)

> ϕ



. (39) 

Proof. This follows directly from the chain rule for Lipschitz functions. Since each RAN unit is strictly Lipschitz continuous (Theorem K.4) and linear layers are bounded operators, the composition remains Lipschitz continuous on compact sets. Unlike architectures using standard rational functions, where K(l) 

> ϕ

could be unbounded (near poles), RAN ensures that every term in the product remains finite, guaranteeing stable forward and backward passes. 

Remark 2 (Vanishing Gradient Mitigation). While Theorem K.4 establishes an upper bound (preventing explosion), the rational structure also mitigates gradient vanishing. For standard activations like Sigmoid, σ′(x) ≈ e−| x| decays exponentially for large inputs. In contrast, for a RAN unit with degrees m = n, asymptotically ϕ(x) ≈ const , but the derivative decays polynomially (algebraically), i.e., |ϕ′(x)| ∼ | x|−k. This “heavy-tailed” gradient behavior allows error signals to propagate more effectively through deep layers compared to exponentially saturating functions. 

## L. Deep Training Stability: Near-Identity Initialization and Signal Preservation 

While Section K establishes that individual Rational Units are locally Lipschitz, training deep networks ( L ≫ 10 ) imposes a stricter requirement: the preservation of gradient signal variance across depth. Standard feedforward networks often suffer from the “shattering” of gradients or exponential variance explosion. In this section, we provide a formal justification for the Residual Gating mechanism introduced in Eq. (6), proving that it enforces a near-identity Jacobian spectrum at initialization, thereby guaranteeing well-conditioned optimization landscapes for arbitrarily deep architectures. 

L.1. Formulation of the Gated Block 

Let hl ∈ Rd denote the hidden state at layer l. A standard RAN layer with Residual Gating is defined as: 

hl+1 = Tl(hl) := hl + αl · (Φ( Wlhl) − hl) , (40) where: • Φ : Rd → Rd is the element-wise application of the Rational Unit ϕ(x) = P (x)/D (x).• Wl ∈ Rd×d is the weight matrix. • αl ∈ [0 , 1] is the learnable gating scalar, initialized to a small value αinit ≈ 0.Rearranging terms, the mapping can be viewed as a convex combination of the identity and the non-linear transformation: 

hl+1 = (1 − αl)Ih l + αlΦ( Wlhl). (41) 

L.2. Spectral Bounds of the Layer Jacobian 

The stability of the backward pass depends on the singular values of the Jacobian matrix Jl = ∂hl+1  

> ∂hl

.

Proposition L.1 (Jacobian Spectrum Control) . Let σmax (Jl) denote the spectral norm (largest singular value) of the layer Jacobian. Under the Lipschitz assumptions from Theorem K.4, the Jacobian norm is bounded by: 

σmax (Jl) ≤ (1 − αl) + αl · Kϕ · ∥ Wl∥2, (42) 

where Kϕ is the Lipschitz constant of the Rational Unit derived in Eq. (32) , and ∥Wl∥2 is the spectral norm of the weights. 

25 Rational ANOVA Networks 

Proof. By the chain rule, the Jacobian of Eq. (40) is: 

Jl = (1 − αl)I + αl · diag (ϕ′(Wlhl)) · Wl. (43) Using the sub-additivity of the spectral norm ( ∥A + B∥2 ≤ ∥ A∥2 + ∥B∥2): 

∥Jl∥2 ≤ ∥ (1 − αl)I∥2 + ∥αl · diag (ϕ′) · Wl∥2 (44) 

= (1 − αl) + αl · ∥ diag (ϕ′)∥2 · ∥ Wl∥2. (45) From Theorem K.4, we know that |ϕ′(x)| ≤ Kϕ for all inputs in the bounded domain. Thus, the spectral norm of the diagonal Jacobian matrix is simply the maximum absolute derivative: 

∥diag (ϕ′)∥2 = max  

> i

|ϕ′(( Wlhl)i)| ≤ Kϕ. (46) Substituting this back yields the proposition. 

L.3. Global Stability Theorem: Preventing Gradient Explosion 

We now examine the propagation of gradients through a network of depth L. The gradient of the loss L with respect to the input h0 is given by the product of layer Jacobians: 

∇h0 L =

> 1

Y

> l=L

Jl

!⊤

∇hL L. (47) 

Theorem L.2 (ϵ-Isometry at Initialization) . Assume the network is initialized such that αl = ϵ for a sufficiently small 

ϵ > 0, and weights are normalized such that ∥Wl∥2 ≈ 1. Then, the deep RAN network acts as an approximate isometry. Specifically, the gradient norm scaling is bounded by: 

exp ( −Lϵγ ) ≤ ∥∇ h0 L∥ 2

∥∇ hL L∥ 2

≤ exp ( Lϵ (Kϕ − 1)) . (48) 

In the limit ϵ → 0 (our initialization strategy), the ratio approaches 1, implying perfect signal preservation regardless of depth L.Proof. We consider the upper bound (gradient explosion). Using the sub-multiplicativity of the spectral norm: 

> L

Y

> l=1

Jl

> 2

≤

> L

Y

> l=1

∥Jl∥2 (49) 

≤

> L

Y

> l=1

(1 − ϵ + ϵK ϕ∥Wl∥2) . (50) Assuming standard initialization ∥Wl∥2 ≈ 1:Norm Product ≤ (1 + ϵ(Kϕ − 1)) L . (51) Using the inequality 1 + x ≤ ex, we obtain: Norm Product ≤ exp 

> L

X

> l=1

ϵ(Kϕ − 1) 

!

= exp ( Lϵ (Kϕ − 1)) . (52) The lower bound follows similarly by analyzing the minimum singular value σmin (Jl), ensuring gradients do not vanish. 26 Rational ANOVA Networks 

Table 12. Comprehensive Benchmark on the Feynman Symbolic Regression Dataset. We compare RAN against MLP and KAN on a diverse set of physical laws. Rows highlighted in gray indicate equations with inherent rational structures (ratios, poles, inverse squares), where RAN consistently achieves machine-precision recovery ( RMSE < 10 −6) and minimal complexity, significantly outperforming spline-based and ReLU-based baselines.                                                                                                                                                                                                                                       

> Eq. ID Formula (Ground Truth) Variables MLP RMSE KAN RMSE RAN RMSE Complexity Status I.16.6 u+v
> 1 + uv/c 2u, v 6.2×10 −41.2×10 −38.5×10 −85 ops Exact Recovery I.9.18 Gm 1m2
> (x2−x1)2+ ( y2−y1)2+... m, x, y, z 1.6×10 −36.6×10 −31.2×10 −66 ops Exact Recovery I.27.6 11/d 1+n/d 2
> d1, d 2, n 2.5×10 −41.9×10 −45.5×10 −84 ops Exact Recovery I.18.4 m1r1+m2r2
> m1+m2
> m, r 3.7×10 −41.5×10 −42.1×10 −75 ops Exact Recovery II.11.27 nα
> 1−nα/ 3ϵE fn, α 7.2×10 −51.4×10 −51.5×10 −74 ops Exact Recovery I.15.3x x−ut
> p1−u2/c 2x, u, t 8.5×10 −41.1×10 −33.2×10 −67 ops Exact Recovery II.36.38 μB kT +μαM ϵc 2μ, B, T 2.2×10 −31.2×10 −34.5×10 −58 ops Approx. (Rational)
> I.6.2 1
> √2πσ 2e−(θ−θ1)22σ2θ, σ 1.5×10 −42.9×10 −51.2×10 −59 ops Pad´ e Approx.
> I.12.11 q(E+Bv sin θ)q, E, B, θ 6.7×10 −49.1×10 −42.5×10 −56 ops Pad´ e Approx.
> I.37.4 I1+I2+ 2 pI1I2cos δI, δ 5.7×10 −43.4×10 −41.8×10 −57 ops Pad´ e Approx.
> II.2.42 κ(T2−T1)Adκ, T, A, d 1.8×10 −47.2×10 −41.1×10 −74 ops Exact Recovery I.44.4 nkT ln( V2/V 1)n, T, V 4.0×10 −42.4×10 −58.5×10 −5N/A Hard (Logarithm)
> III.10.19 μ
> q
> B2
> x+B2
> y+B2
> zμ, B 1.7×10 −48.2×10 −45.2×10 −65 ops Exact Recovery I.40.1 n0e−mgx/kT n, m, x, T 4.0×10 −43.1×10 −42.2×10 −58 ops Pad´ e Approx.

Discussion: The “Safe-Start” Mechanism. Theorem L.2 formalizes why RAN’s Residual Gating is essential. • Without Gating ( ϵ = 1 ): The bound becomes KLϕ . If the rational unit is locally steep ( Kϕ > 1), gradients explode exponentially as eL ln Kϕ . This aligns with the instability observed in the “No Gate” ablation study. • With Gating ( ϵ ≈ 0): The bound is eLϵC ≈ 1 + LϵC . The exponential dependency on depth L is suppressed to a linear dependency (for small ϵ). This regime allows the optimizer to navigate the initial epochs without numerical overflow, gradually increasing αl (and thus effective non-linearity) only as the loss landscape requires. This proof confirms that the combination of Pole-Free Boundedness (Kϕ < ∞, Section K) and Residual Gating (ϵ → 0)provides a mathematically rigorous guarantee of deep training stability. 

## M. Extensive Benchmarking on Feynman Equations 

To demonstrate the generality of our method, we extend our evaluation to the Feynman Symbolic Regression Bench-mark (Udrescu & Tegmark, 2020), encompassing equations from mechanics, electromagnetism, and thermodynamics. Table 12 reports the test RMSE across 14 representative equations. 

Rational Dominance. The results confirm a strong “Rational Inductive Bias.” For equations involving ratios, singularities, or inverse-square laws (highlighted in gray), RAN consistently achieves RMSE values in the range of 10 −7 ∼ 10 −8. For example, on the Relativistic Velocity equation (I.16.6), RAN identifies the exact symbolic structure u+v 

> 1+ uv/c 2

without any manual pruning, whereas KANs struggle to approximate the rational form with splines, stalling at 10 −3.

Pad ´e Approximation Capability. Even for non-rational functions involving exp , sin , or √·, RAN remains competitive. By leveraging the Pad ´e approximation property of rational units (e.g., ex ≈ 1+ x/ 21−x/ 2 ), RAN achieves errors ( ∼ 10 −5) comparable to or better than KANs, while maintaining a smoother, globally defined optimization landscape. 27