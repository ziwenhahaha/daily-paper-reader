---
title: Mining Generalizable Activation Functions
title_zh: 挖掘具有泛化能力的激活函数
authors: "Alex Vitvitskyi, Michael Boratko, Matej Grcic, Razvan Pascanu, Deep Shah, Petar Veličković"
date: 2026-02-05
pdf: "https://arxiv.org/pdf/2602.05688v1"
tags: ["keyword:SR", "query:SR"]
score: 6.0
evidence: 数学函数的进化搜索
tldr: 本研究提出利用AlphaEvolve框架结合大语言模型（LLM）作为变异算子，在广阔的Python函数空间中搜索激活函数。该方法通过引入分布外（OOD）性能作为适应度指标，旨在发现不仅能提升性能，还能编码特定归纳偏置的激活函数。实验证明，即使在小规模合成数据集上，该进化搜索策略也能高效挖掘出具有强泛化能力的有意义激活函数。
motivation: 旨在解决传统激活函数搜索空间受限的问题，并探索如何通过激活函数引导模型具备特定的归纳偏置以提升泛化性。
method: 采用AlphaEvolve框架，利用LLM在Python代码空间进行变异，并以分布外（OOD）性能作为进化搜索的适应度函数。
result: 实验表明，该方法在小规模合成数据集上即可成功发现具有良好泛化性和特定归纳偏置的有意义激活函数。
conclusion: 结合LLM的进化搜索是发现新型激活函数的有效手段，且OOD性能是引导搜索具备特定归纳偏置激活函数的关键指标。
---

## 摘要
激活函数的选择是一个活跃的研究领域，各种提议旨在改进优化的同时保持表达能力。此外，激活函数可以显著改变架构的隐式归纳偏置，从而控制其非线性行为。在本文中，与之前的工作一致，我们认为进化搜索为寻找新的激活函数提供了一个有用的框架，同时我们也提出了两个新颖的观察结果。第一点是，现代流水线（如依赖前沿大语言模型作为变异算子的 AlphaEvolve）允许更广泛且灵活的搜索空间；例如，在一定的 FLOP 预算内搜索所有可能的 Python 函数，从而消除了对手动构建搜索空间的需求。此外，鉴于这些流水线具备表示常识的能力，它们会偏向于有意义的激活函数，从而可能实现更高效的空间搜索。第二个观察结果是，通过该框架，人们不仅可以针对性能提升，还可以针对编码特定归纳偏置的激活函数。这可以通过将分布外数据上的性能作为适应度函数来实现，从而反映架构在独立于分布偏移的情况下遵循数据固有结构的程度。我们对这一提议进行了实证探索，并表明相对小规模的合成数据集就足以让 AlphaEvolve 发现有意义的激活函数。

## Abstract
The choice of activation function is an active area of research, with different proposals aimed at improving optimization, while maintaining expressivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behavior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaEvolve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python functions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their ability to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this framework, one can target not only performance improvements but also activation functions that encode particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale synthetic datasets can be sufficient for AlphaEvolve to discover meaningful activations.