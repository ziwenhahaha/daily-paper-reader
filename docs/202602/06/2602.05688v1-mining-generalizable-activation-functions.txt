Title: Mining Generalizable Activation Functions

URL Source: https://arxiv.org/pdf/2602.05688v1

Published Time: Fri, 06 Feb 2026 02:26:09 GMT

Number of Pages: 24

Markdown Content:
# Mining Generalizable Activation Functions 

Alex Vitvitskyi 1 Michael Boratko 1 Matej Grcic 1 Razvan Pascanu 1 Deep Shah 1 Petar Veliˇ ckovi´ c 1

## Abstract 

The choice of activation function is an active area of research, with different proposals aimed at im-proving optimization, while maintaining expres-sivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behav-ior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaE-volve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python func-tions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their abil-ity to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this frame-work, one can target not only performance im-provements but also activation functions that en-code particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale syn-thetic datasets can be sufficient for AlphaEvolve to discover meaningful activations. 

## 1. Introduction 

The hidden layer of a neural network is the basic building block for modern AI systems. It consists of a learnable lin-ear projection followed by a non-linear activation function ,typically applied element-wise, which enables the stack-  

> *Equal contribution 1Google DeepMind. Correspondence to: Alex Vitvitskyi <avlife@google.com >.
> Preprint. February 6, 2026.

ing of hidden layers to lead to an increase in expressivity (Mont ´ufar et al., 2014; Raghu et al., 2017). This structure is meant to mimic a rudimentary understanding of the bio-logical neurons, where information flows through synaptic connections, whose strength is meant to be represented by the parameters contained within the linear projections. The activation function not only reflects the non-linear behavior of biological neurons, but allows for composing (“stacking”) of several hidden layers without the result collapsing to a single linear projection. However, the parallel between artificial neural networks and biological systems is limited (Crick, 1989). For example, biological systems rely on spike trains (Roy et al., 2019), whereas neural networks rely on continuous-valued signals, allowing the use of gradient based optimization techniques. Therefore, biology can only go so far in terms of indicating what is the correct parametrization of a hidden layer, and in particular, what is the right choice of non-linearity. This realization led to an extensive search for non-linearities in the AI community (e.g Ramachandran et al., 2018), where various activation functions have been explored (Dubey et al., 2022), from hand-crafted activations such as ReLU 

(Nair & Hinton, 2010), hyperbolic tangent (LeCun et al., 2012), GELU (Hendrycks & Gimpel, 2016), and so forth, all the way to explicit automated search procedures. This exploration was driven by the ability of the final architecture to obtain better in-domain performance on standard bench-marks. The improved performance, as shown in various works, comes from a matching between initialization and the choice of activation function, where most benefits come from better behaved gradients and better signal propagation in the model, rather than expressivity. In this work, we perform an evolutionary search for activa-tion functions, in a way that aims to transfer performance from carefully-constructed small-scale experiments to im-proved downstream out-of-domain generalisation. 

## 2. Contributions 

We depart from previous approaches in two important ways. First, rather than evolving using function composition over a pre-defined set of simpler operations (e.g. Ramachan-dran et al., 2018), we explore an unbounded set of func-1 

> arXiv:2602.05688v1 [cs.LG] 5 Feb 2026 Finding a Generalizable Activation Functions AlphaEvolve
> LLMs ensemble
> Prompt sampler
> Best activation functions DB
> Evaluators
> Train Datasets
> OOD Test Datasets
> Training Loop
> Train MLP with AF
> Test MLP with AF activation function (AF)
> Figure 1. Overall description of the evolutionary search framework we use to find activation functions. We rely on an evolutionary search powered by AlphaEvolve (Novikov et al., 2025), designed to optimize test performance for small-scale models trained on carefully constructed, synthetic datasets. We demonstrate that functions discovered in this way are capable of meaningful forms of generalization without sacrificing their general-purpose potency.

tions. To navigate in this infinite space of functions, we rely on the AlphaEvolve framework (Novikov et al., 2025), which operates over the space of all Python programs. A key component in this approach is the reliance on frontier language models (e.g. Gemini (Comanici et al., 2025)) to generate meaningful proposals during search. If desired, these models can also be guided using expert information in the prompt, as a way of injecting additional biases into the search. We will describe our specific setup in the following sections. In contrast to previous works (Ramachandran et al., 2018), we explicitly target validation loss on out-of-domain data 

as our fitness function. The rationale for this decision is to attempt to generate activation functions that allow for better generalization , either through their implicit impact on the loss landscape or training dynamics. While functions discovered in this way will typically not capture explicit symmetries or structures in the data exactly, their ability to target and improve on out-of-domain performance demon-strates the flexibility of our approach. Moreover, our empirical results demonstrate that while stan-dard known activation functions are generally powerful and hard to beat on in-distribution performance, our ap-proach can discover alternatives which exhibit better out-of-domain generalization without sacrificing the in-domain performance. By targeting performance on out-of-domain validation data, our search results in architectural choices with improved robustness to unseen data. While it is theoretically possible to directly search for acti-vation functions or even the entire architecture layout that optimise larger training runs on massive datasets – typically explored within the domain of neural architecture search (NAS) (e.g. Zoph & Le, 2017; Elsken et al., 2019) – this can be computationally expensive, and may lead to overfitting to a particular dataset. Instead, we propose a simpler protocol, which we can re-fer to as the small-scale lab approach, which amounts to optimizing activation functions within small networks on small-scale synthetic data, allowing for rapid iteration of the inner loop of the evolutionary search strategy. This ensures that we can easily train such models from scratch every time we need to compute the fitness value of every proposal. By focusing on out-of-domain generalization, and ensuring the chosen synthetic evaluation data is sufficiently complex, we aim for the discovered activations to straightforwardly apply in more complex settings and help with improving general-ization performance. In work concurrent to ours, Nadimpalli et al. (2025) use classical reinforcement learning environ-ments as another highly relevant testbed; however, they do not explicitly study out-of-domain generalization, and such settings scarcely occur in classical RL tasks. Once we have mined several activation functions using our approach, we evaluate them within established base-line architectures on standard datasets, such as CIFAR-10 (Krizhevsky et al., 2009), ImageNet (Deng et al., 2009), CLRS-30 (Veli ˇckovi ´c et al., 2022) and ogbg-molhiv (Hu 2Finding a Generalizable Activation Functions 

et al., 2020). Datasets such as CIFAR-10 and ImageNet typically do not test for an explicit distribution shift—as opposed to CLRS-30, which directly tests for size gener-alization. The ogbg-molhiv task also attempts to test for a distribution shift, in that it uses an explicit scaffold split 

of its molecular data, requiring generalization to new struc-tures. We find that our approach is able to discover simple activation functions which significantly improve CLRS-30 generalization compared to established baselines, while not penalizing performance on tasks such as ImageNet. 

## 3. AlphaEvolve for activation functions 

AlphaEvolve (Novikov et al., 2025) is an evolutionary cod-ing system, powered by a set of large language models (LLMs). The main components of AlphaEvolve are sam-plers (which propose new solutions), evaluators (which pro-vide feedback on solution quality), and a database of the best solutions proposed so far. In our specific case, AlphaE-volve will aim use its evaluator to incrementally improve the generalization performance of the activation functions produced by the sampler. 

Search Specifically, our instantiation of AlphaEvolve will perform its search by following these steps: 1. Look-up a set of previously proposed activation func-tions (initially, we seed this process with only the stan-dard activation function ReLU( x) = max(0 , x )). 2. Conditioned on the code of the recovered functions, use frontier LLMs to generate new function implemen-tation proposals, aimed towards improving the perfor-mance of the model. 3. The generated proposals are evaluated by instantiating small-scale multilayer perceptron (MLP) architectures with the proposed activation functions, and training them on a series of purpose-built synthetic datasets. 4. The trained models are then evaluated on out-of-distribution test data, and the computed evaluation errors are used as the fitness function for the evolu-tionary process (deciding which functions to keep in the set of proposals for future iterations of the system). 5. Iterate steps 1–4 until satisfactory functions are found. 

Datasets For our purpose-built training/evaluation data, we focus on synthetic regression datasets that are designed to test various aspects of out-of-domain generalization. Con-cretely, our synthetic datasets are input-output examples of the form (x, f (x)) , where the target function, f , is applied to random low-dimensional input data, x ∈ Rd.Our collection of target functions f includes the following:   

> Figure 2. Visualization of in-distribution (training) and out-of-distribution (test) data for the one-dimensional target functions leveraged in our small-scale lab environment. Each function type tests a different kind of generalization in a way that supports rapid model training. Note that ID and OOD panels are different
> functions—there are no discontinuities in the functions we study.

• Polynomials with random coefficients; • Harmonic functions; • The Feynman Symbolic Regression Dataset (Udrescu & Tegmark, 2020). Figure 2 visualizes the examples of such functions that were used for MLP training and evaluation within the Al-phaEvolve loop. Each plot clearly indicates in-distribution training data (in blue) and out-of-distribution test data (in or-ange). These targets are designed to support different kinds of generalizing behaviour – from finding parameters that extrapolate well (polynomials), to being able to encode data with structural regularity (harmonic/periodic functions), all the way to being generally capable on a variety of physics-inspired equations (Feynman lectures). All of these are simple functions that can be learned rela-tively quickly in-distribution on modern hardware, allowing for fast evaluation. To ensure our test data is sufficiently out-of-domain from the training data, we generate random input points over disjoint intervals: train on the range (0 , 0.5) , test on the range (0 .5, 1) or on the range (0 , 1) , test on the range 

(−1, 0) .

Evaluation We use the negative mean squared error (MSE) as the scoring function for AlphaEvolve, which al-lows for iterative improvements of the activation function. In addition, we promote efficiency in the generated func-tions by placing a hard limit on the number of permitted floating-point operations (FLOPs), so that the computational cost of training with the proposed activation functions would not change significantly from the baseline choice of ReLU .AlphaEvolve discovers suitable activation functions iter-atively, usually starting from re-discovering simple well-known activation functions (such as GELU (Hendrycks & Gimpel, 2016)), gradually allowing for more complex com-binations of such functions, and ultimately coming up with very elaborate functions which explicitly inspect the batch 3Finding a Generalizable Activation Functions STEP 1 // Swish (SiLU)  

> Known function from literature

LOSS → -0.019753819331526756  

> 1

return x / (1.0 + jnp.exp(-x))  

> STEP 10 // "Leaky Square-Root Power-Decay"
> Custom combination of useful functions

LOSS → -0.011710595339536667  

> 1

gamma = 0.02  

> 2

alpha_pos = 0.16  

> 3

alpha_neg = 0.04  

> 4

alpha = jnp.where(x >= 0,  

> 5

alpha_pos, alpha_neg)  

> 6

return x * (  

> 7

gamma + 1.0 / jnp.sqrt(  

> 8

1.0 + alpha * x**2))  

> STEP 13 // "Quantum Fluctuation Witch" (QFW)
> Incorporating batch statistics

LOSS → -0.01130743883550167  

> 1

gamma = 0.02  

> 2

alpha_pos = 0.08  

> 3

alpha_neg = 0.02  

> 4

beta = jnp.mean(x, axis=0, keepdims=True)  

> 5

sigma = jnp.std(x, axis=0, keepdims=True)  

> 6

nu = 1.0 + jnp.exp(-sigma)  

> 7

alpha = jnp.where(x >= beta,  

> 8

alpha_pos, alpha_neg)  

> 9

suppression = 1.0 / (  

> 10

1.0 + alpha * jnp.abs(x - beta)**nu)  

> 11

return x * (gamma + suppression)  

> STEP 44 // "Higher-Order Counterfactual Resonance Activation" (HOCRA)
> Overfitting

LOSS → -0.007923223078250885                      

> 1gamma = 0.02 2alpha_pos = 0.08 3alpha_neg = 0.02 4batch_size = x.shape[0] 5if batch_size >= 4: 6x1, x2 = jnp.array_split(x, 2, axis=0) 7skew_sensitivity = 1.0 8boundary_factor = 0.5 9mean1 = jnp.mean(x1, axis=0, keepdims=True) 10 std1 = jnp.std(x1, axis=0, keepdims=True) + 1e-6 11 mean2 = jnp.mean(x2, axis=0, keepdims=True) 12 std2 = jnp.std(x2, axis=0, keepdims=True) + 1e-6 13 z1 = (x1 - mean1) / std1 14 skew1 = jnp.mean(z1**3, axis=0, keepdims=True) 15 kurt1 = jnp.mean(z1**4, axis=0, keepdims=True) - 3.0 16 # many lines omitted...
> 17 y1_final = y1_corrected - alignment_correction1 18 y2_final = y2_corrected - alignment_correction2 19 return jnp.concatenate([y1_final, y2_final], axis=0) 20 # many lines omitted...
> 21 return x * (gamma + suppression)

Figure 3. Illustration of a typical AlphaEvolve evolution when asked to discover new activation functions. Early on, it rediscovers functions present in the literature (Swish/SiLU in this case). It then discovers interesting ways to recombine standard building blocks (polynomials, leaky ReLU, and square roots), at which point it reaches the best tradeoff between score and transferability. Soon after, AlphaEvolve realises that its function does not need to be pointwise, and leverages the batch axis of the input tensor to extract and exploit basic batch statistics. This quickly spirals into constructing highly elaborate functions that achieve excellent score, but heavily overfit to the specifics of the “lab dataset” by utilising multiple moments of the distribution. 

4Finding a Generalizable Activation Functions                       

> Figure 4. Visualisation of four newly discovered pointwise activation functions by our system: Turbulent Activation Function ,Gaussian-Modulated Tangent Unit (GMTU) ,GELUSine and GELU-Sinc-Perturbation .
> Table 1. Overall performance of discovered activation functions by AlphaEvolve on the synthetic generated data, comprising Poly-nomials, Harmonics, and Feynman symbolic regression.
> Activation Function Test Loss Train Loss (×10 −3)(×10 −3)
> Turbulent 29.8 3.5 GMTU 51.9 4.1 GELUSine 54.7 1.4
> GELU-Sinc 68.7 2.2 GELU 78.8 2.0 ReLU 93.1 3.5

axis of the data. Such functions leverage the statistical prop-erties of the complete tensor of activations across a batch, making them more prone to overfitting to statistics present in the synthetic data. As such, it is necessary to identify the “sweet spot” in the evolutionary approach, or otherwise restrict the evolutionary engine for relying too much on such correlations. See Figure 3 for a depiction of these key points within a function evolution run. While we use synthetic datasets for the initial discovery of promising activation functions, we then validate the gen-eralization abilities of the top-performing candidates by replacing default activations in established baseline archi-tectures with the proposed functions. These models are then trained from scratch on standard datasets, including CIFAR-10, CLRS-30 and ogbg-molhiv. This allows us to validate our discoveries, and find activation functions that provide good generalization capabilities independent of our choice of synthetic training datasets. 

## 4. Activation function analysis 

We dedicate the remainder of our work to analysing further some of the most promising activation functions discov-ered by our small-scale lab. We initially select nine such functions that had the most promising performance on our collection of synthetic tasks (and include our initial function, 

ReLU , as a baseline). Out of these functions, we mainly focus on four that are newly discovered by our system – turbulent , Gaussian-Modulated Tangent Unit (GMTU) , GELUSine and GELU-Sinc-Perturbation – their code may be found in Appendix A. All of the functions are pointwise except for the turbu-lent activation, which computes simple batch statistics. As previously described, we expect pointwise functions to be more generally reusable compared to functions computing batchwise statistics; further, they are much easier to visually analyse (see Figure 4). Table 1 contains the overall performance scores of these four functions on the synthetic datasets described in the previous section, in the form of the training and test loss they achieve. We also include two standard baselines – the starting point of our search, the rectified linear unit ( ReLU ) and the Gaussian error linear unit ( GELU ), which is frequently rediscovered by our search. 5Finding a Generalizable Activation Functions                                                                                                                      

> Figure 5. Histogram of the pre-activation entries for trained model on synthetic dataset for: GELUSine and GELU-Sinc-Perturbation and
> ReLU . Note the range tends to be wide enough, i.e. [−2,2] , allowing the model to exploit the structure of the activation functions.
> Table 2. Activation Functions Comparison CIFAR CLRS molhiv ImageNet Feynman 1d poly 20d poly Sph. H.
> ↑top 1 ↑test ↑test ↑top 1 ↓test ↓test ↓test ↓test Activation Function accuracy score AUC accuracy MSE MSE MSE MSE Gaussian-Modulated Tangent Unit (GMTU) 0.915 0.861 0.784 0.676 0.053 0.104 0.015 0.035 Gaussian Error Linear Unit (GELU) 0.948 0.874 0.758 0.745 0.056 0.195 0.056 0.008 GELUSine 0.946 0.867 0.765 0.745 0.043 0.138 0.033 0.005 GELU-Sinc-Perturbation 0.948 0.887 0.776 0.739 0.043 0.160 0.052 0.020 Turbulent Activation Function 0.886 0.833 0.755 0.610 0.024 0.071 0.018 0.006 Quaternion-Inspired 0.514 0.888 0.699 OOM 0.006 0.006 0.004 0.004 Fourier-Informed Spectral Gating (FISG) 0.345 0.894 0.655 0.167 0.002 0.002 0.008 0.001 Phase-Locked Entropic Repulsion OOM 0.891 OOM OOM 0.009 0.001 0.019 0.000 Symmetric Phase-Flipped 0.229 0.878 0.657 0.001 0.002 0.001 0.004 0.001 ReLU 0.947 0.862 0.756 0.735 0.030 0.101 0.162 0.019

In terms of the training loss on the synthetic tasks, GELU 

is competitive with the discovered functions, aligning well with previous observations in the activation search literature. Specifically, it is well understood that standard performant activation functions often converge in similar ways on train-ing data. The critical differentiator is generalization on out of distribution test data: the previously-known functions (GELU and ReLU ) are worse than all discovered activation functions, with the turbulent function yielding a markedly better test loss than all competitors, despite its training loss being identical to that of ReLU .The other interesting observation that can be made inspect-ing these functions—both from their implementations in Appendix A and plots in Figure 4—is that they all have shapes related to established activation functions. Specifi-cally, they usually take the form of the sum or product of two terms. The first term tends to have the form of a stan-dard activation function, e.g. GELU , while the second term is usually a function that has a periodic shape and modulates this standard shape in a way that might be better at fitting periodic patterns. Two key examples are the GELUSine: 

GELUSine( x) = GELU( x) + 0 .1 sin x (1) and the GELU-Sinc-Perturbation: 

GELUSinc( x) = GELU( x)(1 + 0 .5 sinc x)= GELU( x)



1 + sin( πx )2πx 



In both of these cases, we note the usage of sine waves, which might allow a model to store information on in-domain data which would then get more easily retrieved when going out-of-domain, its reappearance guaranteed by the sine function’s periodicity. In Figure 5, for GELU-Sinc-Perturbation and GeLUSine 

as well as ReLU , we visualise the distribution of the pre-activation entries, which on the synthetic data ranges from 

[−2, 2] . This confirms that the activation functions are not used in a limited range where they would exhibit a very simi-lar shape as typical function, but rather the pre-activation are large enough to take advantage of the additional structure of the activation function. As it appears that function combinations are important for generalization, how can we reason about the optimal combi-nations between two given functions, and has AlphaEvolve found them? We attempt to answer this by recognizing that all of the multiplicative constants used in these expressions can be replaced by a configurable hyperparameter, α, and we can then randomly sweep many sensible values of α – see 6Finding a Generalizable Activation Functions 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.50 

> Sym-Phase-Flip
> FISG
> Turbulent
> ReLU
> GMTU
> GeLU-Sine
> GELU
> GELU-Sinc

Activation functions' performance profile 

(log-ratio of standardised scores against ReLU) 

CIFAR10 

CLRS 

ogbg-molhiv 

ImageNet 

Mean Score 

ReLU Baseline 

Figure 6. Performances of the discovered activation functions, plotted against the ReLU baseline, on four downstream tasks (CIFAR-10, CLRS-30, ogbg-molhiv and ImageNet). 0

> 2
> 4
> 6
> 8
> 10
> Test Loss
> GMTU
> 0
> 1
> 2
> 3
> 4
> 5
> GeLUSine
> 0.10
> 0.15
> 0.20
> 0.25
> 0.30
> 0.35
> 0.40
> 0.45
> 0.50
> GELU-Sinc-Pert
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00
> 1.25
> 1.50
> 1.75
> 2.00
> Turbulent
> Discovered Hyperparameters

Figure 7. AlphaEvolve discovers hyperparameter configuration that outperform randomly-selected function combinations of the same form on our small-lab test datasets. 

Appendix C for the specific case of GELUSine. The result of this experiment may be observed in Figure 7, illustrating that the specific values of α proposed by AlphaEvolve (e.g. 

0.1 for GELUSine or 0.5 for GELUSinc) generally outper-form the majority of the random samples drawn from our hyperparameter grid. 

## 5. Downstream evaluation 

We now reintroduce the complete set of ten functions under study—including functions that tend to explicitly take into account batch statistics, such as the Quaternion-inspired ,

FISG , Phase-Locked Entropic Repulsion and Symmetric Phase-Flipped activation. Their implementations may also be found in Appendix A. The complete test results across both our small-scale lab and the downstream tasks may be observed in Table 2, and summarized results relative to our ReLU baseline on the downstream tasks—only for functions that did not cause any out-of-memory issues— may be observed in Figure 6. The tasks selected for this analysis (CIFAR-10, CLRS-30, ogbg-molhiv and ImageNet) do not necessarily share any commonalities with the synthetic datasets we use; the goal of our evaluation is to ensure that the evolution process did not overfit to the small-scale synthetic data, and to assess whether any meaningful out-of-domain generalization has emerged on downstream tasks that require it (in this case, we specifically target CLRS-30, as it has an explicit OOD test dataset). It may be readily observed that all functions exploiting the batch axis severely underperform on the image-based datasets, even if they may occasionally provide OOD ben-efits on the CLRS dataset. This implies that they have sacrificed their general applicability to achieve stronger gen-eralisation on “raw” distribution shifts. In some cases, the batch statistics computations even result in out-of-memory 7Finding a Generalizable Activation Functions 

(OOM) issues. Additionally, it is notable that the turbulent 

activation does not generalise well to any new dataset; in spite of its superior pointwise performance in the lab, it has unfortunately learnt an overly-fitted combination of standard functions, and failed to transfer outside of the lab. On the other side of the performance spectrum, we note that the (re-discovered) Gaussian Error Linear Unit (GELU) ,as well as its (newly-discovered) variants ( GELUSine and 

GELU-Sinc-Perturbation ) all outperform the ReLU baseline. This implies a solid degree of transfer outside of the lab, and validates our approach of using small scale synthetic datasets to discover activation functions that appear to be generically applicable in different task instances. In particular, the novel GELU-Sinc-Perturbation function appears to perform the best overall – offering a differentiat-ing performance on the CLRS-30 benchmark, a representa-tive task requiring OOD generalization, without sacrificing performance on the image benchmarks, and offering im-provements on the molecular benchmark ogbg-molhiv. This discovery implies that it is possible to usefully transfer findings outside of the small-scale lab we built and into larger architectures. Furthermore, it indicates that sim-ple compositions of known performant activations with carefully-calibrated periodic components may unlock useful gains in generalization ability, while retaining the gener-ality and utility of the base function. In future work, we believe it would be important to study exactly how such functions are able to capture these regularities—potentially with theoretical analysis as well. 

## 6. Discussion 

In this work we showcase the efficacy and flexibility of a sys-tem like AlphaEvolve to do a particular, important flavour of architecture search: discovery of new activation func-tions. In contrast with previous work, AlphaEvolve allows for a generic search in the space of all possible activation functions, rather than limiting the search to predefined com-positions over a small set of simple transformations. When generating such functions within the framework, due to the use of capable frontier language models, there is a tendency for the system to produce not only code, but also comments describing the proposed function’s properties (following standard coding practices). Compared to other search mech-anisms, this allows a glimpse into the motivation for the design of proposed activation functions (please see the pro-vided generated code in Appendix A). While the validity of the rationale given by the LLM can be questionable, we argue that this feature can enable the researcher not only to generate new proposed activation function, but to inform them about potential hypotheses on why the activation func-tion is effective—some of these hypotheses may themselves be easily testable. An additional observation is that the most performant acti-vation functions – striking a “sweet spot” between the lab environment and downstream tasks – seem to have a typical form of a sum or product of two terms. The second term in this sum typically takes the form of a periodic signal, such as 

sin or sinc . This choice of periodic signal is interesting, as traditional activation functions tend to be purely aperiodic, and it is expected that this would make modeling periodic data difficult. In particular, standard activation functions can model any observed periodicity well within their training domain, but they will not be able to extrapolate – which is exactly the objective we used in our evolutionary search. 

## 7. Conclusions 

We have shown how AlphaEvolve offers a flexible frame-work for mining generalizable activation functions—an im-portant form of architecture search. Compared to previous approaches, it allows a more generic formulation of the problem, defining the space of possible activation functions as any Python function that respects the required signature of converting a tensor into a tensor of same size. Relying on this framework, we propose a simple and effec-tive protocol, whereby we provide a small synthetic dataset to be used for training within the internal loop of the evo-lutionary search, and show that this approach can discover functions which seem to transfer well to more standard and more computationally expensive benchmarks—in a way that retains strong performance on classical vision tasks while offering higher generalization on tasks requiring OOD gen-eralization. 

## Impact Statement 

This paper fits within a broad amount of work that aims to improve the rate of novel architecture discovery—focusing on a specific aspect of generalizable activations. Any sig-nificant improvements in this space may transfer to more potent self-improving AI systems in the future. 

## References 

Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 , 2025. Crick, F. The recent excitement about neural networks. 

Nature , 1989. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, 8Finding a Generalizable Activation Functions 

L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248–255. Ieee, 2009. Dubey, S. R., Singh, S. K., and Chaudhuri, B. B. Activation functions in deep learning: A comprehensive survey and benchmark. Neurocomputing , 503:92–108, September 2022. Elsken, T., Metzen, J. H., and Hutter, F. Neural architecture search: a survey. JMLR , 20(1):1997–2017, January 2019. ISSN 1532-4435. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,pp. 770–778, 2016. Hendrycks, D. and Gimpel, K. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. 

CoRR , abs/1606.08415, 2016. Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems , 33:22118–22133, 2020. Ibarz, B., Kurin, V., Papamakarios, G., Nikiforou, K., Ben-nani, M., Csord ´as, R., Dudzik, A. J., Bo ˇsnjak, M., Vitvit-skyi, A., Rubanova, Y., et al. A generalist neural algorith-mic learner. In Learning on graphs conference , pp. 2–1. PMLR, 2022. Kipf, T. Semi-supervised classification with graph con-volutional networks. arXiv preprint arXiv:1609.02907 ,2016. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. LeCun, Y., Bottou, L., Orr, G. B., and M ¨uller, K. Efficient backprop. In Neural Networks: Tricks of the Trade -Second Edition . 2012. Mont ´ufar, G., Pascanu, R., Cho, K., and Bengio, Y. On the number of linear regions of deep neural networks. In 

Neural Information Processing Systems , 2014. Nadimpalli, K. V., Chirra, S. R., Varakantham, P., and Bauer, S. Evolving RL: Discovering new activation functions using LLMs. In Towards Agentic AI for Science: Hypoth-esis Generation, Comprehension, Quantification, and Val-idation , 2025. URL https://openreview.net/ forum?id=H2x9juCuJg .Nair, V. and Hinton, G. E. Rectified linear units improve re-stricted boltzmann machines. In International Conference on Machine Learning (ICML) , 2010. Novikov, A., Vu, N., Eisenberger, M., Dupont, E., Huang, P., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F. J. R., Mehrabian, A., Kumar, M. P., See, A., Chaudhuri, S., Holland, G., Davies, A., Nowozin, S., Kohli, P., and Balog, M. Alphaevolve: A coding agent for scientific and algorithmic discovery. Preprint , 2025. Raghu, M., Poole, B., Kleinberg, J. M., Ganguli, S., and Sohl-Dickstein, J. On the expressive power of deep neu-ral networks. In International Conference on Machine Learning, ICML , 2017. Ramachandran, P., Zoph, B., and Le, Q. V. Search-ing for activation functions. ArXiv , abs/1710.05941, 2018. URL https://api.semanticscholar. org/CorpusID:10919244 .Roy, K., Jaiswal, A., and Panda, P. Towards spike-based ma-chine intelligence with neuromorphic computing. Nature ,2019. Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. In Interna-tional Conference on Learning Representations (ICLR) ,2015. Udrescu, S.-M. and Tegmark, M. Ai feynman: A physics-inspired method for symbolic regression. Science ad-vances , 6(16):eaay2631, 2020. Veli ˇckovi ´c, P., Badia, A. P., Budden, D., Pascanu, R., Ban-ino, A., Dashevskiy, M., Hadsell, R., and Blundell, C. The clrs algorithmic reasoning benchmark. In Inter-national Conference on Machine Learning , pp. 22084– 22102. PMLR, 2022. Zoph, B. and Le, Q. Neural architecture search with reinforcement learning. In International Conference on Learning Representations , 2017. URL https:// openreview.net/forum?id=r1Ue8Hcxg .9Finding a Generalizable Activation Functions 

## A. Activation Functions Code 

def activation_function(x: jax.typing.ArrayLike) -> jax.typing.ArrayLike: 

# GeLUSine: A GELU-based activation with a sinusoidal component. # f(x) = GELU(x) + 0.1 * sin(x) ## Rationale: # 1. Strong Baseline: Builds upon GELU, a proven SOTA # activation function, inheriting its desirable properties like smoothness # and stochastic motivation. # 2. Periodic Exploration: The added sinusoidal term introduces periodic, # non-monotonic wiggles. This can help the optimization process explore # the loss landscape more effectively, potentially escaping local minima # and finding more robust solutions. # 3. Enhanced Expressiveness: The periodic component allows neurons to model # more complex, oscillatory patterns, potentially increasing the # representational power of the network without significantly increasing # computational cost. It's a form of implicit frequency analysis. # 4. Controlled Complexity: By adding a small, bounded sinusoidal wave, we # introduce complexity in a controlled manner, avoiding the instability # that might arise from more chaotic functions. 

return jax.nn.gelu(x) + 0.1 * jax.numpy.sin(x) 

def activation_function(x: jax.typing.ArrayLike) -> jax.typing.ArrayLike: 

# GELU-Sinc-Perturbation (GSP): A more stable "crazy" idea. # This activation function perturbs the standard GELU with a scaled, decaying # sinc function. # The original SiGELU (gelu(x) * sinc(x)) suffered from two issues: # 1. Sign flipping for x > 0, which can destabilize training. # 2. It squashes large activations, behaving like sin(pi*x)/pi, which can # cause vanishing gradients. # Rationale: GSP addresses this by using `gelu(x) * (1 + 0.5 * sinc(x)) `.# This preserves GELU's asymptotic behavior (f(x) -> gelu(x) as |x| -> inf), # prevents sign-flipping, and confines the oscillatory "craziness" to a # region around the origin, potentially increasing expressiveness for # low-magnitude activations without sacrificing stability. 

alpha = 0.5 

return jax.nn.gelu(x) * (1.0 + alpha * jax.numpy.sinc(x)) 

def activation_function(x: jax.typing.ArrayLike) -> jax.typing.ArrayLike: 

# Turbulent Activation Function # This function introduces non-monotonic, input-distribution-dependent # "ripples" into a stable activation function. The goal is to prevent # the network's information flow from becoming too "laminar" (e.g., # getting stuck in saturated regions or local minima). # The base function provides a stable, symmetric logarithmic growth. 

base = jax.numpy.sign(x) * jax.numpy.log1p(0.5 * jax.numpy.abs(x)) 

10 Finding a Generalizable Activation Functions 

# A statistics-dependent perturbation term is added, making the activation # non-local and aware of the distribution of its inputs for a given batch. 

mean = jax.numpy.mean(x, keepdims= True )std = jax.numpy.std(x, keepdims= True ) + 1e-6 # Epsilon for stability # Standardize x for the perturbation calculation. 

z = (x - mean) / std 

# The perturbation is a sine wave of the input `x` (ensuring f(0)=0), # modulated by a Gaussian function of the standardized input `z`. This # creates the largest "ripples" for inputs near the batch mean. 

amplitude = 0.2 frequency = 2.0 gaussian_envelope = jax.numpy.exp(-0.5 * z**2) perturbation = amplitude * gaussian_envelope * jax.numpy.sin(frequency * x) 

return base + perturbation 

def activation_function(x): 

# "Gaussian-Modulated Tangent Unit" (GMTU) # This function models a signal passing through a resonant chamber, # creating a primary response followed by a series of decaying echoes. # The goal is to create a complex but smooth activation landscape with # multiple regions of high non-linearity, which can potentially capture # more intricate features in the data. ## Rationale: # 1. Primary Response: A localized, non-periodic response around the origin, # similar to GMTLU, provides a strong, stable non-linearity for small inputs. # 2. Asymptotic Linearity: A linear leak term ensures that for large |x|, # where the Gaussian components decay to zero, the function behaves # linearly, preventing saturation and aiding gradient flow. # -- Gated Response Parameters --

p_alpha = 1.0 # Amplitude 

p_beta = 1.5 # Steepness of tanh 

p_gamma = 0.2 # Decay rate of Gaussian # -- Linear leak --

leak = 0.1 

# --- Calculation ---# 1. Primary Response 

primary_response = p_alpha * jnp.tanh(p_beta * x) * jnp.exp(-p_gamma * x**2) 

# 2. Combine and add linear leak 

return primary_response + (leak * x) 

def activation_function(x: jax.typing.ArrayLike) -> jax.typing.ArrayLike: 

# "Quaternion-Inspired Hypercomplex Gated Activation". # This version introduces parameter self-modulation. First, the phase of the # oscillation for the B parameter (Gaussian width) is modulated by the 

11 Finding a Generalizable Activation Functions 

# value of the A parameter (damping amplitude). Second, the input to the tanh # function for the imaginary part of the complex shift (Ci) is modulated by the # real part (Cr). This creates a coupled dynamic system, leading to a more # complex and input-dependent activation shape. ## This function extends the complex-plane concept by incorporating feedback # from the imaginary component of the complex damping term into the gate, # creating a more intricate and asymmetric activation landscape. # with controlled, high-frequency oscillations contained within a damping envelope. # The gate is modified from `1-Re(Z) ` to `1 - (Re(Z) - k*Im(Z)) `, where Z is # the complex damping term. This is equivalent to rotating and scaling Z. # To enhance the chaotic nature, the amplitudes of the Chebyshev polynomials # are modulated by the output of a logistic map, creating a "chaotic envelope". # The complex shift `C_complex = Cr(x) + i*Ci(x) ` still has its real and imaginary # parts independently controlled by different Chebyshev polynomials of `tanh(x) `.# `Cr(x) ` uses T_4 (4th order) and `Ci(x) ` uses T_3 (3rd order). This mismatch # in polynomial orders is intended to create complex, non-repeating interference # patterns. # The effect of the imaginary part `Ci ` is to introduce a cosine modulation # `cos(2*B*(x-Cr)*Ci) ` under the Gaussian envelope, creating wave-packet-like # features, now supplemented by a sine component from the imaginary part. # This creates a more structured oscillation while maintaining desirable # properties like `f(0)=0 ` and `f(x)->x ` for large `|x| `.## This version is extended to be "quaternion-inspired", adding two more # hypercomplex components (j, k) to the gate, creating a richer activation # landscape from four interacting components (w, i, j, k). 

A_base = 0.1 A_amp = 0.05 A_freq = 4.0 A = A_base + A_amp * jax.numpy.cos(A_freq * x) B_base = 0.5 B_amp = 0.2 B_freq = 2.5 

# Introduce self-modulation: B's phase is modulated by A's value. 

B_phase_mod_coeff = 2.0 B = B_base + B_amp * jax.numpy.sin(B_freq * x + B_phase_mod_coeff * A) 

# Restore a more complex wobble using the Chebyshev polynomial T_4(u) = 8uˆ4 - 8uˆ2 + 1. # This introduces more oscillations to create a richer activation landscape, # leaning back into the "chaotic" spirit of the function's name. # Use a smooth, complex periodic function instead of a chaotic map for modulation. # This creates a "beat" frequency-like modulation which is less erratic # than the logistic map, potentially leading to a smoother optimization landscape. 

mod_freq1 = 2.1 mod_freq2 = 1.3 # Using different frequencies to create complex interference 

y_chaos = (jax.numpy.sin(mod_freq1 * x) * jax.numpy.cos(mod_freq2 * x) + 1.0) / 2.0 C_freq = 2.0 C_amp = 0.05 + 0.1 * y_chaos # Modulate C_amp, avg=0.1 

C_base = 0.1 tanh_x = jax.numpy.tanh(C_freq * x) u = tanh_x cheby_T4 = 8 * u**4 - 8 * u**2 + 1.0 Cr = C_base + C_amp * cheby_T4 # The real part is the original T4 wobble 

12 Finding a Generalizable Activation Functions 

# The imaginary part `Ci ` uses Chebyshev T_3(u) = 4uˆ3 - 3u for chaotic interference. 

Ci_freq = 2.0 Ci_amp = 0.2 + 0.4 * (1.0 - y_chaos) # Modulate w/ inverted chaos, avg=0.4 

Ci_base = 0.0 # Center imaginary part around 0 # Decouple Ci from Cr for a simpler, potentially more stable interaction. # The original coupling created a very complex relationship that might hinder # optimization. By making Ci depend only on x, we simplify the function's # structure and gradient landscape, while retaining the complex interaction # in the later stages of the function. 

u_i = jax.numpy.tanh(Ci_freq * x) cheby_T3 = 4 * u_i**3 - 3 * u_i Ci = Ci_base + Ci_amp * cheby_T3 

# Stabilize the damping term by removing the anti-damping component. # The original formulation jax.numpy.exp(-B * (x - (Cr + 1j*Ci))**2) expands to # jax.numpy.exp(-B*(x-Cr)**2) * jax.numpy.exp(B*Ci**2) * jax.numpy.exp(2j*B*(x-Cr)*Ci). # The exp(B*Ci**2) term can cause instability. We remove it to ensure the # magnitude is always a decaying Gaussian, while preserving the complex phase. 

stable_magnitude = A * x**2 * jax.numpy.exp(-B * (x - Cr)**2) phase = 2 * B * Ci * (x - Cr) 

# Reconstruct the complex number using Euler's formula: exp(i*theta) = cos(theta) + i*sin(th 

complex_rotation = jax.numpy.cos(phase) + 1j * jax.numpy.sin(phase) damping_term_complex = stable_magnitude * complex_rotation 

# --- Quaternion-Inspired Components (j, k) ---# Introduce two more "hypercomplex" components for a quaternion-inspired gate. # These components introduce additional, semi-orthogonal oscillations. 

D_freq = 1.5 D_amp = 0.1 + 0.2 * y_chaos u_d = jax.numpy.tanh(D_freq * x) cheby_T2 = 2 * u_d**2 - 1.0 D_shift = D_amp * cheby_T2 

# Envelope for the j-component, using A and a shifted Gaussian with width B. # Includes x**2 to ensure f(0)=0 and f'(0)=1. 

Q_j_envelope = (0.5 * A * x**2) * jax.numpy.exp(-B * (x - D_shift)**2) 

# Envelope for the k-component, swapping A and B for variety. 

Q_k_envelope = (0.5 * B * x**2) * jax.numpy.exp(-A * (x + D_shift)**2) 

# j-component oscillation, coupled to Ci. 

Q_j = Q_j_envelope * jax.numpy.sin(C_freq * x + Ci) 

# k-component oscillation, coupled to Cr. 

Q_k = Q_k_envelope * jax.numpy.cos(B_freq * x - Cr) 

# --- Gate Calculation ---# The gate is a linear combination of the four (w, i, j, k) components. 

Q_w = jax.numpy.real(damping_term_complex) Q_i = jax.numpy.imag(damping_term_complex) c_i = 0.2 # Feedback coefficient for the i-component 

c_j = 0.15 # Feedback coefficient for the j-component 

c_k = 0.15 # Feedback coefficient for the k-component 

gate = 1.0 - (Q_w - c_i * Q_i - c_j * Q_j - c_k * Q_k) 

return x * gate 

13 Finding a Generalizable Activation Functions 

def activation_function(x: jax.typing.ArrayLike) -> jax.typing.ArrayLike: 

"""Fourier-Informed Spectral Gating (FISG). This function introduces a "crazy" idea: using the entire feature vector's frequency spectrum as a global signal for OOD detection. It departs from local methods like ACAN by performing a Fast Fourier Transform (FFT) to analyze the holistic structure of activations, hypothesizing that OOD samples disrupt the natural frequency distribution. Theoretical Justification: 1. **Global vs. Local Anomaly Detection:** ACAN detects anomalies based on local context (immediate neighbors). This is effective for spike-like noise but may miss subtle, distributed OOD patterns. FISG operates in the Fourier domain, providing a global view of the entire feature vector's structure, allowing it to detect systemic anomalies in the frequency distribution. 2. **The Spectral Smoothness Prior:** Well-generalized features, like natural signals, are hypothesized to have power spectra where energy is concentrated in lower frequencies. OOD data (e.g., adversarial noise, corruptions) often violates this prior by injecting significant energy into high frequencies. FISG directly weaponizes this prior. 3. **Adaptive Damping via Spectral Imbalance:** The function computes a "spectral imbalance" score for each sample|the ratio of high-frequency to total energy. This score adaptively controls a gate that blends the original activation `x` with a smoothed version ( `neighbors_avg `). High imbalance (likely OOD) triggers strong smoothing, damping anomalous high-frequency components. 4. **Decoupled Gradient Signal:** By using `jax.lax.stop_gradient `, the network is not forced to directly minimize spectral imbalance. Instead, the gate acts as a non-differentiable regularizer on the forward pass, challenging the model to find robust features that remain stable even when spectrally-anomalous components are suppressed. """ # --- Hyperparameters ---

sensitivity = 2.0 # Controls how strongly spectral imbalance affects the gate. 

split_fraction = 0.25 # Fraction of frequencies considered "low". 

epsilon = 1e-7 x_float = x.astype(jnp.float32) 

# --- 1. Fourier Analysis (Global Structural Signal) ---# Compute the real FFT along the feature axis, keeping the complex result. 

x_fft = jnp.fft.rfft(x_float, axis=-1) magnitudes = jnp.abs(x_fft) 

# Split frequencies into low and high bands. 

num_freqs = x_fft.shape[-1] split_idx = int(num_freqs * split_fraction) 

14 Finding a Generalizable Activation Functions 

# Calculate energy in high-frequency band vs. total energy. 

high_freq_energy = jnp.sum( magnitudes[..., split_idx:], axis=-1, keepdims= True 

)total_energy = jnp.sum(magnitudes, axis=-1, keepdims= True )

# The OOD signal is the ratio of high-frequency energy to total energy. # A high ratio suggests a spectrally anomalous (e.g., noisy) feature vector. 

spectral_imbalance = jax.lax.stop_gradient( high_freq_energy / (total_energy + epsilon) )

# --- 2. Adaptive Gating based on Spectral Imbalance ---# The gate exponentially decays as spectral imbalance increases. 

gate = jnp.exp(-sensitivity * spectral_imbalance) 

# --- 3. Adaptive High-Frequency Phase Scrambling ---# For feature vectors deemed OOD, we regularize by scrambling the phase of # high-frequency components, disrupting their structure without losing energy. 

low_freq_part = x_fft[..., :split_idx] high_freq_part = x_fft[..., split_idx:] 

# Scramble by complex conjugation (which deterministically negates the phase). 

scrambled_high_freq_part = jnp.conj(high_freq_part) 

# Recombine the spectrum and invert the FFT to get the modified signal. 

scrambled_x_fft = jnp.concatenate( [low_freq_part, scrambled_high_freq_part], axis=-1 )

# The length `n` for irfft must match the original signal length. 

modified_x = jnp.fft.irfft(scrambled_x_fft, n=x_float.shape[-1], axis=-1) 

# Perform principled blending: a convex combination of the original activation # and its phase-scrambled version, governed by the global spectral gate. 

return gate * x + (1.0 - gate) * modified_x.astype(x.dtype) 

def activation_function(x: jax.typing.ArrayLike) -> jax.typing.ArrayLike: 

"""'Phase-Locked Entropic Repulsion' (PLER) for OOD regularization. This function implements a novel gating mechanism based on the interaction between two chaotic systems: a primary system driven by the input `x`, and afixed reference oscillator. The nature of their coupling changes based on the input magnitude, creating two distinct dynamical regimes for ID and OOD inputs. Theory: OOD generalization is enhanced by creating a sharp bifurcation in the activation's dynamical behavior. 1. In-Distribution (Phase-Locking): For small `|x| `, the coupling is attractive, forcing the primary system to synchronize with the stable chaos of the reference oscillator. This "phase-locking" reduces the system's entropy, leading to a stable, predictable gate that preserves in-distribution signals. It creates a stable attractor basin for the ID manifold. 

15 Finding a Generalizable Activation Functions 

2. Out-of-Distribution (State Collapse): For large `|x| `, the coupling becomes repulsive. This bifurcation not only drives the systems apart but also fundamentally alters the primary system's dynamics by introducing a strong attractor at a quiescent (zero) state. This "bifurcation-induced collapse" deterministically silences the neuron's output for OOD inputs, providing a more stable and decisive suppression mechanism than pure chaotic repulsion. 3. Bifurcation Control: The input `x` controls the coupling strength `beta `,smoothly transitioning it from positive (attractive) to negative (repulsive) as `|x| ` increases, thereby controlling the bifurcation between the two regimes. """ # --- Primary System Parameters (Input-Dependent) ---

r = 2.5 + 1.5 * jnp.tanh(x**2 / 4.0) alpha = 0.1 * jnp.tanh(x**2 / 16.0) 

# --- Reference Oscillator Parameters (Fixed) ---

r_ref = 3.9 alpha_ref = 0.05 

# --- Inter-System Coupling (Bifurcation Control) ---# `beta ` smoothly transitions from positive (attractive) for ID inputs # to negative (repulsive) for OOD inputs. 

beta = 0.1 * (1.0 - 2.0 * jnp.tanh(x**2 / 8.0)) 

# --- Chaotic Resonance Tunneling Parameters ---# `omega_ref ` creates an input-dependent "resonant frequency". When this # frequency is near zero, it signifies a resonance condition. 

omega_ref = jnp.cos(x * 2.5) 

# `resonance_gate ` is a sharp filter that is only "open" when the resonance # condition is met. This creates narrow "absorption bands" in the input space # that act as traps for OOD signals. 

resonance_gate = jnp.exp(-25.0 * omega_ref**2) 

# --- System Initialization ---# Primary system initialized based on input `x`.

y = 0.5 + 0.49 * jnp.tanh(x / 4.0) z = 0.5 - 0.49 * jnp.tanh(x / 4.0) 

# Reference system with fixed initial conditions, matching batch shape. 

y_ref = jnp.full_like(x, 0.2) z_ref = jnp.full_like(x, 0.8) 

# "Chaotic Memory" state initialized to zero. 

c = jnp.zeros_like(x) 

# --- Iterate the Coupled Dynamical Systems ---

def pler_map_step(i, state): y_val, z_val, y_ref_val, z_ref_val, c_val = state 

# 1. State-Dependent Bifurcation Control # Based on the chaotic memory from the *previous* step ( `c_val `), we # modulate the coupling `beta `. If the system was unstable, `beta ` is # pushed towards negative (repulsive) values, dynamically engaging the # OOD suppression mechanisms. This makes OOD detection sensitive to the 

16 Finding a Generalizable Activation Functions 

# input's dynamical impact, not just its static magnitude. 

instability_feedback = jnp.tanh(c_val * 4.0) beta_eff = beta - 0.2 * instability_feedback is_ood = 1.0 / (1.0 + jnp.exp(beta_eff * 50.0)) 

# 2. Internal dynamics of the primary system, with adaptive dissipation 

coupling_internal = alpha * (z_val - y_val) y_dyn = r * y_val * (1 - y_val) + coupling_internal z_dyn = r * z_val * (1 - z_val) - coupling_internal 

# Bifurcation-Induced Collapse & Adaptive Dissipation: For OOD inputs, # two mechanisms are triggered to suppress the signal. # 1. Collapse: An additive force creates a strong attractor at zero. # 2. Dissipation: A multiplicative force dampens remaining oscillations. # NEW: Bifurcation-Induced Collapse. This additive force creates a strong # attractor at zero for OOD inputs, decisively silencing the neuron. 

collapse_strength = 0.5 y_dyn -= is_ood * collapse_strength * y_val z_dyn -= is_ood * collapse_strength * z_val 

# Dissipation strength `gamma ` is gated by the OOD switch and chaotic memory. 

gamma = is_ood * jnp.tanh(c_val * 4.0) 

# Apply multiplicative dissipation to suppress the internal dynamics. 

y_next = y_dyn * (1.0 - gamma) z_next = z_dyn * (1.0 - gamma) 

# NEW: Chaotic Resonance Tunneling. # If the input hits a resonant frequency, a strong "tunneling" force # is activated, which rapidly collapses the primary system's state # towards a neutral midpoint (0.5). This provides a secondary, # value-specific OOD suppression mechanism. 

tunneling_strength = 0.6 tunnel_force_y = resonance_gate * tunneling_strength * (0.5 - y_next) tunnel_force_z = resonance_gate * tunneling_strength * (0.5 - z_next) y_next += tunnel_force_y z_next += tunnel_force_z 

# 5. Internal dynamics of the reference system 

coupling_ref_internal = alpha_ref * (z_ref_val - y_ref_val) y_ref_next = r_ref * y_ref_val * (1 - y_ref_val) + coupling_ref_internal z_ref_next = r_ref * z_ref_val * (1 - z_ref_val) - coupling_ref_internal 

# 6. Chaotic Memory Update # `c` accumulates the residual divergence between y and z after dissipation, # acting as a memory of recent instability. 

instability = jnp.abs(y_next - z_next) c_next = 0.8 * c_val + 0.2 * instability # EMA of instability # 7. Synchronizing / Repulsive coupling with "Chaotic Memory Feedback" # For OOD inputs (negative beta), the repulsive force is non-linearly # amplified by the primary system's own state `z`, leading to more # rapid and chaotic signal scrambling. The tanh term acts as a gate, # ensuring this effect is negligible for ID inputs. # For OOD inputs, the repulsive force is amplified by two factors: 

17 Finding a Generalizable Activation Functions 

# a) `ood_modulation `: instantaneous state-dependent amplification. # b) `ood_amplification `: history-dependent amplification from `c`.# This creates a positive feedback loop for OOD signals. 

ood_modulation = 1.0 + jnp.tanh(jnp.abs(beta_eff) * 5.0) * z_val**2 ood_amplification = 1.0 + jnp.tanh(c_next * 2.0) coupling_sync = (beta_eff * (y_ref_val - y_val) * ood_modulation * ood_amplification )

# Apply the coupling force. For ID inputs (is_ood˜0), the coupling is # a symmetric action-reaction pair. For OOD inputs (is_ood˜1), we break # this symmetry by nullifying the reaction force. This turns the interaction # into a one-way "parasitic" drain, guaranteeing the primary system's collapse. 

y_next += coupling_sync y_ref_next -= (1.0 - is_ood) * coupling_sync 

# Clip all states to maintain stability within the [0, 1] interval. 

y_next = jnp.clip(y_next, 0.0, 1.0) z_next = jnp.clip(z_next, 0.0, 1.0) y_ref_next = jnp.clip(y_ref_next, 0.0, 1.0) z_ref_next = jnp.clip(z_ref_next, 0.0, 1.0) 

return y_next, z_next, y_ref_next, z_ref_next, c_next 

# More iterations to allow the complex dynamics to develop. 

y, z, _, _, _ = jax.lax.fori_loop( 0, 10, pler_map_step, (y, z, y_ref, z_ref, c) )

# The final gate is derived from the primary system's state. 

gate = (y + z) / 2.0 

return x * gate 

def activation_function(x: jax.typing.ArrayLike) -> jax.typing.ArrayLike: 

"""A Symmetric Phase-Flipped Activation Function for OOD Generalization. This function models a neuron's activation by switching between two symmetric, phase-flipped chaotic states. This is inspired by the principle of creating azero-mean, high-variance energy landscape for OOD inputs to prevent confident extrapolation. The switching is controlled by a pseudo-chaotic function, creating a fractal-like decision boundary in the OOD transition region. Theoretical Justification for OOD: 1. **Stable ID Superposition:** For in-distribution data (small `|x| `), a localized chaotic perturbation term is near-zero. Both phase-flipped states collapse to the same stable function, which behaves like a scaled identity, ensuring effective learning of ID features. 2. **Adaptive Chaotic Decoherence:** In the OOD transition region, the chaotic term becomes significant. The nature of this chaos is adaptive: for mildly OOD inputs, a lower-frequency perturbation is used, while for strongly OOD inputs (as determined by the collective energy of a neuron and its neighbors), the function switches to a high-frequency, high-amplitude chaotic mode. This state-dependent blending creates a highly non-linear gradient landscape that aggressively resists coherent extrapolation. 

18 Finding a Generalizable Activation Functions 

3. **Meta-Modulated Chaotic Amplitude:** Instead of a fixed-profile chaotic regularizer, the amplitude of the chaotic term is itself modulated by a function of the input's magnitude. This `meta_modulator ` is designed to be near 1 for ID inputs but aggressively amplifies the chaos in the critical ID-to-OOD transition zone. This creates a much sharper, more repulsive gradient landscape precisely where the model is most vulnerable to smooth extrapolation, while ensuring the amplification effect decays for far-OOD inputs, contributing to the safe collapse to zero. 4. **Intrinsic Collapse to Zero for Far-OOD:** For far-OOD inputs, the base signal `x` is smoothly attenuated by an exponential decay factor *inside* the `tanh `. Simultaneously, the chaotic amplitude naturally decays. This causes both phase-flipped states to intrinsically and smoothly converge to zero, ensuring a safe default output without a brittle external gate. This unified mechanism provides a smoother gradient landscape at the OOD boundary. 5. **Non-Local Phase Entanglement and State Propagation:** The chaotic phase of each neuron is coupled to its neighbors. Crucially, the transition to the more aggressive chaotic state also depends on the neighbors' energy, allowing "agitated" states to propagate like waves. For OOD inputs that violate learned spatial correlations, this triggers propagating waves of high-frequency phase decoherence, creating a volatile, high-dimensional gradient landscape that aggressively resists confident extrapolation. 6. **Spatially-Aware Chaos Triggering:** The transition to the high-chaos 'agitated' state is triggered not only by high activation energy but also by high spatial inconsistency, measured by a discrete Laplacian. This makes the neuron highly sensitive to OOD inputs that violate learned local correlations (e.g., unnatural textures or adversarial noise), even if their activation magnitudes are not extreme. By directly coupling structural anomaly detection to the chaos-inducing mechanism, the function provides a more targeted and robust defense against a wider variety of OOD patterns, rather than relying solely on magnitude-based heuristics. """ 

M = 10.0 # Controls the saturation level of the base function. 

C = 10.0 # Controls the transition boundary from ID to OOD region. 

beta = 2.0 # Controls the max amplitude of the high-frequency component. 

freq = 1.0 # Controls the base frequency of the oscillatory component. 

chirp_k = 0.5 # Controls the rate of frequency increase (chirp). 

A_disrupt = 0.2 # Amplitude of the 'calm' phase disruption. 

freq_disrupt = 15.0 # Frequency of the 'calm' phase disruption. 

coupling_strength = 2.0 # Strength of neighbor-based phase coupling. # --- State-Dependent Adaptive Chaos Hyperparameters ---

A_disrupt_agitated = 0.8 # Amplitude of the 'agitated' phase disruption. 

freq_disrupt_agitated = 40.0 # Frequency of the 'agitated' phase disruption. 

k_blend = 2.0 # Controls the steepness of the blend between modes. 

laplacian_strength = 5.0 # Strength of the spatial inconsistency term. 

k_decay = 2.0 # Controls the rate of decay for far-OOD inputs. # --- Quantum Switching Hyperparameters ---

freq_switch = 50.0 # High frequency to create chaotic switching. 

power_switch = 3.0 # Non-linearity for the switching phase. 

A_switch_disrupt = 0.5 # Amplitude of switching phase disruption. 

19 Finding a Generalizable Activation Functions 

freq_switch_disrupt = 25.0 # Frequency of switching phase disruption. # --- Meta-Modulation Hyperparameters ---

gamma_meta = 2.0 # Controls the amplification of chaos in the OOD transition. # --- Meta-Modulation of OOD Response ---# This term adaptively amplifies the chaotic signal in the critical OOD # transition region, creating a sharper, more repulsive gradient landscape # exactly where confident extrapolation is most dangerous. 

u = (x / C)**2 

# The modulator peaks slightly earlier (u=1.5) than the base amplitude (u=2.0), # creating a pre-emptive amplification of the chaotic response. 

meta_modulator = 1.0 + gamma_meta * u * jnp.exp(-u / 1.5) 

# --- OOD Regularizer (Localized Chaotic Wave) ---# This component creates a "ring of chaos" that decays for far-OOD inputs. 

amplitude = beta * u * jnp.exp(-u / 2.0) 

# The phase is quadratic, but with a high-frequency sinusoidal disruption. # This makes the local frequency non-monotonic and chaotic for OOD inputs, # acting as a stronger regularizer against confident extrapolation. 

phase_base = freq * x + chirp_k * (x**2) * jnp.sign(x) / C 

# Get neighbors for coupling and state-dependent chaos. 

x_prev = jnp.roll(x, shift=1, axis=-1) x_next = jnp.roll(x, shift=-1, axis=-1) 

# --- Adaptive Phase Disruption ---# The chaotic disruption smoothly transitions from a 'calm' to an 'agitated' # mode based on both collective energy and local spatial inconsistency. 

local_energy_sq = x**2 + 0.25 * (x_prev**2 + x_next**2) local_laplacian = x - 0.5 * (x_prev + x_next) 

# The trigger for the agitated state combines energy (magnitude) and spatial # inconsistency (Laplacian), making it sensitive to a wider class of OOD inputs. 

ood_metric = (k_blend * (local_energy_sq - C**2) / C**2 + laplacian_strength * (local_laplacian / C)**2) alpha = jax.nn.sigmoid(ood_metric) phase_disruption_calm = A_disrupt * jnp.sin(freq_disrupt * x) 

# The agitated phase disruption is made sensitive to local spatial # inconsistencies (approximated by the discrete Laplacian). This allows the # neuron to react more aggressively to OOD inputs that violate learned # spatial correlations (e.g., unnatural textures or edges), providing a # more targeted OOD response. 

phase_disruption_agitated = A_disrupt_agitated * jnp.sin( freq_disrupt_agitated * x + laplacian_strength * local_laplacian )phase_disruption = (1.0 - alpha) * phase_disruption_calm + alpha * phase_disruption_agitated 

# Non-local phase coupling creates waves of chaos for OOD patterns. # We use a simple, asymmetric coupling stencil: 0.5*x_{i-1} - 1.0*x_{i+1}, # implemented efficiently using jnp.roll along the last axis. 

phase_coupling = coupling_strength * (0.5 * x_prev - 1.0 * x_next) / C phase = phase_base + phase_disruption + phase_coupling y_detail = amplitude * jnp.sin(phase) 

20 Finding a Generalizable Activation Functions 

# --- Symmetric Phase-Flipped State-Switching with Intrinsic Collapse ---# For far-OOD inputs, an exponential gate `g_x ` attenuates the base signal # `x`, while the chaotic amplitude `y_detail ` also decays. This causes # both states to smoothly converge to zero, providing a robust and # gradient-friendly collapse without an external multiplicative gate. 

g_x = jnp.exp(-((jnp.abs(x) / (k_decay * C)))**4) y_state_plus = M * jnp.tanh((g_x * x + meta_modulator * y_detail) / M) y_state_minus = M * jnp.tanh((g_x * x - meta_modulator * y_detail) / M) 

# The switching phase is made non-monotonic by adding a sinusoidal disruption. # This fractalizes the switching boundary, making it harder to learn/exploit. 

switch_phase_base = freq_switch * (x / C)**power_switch switch_phase_disruption = A_switch_disrupt * jnp.sin(freq_switch_disrupt * x / C) switch_phase = switch_phase_base + switch_phase_disruption should_be_plus = jnp.cos(switch_phase) > 0.0 output = jnp.where(should_be_plus, y_state_plus, y_state_minus) 

return output 

## B. AlphaEvolve meta-prompt 

We slightly modify AlphaEvolve meta-prompt used within our setup to the following, in order to steer it towards generalizable functions more easily: Act as a Senior Machine Learning Researcher specializing in model robustness and OOD (Out-of-Distribution) generalization. Your task is to iteratively improve the OOD Evaluation Metric by modifying the activation functions in the provided code, where larger values are better. Theoretical Justification: For each proposal, explicitly explain why it mathematically supports OOD generalization better than the baseline. 

## C. GELUSine ablations 

We explore effect of the multiplicative hyperparameter in the GELUSine function, by studying activation functions of the form GELU( x) + α sin x. Our analysis shows that carefully combining the GELU and sine functions is important for minimizing the test loss on synthetic datasets, and that AlphaEvolve’s discovered choice of α = 0 .1 aligns well with the empirical evaluation of many random choices of α.0.0 0.1 0.2 0.3 0.4 0.5                

> alpha
> 0.030
> 0.035
> 0.040
> 0.045
> 0.050
> 0.055
> test loss
> 20d poly
> 0.0 0.1 0.2 0.3 0.4 0.5
> alpha
> 0.2
> 0.3
> 0.4
> 0.5
> 1d poly
> 0.0 0.1 0.2 0.3 0.4 0.5
> alpha
> 0.02
> 0.04
> 0.06
> 0.08
> 0.10
> Feynman
> 0.0 0.1 0.2 0.3 0.4 0.5
> alpha
> 0.01
> 0.02
> 0.03
> 0.04
> Spherical Harmononics

Figure 8. Comparison of test loss for different values of alpha and different datasets. 

## D. Datasets 

We use following synthetic datasets for training: Feynman Equations, Random polynomials, Spherical Harmonics and Random sin products. Each AlphaEvolve run uses just one dataset for training. For verification we use CIFAR-10, CLRS-30 and ImageNet 21 Finding a Generalizable Activation Functions 10 5 0 5 10                     

> 0
> 2
> 4
> 6
> 8
> 10
> f(x)
> GeLUSine(0.0)
> 10 50510
> 0
> 2
> 4
> 6
> 8
> 10
> GeLUSine(0.1)
> 10 50510
> 0
> 2
> 4
> 6
> 8
> 10
> GeLUSine(0.2)
> 10 50510
> 0
> 2
> 4
> 6
> 8
> 10
> GeLUSine(0.3)
> 10 50510
> 0
> 2
> 4
> 6
> 8
> 10
> GeLUSine(0.4)
> 10 50510
> 0
> 2
> 4
> 6
> 8
> 10
> GeLUSine(0.5)

Figure 9. GeLUSine activation function for different values of alpha. 

D.1. Feynman Equations 

Feynman Equations dataset consists of 100 equations from the the Feynman Lectures on Physics. Dataset was introduced in (Udrescu & Tegmark, 2020) and available as part of Feynman Symbolic Regression Database 

D.2. Random Polynomials 

We generate two sets of random polynomials: 1d poly and 20d poly each containing 100 random polynomials. 1d poly consists of polynomials of one variable and with degree in [0, 9] interval and coefficients in (0,1) range. 20d poly is polynomials of up 20 different variables and varying degree. 

D.3. Spherical Harmonics 

Spherical harmonics are generalizations of sine and cosine waves used in Fourier series on the sphere. 

D.4. Random sin Products 

Dataset consists of random function in the form of sin (θ · x) · sin (ϕ · x) · sin (ψ · x) , where θ, ϕ and ψ are randomized and fixed for duration of training for each function. 

D.5. CIFAR-10 

The CIFAR-10 (Krizhevsky et al., 2009) dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. 

D.6. CLRS-30 

CLRS Algorithmic Reasoning Benchmark (Veli ˇckovi ´c et al., 2022), covering classical algorithms from the Introduction to Algorithms textbook. 

D.7. ImageNet-1K 

ImageNet-1K (Deng et al., 2009) is a subset of the full ImageNet which contains 1271167 train and 50000 test images. 

D.8. ogbg-molhiv 

This dataset, part of the Open Graph Benchmark (Hu et al., 2020), requires binary classification of an input small molecule, depending on whether or not it would successfully inhibit HIV replication. The dataset comprises 41127 small molecules, with a scaffold split applied. 

## E. Architectures and Hyperparameters 

E.1. Internal training loop 

For internal training loop and synthetic datasets evaluation we use a simple MLP with hyperparameters provided in Table 3. 

E.2. CIFAR-10 

For CIFAR-10 training we use the VGG Network (Simonyan & Zisserman, 2015) with batchnorm and without maxpool. 22 Finding a Generalizable Activation Functions         

> Table 3. Hyperparameters Hyperparameter Value Internal layers 3Features 64 Learning rate 1e-3 Batch size 128 Training steps 50 Loss MSE

E.3. CLRS-30 

We trained on trajectories of the following algorithms: quicksort , binary search ,

find maximum subarray kadane , matrix chain order , activity selector , bfs ,

naive string matcher , graham scan . We train on lengths of up to 16 using Triplet-GMPNN processor (Ibarz et al., 2022) and report mean test score across those algorithms with test length of 64. 

E.4. ImageNet-1k 

For ImageNet-1k training we use the ResNet-50 architecture (He et al., 2016). 

E.5. ogbg-molhiv 

We train a standard five-layer graph convolutional network (Kipf, 2016, GCN) model on ogbg-molhiv, with a hidden size of 256. 23 Finding a Generalizable Activation Functions 

## F. Activation Functions discovered on each dataset  

> Table 4. Activation Functions Discovered

Activation Function Dataset Function Type Gaussian-Modulated Tangent Unit (GMTU) sin product scalar GELUSine sin product scalar GELU-Sinc-Perturbation polynomials scalar Turbulent Activation Function Feynman equations tensor Quaternion-Inspired Feynman equations scalar Fourier-Informed Spectral Gating (FISG) Feynman equations tensor Phase-Locked Entropic Repulsion spherical harmonics scalar Symmetric Phase-Flipped polynomials tensor 24