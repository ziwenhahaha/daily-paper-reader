Title: Clifford Kolmogorov-Arnold Networks

URL Source: https://arxiv.org/pdf/2602.05977v1

Published Time: Fri, 06 Feb 2026 02:27:27 GMT

Number of Pages: 9

Markdown Content:
# Clifford Kolmogorov-Arnold Networks 

Matthias Wolff 

Department of Computer Science 

University of M¨ unster 

M¨ unster, Germany 0009-0006-1432-6265 

Francesco Alesiani 

NEC Laboratories Europe GmbH 

Heidelberg, Germany 0000-0003-4413-7247 

Christof Duhme 

Department of Computer Science 

University of M¨ unster 

M¨ unster, Germany 0009-0004-1853-2862 

Xiaoyi Jiang 

Department of Computer Science 

University of M¨ unster 

M¨ unster, Germany 0000-0001-7678-9528 

Abstract —We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function ap-proximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks. 

Index Terms —Kolmogorov-Arnold Networks, Complex-Valued Neural Networks, Hypercomplex Neural Networks 

I. I NTRODUCTION 

The Kolmogorov-Arnold Representation Theorem (KAT) [1] provides an alternative way of representing functions of multiple variables, including continuous and discontinuous functions [2]. While it is a useful theoretical tool, it’s ap-plication was only marginally explored [3]. After Liu et al. [4] have recently sparked new interest in the field proposing Kolmogorov-Arnold Networks (KANs), there has been a surge of extensions as well as applications of this new architecture [5]. Especially for function fitting tasks, KANs have shown to be a promising alternative to conventional Multilayer Percep-trons (MLPs) [6] while also offering intrinsic interpretability. Wolff et al. [7] have transferred the advantages of KANs into the field of Complex-Valued Neural Networks (CVNNs) and introduced a Complex-Valued Kolmogorov-Arnold Net-work (CVKAN) by making use of Radial Basis Functions (RBFs). Che et al. [8] have then adapted the underlying KAT to the complex domain with mathematical proofs and further improved the CVKAN by changing the residual activation function and making the shape of the RBFs learnable. However, scientific discovery, engineering, computer vision or robotic applications work with high dimensional representa-tions, for example, to describe electromagnetic fields, weather state variables, 3-dimensional objects, or multi-joint robot arms. In these scenarios, complex numbers are not sufficient.           

> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.

In this work we therefore propose Clifford Kolmogorov-Arnold Network (ClKAN), an extension to CVKAN for higher dimensions than complex numbers. By increasing the dimensionality of the grid on which the RBFs are defined and using the geometric product defined in the Clifford Algebra (CA), we can learn a function on each edge connecting two nodes in the ClKAN that maps from Cl 7 → Cl, similar to KAN and CVKAN that map from R 7 → R or C 7 → C, respectively. Overall our contributions can be summarized as:  

> •

We extend the CVKAN framework [7] to the hyper-complex domain using Clifford algebras (Section IV).  

> •

We propose two different type of RBFs as function basis (Section IV-A).  

> •

We propose to transition from a uniform grid to a ran-domly sampled grid to mitigate the curse of dimension-ality. In particular, we introduce the use of Randomized Quasi Monte Carlo (RQMC) scrambled Sobol sequence 

for the grid generation (Section IV-C).  

> •

We show the expressivity of the proposed approach when using the Sobol grid (Section IV-D).  

> •

We study the effect of different batch normalization tech-niques suitable for Clifford domain and KAN architecture in general (Section IV-E).  

> •

We will provide our code of the model and the experi-ments as an open source library to promote open science. 1

In addition, we evaluate (Section V) ClKAN against the conventional CVKAN (described in Section III-B) in complex-valued experiments, we study the influence of different batch normalization strategies, and the use of the proposed RBFs (Section V-A and Section V-B). Further, we study the suit-ability of our proposed ClKAN with higher dimensional CAs (introduced in Section III-A), e.g. quaternions, Cl(2) ,

Cl(1 , 1) , in synthetic function fitting tasks. We show that our proposed Sobol grid approach is helpful for reducing the number of parameters (Section V-C), while providing flexible performances.    

> 1Link to code base: https://github.com/M-Wolff/CliffordKAN
> arXiv:2602.05977v1 [cs.LG] 5 Feb 2026 Scalars Vectors Bivectors Trivectors

Fig. 1: Visualization of Cl(3) grades (scalars, vectors, bivec-tors and trivectors). II. R ELATED WORK 

A. Kolmogorov-Arnold Function Representation 

Kolmogorov-Arnold function representation theorem pro-vides an alternative tool for function approximation [9], with the potential to address the curse of dimensionality [10]. The original KAN architecture has been extended to Convolu-tional Neural Networks (CNNs) [11], [12], and to transformer models [13]. To apply KANs to PolSAR classification, [14] developed a complex-valued convolutional KAN. Furthermore, matrix group equivariance has been integrated into the KAN architectural [15], while [16] provides an extension for geo-metrical symmetries and invariance. Adaptive KAN architec-ture [17] reduces the need for hyperparameter optimization. Liu et al. [18] explore the use of KANs in science with symbolic regression. While [7], [8] explore the applied and theoretical aspects of complex KAN, and [19] studies the role of residual connections. 

B. Clifford Algebra 

Clifford’s Geometric Algebra (GA) provides a powerful mathematical tool for describing spatial relationships and transformations. GA generalizes complex numbers and quater-nions into a unified system [20]. CA has found application in modeling Partial Differential Equations (PDEs) [21] or in mod-eling rotational and translation ( E(3) ) or Lorentz ( O(1 , 3) )symmetries [22]–[24], which are used to describe interactions in physical systems. Conformal Geometric Algebra (CGA) is used in robotic vision [25], for inverse kinematics [26], or in computer vision [27]. III. B ACKGROUND 

A. Clifford algebra 

Clifford algebra [20], [23], [28] extends Euclidean vector space to a full algebra by introducing the concept of geometric product. Clifford algebra finds application in various areas of mathematics, physics, science, and engineering. If V is a finite-dimensional vector space of dimension n = dim V ,over a field F (R in the paper) equipped with a quadratic form q : V → F, then Cl(V, q) is the Clifford Algebra, a unitary, associative, noncommutative algebra generated by V ,where v2 = q(v). Each element of the algebra is a linear combination of a finite product of vectors vij ∈ V ,

x = X

> i∈I

civi, 1 . . . v i,k i ,

with ci ∈ F, k i ∈ N0, I finite. The bilinear form associated with Cl(V, q) is defined as 

b(v1, v 2) = 12 (q(v1 + v2) − q(v1) − q(v2)) , which satisfy the identify v1v2 + v2v1 = 2 b(v1, v 2), ∀v1, v 2 ∈ V . This defines the geometric product v1v2 on which the algebra is defined. If 

v1, v 2 are orthogonal then b(v1, v 2) = 0 and therefore v2v1 =

−v1v2. If e1, . . . , e n is an orthogonal basis of V , then the set of tuples eI = ei1 . . . e i|I| , I = ( i1, . . . , i |I|) ⊆ [n] forms an orthogonal basis for Cl(V, q). Therefore, the dimension of the Clifford Algebra is D = dim Cl(V, q) = 2 n. We can partition the algebra into vector spaces Cl(m)(V, q), m = 0, . . . , n 

called grades (see Fig. 1), where the dim Cl(m)(V, q) =   nm

.Notably, Cl(0) (V, q) = F, and Cl(1) (V, q) = V . The ele-ments of Cl(2) (V, q) and Cl(3) (V, q) are called bivectors and trivectors, while Cl(n)(V, q) contains the pseudoscalar. The bilinear operator implies the scalar product x · y = b(x, y ) =  

> 12

(xy + yx ), x, y ∈ Cl(V, q), similarly we can define the external product as x ∧ y = 12 (xy − yx ). We can now write the geometric product between two vectors v, u ∈ V as 

vu = v · u + v ∧ u

The resulting object has two components, a scalar v · u and a bivector v ∧ u, which represents an area. When CA is used in e.g. describing the Maxwell’s equations [29], the electric and magnetic fields can be represented as a single element in the algebra, with F = E + e123 cB , with E and B the electric and magnetic vectors, while e123 is the pseudo-scalar and c

is the light speed. Further notice that the element e123 cB is a bivector. We can now write, from the original 12 equations, the Maxwell’s equations as (1 /c ∇t + ∇x)F = 1 /ϵ 0(ρ − J/c ),where ρ, J represent charges (scalar) and the electric currents (vector), and ϵ0 the dielectric constant. For convenience, we refer to the Clifford Algebra as Cl(p, q, r ), Clp,q,r , or simply 

Cl, where the triplet (p, q, r ) counts the number of elements that square to 1, −1 and 0, i.e. q(ei) = {1, −1, 0}.Notable examples of Clifford Algebras are the Euclidean Geometric Algebra (EGA), for example Cl(2) is used in PDE to model weather [21], or Cl(3) is used to model Maxwell’s equations [29]. The Projective Geometric Algebra (PGA) 

Cl(3 , 0, 1) and the Conformal Geometric Algebra (CGA) 

Cl(4 , 1) are used for computer vision [25], and robotic ma-nipulation [26] applications. The Spacetime Algebra Cl(3 , 1) 

describes the Minkowski space and is used to model relativistic physics [30]. 

B. Complex-valued Kolmogorov-Arnold Network 

Wolff et al. [7] extended the KAN [4] framework to the complex domain with the CVKAN inspired by the CVNN, which are known for improved generalization in signal pro-cessing and physics-related tasks. CVKAN introduces the complex RBF as the base function of the expansion of the univariate function for the KAN architecture. The Complex RBF ϕC : C → R is defined as ϕC(x) = exp( −| x|2),with |.| the absolute value of the complex number x ∈ C,and centered on a regular grid in C, i.e. grid points 

g ∈ C, resulting in the univariate function of the form ΦC(x) : C → R = P 

> g∈G

wg ϕC(x − g). CVKAN also intro-duces CSigmoid Linear Unit (SiLU), complex nonlinearity, defined as CSiLU = w (SiLU (ℜ(x)) + iSiLU (ℑ(x))) + β,where w, β ∈ C are a learnable weight and bias, and ℜ(x)

and ℑ(x) are the real and imaginary part of x ∈ C, and SiLU (x) = x(1 + e−x)−1.IV. M ETHODOLOGY 

In this section, we describe how a ClKAN is con-structed for any Clifford algebra Cl(p, q, r ). The choice of 

n = p + q + r ∈ N+ defines specific algebras (Section III-A). This algebra is then used for all RBFs inside the whole 

ClKAN and dictates the rules for all other calculations, e.g. multiplication with weights. 

A. ClKAN Radial Basis Functions 

Similar to CVKAN [7] we define a naive RBF (1a) with 

x ∈ Cl and ∥x∥ representing the Clifford algebra Cl norm of x. This formulation for our naive RBF corresponds to a mapping of ϕ(x) : Cl → R = Cl(0) . By using this mapping, the high-dimensional Clifford algebra space is represented by a real-valued number, the output of the RBF, therefore potentially losing information about the rich structure of the input space. We thus propose the alternative Clifford RBF (1b) that retains information about the spatial properties of the Clifford-valued input. We achieve this by multiplying the output of the RBF with the input itself. The product of an element in Cl

by a real number is still an element of Cl, and represents the scaled version of its components. 

ϕ(x) = exp  −∥ x∥2 (1a) ϕ(x)Cl = x ϕ (x) (1b) Equation (1b) then constitutes a mapping ϕ(x)Cl : Cl → Cl

and the output is equal to the input x ∈ Cl weighted by the real-valued output of naive RBF ϕ(x).Following CVKAN, for x ∈ Cl is ClSiLU the residual activation function with learnable weight and bias w, β ∈ Cl:

ClSiLU (x) = wSiLU Cl(x) + β (2) where SiLU Cl denotes the dimension-wise application of the SiLU function. 

B. ClKAN uniform grid 

To construct a learnable activation function Cl → Cl, we need to combine multiple RBFs. The RBFs have to be centered around grid points, so we need to define a grid where each grid point g is also an element of the same underlying Clifford algebra g ∈ Cl.In CVKAN for complex numbers this grid was defined as a uniform grid with Ng ×Ng grid points g ∈ C within some fixed ranges, e.g. [−2−2i, 2+2 i], to likely fall inside the grid of the next layer after batch normalization changes the distribution to μ = 0 , σ = 1 . This also allows to use a fixed grid range and not rely on grid extension [4], [31] or learnable grid offsets [32] that might slow down or complicate the learning process. This approach can be intuitively transferred to ClKAN. Let 

D be the dimensionality of Clifford algebra Cl. For example 

D = 2 for complex numbers and D = 4 for quaternions. Then the grid G consists of (Ng )D grid points g ∈ Cl. If we adopt the Ng = 8 number of grid points per dimension from CVKAN, then for quaternions we end up with a grid of size 

84 = 8 ×8×8×8 and a total of 4096 grid points g ∈ Cl(0 , 2) .Consistent with CVKAN each grid point g ∈ G is associated with a RBF centered around it and thus a learnable weight 

wg ∈ Cl that controls the influence of the RBF at grid point 

g on the learnable activation function. The RBF to use can either be the naive (1a) or the Clifford RBF (1b) to construct the learnable activation function (3) or (4), respectively. 

Φ( x) = X

> g∈G

wg ϕ(x − g) (3) 

ΦCl(x) = X

> g∈G

wg ϕCl(x − g) (4) If we use a uniform grid, also referred to as full grid , the number of learnable weights increases exponentially with the number of dimensions in the Clifford algebra (Section III-A). Therefore, we propose an alternative grid generation approach that mitigates the curse of dimensionality. 

C. RQMC Sobol Grid 

To reduce the number of trainable parameters, we introduce a random grid approach. For each dimension of g ∈ Cl(p, q, r )

we sample a random number inside a fixed grid range, e.g. 

[−2, +2] , and repeat this process for all of the grid points. Similar to how random search was shown to be a better approach than grid search for hyperparameter optimization [33], we aim to achieve a sufficient coverage of the hypercube of all possible grid points [−2, +2] D while requiring less grid points than the full grid approach. The sampled grid points act like supports for our learnable activation function Φ( x)

or ΦCl(x). A natural requirement for these points is to cover the hypercube evenly, for each realization, and not only in expectation. However, we noticed that naively sampled random grid points do not necessarily show this property. We therefore introduce the RQMC scrambled Sobol sequence grid, or Sobol grid , where we generate the grid points using quasi-random Sobol sequence [34], [35]. As shown in Fig. 2, these quasi-random numbers cover the space more evenly than true random numbers and therefore are better suited for our sampling of grid points. We call this variant of the model the Sobol Clifford Kolmogorov-Arnold Network (Sobol-CliffordKAN). In our experiments, we only work with this kind of random grid and don’t use the naive random grid. Dimension-wise     

> Node-wise
> Component-wise
> Nodes
> Batch
> Clifford Dimension
> ···
> · · ·
> ···
> · · ·
> ···
> ··· · · · ··· · · · ···

Fig. 3: Visualization of dimension-wise (orange), node-wise 

(green) and component-wise (blue) batch normalization. 

Fig. 2: Comparison of full grid, random grid, and quasi-random Sobol grid using Sobol sequences with 64 grid points each. 

D. Expressivity of Sobol-CliffordKAN 

Using the uniform grid does not scale in high-dimensional space, however, using a uniform random grid results in very high variance. By introducing the Sobol grid , we reduce the variance of the model (4). We now analyze the properties of the proposed approach. A (t, m, d )-net in base b, with 0 ≤ t ≤ m,is a set of samples y1, . . . , y bm ∈ [0 , 1] d such that for each hyper-cube of volume bt−m it contains exactly bt points. Here, the total number of points is called n = bm = |G|, while in the rest of the paper n is the dimension of the vector space. We now consider the integral h(x) = R 

> [0 ,1] d

g(y)k(x − y)dy .If we sample {yi}i∈[n] in [0 , 1] d, we can use Monte Carlo (MC) to estimate h(x) ≈ 1

> n

Pni=1 g(yi)k(x − yi) = ˆhn(x).We notice that (4) Φ( x) = P 

> g∈G

wg ϕ(x − g) = ˆhn(x),with g(yi) = wg and k(x − yi) = ϕ(x − g). If we denote 

σ2(h(x)) = E[∥h(x) − μ(x)∥2

> ∞

], it follows that: 

Property IV.1. If the grid points are generated using a scram-bled (t, m, d )-net in base b [35], [36], and g(y), k (x) are smooth functions, s.t. fx(y) = g(y)k(x−y) ∈ L1+ ϵ[0 , 1] d, ϵ > 

0, then 

E[∥ˆμn(h(x)) − μ(h(x)) ∥2

> ∞

] ≤ Γ σ2(h(x)) 

n = O(n−1). (5) 

Proof: We apply the result of [35], [37] to the integration of function fx(y) = g(y)k(x − y). E[∥ˆμxn − μx∥2] ≤ Γ σ2 

> x
> n

,with σ2 

> x

= σ2(fx) = E[∥fx(y) − μx∥2], ∀x ∈ [0 , 1] d, and 

ˆμxn = 1

> n

Pni=1 fx(yi). This is also true for the maximum of the variances, and therefore the result follows. We can now state our lemma 

Lemma IV.2. If we use the Sobol grid of size n, then 

Φ( x) = P 

> g∈G

wg ϕ(x − g) is an unbiased estimator of 

h(x) = R 

> [0 ,1] d

g(y)ϕ(x − y)dy with variance O(n−1).Proof: Using the Sobol scrambled sequence, we define the estimator of h(x) with the condition in Pr. IV.1 as 

ˆhn(x) = 1

> n

Pni=1 g(yi)k(x − yi). As before, we rewrite 

Φ( x) = P 

> g∈G

wg ϕ(x − g) = ˆhn(x), with g(yi) = wg and 

k(x − yi) = ϕ(x − g). Therefore, from Pr. IV.1, we have that 

Φ( x) is an unbiased estimator h(x) with variance O(n−1).Therefore, training our KAN network means training the function g(y), evaluated at the grid points. Our proposed Sobol-CliffordKAN can learn any function that can be written as a convolution with the kernel ϕ(x) with an error of O(n−1).We further notice that with this interpretation, the role of the kernel is to smooth the learned function g(y), and for the limit of β → ∞ and n → ∞ , we recover the original function, if we consider the new kernel k(x) = β/√πϕ(βx ) →β→∞ = δ(x).

E. Batch normalization 

Since the batch normalization approaches of CVKAN only consider complex numbers [7], we introduce three types of normalization for the arbitrarily high dimensional Clifford algebras (see Fig. 3), to make sure inputs to the next layer of our ClKAN likely stay within the grid ranges:  

> •

Dimension-wise batch normalization applies batch nor-malization over each individual dimension of all nodes in one layer combined.  

> •

In node-wise batch normalization we normalize the input in each single node of the KAN architecture over all it’s dimensions at once. This approach is intuitively the best fitting approach for KANs, as each node can compute different correlations in the data and therefore each node’s output should be normalized independently of other nodes in the same layer.  

> •

Component-wise batch normalization is a combination of dimension-wise and node-wise batch normalization, as each node and each dimension get normalized indepen-dently over all samples in the batch. Additionally no-normalization can be applied, but then there is a high chance that inputs into the next layer exceed the grid ranges and are far away from every grid point, so that the node’s output for such out of range data is 0.V. E XPERIMENTS 

In all of our experiments we applied a 5-fold cross-validation, where we also re-initialized the weights and Sobol grids for each run, and used a validation loss plateau scheduler with an initial learning rate of 0.1 - in contrast to [7] who used a learning rate of 0.01 -, a reduction factor 0.9, patience of 

20 epochs and a threshold of 0.001 . We also applied early stopping after the validation loss has not decreased by at least 

0.001 in the last 200 epochs. For the full grid approach we adapted the number of grid points per dimension D from (a) square (6a) dataset.   

> (b) squaresquare (6d) dataset.

Fig. 4: Overview of Mean Square Error (MSE) for all experiments for complex-valued synthetic function fitting tasks on formulas (6a) and (6d) with color indicating the type of batch normalization, shape the type of RBF and shading the architecture size with shaded regions representing the small model architecture. X-axis labels correspond to CVKAN baseline, Full grid and Sobol grid with number of grid points per dimension. For better readability S-5 and S-7 have been omitted. Y-axis shown in log-scale. CVKAN as 8, so that the total number of grid points is 

(Ng )D = 8 D . For the Sobol grid we selected 2 to 8 grid points per dimension to study the minimum required number of random grid points for similar performance as full grid. We also chose grid ranges of [−2, 2] for each dimension compliant with CVKAN. For all regression tasks we trained on MSE and evaluated on the MSE and Mean Average Error (MAE) metric, while for classification tasks we used Cross-Entropy (CE) for training and additionally evaluated the accuracy. 

A. Complex-valued basic synthetic formulas 

First, we evaluated if our ClKAN achieves the same perfor-mance as CVKAN on simple complex-valued function-fitting tasks. Therefore we compare our network against CVKAN on datasets generated by the four complex-valued synthetic formulas square (6a), sin (6b), mult (6c) and squaresquare 

(6d), just like Wolff et al. [7]. We use 5000 samples for train-and validation-split combined and another 5000 samples for test-split and adapt the same architecture sizes as [7]. For uni-variate functions (6a) and (6b) we use KAN architectures of sizes [1 , 1] and [1 , 2, 1] , for bi-variate functions mult (6c) 

[2 , 2, 1] and [2 , 4, 2, 1] and for squaresquare (6d) [2 , 1, 1] and 

[2 , 4, 2, 1] .

f1(x) = x2 (6a) 

f2(x) = sin( x) (6b) 

f3(x1, x 2) = x1 · x2 (6c) 

f4(x1, x 2) = ( x21 + x22)2 (6d) 

B. Complex-valued larger datasets 

We also evaluated on larger datasets more connected to the real-world. We used the same complex-valued holography (7) dataset from CVKAN with 100 .000 samples for train and validation split combined and another 100 .000 samples for the test split. This formula is motivated by holography from physics, as the inputs into the function are a reference beam with electric field strength ER, an object beam E0 and a reconstruction beam bER used to reconstruct a hologram HH = bER · | ER + E0|2 (7) with H, bER, E R, E 0 ∈ C. Additionally we briefly analyzed the performance on the knot dataset [38], which was also used for evaluation in CVKAN. 

C. Higher-dimensional Clifford algebras 

To analyze how our ClKAN performs in higher dimensions than complex, we trained on datasets also generated by the formulas (6a), (6c) and (6d) but with x, x 1, x 2 ∈ Cl(p, q, r ).We evaluate the performance on key Geometrical Algebras: 1) Euclidean GA over R2, i.e. Cl(2) , 2) the quaternion isomorphic Cl(0 , 2) , 3) the zero dimensional Conformal GA 

Cl(1 , 1) , and 4) the one dimensional Projective Geometric Algebra (PGA) Cl(1 , 0, 1) .TABLE I: Comparison of the Best Performing Model (MSE) for Each of the Four Complex-valued Function Fitting Tasks. Values for Che et al. [8] and Wolff et al. with lr=0.01 [7] Copied from Their Respective Paper. Values for ClKAN Stem from the Best Models Across All RBF, Batch Normalization and Grid Strategies and All Evaluated Architecture Sizes.                      

> Dataset CVKAN improved ClKAN lr=0.01 [7] lr = 0.1 CVKAN [8] square 0.013 0.001 0.009 0.001 sin 0.005 0.001 0.005 0.001 mult 0.045 0.005 0.029 0.002 squaresquare 8.150 1.665 7.355 2.049

Because the underlying Clifford algebras are four-dimensional in contrast to the previously used two-dimensional complex values, we need more data points to cover the space sufficiently. For complex-valued function fitting we sampled 

5000 data points in a range [−2, 2] for each dimension. There-fore, we had a sampling density of 5000 42 . In order to achieve the same sampling density in a four-dimensional hypercube, we need N 

> 44

= 5000 42 and therefore N = 16 · 5000 = 80000 

samples instead for combined train- and validation- as well as test-split. We ran experiments with the most promising batch normal-ization and RBF computation strategy given by the previous experiments (cf. Section V-A). Furthermore we conducted exhaustive experiments for grid type and number of random grid points, to see how well our proposed Sobol grid approach behaves in higher dimensions. VI. R ESULTS 

In this section we will show and describe the most inter-esting results. We omit the plot for complex-valued function fitting on sin (6b) and mult (6c) dataset and leave out Sobol grids with Ng ∈ { 5, 7} for better readability. We will make all the results available in a machine readable way together with additional plots and tables inside our repository for the code base. 

A. Comparison to baseline 

First we want to compare our ClKAN to the baseline CVKAN as well as to it’s improved version [8]. Table I compares the best performing model from CVKAN by Wolff et al. [7], improved CVKAN by Che et al. [8] and our ClKAN for each complex-valued function fitting dataset. It should be noted that in contrast to CVKAN that used 5000 data points for 5-fold cross-validation training and reported validation losses, we introduce an additional test split of the same size and report test losses but still only do training and cross-validation on 

5000 data points. This dataset setup was used for CVKAN with lr= 0.1 and our ClKAN. It can be seen that the increased learning rate of 0.1 makes the results of CVKAN far better and that our ClKAN achieves similar results with the same learning rate. The improved version of CVKAN is better than standard CVKAN with 0.01 learning rate, but can not keep up with the finer tuned learning rate. 

B. Complex-valued basic synthetic formulas 

Fig. 4a and Fig. 4b show that our full grid ClKAN performs similarly well as CVKAN with the same number of param-eters. The performance of the Sobol grid strategy gets better with an increasing amount of grid points per dimension and for Ng = 8 grid points per dimension the Sobol grid approach is on par with the full uniform grid. The larger architectures perform consistently better than smaller architectures. Fig. 4a has smaller shaded regions and showcases only batch normal-ization no-normalization for small architectures because the small architecture size for the square dataset is [1 , 1] and we never apply batch normalization on the final output of the model. Therefore, other batch normalization approaches than 

no-normalization are not applicable for this architecture size. For the choice of RBF calculation the naive (1a) and Clif-ford approach (1b) perform similarly well. When comparing the different batch normalization approaches no clear winner can be spotted. In some cases (e.g. square , full grid and naive RBF with the big architecture in Fig.4a) no-normalization 

performs best, in other cases (e.g. squaresquare , full grid and Clifford RBF with the big architecture in Fig.4b) node-wise 

or dim-wise batch normalization are the best choice. TABLE II: Comparison of CVKAN and ClKAN on the Holography Dataset (7) for the Two Largest Architectures and Node-wise Batch Normalization. Choice of RBF is Clifford RBF (1b) for ClKAN. GT=Grid Type ∈ {Full Grid and 

Sobol Grid }, Ng = Number of Grid Points per Dimension, 

Np=# Parameters                                                                                                  

> Model NpGT NgTest MSE Test MAE
> 3×10 ×3×1
> CVKAN 8342 F80.021 ±0.003 0.090 ±0.011
> ClKAN 8342 F80.030 ±0.005 0.092 ±0.008
> 782 S22.799 ±0.780 1.211 ±0.167
> 1412 S30.713 ±0.135 0.602 ±0.069
> 2294 S40.275 ±0.133 0.348 ±0.099
> 3428 S50.101 ±0.030 0.192 ±0.034
> 4814 S60.070 ±0.019 0.154 ±0.031
> 6452 S70.043 ±0.006 0.117 ±0.009
> 8342 S80.036 ±0.008 0.111 ±0.012
> 3×10 ×5×3×1
> CVKAN 12972 F80.016 ±0.001 0.066 ±0.003
> ClKAN 12972 F80.025 ±0.004 0.069 ±0.006
> 1212 S21.891 ±0.275 0.952 ±0.078
> 2192 S30.272 ±0.036 0.358 ±0.022
> 3564 S40.093 ±0.021 0.179 ±0.017
> 5328 S50.059 ±0.012 0.123 ±0.010
> 7484 S60.041 ±0.006 0.100 ±0.012
> 10032 S70.029 ±0.004 0.081 ±0.005
> 12972 S80.028 ±0.004 0.079 ±0.003

C. Complex-valued larger datasets 

The results on the holography dataset (7) in Table II show that ClKAN with full grid and Sobol grid with Ng = 8 and the baseline CVKAN perform similarly well. For the Sobol grids 

with Ng = 7 , and therefore ≈ 75% of parameters, the test MSE is almost the same compared to a full grid model. With TABLE IV: Comparison of ClKAN on the Cl(1 , 0, 1) 

for square and squaresquare Datasets, Largest Architec-tures and node-wise Batch Normalization. Choice of RBF is Clifford RBF (1b). GT=Grid Type ∈ {Full Grid and 

Sobol Grid }, Ng = Number of Grid Points per Dimension, 

Np=# Parameters.                                                                                       

> NpGT NgTest MSE Test MAE square 1×2×1
> 65572 F80.968 ±0.834 0.585 ±0.381
> 292 S20.054 ±0.018 0.187 ±0.032
> 1332 S30.004 ±0.001 0.054 ±0.005
> 4132 S40.006 ±0.004 0.058 ±0.020
> 10036 S50.102 ±0.199 0.148 ±0.217
> 20772 S60.177 ±0.171 0.270 ±0.196
> 38452 S70.027 ±0.042 0.096 ±0.079
> 65572 S80.045 ±0.076 0.114 ±0.105
> squaresquare 2×4×2×1
> 295068 F833 .586 ±32 .646 3.492 ±2.202
> 1308 S220 .578 ±6.499 3.194 ±0.481
> 5988 S31.685 ±0.367 0.900 ±0.108
> 18588 S40.445 ±0.074 0.453 ±0.035
> 45156 S50.270 ±0.082 0.345 ±0.054
> 93468 S60.178 ±0.052 0.285 ±0.036
> 173028 S70.203 ±0.077 0.304 ±0.063
> 295068 S81.425 ±2.414 0.609 ±0.557

very low number of grid points Ng ∈ { 2, 3, 4} the model can not approximate the underlying function of the dataset well. When comparing the different batch normalization ap-proaches in Table III, it can be clearly seen that no-normalization performs by far the worst and the model is not able to learn anything. The other three approaches for batch normalization perform on par with very low differences between them. We also analyzed the performance of our model on the knot dataset (Section V), but the accuracies of all models regardless of grid strategy and grid size were so close to each other and mostly above 90% that we chose not to include the results as they would not give additional insight to recognize differences between the candidates. 

D. Higher-dimensional Clifford algebras 

Fig. 5 shows all experiments on higher-dimensional Clifford algebras. Similar to Section VI-B, Sobol grid with the same number of parameters performs approximately as good as the full grid approach. However, in this four-dimensional Clifford algebra, the Sobol grid experiments with Ng ∈ { 3, 4} show very promising results with a drastically reduced number of parameters ( ≈ 2 − 6% of parameters). Overall, the mult (6c) and square (6a) datasets can be learned with a lower MSE than squaresquare (6d) dataset. The Cl(1 , 0, 1) profits the most from the Sobol grid approach and shows even better performance with Ng = 4 than for the full grid. In Table IV it is shown, that for Cl(1 , 0, 1) and the square 

and squaresquare dataset the results of Sobol grid are consis-tently better than full grid. Some grid sizes, e.g. Ng ∈ { 5, 6}

for square and Ng = 8 for squaresquare , perform worse than other grid sizes with high standard deviations. 

E. Discussion 

In our experiments (Section VI) we found out that the way of calculating the RBFs does not seem to have a big influence on the model’s performance, while the type of batch normalization to use depends on the dataset and there can be cases where no-normalization at all seems beneficial, while in other cases models without batch normalization don’t train at all. If normalization is helpful, the concrete type of batch TABLE III: Comparison of Batch Normalization Schemes for CVKAN and ClKAN on the Holography Dataset (7) for the Biggest Architecture [3 , 10 , 5, 3, 1] and Full Grid with Ng = 8 .Choice of RBF is Clifford RBF (1b) for ClKAN.                            

> Model Norm Test MSE Test MAE CVKAN nodew. 0.016 ±0.001 0.066 ±0.003
> dimw. 0.015 ±0.003 0.062 ±0.008
> compw. 0.017 ±0.002 0.063 ±0.004
> no 51 .059 ±62 .526 3.278 ±3.951
> ClKAN nodew. 0.025 ±0.004 0.069 ±0.006
> dimw. 0.021 ±0.003 0.060 ±0.005
> compw. 0.028 ±0.009 0.071 ±0.006
> no 0.867 ±1.061 0.354 ±0.382

normalization - be it node-wise , dimension-wise or component-wise - does not matter much. The squaresquare dataset is by far the most challenging one to train on. We suspect this depends on the large variability of it’s output values, compared to square . We noticed very early in our experiments that CVKAN as well as ClKAN were highly dependent on a good initialization. Therefore, we have increased the learning rate by a factor of 10 to a value of 

0.1 in comparison to [7]. This enables the models to drift further away from the weights given by random initialization and therefore reach more favorable regions. The results across different training runs and during crossfold-validation also became more stable. This small change alone allowed us to already beat the improved CVKAN [8] by a lot. For higher dimensions (Section VI-C) we have shown that our proposed Sobol grid can be a good tool for parameter reduction in higher dimensions to mitigate the otherwise exponential growth of parameters for the full grid. Some Clifford algebras, e.g. Cl(1 , 0, 1) , even benefit from the Sobol grid approach in terms of accuracy with drastically reduced parameters. Although it must be noted that Sobol grid some-times also produces worse results than full grid and some grid sizes in our experiments for specific datasets produce high standard deviations. This is a general discovery in the experiments, since some experimental setups, that should work in theory, only work for some cross-validation runs in practice therefore producing high MSE on average and a high standard deviation. With the increased learning rate the experiments overall became more stable and reproducable across runs, but some instabilities can still be observed across all tasks. Fig. 5: Function-fitting experiments on higher dimensional Clifford algebras. Color represents Clifford algebra used, shape the dataset mult (6c), square (6a) and squaresquare (6d) and shading the model architecture per dataset with shaded regions representing the small model architecture. X-axis labels correspond to Full grid and Sobol grid with number of grid points per dimension. For better readability S-5 and S-7 have been omitted. Y-axis shown in log scale. VII. C ONCLUSION 

We have shown that our ClKAN achieves similar perfor-mance on complex-valued datasets compared to CVKAN as a baseline. Additionally, we have demonstrated that ClKAN can also be successfully applied in higher dimensions for function fitting tasks. Furthermore, we have shown that our introduced Sobol grid can be used to mitigate the curse of dimensionality for higher dimensional Clifford algebras, where a conventional full uniform grid approach would require an exponential number of parameters. Surprisingly, there are cases where the limited number of parameters in a relatively small Sobol grid 

actually benefit the model’s performance and lead to better results than the full grid. Further research could be dedicated in the direction of learn-able RBF shape and alternative residual activation functions than SiLU with improved learning rate, similarly to [8] for CVKAN. Additionally, future work could focus on improving the stability of KANs and CVKAN and ClKAN in particular, making them even less dependent on good initializations and more robust across different runs. ACKNOWLEDGMENT 

LLMs (ChatGPT 5.2 and Google Gemini) were used for sentence-level reformulation, or as search tool, but not for text generation. REFERENCES [1] A. N. Kolmogorov, On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables . American Mathematical Society, 1961. [2] A. Ismayilova and V. E. Ismailov, “On the kolmogorov neural networks,” 

Neural Networks , vol. 176, p. 106333, 2024. [3] D. A. Sprecher and S. Draghici, “Space-filling curves and kolmogorov superposition-based neural networks,” Neural Networks , vol. 15, no. 1, pp. 57–67, 2002. [4] Z. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Soljacic, T. Hou, and M. Tegmark, “KAN: Kolmogorov–arnold networks,” in ICLR , 2025, pp. 70 367–70 413. [5] S. Somvanshi, S. A. Javed, M. M. Islam, D. Pandit, and S. Das, “A survey on kolmogorov-arnold network,” ACM Computing Surveys ,vol. 58, no. 2, pp. 1–35, 2025. [6] R. Yu, W. Yu, and X. Wang, “KAN or MLP: A fairer comparison,” 

arXiv:2407.16674 , 2024. [7] M. Wolff, F. Eilers, and X. Jiang, “CVKAN: Complex-valued kolmogorov-arnold networks,” in IJCNN , 2025, pp. 1–9. [8] R. Che, L. af Klinteberg, and M. Aryapoor, “Improved complex-valued Kolmogorov–Arnold networks with theoretical support,” in 24th EPIA Conference on Artificial Intelligence . Springer Nature Switzerland, 2026, pp. 439–451. [9] M.-J. Lai and Z. Shen, “The Kolmogorov superposition theorem can break the curse of dimensionality when approximating high dimensional functions,” arXiv:2112.09963 , 2021. [10] T. Poggio, “How deep sparse networks avoid the curse of dimensionality: Efficiently computable functions are compositionally sparse,” CBMM Memo , vol. 10, p. 2022, 2022. [11] M. M. Ferdaus, M. Abdelguerfi, E. Ioup, D. Dobson, K. N. Niles, K. Pathak, and S. Sloan, “KANICE: kolmogorov-arnold networks with interactive convolutional elements,” in 4th International Conference on AI-ML Systems (AIMLSystems) , 2024, pp. 13:1–13:10. [12] A. D. Bodner, A. S. Tepsich, J. N. Spolski, and S. Pourteau, “Convolu-tional kolmogorov-arnold networks,” arXiv:2406.13155 , 2024. [13] X. Yang and X. Wang, “Kolmogorov-arnold transformer,” in ICLR , 2025. [14] Z. Kuang, H. Bi, Z. Lv, and C. Xu, “Exploring complex-valued convolu-tional Kolmogorov-Arnold networks for PolSaR image classification,” in 

IEEE International Geoscience and Remote Sensing Symposium , 2025, pp. 1945–1949. [15] L. Hu, Y. Wang, and Z. Lin, “Incorporating arbitrary matrix group equivariance into KANs,” in ICML , 2025. [16] F. Alesiani, T. Maruyama, H. Christiansen, and V. Zaverkin, “Geometric Kolmogorov-Arnold Superposition Theorem,” arXiv:2502.16664 , 2025. [17] F. Alesiani, H. Christiansen, and F. Errica, “Variational kolmogorov-arnold network,” arXiv:2507.02466 , 2025. [18] Z. Liu, M. Tegmark, P. Ma, W. Matusik, and Y. Wang, “Kolmogorov-arnold networks meet science,” Phys. Rev. X , vol. 15, p. 041051, 2025. [19] R. C. Yu, S. Wu, and J. Gui, “Residual kolmogorov-arnold network for enhanced deep learning,” arXiv:2410.05500 , 2024. [20] D. Lundholm and L. Svensson, “Clifford algebra, geometric algebra, and applications,” arXiv:0907.5356 , 2009. [21] J. Brandstetter, R. van den Berg, M. Welling, and J. K. Gupta, “Clifford neural layers for pde modeling,” in ICLR , 2023. [22] J. Brehmer, P. De Haan, S. Behrends, and T. Cohen, “Geometric algebra transformers,” in NeurIPS , 2023. [23] D. Ruhe, J. Brandstetter, and P. Forr´ e, “Clifford group equivariant neural networks,” NeurIPS , pp. 62 922–62 990, 2023. [24] D. Ruhe, J. K. Gupta, S. De Keninck, M. Welling, and J. Brandstetter, “Geometric Clifford algebra networks,” in ICML , 2023, pp. 29 306– 29 337. [25] E. Bayro-Corrochano, L. Reyes-Lozano, and J. Zamora-Esquivel, “Con-formal geometric algebra for robotic vision,” Journal of Mathematical Imaging and Vision , vol. 24, pp. 55–81, 2006. [26] D. Hildenbrand, J. Zamora, and E. Bayro-Corrochano, “Inverse kine-matics computation in computer graphics and robotics using conformal geometric algebra,” Advances in applied Clifford algebras , vol. 18, pp. 699–713, 2008. [27] L. Dorst and S. Mann, “Geometric algebra: a computational framework for geometrical applications,” IEEE Computer Graphics and Applica-tions , vol. 22, no. 3, pp. 24–31, 2002. [28] A. Crumeyrolle, Orthogonal and symplectic Clifford algebras: Spinor structures . Springer Science & Business Media, 2013, vol. 57. [29] J. M. Chappell, S. P. Drake, C. L. Seidel, L. J. Gunn, A. Iqbal, A. Al-lison, and D. Abbott, “Geometric algebra for electrical and electronic engineers,” Proceedings of the IEEE , vol. 102, no. 9, pp. 1340–1363, 2014. [30] D. Hestenes and A. Lasenby, Space-time algebra . Springer, 2015. [31] J. Moody and J. Usevitch, “Automatic grid updates for Kolmogorov-Arnold networks using layer histograms,” arXiv:2511.08570 , 2025. [32] L. N. Zheng, W. E. Zhang, L. Yue, M. Xu, O. Maennel, and W. Chen, “Free-knots Kolmogorov-Arnold network: On the analysis of spline knots and advancing stability,” arXiv:2501.09283 , 2025. [33] J. Bergstra and Y. Bengio, “Random search for hyper-parameter opti-mization,” Journal of Machine Learning Research , vol. 13, no. 10, pp. 281–305, 2012. [34] I. M. Sobol’, “The distribution of points in a cube and the accurate evaluation of integrals,” Zhurnal Vychislitel’noi Matematiki i Matem-aticheskoi Fiziki , vol. 7, no. 4, pp. 784–802, 1967. [35] A. B. Owen and D. Rudolf, “A strong law of large numbers for scrambled net integration,” SIAM Review , vol. 63, no. 2, pp. 360–372, 2021. [36] A. B. Owen, “Scrambling Sobol’ and Niederreiter-Xing points,” Journal of Complexity , vol. 14, no. 4, pp. 466–489, 1998. [37] ——, “Randomly permuted (t, m, s)-nets and (t, s)-sequences,” in 

Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing .Springer, 1995, pp. 299–317. [38] A. Davies, P. Veliˇ ckovi´ c, L. Buesing, S. Blackwell, D. Zheng, N. Tomaˇ sev, R. Tanburn, P. Battaglia, C. Blundell, A. Juh´ asz et al. ,“Advancing mathematics by guiding human intuition with AI,” Nature ,vol. 600, no. 7887, pp. 70–74, 2021.