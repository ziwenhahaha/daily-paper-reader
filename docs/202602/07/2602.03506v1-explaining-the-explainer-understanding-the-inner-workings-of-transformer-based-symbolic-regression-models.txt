Title: Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models

URL Source: https://arxiv.org/pdf/2602.03506v1

Published Time: Wed, 04 Feb 2026 02:11:10 GMT

Number of Pages: 29

Markdown Content:
# Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

Arco van Breda 1 Erman Acar 1

## Abstract 

Following their success across many domains, transformers have proven effective for symbolic regression (SR); however, the internal mech-anisms underlying operator generation remain largely unexplored. Although mechanistic inter-pretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. We introduce PATCHES , an evo-lutionary circuit discovery algorithm that iden-tifies compact and correct circuits for SR. Us-ing PATCHES , we isolate 28 circuits, providing the first circuit-level characterisation of an SR transformer. We validate these findings through a causal evaluation framework based on key no-tions such as faithfulness, completeness, and min-imality . Our analysis shows that mean patching with performance-based evaluation most reliably isolates functionally correct circuits. In contrast, we demonstrate that direct logit attribution and probing classifiers primarily capture correlational features rather than causal ones, limiting their util-ity for circuit discovery. Overall, these results establish SR as a high-potential application do-main for mechanistic interpretability and propose a principled methodology for circuit discovery. 

## 1. Introduction 

As deep learning models increasingly automate scientific discovery and reasoning (Wang et al., 2023a; Cui et al., 2025), interpreting their outputs becomes critical. Symbolic Regression (SR) stands out in scientific discovery by pro-ducing explicit mathematical equations describing the data. Unlike post-hoc techniques such as LIME or SHAP, which approximate model behaviour locally without capturing rela-tions between variables (Ribeiro et al., 2016; Molnar, 2019; Lundberg & Lee, 2017), SR models aim to uncover the exact input-output relationship over the entire dataset. Despite this transparency, modern NeuroSymbolic SR mod-

> 1

University of Amsterdam, Amsterdam, The Netherlands. Cor-respondence to: Arco van Breda <a.vanbreda@uva.nl >.

Preprint. February 4, 2026. 

els that generate these expressions often remain black boxes themselves. Their internal workings and decision-making processes remain poorly understood, which is paradoxical: SR is valued for its interpretability, yet the mechanism that produces explanations for the data itself lacks transparency. This contradiction highlights the need for interpretability methods to uncover how these models arrive at their sym-bolic outputs. To resolve the interpretability gap, we turn to mechanistic in-terpretability (MI), a field dedicated to reverse-engineering neural networks into human-understandable forms (Olah et al., 2020). A key technique within MI is circuit discovery ,which seeks to identify circuits; interpretable subgraphs of a model that explain a particular behaviour. While MI has successfully uncovered algorithms within language (Nanda et al., 2023; Conmy et al., 2023; Hanna et al., 2023; Yu et al., 2024; Lan et al., 2024; Hanna et al., 2024) and vision mod-els (Olah et al., 2020; Rajaram et al., 2024; Szegedy et al., 2014), it has not yet been applied to SR. Bridging this gap requires identifying the specific components, e.g., attention heads or multilayer perceptron blocks (MLPs), responsible for generating symbolic operators, such as unary (e.g., sin ,

exp ) and binary (e.g., +, ×) functions. Applying MI to SR presents significant methodological chal-lenges. The standard technique for isolating circuits, acti-vation patching , lacks consensus, with strategies such as 

Mean and Resample patching often yielding conflicting re-sults (Heimersheim & Nanda, 2024). Validation standards also vary between model-based metrics (e.g., logits) and 

functional-based metrics (e.g., accuracy). Finally, circuit discovery methods differ in their approach to causality, rang-ing from cumulative searches (Conmy et al., 2023; Lan et al., 2024) to Direct Logit Attribution (Liu et al., 2025). To address these challenges, we introduce a unified frame-work for circuit discovery in SR, visualised in Figure 1. The first stage of the pipeline isolates specific symbolic be-haviours (such as the generation of a sin token) within the Transformer encoder. Then we employ a Probabilistic Algo-rithm for Tuning Circuits through Heuristic Evolution and Search ( PATCHES ), our novel circuit discovery algorithm, to optimise a sparse mask over the model’s components, identifying candidate circuits that maximise performance 1

> arXiv:2602.03506v1 [cs.LG] 3 Feb 2026

Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models Input        

> Layers
> sin
> Model
> Components
> x1+
> Circuit Discovery Circuit Evaluation
> Faithful Complete Minimal
> Active Patched
> Configuration
> Mean Patching Functional Eval
> 1
> Evaluate Generate
> Refine
> 2Candidate Solutions
> 3
> Accuracy ↑
> Fitness Evaluation
> Selection
> Circuit Size ↓
> 4Circuit
> ≈
> Solution Space
> Samples
> Residual Stream
> H1 H8 H2 MLP

Figure 1. The PATCHES Framework. Left: Model schematic: equation samples and previously decoded samples are processed by layers (rows) and components (columns) to predict the target token (e.g., sin ). Center: Discovery loop. We (1) configure the patching strategy, (2-3) sample candidate masks via CMA-ES, and (4) refine the search distribution based on a fitness trade-off between performance and circuit size. Right: Validation criteria based on the best circuit (green). M: Full model; MC : Only circuit active; M↓C : Only circuit complement active; MC↓ci : Only circuit active without circuit component ci.

while minimising size. Finally, to ensure circuit correctness, we evaluate these circuits against three formal criteria: 1. Faithfulness : requiring the circuit to accurately repro-duce the model’s target token (Hanna et al., 2024) 2. Completeness : all necessary components are included in the circuit (Wang et al., 2023b), 3. Minimalilty : no redundant components are present in the circuit (Conmy et al., 2023). We summarise our main contributions below: • We apply circuit discovery to transformer-based SR, isolating circuits responsible for specific unary (e.g., 

sin , exp ) and binary (e.g., +, ×) operator behaviours. To the best of our knowledge, our work is the first such investigation. • In doing so, we present a novel circuit discovery method; PATCHES , which employs the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) that yields smaller circuits than traditional iterative patch-ing for SR. • We establish formal definitions for faithfulness , com-pleteness , and minimality to evaluate circuit correct-ness. We validate the sufficiency of these definitions through extensive experiments, providing a model-agnostic pipeline designed for easy adaptation across domains. • We systematically compare circuit discovery tech-niques, evaluating trade-offs between patching strate-gies (i.e., Mean and Resample patching). We further distinguish model-based from functional-based evalua-tion metrics, and show that Direct Logit Attribution and probing are not ideal for identifying causal relations. 

## 2. Related Work 

Mechanistic interpretability and circuit discovery. 

Mechanistic interpretability (MI) aims to reverse engineer neural networks into interpretable algorithms, often for-malised as circuits , minimal subgraphs sufficient for generat-ing a target token (Miller et al., 2024). Early work analysed individual neurons and attention patterns, later evolving into circuit-level analyses that trace causal pathways through transformers (Olah et al., 2020; Elhage et al., 2021). Circuit discovery has since been applied to tasks such as induction heads, arithmetic, and algorithmic reasoning in language models (Park et al., 2025; Conmy et al., 2023). However, MI has not yet been applied to SR models to the best of our knowledge, despite their structured outputs and inherent interpretability. Our work aims to fulfil this gap. 

Patching and causal interventions. A central tool in MI is activation patching, which tests causal involvement of a component by replacing internal activations during a for-ward pass and measuring output changes (Zhang & Nanda, 2024). To avoid out-of-distribution effects from zeroing or noising, two strategies are common: mean patching (Wang et al., 2023b), replacing activations with their dataset mean, and resample patching , replacing them with those from a corrupted input (Meng et al., 2023). These strategies differ in assumptions and stability; mean patching is input-independent but dataset-sensitive, while resample patching is instance-specific. Despite widespread use, systematic comparisons between them remain underexplored. 

Functional vs model-based evaluation. Circuit correct-ness is typically evaluated using either functional-based or model-based metrics. Functional evaluations measure task-level behaviour such as accuracy or top-k correctness (Yu et al., 2024), while model-based evaluations measure changes in internal quantities such as logit differences or normalised logit scores (Zhang & Nanda, 2024).We employ both methods to systematically compare their efficacy. 2Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

Symbolic regression models. We analyse Neuro Sym-bolic Regression that Scales (NeSymReS) (Biggio et al., 2021), a foundational framework that introduced the transformer-based sequence modelling to SR. Its architec-ture inspires newer models such as SymFormer (Vastl et al., 2022), DGSR (Holt et al., 2023), and TPSR (Shojaee et al., 2023). Given its role in establishing this paradigm, NeSym-ReS is particularly well-suited for mechanistic analysis. NeSymReS uses an encoder–decoder architecture. Numeri-cal datasets (X, y ) are encoded into a latent representation, which the decoder then uses to autoregressively generate mathematical expressions in prefix notation (e.g., sin( x) as 

[sin , x ]). Figure 2 left provides a visualisation, with further details in Appendix A. Numerical constants are represented by a placeholder token c, whose values are optimised post-hoc using BFGS (Fletcher, 1987). 

Probing classifiers. Probing determines if specific infor-mation is decodable from a component’s activations (Hewitt & Manning, 2019), but it cannot distinguish between in-formation the model actually uses (causation) and mere artefacts (correlation) (Belinkov, 2022). While prior work cites this limitation to justify choosing mechanistic analysis over probing (Elazar et al., 2021; Davies & Khakzar, 2024), we take the inverse approach. We apply probes to compo-nents already verified as causally essential by our circuit discovery, using these circuits as a ground truth to test if probing accuracy truly reflects functional utility. 

## 3. Methodology Mean Patch           

> Cache
> Resample Patch
> 1. x 1+ cos(x 2)Cache
> Cache
> Cache
> Resample STR
> 1. x 1+ cos(x 2)Cache 2. x 1+ tan(x 2)n. x 1+ exp(x 2)
> Cache
> Cache
> Model
> 1. sin(x 2)• x 1
> 2. log(x 1• x 2)1000. x 12+ x 2
> Select patch Strategy
> Average
> MLP MLP
> Head 1
> Head 2
> Head 8
> Head 1
> Head 2
> Head 8
> Input
> 0.5 0.3 3.1 0.5 0.7 1.3 0.3 1.1 -2

## +x1 + 

> Average

Figure 2. Diagram illustrating three patching strategies applied to the formula x1 + sin( x2), target being sin . The patching strategy is selected manually. Corresponding patches are cached, averaged, and used to modify the selected parts of the model; in this case, Layer 1’s feedforward block and Layer 2’s Head 1. 

3.1. Patching 

Patching isolates causal influence by replacing the activa-tions of specific components (attention heads or MLPs) with counterfactual values (Conmy et al., 2023). We perform this intervention during the forward pass of the target token generation, as shown in Figure 1 (Model) for the sin oper-ator. We leverage the NNSight library from Fiotto-Kaufman et al. to intercept and overwrite the activations during in-ference. To avoid the out-of-distribution shifts caused by zero- or noise-patching (Zhang & Nanda, 2024; Miller et al., 2024), we employ three distribution-aware strategies shown in Figure 2 which we explain next. 

Mean patching (Blue panel) replaces a component’s acti-vation with a fixed mean computed over a dataset average (Wang et al., 2023b). This patch is input-independent; we select 1000 equations seen by the model during training and compute the average activation for every component. 

Resample patching (Yellow panel) calculates a patch using specific counterfactuals generated for every equation in the dataset. For each input, we create corrupted versions by replacing the target token with all alternative unary or binary tokens except for semantically related tokens. We then replace the component’s activation with the mean activation computed across these corrupted inputs (Nanda et al., 2023). 

Resample Symmetric Token Replacement (STR) (Pur-ple panel) is a single-sample variant of resample patching where the activation is replaced by that of the closest se-mantically related token (Zhang & Nanda, 2024; Vig et al.; Heimersheim & Nanda, 2024). While mean patching requires tuning a hyper-parameter related to what samples are included in the mean patch, with resample patching, we select appropriate corrupted variations by defining what operators can replace the cur-rent operation. Given these trade-offs and the lack of prior comparisons between these methods, we will compare both. 

3.2. Circuit Discovery Pipeline with PATCHES 

To discover the circuits, we propose PATCHES as an alter-native to traditional iterative patching methods for circuit discovery. Unlike other approaches, which rely on the se-quence in which components are patched, risking larger or incorrect circuits due to local dependencies, PATCHES 

performs global optimisation over all components simulta-neously. Our approach makes use of three key elements: target token-specific datasets for activation patching, the  

> PATCHES

algorithm itself, and the well-defined circuit eval-uation criteria. 

Dataset Selection. To ensure reliable and interpretable circuit discovery, we generate separate datasets for each target token, such as add , log , or sin , based on three strict criteria. First, we select only expressions that the 3Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

model correctly reconstructs to ensure failures are due only to patching. Second, to prevent attribution ambiguity, sam-ples must contain the target token but exclude semantically related excluded tokens (e.g., excluding cos when targeting 

sin ). Finally, to ensure fair comparison across strategies, we use the same fixed set of 500 expressions per target token: 100 for discovery and evaluation, and 400 to test generalisation. 

PATCHES employs the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) algorithm, which optimises stochastic, non-convex spaces by iteratively sampling from a multivariate Gaussian distribution (Auger & Hansen, 2005). Unlike reinforcement learning, CMA-ES requires no gra-dients or intermediate rewards, relying solely on fitness evaluations to efficiently discover sparse circuits (see Ap-pendix B for more details). The discovery pipeline follows the workflow illustrated in Figure 1 (Circuit Discovery). First (1), we initialise the configuration with the target token, accompanying dataset, patching method, and evaluation strategy. To enable con-tinuous optimisation of discrete structures (2–3), we model candidate solutions as probabilistic masks. CMA-ES sam-ples these as vectors x ∈ [0 , 1] d, where xi represents the exclusion probability for component i (head or MLP) and 

d is the number of components in the model; any compo-nent with xi > 0.5 is removed from the circuit. Finally (4), the search minimises a dual-objective fitness function that balances circuit size |C| against performance penalties: 

F (C) = |C| + λ

> m

X

> i=1

max(0 , T i − Si(C)) 

where λ = 100 is a penalty constant, and max(0 , T i −

Si(C)) quantifies the failure to meet the performance thresh-old Ti for the evaluation metric Si. We initialise the prob-ability of a component being part of the candidate circuit at 0.5 (std 0.1) to encourage exploration near the exclusion threshold. After the evolutionary phase, we apply itera-tive patching from (Lan et al., 2024) to ensure minimality, which will be formally defined in the following section 1.This approach yields small, faithful circuits that preserve the model’s target token. 

Circuit Evaluation Criteria. After identifying candidate circuits through PATCHES , the next step is to assess whether these circuits genuinely explain the target token. This as-sessment is performed using three key criteria: faithfulness ,

completeness , and minimality as shown in Figure 1 (Circuit Evaluation). We distinguish between functional-level evalu-ation (Yu et al., 2024), and model-level evaluation metrics   

> 1In practice, this step rarely finds smaller circuits. Thus
> PATCHES is often able to find minimal circuits on its own.

(Zhang & Nanda, 2024). Functional-level metrics compare the top-k performance of the full model, M, and the circuit, 

C, we use accuracy as the performance measure. Model-level evaluation measures the difference in the target token’s logit score between the full model and the circuit. We allow for a small performance degradation, δ, in the cir-cuits as a trade-off between circuit correctness and sparsity. 

δ is monotonic: if a circuit satisfies a property for some δ1,it necessarily satisfies it for any δ2 ∈ (δ1, 1] .

Faithfulness assesses whether a circuit, C, reproduces the full model M’s behavior for a target token. We evaluate this by patching the circuits complement: MC , thereby leaving only the circuit intact. A circuit C is functionally faithful up to δf for a model M:

|T k,t (MC |D , ˆy<t ) − T k,t (M|D , ˆy<t )| ≤ δf ∀k, 

and, a circuit C is model faithful up to δf for a model M:

1

|D| 

X

> x∈D

(M(x) − M C (x)) ≤ δf ,

where Tk,t (M | D , ˆy<t ) denotes the top-k accuracy of model M over dataset D at timestep t, given previously predicted tokens ˆy<t and δf ∈ [0 , 1] is the threshold con-trolling acceptable degradation in performance of the full model compared to the circuit of the target token. 

Completeness verifies that the circuit contains all essential components. We test this by patching the circuit from the full model: M↓C , and ensuring that the model’s ability to predict the target token degrades significantly. A circuit C is functionally complete up to δc for a model 

M:

Tk,t (M↓C |D , ˆy<t ) ≤ δc ∀k

A circuit C is model complete up to δc for a model M:

1

|D| 

X

> x∈D

M↓C (x) ≤ δc

Minimality ensures that the circuit contains no redundant components (Wang et al., 2023b; Lan et al., 2024). We evaluate minimality by first patching the circuit complement, 

Mc, then we patch each component ci ∈ C individually; a circuit is minimal if the performance of the modified circuit 

MC↓ci drops by at least δf compared to the unpatched model: A circuit C is functionally minimal up to δf for a model M:

|T k,t (M|D , ˆy<t ) − T k,t (MC↓ci |D , ˆy<t )| ≥ δf , ∀i∀k

A circuit C is model minimal up to δf for a model M:

1

|D| 

X

> x∈D

(M(x) − M C↓ci (x)) ≥ δf , ∀i

4Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models                                                                                                                                                                                                                                    

> Table 1. Baseline and Circuit Discovery Results on the generalisation set. Performance is measured per target token (Tgt) in Top-k
> accuracy (T1–T3) and normalised Logit Score (Lgt). Baseline: Scores for the Full Model and the Patched Model using Mean, Resample, or STR patching. Circuit: Discovered Circuit Size (CS), Faithfulness, and Completeness evaluated via Functional Accuracy (Acc) or Model Logits (Lgt). Ëindicates the circuit satisfies all 3 correctness criteria. Green: above full model performance, red: below threshold.
> BASELINE CIRCUIT Full Model ↑Patched Model ↓Faithful ↑Comp. ↓
> Tgt Patch Eval T1 T2 Lgt T1 T2 T3 Lgt CS ↓T1 T2 T3 Lgt T3 Lgt Ë
> Add Res. Acc 0.93 0.94 0.92 0.16 0.86 0.99 0.21 57 0.85 0.94 0.99 0.72 0.99 0.23 é
> Res. Lgt 0.93 0.94 0.92 0.16 0.86 0.99 0.21 67 0.92 0.94 1.00 0.92 0.99 0.21 é
> Log
> Mean Acc 0.31 0.63 0.31 0.00 0.00 0.12 0.00 50 0.66 1.00 1.00 0.44 0.15 0.01 Ë
> Mean Lgt 0.31 0.63 0.31 0.00 0.00 0.12 0.00 47 0.00 0.96 1.00 0.23 0.12 0.00 Ë
> Res. Acc 0.31 0.63 0.31 0.00 0.06 0.54 0.00 84 0.26 0.58 0.90 0.28 0.52 0.02 é
> Res. Lgt 0.31 0.63 0.31 0.00 0.06 0.54 0.00 47 0.23 0.41 0.79 0.21 0.53 0.05 é
> STR Exp 0.31 0.63 0.31 0.00 0.20 0.33 0.00 70 0.24 0.60 0.92 0.30 0.45 0.63 é
> Sin
> Mean Acc 0.73 0.83 0.70 0.00 0.00 0.00 0.00 57 0.70 0.79 1.00 0.51 0.00 0.00 Ë
> Mean Lgt 0.73 0.83 0.70 0.00 0.00 0.00 0.00 52 0.79 0.83 1.00 0.63 0.00 0.00 Ë
> Res. Acc 0.73 0.83 0.70 0.03 0.12 0.24 0.04 63 0.65 0.81 0.87 0.59 0.28 0.04 é
> Res. Lgt 0.73 0.83 0.70 0.03 0.12 0.24 0.04 62 0.65 0.80 0.86 0.60 0.24 0.04 Ë
> STR Cos 0.73 0.83 0.70 0.00 0.50 0.70 0.03 69 0.55 0.74 0.91 0.39 0.70 0.03 é

## 4. Circuit Discovery Results 

We now turn to experimental results, starting with baseline performance before analysing PATCHES circuits. We focus discovery on the encoder of our autoregressive model to isolate latent representation generation. With PATCHES , we explore the discovery of three operators; 

add , log , sin and illustrate an additional five operators in Appendix D. We compare circuits found with Resam-ple Patching, Mean patching, Functional and Model-based performance metrics 2.

4.1. Model Performance 

Before analysing circuit discovery, we first assess the un-patched model to establish its baseline performance and limitations. A more detailed evaluation of NeSymReS is provided in Appendix C. To isolate model behaviour, we exclude formulas requiring constants, thereby avoiding re-liance on the external BFGS optimiser. Our model can reconstruct 61.3% of formulas without con-stants (see Figure 7), is best in predicting addition and mul-tiplication and struggles with logarithms (see Figure 9). We also show that our model has trouble with longer equations (see Figure 9 a-c) and verify correct use by testing on the same dataset used by the original authors (see Figure 10). 

4.2. Baselines 

Establishing a baseline is vital for circuit discovery: we must ensure the full model reliably performs the task, and conversely, that patching disrupts the target token for causal isolation. Table 1 (BASELINE) shows baselines across op-

> 2All code will be made available upon publication.

erations. We evaluate top-k accuracy with k = 3 , reflecting the model’s beam search behavior (See Figure 8b). Ideally, patching all components should substantially de-grade model performance. However, resample patching fails to suppress the sin operator, retaining high perfor-mance. This indicates that the intervention does not fully erase target token information, thereby obscuring causal effects and hindering circuit discovery. This issue is even more pronounced with STR patching; as shown in Appendix D Table 3, all tested operators retain excessively high baseline scores. Consequently, we caution against using resample STR without explicitly verifying that they sufficiently disrupt the model’s behaviour. Similarly, addition and multiplication retain high baselines due to dataset overrepresentation, causing the model to de-fault to them even when patched. To mitigate this, we re-strict analysis to resample patching for these tokens, which more effectively removes correlated information. 

4.3. Circuits The Sine Mean Functional Circuit. We illustrate the dis-covery process with the sin operator using Mean Patching and Functional evaluation. First, we generate 500 equa-tions containing sin , excluding cos to ensure specificity. Second, the baseline: the full model achieves 100% top-3 accuracy, while the fully mean-patched model drops to 0.00%, see Table 1 (BASELINE), confirming the patch sup-presses the target token. Third, PATCHES finds a minimal subgraph of 57 components. Finally, evaluation confirms correctness by recovering full accuracy (Faithful: 1.00), and its complement fails to generate the target token (Complete: 0.00), see Table 1 (CIRCUIT). 5Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models add RF 

> add RM
> log MF
> log MM
> log RF
> log RM
> log RFSTR
> sin MF
> sin MM
> sin RF
> sin RM
> sin RFSTR
> Target Token + Config
> add RF
> add RM
> log MF
> log MM
> log RF
> log RM
> log RFSTR
> sin MF
> sin MM
> sin RF
> sin RM
> sin RFSTR
> Target Token + Config
> 50
> 55
> 60
> 65
> 70
> 75
> 80
> Overlap Percentage (%)

(a) MLP H1 H2 H3 H4 H5 H6 H7 H8  

> Components
> L1.1
> L1.2
> L2.1
> L2.2
> L3.1
> L3.2
> L4.1
> L4.2
> L5.1
> L5.2
> L6.1
> L6.2
> OUT
> Layers
> 0
> 2
> 4
> 6
> 8
> 10
> 12
> # Circuits

(b) Figure 3. Confusion plots illustrating the diversity of circuits identified in the model. (a) Overlap percentages between operators; the diagonal (black) indicates circuit length. RF: Resample Functional; RM: Resample Model; MF Mean Functional; MM: Mean Model; RFSTR: Resample Functional STR (b) Usage frequency of components in the selected circuits. L x.x denotes layer x.x ; OUT the output projection; MLP the multilayer perceptron block; H x attention head x. Full tables and figures in Appendix F. 

Overall results shown in Table 1 present the 12 circuits discovered across different configurations on the generalisa-tion set with all 28 circuits observed in Table 4. Discovery set results are shown in Table 5. Discovery set scores are similar to the generalisation results, confirming that the discovered circuits generalise well to unseen equations. To evaluate the circuits, we require faithfulness to be within 10 percentage points of the baseline ( δf = 0 .1), and patched circuit performance to fall below 25% for completeness (δc= 0 .25) . These thresholds were chosen empirically to balance model flexibility and behavioural specificity. Out of the 12 evaluated circuits, one does not meet the faith-fulness threshold under its respective evaluation strategy. For completeness, seven circuits exceed the 25% cutoff. However, this is often due to high baseline scores, making it harder to suppress the behaviour through patching. Ex-cluding cases where patched model baseline performance exceeds 25%, only a single circuit, sin -RF, remains incom-plete. We report both successful and unsuccessful cases to emphasise the importance of strong baselines and careful consideration of patching strategies. In total, 13 of the 28 circuits satisfy all three criteria: faith-fulness, completeness, and minimality, and are therefore considered correct 3. We illustrate that our approach can be expand upon easily to multi-token circuits based on the work from Garc´ ıa-Carrasco et al. in Appendix E. 

> 3

We omit minimality from Table 1 as it is enforced by PATCHES ,which inherently solves for the minimal subset of components required by the chosen evaluation strategy. 

Circuits comprise 40%–78% of the model. We attribute this density to SR’s constraints: unlike NLP, where vast vocabularies enable sparsity, our ∼20-token vocabulary ne-cessitates a distributed, polysemantic encoding of complex syntax (e.g., nesting). This high utilization is not unique to SR; similar circuit densities are observed in sequence continuation tasks (Lan et al., 2024). Consequently, us-age variations appear driven by patching and evaluation strategies rather than intrinsic complexity; we analyse these effects below. 

Functional Vs. Model-based performance. Functional-based evaluation yields shorter circuits for most operators (Table 4), though neither method is strictly superior. While logit-metrics capture subtle internal effects, they may not reflect meaningful output changes, which are critical in autoregressive settings where small shifts flip predictions. Since we are interested in explaining the overall behaviour to users rather than fine-tuning internal states, we advocate for functional evaluation. 

Mean Vs. Resample Patching results illustrate that for 

sin and log (and cos , tan ), resample patching consis-tently produces longer circuits. This is expected, as it ex-cludes the target operation and requires more components to preserve performance. While faithfulness remain similar, completeness often worsens. The same is true for resam-ple STR patching, which also produces longer circuits and fails the completeness threshold in three of four cases. We therefore recommend mean patching, which offers correct circuits more often, and is computationally more efficient while generating smaller, more interpretable circuits. 6Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models add RF 

> add RM
> log MF
> log MM
> log RF
> log RM
> sin MF
> sin MM
> sin RF
> sin RM
> Target Token + Config
> 80
> 85
> 90
> 95
> 100
> Accuracy (%)
> Circuit
> Complement

(a) add RF  

> add RM
> log MF
> log MM
> log RF
> log RM
> sin MF
> sin MM
> sin RF
> sin RM
> Target Token + Config
> 0
> 20
> 40
> 60
> 80
> Circuit length
> PATCHES (Ours)
> Iterative Patching
> (In)correct circuit

(b) Figure 4. Probing and Iterative Patching Results. RF: Resample Functional, RM: Resample Model, MF Mean Functional, MM: Mean Model, RFSTR: Resample Functional STR. (a) Probe accuracy comparisons between circuit and complement components. Full Table in Appendix H. (b) Circuit length comparison between PATCHES and Iterative Patching. é/Ë indicate incorrect and correct circuits respectively. Full Table in Appendix G. 

4.4. Verification Experiments 

We verify circuits via three experiments. First, we check component usage and overlap to ensure distinctness. Next, we illustrate that probing circuit components contain more linearly decodable information. The first two verification steps are displayed in Figure 3. To confirm that circuits are not merely subsets of one an-other, we report overlap percentages in Figure 3a. As ex-pected, higher correlations appear for longer circuits and when the same operators are compared. Figure 3b illustrates which components are utilised across layers, revealing a well-distributed usage pattern in which the full model con-tributes to these operators. Full results for all circuits, along with an additional verification experiment, are provided in Appendix F. 

Probing. We use probing as an additional verification ex-periment to assess whether components identified by circuit discovery also exhibit stronger linear decodability, and to sit-uate our findings within the broader probing literature. We compare probing performance between components inside the circuit and components outside the circuit. To do so, we randomly sample 10 components from the circuit and 10 components from the complement of the circuit. For each of these 20 components, we train a prob-ing model using 10 different random seeds, resulting in a total of 200 trained models per evaluation and patching con-figuration (see Appendix H for hyperparameter settings). Subsequently, we calculate the average performance across seeds for both circuit and circuit complement components. A paired t-test is conducted to assess the significance of the performance difference between groups. Across all target tokens, probing accuracy is generally higher for circuit components than for components in the circuit complement, as shown in Figure 4a. This suggests that circuit components tend to encode more linearly decod-able information about the model’s output behaviour. In addition, circuit components exhibit similar or lower stan-dard deviation in probe accuracy, indicating more consistent information encoding across sampled components. Despite these trends, differences in probing accuracy are rarely statistically significant. Only the log operator shows a significant gap between circuit and out-of-circuit compo-nents, likely due to its smaller and more isolated circuit. In addition to verifying that more information is stored in circuit components, these results reinforce prior findings that high probing performance does not necessarily imply causal relevance (Ravichander et al., 2021; Elazar et al., 2021; Davies & Khakzar, 2024). While circuit components more reliably represent behaviourally relevant information, probing alone fails to confirm that the model uses this infor-mation during prediction, as probing accuracy remains high even for complement components. 

4.5. Alternative Circuit discovery techniques. 

To illustrate our circuits are compact we compare our method to Iterative Patching (Lan et al., 2024; Conmy et al., 2023), which initialises a candidate circuit as the full model and sequentially patches components layer-by-layer. If per-formance does not decrease significantly, the component is deemed non-essential and removed from the candidate circuit. The algorithm alternates between backward and forward sweeps until convergence. However, empirical test-7Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models MLP H1 H2 H3 H4 H5 H6 H7 H8 

> Components
> L1.1
> L1.2
> L2.1
> L2.2
> L3.1
> L3.2
> L4.1
> L4.2
> L5.1
> L5.2
> L6.1
> L6.2
> OUT
> Layers
> 0.05
> 0.10
> 0.15
> 0.20
> 0.25
> 0.30
> 0.35
> Logit Score

(a) 0 10 20 30 40 50 60 70 80 90 100 110  

> Number of Heads Included
> 0.0
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> Logit Score
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Top-k Accuracy
> Top-1
> Top-2
> Top-3
> Logit Score

(b) Figure 5. Direct attribution and faithfulness evaluation . (a) Change in logit score vs. sin mean when patching individual heads, averaged over 100 samples. L x.x denotes layer x.x ; OUT the output projection; MLP the multilayer perceptron block; H x attention head x.

(b) Faithfulness evaluation of sin mean importance ranking; thresholds (dashed lines) from Table 1. 

ing reveals that this greedy, order-dependent approach often fails to find small circuits, as early pruning decisions can lock the search into local optima. We compare PATCHES to Iterative Patching by applying Iterative Patching across the same operations as in Section 4.2; additional results are provided in Appendix G. Figure 4b illustrates that circuits found by Iterative Patch-ing are overall larger. In addition to being larger circuits, these circuits do not generalise as well as the circuits found by PATCHES as the log Mean Functional circuit and sin 

Resample Model circuits are found to be incorrect on the generalisation set, which is not the case for PATCHES . These examples illustrate that PATCHES produces smaller circuits and that more components does not guarantee better perfor-mance and may introduce confounding heads. 

## 5. Direct Logit Attribution 

Unlike cumulative circuit discovery methods such as  

> PATCHES

and Iterative Patching, Direct Logit Attribution evaluates components individually by patching a component out and measuring their logit change. Components with the highest impact are selected to form circuits, but these meth-ods often omit standard circuit correctness evaluations. We evaluate circuits by first testing for faithfulness, followed by minimality and completeness. To find a circuit via Direct Logit Attribution, we iteratively reintroduce components to a fully patched model, starting with the component showing the largest individual logit or performance difference when patched. We continue this process until either the logit score or the Top-k accuracies meet the respective thresholds. Results displayed in Figure 5 suggest that Direct Logit At-tribution struggles to produce a faithful circuit, requiring almost all components to meet model and functional faith-fulness, creating non-minimal circuits. We illustrate that this ordering and reintroducing of components has little cor-relation with model usage as we see no score improvement for the first 50 components. Additional results in Appendix I Figure 18 show that resample patching, despite earlier gains, still requires many components to achieve faithfulness. These results illustrate that this method does not produce circuits compatible with our evaluation strategies, as it fails the minimality criterion required to isolate specific target tokens. While the circuits are technically complete, since nearly the full model is needed to reach high performance, they meet neither faithfulness nor minimality standards. 

## 6. Conclusion 

In this work, we presented the first mechanistic interpretabil-ity analysis of transformer-based symbolic regression. By introducing PATCHES , an evolutionary circuit discovery al-gorithm, we identified 28 circuits from which 13 were found to generalise well to unseen equations. These circuits were found to be faithful, complete and minimal for unary and binary operators. Our systematic comparison reveals that standard greedy search methods often fail to find small circuits for symbolic regression, and that mean-patching combined with functional evaluation offers a more reliable framework for circuit discovery. Crucially, our findings challenge the utility of Direct Logit Attribution and probing classifiers for causal analysis, as neither reliably correlates with the components actually used by the model. 8Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

## Impact Statement 

“This paper presents work whose goal is to advance the field of mechanistic interpretability. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.” 

## References 

Auger, A. and Hansen, N. Restart cma evolution strategies with increasing population size. In 2005 IEEE Congress on Evolutionary Computation , volume 2, pp. 1769–1776. IEEE, 2005. Belinkov, Y. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics , 48(1):207–219, April 2022. ISSN 0891-2017, 1530-9312. doi: 10.1162/ coli a 00422. Biggio, L., Bendinelli, T., Neitz, A., Lucchi, A., and Paras-candolo, G. Neural symbolic regression that scales. (arXiv:2106.06427), June 2021. doi: 10.48550/arXiv.2 106.06427. URL http://arxiv.org/abs/2106 .06427 . arXiv:2106.06427 [cs]. Boyd, S., Kim, S.-J., Vandenberghe, L., and Hassibi, A. Atutorial on geometric programming . Stanford University, 2007. Conmy, A., Mavor-Parker, A. N., Lynch, A., Heimer-sheim, S., and Garriga-Alonso, A. Towards auto-mated circuit discovery for mechanistic interpretability. (arXiv:2304.14997), October 2023. doi: 10.48550/arX iv.2304.14997. URL http://arxiv.org/abs/23 04.14997 . arXiv:2304.14997 [cs]. Cui, Z., Qi, C., Zhou, T., Yu, Y., Wang, Y., Zhang, Z., Zhang, Y., Wang, W., and Liu, Y. Artificial intelligence and food flavor: How ai models are shaping the future and revolutionary technologies for flavor food development. 

Comprehensive Reviews in Food Science and Food Safety ,24(1):e70068, January 2025. ISSN 1541-4337, 1541-4337. doi: 10.1111/1541-4337.70068. Davies, A. and Khakzar, A. The cognitive revolution in interpretability: From explaining behavior to interpret-ing representations and algorithms. (arXiv:2408.05859), August 2024. doi: 10.48550/arXiv.2408.05859. URL http://arxiv.org/abs/2408.05859 .arXiv:2408.05859 [cs]. Elazar, Y., Ravfogel, S., Jacovi, A., and Goldberg, Y. Am-nesic probing: Behavioral explanation with amnesic coun-terfactuals. Transactions of the Association for Com-putational Linguistics , 9:160–175, March 2021. ISSN 2307-387X. doi: 10.1162/tacl a 00359. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. A math-ematical framework for transformer circuits. Trans-former Circuits Thread , 2021. https://transformer-circuits.pub/2021/framework/index.html. Fiotto-Kaufman, J., Loftus, A. R., Todd, E., Brinkmann, J., Pal, K., Troitskii, D., Ripa, M., Belfki, A., Rager, C., Juang, C., Mueller, A., Marks, S., Sharma, A. S., Lucchetti, F., Prakash, N., Brodley, C., Guha, A., Bell, J., Wallace, B. C., and Bau, D. Nnsight and ndif: De-mocratizing access to open-weight foundation model internals. (arXiv:2407.14561), January 2025. doi: 10.48550/arXiv.2407.14561. URL http://arxi v.org/abs/2407.14561 . arXiv:2407.14561 [cs]. Fletcher, R. Practical Methods of Optimization . John Wiley & Sons, New York, NY, USA, 2 edition, 1987. Garc ´ıa-Carrasco, J., Mat ´e, A., and Trujillo, J. Detecting and understanding vulnerabilities in language models via mechanistic interpretability. In Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intel-ligence , IJCAI-2024, pp. 385–393. International Joint Conferences on Artificial Intelligence Organization, Au-gust 2024. doi: 10.24963/ijcai.2024/43. URL http: //dx.doi.org/10.24963/ijcai.2024/43 .Hanna, M., Liu, O., and Variengien, A. How does gpt-2 compute greater-than?: Interpreting mathematical abili-ties in a pre-trained language model, 2023. URL https: //arxiv.org/abs/2305.00586 .Hanna, M., Pezzelle, S., and Belinkov, Y. Have faith in faithfulness: Going beyond circuit overlap when finding model mechanisms, 2024. URL https://arxiv.or g/abs/2403.17806 .Hansen, N. The cma evolution strategy: A tutorial, 2023. URL https://arxiv.org/abs/1604.00772 .Heimersheim, S. and Nanda, N. How to use and interpret ac-tivation patching. (arXiv:2404.15255arXiv:2404.15255), April 2024. doi: 10.48550/arXiv.2404.15255. URL http://arxiv.org/abs/2404.15255 .arXiv:2404.15255 [cs]. Hewitt, J. and Manning, C. D. A structural probe for finding syntax in word representations. In Burstein, J., Doran, C., and Solorio, T. (eds.), Proceedings of the 2019 Confer-ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-gies, Volume 1 (Long and Short Papers) , pp. 4129–4138, 9Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1419. URL https://aclanthology.org/N19-141 9/ .Holt, S., Qian, Z., and Schaar, M. v. d. Deep gener-ative symbolic regression. (arXiv:2401.00282), De-cember 2023. doi: 10.48550/arXiv.2401.00282. URL http://arxiv.org/abs/2401.00282 .arXiv:2401.00282 [cs]. Lample, G. and Charton, F. Deep learning for symbolic mathematics. (arXiv:1912.01412), December 2019. doi: 10.48550/arXiv.1912.01412. URL http://arxiv. org/abs/1912.01412 . arXiv:1912.01412 [cs]. Lan, M., Torr, P., and Barez, F. Towards interpretable se-quence continuation: Analyzing shared circuits in large language models. (arXiv:2311.04131), October 2024. doi: 10.48550/arXiv.2311.04131. URL http://arxi v.org/abs/2311.04131 . arXiv:2311.04131 [cs]. Lee, J., Lee, Y., Kim, J., Kosiorek, A. R., Choi, S., and Teh, Y. W. Set transformer: A framework for attention-based permutation-invariant neural networks. Liu, Q., Mao, J., and Wen, J.-R. How do large language models understand relevance? a mechanistic interpretabil-ity perspective. (arXiv:2504.07898arXiv:2504.07898), April 2025. doi: 10.48550/arXiv.2504.07898. URL http://arxiv.org/abs/2504.07898 .arXiv:2504.07898 [cs]. Lundberg, S. M. and Lee, S.-I. A unified approach to in-terpreting model predictions. In Advances in Neural Information Processing Systems 30 (NeurIPS 2017) , pp. 4765–4774, 2017. URL https://arxiv.org/ab s/1705.07874 .Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in gpt. (arXiv:2202.05262), January 2023. doi: 10.48550/a rXiv.2202.05262. URL http://arxiv.org/abs/ 2202.05262 . arXiv:2202.05262 [cs]. Meurer, A., Smith, C. P., Paprocki, M., ˇCert ´ık, O., Kir-pichev, S. B., Rocklin, M., Kumar, A., Ivanov, S., Moore, J. K., Singh, S., Rathnayake, T., Vig, S. L., Granger, B. E., Muller, R., Bonazzi, F., Gupta, H., Vats, S., and Johansson, F. Sympy: symbolic computing in python. PeerJ Computer Science , 3:e103, 2017. doi: 10.7717/peerj-cs.103. Miller, J., Chughtai, B., and Saunders, W. Trans-former circuit faithfulness metrics are not robust. (arXiv:2407.08734), July 2024. doi: 10.48550/arXiv .2407.08734. URL http://arxiv.org/abs/24 07.08734 . arXiv:2407.08734 [cs]. Molnar, C. Interpretable Machine Learning: AGuide for Making Black Box Models Explainable .Lulu.com, 2019. 3rd edition available online at https://christophm.github.io/interpretable-ml-book/. Nanda, N., Rajamanoharan, S., Kramar, J., and Shah, R. Fact finding: Attempting to reverse-engineer factual re-call on the neuron level, Dec 2023. URL https: //www.alignmentforum.org/posts/iGuwZ THWb6DFY3sKB/fact-finding-attemptin g-to-reverse-engineer-factual-recall .Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. Zoom in: An introduction to circuits. 

Distill , 2020. doi: 10.23915/distill.00024.001. URL 

https://distill.pub/2020/circuits/zo om-in .OpenStax. University Physics Volume 1 . OpenStax, 2023. URL https://openstax.org/books/unive rsity-physics-volume-1/pages/15-4-pen dulums . Accessed: 2025-05-25. Park, C. F., Lee, A., Lubana, E. S., Yang, Y., Okawa, M., Nishi, K., Wattenberg, M., and Tanaka, H. Iclr: In-context learning of representations. (arXiv:2501.00070), May 2025. doi: 10.48550/arXiv.2501.00070. URL http:// arxiv.org/abs/2501.00070 . arXiv:2501.00070 [cs]. Rajaram, A., Chowdhury, N., Torralba, A., Andreas, J., and Schwettmann, S. Automatic discovery of visual circuits. (arXiv:2404.14349), April 2024. doi: 10.48550/arXiv.2 404.14349. URL http://arxiv.org/abs/2404 .14349 . arXiv:2404.14349 [cs]. Ravichander, A., Belinkov, Y., and Hovy, E. Probing the probing paradigm: Does probing accuracy entail task relevance? (arXiv:2005.00719), March 2021. doi: 10.4 8550/arXiv.2005.00719. URL http://arxiv.org/ abs/2005.00719 . arXiv:2005.00719 [cs]. Ribeiro, M. T., Singh, S., and Guestrin, C. “why should i trust you?”: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ,pp. 1135–1144. ACM, 2016. doi: 10.1145/2939672.29 39778. URL https://doi.org/10.1145/2939 672.2939778 .Shojaee, P., Meidani, K., Farimani, A. B., and Reddy, C. K. Transformer-based planning for symbolic regression. (arXiv:2303.06833), October 2023. doi: 10.48550/arX iv.2303.06833. URL http://arxiv.org/abs/23 03.06833 . arXiv:2303.06833 [cs]. 10 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. (arXiv:1409.4842), September 2014. doi: 10.48550/arXiv.1409.4842. URL http://arxiv.org/abs/1409.4842 .arXiv:1409.4842 [cs]. Tyto Robotics. How to increase drone flight time and lift capacity. https://www.tytorobotics.com /blogs/articles/how-to-increase-dro ne-flight-time-and-lift-capacity , 2023. Accessed: 2025-05-25. Udrescu, S.-M. and Tegmark, M. Ai feynman: aphysics-inspired method for symbolic regression. (arXiv:1905.11481), April 2020. doi: 10.48550/arX iv.1905.11481. URL http://arxiv.org/abs/19 05.11481 . arXiv:1905.11481 [physics]. Vastl, M., Kulh ´anek, J., Kubal ´ık, J., Derner, E., and Babu ˇska, R. Symformer: End-to-end symbolic regression us-ing transformer-based architecture. (arXiv:2205.15764), October 2022. doi: 10.48550/arXiv.2205.15764. URL http://arxiv.org/abs/2205.15764 .arXiv:2205.15764 [cs]. Vig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S. Investigating gender bias in language models using causal mediation analysis. Wang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., Chandak, P., Liu, S., Van Katwyk, P., Deac, A., Anand-kumar, A., Bergen, K., Gomes, C. P., Ho, S., Kohli, P., Lasenby, J., Leskovec, J., Liu, T.-Y., Manrai, A., Marks, D., Ramsundar, B., Song, L., Sun, J., Tang, J., Veli ˇckovi ´c, P., Welling, M., Zhang, L., Coley, C. W., Bengio, Y., and Zitnik, M. Scientific discovery in the age of artificial intelligence. Nature , 620(7972): 47–60, August 2023a. ISSN 0028-0836, 1476-4687. doi: 10.1038/s41586-023-06221-2. Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. Interpretability in the wild: A circuit for indirect object identification in gpt-2 small. 2023b. Yu, L., Niu, J., Zhu, Z., and Penn, G. Functional faithfulness in the wild: Circuit discovery with differentiable com-putation graph pruning. (arXiv:2407.03779), July 2024. doi: 10.48550/arXiv.2407.03779. URL http://arxi v.org/abs/2407.03779 . arXiv:2407.03779 [cs]. Zhang, F. and Nanda, N. Towards best practices of activation patching in language models: Metrics and methods, 2024. URL https://arxiv.org/abs/2309.16042 .11 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

## A. Neural Symbolic Regression that Scales 

Recent work has extended the Transformer architecture to SR. Neural Symbolic Regression that Scales (NeSymReS), introduced by Biggio et al. (Biggio et al., 2021), is the first large-scale pre-trained Transformer model designed for SR. This thesis uses NeSymReS due to its pre-trained weights and well-documented open-source code. While newer models have been proposed, many build on NeSymReS and adopt similar architectures, often with less accessible or poorly documented codebases. To give a solid understanding of the model, the next paragraphs will describe the pipeline, encoder-decoder architecture, as well as the training setup of NeSymReS. 

Pipeline To uncover equations that represent the input (X, y ) pairs, the NeSymReS pipeline follows four stages for each equation. First, a ground-truth infix expression f ∗ is generated using the framework from Lample et al. (Lample & Charton, 2019). Based on this framework, a skeleton representing the symbolic structure of the expression is sampled. The skeleton is the full equation with constants abstracted as variables (e.g., sin (x1) + c1 · x2). These constants are then instantiated with sampled values from predefined ranges. Second, the completed expression is evaluated over a range of input values to generate input-output pairs (X, y ). Third, these observations are encoded by NeSymReS into a latent representation, which the decoder uses to auto-regressively generate a prefix skeleton formula. Finally, during inference, beam search identifies the most likely candidate equations, and the constants are refined using the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm, to produce the final predicted expression f pred .

Encoder Architecture The encoder used in NeSymReS is a set encoder , introduced by Lee et al., designed to process unordered sets. Set encoders ensure permutation equivariance : reordering the input rows does not affect the output representation. These encoders are particularly well-suited to symbolic regression tasks, where the order of input examples should not influence the model’s predictions. To address the quadratic complexity O(n2) of standard self-attention (with respect to the input size n), the set encoder uses a more efficient attention mechanism based on trainable inducing points . These inducing points act as a learned, fixed-size bottleneck through which input elements interact. By projecting the input set into a smaller, intermediate set of m inducing points, the attention computation is reduced to O(n · m), which significantly improves scalability. The NeSymReS encoder is composed of six stacked layers, each implementing an Induced Set Attention Block (ISAB) .ISABs introduce the set of trainable inducing points I ∈ Rm×d, with each ISAB containing two Multihead Attention Blocks (MABs) , applied sequentially: 

ISAB( X) = MAB( X, H) where H = MAB( I, X)

First, the inducing points attend to the inputs to produce a condensed representation H. Then, each input token attends to H,allowing information to flow indirectly between all input elements via the shared inducing set. Each MAB uses eight attention heads followed by a feedforward block. The encoder operates with a hidden dimensionality of 512 and uses 50 inducing points. See Figure 6 for an illustration of a single ISAB layer. Add Norm 

> Head 1
> Head 2
> Head 8
> MultiHeadAttention
> Add Norm
> Feed Forward

MAB 

> Add Norm
> Head 1
> Head 2
> Head 8
> MultiHeadAttention
> Add Norm
> Feed Forward

MAB 

ISAB  

> Induction
> Observations
> Figure 6. Diagram of a set-encoder architecture. Each ISAB layer (blue) contains two MAB blocks (grey), which consist of MultiHeadAt-tention (orange) and Feed Forward modules with residual connections (dashed arrows).

12 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

Decoder Architecture The decoder in NeSymReS is a standard transformer decoder that autoregressively generates mathematical expressions. During inference, at each timestep, n candidate solutions are generated, where n is twice the beam size. For each candidate, the probability of the most likely character (from the set of all output symbols) is computed and added to the current beam score, which represents the cumulative logit score of the sequence decoded so far. The n/ 2

highest-scoring candidates are retained for the next generation step, while the rest are discarded. If the end token ( <F> ) is generated, the corresponding equation is checked for validity and, if valid, added to a list of potential hypotheses. Once the maximum equation length is reached, constants in the hypotheses are optimised using the BFGS algorithm and compared against the true y-values. Formulas with the smallest deviation between predicted ( ypred ) and true ( ytrue ) values are returned to the user. 

Pre-Training NeSymReS is trained on 100 million equations 4. The training dataset contains 1.2 million unique skeletons with an average length of 8.2 operators. The model’s task is to predict the prefix-notated formula that corresponds to the given observations. During training, the decoder receives the ground truth previous tokens rather than generating the sequence autoregressively. The model is trained with minibatches of size 150, using a negative log-likelihood loss function for 23 epochs over three days on an RTX 2080. 

Gradient-Based Search with BFGS In SR, models often predict expressions containing both structural components (e.g., operators and variables) and numerical constants. While structure can be learned using neural networks such as transformers, the precise values of constants typically cannot be predicted directly due to the limited output vocabulary. To address this, NeSymReS treats constants as free parameters c and optimises them post-hoc using gradient-based methods. One common approach is to apply the BFGS (Broyden–Fletcher–Goldfarb–Shanno) algorithm, a quasi-Newton method for unconstrained non-linear optimization by Fletcher. BFGS is particularly well-suited for this task due to its efficiency and ability to approximate the Hessian matrix, enabling faster convergence than basic gradient descent. Given a symbolic expression f (x; θ), where θ denotes the set of constant parameters in the expression, we define a loss function over a dataset {(xi, y i)}Ni=1 :

L(θ) = 1

N

> N

X

> i=1

(f (xi; θ) − yi)2

This loss measures how well the expression with current constants fits the target data. The BFGS algorithm iteratively updates θ to minimize L(θ), using both gradient information and an approximation of the inverse Hessian: 

θk+1 = θk − Hk∇L (θk)

where Hk is an approximation to the inverse of the Hessian at step k, and ∇L (θk) is the gradient of the loss with respect to the constants. 

> 4While NeSymReS was also trained on smaller datasets, this particular model achieved the best performance according to Biggio et al. (Biggio et al., 2021) and is therefore used in this work.

13 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

## B. Covariance Matrix Adaptation Evolution Strategy 

This section further introduces the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), a core component of our  

> PATCHES

circuit discovery method. To provide the necessary context, we briefly review the foundations of evolutionary computing and explain the core principles of CMA-ES. Evolutionary computing (EC) is a family of optimalisation algorithms based on natural selection. A typical evolutionary algorithm is iterative and returns a population of candidate solutions after every generation. These get evaluated on their fitness (performance on the task) and with this information a selection is made on the best candidates. These candidates are then recombined with each other and mutated to create a new generation of candidates. Unlike reinforcement learning which sometimes requires gradients or intermediate rewards, evolutionary strategies solely operate on the fitness evaluation making them particularly suitable for optimising complex, non-differentiable objectives, such as discovering sparse circuits in large models. One such EC algorithm is CMA-ES, a stochastic optimisation method for non-convex, high-dimensional search spaces. The algorithm adapts a covariance matrix C to capture parameter correlations (Hansen, 2023; Auger & Hansen, 2005). CMA-ES maintains a multivariate Gaussian distribution N (m, σ 2C), where m is the mean vector representing the current estimate of the optimal solution, σ is the global step size 5, and C encodes the shape and orientation of the search distribution. In each generation, λ candidate solutions xi are sampled from the distribution. After evaluating their fitness, the mean vector m is updated through a weighted average of the top-performing candidates. This guides the search towards better regions of the solution space. The covariance matrix C is then updated as: 

C = (1 − ccov )C + ccov pcp⊤ 

> c

+ ccov 

> λ

X

> i=1

wi

(xi − m′)( xi − m′)⊤

σ2

Here, ccov controls how quickly the covariance adapts, and pc is the evolution path, or momentum. It accumulates the directions over generations and is updated as: 

pc = (1 − cc)pc + pcc(2 − cc) · m − m′

σ

Additionally, the global step size σ is also adapted, based on how far the search is progressing. This is controlled by the conjugate evolution path pσ :

σ = σ · exp 

 ∥pσ ∥

q

1 − (1 − cσ )2·(t+1) 

− 1

 · 1

dσ

pσ = (1 − cσ )pσ + pcσ (2 − cσ ) · C−1/2 m − m′

σ

This reflects how CMA-ES adapts its step size: increasing it when progress is consistent, and reducing when the search needs to focus on fine-tuning. 

> 5The step size in CMA-ES serves a similar purpose to the learning rate in deep learning; it determines the scale of updates during optimisation.

14 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

## C. Model Performance 

To evaluate NeSymReS performance, we use the uploaded 100Mil pretrained model weights 6. Generating the equations is supported by both beam search and the BFGS algorithm, whose individual contributions to performance are assessed. To validate the findings reported by Biggio et al., we replicate their methodology as closely as possible and evaluate performance on the Feynman dataset (Udrescu & Tegmark, 2020). 

Table 2. Comparison of true and model-generated equations by reconstruction strategies. 

Evaluation Type True Equation Recreated Equation Skeleton 2 · x1 − x2

x3

2 · x1 − x2

x3

Inserting 0 x1

x42

x1

x42 · (c + 1) 

Inserting 1 − sin( x2) + tan( x21) − sin( c · x2) + tan( x21)

Inserting 0/1 cos 



x1 · (x2 + 1) 



cos 



x1 · (c · x1 + c + x2)



Point eval x1 ·  x1 · x2 + x1 + 1  x1 ·  x1 · (x2 + 1) + 1 

Model performance is evaluated in four stages. First, we assess the raw skeleton output, equations where constants are replaced by variables, by simplifying both the predicted and true equations using Sympy (Meurer et al., 2017), without applying BFGS. Second, manual inspection displayed that the model often captures the correct structure but tends to over-generate constants; replacing these with zero or one and simplifying frequently recovers the correct formula. If the first two steps do not recover the correct formula, we perform pointwise evaluation by providing 200 inputs into both the true and predicted expressions to test for equivalence. Finally, we apply BFGS to optimise constants, although this step is slow, CPU-bound, and does not reflect the model’s actual generative capabilities. Examples are shown in Table 2. 

C.1. Performance on Equations With and Without Constants 

To evaluate model performance, we use two datasets: one without constants, where the model can recover the correct formula without BFGS, and one with floating-point constants, which often requires BFGS to fill in missing values. The beam size (32) and BFGS setup (10 restarts) follow the configuration from NeSymReS (Lample & Charton, 2019; Biggio et al., 2021). Due to BFGS’s computational cost, we evaluated 10 000 samples for the no-constant dataset and 1 000 for the constant dataset. Correct Incorrect Unable to recreate 

> 0
> 2000
> 4000
> 6000
> Count
> 61.3%
> 7.0%
> 31.8%
> Skeleton (57.5%)
> 0 Inserted (1.0%)
> 1 Inserted (1.4%)
> 0/1 Inserted (0.3%)
> Point Evaluation (0.8%)
> BFGS (0.4%)

(a) Correct Incorrect Unable to recreate  

> 0
> 100
> 200
> 300
> 400
> Count
> 42.9%
> 17.9%
> 39.2%
> Skeleton (17.1%)
> 0 Inserted (0.3%)
> 1 Inserted (0.0%)
> 0/1 Inserted (0.0%)
> Point Evaluation (0.9%)
> BFGS (24.6%)

(b) Figure 7. Comparison of model performance with beam size 32, showing distributions of correct, incorrect, and unrecreatable samples. The correct category is subdivided by reconstruction strategy. (a) 10 000 samples without constants. (b) 1 000 samples with constants. 

The results in Figure 7 display that a substantial portion of true formulas could not be recreated: 31 .8% without constants and 39 .2% with constants. This suggests that in many cases the decoder returned only the start token <S> or invalid prefix notation. Figure 7a shows that 61 .3% of the 10 000 formulas were correctly reconstructed, with strong skeleton performance. Substituting constants with zero or one yields a modest 2.7% improvement, reflecting the model’s tendency to overgenerate constants. 

> 6

Retrieved from: https://drive.google.com/drive/folders/1LTKUX-KhoUbW-WOx-ZJ8KitxK7Nov41G 

15 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

Figure 7b shows performance drops with floating-point constants: only 42 .9% of formulas are correctly recreated, and the error rate among incorrect outputs increases. BFGS accounts for over half of the final performance, as the model cannot generate most constants directly. These results indicate that while the model effectively recovers structural skeletons, its reliance on BFGS to generate constants reveals a gap between NeSymReS constant generation and the refining of BFGS. Since BFGS is a post-processing step and not part of the model, its contribution reflects an external correction rather than learned numerical understanding. Therefore for further experiments we will utilize the dataset without constants. 

C.2. Effect of Beam Size on Model Performance 

To assess the impact of beam search, the model was evaluated across beam sizes ranging from 20 to 25. Due to computational constraints, this experiment was conducted on 1 000 datapoints. The results are shown in Figure 8, with a more detailed view of the 25 beam size in Figure 7. 1 2 4 8 16     

> Number of beams used
> 0
> 200
> 400
> 600
> Count
> 549 569 625 625 627
> Skeleton
> 0 Inserted
> 1 Inserted
> 0/1 Inserted
> Point Evaluation
> BFGS

(a) 0 5 10 15 20 25 30  

> Beam index
> 0
> 10 1
> 10 2
> 10 3
> Frequency (log scale)

(b) Figure 8. Comparison of model performance across beam sizes without constants. Each bar is further subdivided based on reconstruction strategies. (a) Performance on 1 000 samples without floating-point constants. (b) Distribution of beam positions of correct solutions over 10 000 samples without constants, using beam size 32, with 6 125 correct solutions. 

Figure 8a shows performance improves with larger beam sizes, including better skeleton reconstruction, but the overall benefit of beam search remains small. This means that while larger beams let the model consider more candidate sequences, most improvements come from relatively small increases in beam size. The distribution plot in Figure 8b shows that with a beam size of 32, most correct solutions appear within the first three beams. This indicates the top-ranked predictions are highly reliable, and exploring beyond these beams seldom improves results. Almost no correct solutions are found beyond beam 15, demonstrating that increasing beam size further offers limited benefit while increasing computational cost. 

C.3. Model Difficulties 

In addition to overall performance, analysing the operation composition of formulas the model can and cannot recreate reveals further strengths and limitations. Figures 9a ( n = 6125 ) and 9b ( n = 3125 ) compare true equations successfully recreated versus those not recreated (CNR). Notably, the log operation appears only 56 times in the recreated set versus 1,676 times in the CNR set, highlighting a major challenge. The exp operator shows similar occurrence counts in both figures; however, since the CNR set is half the size, its relative frequency is higher in the unable to recreate dataset, posing another limitation. Comparing Figures 9a and 9c reveals similar operation distributions, however, recreated formulas contain relatively more multiplication and fewer x1 terms, suggesting the model tends to produce more concise expressions, as also observed in Figure 9d. The violin plot in Figure 9d illustrates equation length distributions: correctly recreated equations average 5.7 elements, while incorrect ones average 11.2 tokens, indicating significant overgeneration when the model is uncertain. Longer equations also pose more difficulty, reflected in higher mean lengths for the Incorrect and CNR subsets. 

C.4. Feynman AI Dataset 

The Feynman AI Dataset is a collection of mathematical expressions and physical laws used to benchmark SR algorithms (Udrescu & Tegmark, 2020). We use this dataset to evaluate real-world performance and compare our results to those 16 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models x_1 mul add x_2 pow x_3 cos sin tan exp abs log     

> Elements
> 0
> 2000
> 4000
> 6000
> 8000
> Frequency
> 9378
> 8062
> 7160
> 6028
> 4882
> 1933
> 1226 1212 1157 812
> 69 56

(a) x_1 mul pow add x_2 log 1/2 x_3 exp tan sin cos       

> Elements
> 0
> 1000
> 2000
> 3000
> 4000
> 5000
> Frequency
> 5029
> 3923 3640
> 3194
> 2765
> 1676
> 1353
> 705 703 475 439 411

(b) mul x_1 add x_2 pow x_3 cos sin tan exp abs log       

> Elements
> 0
> 2000
> 4000
> 6000
> Frequency
> 7374 7041
> 4969 4782
> 3757
> 2074
> 968 930 862 522
> 46 30

(c) Complete      

> Dataset
> CD ICD CR ICR CNR
> Category
> 0
> 10
> 20
> 30
> Length
> Mean

(d) Figure 9. Distribution of elements and mean equation lengths for the NeSymReS model tested on 10,000 equations without constants. (a) 

Elements in correctly recreated true equations ( n = 6 , 125 ); counts above 6, 125 indicate multiple occurrences per equation. (b) Elements in true equations the model failed to recreate (CNR, n = 3 , 175 ). (c) Elements in the model’s correctly recreated equations. (d) Equation length distributions for different subsets: (I)CD = (In)Correct Dataset, (I)CR = (In)Correctly Recreated, CNR = Could Not Recreate. 

reported in the original paper. Correct Incorrect Unable to recreate 

> 0
> 10
> 20
> 30
> Count
> 66.7%
> 33.3%
> 0.0%
> Skeleton (7.8%)
> 0 Inserted (5.9%)
> 1 Inserted (7.8%)
> 0/1 Inserted (0.0%)
> Point Evaluation (17.6%)
> BFGS (27.5%)

(a) Correct Incorrect Unable to recreate  

> 0
> 10
> 20
> 30
> Count
> 66.7%
> 33.3%
> 0.0%
> Skeleton (11.8%)
> 0 Inserted (5.9%)
> 1 Inserted (3.9%)
> 0/1 Inserted (3.9%)
> Point Evaluation (17.6%)
> BFGS (23.5%)

(b) Figure 10. Feynman AI Dataset, common benchmarking tool for SR. containing 100 physics based equations from which 51 are able to be used by this model. Others have too many variables. (a) Performance on 200 observations. (b) Performance on 1000 observations. 

The original study done by NeSymReS reports an accuracy between 65% and 75% on this dataset, and Figure 10 shows results within this margin (Biggio et al., 2021). For this final experiment, we tested one remaining hyperparameter: the number of observations required for the encoder to effectively learn the underlying formula. Using 200 observations, chosen empirically as a trade-off between performance and runtime, our results are shown in Figure 10a. The model correctly generates the skeleton for 7.8% of the 51 expressions. Replacing the model’s constants with zero or one improves this by 13 .7% , and applying the BFGS algorithm solves an additional 27 .5% . Increasing the number of observations to 1, 000 

(Figure 10b) slightly improves skeleton accuracy to 11 .8% , but does not enhance overall performance. This suggests that 

200 observations are sufficient for the encoder to learn the equation’s underlying structure. 17 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

## D. Additional Circuit Discovery Results 

Circuit configurations were selected based on practical constraints: for Add and Mul , mean patching produced trivial circuits of length zero and were excluded; for Pow and Exp , only mean patching was used due to limited compute and the prior discovery of correct circuits. All other operations include both mean and resample patching for comparison.  

> PATCHES

runs for 250 generations with a population size of 40 7. To promote a diverse starting population, access to the covariance matrix is disabled for the first 10 generations, and the global step size is initialised to 0.5.   

> Table 3. Model and Patched Model baseline scores, measured in top-kaccuracy (T1, T2, T3) and normalised logit score (LS) for R(esample) and M(ean) patching.

Op Patching Model ↑ Patched Model ↓

T1 T2 T3 LS T1 T2 T3 LS 

Add R 0.93 0.94 1.00 0.92 0.16 0.86 0.99 0.21 

Cos 

M 0.95 0.97 1.00 0.92 0.00 0.00 0.00 0.00 R 0.95 0.97 1.00 0.92 0.00 0.00 0.02 0.00 Sin 0.95 0.97 1.00 0.92 0.10 0.25 0.45 0.00 

Exp M 0.60 0.75 1.00 0.58 0.00 0.00 0.00 0.00 

Log 

M 0.31 0.63 1.00 0.31 0.00 0.00 0.12 0.00 R 0.31 0.63 1.00 0.31 0.00 0.06 0.54 0.00 Exp 0.31 0.63 1.00 0.31 0.00 0.20 0.33 0.00 

Mul R 0.87 0.98 1.00 0.87 0.15 0.95 1.00 0.19 

Pow M 0.92 0.96 1.00 0.91 0.00 0.00 1.00 0.00 

Sin 

M 0.73 0.83 1.00 0.70 0.00 0.00 0.00 0.00 R 0.73 0.83 1.00 0.70 0.03 0.12 0.24 0.04 Cos 0.73 0.83 1.00 0.70 0.00 0.50 0.70 0.03 

Tan 

M 0.50 0.59 1.00 0.47 0.00 0.00 0.00 0.00 R 0.50 0.59 1.00 0.47 0.00 0.03 0.08 0.01 Sin 0.50 0.59 1.00 0.47 0.10 0.30 0.42 0.00 

> 7This setting was selected as the best trade-off between computational efficiency and circuit quality during preliminary testing.

18 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

Table 4. Faithfulness and completeness test scores, measured by top-k accuracy (T1–T3) and normalised Logit Score (LS). CFG indicates patching/evaluation strategies: Mean (M), Resample (R), or CoT patching (operation specified in functional evaluation). Evaluation types: Model (M), Functional (F). BL provides baselines for the operator. Circuit Length (CL) should be minimised ( ↓); faithfulness maximised (↑); completeness minimised ( ↓). #C indicates the number of circuits found: 1 denotes a unique circuit; values greater than 1 indicate a circuit class. Correct denotes whether the circuit reproduces the behaviour under the evaluation strategy. Green: above full model performance, red: below threshold. 

Op CFG CL ↓ Faithful ↑ Complete ↓ Correct BL T1 T2 T3 LS T3 LS 

Add 

> T1: 0.93 T2: 0.94 LS: 0.92

RF 57 0.85 0.94 0.99 0.72 0.99 0.23 é

RM 67 0.92 0.94 1.00 0.92 0.99 0.21 é

Cos 

> T1: 0.95 T2: 0.97 LS: 0.91

MF 61 0.87 0.92 0.99 0.49 0.00 0.00 Ë

MM 77 0.98 0.99 0.99 0.86 0.00 0.00 Ë

RF 77 0.79 0.94 0.95 0.51 0.00 0.00 é

RM 81 0.95 0.97 0.97 0.81 0.00 0.00 Ë

Sin-F 79 0.82 0.93 0.95 0.62 0.45 0.04 é

Exp 

> T1: 0.60 T2: 0.75 LS: 0.58

MF 58 0.52 0.84 0.90 0.38 0.00 0.00 Ë

MM 61 0.54 0.68 0.73 0.45 0.00 0.00 é

Log 

> T1: 0.30 T2: 0.62 LS: 0.31

MF 50 0.66 1.00 1.00 0.44 0.15 0.01 Ë

MM 47 0.00 0.96 1.00 0.23 0.12 0.00 Ë

RF 84 0.26 0.58 0.90 0.28 0.52 0.02 é

RM 47 0.23 0.41 0.79 0.21 0.53 0.05 é

Exp-F 70 0.24 0.60 0.92 0.30 0.45 0.63 é

Mul 

> T1: 0.86 T2: 0.97 LS: 0.87

RF 61 0.70 0.94 0.99 0.59 1.00 0.19 é

RM 65 0.78 0.94 0.99 0.72 1.00 0.18 é

Pow 

> T1: 0.92 T2: 0.95 LS: 0.91

MF 88 0.86 0.90 0.95 0.58 0.00 0.00 Ë

MM 91 0.89 0.96 0.97 0.82 0.10 0.00 Ë

Sin 

> T1: 0.73 T2: 0.83 LS: 0.70

MF 57 0.70 0.79 1.00 0.51 0.00 0.00 Ë

MM 52 0.79 0.83 1.00 0.63 0.00 0.00 Ë

RF 63 0.65 0.81 0.87 0.59 0.28 0.04 é

RM 62 0.65 0.80 0.86 0.60 0.24 0.04 Ë

Cos-F 69 0.55 0.74 0.91 0.39 0.70 0.03 é

Tan 

> T1: 0.50 T2: 0.58 LS: 0.47

MF 65 0.38 0.60 0.92 0.29 0.00 0.00 Ë

MM 66 0.41 0.65 0.75 0.38 0.00 0.00 Ë

RF 76 0.36 0.60 0.87 0.30 0.08 0.01 é

RM 63 0.35 0.54 0.71 0.33 0.08 0.01 é

Sin-F 79 0.38 0.53 0.88 0.30 0.23 0.02 é

19 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models add-RF add-RM cos-MF cos-MM 

cos-RF cos-RM cos-RF-CoT exp-MF 

exp-MM log-MF log-MM log-RF 

log-RM log-RF-CoT mul-RF mul-RM 

pow-MF pow-MM sin-MF sin-MM 

sin-RF sin-RM sin-RF-CoT tan-MF 

tan-MM tan-RF tan-RM tan-RF-CoT 

Figure 11. Circuits across all methods and operations. While usage patterns vary across components, no clear or consistent correlations emerge between component usage and specific operations or discovery methods. This suggests that circuit composition is not tightly tied to the type of symbolic operation or the patching strategy used. Vertical: layers, Horizontal MLP + heads 1-8. 

20 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

Table 5. Faithfulness and completeness train scores, Same structure as Table 4. 

Op CFG CL ↓ Faithful ↑ Complete ↓ Correct BL T1 T2 T3 LS T3 LS 

Add 

> T1: 0.91 T2: 0.93 LS: 0.91

RF 57 0.84 0.90 0.99 0.70 0.95 0.19 é

RM 67 0.88 0.94 0.99 0.87 0.95 0.17 é

Cos 

> T1: 0.92 T2: 0.93 LS: 0.92

MF 61 0.87 0.92 0.99 0.48 0.00 0.00 Ë

MM 77 0.97 0.97 0.98 0.82 0.00 0.00 Ë

RF 77 0.84 0.94 0.95 0.53 0.01 0.00 Ë

RM 81 0.94 0.96 0.96 0.82 0.01 0.00 Ë

Sin-F 79 0.85 0.94 0.96 0.61 0.53 0.04 é

Exp 

> T1: 0.59 T2: 0.74 LS: 0.59

MF 58 0.53 0.83 0.90 0.38 0.00 0.00 Ë

MM 61 0.61 0.73 0.75 0.49 0.00 0.00 Ë

Log 

> T1: 0.30 T2: 0.62 LS: 0.31

MF 50 0.61 1.00 1.00 0.44 0.15 0.00 Ë

MM 47 0.00 0.96 1.00 0.23 0.05 0.00 Ë

RF 84 0.29 0.55 0.90 0.30 0.46 0.00 Ë

RM 47 0.24 0.35 0.74 0.21 0.44 0.00 Ë

Sin-F 70 0.32 0.55 0.90 0.23 0.42 0.00 é

Mul 

> T1: 0.86 T2: 0.97 LS: 0.87

RF 61 0.76 0.95 1.00 0.60 1.00 0.24 é

RM 65 0.83 0.96 1.00 0.77 1.00 0.24 é

Pow 

> T1: 0.92 T2: 0.95 LS: 0.91

MF 88 0.84 0.92 0.95 0.57 0.00 0.00 é

MM 91 0.89 0.98 0.98 0.81 0.10 0.00 é

Sin 

> T1: 0.65 T2: 0.78 LS: 0.62

MF 57 0.63 0.78 1.00 0.48 0.00 0.00 Ë

MM 52 0.76 0.80 1.00 0.60 0.00 0.00 Ë

RF 63 0.63 0.80 0.90 0.57 0.21 0.03 Ë

RM 62 0.66 0.78 0.82 0.59 0.17 0.03 Ë

Cos-F 69 0.59 0.67 0.90 0.50 0.65 0.07 é

Tan 

> T1: 0.50 T2: 0.58 LS: 0.47

MF 65 0.40 0.59 0.92 0.29 0.00 0.00 Ë

MM 66 0.43 0.64 0.75 0.39 0.00 0.00 Ë

RF 76 0.40 0.61 0.90 0.32 0.12 0.02 Ë

RM 63 0.41 0.55 0.66 0.37 0.12 0.02 Ë

Sin-F 79 0.45 0.50 0.91 0.21 0.25 0.00 é

## E. Multi-Token Circuits 

Table 6 displays baseline, train, and test performance on two multitoken circuits namely monomial and posynomial function classes. We chose to focus on the mean functional configuration due to limited compute and mean patching requiring fewer resources, with functional evaluation typically leading to better end-user performance. 

E.1. Monomial and Posynomial Function Classes 

In SR, some methods constrain the search space to specific function classes to improve tractability or reflect domain knowledge. Two such classes are monomial and posynomial functions, which are particularly relevant in domains such as engineering, physics, and control systems, where functional relationships often follow power laws or multiplicative structures (Boyd et al., 2007; Udrescu & Tegmark, 2020). therefore we choose to find circuits for these function classes. A monomial function is defined as: 

f (x) = c · xa1 

> 1

xa2 

> 2

· · · xan 

> n

,

where c > 0 and ai ∈ R. These functions are positive for all xi > 0.21 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

For instance, in physics education, the period T of a pendulum is commonly expressed as a monomial: T = 2 π

q Lg , where 

L is the length of the pendulum and g is the gravitational constant (OpenStax, 2023). This expression fits the monomial form, as it consists of a constant ( 2π) multiplied by variables ( L/g ) raised to a power ( 12 ). If prior knowledge suggests such a relationship, restricting the model to monomials can lead to faster and more accurate discovery. A posynomial function generalizes this form: 

f (x) = 

> K

X

> k=1

ck · xa1k 

> 1

xa2k 

> 2

· · · xank  

> n

,

where ck > 0 and aik ∈ R. Posynomials are essentially weighted sums of monomials and preserve the interpretability of individual terms while capturing more complex relationships. Posynomial functions, in contrast, are useful when modelling more complex systems that involve sums of monomials. For instance, in designing battery-powered drones, engineers may want to model flight time as a function of several factors like battery capacity, weight, and motor efficiency. Each of these factors contribute multiplicatively to the total, and their combined effect may be expressed as a sum of monomials (Tyto Robotics, 2023). 

E.2. Results  

> Table 6. Evaluation scores for Monomial and Posynomial circuits on MF configuration

Func Class Eval CL ↓ Faithful ↑ Complete ↓ Correct 

T1 T2 T3 LS T3 LS 

Monomial 

Baseline 69 0.79 0.88 1.00 0.87 0.15 0.07 

ËTrain 0.69 0.85 0.95 0.56 0.00 0.00 Test 0.82 0.89 1.00 0.60 0.09 0.05 

Posynomial 

Baseline 55 0.66 1.00 1.00 0.65 0.05 0.00 

éTrain 0.56 0.96 1.00 0.47 0.00 0.00 Test 0.55 0.93 1.00 0.60 0.09 0.05 The results displayed in Table 6 illustrate a correct circuit for monomial, even outperforming train and baseline performance on the test set for Top-1 and Top-2 accuracy. The circuit is not unique where we can replace 2 heads with 3 other heads thus having a circuit class of 5 (one combination gives an incorrect circuit) circuits. The posynomial circuit is incorrect as it has a 1% too low top-1 accuracy but is complete and unique. We are thus able to find circuits of not only a single component, but extent it to multi-token circuits. 

## F. Additional Verification Experiments 

F.1. Component Usage 

As an additional verification experiment that circuits are not merely subsets of one another but rely on distinct components, Figure 12 shows how many circuits use each model component, including MLPs and attention heads across all layers. One component appears in every circuit: the output (OUT) MLP, highlighted with yellow borders. This is expected, as the OUT MLP maps the final internal representation to the model’s output token distribution and is thus always involved in generating the predicted behaviour. A notable trend is the higher utilisation of the second Multi-Head Attention Block (MAB2) compared to the first (MAB1) across layers. This can be attributed to the presence of a residual connection over the first MAB, which allows the model to bypass its outputs entirely, whereas no such residual exists over the second within a layer, only over the entire ISAB (see Figure 6). Interestingly, there is no clear preference for either early or late layers; component usage appears distributed relatively evenly across the encoder. This suggests that the model leverages information from various levels of abstraction throughout. In addition, there is no apparent correlation between function class and component usage; circuits for sin , cos , and tan 

do not more closely resemble each other than they do circuits for add , mul , or pow (see Figure 11). The output projection 22 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models MLP H1 H2 H3 H4 H5 H6 H7 H8 

Attention Heads 

> L1.1
> L1.2
> L2.1
> L2.2
> L3.1
> L3.2
> L4.1
> L4.2
> L5.1
> L5.2
> L6.1
> L6.2
> OUT
> Layers
> 5
> 10
> 15
> 20
> 25
> # Circuits

Figure 12. Usage of components in all circuits. Darker blue means more circuits use this component. Only the OUT MLP is used by all found circuits meaning all components have their own 

layer (OUT) is frequently used, which is expected as it maps internal representations back into the vocabulary embedding space for token generation. 

F.2. Overlap Percentages cos MF 

> cos MM
> exp MF
> exp MM
> log MF
> log MM
> pow MF
> pow MM
> sin MF
> sin MM
> tan MF
> tan MM
> Functions
> cos MF
> cos MM
> exp MF
> exp MM
> log MF
> log MM
> pow MF
> pow MM
> sin MF
> sin MM
> tan MF
> tan MM
> Functions
> 40
> 50
> 60
> 70
> 80
> Overlap Percentage (%)

(a) add RF  

> cos MF
> cos RF
> cos RFCTR
> exp MF
> log MF
> log RF
> log RFCTR
> mul RF
> pow MF
> sin MF
> sin RF
> sin RFCTR
> tan MF
> tan RF
> tan RFCTR
> Functions
> add RF
> cos MF
> cos RF
> cos RFCTR
> exp MF
> log MF
> log RF
> log RFCTR
> mul RF
> pow MF
> sin MF
> sin RF
> sin RFCTR
> tan MF
> tan RF
> tan RFCTR
> Functions
> 40
> 50
> 60
> 70
> 80
> Overlap Percentage (%)

(b) Figure 13. Overlap matrices comparing circuit similarity across methods. Each cell reports the maximum overlap percentage between two circuits; diagonal entries indicate circuit lengths (number of components). (a) Overlap between model-faithful (MM) and functionally-faithful (MF) circuits using mean patching. (b) Overlap between mean-patched (MF) and resample-patched (RF) circuits, both evaluated for functional faithfulness. 

To confirm that real circuits are discovered, not just active components, Figure 13a displays the component overlap between model- and functionally evaluated circuits. Circuits for the same operation share most components, confirming that different operations yield distinct circuits. Log circuits show little overlap with others, likely due to weak model performance and the distinct structure of log operations. There is no consistent similarity pattern across strategies; model- and functionally faithful circuits do not cluster more closely with their own type Figure 13b displays the overlap in circuits between resample and mean patching strategies, showing overlap within circuits of the same operation are highest. Additionally, circuits derived from resample patching exhibit greater overlap than those from mean patching, likely due to their increased size. A complete overlap matrix can be found in Figure 14. 23 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models add RF 

> add RM
> cos MF
> cos MM
> cos RF
> cos RM
> cos RFSTR
> exp MF
> exp MM
> log MF
> log MM
> log RF
> log RM
> log RFSTR
> mul RF
> mul RM
> pow MF
> pow MM
> sin MF
> sin MM
> sin RF
> sin RM
> sin RFSTR
> tan MF
> tan MM
> tan RF
> tan RM
> tan RFSTR

Functions 

add RF 

add RM 

cos MF 

cos MM 

cos RF 

cos RM 

cos RFSTR 

exp MF 

exp MM 

log MF 

log MM 

log RF 

log RM 

log RFSTR 

mul RF 

mul RM 

pow MF 

pow MM 

sin MF 

sin MM 

sin RF 

sin RM 

sin RFSTR 

tan MF 

tan MM 

tan RF 

tan RM 

tan RFSTR 

> Functions

30 

40 

50 

60 

70 

80 

> Overlap Percentage (%)

Figure 14. Overlap matrix comparing circuit similarity across all methods. Each cell reports the maximum overlap percentage between two circuits; diagonal entries indicate circuit lengths (number of components) 

24 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

F.3. Recovery Scores 

To further assess the circuits, we test whether they can outperform the full model in reproducing specific behaviours. To assess this, we used an experimental setting where the baseline model fails to accurately recreate the TgT. The complement of each circuit was patched out, leaving only the circuit intact, and top-3 accuracy was measured on a 100-example test set. Results are shown in Figure 15. cos-MF   

> cos-MM
> exp-MF
> exp-MM
> log-MF
> log-MM
> pow-MF
> pow-MM
> sin-MF
> sin-MM
> tan-MF
> tan-MM
> Configuration
> 0.00
> 0.05
> 0.10
> 0.15
> 0.20
> 0.25
> Improvement
> 5.1%
> 2.4%
> 10.7% 8.6%
> 23.2%
> 15.9%
> 0.0% 0.0%
> 13.3%
> 11.1%
> 8.5%
> 1.3%
> Top-1
> Top-2
> Top-3

Figure 15. Improvement score of each circuit over the full model. For 100 samples where the full model failed to predict the correct behaviour in the top-3, the circuit model was tested. 

Improvement scores vary from zero to 23.2%. We show moderate improvements for cos and tan operators and no improvement for pow . As observed in Section C.3 these operators already perform well with pow performing best. For 

log , exp , and sin we display higher performance increases also corresponding to weaker performing operators as seen in Figure 9. Addition and multiplication operators are not displayed because they observe the same trends as pow .These improvements provide further evidence that the circuits accurately capture the intended behaviour. The fact that performance improves when the rest of the model is ablated supports the hypothesis that the retained subcomponents are not only sufficient but also more specialised and less confused by unrelated computation. This behaviour resembles that of a mixture of experts model (MoE), in which different submodules specialise in different tasks. However, unlike traditional MoE systems where routing is learned explicitly, here the specialisation emerges implicitly, and is revealed through circuit extraction. This suggests that even within densely connected models, modular and interpretable substructures are recoverable. 25 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

## G. Iterative Patching: Additional Results 

Table 7. Faithfulness and completeness test scores for Iterative Patching. 

Op CFG CL ↓ Faithful ↑ Complete ↓ Correct BL T1 T2 T3 LS T3 LS 

Add 

> T1: 0.93 T2: 0.94 LS: 0.92

RF 87 0.87 0.93 1.00 0.73 0.99 0.19 é

RM 94 0.87 0.93 1.00 0.83 0.99 0.19 é

Cos 

> T1: 0.95 T2: 0.97 LS: 0.91

MF 75 0.87 0.94 0.98 0.46 0.00 0.00 Ë

MM 87 0.97 0.98 0.99 0.812 0.00 0.00 Ë

RF 90 0.85 0.95 0.96 0.63 0.02 0.00 Ë

RM 103 0.89 0.97 0.97 0.81 0.02 0.00 é

Exp 

> T1: 0.60 T2: 0.75 LS: 0.58

MF 75 0.52 0.64 0.86 0.40 0.00 0.00 é

MM 86 0.55 0.63 0.72 0.47 0.00 0.00 é

Log 

> T1: 0.30 T2: 0.62 LS: 0.31

MF 87 0.24 0.47 0.94 0.22 0.00 0.00 é

MM 76 0.24 0.30 0.95 0.22 0.12 0.00 Ë

RF 86 0.24 0.57 0.86 0.23 0.46 0.00 é

RM 77 0.23 0.44 0.80 0.21 0.53 0.00 é

Mul 

> T1: 0.86 T2: 0.97 LS: 0.87

RF 92 0.73 0.96 0.99 0.66 1.00 0.19 é

RM 99 0.79 0.97 1.00 0.74 1.00 0.19 é

Pow 

> T1: 0.92 T2: 0.95 LS: 0.91

MF 99 0.87 0.93 0.96 0.49 1.00 0.00 é

MM 110 0.90 0.96 0.99 0.83 1.00 0.00 é

Sin 

> T1: 0.73 T2: 0.83 LS: 0.70

MF 56 0.68 0.80 0.90 0.52 0.00 0.00 é

MM 60 0.70 0.86 0.87 0.64 0.00 0.00 Ë

RF 72 0.70 0.82 0.90 0.58 0.19 0.02 é

RM 72 0.67 0.80 0.87 0.59 0.24 0.03 é

Tan 

> T1: 0.50 T2: 0.58 LS: 0.47

MF 79 0.45 0.60 0.90 0.30 0.00 0.00 é

MM 66 0.48 0.69 0.87 0.36 0.00 0.00 é

RF 80 0.44 0.64 0.88 0.33 0.08 0.01 é

RM 91 0.37 0.58 0.72 0.34 0.08 0.01 é

26 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

## H. Probing: Additional Results and Hyperparameters Observations   

> Feed Forward
> Head 1
> Head 2
> Head 8
> 192 X 50
> 192 X 50 192
> 192 X N 192 X 50 X N
> 192 10 1
> 192 X 50
> Cache Activations

Transformer Layer Probing Classifier 

Figure 16. Probing process overview. Activations from a selected attention head (head 2) are cached (green arrow), mean-pooled over the induction dimension, and passed to a feedforward neural network (FFNN) to predict whether the equation contains the target element. The left panel shows a simplified transformer layer with residual flow (dashed arrow), and the right panel shows the probing classifier. 

Our probing model P is implemented as a multilayer perceptron (MLP) with a single linear layer followed by a ReLu activation. The 64-dimensional input is projected to a hidden layer of 10 units and then to a single scalar output constrained with a sigmoid to be between [0 , 1] . Each attention head in the target model outputs activations of shape (192 , 50) ,corresponding to 50 induction points per input and 64-dimensional representations per operation. To prepare the input xp for the probe P (xp), we aggregate these token-level activations into a single vector of shape (192 , ) by applying mean pooling across the induction dimension (see Figure 16). The model is trained on a balanced dataset of 1 000 samples, with a stratified split of 70% training, 10% validation, and 20% test data. We use a batch size of 32, a learning rate of 1 × 10 −4, and train for 200 epochs using the Adam optimizer and binary cross-entropy loss. The best-performing model on the validation set is selected for evaluation. Given that training takes approximately 12 seconds, no early stopping or patience scheduling was necessary. To ensure that the probing classifier had enough data in training (without overfitting) we provide an exemplary loss and accuracy curve in Figure 17. The smooth and consistent trend in both curves suggests stable learning dynamics, with no visible signs of overfitting (e.g., training accuracy improving while validation accuracy degrades) or underfitting (e.g., both loss and accuracy stagnating at poor values). 0 50 100 150 200 

Epoch 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

> Loss

## Loss 

0 50 100 150 200 

Epoch 

40 

50 

60 

70 

80 

90 

100 

> Accuracy

## Accuracy 

Val Acc 

Train Acc 

Figure 17. Loss and accuracy curves of the probing classifier trained for 200 epochs. 

27 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models 

Table 8. Accuracy and statistical comparison between Circuit and Complement across different operations and setups. Bold are the higher mean accuracy scores out of every setup, green indicates a statistically significant difference between both. 

Operation Setup Circuit Complement P-value ↓

Mean ↑ Std ↓ Mean ↑ Std ↓

Add RF 0.95 0.05 0.90 0.09 0.208 RM 0.94 0.05 0.90 0.08 0.236 

Cos 

MF 0.98 0.01 0.97 0.01 0.402 MM 0.97 0.01 0.97 0.01 0.541 RF 0.97 0.01 0.97 0.01 0.764 RM 0.97 0.01 0.97 0.01 0.495 Sin 0.97 0.01 0.97 0.01 0.532 

Exp MF 0.96 0.02 0.95 0.02 0.186 MM 0.96 0.02 0.95 0.02 0.617 

Log 

MF 0.96 0.02 0.88 0.08 0.015 MM 0.96 0.02 0.88 0.09 0.029 RF 0.93 0.06 0.90 0.09 0.404 RM 0.96 0.02 0.90 0.08 0.051 Exp 0.90 0.07 0.90 0.08 0.938 

Mul RF 0.92 0.03 0.90 0.04 0.206 RM 0.91 0.04 0.89 0.04 0.189 

Pow MF 0.98 0.02 0.98 0.01 0.482 MM 0.98 0.02 0.98 0.01 0.628 

Sin 

MF 0.94 0.03 0.93 0.03 0.661 MM 0.94 0.03 0.93 0.03 0.565 RF 0.93 0.03 0.93 0.03 0.878 RM 0.93 0.03 0.94 0.03 0.597 Cos 0.93 0.03 0.94 0.03 0.766 

Tan 

MF 0.92 0.06 0.94 0.05 0.569 MM 0.95 0.03 0.90 0.06 0.067 RF 0.93 0.04 0.92 0.06 0.580 RM 0.95 0.03 0.90 0.06 0.061 Sin 0.93 0.04 0.92 0.06 0.580 

## I. Direct Logit Attribution Additional Results 

To assess whether the findings in Figure 5 generalise across different patching types and operators, we report additional results in Figure 18. These results confirm that the observed pattern is consistent: circuits constructed using direct logit attribution are larger than those identified using PATCHES or iterative patching techniques. This procedure is applied to 100 training samples for the different configurations to provide examples of both patching methods. We observe the same trends across operators, indicating that the tendency of direct logit attribution to produce overly large circuits is not specific to a particular setting. Consequently, irrespective of the operator or patching strategy employed, direct logit attribution appears to be a less suitable method for circuit discovery, as it systematically includes substantially more components than necessary. Moreover, it is unclear how an appropriate cut-off for such circuits should be defined. In this paper, we apply the same thresholding procedure used for PATCHES circuits to enable a fair comparison; however, this choice remains inherently arbitrary, and alternative thresholds could lead to markedly different circuit sizes. 28 Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models MLP H1 H2 H3 H4 H5 H6 H7 H8 

Components 

L1.1 

L1.2 

L2.1 

L2.2 

L3.1 

L3.2 

L4.1 

L4.2 

L5.1 

L5.2 

L6.1 

L6.2 

OUT 

> Layers

0.1 

0.2 

0.3 

0.4 

0.5 

> Logit Score

(a) 0 10 20 30 40 50 60 70 80 90 100 110 

Number of Heads Included 

0.0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

> Logit Score

0.0 

0.2 

0.4 

0.6 

0.8 

1.0  

> Top-k Accuracy
> Top-1
> Top-2
> Top-3
> Logit Score

(b) MLP H1 H2 H3 H4 H5 H6 H7 H8 

Components 

L1.1 

L1.2 

L2.1 

L2.2 

L3.1 

L3.2 

L4.1 

L4.2 

L5.1 

L5.2 

L6.1 

L6.2 

OUT 

> Layers

0.05 

0.10 

0.15 

0.20 

0.25 

0.30 

0.35 

> Logit Score

(c) 0 10 20 30 40 50 60 70 80 90 100 110 

Number of Heads Included 

0.0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

> Logit Score

0.0 

0.2 

0.4 

0.6 

0.8 

1.0  

> Top-k Accuracy
> Top-1
> Top-2
> Top-3
> Logit Score

(d) MLP H1 H2 H3 H4 H5 H6 H7 H8 

Components 

L1.1 

L1.2 

L2.1 

L2.2 

L3.1 

L3.2 

L4.1 

L4.2 

L5.1 

L5.2 

L6.1 

L6.2 

OUT 

> Layers

0.05 

0.10 

0.15 

0.20 

0.25 

0.30 

> Logit Score

(e) 0 10 20 30 40 50 60 70 80 90 100 110 

Number of Heads Included 

0.0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

> Logit Score

0.0 

0.2 

0.4 

0.6 

0.8 

1.0  

> Top-k Accuracy
> Top-1
> Top-2
> Top-3
> Logit Score

(f) Figure 18. Direct attribution and faithfulness evaluation Additional Results . (a,c,e) Change in logit score when patching individual heads, averaged over 100 samples. (b, d, e) Faithfulness evaluation of importance ranking; thresholds (dashed lines) from Table 3. (a, b) 

Sin Resample; (c, d) Exp Mean, (e, f) Exp Resample. 

29