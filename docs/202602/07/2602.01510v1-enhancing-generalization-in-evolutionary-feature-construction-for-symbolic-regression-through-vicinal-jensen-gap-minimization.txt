Title: Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization

URL Source: https://arxiv.org/pdf/2602.01510v1

Published Time: Tue, 03 Feb 2026 03:08:55 GMT

Number of Pages: 23

Markdown Content:
> JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1

# Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization 

Hengzhe Zhang, Qi Chen, Member, IEEE, Bing Xue, Fellow, IEEE, Wolfgang Banzhaf, Member, IEEE, Mengjie Zhang, Fellow, IEEE 

Abstract ‚ÄîGenetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term‚Äîeither finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Further-more, to mitigate manifold intrusion‚Äîwhere data augmentation may generate unrealistic samples that fall outside the data manifold‚Äîwe propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity mea-sures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance. 

Index Terms ‚ÄîGenetic programming, symbolic regression, evo-lutionary feature construction, overfitting, vicinal risk minimiza-tion 

I. I NTRODUCTION 

Automated feature construction is a popular technique in the domain of automated machine learning (AutoML) [1]. For a given dataset (X, Y ), automated feature construction aims to enhance the learning performance of a machine learning algorithm A by constructing a set of high-level features 

œï1, . . . , œï m from the original (low-level) feature space X.Unlike deep learning, which relies on numerous parameters to construct features, GP-based feature construction focuses on creating symbolic models to construct features in function space to improve the performance of learning algorithms. 

H. Zhang, Q. Chen, B. Xue and M. Zhang are with the Centre for Data Sci-ence and Artificial Intelligence & School of Engineering and Computer Sci-ence, Victoria University of Wellington, PO Box 600, Wellington 6140, New Zealand (e-mails: hengzhe.zhang@ecs.vuw.ac.nz; qi.chen@ecs.vuw.ac.nz; bing.xue@ecs.vuw.ac.nz; and mengjie.zhang@ecs.vuw.ac.nz). W. Banzhaf is with the Department of Computer Science and Engineer-ing, Michigan State University, East Lansing, MI 48824, USA (e-mails: banzhafw@msu.edu). This work was supported in part by the Marsden Fund of New Zealand Government under Contract VUW1913, and Contract VUW2016; in part by the Science for Technological Innovation Challenge (SfTI) Fund under Grant E3603/2903; in part by the MBIE Data Science SSIF Fund under Contract RTVU1914; and in part by the MBIE Endeavor Research Programme under Contract C11X2001 and Contract UOCX2104. 

Genetic Programming (GP), an evolutionary computation tech-nique, provides key advantages such as interpretable repre-sentations, gradient-free search, and global optimization ca-pabilities. These strengths make GP particularly effective for feature construction in non-differentiable symbolic spaces and for optimizing non-differentiable loss functions. However, a significant challenge with modern evolutionary feature con-struction methods, including GP, is overfitting. While these methods can achieve impressive performance on training sets, the features they construct might not generalize well to un-seen data, mirroring the challenges faced by many automatic feature construction methods, such as deep neural networks and kernel methods. The issue of overfitting in GP has been widely studied. A well-recognized approach to controlling model complexity is by regulating model size [2]. However, growing evidence suggests that model size alone is insufficient to fully characterize model complexity [3]. For example, using model size as a complexity measure cannot differen-tiate the complexity between x1 √ó x2 and sin(sin( x)) . Thus, incorporating semantic considerations is crucial. Building on this insight, several studies in GP address overfitting from the perspective of statistical machine learning theory [4, 5]. Nonetheless, traditional metrics such as VC-dimension [4] or Rademacher complexity [5] often fall short in accurately estimating generalization performance when applied to deep-learning-based feature construction techniques. With GP being recognized as a data-efficient deep learning technique [6, 7], it is essential to explore more effective methods for control-ling overfitting. In the realm of deep learning, vicinal risk minimization (VRM) [8] has recently garnered significant success. The key idea of VRM is to not only minimize the training loss on the original training samples but also on the vicinal samples surrounding each training instance, which are synthesized through data augmentation methods in practice [9]. The intuition behind VRM is that by minimizing the empirical loss around the neighborhood N (xi) of each training instance xi, the model can maintain its behavior when test data slightly deviates from the training data. Formally, for a loss function L, VRM optimizes the following objective function: 

V(f ) = 1

n

> n

X

> i=1

Z

L(f (x), y ) dP xi (x), (1) where x denotes a vicinal example that is a nearby datapoint of a training example xi on the data manifold, and Pxi (x) 

> arXiv:2602.01510v1 [cs.LG] 2 Feb 2026 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2

represents the probability distribution of vicinal examples x

in the neighborhood of xi. Intuitively, vicinal examples x that are further from the training data xi have lower probabilities 

Pxi (x). In deep learning, vicinal risk has been widely adopted as an optimization objective to mitigate overfitting. However, traditional VRM does not explicitly disentangle the empirical loss and the regularization term from vicinal risk, instead optimizing both jointly by minimizing the empirical loss on synthesized samples from the neighborhoods of the original training data [9]. To adapt VRM for evolutionary feature construction, this paper proposes decomposing the vicinal risk 

V(f ) into an empirical loss component and a regularization term to more effectively control overfitting, motivated by several key considerations:  

> ‚Ä¢

Flexible Trade-off between Accuracy and Smoothness: 

Decomposing vicinal risk into an empirical loss compo-nent, 1

> n

Pni=1 

R L (f (xi), y i) dP (x), and a regularization term allows for fine-grained control over the balance between training loss and model smoothness. This flex-ible balance enables a more nuanced trade-off between empirical loss and regularization.  

> ‚Ä¢

Accurate Training Error: In Eq. (1), the integral in 

V(f ) is typically estimated using samples from the neigh-borhood N (xi), which can lead to inaccuracies due to limited sampling. However, empirical loss can be com-puted accurately on the original training data. Decompos-ing vicinal risk helps mitigate these inaccuracies by en-suring that the empirical loss, i.e., 1

> n

Pni=1 L (f (xi), y i),is accurately calculated.  

> ‚Ä¢

Optimizing Validation Loss: Traditional vicinal loss entails empirical loss [9]. However, if we intend to replace empirical loss with alternative loss functions, such as cross-validation loss‚Äîwhich is widely used in feature construction and AutoML‚Äîthe decomposition of the vicinal loss becomes necessary. To decompose vicinal risk, this paper proves that vicinal risk is bounded by the sum of two terms: empirical loss and either a finite difference or a vicinal Jensen gap. Specifically, if vicinal examples are synthesized via noise perturbation then a finite difference is the regularization term. If synthesized using the mixup method [9], the vicinal Jensen gap becomes the regularization term. Mixup is a data augmentation technique that linearly interpolates both the inputs and the labels of pairs of examples. After decomposition, the empirical loss term, represented by cross-validation loss, can be accurately calculated, while the finite difference or vicinal Jensen gap serves as a regularization objective to control overfitting. These two objectives can be optimized using a weighted-sum objective function. However, different datasets with varying noise levels may require different regularization strengths. To address this, we propose a noise estimation strategy to adap-tively adjust the regularization strength based on the estimated noise level in the dataset. This adaptive approach highlights a key benefit of vicinal risk decomposition: it allows flexible control over model complexity without imposing excessive regularization that could hinder model fitting. In summary, the overall goal of this paper is to enhance generalization in evolutionary feature construction for symbolic regression by decomposing vicinal risk into empirical loss and regulariza-tion terms. This decomposition enables effective overfitting control by flexibly balancing model accuracy and smoothness. The objectives/contributions of this paper are summarized as follows:  

> ‚Ä¢

A regularized GP is proposed to optimize both cross-validation loss and either the vicinal Jensen gap or the finite difference to control the overfitting in evolutionary feature construction algorithms 1. Additionally, a noise estimation strategy is proposed to adjust the regularization strength adaptively.  

> ‚Ä¢

An upper bound on perturbation-based vicinal risk min-imization is derived, demonstrating that perturbation-based vicinal risk minimization can be decomposed into empirical risk and finite difference.  

> ‚Ä¢

An upper bound on mixup-based vicinal risk minimiza-tion is established, showing that mixup-based vicinal risk minimization can be decomposed into empirical risk and the vicinal Jensen gap.  

> ‚Ä¢

To alleviate the discrepancy between synthesized and real data, a manifold intrusion detection strategy is proposed, thereby avoiding the issue of Jensen gap minimization overly penalizing effective models. The remainder of this paper is organized as follows. Sec-tion II reviews existing techniques for overfitting control and evolutionary feature construction. Section III decomposes the vicinal risk into empirical loss and regularization terms, laying the foundation for the proposed algorithm. Section IV describes the proposed algorithm in detail. Section V presents the experimental settings, followed by the experimental results in Section VI and additional analysis in Section VII. Finally, Section VIII summarizes the paper and suggests potential directions for future research. II. R ELATED WORK 

A. Vicinal Risk Minimization 1) Vicinal Risk Minimization in Deep Learning: Vicinal risk minimization is a well-established technique that has achieved significant success in deep learning, particularly in computer vision tasks. Vicinal examples can be synthesized under various assumptions, such as randomly masking out a patch of an image (CutOut) [10], blending two images (MixUp) [9], or combining an image with its augmented version (AugMix) [11]. However, most research on vicinal risk minimization has focused on image classification problems, while its application to tabular data and regression tasks remains limited [12]. 

2) Vicinal Risk Minimization in GP: In the context of GP, VRM has primarily been applied to classification tasks [13], under the assumption that the target class yi remains stable within the vicinity of training examples. Accordingly, VRM has been used to improve the generalization of GP-based classifiers [13] and evolutionary decision tree classifiers [14].  

> 1Source Code: https://github.com/hengzhe-zhang/EvolutionaryForest/blob/ master/experiment/methods/VJM GP.py JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3

However, this assumption‚Äîthat target labels yi remain un-changed with small perturbations in the training data‚Äîmay not hold for regression tasks, where target values are continuous. Therefore, further investigation is required to adapt VRM for regression datasets. 

B. Overfitting Control for GP 

The issue of generalization in GP has gained significant attention in recent years because the goal of GP is not only to achieve good fitness but also to obtain models that perform well on unseen data [15]. Early work on controlling over-fitting in GP focused on reducing the model size. However, recent research indicates that generalization performance is more complex than merely achieving a small model [3]. Based on these findings, researchers in GP have explored tools from theoretical machine learning, such as Tikhonov regularization [16], VC-dimension [4], and Rademacher com-plexity [5], to control overfitting. However, modern feature learning techniques, such as deep neural networks, possess high VC dimensions yet generalize well, making traditional complexity measures fail to accurately predict the generaliza-tion performance of modern machine learning methods [17]. Another type of overfitting control technique in machine learn-ing is ensemble-based learning, either through homogeneous ensembles [1] or heterogeneous ensembles [18], based on bias-variance theory [19]. However, ensemble methods are not suitable in scenarios where interpretability is essential. Finally, several practical machine learning techniques, such as random sampling [20], early stopping [21], semi-supervised learning [22], and soft targets [23], have shown effectiveness in GP, although the theoretical foundation of these methods is limited. 

C. Evolutionary Feature Construction 

Based on the evaluation method, evolutionary feature con-struction can be categorized into filter-based, wrapper-based, and embedded methods. The filter-based method evaluates features using general metrics, such as information gain [24] or impurity [25]. This method is fast and can construct fea-tures that generalize well across different classifiers. Wrapper-based methods evaluate features based on a specific learning algorithm and can achieve superior performance with that algorithm. For wrapper-based feature construction, multi-tree GP methods have been widely used and show superior per-formance, such as Multidimensional Multiclass GP with Mul-tidimensional Populations (M3GP) [26], Feature Engineering Automation Tool (FEAT) [2], and Gene-pool Optimal Mix-ing Evolutionary Algorithm for GP (GP-GOMEA) [27]. The embedded method integrates feature construction directly into the learning process, with symbolic regression [28] being a typical example. This paper focuses on wrapper-based feature construction due to its superior performance with a given learning algorithm. III. V ICINAL RISK DECOMPOSITION 

In this section, we decompose vicinal risk minimization into loss and regularization terms. Based on this decomposition, we propose an empirical method for calculating the regularization terms in Section IV-B, which are used to control overfitting in GP. 

Theorem 1. Let f : X ‚Üí Y be a machine learning model mapping from the input space X to the output space Y.Consider the VRM framework where each input xi ‚àà X is perturbed by a noise vector œµ, resulting in a vicinal training instance with input xvic = xi + œµ and the corresponding target 

yvic = yi. Under these conditions, the following bound holds: 

(yvic ‚àíf (xvic )) 2 ‚â§ 2( yi ‚àíf (xi)) 2 +2( f (xi)‚àíf (xi +œµ)) 2. (2) The proof is provided in supplementary material. The first term of the bound in Eq. (2) is the traditional empirical loss, 

(yi ‚àí f (xi)) 2. The second term represents the stability of the model under perturbations, specifically (f (xi) ‚àí f (xi + œµ)) 2,which corresponds to the square of a finite difference value. The interpretation of this finite difference depends on œµ. If œµ

is sufficiently small, the second term, (f (xi) ‚àí f (xi + œµ)) 2,approximates the square of the derivative. In practice, for VRM based on finite difference minimization, the objective or fitness value of a GP individual Œ¶ with a linear model F can be formulated as follows: minimize 

Ô£±Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£≥

O1(Œ¶) = X 

> x,y ‚àà(X,Y )

[F (Œ¶( x)) ‚àí y]2 ,O2(Œ¶) = X

> x‚ààX

[F (Œ¶( x + œµ)) ‚àí F (Œ¶( x))] 2 .

(3) The perturbation œµ is usually generated by Gaussian noise, but the objective function shown in Eq. (3) is applicable to any perturbation noise. In fact, the objective function 

[F (Œ¶( x + œµ)) ‚àí F (Œ¶( x))] 2, which measures the difference between predictions on original data F (Œ¶( x)) and corrupted data F (Œ¶( x + œµ)) , has been used in evolutionary feature construction [29] and symbolic regression [30] for variance reduction [29] and sharpness minimization [30], respectively. While perturbation-based VRM methods are straightforward to implement, they have limitations. Specifically, most regres-sion models respond continuously to input changes, making the regularization term [F (Œ¶( x + œµ)) ‚àí F (Œ¶( x))] 2 somewhat restrictive. Therefore, in the following, we decompose mixup-based VRM, deriving a regularization term more suitable for evolutionary feature construction in regression tasks. 

Theorem 2. Within the framework of mixup-based VRM, for any pair of samples (xi, y i) and (xj , y j ), where (xj , y j ) is in the neighborhood of (xi, y i), and for a mixup coefficient 

Œª ‚àà [0 , 1] , the loss on the vicinal example defined by xvic =

Œªx i + (1 ‚àí Œª)xj and yvic = Œªy i + (1 ‚àí Œª)yj satisfies the following inequality: 

(yvic ‚àí f (xvic )) 2 ‚â§ 3( Œª(yi ‚àí f (xi))) 2

+ 3((1 ‚àí Œª)( yj ‚àí f (xj ))) 2

+ 3 ( Œªf (xi) + (1 ‚àí Œª)f (xj ) ‚àí f (xvic )) 2 .

(4) JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4

The proof is provided in supplementary material. Theorem 2 shows that minimizing mixup-based VRM can be achieved by minimizing the vicinal Jensen gap around all training instances 

xi in the training data X. It is worth noting that the term ‚Äúvicinal‚Äù here actually relates to how a sample xj is sampled from the neighborhood of sample xi. If the neighborhood def-inition is infinitely large, the vicinal Jensen gap minimization will degenerate into minimizing the global Jensen gap of the function f . Eq. (4) consists of two parts. The first part is the mean squared loss (Œª(yi ‚àí f (xi))) 2 + ((1 ‚àí Œª)( yj ‚àí f (xj ))) 2,and the second part is (Œªf (xi) + (1 ‚àí Œª)f (xj ) ‚àí f (xvic )) 2.These two parts can form two objective functions, O1(Œ¶) and 

O2(Œ¶) . Thus, for mixup-based VRM, the objective function is formulated as: minimize 

Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥

O1(Œ¶) = X

(x,y )‚àà(X,Y )

[F (Œ¶( x)) ‚àí y]2 ,O2(Œ¶) = X

xi,x j ‚ààX



ŒªF (Œ¶( xi)) + (1 ‚àí Œª)F (Œ¶( xj )) 

‚àí F (Œ¶( Œªx i + (1 ‚àí Œª)xj )) 

2

.

(5) The traditional vicinal risk is V(f ) =

1

n

Pni=1 

R L (f (x), y ) dP xi (x). From the decomposition results, it is clear that the target Y is eliminated from the regularization term O2(Œ¶) , making it a better regularization objective than the traditional vicinal risk V(f ). This regularization term is only related to the smoothness of the evolved function itself, while the fitting accuracy is determined solely by the empirical loss O1(Œ¶) . It is worth noting that the regularization term is still data-dependent because X remains involved, meaning the smoothness is not enforced across the entire feature space but only within regions where samples exist. This leads to a more realistic regularization for real-world applications, especially when the data lies on a low-dimensional manifold embedded in a high-dimensional space. Furthermore, smoothness is considered between nearby pairs of xi and xj , rather than between all pairs of samples in the training set, further avoiding over-penalization. This approach better respects the local geometry of the data manifold rather than enforcing unnecessary global smoothness across irrelevant regions. Such a decomposition enables flexible control over the trade-off between the fitting accuracy and the smoothness of the evolved function. We illustrate the decomposition with intuitive examples for both Gaussian noise-based VRM and mixup-based VRM in Fig. 1a and Fig. 1b, respectively. As depicted in Fig. 1a, minimizing the loss on synthetic samples generated with Gaussian noise derives an overfitting control method that penalizes rapid changes in the function‚Äôs output in response to small input variations. Complexity in this case is measured by the finite difference [F (Œ¶( x + œµ)) ‚àí F (Œ¶( x))] 2,as defined in Eq. (3). Fig. 1b illustrates that minimizing the loss on samples synthesized via the mixup method derives an overfitting control method that penalizes convexity and concavity, thereby promoting local linearity. In this case, complexity is measured by the vicinal Jensen gap                            

> x
>  
>  
> 
> 
> 
> )#(  $# !)
> #( &# #"-( $#
> "$$()#( $# sin (x)$"%!+ (, 
> $&(!,#*#)#( $# sin (3 x)+0.1 x$"%!+ (, 
> &$#$)#!)()( #)#( $# sin (20 x)+x

2 /50 $"%!+ (,     

> & #!"%!
> ,#(' -"%!

(a) Gaussian noise-based VRM ‚Üí Finite Difference                              

> x
>  
>  
> 
> 
> 
> )#(  $# !)
> #!#'#% #"-( $#
> "$$()#( $# sin (x)$"%!+ (, 
> $&(!,#*#)#( $# sin (3 x)+0.1 x$"%!+ (, 
> &$#$)#!)()( #)#( $# sin (20 x)+x

2 /50 $"%!+ (,      

> & #!"%!
> ,#(' -"%!
> #'#%

(b) Mixup-based VRM ‚Üí Vicinal Jensen Gap 

Fig. 1: Illustrative examples of Gaussian noise-based and mixup-based VRM. The predictions of the original and syn-thesized samples made by the fluctuating function are shown. 



ŒªF (Œ¶( xi)) + (1 ‚àí Œª)F (Œ¶( xj )) ‚àí F (Œ¶( Œªx i + (1 ‚àí Œª)xj )) 

2

,as defined in Eq. (5). Here, Œª is sampled from a Beta distribution Beta (Œ±, Œ± ), which is widely used to sample the mixing coefficient in mixup methods [9]. IV. T HE PROPOSED ALGORITHM 

In this section, we first introduce the general framework for evolutionary feature construction that integrates the proposed vicinal Jensen gap or finite difference. Next, we propose an empirical method for calculating the regularization term, based on the decomposed regularization term described in Section III. Then, we propose a noise level estimation strategy to adaptively determine the regularization strength. Finally, we introduce a manifold intrusion detection mechanism to prevent the Jensen gap minimization from overly penalizing effective models. 

A. Algorithm Framework 

In this paper, we propose a Vicinal Jensen Gap Minimization-based GP (VJM-GP) for feature construction. Alternatively, the regularization objective of the vicinal Jensen gap can be replaced with finite difference minimization, result-ing in the Finite Difference Minimization-based GP (FDM-GP). Both variants share the same overall algorithmic frame-work. Our framework builds on the conventional structure JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5Population Initialization Population Initialization  

> Offspring Evaluation Offspring Evaluation
> Offspring Generation Offspring Generation
> Termination?
> End End Yes
> No
> Parent Selection Parent Selection
> Vicinal Jensen Gap Calculation Vicinal Jensen Gap Calculation
> Vicinal Data Synthesis Vicinal Data Synthesis
> Archive Maintenance Archive Maintenance

Fig. 2: Workflow of Vicinal Jensen Gap Minimization-based GP. of evolutionary feature construction while introducing novel fitness functions. As illustrated in Fig. 2, two key steps are integrated into the conventional evolutionary feature construc-tion framework: first, synthesizing vicinal data at the beginning of the evolution process, and second, using the synthesized data during the evolution process to calculate the vicinal Jensen gap or finite difference.  

> ‚Ä¢

Population Initialization: In the initialization phase, a population P is randomly generated by initializing a set of individuals. Each individual initially contains a single GP tree. However, during the evolutionary process, individuals can evolve to include multiple GP trees, each representing a constructed feature [26]. The number of GP trees per individual is dynamically determined through tree addition and deletion operators applied dur-ing the mutation stage.  

> ‚Ä¢

Parent Selection: Parents are selected from the popula-tion using the lexicase selection operator [31]. Lexicase selection operates by randomly generating filters to iter-atively narrow down the population, thereby identifying promising parent individuals. For each filter correspond-ing to a randomly selected instance k, the minimum squared error min Œ¶‚Ä≤‚ààP Lk(Œ¶ ‚Ä≤) is computed across all individuals Œ¶‚Ä≤ ‚àà P , along with the median absolute de-viation œµk of GP individuals for that instance. Individuals that satisfy Lk(Œ¶) ‚â§ min Œ¶‚Ä≤‚ààP Lk(Œ¶ ‚Ä≤) + œµk are retained, while others are eliminated. This process is repeated with multiple filters until only one individual remains, which is then selected as a parent. To select multiple parents, the lexicase selection operator is invoked multiple times.  

> ‚Ä¢

Offspring Evaluation: Offspring are evaluated using cross-validation and vicinal risk estimation to obtain ob-jective values O1(Œ¶) and O2(Œ¶) , respectively. Typically, cross-validation is performed by partitioning the data into multiple folds and executing linear regression on each fold. However, for linear regression, leave-one-out cross-validation (LOOCV) can be computed efficiently [32]. Instead of solving n separate least-squares optimization problems, the LOOCV error vector e can be calculated as Eq. (6): 

e =

 Y ‚àí Xw

1 ‚àí h

2

, (6) where w denotes the optimal weights, and h represents the leverage values. The optimal weights w for a regu-larization coefficient Œ± are computed as: 

w =  X‚ä§X + Œ±I‚àí1 X‚ä§Y. (7) The leverage values h are calculated as: 

h = diag 



X  X‚ä§X + Œ±I‚àí1 X‚ä§

. (8) Intuitively, the leverage hii measures the influence of each data point xi on the model‚Äôs weights, with larger values indicating greater impact. Using Eq. (6), a vector of leave-one-out cross-validation errors is computed, which contains n values representing the loss for each of the 

n training instances. In addition to cross-validation loss, regularization terms such as finite difference or vicinal Jensen gap are calculated based on the method introduced in Section IV-B.  

> ‚Ä¢

Offspring Generation: Offspring are generated from the selected parents using genetic operators including random subtree crossover, mutation, tree addition, and tree deletion [26]. Random subtree crossover exchanges subtrees between two parent GP trees, facilitating the combination of building blocks from different individuals. Random subtree mutation introduces new genetic material by modifying parts of a GP tree. The tree addition and deletion operators modify the number of trees in each individual by either adding a GP tree using the ramped-half-and-half method or randomly removing a GP tree, dynamically adjusting the number of constructed features.  

> ‚Ä¢

Environmental/Survival Selection: The environmental selection process selects a population of N individuals from the combined pool of parents and offspring using non-dominated sorting with crowding distance [33]. This approach ranks individuals based on Pareto dominance and maintains diversity within the population by con-sidering crowding distance. The top-N individuals are chosen to survive to the next generation. This selection mechanism is essential because lexicase selection only considers the semantics of individuals, neglecting the reg-ularization term. By incorporating non-dominated sorting, the environmental selection operator ensures that both objectives‚Äîcross-validation loss and regularization‚Äîare simultaneously optimized, promoting better generaliza-tion performance.  

> ‚Ä¢

Archive Maintenance: An external archive is maintained to store the historically best individuals for making pre-dictions on unseen data. Throughout the optimization process, a Pareto set of two objectives is obtained. How-ever, for making predictions on unseen data, a scalarized objective function is required to select a final model. Specifically, the best model is determined by the min-imum sum of two objectives, i.e., 

O1(x) + œÑ O 2(x), (9) JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6

where O1(x) is the cross-validation loss, O2(x) is the regularization term, i.e., the vicinal Jensen gap, and œÑ

is the balancing coefficient. By default, œÑ is set to 1. However, for highly noisy datasets, it might be better to set œÑ to a larger value to reduce fitting noise, with the noise estimation strategy shown in Section IV-C. The processes of parent selection, offspring generation, off-spring evaluation, environmental selection, and archive main-tenance are iteratively repeated until the desired number of iterations is reached. Finally, the best model in the archive, as determined by Eq. (9), is used to make predictions on unseen data. 

B. Empirical Vicinal Risk Decomposition 

At the beginning of empirical vicinal risk estimation, con-structed features Œ¶( X) are first derived from the original features X. A linear regression model F is then trained using these constructed features Œ¶( X) as input features. Once trained, the linear model F remains fixed. The vicinal risk can then be estimated based on Eq. (3) or Eq. (5). To ensure robustness, vicinal risk is estimated over K iterations, where 

K is a hyperparameter. The process of empirical vicinal risk estimation includes four key steps, as outlined in Algorithm 1.  

> ‚Ä¢

Vicinal Data Synthesis (Line 4): In this stage, vicinal data Xvic is synthesized either by adding Gaussian noise or using the mixup method, depending on whether the VRM focuses on finite difference or the Jensen gap. 

‚Äì Finite Difference: To optimize the finite difference, vicinal data is synthesized by adding Gaussian noise, as specified in Eq. (10): 

xvic = xi + œµ, œµ ‚àº N (0 , œÉ 2). (10) 

‚Äì Vicinal Jensen Gap: To optimize the vicinal Jensen gap, new samples are synthesized using the mixup method, as defined in Eq. (11): 

xvic = Œªx i + (1 ‚àí Œª)xj , Œª ‚àº Beta (Œ±, Œ≤ ), (11) where Beta (Œ±, Œ≤ ) represents the Beta distribution with control parameters Œ± and Œ≤. In this paper, both Œ± and Œ≤ are set to 10, i.e., Œ± = Œ≤ =10 , as setting them to the same value is common practice in mixup [9]. A sensitivity analysis of Œ±

is provided in Section G-A of the supplementary material, demonstrating that the method maintains stable performance across a range of Œ± values. A key assumption of mixup is that vicinal samples are synthesized from nearby data points. Therefore, for each sample xi, y i, only samples xj , y j that are similar to xi, y i have a high probability of being mixed. To achieve this, we first define the distance kernel between two samples, as shown in Eq. (12): 

K(yi, y j ) = e‚àíŒ≥‚à•yi‚àíyj ‚à•2

, (12) where Œ≥ is a spread parameter. Here, the distance is defined based on the distance between labels. Defining the distance based on either y alone or both x and y has respective advantages, with the pros and cons discussed in Section C of the supple-mentary material. The kernel distance is normalized to compute the probability P (yj |yi), as shown in Equation (13): 

P (yj |yi) = K(yj , y i)

P   

> yk‚ààY\{ yi}

K(yk, y i) , (13) where yk ‚àà Y \ { yi} ensures that the probability of sampling yi when the current instance is yi is explicitly set to zero to avoid redundant regulariza-tion. For each training sample xi, K paired samples are sampled according to the probability distribution 

P (yj |yi).For each random seed in K iterations, the vicinal data varies. To ensure efficient utilization of computational resources and fair comparisons of vicinal risk, the syn-thesized vicinal data is cached and reused across different individuals.  

> ‚Ä¢

Vicinal Feature Construction (Line 6): Features are constructed for the synthesized vicinal data Xvic . These features are then input into the fixed model F to obtain predictions ÀÜY . 

> ‚Ä¢

Vicinal Jensen Gap/Finite Difference Calculation (Lines 7-8): The predicted results ÀÜY are compared against the target labels. 

‚Äì Gaussian Noise: For samples synthesized with Gaussian noise, the target labels yvic remain the same as the original target yi.

‚Äì Mixup: For samples synthesized using mixup, the target label yvic is defined as in Eq. (14), based on Eq. (5): 

yvic = Œª ¬∑ F (Œ¶( xi)) + (1 ‚àí Œª) ¬∑ F (Œ¶( xj )) . (14) In both cases, we compute the worst-case squared error between the prediction ÀÜyi and the target label yvic for each instance over K iterations. The worst-case squared error is used instead of the average squared error to encourage the development of a model that is more robust to unseen data across different scenarios. Specifically, for each xi, the vicinal Jensen gap or finite difference is computed as: 

Vi = max 

> k‚àà{ 1,...,K }

(ÀÜ yki ‚àí ykvic )2, (15) where ÀÜyki and ykvic represent the prediction and target label for the k-th iteration, respectively. The final regularization term V is the average of Vi over all training instances, as detailed in Algorithm 1. 

C. Noise Estimation Strategy 

There are various strategies for adjusting the weight œÑ in Eq. (9) based on estimated noise level or estimated com-plexity of datasets. In this paper, we propose adjusting œÑ

according to the noise level in the dataset. This approach is supported by observations in Fig. 3, which illustrate that the cross-validation predictions of the same sinusoidal function JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7

Algorithm 1 Vicinal Jensen Gap/Finite Difference Minimiza-tion 

Require: GP Tree Œ¶, Input X, Target Outputs Y , Linear Model F , Number of Iterations K 

> 1:

Initialize Regularization Term: V ‚Üê 0 

> 2:

Œ¶( X) ‚Üê Feature Construction ( X, Œ¶) 

> 3:

for k = 1 , . . . , K do  

> 4:

Xvic , Y vic ‚Üê Data Synthesis ( X, Y, k ) ‚ñ∑ Cached  

> 5:

Œ¶( Xvic ) ‚Üê Feature Construction ( Xvic , Œ¶) 

> 6:

ÀÜY ‚Üê Prediction( F, Œ¶( Xvic )) 

> 7:

for i = 1 , . . . , N do  

> 8:

Vi ‚Üê max( Vi, (ÀÜ yki ‚àí ykvic )2)

Ensure: Regularization Term V = 1

> N

PNi=1 Vi‚àí ‚àí    

‚àí 

‚àí 

 

 

    

>      

       

  



  

>    

      

Fig. 3: Estimating Noise Level with Extremely Randomized Trees. vary with different noise levels. Specifically, when the noise level is low, Extremely Randomized Trees (Extra Trees) [34] can make accurate predictions. However, as the noise level increases, these trees tend to produce less accurate predictions. Consequently, we use the five-fold cross-validation R2 score of training data with Extra Trees as a metric to estimate the noise level in the datasets. Based on the estimated noise level 

R2, the weighting coefficient œÑ is defined as follows: 

œÑ =

(

1 if R2 ‚â• 0.5,

10 if R2 < 0.5. (16) 

D. Manifold Intrusion Detection 

The vicinal Jensen gap minimization encourages local lin-earity between two selected points xi and xj . However, Jensen gap minimization might conflict with the true underlying func-tion if the relationship between xi and xj is highly nonlinear, leading to a phenomenon known as manifold intrusion [35]. An example of manifold intrusion is illustrated in Fig. 4, where the label of a synthesized sample deviates significantly from the true function due to incorrect assumptions about linearity. To mitigate manifold intrusion in synthesized vicinal data, we propose a manifold intrusion detection strategy to discard intrusion points. In this strategy, we predict the label for synthesized data points xvic using Extra Trees T trained on the original training data (X, Y ), denoted as ytree = T (xvic ), as indicated by the square in Fig. 4. Assuming Œ¶( xi) < Œ¶( xj ),‚àí ‚àí ‚àí    )

‚àí 

‚àí 

 

 

 *       

>  ! Intrusion Example with Highly Curved Data

Original Data 

Selected Points Synthesized Sample 

Ground Truth 

(Synthesized Sample) 

Predicted by Extra Trees 

Fig. 4: An example of manifold intrusion, where the ground truth and the prediction by Extra Trees overlap, but the synthesized sample deviates significantly from these two. the lower and upper bounds for the synthesized instance are defined as: Lower Bound = ( Œª‚àíŒº)¬∑F (Œ¶( xi))+(1 ‚àí(Œª‚àíŒº)) ¬∑F (Œ¶( xj )) ,

(17) Upper Bound = ( Œª+Œº)¬∑F (Œ¶( xi))+(1 ‚àí(Œª+Œº)) ¬∑F (Œ¶( xj )) ,

(18) where Œº is a hyperparameter representing the margin. The value of Œº depends on the confidence in the accuracy of the reference model, which in this paper is the Extra Trees regressor. If the reference model is expected to be accurate, 

Œº is set to a small value. Conversely, if the reference model‚Äôs reliability is questionable, Œº is set to a large value. In this paper, we use the cross-validation R2 scores on the training set, as calculated in Section IV-C, to estimate the reliability of the reference model. If the model is deemed reliable, Œº is set to 0.05, ensuring that the interval Upper Bound ‚àí Lower Bound equals 0.1 ¬∑

h

F (Œ¶( xi)) ‚àí F (Œ¶( xj )) 

i

. Otherwise, Œº is set to ‚àû, effectively disabling manifold intrusion detection. If 

Œ¶( xi) > Œ¶( xj ), the lower and upper bounds are swapped accordingly. Based on the lower and upper bounds, if the predictions from Extra Trees T significantly deviate from the synthesized data values, it suggests that the region is difficult to synthesize accurately, and the sample should be discarded. Formally, data points are discarded if either of the following conditions holds: 

ytree < Lower Bound or ytree > Upper Bound . (19) These conditions indicate that minimizing the Jensen gap in that region is inconsistent with the true data distribution, suggesting the synthesized point should be excluded. Data points that meet these conditions are discarded, and new samples are generated. To prevent infinite regeneration, after every ten unsuccessful attempts, the Œ± parameter in the Beta distribution in Equation (11) is multiplied by 10 while keeping 

Œ≤ unchanged, thereby increasing the weight of xi in the synthetic data. The maximum number of regeneration attempts is capped at 100. If this limit is reached, the data is left as is. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8

TABLE I: Parameter settings for VJM-GP.             

> Parameter Value
> Maximal Population Size 200 Number of Generations 100 Crossover and Mutation Rates 0.9 and 0.1 Tree Addition Rate 0.5 Tree Deletion Rate 0.5 Initial Tree Depth 0-3 Maximum Tree Depth 10 Initial Number of Trees 1Maximum Number of Trees 10 Elitism (Number of Individuals) 1Iterations of Estimation ( K)10 Functions +, -, *, AQ, Square, Log, Sqrt, Max, Min, Sin, Cos, Abs, Negative

V. E XPERIMENTAL SETTINGS 

A. Datasets 

The regression datasets used in this paper are sourced from 120 black-box datasets in the Penn Machine Learning Benchmark (PMLB) [36]. We focus exclusively on real-world datasets due to their inherent complexity. After excluding datasets synthesized by Friedman, a total of 58 real-world datasets are used in the experiments. 

B. Parameter Settings 

The parameter settings are detailed in Table I. Following tradition, a combination of a high crossover rate and a low mutation rate is used to facilitate the leveraging of building blocks. To prevent division-by-zero errors, the analytical quo-tient (AQ) [37] is used to replace the conventional division operator, which is defined as AQ = a‚àö1+ b2 . For the minimum and maximum operators, two arguments are allowed. The sine and cosine functions are defined as sin( œÄ √ó x) and cos( œÄ √ó x),respectively. Ephemeral constants are uniformly sampled from the range [‚àí1, 1] . The archive elitism mechanism is used only in standard GP, as the non-dominated sorting [33] in the proposed method inherently preserves elite individuals. For finite difference regularization, the standard deviation of the Gaussian noise, œÉ, is set to 0.5. For vicinal Jensen gap regularization, the spread parameter Œ≥ is set to 0.5. 

C. Evaluation Protocol 

To ensure a reliable comparison, each method performs 30 independent runs on each dataset. In each run, 100 samples are randomly chosen as training data [38], and the remaining samples are used for testing. For smaller datasets where this split results in fewer than 100 test samples, a 50%-50% split is applied to ensure a sufficient number of test samples. A target encoder [39] is used to transform categorical input variables into numerical variables, as mixup requires numerical data. For the evaluation metric, R2 is employed to eliminate magnitude differences between different datasets. During prediction, the maximum and minimum label values in the training data are recorded, and the predictions are clipped to these values to prevent excessively deviating predictions. The R2 score is defined as R2 = 1 ‚àí 

> P
> i(yi‚àíÀÜyi)2
> P
> i(yi‚àí¬Øy)2

, where ÀÜyi represents the predictions, yi denotes the ground truth, and ¬Øy is the mean of the ground truth values. After obtaining test scores, a Wilcoxon signed-rank test with a significance level of 0.01 is used to determine whether one method is statistically better, similar, or worse than another method. 

D. Baseline Algorithms 

For the baseline algorithms, we compare the proposed VJM-GP with 8 baseline methods, including standard GP and seven GP methods with various complexity measures:  

> ‚Ä¢

Pessimistic Vicinal Risk Minimization (P-VRM) [40]: P-VRM implicitly optimizes the vicinal Jensen gap as shown in Theorem 2. The objective functions of P-VRM are: minimize 

Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥

O1(Œ¶) = X

> x‚ààX

[F (Œ¶( x)) ‚àí Y ]2 ,O2(Œ¶) = X 

> xi,x j‚ààX



Œªy i + (1 ‚àí Œª)yj

‚àí F (Œ¶( Œªx i + (1 ‚àí Œª)xj )) 

2

.

(20) The process by which P-VRM generates vicinal samples follows the same procedure outlined in Section IV-B. The primary distinction between P-VRM and VJM is that P-VRM implicitly incorporates the training loss in its second objective.  

> ‚Ä¢

Parsimony Pressure (PP) [41]: PP controls model size to enhance generalization performance. Specifically, it optimizes the number of nodes across all GP trees in an individual Œ¶. 

> ‚Ä¢

Tikhonov Regularization (TK) [16]: TK regularizes overly large predictions using the regularization term 

|| f (X)|| , where f represents the combination of GP trees and the linear model for making predictions. This approach penalizes excessively large predictions to ensure stability on unseen data.  

> ‚Ä¢

Grand Complexity (GC) [16]: GC integrates PP and TK to control overfitting, treating the dominance relationship between these objectives as a single optimization objec-tive .  

> ‚Ä¢

Rademacher Complexity (RC) [5]: RC measures the capacity of a model to fit the training data with random labels. It is formally defined as: 

Rn(L) = E

"

sup 

> l‚ààL

1

n

> n

X

> i=1

œÉil (xi, y i)

#

, (21) where œÉi is a Rademacher random variable taking values in {‚àí 1, 1} with equal probability.  

> ‚Ä¢

Weighted Maximal Information Coefficient between Residuals and Variables (WCRV) [42]: WCRV consists of two terms. For important features xk in the GP tree that have a large correlation with the target compared to the median correlation mv among all features, i.e., 

MIC xk ,Y ‚â• mv , the first term MIC xk ,R penalizes the correlation between high-importance features and the JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9

residuals, weighted by feature importance MIC xk ,Y , thus avoiding regularity in the residuals. For unimportant features in the GP tree, i.e., MIC xk ,Y < mv , the second term encourages the GP to select a minimal number of low-relevance features, 1 ‚àí MIC xk ,Y . Based on these criteria, WCRV is defined as follows: 

WCRV(Œ¶) = X 

> MIC xk ,Y ‚â•mv

MIC xk ,Y √ó MIC xk ,R 

+ X 

> MIC xk ,Y <mv

 1 ‚àí MIC xk ,Y 

 . (22) Ideally, both terms should be low for a low-complexity model, and thus WCRV is minimized to encourage a low-complexity model.  

> ‚Ä¢

Correlation between Input and Output Distances (IODC) [23]: IODC encourages a linear relationship between input correlation I and output correlation O,denoted by Cov(I , O) . Formally, IODC is defined as: 

IODC(Œ¶) = Cov(I , O) 

œÉIœÉO

, (23) where œÉI and œÉO are the standard deviations of I and O,respectively. The key idea here is that a low-complexity model should exhibit a high correlation between input and output distances. Therefore, IODC is maximized to promote low model complexity.  

> ‚Ä¢

Standard GP: Standard GP uses the cross-validation loss as the optimization objective. To ensure a fair comparison, all baseline algorithms are implemented within the same GP framework as the proposed VJM-GP, introduced in Section IV-A. To balance training performance with model complexity, the minimum Manhattan distance (MMD)-based knee point selection method [43] is used to select individuals from the Pareto front of solutions. Specifically, the individual with the minimal sum of the two objectives, O1 + O2, is selected as the final model. Here, 

O1 and O2 represent the normalized objective values, as complexity measures are on a different scale compared to mean square error for the baseline methods. For P-VRM, the final model is chosen based on the minimal vicinal risk among the models on the Pareto front, rather than using the knee point. This is because the second objective in P-VRM encompasses both empirical loss (i.e., the first objective) and the regularization term, as detailed in Eq. (2). Using the VRM objective alone is a standard practice in deep learning [9]. VI. E XPERIMENTAL RESULTS 

In this section, we first compare the vicinal Jensen gap (VJM) to finite difference (FDM) as regularizers. Given the superiority of VJM, we then focus on comparing VJM with seven alternative complexity measures in terms of test R2

scores, training R2 scores, model sizes, and training time within the same evolutionary feature construction framework. Finally, we compare VJM-GP with 15 symbolic regression and machine learning algorithms to demonstrate the effectiveness of evolutionary feature construction with VJM.          

> 
> 
> 
> 
>         
>  
> (a) Training R2scores. 
> 
> 
> 
> 
>         
>   (b) Test R2scores.

Fig. 5: Comparison between optimizing vicinal Jensen gap versus finite difference. (‚Äú+‚Äù,‚Äú ‚àº‚Äù, and ‚Äú-‚Äù represent the num-ber of datasets where optimizing vicinal Jensen gap performs better, similar, or worse, respectively, compared to optimizing finite difference.) 

A. Vicinal Jensen Gap versus Finite Difference 

In Section IV-B, mixup-based VRM is decomposed into training loss and vicinal Jensen gap, while perturbation-based VRM is decomposed into training loss and finite difference. A comparison of R2 scores for using the Jensen gap and finite difference on the training and test sets is presented in Fig. 5a and Fig. 5b, respectively. Experimental results demonstrate that regularizing GP with the Jensen gap generally yields superior performance compared to regularizing with finite difference. As illustrated in Fig. 5b, regularization with Jensen gap outperforms regularization with finite difference on 15 datasets and performs worse on only 5 datasets regarding test R2 scores. This indicates that the Jensen gap is a more effective regularizer for controlling overfitting. Consequently, in the following sections, we focus on optimizing the vicinal Jensen gap. 

B. Comparisons on Test R2 Scores 1) General Analysis: The comparison of test R2 scores is presented in Table II. In summary, different methods for controlling overfitting exhibit varying levels of effectiveness, with VJM emerging as the most effective among them. The results demonstrate that VJM significantly enhances general-ization performance compared to standard GP on 36 datasets, while it underperforms on only 5 datasets, highlighting VJM‚Äôs high efficacy in controlling overfitting. Other methods, aside from P-VRM, perform worse than VJM on more than half of the datasets and exceed VJM on only 0-5 datasets, further confirming the superiority of using VJM as a regularization metric. Fig. 6 provides evolutionary plots of test R2 scores on four representative datasets, which clearly illustrate that VJM effectively controls overfitting. In contrast, methods like IODC suffer from overfitting, with test R2 scores declining slightly in later generations, resulting in inferior performance compared to VJM. 

2) VJM vs VRM: In the deep learning domain, a common practice is to optimize VRM directly rather than decomposing it into training loss and vicinal Jensen gap and optimizing them simultaneously. However, as evidenced by Table II, VJM outperforms P-VRM on 22 datasets and underperforms on only 6 dataset. The experimental results suggest that optimizing the JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10 

TABLE II: Statistical comparison of test R2 scores when optimizing various model complexity measures. (‚Äú+‚Äù,‚Äú ‚àº‚Äù, and ‚Äú-‚Äù indicate using the method in a row is statistically better than, similar to or worse than using the method in a column.)                                                                                               

> P-VRM PP RC GC IODC TK WCRV Standard GP VJM 22(+)/30( ‚àº)/6(-) 29(+)/27( ‚àº)/2(-) 51(+)/6( ‚àº)/1(-) 39(+)/18( ‚àº)/1(-) 39(+)/18( ‚àº)/1(-) 45(+)/13( ‚àº)/0(-) 43(+)/11( ‚àº)/4(-) 36(+)/17( ‚àº)/5(-)
> P-VRM ‚Äî26(+)/26( ‚àº)/6(-) 49(+)/7( ‚àº)/2(-) 30(+)/27( ‚àº)/1(-) 33(+)/23( ‚àº)/2(-) 47(+)/10( ‚àº)/1(-) 39(+)/17( ‚àº)/2(-) 33(+)/18( ‚àº)/7(-)
> PP ‚Äî‚Äî36(+)/16( ‚àº)/6(-) 19(+)/34( ‚àº)/5(-) 22(+)/26( ‚àº)/10(-) 30(+)/27( ‚àº)/1(-) 24(+)/28( ‚àº)/6(-) 23(+)/26( ‚àº)/9(-)
> RC ‚Äî‚Äî‚Äî4(+)/16( ‚àº)/38(-) 3(+)/31( ‚àº)/24(-) 6(+)/25( ‚àº)/27(-) 8(+)/23( ‚àº)/27(-) 13(+)/14( ‚àº)/31(-)
> GC ‚Äî‚Äî‚Äî‚Äî17(+)/33( ‚àº)/8(-) 23(+)/33( ‚àº)/2(-) 15(+)/39( ‚àº)/4(-) 19(+)/23( ‚àº)/16(-)
> IODC ‚Äî‚Äî‚Äî‚Äî‚Äî20(+)/24( ‚àº)/14(-) 15(+)/33( ‚àº)/10(-) 20(+)/15( ‚àº)/23(-)
> TK ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî9(+)/30( ‚àº)/19(-) 9(+)/22( ‚àº)/27(-)
> WCRV ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî14(+)/25( ‚àº)/19(-) 
> "%&!#"
> 
> 
> 
> $ "
> 
> "%&!#"
> 
> 
> $ " 
> 
> "%&!#"
> 
> 
> $ " 
> 
> "%&!#"
>  
> 
> $ " 
> 
> 
> 
> 
> 
> 
> 
>  
> &"%

Fig. 6: Evolutionary plots of the test R2 scores for different complexity control methods. vicinal Jensen gap and cross-validation loss on training data is more advantageous than directly optimizing mixup-based VRM. There are four reasons for this advantage. First, decom-posing mixup-based VRM into training loss and vicinal Jensen gap allows for a flexible balance between these objectives through a noise estimation mechanism. Section VII-C further illustrates the benefits of this approach. Second, VRM esti-mation can be imprecise in scenarios with limited vicinal risk sampling. By separating the empirical loss (Y ‚àí Xw)2 from VRM, we obtain a more accurate estimate of the empirical loss. Third, P-VRM, which jointly optimizes both training er-ror and VRM, is similar to Œ±-dominance-based multi-objective optimization. Since VRM includes both training error and a regularization term, the empirical loss may be overemphasized, potentially obstructing the discovery of low-complexity so-lutions and negatively affecting generalization performance. Fourth, the loss value incorporated in vicinal risk within P-VRM is empirical loss rather than cross-validation loss, which may lead to an overly optimistic estimation of generalization performance and increase the risk of overfitting. Therefore, optimizing the vicinal Jensen gap and cross-validation loss on training data proves to be a more effective strategy than directly optimizing VRM. 

3) VJM vs Parsimony Pressure: The results in Table II indicate that controlling tree size remains an effective measure for managing overfitting in evolutionary feature construction. In evolutionary feature construction, small GP trees can have good fitting capabilities as long as they are complemen-tary, which differs from single-tree-based symbolic regression methods [5]. However, Table II suggests that while PP is a competitive method for overfitting control, it is not as effective as VJM. VJM outperforms PP on 29 datasets and performs worse on only 2 datasets. These results suggest that relying solely on tree size as a measure of model complexity is insuf-ficient, highlighting the importance of controlling functional complexity for effective overfitting control. 

C. Comparisons on Training R2 Scores 

The experimental results comparing training R2 scores are shown in Table III. These results reveal that standard GP achieves significantly higher training R2 scores compared to VJM, with superior performance on 56 datasets. Fig. 7 further confirms the superiority of standard GP in terms of training R2 scores. However, as indicated in Table II, standard GP suffers from severe overfitting despite its higher training 

R2. This shows that solely optimizing training R2 scores is insufficient for achieving good generalization in evolutionary feature construction methods. Fig. 8 and Fig. 9 illustrate the evolutionary plots of corresponding training and test R2

scores. As shown in Fig. 8, training and test R2 scores are highly correlated when using VJM. In contrast, Fig. 9 demonstrates that improvements in training R2 scores with standard GP do not consistently lead to better test R2 scores and can even worsen performance, indicating overfitting in fea-ture construction. Although controlling model complexity can improve generalization, it is also worth noting that complexity control should be moderate. Overly strict penalization can negatively impact generalization performance. For instance, methods like PP and GC exhibit lower training R2 scores than VJM on 30 and 15 datasets, respectively. As shown in Table II, these complexity control methods lead to worse generalization compared to VJM. Therefore, effective overfitting control requires a careful definition of model complexity, as overly stringent assumptions may over-penalize useful models and yield suboptimal results. 

D. Comparisons on Tree Size 

A comparison of tree sizes is shown in Fig. 10a. The experimental results demonstrate that different complexity measures lead to models of varying sizes. VJM can reduce the model size compared to standard GP. Specifically, the median model size decreases from 55.25 to 20. This suggests that VJM implicitly favors simpler models, which could enhance JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11 

TABLE III: Statistical comparison of training R2 scores when optimizing various model complexity measures. 

P-VRM PP RC GC IODC TK WCRV Standard GP VJM 9(+)/17( ‚àº)/32(-) 21(+)/7( ‚àº)/30(-) 49(+)/6( ‚àº)/3(-) 28(+)/15( ‚àº)/15(-) 24(+)/14( ‚àº)/20(-) 30(+)/8( ‚àº)/20(-) 28(+)/16( ‚àº)/14(-) 0(+)/2( ‚àº)/56(-) 

P-VRM ‚Äî 19(+)/22( ‚àº)/17(-) 54(+)/3( ‚àº)/1(-) 39(+)/12( ‚àº)/7(-) 29(+)/19( ‚àº)/10(-) 29(+)/18( ‚àº)/11(-) 33(+)/16( ‚àº)/9(-) 1(+)/4( ‚àº)/53(-) 

PP ‚Äî ‚Äî 57(+)/1( ‚àº)/0(-) 44(+)/13( ‚àº)/1(-) 30(+)/26( ‚àº)/2(-) 30(+)/24( ‚àº)/4(-) 36(+)/19( ‚àº)/3(-) 0(+)/2( ‚àº)/56(-) 

RC ‚Äî ‚Äî ‚Äî 0(+)/3( ‚àº)/55(-) 3(+)/6( ‚àº)/49(-) 0(+)/6( ‚àº)/52(-) 0(+)/13( ‚àº)/45(-) 0(+)/0( ‚àº)/58(-) 

GC ‚Äî ‚Äî ‚Äî ‚Äî 12(+)/31( ‚àº)/15(-) 16(+)/24( ‚àº)/18(-) 21(+)/26( ‚àº)/11(-) 0(+)/1( ‚àº)/57(-) 

IODC ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî 15(+)/23( ‚àº)/20(-) 20(+)/24( ‚àº)/14(-) 0(+)/0( ‚àº)/58(-) 

TK ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî 23(+)/21( ‚àº)/14(-) 0(+)/4( ‚àº)/54(-) 

WCRV ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî 0(+)/0( ‚àº)/58(-)                        

>  #$!
> 
> 
> 
> " 
>  
>  #$!
> 
> 
> "  
>  
>  #$!
> 
> 
>  
> " 
>  
>  #$!
> 
> 
>  
> "  
> 
> 
> 
> 
> 
> 
> 
> 
> $ # 

Fig. 7: Evolutionary plots of the training R2 scores for various complexity control methods. 0 50 100 

Generation 

0.3 

0.4 

> Training  R2 Score

0.25 

0.30 

> Test  R2 Score

OpenML_1028 

Pearson:0.99 

0 50 100 

Generation 

0.6 

0.8 

> Training  R2 Score

0.5 

0.6 

0.7 

> Test  R2 Score

OpenML_294 

Pearson:0.978 

0 50 100 

Generation 

0.4 

0.6 

> Training  R2 Score

0.3 

0.4 

> Test  R2 Score

OpenML_547 

Pearson:0.95 

0 50 100 

Generation 

0.2 

0.3 

> Training  R2 Score

0.05 

0.10 

> Test  R2 Score

OpenML_574 

Pearson:0.765 

Training R2 Test R2 

Fig. 8: Evolutionary plots of the training and test R2 scores 

for VJM-GP. Note that the plot includes two axes. interpretability. An interesting observation is that optimizing WCRV can result in similar tree sizes; however, the results in Table II indicate that optimizing WCRV achieves much worse test performance compared to VJM. This suggests that improving generalization is more closely related to reducing functional complexity rather than merely minimizing model size. 0 50 100 

Generation 

0.4 

0.6 

> Training  R2 Score

0.20 

0.25 

> Test  R2 Score

OpenML_1028 

Pearson:-0.653 

0 50 100 

Generation 

0.6 

0.8 

> Training  R2 Score

0.6 

0.7 

> Test  R2 Score

OpenML_294 

Pearson:0.854 

0 50 100 

Generation 

0.4 

0.6 

> Training  R2 Score

0.3 

0.4 

> Test  R2 Score

OpenML_547 

Pearson:-0.378 

0 50 100 

Generation 

0.25 

0.50 

0.75 

> Training  R2 Score

0.05 

0.00 

0.05 

> Test  R2 Score

OpenML_574 

Pearson:-0.661 

Training R2 Test R2 

Fig. 9: Evolutionary plots of the training and test R2 scores 

for standard GP.         

> 
> 
> 
> 
>  

(a) Tree Sizes           

> 
> 
> 
>    

(b) Training Time 

Fig. 10: Comparison of tree sizes and training times over 58 datasets when optimizing different model complexity mea-sures. 

E. Comparisons on Training Time 

Comparisons of training time for different complexity mea-sures are shown in Fig. 10b. The results indicate that opti-mizing semantic complexity measures, such as VJM, requires more time than optimizing syntactic complexity measures, like PP. On average, VJM takes 1041 seconds, whereas PP takes only 250 seconds. This difference arises from the need to construct features based on vicinal data in VJM, which increases the computational cost. Theoretically, the time complexity of standard GP is O(|P |G), where |P | is the population size and G is the number of generations. For VJM-GP, the time complexity increases to O(|P |G(K + 1)) , as each individual is evaluated on the vicinal data for K rounds of vicinal Jensen gap estimation, significantly increasing the training time. However, in practice, not all individuals require 

K rounds of estimation. For less promising individuals, fewer estimation rounds may suffice. A discussion on an early stop-JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12 

ping strategy to accelerate VJM-GP is provided in Section E of the supplementary material. Moreover, since overfitting cannot be effectively addressed by simply increasing the number of generations, the additional training time required for VJM is considered a worthwhile trade-off. 

F. Comparisons with Other Learning Algorithms 

In this section, we follow the evaluation protocol of state-of-the-art symbolic regression benchmarks [44] to compare our proposed method against popular machine learning and symbolic regression methods. The training and test data are split according to the same protocol used throughout this paper. To ensure that baseline algorithms are well-tuned, their hyperparameters are optimized on the training data using the parameter grid provided in Table XII in the supplementary material. Given the limited number of samples, hyperparam-eters are tuned using grid search [45] rather than halving grid search [46]. The results presented in Fig. 11 demonstrate that our proposed method achieves competitive test R2 scores compared to 15 existing symbolic regression and machine learning methods. The existing popular GP-based regression method, Operon [47], performs poorly when the number of samples is limited. In comparison, our proposed method effec-tively controls overfitting and achieves the best performance. Nevertheless, the proposed objective function is general and could be integrated into existing GP-based regression methods to enhance their performance. Compared to popular gradient boosting decision trees such as XGBoost [48], VJM-GP also demonstrates very good performance. A Wilcoxon signed-rank test with the Benjamini & Hochberg correction indicates that VJM-GP significantly outperforms XGBoost, as shown in Fig. 12. Furthermore, when compared to deep learning methods designed for small tabular datasets, such as Ex-celFormer [49], which leverages the mixup technique to boost generalization, VJM-GP achieves notably better results. This highlights the strength of evolutionary feature construction in scenarios with limited data. Regarding model size, VJM-GP is an order of magnitude smaller than XGBoost, highlighting its potentially superior interpretability. In terms of training time, the proposed evolutionary feature construction method is time-consuming, but average training time remains within two hours. In practical applications, the algorithm could be sped up with early stopping if the best fitness does not improve over a consecutive number of iterations. Overall, these results demonstrate that VJM not only significantly improves the generalization performance of evolutionary fea-ture construction methods but also enables GP to achieve top performance compared to popular machine learning and symbolic regression methods in scenarios where the number of samples is limited. VII. F URTHER ANALYSIS 

In this section, we first examine the effectiveness of VJM for learning from noisy labels, another important scenario where overfitting frequently occurs. Then, we delve into two key components of the proposed VJM-GP algorithm: the manifold intrusion detection strategy and the noise estimation strategy in the model selection stage, to study the effect of these two improvements made to VJM-GP. Additionally, we provide a visualization of the final constructed features to offer an intuitive comparison between GP trees constructed by VJM-GP and those constructed by standard GP. 

A. Label Noise Learning 

Learning with limited samples and labels corrupted by noise are two challenging scenarios where evolutionary feature con-struction tends to overfit. In this section, we demonstrate the effectiveness of the proposed vicinal Jensen gap minimization in controlling overfitting when learning with noisy labels. We focus on 36 datasets with fewer than 2000 instances due to computational constraints. For these datasets, 80% of samples are randomly chosen as training data, with the remaining 20% used for testing. Training labels are corrupted with noise drawn from a Gaussian distribution N (0 , 1) , while test labels remain uncorrupted. In addition to standard GP, we compare VJM with two baseline complexity measures that performed well in Section VI-B: P-VRM and PP. A summary of statistical comparisons is shown in Fig. 13. The experimental results indicate that VJM outperforms P-VRM, PP, and standard GP in the context of learning with noisy labels. Specifically, VJM performs significantly better than PP on 24 datasets, similarly on 9 datasets, and worse on only 3 datasets. VJM also performs significantly better than standard GP on 31 datasets, similarly on 3 datasets, and worse on 2 datasets. Compared to P-VRM, VJM outperforms it on 19 datasets, performs similarly on 14 datasets, and is worse on 3 datasets. These results confirm that VJM effectively controls overfitting and enhances generalization performance when learning with noisy labels. 

B. Manifold Intrusion Detection 

This section presents ablation studies on the manifold intrusion detection strategy. The experimental results of the statistical comparison on the training set and test set are shown in Fig. 14(a) and Fig. 14(b) , respectively. These results indicate that the proposed manifold intrusion detection strategy significantly improves the test R2 scores on 6 datasets and performs worse on only 2 datasets. When examining the train-ing R2 scores, the manifold intrusion detection mechanism improves performance on 16 datasets and does not worsen performance on any dataset. These results confirm that the improvements from manifold intrusion detection are primarily driven by the reduction of regularization effects introduced by incorrectly synthesized samples, which boosts training performance and, in turn, improves generalization. 

C. Noise Estimation Strategy 

In this paper, we propose decomposing vicinal risk into empirical loss and regularization terms. We then design a noise estimation strategy based on the cross-validation loss of extremely randomized trees to determine the coefficient for weighting the vicinal Jensen gap. The experimental results for training R2 scores and test R2 scores with and without the proposed adaptive strategy are shown in Fig. 15(a) and JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13                  

> 
> 
> (")')+#,-
> .-+ +##,
> 
> " )),-
> 
> & ,-%!#-
> 
> 
> .!#&)+'#+
> ,,) +,
> %(# +#$+#,,%)(
> *#+)(
> #+(#&%"$#
> 
> R
> 2
> #,-
>   
> )"#&%/#
> 

   

   

> +%(%($ %'#,

Fig. 11: Mean R2 scores, model sizes, and training time of various algorithms across 58 regression problems. (Stars indicate symbolic regression algorithms.)   &&)* #)*!* +#  &($( +*( ()  (%#!   ))&() !%(  ())!&%  '(&% %&$  &()*    #)*!* +# &($( +*( ()  (%#!  ))&() !%( ())!&%  '(&% %&$ &()*               

> !#&+&% )! %(%" *)*  )*  Œ±= 
> %& )! %!!%
>  Œ±
>    Œ±
>    Œ±
>    Œ±

Fig. 12: Pairwise statistical comparisons of test R2 scores on 58 regression problems.                 

> 
>    
>   
> 
> 
> 
>   
>   
> 
>  

Fig. 13: Statistical comparison of test R2 scores when learning with noisy labels for different algorithms. Fig. 15(b) , respectively. These results show that adjusting   

> 
> 
> 
>           
>   

(a) Training R2 scores.    

> 
> 
> 
>           
>  

(b) Test R2 scores. 

Fig. 14: Performance comparison with and without manifold intrusion detection. (‚Äú+‚Äù,‚Äú ‚àº‚Äù, and ‚Äú-‚Äù represent the number of datasets where using manifold intrusion detection performs better, similar to, or worse than not using it.) the regularization weight based on dataset characteristics leads to significantly better performance on 12 datasets and only worse performance on one dataset. These results validate the effectiveness of our noise estimation strategy and demonstrate the necessity of decomposing vicinal risk into empirical loss and the vicinal Jensen gap. Without such decomposition, there would be no opportunity to set a weight. 

D. Visualization of the Final Models 

Table IV and Table V present a visualization of the final constructed features and their corresponding coefficients in the linear model for the dataset ‚ÄúOpenML 547‚Äù, representing the constructed features using VJM-GP and standard GP, respec-tively. This real-world dataset has practical value. Specifically, task aims to predict the concentration of NO2 at each hour in Oslo, Norway. The first original feature, X0, represents the number of cars per hour, which is intuitively positively correlated with the concentration of NO2. The features constructed by standard GP include subtrees such as Cos(Min(0.93, X0)) , which lack plausible jus-tification, as there is no reasonable basis for the number of JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14   

> 
> 
> 
> 
>        
>  

(a) Training R2 scores.    

> 
> 
> 
>           
>   

(b) Test R2 scores. 

Fig. 15: Performance comparison with and without the noise estimation strategy. (‚Äú+‚Äù,‚Äú ‚àº‚Äù, and ‚Äú-‚Äù represent the number of datasets where using the noise estimation strategy performs better, similar, or worse, respectively, compared to not using it.) TABLE IV: An example of constructed features and their corresponding coefficients using VJM-GP.            

> Coefficient Feature 0.1692 Max(Neg(X1), Sub(Max(Add(X3, X6), Sub(X1, AQ(Max(X1, X5), X2))), X1)) 0.3367 Add(X0, 0.01) -0.1066 Min(Max(0.45, X5), Add(X5, Min(X2, 0.48))) 0.2449 Sub(X3, Abs(Min(X4, Sub(Max(Add(X3, X6), X4), 0.01)))) -0.0955 Abs(Abs(Min(Neg(Add(X5, X0)), -0.95))) -0.2833 AQ(Sub(Min(X2, X3), Neg(X1)), Max(X4, X2)) 0.3367 X0 0.1146 Max(X3, X4) -0.0836 X2 0.0219 Square(Square(X6))

TABLE V: An example of constructed features and their corresponding coefficients using Standard GP.            

> Coefficient Feature 0.5959 Max(Neg(X1), Sub(Neg(Add(X1, X0)), Sub(Min(X2, Log(Sub(X6, X4))), Add(X3, X6)))) 0.3167 Neg(X2) 0.1840 Log(AQ(X2, X0)) 0.1825 Sin(Abs(X4)) 0.1284 Add(Mul(X1, X4), Neg(X6)) 0.1437 Cos(Sub(X6, X4)) -0.1535 Square(Cos(Min(0.93, X0))) 0.9177 Neg(AQ(Neg(AQ(X0, Sin(Abs(X4)))), Sub(X2, X2))) -0.1040 X5 -0.0605 Sin(X6)

cars per hour to exhibit a sinusoidal correlation with NO2 concentration. In contrast, the features constructed by VJM-GP involve expressions such as Add(X0, 0.01) , which directly capture the expected positive relationship between NO2 concentration and the number of cars, aligning with domain knowledge. In terms of training performance, as shown in Table IV, VJM-GP achieves a training R2 score of 0.74, whereas standard GP achieves 0.82. However, for test performance, VJM-GP achieves a test R2 score of 0.46, while standard GP achieves 0.36. These results indicate that although standard GP performs better on the training set, it constructs unrealistic features and overfits the training data, leading to poor gener-alization to unseen data. These findings confirm that VJM-GP can discover features that align with domain knowledge, resulting in superior generalization performance. VIII. C ONCLUSIONS 

In this paper, we decompose vicinal risk into two compo-nents: empirical loss and regularization terms, specifically the vicinal Jensen gap and finite difference. This decomposition not only provides deeper insight into the mechanisms behind vicinal risk estimation but also offers an opportunity to de-sign a regularized GP framework for controlling overfitting in evolutionary feature construction algorithms. Additionally, the decomposition enables us to develop a noise estimation strategy to adjust the regularization pressure based on the noise level of datasets. To avoid manifold intrusion from incorrect data augmentation, we propose an intrusion detection strategy. With these improvements, the experimental results show that optimizing training error and the Jensen gap leads to better generalization performance compared with standard GP and training with seven other complexity measures, in-cluding directly applying mixup-based VRM to GP without decomposition. Furthermore, our findings suggest that effec-tive overfitting control requires a focus on reducing functional complexity, and merely controlling model size is insufficient. Ablation studies demonstrate that the noise estimation strategy significantly improves generalization performance for datasets with different levels of complexity, and the studies on manifold intrusion detection show the necessity of avoiding manifold intrusion in data synthesis. In this work, we primarily explore the effectiveness of the proposed method on GP-based feature construction algorithms. To improve efficiency, the proposed method can be accelerated using caching techniques to avoid re-evaluating performance on repeated GP trees, thereby re-ducing computational costs. Moreover, with the recent surge in using deep learning to solve symbolic regression tasks, it would be interesting to investigate whether the proposed method can enhance the generalization performance of equa-tion learner-based [50], Transformer-based [51], or reinforce-ment learning-based [52] symbolic regression methods. REFERENCES [1] H. Zhang, A. Zhou, and H. Zhang, ‚ÄúAn evolutionary forest for regres-sion,‚Äù IEEE Transactions on Evolutionary Computation , vol. 26, no. 4, pp. 735‚Äì749, 2022. [2] W. La Cava and J. H. Moore, ‚ÄúLearning feature spaces for regression with genetic programming,‚Äù Genetic Programming and Evolvable Ma-chines , vol. 21, pp. 433‚Äì467, 2020. [3] L. Vanneschi, M. Castelli, and S. Silva, ‚ÄúMeasuring bloat, overfitting and functional complexity in genetic programming,‚Äù in Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation ,2010, pp. 877‚Äì884. [4] Q. Chen, M. Zhang, and B. Xue, ‚ÄúStructural risk minimization-driven genetic programming for enhancing generalization in symbolic regres-sion,‚Äù IEEE Transactions on Evolutionary Computation , vol. 23, no. 4, pp. 703‚Äì717, 2018. [5] Q. Chen, B. Xue, and M. Zhang, ‚ÄúRademacher complexity for enhancing the generalization of genetic programming for symbolic regression,‚Äù 

IEEE Transactions on Cybernetics , vol. 52, no. 4, pp. 2382‚Äì2395, 2022. [6] Y. Bi, B. Xue, and M. Zhang, ‚ÄúGenetic programming-based evolutionary deep learning for data-efficient image classification,‚Äù IEEE Transactions on Evolutionary Computation , vol. 28, no. 2, pp. 307‚Äì322, 2022. [7] X. Wu, D. Wang, H. Chen, L. Yan, Y. Xiao, C. Miao, H. Ge, D. Xu, Y. Liang, K. Wang et al. , ‚ÄúNeural architecture search for text classi-fication with limited computing resources using efficient cartesian ge-netic programming,‚Äù IEEE Transactions on Evolutionary Computation ,vol. 28, no. 3, pp. 638‚Äì652, 2024. [8] O. Chapelle, J. Weston, L. Bottou, and V. Vapnik, ‚ÄúVicinal risk mini-mization,‚Äù Advances in Neural Information Processing Systems , vol. 13, pp. 416‚Äì422, 2000. [Online]. Available: https://proceedings.neurips.cc/ paper/2000/hash/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Abstract.html [9] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, ‚Äúmixup: Beyond empirical risk minimization,‚Äù in International Conference on Learning Representations . OpenReview.net, 2018. [10] M. Sajjadi, M. Javanmardi, and T. Tasdizen, ‚ÄúRegularization with stochastic transformations and perturbations for deep semi-supervised JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15 

learning,‚Äù Advances in Neural Information Processing Systems , vol. 29, pp. 1163‚Äì1171, 2016. [Online]. Available: https://proceedings.neurips. cc/paper/2016/hash/30ef30b64204a3088a26bc2e6ecf7602-Abstract.html [11] D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, and B. Lakshminarayanan, ‚ÄúAugmix: A simple data processing method to improve robustness and uncertainty,‚Äù in International Conference on Learning Representations . OpenReview.net, 2020. [Online]. Available: https://openreview.net/forum?id=S1gmrxHFvB [12] H. Yao, Y. Wang, L. Zhang, J. Y. Zou, and C. Finn, ‚ÄúC-mixup: Improving generalization in regression,‚Äù Advances in Neural Information Processing Systems , vol. 35, pp. 3361‚Äì3376, 2022. [13] J. Ni and P. Rockett, ‚ÄúTraining genetic programming classifiers by vicinal-risk minimization,‚Äù Genetic Programming and Evolvable Ma-chines , vol. 16, pp. 3‚Äì25, 2015. [14] Y. Cao and P. I. Rockett, ‚ÄúThe use of vicinal-risk minimization for training decision trees,‚Äù Applied Soft Computing , vol. 31, pp. 185‚Äì195, 2015. [15] A. Agapitos, R. Loughran, M. Nicolau, S. Lucas, M. O‚ÄôNeill, and A. Brabazon, ‚ÄúA survey of statistical machine learning elements in ge-netic programming,‚Äù IEEE Transactions on Evolutionary Computation ,vol. 23, no. 6, pp. 1029‚Äì1048, 2019. [16] J. Ni and P. Rockett, ‚ÄúTikhonov regularization as a complexity measure in multiobjective genetic programming,‚Äù IEEE Transactions on Evolu-tionary Computation , vol. 19, no. 2, pp. 157‚Äì166, 2014. [17] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, ‚ÄúUn-derstanding deep learning (still) requires rethinking generalization,‚Äù 

Communications of the ACM , vol. 64, no. 3, pp. 107‚Äì115, 2021. [18] H. Zhang, A. Zhou, Q. Chen, B. Xue, and M. Zhang, ‚ÄúSR-Forest: A genetic programming based heterogeneous ensemble learning method,‚Äù 

IEEE Transactions on Evolutionary Computation , vol. 28, no. 5, pp. 1484‚Äì1498, 2024. [19] C. A. Owen, G. Dick, and P. A. Whigham, ‚ÄúCharacterizing genetic programming error through extended bias and variance decomposition,‚Äù 

IEEE Transactions on Evolutionary Computation , vol. 24, no. 6, pp. 1164‚Äì1176, 2020. [20] I. Gonc ¬∏alves and S. Silva, ‚ÄúBalancing learning and overfitting in genetic programming with interleaved sampling of training data,‚Äù in Genetic Programming: 16th European Conference, EuroGP 2013, Vienna, Aus-tria, April 3-5, 2013. Proceedings 16 . Springer, 2013, pp. 73‚Äì84. [21] C. Tuite, A. Agapitos, M. O‚ÄôNeill, and A. Brabazon, ‚ÄúEarly stopping cri-teria to counteract overfitting in genetic programming,‚Äù in Proceedings of the 13th Annual Conference Companion on Genetic and Evolutionary Computation , 2011, pp. 203‚Äì204. [22] S. Silva, L. Vanneschi, A. I. Cabral, and M. J. Vasconcelos, ‚ÄúA semi-supervised genetic programming method for dealing with noisy labels and hidden overfitting,‚Äù Swarm and Evolutionary Computation , vol. 39, pp. 323‚Äì338, 2018. [23] L. Vanneschi and M. Castelli, ‚ÄúSoft target and functional complexity reduction: A hybrid regularization method for genetic programming,‚Äù 

Expert Systems with Applications , vol. 177, p. 114929, 2021. [24] J. Ma, X. Gao, and Y. Li, ‚ÄúMulti-generation multi-criteria feature construction using genetic programming,‚Äù Swarm and Evolutionary Computation , vol. 78, p. 101285, 2023. [25] K. Neshatian, M. Zhang, and P. Andreae, ‚ÄúA filter approach to multiple feature construction for symbolic learning classifiers using genetic pro-gramming,‚Äù IEEE Transactions on Evolutionary Computation , vol. 16, no. 5, pp. 645‚Äì661, 2012. [26] L. MuÀú noz, L. Trujillo, S. Silva, M. Castelli, and L. Vanneschi, ‚ÄúEvolving multidimensional transformations for symbolic regression with m3gp,‚Äù 

Memetic Computing , vol. 11, pp. 111‚Äì126, 2019. [27] M. Virgolin, T. Alderliesten, and P. A. Bosman, ‚ÄúOn explaining machine learning models by evolving crucial and compact features,‚Äù Swarm and Evolutionary Computation , vol. 53, p. 100640, 2020. [28] C. Wang, Q. Chen, B. Xue, and M. Zhang, ‚ÄúShapley value based feature selection to improve generalization of genetic programming for high-dimensional symbolic regression,‚Äù in Australasian Conference on Data Science and Machine Learning . Springer, 2023, pp. 163‚Äì176. [29] H. Zhang, Q. Chen, B. Xue, W. Banzhaf, and M. Zhang, ‚ÄúBias-variance decomposition: An effective tool to improve generalization of genetic programming-based evolutionary feature construction for regression,‚Äù in 

Proceedings of the Genetic and Evolutionary Computation Conference ,2024, pp. 998‚Äì1006. [30] I. Bakurov, N. Haut, and W. Banzhaf, ‚ÄúSharpness-aware minimization in genetic programming,‚Äù arXiv preprint arXiv:2405.10267 , 2024. [31] W. La Cava, T. Helmuth, L. Spector, and J. H. Moore, ‚ÄúA probabilistic and multi-objective analysis of lexicase selection and Œµ-lexicase selec-tion,‚Äù Evolutionary Computation , vol. 27, no. 3, pp. 377‚Äì402, 2019. [32] T. Hastie, R. Tibshirani, J. H. Friedman, and J. H. Friedman, The elements of statistical learning: Data mining, inference, and prediction .Springer, 2009. [33] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, ‚ÄúA fast and elitist multiobjective genetic algorithm: Nsga-ii,‚Äù IEEE Transactions on Evo-lutionary Computation , vol. 6, no. 2, pp. 182‚Äì197, 2002. [34] P. Geurts, D. Ernst, and L. Wehenkel, ‚ÄúExtremely randomized trees,‚Äù 

Machine learning , vol. 63, no. 1, pp. 3‚Äì42, 2006. [35] H. Guo, Y. Mao, and R. Zhang, ‚ÄúMixup as locally linear out-of-manifold regularization,‚Äù in Proceedings of the AAAI conference on artificial intelligence , vol. 33, no. 01, 2019, pp. 3714‚Äì3722. [36] R. S. Olson, W. La Cava, P. Orzechowski, R. J. Urbanowicz, and J. H. Moore, ‚ÄúPMLB: a large benchmark suite for machine learning evaluation and comparison,‚Äù BioData mining , vol. 10, no. 1, pp. 1‚Äì13, 2017. [37] J. Ni, R. H. Drieberg, and P. I. Rockett, ‚ÄúThe use of an analytic quotient operator in genetic programming,‚Äù IEEE Transactions on Evolutionary Computation , vol. 17, no. 1, pp. 146‚Äì152, 2012. [38] M. Nicolau and A. Agapitos, ‚ÄúChoosing function sets with better generalisation performance for symbolic regression models,‚Äù Genetic programming and evolvable machines , vol. 22, no. 1, pp. 73‚Äì100, 2021. [39] D. Micci-Barreca, ‚ÄúA preprocessing scheme for high-cardinality categor-ical attributes in classification and prediction problems,‚Äù ACM SIGKDD Explorations Newsletter , vol. 3, no. 1, pp. 27‚Äì32, 2001. [40] H. Zhang, Q. Chen, B. Xue, W. Banzhaf, and M. Zhang, ‚ÄúP-mixup: Im-proving generalization performance of evolutionary feature construction with pessimistic vicinal risk minimization,‚Äù in International Conference on Parallel Problem Solving from Nature . Springer, 2024, pp. 201‚Äì220. [41] F. O. de Franc ¬∏a, ‚ÄúAlleviating overfitting in transformation-interaction-rational symbolic regression with multi-objective optimization,‚Äù Genetic Programming and Evolvable Machines , vol. 24, no. 2, p. 13, 2023. [42] Q. Chen, B. Xue, and M. Zhang, ‚ÄúImproving symbolic regression based on correlation between residuals and variables,‚Äù in Proceedings of the 2020 Genetic and Evolutionary Computation Conference , 2020, pp. 922‚Äì930. [43] W.-Y. Chiu, G. G. Yen, and T.-K. Juan, ‚ÄúMinimum manhattan distance approach to multiple criteria decision making in multiobjective opti-mization problems,‚Äù IEEE Transactions on Evolutionary Computation ,vol. 20, no. 6, pp. 972‚Äì985, 2016. [44] W. L. Cava, P. Orzechowski, B. Burlacu, F. O. de Franca, M. Virgolin, Y. Jin, M. Kommenda, and J. H. Moore, ‚ÄúContemporary symbolic regres-sion methods and their relative performance,‚Äù in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1) , 2021. [45] C. Haider, F. O. de Franca, B. Burlacu, and G. Kronberger, ‚ÄúShape-constrained multi-objective genetic programming for symbolic regres-sion,‚Äù Applied Soft Computing , vol. 132, p. 109855, 2023. [46] F. O. De Franc ¬∏a, ‚ÄúTransformation-interaction-rational representation for symbolic regression: a detailed analysis of srbench results,‚Äù ACM Transactions on Evolutionary Learning , vol. 3, no. 2, pp. 1‚Äì19, 2023. [47] B. Burlacu, G. Kronberger, and M. Kommenda, ‚ÄúOperon c++ an efficient genetic programming framework for symbolic regression,‚Äù in Proceed-ings of the 2020 Genetic and Evolutionary Computation Conference Companion , 2020, pp. 1562‚Äì1570. [48] T. Chen and C. Guestrin, ‚ÄúXgboost: A scalable tree boosting system,‚Äù in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2016, pp. 785‚Äì794. [49] J. Chen, J. Yan, Q. Chen, D. Z. Chen, J. Wu, and J. Sun, ‚ÄúCan a deep learning model be a sure bet for tabular prediction?‚Äù in Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , 2024, pp. 288‚Äì296. [50] J. Dong, J. Zhong, W.-L. Liu, and J. Zhang, ‚ÄúEvolving equation learner for symbolic regression,‚Äù IEEE Transactions on Evolutionary Computation , 2024. [51] P.-A. Kamienny, S. d‚ÄôAscoli, G. Lample, and F. Charton, ‚ÄúEnd-to-end symbolic regression with transformers,‚Äù Advances in Neural Information Processing Systems , vol. 35, pp. 10 269‚Äì10 281, 2022. [52] T. Mundhenk, M. Landajuela, R. Glatt, C. P. Santiago, B. K. Petersen 

et al. , ‚ÄúSymbolic regression via deep reinforcement learning enhanced genetic programming seeding,‚Äù Advances in Neural Information Pro-cessing Systems , vol. 34, pp. 24 912‚Äì24 923, 2021. [53] M. Kotanchek, G. Smits, and E. Vladislavleva, ‚ÄúPursuing the pareto paradigm: tournaments, algorithm variations and ordinal optimization,‚Äù 

Genetic Programming Theory and Practice IV , pp. 167‚Äì185, 2007. [54] H. Zhang, Q. Chen, B. Xue, W. Banzhaf, and M. Zhang, ‚ÄúAutomatically choosing selection operator based on semantic information in evolution-ary feature construction,‚Äù in Pacific Rim International Conference on Artificial Intelligence . Springer, 2023, pp. 385‚Äì397. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16 

# Supplementary Materials for ‚ÄúEnhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization‚Äù 

Hengzhe Zhang, Qi Chen, Member, IEEE, Bing Xue, Fellow, IEEE, Wolfgang Banzhaf, Member, IEEE, Mengjie Zhang, Fellow, IEEE 

APPENDIX APROOF OF THEOREM 

A. Proof of Theorem 1 Proof. The squared loss for the vicinal sample, (yvic ‚àí

f (xvic )) 2, is expanded as follows: 

(yvic ‚àí f (xvic )) 2

= [ yi ‚àí f (xi + œµ)] 2

= [ yi ‚àí f (xi) + f (xi) ‚àí f (xi + œµ)] 2

= ( yi ‚àí f (xi)) 2 + 2( yi ‚àí f (xi))( f (xi) ‚àí f (xi + œµ)) + ( f (xi) ‚àí f (xi + œµ)) 2

‚â§ (yi ‚àí f (xi)) 2 + ( yi ‚àí f (xi)) 2

+ ( f (xi) ‚àí f (xi + œµ)) 2 + ( f (xi) ‚àí f (xi + œµ)) 2

= 2( yi ‚àí f (xi)) 2 + 2[ f (xi) ‚àí f (xi + œµ)] 2.

(24) 

B. Proof of Theorem 2 Proof. The squared error on the vicinal example is given by 

(yvic ‚àí f (xvic )) 2. We can decompose this error as follows: 

(yvic ‚àí f (xvic )) 2 = [ Œªy i + (1 ‚àí Œª)yj ‚àí f (Œªx i + (1 ‚àí Œª)xj )] 2

= [ Œª(yi ‚àí f (xi)) + (1 ‚àí Œª)( yj ‚àí f (xj )) + ( Œªf (xi) + (1 ‚àí Œª)f (xj ) ‚àí f (xvic ))] 2

= [ Œª(yi ‚àí f (xi)) + (1 ‚àí Œª)( yj ‚àí f (xj )) +Œªf (xi) + (1 ‚àí Œª)f (xj ) ‚àí f (xvic )] 2

= [ Œª(yi ‚àí f (xi)) + (1 ‚àí Œª)( yj ‚àí f (xj )) + ( Œªf (xi) + (1 ‚àí Œª)f (xj ) ‚àí f (xvic ))] 2

‚â§ 3 ( Œª(yi ‚àí f (xi))) 2 + 3 ((1 ‚àí Œª)( yj ‚àí f (xj ))) 2

+ 3 ( Œªf (xi) + (1 ‚àí Œª)f (xj ) ‚àí f (xvic )) 2 ,

(25) where the inequality follows from the fact that (a + b + c)2 ‚â§

3a2 + 3 b2 + 3 c2 for any real numbers a, b, and c.APPENDIX BAN INTUITIVE EXAMPLE OF VICINAL JENSEN GAP 

REGULARIZATION 

To understand how the derived vicinal Jensen gap influences overfitting control, we analyze the vicinal risk decomposition for both a highly complex function and a smooth function,                       

> x
> 
> 
> (x)
> (x)=1.0 x+1. 2+1.5sin( x )+1.0sin(2 x )
>    
>    

(a) Rugged Symbolic Model       

> 3 Œª
>  
> 

    

> 3   Œª
> 
> 

  

> 3Œî

        

>    
>   
>       
> 

(b) Loss Components                       

> x
> 
> 
> 
> 
> 
> (x)
> (x)=1.0 x+1. 2+0.5sin(
> x
> 2
> )+0. 2sin(
> x
> 4
> )
>    
>    

(c) Smooth Symbolic Model       

> 3 Œª
>  
> 

    

> 3   Œª
> 
> 

  

> 3Œî

      

>        
>   
>        
> 

(d) Loss Components 

Fig. 16: Comparison of rugged and smooth symbolic models with vicinal Jensen gap decomposition. as shown in Fig. 16b and Fig. 16d, respectively. For sim-plicity, we define Œ¥i = yi ‚àí fxi , Œ¥j = yj ‚àí fxj , and 

‚àÜ = Œª ¬∑ fxi + (1 ‚àí Œª) ¬∑ fxj ‚àí fvic . The training samples, presented in Fig. 16a and Fig. 16c, are consistent across both cases. While the empirical loss, as shown in Fig. 16b and Fig. 16d, does not demonstrate the advantage of the smooth function, the vicinal Jensen gap provides clearer insight. The smooth function exhibits a significantly smaller vicinal Jensen gap compared to the complex function, suggesting that minimizing the vicinal Jensen gap can effectively promote smoother functions, thereby reducing overfitting. APPENDIX CKERNEL ANALYSIS 

In this paper, the closeness between instances is measured using the kernel defined in Equation (12), which simplifies the distance metric D(( xi, y i), (xj , y j )) to D(yi, y j ). For regression problems where the feature space X has low dimensionality, the distance can alternatively be measured using D(( xi, y i), (xj , y j )) . Specifically, the kernel is defined as: 

D(( xi, y i), (xj , y j )) = exp  ‚àíŒ≥‚à•(xi, y i) ‚àí (xj , y j )‚à•2 ,

(26) where ‚à•(xi, y i) ‚àí (xj , y j )‚à•2 indicates that the features X are concatenated with the labels Y before computing the distance. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17   

> 
> 
> 
>           
>  

Fig. 17: Statistical comparison of test R2 scores when using Mix Y instead of Mix XY .This method is referred to as Mix XY , in contrast to Mix Y ,which uses only the label Y for distance computation. In theory, distinct assumptions underlie Mix Y and Mix XY . In Mix Y , two points are sampled based on their Y values, and linear regularization is applied between these points. This allows the algorithm to synthesize instances that may be distant in feature space, with the vicinal Jensen gap applied over a larger region. This is particularly important in cases where instances are often sparsely distributed. For instance, as shown in Fig. 18, consider a one-dimensional case with four samples: (1 , 1) , (1 .1, 1.1) , (5 , 1.1) , and (5 .1, 1.2) . Here, Mix Y can synthesize a sample between (1 .1, 1.1) and (5 , 1.1) ,achieving regularization, whereas Mix XY cannot. In contrast, Mix XY requires closeness in both the Y and X

spaces, making manifold intrusion less likely. For example, as shown in Fig. 19, in a case with samples (1 , 1) , (2 , 5) ,and (3 , 1) , Mix Y may interpolate between (1 , 1) and (3 , 1) ,potentially leading to manifold intrusion. In contrast, Mix XY 

is less prone to this issue, as (1 , 1) and (3 , 1) are far apart when the X space is taken into account. To address the problem of manifold intrusion, a detection mechanism is proposed in Section IV-D. However, this mechanism may not detect all instances of manifold intrusion, as some intrusion points may lie within the confidence bound defined in Equation (19). The experimental results, presented in Fig. 17, demonstrate that both Mix Y and Mix XY are suited for specific types of datasets. Specifically, Mix Y performs significantly better on 8 datasets, but performs worse on 5 datasets. This confirms that different distance kernels rely on different assumptions, and selecting the most suitable kernel based on an analysis of the data‚Äîsuch as assessing whether the data are sparsely distributed in feature space‚Äîcan lead to better results when using VJM-GP. APPENDIX DCOMPARISON WITH SAME RUNTIME BUDGET 

As shown in Fig. 10(b) , the proposed method requires more training time compared to baseline methods such as PP. However, overfitting is a complex issue that cannot be resolved merely by allocating more training time. To investigate this, we compared the proposed method against PP and standard GP under an identical runtime budget. Specifically, all methods were allowed to run for an unlimited number of generations within a runtime limit of 1000 seconds, as shown in Fig. 20. Under these conditions, the statistical comparison of test 1 2 3 4 5

X

1.00 

1.05 

1.10 

1.15 

1.20    

> Y
> Mix
> XY Cannot Achieve Regularization

Samples Synthesized Point 

Fig. 18: A case where Mix Y achieves sufficient regularization, but Mix XY lacks regularization strength. The synthesized point in this example is generated by Mix Y .1.0 1.5 2.0 2.5 3.0 

X

1

2

3

4

5    

> Y
> Mix
> YLeading to Manifold Intrusion

Samples Synthesized Point 

Fig. 19: A case where Mix Y leads to manifold intrusion.   

> 
> 
> 
>  

Fig. 20: Comparison of training time between the proposed method and baseline methods under the same runtime budget. The minor difference arises because the program waits for the completion of the current generation before terminating after reaching the time limit. TABLE VI: Statistical comparison of test R2 scores between the proposed method and baseline methods under the same runtime budget. 

PP Standard GP VJM 34(+)/20( ‚àº)/4(-) 39(+)/14( ‚àº)/5(-) 

PP ‚Äî 27(+)/25( ‚àº)/6(-) 

R2 scores is presented in Table VI. The results show that even with an increased number of generations to achieve the same runtime budget, both PP and standard GP remain inferior to VJM-GP. Consequently, the necessity of employing VJM to mitigate overfitting is validated, despite its higher computational cost. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 18 

Algorithm 2 Vicinal Jensen Gap/Finite Difference Minimiza-tion with Early Stopping 

Require: GP Tree Œ¶, Input X, Target Outputs Y , Linear Model F , Number of Iterations K, Current Best Individual 

Œ¶Best  

> 1:

Initialize Regularization Term: V ‚Üê 0 

> 2:

Œ¶( X) ‚Üê Feature Construction ( X, Œ¶) 

> 3:

for k = 1 , . . . , K do  

> 4:

Xvic , Y vic ‚Üê Data Synthesis ( X, Y, k ) ‚ñ∑ Cached  

> 5:

Œ¶( Xvic ) ‚Üê Feature Construction ( Xvic , Œ¶) 

> 6:

ÀÜY ‚Üê Prediction( F, Œ¶( Xvic )) 

> 7:

for i = 1 , . . . , N do  

> 8:

Vi ‚Üê max( Vi, (ÀÜ yki ‚àí ykvic )2) 

> 9:

Vcurrent = 1

> N

PNi=1 Vi 

> 10:

if O1(Œ¶) + œÑ Vcurrent > O 1(Œ¶ Best ) + œÑ O 2(Œ¶ Best )

then  

> 11:

return Vcurrent ‚ñ∑ Terminate Early 

Ensure: Regularization Term V = 1

> N

PNi=1 Vi

APPENDIX ESPEEDING UP VICINAL JENSEN GAP ESTIMATION WITH 

EARLY STOPPING 

A. Early Stopping Algorithm 

For each synthesized vicinal data point, the feature construc-tion process must be executed, making the vicinal Jensen gap estimation process time-consuming. However, not all individu-als require a full evaluation. The following two characteristics of VJM-GP can be leveraged to accelerate this process:  

> ‚Ä¢

Best Objective-Driven Selection: The final model is se-lected based on the individual with the best combination of two objective values, as defined in Equation (9), over the entire evolutionary process.  

> ‚Ä¢

Monotonicity: The vicinal Jensen gap, O2(Œ¶) , monoton-ically increases with the number of estimation rounds k.Assume the historically best individual is Œ¶Best . For k =2, . . . , K , if the current individual Œ¶ has a combined objective value O1(Œ¶) + œÑ Vcurrent that is higher than O1(Œ¶ Best ) + 

œÑ O 2(Œ¶ Best ), the vicinal Jensen gap estimation for Œ¶ can be terminated early, as the individual cannot outperform the best individual. To ensure that the second objective, O2(Œ¶) , is not entirely unknown, at least one round of vicinal Jensen gap esti-mation is conducted for every individual. This is necessary for multi-objective optimization, where all individuals must have defined objective values to guide the evolutionary progress. The pseudocode for this approach is provided in Algorithm 2. 

B. Results 

To examine the efficiency improvement of the proposed early stopping strategy, a comparison of time costs is presented in Fig. 21. The standard VJM-GP takes 1058 seconds on average across 58 datasets, while VJM-GP with the early stopping strategy reduces this to 381 seconds on average. This demonstrates that the early stopping strategy signifi-cantly reduces computational costs, achieving approximately a VJM  

> EarlyStop
>  
>    

Fig. 21: Comparison of training time with and without the early stopping technique. TABLE VII: Statistical comparison of test R2 scores with and without the early stopping technique.      

> VJM PP VJM EarlyStop 4(+)/43( ‚àº)/11(-) 29(+)/25( ‚àº)/4(-)
> VJM ‚Äî29(+)/27( ‚àº)/2(-)

threefold acceleration. Thanks to this acceleration, the gap in computational time compared to PP has been largely reduced. Regarding test R2 scores, a comparison is presented in Table VII. The proposed method performs significantly better than the version without the speedup strategy on 4 datasets and worse on 11 datasets, indicating a slight trade-off in accuracy. Compared with PP, the proposed method is still significantly better on 29 datasets and significantly worse on 4 datasets only. These results indicate that the early-stop of VJM-GP is practically useful when training time is constrained. The slight reduction in test R2 scores introduced by the speedup strategy arises because early-stopped individuals may have inaccurately estimated vicinal Jensen gaps. This inaccuracy can limit the evolutionary algorithm‚Äôs ability to identify models with low vicinal Jensen gaps. Nevertheless, the results demonstrate that the speedup strategy effectively balances computational efficiency and predictive performance. In the future, exploring the use of surrogate models to more accurately estimate the vicinal Jensen gap could further improve both the accuracy and efficiency of the estimation process, potentially leading to better overall results. APPENDIX FSELECTION OPERATORS 

In this section, we compare three different selection op-erators: dominance-based tournament selection in NSGA-II, Pareto tournament selection, and lexicase selection.  

> ‚Ä¢

Dominance-based Tournament Selection (DTS) [33]: 

This method uses the dominance relationship to select individuals. Specifically, two individuals are randomly selected from the population, and the one that dominates the other is chosen. If neither dominates, one is selected randomly.  

> ‚Ä¢

Pareto Tournament Selection (PTS) [53]: The key idea of Pareto tournament selection is to select a subset of individuals from the current population, with the subset size set to 10% of the population in this paper. These individuals are then sorted based on two objectives, and the entire first Pareto front is selected. Since the crossover JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 19    

> 
> 
> 
> 
>  

Fig. 22: Comparison of using different selection operators. operation requires pairs of individuals, the total number of selected individuals is rounded down to the nearest even number to ensure divisibility by two.  

> ‚Ä¢

Lexicase Selection: Lexicase selection is used as the baseline operator in this comparison. As described in Section IV-A, this paper employs automatic œµ-lexicase selection, a variant tailored for regression tasks. For dominance-based and Pareto tournament selection, the leave-one-out cross-validation loss and the vicinal Jensen gap are used as the two objectives, consistent with the objective values employed by the environmental selection operator. The statistical comparison of test R2 scores is summarized in Table VIII. The results indicate that lexicase selection outper-forms dominance-based tournament selection on 13 datasets while performing worse on 5 datasets. Similarly, lexicase se-lection outperforms Pareto tournament selection on 10 datasets and underperforms on 2 datasets. These observations suggest that performance differences between the selection operators are evident but limited to a subset of datasets. Generally, dominance-based and Pareto tournament selection rely on aggregated fitness scores for optimization, which tends to favor generalists. In contrast, lexicase selection fully lever-ages semantic information to prioritize specialists. By doing so, lexicase selection helps identify building blocks that are useful for specific instances, which the crossover operator can subsequently combine into better solutions, thereby escaping local optima. However, when comparing training time, Pareto tournament selection is significantly faster than lexicase selec-tion, as shown in Fig. 22. This lower training time is not only due to the selection process of Pareto tournament being more time-efficient than lexicase selection but also because Pareto tournament selection explicitly leverages low-complexity solu-tions to generate offspring, thereby reducing the time required for fitness evaluation. It is important to note that lexicase selection and Pareto tournament selection are not mutually exclusive, and further investigations on their comparisons are still needed. In fact, it may be more beneficial to combine both methods through an adaptive operator selection mechanism that dynamically determines the most appropriate selection operator based on evolutionary information [54]. APPENDIX GPARAMETER ANALYSIS 

A. Alpha in Beta Distribution 

The alpha value in the Beta distribution determines the ratio of synthetic samples Œª used in synthesizing vicinal samples. TABLE VIII: Statistical comparison of test R2 scores using different selection operators.      

> DTS Lexicase PTS 0(+)/52( ‚àº)/6(-) 5(+)/40( ‚àº)/13(-)
> DTS ‚Äî2(+)/46( ‚àº)/10(-)

TABLE IX: Statistical comparison of test R2 scores using different alpha values in the Beta distribution.                    

> 100 110 0(+)/58( ‚àº)/0(-) 0(+)/56( ‚àº)/2(-)
> 100 ‚Äî1(+)/57( ‚àº)/0(-)       
> 
> 
> 
> 
>             
>       
>       

Fig. 23: Probability density functions of the Beta distribution for different values of Œ± = Œ≤.Specifically, a larger alpha value indicates that synthetic sam-ples are generated from a more balanced mix of two original samples, whereas a smaller alpha means that synthetic samples are predominantly derived from a single source sample, as illustrated in Fig. 23. The experimental results presented in Table IX demonstrate that the sensitivity of the R2 scores to alpha is minimal. For example, increasing alpha from 10 to 100 leads to no significant change in the final R2 score. Therefore, in real-world applications, an alpha value of 10 is deemed sufficient. 

B. Spread of Sampling 

The spread parameter Œ≥ determines the range for defining vicinal examples. As shown in Fig. 24, a smaller spread pa-rameter includes more distant examples as neighbors, applying Jensen gap regularization to broader regions. Conversely, a larger spread parameter restricts focus to closer examples, so the regularization primarily smooths over nearby samples. The experimental results for different spread parameters are presented in Table X. The test R2 scores indicate that VJM is generally insensitive to the selection of the spread parameter. For instance, reducing the spread parameter from 0.5 to 0.1 results in a noticeable difference in only 4 datasets. This insensitivity suggests that, for real-world applications, a spread parameter of 0.5 is sufficient. 

C. œÉ in Finite Difference 

The parameter œÉ in finite difference-based vicinal data synthesis determines the magnitude of noise added to the synthesized data. In this section, we analyze the sensitivity of test R2 scores to different œÉ values. Specifically, œÉ values of JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 20 ‚àí ‚àí   

ya

‚àí 

‚àí 





 

> yb
>  

 

 

 

 

 

 

 

   

‚àí ‚àí   

ya

‚àí 

‚àí 





 

> yb
>  

 

 

 

 

   

‚àí ‚àí   

ya

‚àí 

‚àí 





 

> yb
>  

 

 

 

 

   

Fig. 24: Kernel values for different bandwidths Œ≥ for pairs of ya and yb.TABLE X: Statistical comparison of test R2 scores using different spread parameters. 

0.5 1.0 0.1 3(+)/54( ‚àº)/1(-) 1(+)/55( ‚àº)/2(-) 

0.5 ‚Äî 0(+)/57( ‚àº)/1(-) 

TABLE XI: Statistical comparison of test R2 scores using different œÉ values in finite difference-based synthesis. 

0.1 0.25 0.05 4(+)/45( ‚àº)/9(-) 25(+)/26( ‚àº)/7(-) 

0.1 ‚Äî 30(+)/23( ‚àº)/5(-) 

0.05, 0.1, and 0.25 are tested for comparison. The experimental results are presented in Table XI. The results indicate that vicinal data synthesis is sensitive to the choice of œÉ. For instance, œÉ = 0 .1 performs significantly better than œÉ = 0 .05 

on 9 datasets but worse on 4 datasets. Similarly, œÉ = 0 .1

outperforms œÉ = 0 .25 on 30 datasets but underperforms on 5 datasets. These results suggest that œÉ = 0 .1 serves as a good balance point in general. Compared with the vicinal Jensen gap, finite difference regularization appears more sensitive to the hyperparameter œÉ. A possible explanation is that, for real-world datasets, local linearity is common. Thus, encouraging the range [a, b ] or [a, b + œµ] to exhibit linearity does not make a substantial difference for small œµ. In contrast, finite differ-ence regularization imposes a stronger constraint. Therefore, regularizing [a, b ] or [a, b + œµ] to be constant can lead to more pronounced differences in outcomes. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 21 

TABLE XII: The hyperparameter spaces of machine learning methods used in grid search [44, 49]. 

Method Hyperparameters AdaBoost {‚Äòlearning rate‚Äô: (0.01, 0.1, 1.0, 10.0), ‚Äòn estimators‚Äô: (10, 100, 1000) }

KernelRidge {‚Äòkernel‚Äô: (‚Äòlinear‚Äô, ‚Äòpoly‚Äô, ‚Äòrbf‚Äô, ‚Äòsigmoid‚Äô), ‚Äòalpha‚Äô: (0.0001, 0.01, 0.1, 1), ‚Äògamma‚Äô: (0.01, 0.1, 1, 10) }

LGBM {‚Äòn estimators‚Äô: (10, 50, 100, 250, 500, 1000), ‚Äòlearning rate‚Äô: (0.0001, 0.01, 0.05, 0.1, 0.2), ‚Äòsubsample‚Äô: (0.5, 0.75, 1), ‚Äòboosting type‚Äô: (‚Äògbdt‚Äô, ‚Äòdart‚Äô, ‚Äògoss‚Äô) }

MLP {‚Äòactivation‚Äô: (‚Äòlogistic‚Äô, ‚Äòtanh‚Äô, ‚Äòrelu‚Äô), ‚Äòsolver‚Äô: (‚Äòlbfgs‚Äô, ‚Äòadam‚Äô, ‚Äòsgd‚Äô), ‚Äòlearning rate‚Äô: (‚Äòconstant‚Äô, ‚Äòinvscal-ing‚Äô, ‚Äòadaptive‚Äô) }

ExcelFormer {‚Äòmix type‚Äô: (‚Äòfeat mix‚Äô, ‚Äòhidden mix‚Äô, ‚Äònone‚Äô) }

RandomForest {‚Äòn estimators‚Äô: (10, 100, 1000), ‚Äòmin weight fraction leaf‚Äô: (0.0, 0.25, 0.5), ‚Äòmax features‚Äô: (‚Äòsqrt‚Äô, ‚Äòlog2‚Äô, None) }

ExtraTrees {‚Äòn estimators‚Äô: (10, 100, 1000), ‚Äòmin weight fraction leaf‚Äô: (0.0, 0.25, 0.5), ‚Äòmax features‚Äô: (‚Äòsqrt‚Äô, ‚Äòlog2‚Äô, None) }

LinearRegression N/A ElasticNet {‚Äòalpha‚Äô: (1e-06, 1e-04, 0.01, 1), ‚Äòl1 ratio‚Äô: (0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1) }

LassoLars {‚Äòalpha‚Äô: (1e-04, 0.001, 0.01, 0.1, 1) }

SVR {‚ÄòC‚Äô: (0.1, 1, 10, 100), ‚Äòepsilon‚Äô: (0.01, 0.1, 0.5, 1), ‚Äòkernel‚Äô: (‚Äòlinear‚Äô, ‚Äòrbf‚Äô, ‚Äòpoly‚Äô) }

KNeighbors {‚Äòn neighbors‚Äô: (1, 3, 5, 10), ‚Äòweights‚Äô: (‚Äòuniform‚Äô, ‚Äòdistance‚Äô), ‚Äòp‚Äô: (1, 2) }

XGB {‚Äòn estimators‚Äô: (10, 50, 100, 250, 500, 1000), ‚Äòlearning rate‚Äô: (0.0001, 0.01, 0.05, 0.1, 0.2), ‚Äògamma‚Äô: (0, 0.1, 0.2, 0.3, 0.4), ‚Äòsubsample‚Äô: (0.5, 0.75, 1) }

Operon {‚Äòpopulation size‚Äô: (500,), ‚Äòpool size‚Äô: (500,), ‚Äòmax length‚Äô: (50,), ‚Äòallowed symbols‚Äô: (‚Äòadd,mul,aq,constant,variable‚Äô,), ‚Äòlocal iterations‚Äô: (5,), ‚Äòoffspring generator‚Äô: (‚Äòbasic‚Äô,), ‚Äòtournament size‚Äô: (5,), ‚Äòreinserter‚Äô: (‚Äòkeep-best‚Äô,), ‚Äòmax evaluations‚Äô: (500000,) }{‚Äòpopulation size‚Äô: (500,), ‚Äòpool size‚Äô: (500,), ‚Äòmax length‚Äô: (25,), ‚Äòallowed symbols‚Äô: (‚Äòadd,mul,aq,exp,log,sin,tanh,constant,variable‚Äô,), ‚Äòlocal iterations‚Äô: (5,), ‚Äòoffspring generator‚Äô: (‚Äòbasic‚Äô,), ‚Äòtournament size‚Äô: (5,), ‚Äòreinserter‚Äô: (‚Äòkeep-best‚Äô,), ‚Äòmax evaluations‚Äô: (500000,) }{‚Äòpopulation size‚Äô: (500,), ‚Äòpool size‚Äô: (500,), ‚Äòmax length‚Äô: (25,), ‚Äòallowed symbols‚Äô: (‚Äòadd,mul,aq,constant,variable‚Äô,), ‚Äòlocal iterations‚Äô: (5,), ‚Äòoffspring generator‚Äô: (‚Äòbasic‚Äô,), ‚Äòtournament size‚Äô: (5,), ‚Äòreinserter‚Äô: (‚Äòkeep-best‚Äô,), ‚Äòmax evaluations‚Äô: (500000,) }{‚Äòpopulation size‚Äô: (100,), ‚Äòpool size‚Äô: (100,), ‚Äòmax length‚Äô: (50,), ‚Äòallowed symbols‚Äô: (‚Äòadd,mul,aq,constant,variable‚Äô,), ‚Äòlocal iterations‚Äô: (5,), ‚Äòoffspring generator‚Äô: (‚Äòbasic‚Äô,), ‚Äòtournament size‚Äô: (3,), ‚Äòreinserter‚Äô: (‚Äòkeep-best‚Äô,), ‚Äòmax evaluations‚Äô: (500000,) }{‚Äòpopulation size‚Äô: (100,), ‚Äòpool size‚Äô: (100,), ‚Äòmax length‚Äô: (25,), ‚Äòallowed symbols‚Äô: (‚Äòadd,mul,aq,exp,log,sin,tanh,constant,variable‚Äô,), ‚Äòlocal iterations‚Äô: (5,), ‚Äòoffspring generator‚Äô: (‚Äòbasic‚Äô,), ‚Äòtournament size‚Äô: (3,), ‚Äòreinserter‚Äô: (‚Äòkeep-best‚Äô,), ‚Äòmax evaluations‚Äô: (500000,) }{‚Äòpopulation size‚Äô: (100,), ‚Äòpool size‚Äô: (100,), ‚Äòmax length‚Äô: (25,), ‚Äòallowed symbols‚Äô: (‚Äòadd,mul,aq,constant,variable‚Äô,), ‚Äòlocal iterations‚Äô: (5,), ‚Äòoffspring generator‚Äô: (‚Äòbasic‚Äô,), ‚Äòtournament size‚Äô: (3,), ‚Äòreinserter‚Äô: (‚Äòkeep-best‚Äô,), ‚Äòmax evaluations‚Äô: (500000,) }

FFX N/A JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 22 

TABLE XIII: Detailed median test R2 of top-performing algorithms on all 58 PMLB datasets. VJM-GP XGB RandomForest ExtraTrees MLP AdaBoost SVR ElasticNet 1027 0.840565 0.826994 0.824332 0.812448 0.855435 0.788757 0.854015 0.854696 1028 0.334995 0.290876 0.274882 0.198578 0.334498 0.303251 0.304832 0.318656 1029 0.542249 0.441439 0.395716 0.293007 0.526766 0.442702 0.540116 0.543636 1030 0.345114 0.238939 0.247096 0.190406 0.341778 0.239396 0.326406 0.345797 1089 0.726869 0.647972 0.709673 0.715010 0.735213 0.624932 0.747592 0.740863 1096 0.825504 0.704897 0.707586 0.550733 0.753582 0.694478 0.813204 0.788902 1191 0.198603 0.182081 0.201308 0.189125 0.140607 0.179012 0.174385 0.172814 1193 0.529916 0.516908 0.536592 0.530689 0.518055 0.527700 0.504949 0.540347 1196 0.407728 0.344633 0.370030 0.356128 0.398170 0.357886 0.410797 0.420361 1199 0.385372 0.346492 0.363977 0.361324 0.377553 0.345141 0.370149 0.378311 1201 -0.000504 -0.061526 0.018752 0.017796 -0.006524 -0.045104 0.003838 0.016853 1203 0.465604 0.459101 0.481883 0.472472 0.447231 0.504761 0.432235 0.466732 1595 -0.050663 -0.080785 -0.010936 -0.009038 -0.006687 -0.061970 -0.024205 -0.005064 192 0.492727 0.493044 0.487049 0.370296 0.560498 0.564445 0.434034 0.368128 195 0.816963 0.830318 0.868035 0.854877 0.747347 0.801928 0.741180 0.743558 197 0.894962 0.941402 0.935246 0.954910 0.869832 0.944216 0.357505 0.544948 201 0.767137 0.816091 0.778009 0.844249 0.465433 0.737237 0.676101 0.190848 207 0.751995 0.829024 0.852168 0.861556 0.759248 0.815315 0.729024 0.748505 210 0.809208 0.768884 0.796473 0.793094 0.845722 0.738728 0.886208 0.871676 215 0.918329 0.905344 0.896656 0.884696 0.882206 0.888312 0.818823 0.674278 218 0.423724 0.414668 0.435067 0.450343 0.313891 0.404602 0.267256 0.213272 225 0.340687 0.484648 0.467970 0.535939 0.329063 0.474318 0.346464 0.334667 227 0.893457 0.928316 0.923821 0.949588 0.857528 0.933657 0.371028 0.530435 228 0.551606 0.683940 0.638867 0.524560 0.656204 0.618829 0.511206 0.583576 229 0.856066 0.832123 0.811026 0.798169 0.783388 0.802321 0.742188 0.756406 230 0.794644 0.819710 0.715052 0.858531 0.737586 0.729656 0.746859 0.743686 294 0.748777 0.746761 0.765740 0.801421 0.763300 0.752101 0.802043 0.662896 344 0.996209 0.951705 0.942087 0.982543 0.994545 0.927896 0.938874 0.795322 4544 0.691813 0.585010 0.532841 0.599908 0.526252 0.580982 0.552433 0.694516 485 0.454381 0.508044 0.458570 0.425216 0.424676 0.446619 0.307905 0.391354 503 0.698187 0.676443 0.663319 0.674810 0.702977 0.638516 0.711517 0.721085 505 0.996280 0.985465 0.978513 0.990148 0.993126 0.978002 0.995344 0.994654 519 0.724352 0.686646 0.620122 0.610716 0.734809 0.701810 0.735497 0.738487 522 0.080212 0.241097 0.219749 0.225801 0.102416 0.193508 0.115079 0.065844 523 0.935079 0.942572 0.947106 0.931709 0.928203 0.941711 0.920709 0.936675 527 0.956401 0.681786 0.729301 0.746947 0.784684 0.700967 0.933900 0.751345 529 0.761634 0.606867 0.534707 0.587069 0.776545 0.514274 0.775769 0.781650 537 0.597876 0.536337 0.510514 0.524523 0.617349 0.492070 0.572244 0.591215 542 0.474008 0.301537 0.337266 0.376004 0.249359 0.393924 0.251514 0.330857 547 0.447828 0.473047 0.478522 0.479872 0.451461 0.479329 0.441376 0.461079 556 0.795224 0.838565 0.845691 0.793570 0.738266 0.870867 0.768854 -0.003486 557 0.867248 0.857777 0.849656 0.824656 0.794932 0.848236 0.771608 -0.001402 560 0.982260 0.964594 0.967752 0.973442 0.967967 0.950733 0.985790 0.978617 561 0.959809 0.887257 0.734116 0.822712 0.936104 0.708708 0.796391 0.794266 562 0.893457 0.928316 0.923821 0.949588 0.857528 0.933657 0.371028 0.530435 564 0.901678 0.779753 0.655387 0.695884 0.699820 0.680869 0.715631 0.696366 573 0.894962 0.941402 0.935246 0.954910 0.869832 0.944216 0.357505 0.544948 574 0.134109 0.297048 0.307811 0.320709 0.055241 0.187690 0.224135 0.056789 659 0.604900 0.394315 0.543908 0.407758 0.498848 0.516694 0.659796 0.546316 663 0.997748 0.995491 0.976191 0.987455 0.998490 0.955134 0.998191 0.972364 665 0.316847 0.155599 0.142349 0.115811 0.292470 0.313316 0.298176 0.291333 666 0.536075 0.498181 0.473588 0.538856 0.526476 0.500480 0.471409 0.529492 678 0.276509 0.115992 0.187108 0.166884 0.214906 0.141751 0.253331 0.258067 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 23 

687 0.457557 0.425380 0.465336 0.439297 0.387792 0.446510 0.308491 0.383927 690 0.958719 0.966728 0.963135 0.967320 0.969688 0.955554 0.970536 0.892017 695 0.841782 0.806271 0.823695 0.841566 0.855735 0.822629 0.862782 0.866051 706 0.628841 0.542783 0.564594 0.619290 0.607258 0.547730 0.515102 0.582595 712 0.746153 0.740555 0.717430 0.689963 0.746347 0.721980 0.738814 0.747513