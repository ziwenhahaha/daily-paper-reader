Title: Learning to Optimize by Differentiable Programming

URL Source: https://arxiv.org/pdf/2601.16510v1

Published Time: Mon, 26 Jan 2026 01:24:51 GMT

Number of Pages: 36

Markdown Content:
# Learning to Optimize by Differentiable Programming 

LIPING TAO, Nanyang Technological University, Singapore 

XINDI TONG, Nanyang Technological University, Singapore 

CHEE WEI TAN ‚àó, Nanyang Technological University, Singapore Solving massive-scale optimization problems requires scalable first-order methods with low per-iteration cost. This tutorial highlights a shift in optimization: using differentiable programming not only to execute algo-rithms but to learn how to design them. Modern frameworks such as PyTorch, TensorFlow, and JAX enable this paradigm through efficient automatic differentiation. Embedding first-order methods within these sys-tems allows end-to-end training that improves convergence and solution quality. Guided by Fenchel‚ÄìRockafellar duality, the tutorial demonstrates how duality-informed iterative schemes such as ADMM and PDHG can be learned and adapted. Case studies across LP, OPF, Laplacian regularization, and neural network verification illustrate these gains. CCS Concepts: ‚Ä¢ Mathematics of computing ‚Üí Mathematical optimization .Additional Key Words and Phrases: Differentiable Programming, First-Order Optimization, Fenchel-Rockafeller Duality, Deep Learning Software. 

ACM Reference Format: 

Liping Tao, Xindi Tong, and Chee Wei Tan. 2018. Learning to Optimize by Differentiable Programming. J. ACM 37, 4, Article 111 (August 2018), 36 pages. https://doi.org/XXXXXXX.XXXXXXX 

1 Introduction 

Optimization problems are fundamental in operations research, economics, engineering, and com-puter science, covering a wide range of formulations and applications [5, 18]. Although advances in optimization algorithms have greatly improved reliability and efficiency, scalability remains a key challenge as computational costs grow and convergence becomes harder to guarantee with increasing problem size. Large-scale and complex optimization tasks face two main difficulties, effective problem formulation and high computational cost [30]. Solving such problems often de-mands significant human expertise and computational resources, since traditional solvers struggle to maintain efficiency as complexity increases [83]. In recent years, learning to optimize has emerged as a promising paradigm at the intersection of machine learning and optimization [134]. By leveraging data-driven approaches‚Äîparticularly deep learning‚Äîlearning to optimize enhances traditional optimization algorithms and generalizes from training tasks to unseen problems, often surpassing classical solvers in speed and efficiency, especially in large-scale or nonconvex settings [34]. This vision has inspired initiatives such as the AI4OPT Institute of NSF, which integrates artificial intelligence and optimization technologies  

> ‚àó‚àó

(Corresponding Author) 

Authors‚Äô Contact Information: Liping Tao, liping.tao@163.com, Nanyang Technological University, Singapore, Singapore; Xindi Tong, xtong308@gmail.com, Nanyang Technological University, Singapore, Singapore; Chee Wei Tan, cheewei.tan@ ntu.edu.sg, Nanyang Technological University, Singapore, Singapore. 

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ¬© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM 1557-735X/2018/8-ART111 https://doi.org/XXXXXXX.XXXXXXX J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.  

> arXiv:2601.16510v1 [cs.MS] 23 Jan 2026 111:2 Tao, Tong and Tan
> Fig. 1. Learning to Optimize via Differentiable Programming by Combining Duality and First-order Methods.

[141], and LEANOPT on the AAAI platform, which incorporates modern machine learning meth-ods into the operations research toolkit with a strong focus on constrained optimization [88]. More broadly, the rapid progress of machine learning has fueled not only the design of new optimization algorithms and the revival of classical ones, but also the development of machine learning-based strategies for tackling traditionally hard optimization problems [17]. Within this landscape, first-order methods [16], such as gradient descent and its variants, remain fundamental to modern machine learning and artificial intelligence. Proximal algorithms [111], as extensions of first-order methods, are particularly effective for handling constraints and non-smooth regularization. Approaches like the proximal gradient method [154], the Alternating Di-rection Method of Multipliers (ADMM) [26], and their accelerated variants efficiently solve prob-lems combining smooth and non-smooth components through iterative subproblem decomposi-tion [148]. However, proximal algorithms face challenges including computational overhead, hy-perparameter sensitivity, and limited scalability in high-dimensional or non-convex settings [86]. The learning-to-optimize framework offers a promising remedy by learning proximal updates or adaptive rules from data [86], thereby accelerating convergence, enhancing robustness, and ex-tending proximal methods to larger and more complex problems. Nonetheless, a central question persists: how close are the solutions produced by machine learning models to the true optimum? Evaluating and certifying solution quality remains a fundamental issue. Duality theory provides a unifying perspective for learning and certifying solutions to optimiza-tion problems by leveraging both classical Lagrangian or Fenchel duality and variational duality principles [135, 137]. Classical Lagrangian duality reformulates constrained problems by introduc-ing multipliers, yielding tractable dual formulations that enable principled evaluation of primal op-timality under strong duality [20, 80] and revealing structural links between primal and dual vari-ables [48]. Complementing this view, Fenchel conjugate [27] provides dual representations for un-constrained or composite objectives, offering variational characterizations that underlie proximal operators, regularization schemes, and many differentiable optimization layers. Together, these duality tools underpin modern dual learning, which iteratively refines constraints and guides op-timization and has shown strong effectiveness for conic, nonlinear, and large-scale problems [98]. The dual Lagrangian learning framework developed by Van Hentenryck and Tanneau [135], along with advances in cone theory [103], operator splitting [106], and power-function duality [129], highlights the scalability and theoretical depth of this duality-driven approach to differentiable programming. Although first-order methods and duality theory have advanced optimization learning and solu-tion verification in learning-to-optimize frameworks, their reliance on gradient derivations limits scalability as problem size grows [55, 127]. To overcome this, as shown in Figure 1, recent research  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:3

integrates differentiable programming [23, 122] with duality theory and first-order methods, es-tablishing a scalable and efficient paradigm for optimization. Differentiable programming com-bines computer programs with automatic differentiation [15, 113], treating programs‚Äîincluding those with control flow, recursion, and complex structures‚Äîas composable modules for efficient end-to-end differentiation. Unlike traditional numerical or black-box optimization, differentiable programming embeds optimization problems, iterative algorithms, and even discrete approxima-tions within a differentiable framework, enabling seamless gradient propagation and uncovering complex high-dimensional relationships [7, 76]. As gradient evaluation lies at the core of high-dimensional learning, differentiable programming enhances this process across domains‚Äîfrom deep learning and control systems to scientific computing‚Äîrepresenting both the natural evolu-tion of deep learning and a general foundation for trainable computation [122, 132]. Given A ‚àà Rùëö √óùëõ and b ‚àà Rùëö , consider the Nonnegative Least Squares (NNLS) problem [39, 49], a quadratic cone program that seeks a nonnegative vector x minimizing the residual ‚ÄñAx ‚àí b‚Äñ22:min  

> x‚â•01
> 2

‚ÄñAx ‚àí b‚Äñ22. (NNLS) In this tutorial, we use the NNLS problem (NNLS) as a running example to illustrate how differen-tiable programming, first-order methods, and duality theory can be integrated within learning-to-optimize frameworks. Our main contributions are summarized as follows: 

‚Ä¢ We present a systematic overview of the theoretical foundations of differentiable program-ming, together with a review of major software frameworks and representative differentiable programming implementations. 

‚Ä¢ We examine how differentiable programming integrates with optimization. Specifically, we revisit cone programming and its core principles, discuss differentiable strategies for em-bedding optimization within this framework, present duality theory and fundamental first-order methods in a differentiable setting, and analyze PyTorch-based implementations for primal‚Äìdual cone programs with accompanying source code 1.

‚Ä¢ Through representative case studies, we demonstrate the benefits of combining differentiable programming, duality theory, and first-order methods for solving linear and quadratic cone programs, supported by reproducible PyTorch implementations. 

Organization. The paper is organized as follows. Section 1 introduces the background and out-lines the main contributions. Section 2 reviews the fundamentals of differentiable programming, including software frameworks and implementations. Section 3 explores the integration of differ-entiable programming with cone programming, duality theory, first-order methods, and PyTorch implementation. Section 4 presents PyTorch-based case studies unifying differentiable program-ming, duality theory, and first-order methods for end-to-end optimization, and Section 5 concludes the tutorial. 

2 Differentiable Programming 

Differentiable programming is a paradigm that treats differentiability as a first-class abstraction, enabling complex programs, including those with control flow and data structures like neural net-works, to be composed of differentiable functions and optimized end-to-end via automatic differen-tiation [23, 145]. By modeling computations as differentiable computational graphs, differentiable programming unifies programming and optimization, allowing efficient gradient and higher-order derivative computation. In modern machine learning, first-order differentiable programs leverage gradients, momentum, and adaptive steps for scalable nonconvex optimization, while second-order variants incorporate curvature information such as Hessians or their approximations to enhance  

> 1Source code will be available at https://github.com/convexsoft/AITIP. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:4 Tao, Tong and Tan

convergence and robustness in precision-critical problems [109]. In the following sections, we present the theoretical foundations of differentiable programming and review the software and frameworks that support differentiable programming, concluding with recent implementations. 

2.1 Theoretical Foundations of Differentiable Programming 

This section presents the theoretical foundations of differentiable programming, covering gradient calculus, computation graphs, and automatic differentiation, and demonstrated these principles through a detailed analysis of the NNLS problem (NNLS). 

2.1.1 Gradient Calculus. Consider a differentiable function ùëì : RùëÉ ‚Üí R. The gradient of ùëì at a point ùë§ ‚àà RùëÉ is defined as the column vector of its partial derivatives: 

‚àáùëì (ùë§ ) :=

¬©¬´ 

> ùúïùëì
> ùúïùë§ 1

(ùë§ )

... 

> ùúïùëì
> ùúïùë§ ùëÉ

(ùë§ )

¬™¬Æ¬Æ¬Æ¬¨

= ¬©¬´

ùúïùëì (ùë§ ) [ ùëí 1]

...ùúïùëì (ùë§ ) [ ùëí ùëÉ ]

¬™¬Æ¬Æ¬¨

‚àà RùëÉ , (1) where {ùëí 1, . . . , ùëí ùëÉ } denote the standard basis of RùëÉ . By the linearity of the differential, the direc-tional derivative of ùëì at ùë§ along a direction ùë£ = √çùëÉ ùëñ =1 ùë£ ùëñ ùëí ùëñ is given by ùúïùëì (ùë§ ) [ ùë£ ] = √çùëÉ ùëñ =1 ùë£ ùëñ , ùúïùëì (ùë§ ) [ ùëí ùëñ ] =

„Äàùë£, ‚àáùëì (ùë§ )„Äâ ‚àà R, where „Äà¬∑ , ¬∑„Äâ denotes the standard Euclidean inner product. 

‚Ä¢ Jacobian Matrix: Consider a differentiable function ùëì : RùëÉ ‚Üí RùëÄ . The Jacobian matrix of ùëì 

at a point ùë§ ‚àà RùëÉ is defined as the matrix of all first-order partial derivative: 

ùúïùëì (ùë§ ) :=

¬©¬´ 

> ùúïùëì 1
> ùúïùë§ 1

(ùë§ ) ¬∑ ¬∑ ¬∑ ùúïùëì 1 

> ùúïùë§ ùëÉ

(ùë§ )

... . . . ... 

> ùúïùëì ùëÄ
> ùúïùë§ 1

(ùë§ ) ¬∑ ¬∑ ¬∑ ùúïùëì ùëÄ  

> ùúïùë§ ùëÉ

(ùë§ )

¬™¬Æ¬Æ¬Æ¬¨

‚àà RùëÄ √óùëÉ . (2) Equivalently, the Jacobian can be written as a stack of transposed component gradients, ùúïùëì (ùë§ ) =

(‚àáùëì 1 (ùë§ )‚ä§, . . . , ‚àáùëì ùëÄ (ùë§ )‚ä§)‚ä§ ‚àà RùëÄ √óùëÉ . By linearity of the differential, the directional derivative of ùëì 

at ùë§ along ùë£ = √çùëÉ ùëñ =1 ùë£ ùëñ ùëí ùëñ ‚àà RùëÉ is ùúïùëì (ùë§ ) [ ùë£ ] = √çùëÉ ùëñ =1 ùë£ ùëñ ùúï ùëñ ùëì (ùë§ ) = ùúïùëì (ùë§ ) ùë£ ‚àà RùëÄ , where ùúï ùëñ ùëì (ùë§ )

denotes the partial derivative of ùëì with respect to the ùëñ -th coordinate of ùë§ .

‚Ä¢ Chain Rule: Consider two differentiable functions ùëì : RùëÉ ‚Üí RùëÄ and ùëî : RùëÄ ‚Üí RùëÖ . If ùëì 

is differentiable at ùë§ ‚àà RùëÉ and ùëî is differentiable at ùëì (ùë§ ) ‚àà RùëÄ , then the composition ùëî ‚ó¶ ùëì is differentiable at ùë§ , and its Jacobian is given by the chain rule: 

ùúï (ùëî ‚ó¶ ùëì )( ùë§ ) = ùúïùëî (ùëì (ùë§ )) ùúïùëì (ùë§ ). (3) 

2.1.2 Computational Graph. Computational graphs [14, 23], also known as Kantorovich graphs, are Directed Acyclic Graphs (DAGs) [41] that represent dependencies among inputs, intermedi-ate variables, and outputs. Nodes correspond to quantities such as inputs, parameters, or opera-tions such as addition, multiplication, and nonlinear activations, while edges encode data depen-dencies or atomic operations connecting them. Once constructed, forward propagation efficiently computes the output of program [122]. Specifically, derivatives are obtained using the Bauer for-mula [105], which recursively expresses ùúïùë£ ùëó /ùúïùë£ ùëñ through parent-node gradients: 

ùúïùë£ ùëó 

ùúïùë£ ùëñ 

=

√ï

> ùë§ ‚Üíùë£ ùëó

ùúïùë£ ùëó 

ùúïùë§ ¬∑ ùúïùë§ 

ùúïùë£ ùëñ 

. (4)  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:5

This recursive relation provides an exact graph-based form of the chain rule and enables backprop-agation, the foundation of automatic differentiation. Extending naturally to higher-order deriva-tives, computational graphs form the backbone of modern frameworks such as PyTorch, Tensor-Flow, and JAX, and serve as a core abstraction in differentiable programming for large-scale opti-mization. 

2.1.3 Automatic Differentiation. Automatic Differentiation (AD) is a key computational technique for efficiently and accurately computing derivatives of functions represented as computer pro-grams [15]. Unlike numerical differentiation, which introduces approximation errors, or symbolic differentiation, which suffers from expression explosion, automatic differentiation systematically applies the chain rule over a computational graph by extending variables with derivative infor-mation and redefining operators accordingly [99]. Serving as the foundation of differentiable pro-gramming, automatic differentiation unifies calculus and programming, enabling scalable gradient-based optimization in frameworks such as PyTorch [23, 63, 113]. Its reach extends beyond simple operation chains to programs with control flow, data structures, and computational effects, broad-ening the scope of differentiable modeling [1]. Among the various modes of automatic differentiation, reverse-mode automatic differentia-tion [69]‚Äîthe mechanism underlying backpropagation‚Äîplays a central role. It constructs a com-putational graph during the forward pass and propagates gradients backward, making it especially effective for large-scale optimization problems with many parameters [101]. Recent research has significantly strengthened the theoretical underpinnings of reverse-mode automatic differentia-tion. For example, Abadi et al. [1] introduced a minimal differentiable programming language; Innes et al. [72] demonstrated automatic differentiation as a first-class language feature support-ing custom adjoints and mixed-mode differentiation; and Wang et al. [145, 146] recast automatic differentiation through the lens of continuations, enabling dynamic, modular, and composable dif-ferentiation. Particularly, it is commonly accepted that backpropagation is simply the chain rule. As em-phasized by Vieira [143], however, backpropagation is not merely the repeated application of the analytic chain rule to a symbolic expression; rather, it emerges from interpreting a program as a system of equality constraints linking intermediate variables and then solving the corresponding linear Lagrangian or adjoint system induced by these constraints. In this Lagrangian or adjoint-state view, the forward pass enforces the constraints that define intermediate values, while the backward pass computes the Lagrange multipliers that propagate sensitivities backward through the computational graph [67]. This perspective explains why reverse-mode automatic differentia-tion evaluates gradients at a cost comparable to a single forward execution and why it naturally extends to programs with branching, loops, and implicit computations. In what follows, we revisit the NNLS problem (NNLS) introduced earlier and analyze backpropagation from both perspectives: the computational-graph chain-rule view and the Lagrangian (adjoint-state) view. 

Example 2.1 (Backpropagation via Chain Rule and Lagrangian). Consider the NNLS prob-lem: 

min  

> x‚â•01
> 2

‚ÄñAx ‚àí b‚Äñ22. (5) 

Introduce the intermediate variables z = Ax , y = z ‚àí b, and ùëì = 1 

> 2

‚Äñy‚Äñ22. Temporarily ignoring the nonnegativity constraint x ‚â• 0, problem (5) can be rewritten as: 

min ùëì 

s.t. z = Ax , y = z ‚àí b, ùëì = 1 

> 2

‚Äñy‚Äñ22 . (6)  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:6 Tao, Tong and Tan

(1) Backpropagation via the Chain Rule: As shown in Figure 2, applying the chain rule along the computational graph yields: 

‚àáx ùëì = ùúïùëì 

ùúï y

ùúï y

ùúï z

ùúï z

ùúï x = y I A = A‚ä§ (Ax ‚àí b), (7) 

where I denotes the identity operator. The conventional backpropagation path is along ùëì ‚Üí y ‚Üí z ‚Üí x.

> Fig. 2. Backpropagation in PyTorch via Chain Rule.

(2) Backpropagation as an Adjoint/Lagrangian System: Introduce Lagrange multipliers ùùÄ , ùùÅ ,and ùúå for the constraints in (6) . The Lagrangian is: 

L ( x, z, y, ùëì , ùùÄ , ùùÅ , ùúå ) = ùëì + ùùÄ ‚ä§ (Ax ‚àí z) + ùùÅ ‚ä§ (z ‚àí b ‚àí y) + ùúå ( 1 

> 2

‚Äñy‚Äñ22 ‚àí ùëì ). (8) 

Taking derivatives with respect to the multipliers recovers the forward pass: 

‚àáùùÄ L = Ax ‚àí z = 0, ‚àáùùÅ L = z ‚àí b ‚àí y = 0, ‚àáùúå L = 1 

> 2

‚Äñy‚Äñ22 ‚àí ùëì = 0, (9) 

corresponding exactly to the constraints in (6) . Next, differentiating with respect to the intermediate variables gives the adjoint (backward) equations: 

‚àázL = ‚àíùùÄ + ùùÅ = 0, ‚àáyL = ‚àíùùÅ + ùúå y = 0, ‚àáùëì L = 1 ‚àí ùúå = 0. (10) 

Solving these adjoint relationships yields ùùÄ = ùùÅ = ùúå y. Finally, the gradient with respect to x becomes: 

‚àáx ùëì = A‚ä§ùùÄ = A‚ä§ùùÅ = A‚ä§ (ùúå y) = A‚ä§y = A‚ä§ (Ax ‚àí b), (11) 

which coincides exactly with the chain-rule computation in (7) .

Example 2.1 shows that backpropagation can be seen either as repeated chain-rule application or as solving a Lagrangian-based adjoint system. Both yield the same gradients, with the adjoint view clarifying how backpropagation extends naturally to complex programs, implicit layers, and constrained computations. 

2.2 Software and Frameworks of Differentiable Programming 

This section introduces key frameworks that enable differentiable programming, including Py-Torch [77], JAX [28], and Julia [22], which provide automatic differentiation and efficient numeri-cal computation, forming the practical foundation for differentiable programming. 

2.2.1 PyTorch. Developed by Facebook‚Äôs AI Research lab, PyTorch is an open-source deep learn-ing framework that combines a flexible programming interface, efficient tensor computation, and seamless GPU acceleration [77]. Its dynamic computation graph enables intuitive model construc-tion, debugging, and experimentation, while the autograd engine provides powerful reverse-mode automatic differentiation for large-scale numerical computation. Core modules such as torch.nn for neural network components, torch.optim for optimization routines, and torch.utils.data for data handling support a broad spectrum of machine learning and scientific workflows. Beyond its role as a deep learning library, PyTorch serves as a unifying platform for differentiable programming by integrating numerical computation, automatic differentiation, and optimization into a coher-ent system [74]. With autograd allowing gradients to flow through arbitrary control flow, loops, and user-defined data structures [70, 113], PyTorch enables end-to-end differentiation of complex  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:7

and parameterized programs, including iterative solvers, optimization algorithms, and physical simulations. This capability transforms traditional programming into an optimization-aware par-adigm, providing both the theoretical expressiveness and practical performance necessary to de-velop differentiable algorithms, learning-enhanced numerical methods, and bilevel optimization frameworks [86, 145]. As a result, PyTorch has become a cornerstone of modern machine learning, scientific computing, and differentiable programming research [114]. 

2.2.2 JAX. Developed by Google Research, JAX is a high-performance framework for numerical computing and machine learning [28]. Built upon NumPy [140], it can be viewed as a differentiable extension of the NumPy system that integrates numerical computation, automatic differentiation, and compilation-driven optimization into an efficient framework. By combining the flexibility of the Python scientific computing ecosystem with robust automatic differentiation and accelerated compilation, JAX delivers performance and composability for research as well as engineering ap-plications [123, 144]. Within the field of differentiable programming, JAX plays a central role. Through support for automatic differentiation, just-in-time compilation, and parallel execution primitives, the framework enables efficient differentiation of arbitrary Python functions and facil-itates large-scale parallel computation [23]. The automatic differentiation system in JAX is based on function transformations that apply to any function expressed through JAX primitives. These transformations, including grad, jacfwd, jacrev, vmap, and jit, can be combined to construct com-plex differentiable models and algorithms with minimal implementation effort [128]. Beyond deep neural network models [25], JAX supports end-to-end differentiation of optimization algorithms, physical simulations, probabilistic models, and numerical solvers. JAX has become a core tool in differentiable programming, widely used in automatic-differentiation‚Äìbased optimization, scien-tific computing, and differentiable physics simulations [95]. 

2.2.3 Julia. Julia is a modern dynamic language for technical computing, designed as a high-performance environment for numerical analysis, data science, and scientific research. From the outset, it was built to integrate modern programming language technologies with a focus on speed and efficiency [22]. Combining the ease of dynamic languages like Python with the performance of compiled languages such as C and Fortran through LLVM-based just-in-time compilation, Julia offers concise, mathematically expressive syntax ideal for algorithm development and large-scale numerical simulation [21, 65]. Beyond the language itself, Julia represents an ecosystem that natu-rally embodies the philosophy of differentiable programming [72, 139]. Specifically, it plays a vital role in this domain through its native support for automatic differentiation and composable pro-gram transformations [117]. The automatic differentiation environment in Julia is developed in a way that enables composability among different automatic differentiation systems. Julia‚Äôs auto-matic differentiation frameworks operate directly on standard Julia syntax, its standard library, ar-ray implementations, and GPU acceleration tools. Consequently, packages written in Julia are fully differentiable without requiring awareness of specific automatic differentiation systems or modi-fication across multiple tools [124]. Libraries such as Zygote.jl, ForwardDiff.jl, and ChainRules.jl can automatically differentiate arbitrary Julia code, including loops, control flows, and user-defined data structures, without relying on external computation graphs. This design elevates differentia-tion to a first-class language feature, allowing optimization, simulation, and machine learning to be seamlessly integrated within the same computational environment [108]. 

2.3 Recent Implementations of Differentiable Programming in Optimization 

Differentiable programming reframes optimization in machine learning by embedding optimiza-tion problems as trainable, differentiable components within learning architectures [7]. Instead of functioning solely as terminal solvers, optimization layers integrate objectives, constraints, and 

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:8 Tao, Tong and Tan 

Table 1. Summary of Major Software Frameworks and Implementations of Differentiable Programming. 

Category Tool Feature Purpose   

> Software and Frameworks
> PyTorch [77]

‚Ä¢ Dynamic computation graph 

‚Ä¢ GPU-accelerated tensor operations 

‚Ä¢ Reverse-mode AD via autograd 

‚Ä¢ End-to-end differentiation of programs 

‚Ä¢ Support for iterative solvers and opti-mization algorithms   

> JAX [28]

‚Ä¢ Differentiable NumPy API 

‚Ä¢ JIT compilation via XLA 

‚Ä¢ Composable transforms: grad, vmap, jit 

‚Ä¢ Differentiable numerical computing 

‚Ä¢ Scientific simulation 

‚Ä¢ Large-scale parallel differentiable pro-gramming   

> Julia [22]

‚Ä¢ High-performance scientific language 

‚Ä¢ Composable AD ecosystem: Zygote, ForwardDiff, ChainRules 

‚Ä¢ Native differentiability for algorithms 

‚Ä¢ Optimization-aware scientific computing 

‚Ä¢ Seamless integration of simulation and differentiation   

> Differentiable Optimization Layers
> CVXPYLayer [40]

‚Ä¢ Differentiable convex optimization layers 

‚Ä¢ Implicit differentiation of KKT conditions 

‚Ä¢ Embedding convex programs as differen-tiable modules   

> PyEPO [133]

‚Ä¢ Predict-then-optimize framework 

‚Ä¢ Differentiable LP/MIP solvers 

‚Ä¢ Surrogate gradients for discrete decisions 

‚Ä¢ End-to-end learning for decision-focused optimization   

> DDNs [59, 60]

‚Ä¢ Implicit optimization layers 

‚Ä¢ Gradients from KKT system and second-order structure 

‚Ä¢ Solver-free, scalable differentiable opti-mization 

‚Ä¢ Integrating optimization steps into neural networks 

‚Ä¢ Differentiation via implicit function theory 

> Elements of Differentiable Programming
> [23]

‚Ä¢ Unified treatment of AD, proximal meth-ods, fixed-point estimators 

‚Ä¢ Iterative algorithm layers 

‚Ä¢ Probabilistic and optimization-based inference 

‚Ä¢ Theoretical foundation for differentiable algorithm design 

‚Ä¢ Bridging algorithms, solvers, and auto-matic differentiation 

algorithms into end-to-end gradient-based training through automatic differentiation [15]. This enables scalable systems that avoid manual gradient derivations and extend differentiability to complex algorithmic structures such as control loops, discrete decisions, and physical simula-tions [25, 122]. Treating optimization as a differentiable layer has led to wide-ranging applications, including predictor‚Äìverifier training [45], deep equilibrium models [13], differentiable tensor net-works [92], differentiable vision pipelines [90], physics simulators [70, 107], differentiable MPC [8], bilevel hyperparameter learning [115], robustness via convex relaxations [149, 156], and differen-tiable sorting and ranking [24]. These developments establish differentiable programming as a unifying paradigm linking optimization, control, and scientific computing. The following section introduces representative implementations and studies that embed differentiable programming within optimization frameworks. 

2.3.1 CVXPYLayer. CVXPY, developed by Steven Diamond and Stephen Boyd, is an open-source Python DSL for modeling and solving convex optimization problems [40]. Building on the ear-lier CVX system [61], it implements the disciplined convex programming ruleset and extends it through disciplined parametrized programming. CVXPY provides an expressive, Pythonic in-terface for declaring variables, objectives, and constraints while abstracting solver details. It au-tomatically canonicalizes problems into standard convex forms and interfaces with specialized solvers such as SCS, ECOS, and OSQP [125]. Building on this foundation, CVXPYLayers extends convex optimization into the differentiable-programming domain. Its CVXPYLayer class exposes 

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:9 

the mapping from problem parameters to optimal solutions as a differentiable function by implic-itly differentiating the Karush‚ÄìKuhn‚ÄìTucker conditions [126]. This enables seamless integration with PyTorch, TensorFlow, and JAX, allowing gradients to propagate through optimization lay-ers. CVXPY and CVXPYLayers bridge convex optimization, duality, and automatic differentiation, transforming traditional solvers into learnable modules for end-to-end machine learning and com-puting. 

2.3.2 PyEPO. PyEPO, introduced by Tang and Khalil [133], is a unified PyTorch-based framework for end-to-end predict-then-optimize learning in linear and integer programming. Classical two-stage pipelines train a predictive model by minimizing a statistical loss before solving a down-stream optimization problem, whereas PyEPO integrates the optimizer directly into the computa-tional graph and exposes decision-aware gradients to upstream predictors. To support this capabil-ity, the framework consolidates several influential families of surrogate-gradient and perturbation-based techniques, including SPO+, differentiable black-box solvers, differentiable perturbed opti-mizers, and perturbed Fenchel‚ÄìYoung losses. These approaches approximate or smooth the in-herently non-differentiable mapping from objective coefficients to optimal decisions, enabling optimization-aware training even in discrete or polyhedral decision spaces. PyEPO further pro-vides high-level interfaces to commercial LP and MIP solvers such as Gurobi and COPT, supports batched forward and backward evaluations, and standardizes benchmark tasks including shortest-path, knapsack, and traveling-salesperson problems. By systematizing a broad class of differen-tiable decision-focused learning methods within a modern automatic differentiation framework, PyEPO serves as a comprehensive experimental platform that demonstrates the practical value of differentiable programming for large-scale decision-making and combinatorial optimization. 

2.3.3 DDNs. Gould et al. [59, 60] introduced Deep Declarative Networks (DDNs), a unified frame-work that integrates optimization problems directly into deep learning architectures. In a DDN, a layer is defined not by an explicit forward computation, but implicitly as the solution to a param-eterized optimization problem. Instead of unrolling an iterative solver, the authors derive closed-form differentiation rules using the implicit function theorem, enabling efficient backpropagation through both unconstrained and constrained optimization layers. Their work shows that the re-quired Jacobian‚Äìvector products can be computed using the problem‚Äôs optimality conditions, of-ten involving only linear systems that depend on the local curvature of the objective. This yields a highly scalable alternative to solver unrolling. Subsequent developments [58] draw connections between convex analysis, duality, and automatic differentiation, further reducing the computa-tional cost of implicit differentiation and facilitating the integration of optimization layers into modern deep learning frameworks such as PyTorch. Overall, DDNs offer a principled way to em-bed structured optimization modules inside neural networks, combining modeling flexibility with computational efficiency. 

2.3.4 Elements of Differentiable Programming. Blondel and Roulet [23] introduce a unified per-spective on differentiable programming, defining it as the principled construction of programs that integrate control flow, data structures, solvers, and algorithms in a way that keeps them fully differentiable and trainable through gradient-based optimization. Their framework systematically organizes the fundamental components of this paradigm, including automatic differentiation in forward and reverse modes with higher-order Hessian‚Äìvector computations, differentiable control flow through iterative and scanning structures, implicit and fixed-point layers that treat solvers as differentiable modules, optimization-inspired and proximal layers, and probabilistic gradient esti-mators such as score-function and reparameterization methods. The authors further demonstrate how classical ideas from optimization, including first-order, quasi-Newton, and natural gradient  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:10 Tao, Tong and Tan

methods, together with concepts from convex analysis such as Fenchel duality, Bregman diver-gences, and proximal mappings, can be naturally incorporated into modern automatic differentia-tion systems. Collectively, this framework offers a coherent foundation for designing differentiable computational systems that bridge algorithmic programming and deep learning, enabling complex algorithmic procedures to function as trainable components within neural architectures. 

3 Differentiable Programming for Optimization 

Many optimization problems share recurring structural patterns, making the integration of differ-entiable programming and optimization increasingly powerful. Park et al. [112] proposed a self-supervised primal‚Äìdual framework that jointly trains primal and dual networks to approximate op-timal solutions, emulating augmented Lagrangian dynamics without external solvers and illustrat-ing how differentiable programming enables end-to-end optimization over objectives, constraints, and solvers. Building on this idea, Agrawal et al. [2, 3] introduced differentiable conic program-ming through Jacobian-free operators and disciplined parameterized programming; Amos et al. [9] formulated quadratic programs as neural layers via implicit KKT differentiation; and Schaller et al. [126] automated this process with CVXPYgen. Further developments include proximal implicit differentiation for non-smooth problems [19] and loss-guided differentiation through black-box combinatorial solvers [116]. Constante-Flores et al. [37] proposed a model-agnostic framework that embeds input-dependent linear constraints into neural outputs via convex combinations of task and safe subnetworks, ensuring exact feasibility without iterative optimization. These works recast classical solvers as scalable differentiable modules, enabling end-to-end integration of learn-ing and optimization. We next examine this interplay through the cone programming, differen-tiable strategy, duality theory, and first-order methods, using NNLS-based examples as illustrative cases. 

3.1 Cone Programming 

Before introducing cone programming in detail, we briefly revisit conic optimization. It refers to problems whose feasible set is the intersection of an affine subspace with a convex cone, offering a unified framework that encompasses many convex programs through possibly nonlinear objec-tives or mappings into the cone. 

3.1.1 Conic Optimization. Conic optimization problems [51] constitute a broad class of nonlinear optimization problems whose feasible regions are characterized by the intersection of an affine subspace‚Äîrepresenting a system of linear equalities‚Äîand a nonempty closed convex cone K:min  

> ùë•

ùëì (ùë• )

s.t. ùê¥ùë• = ùëè, ùë• ‚àà K . (12) Here, the convex cone K is often expressed as a Cartesian product of orthants, second-order cones, and positive semidefinite cones [44, 120]. When K is the nonnegative orthant, Rùëõ  

> +

= {ùë• ‚àà

Rùëõ | ùë• ùëñ ‚â• 0}, the conic optimization problem reduces to a standard linear program of the form min ùë• ùëì (ùë• ) subject to ùê¥ùë• = ùëè and ùë• ‚â• 0, making linear programming a special case of conic opti-mization. When K is the second-order cone, Lùëõ = {( ùë•, ùëß ) ‚àà Rùëõ ‚àí1 √óR | | ùë• |2 ‚â§ ùëß }, the resulting prob-lems are Second-Order Cone Programs (SOCPs) [64, 94], widely used in engineering and physics. A further generalization employs the semidefinite cone, Sùëõ = {ùëã ‚àà Sùëõ | ùëã  0}, where Sùëõ denotes symmetric ùëõ √ó ùëõ matrices. The related copositive cone, Cùëõ = {ùëã ‚àà Sùëõ | ùëé ‚ä§ùëãùëé ‚â• 0, ; ‚àÄùëé ‚àà Rùëõ +} , re-laxes positive semidefiniteness by enforcing nonnegativity only on the nonnegative orthant. These cones illustrate the expressive scope of conic optimization, showing that an appropriate choice of 

K can unify a wide range of convex problems within a single framework.  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:11

3.1.2 Cone Programming. Cone programming [120] is a specialized subclass of conic optimization that unifies many classical convex problem families, including Linear Programming (LP) [75, 130], Quadratic Programming (QP) [54, 84], and Semidefinite Programming (SDP) [142, 155]. While every cone program is a conic optimization problem, the converse does not hold. In cone program-ming, both the objective and constraints are linear, and the standard form is: min  

> ùë•

ùëê ‚ä§ùë• 

s.t. ùê¥ùë• = ùëè, ùë• ‚àà K , (13) where K is a convex cone, ùë• ‚àà Rùëõ is the decision variable, ùëê ‚àà Rùëõ is the cost vector, ùê¥ ‚àà Rùëö √óùëõ is the constraint matrix, and ùëè ‚àà Rùëö is the right-hand-side vector. With their structured convexity and strong duality, cone programs anchor modern convex optimization and especially primal‚Äìdual interior-point methods, and they are central to differentiable programming where convex, differ-entiable optimization layers are required [2]. 

3.2 Differentiable Strategy for Differentiable Programming in Cone Programs 

In this section, we examine the applications of differentiable programming, with a particular focus on the PyTorch framework, in the context of cone programming following [27, 119, 151]. Firstly, let us consider the following cone programming: min  

> ùë•

ùëì (ùë• )

s.t. ùê¥ùë• = ùëè, ùë• ‚â• 0, (14) where ùëì : Rùëõ ‚Üí R is a convex and continuously differentiable objective function, ùë• ‚àà Rùëõ denotes the vector of decision variables, ùê¥ ‚àà Rùëù √óùëõ and ùëè ‚àà Rùëù specify the linear equality constraints, and the ùë• ‚â• 0 is imposed elementwise to ensure nonnegativity of the solution. 

Remark 1. PyTorch is primarily designed for unconstrained optimization through automatic differen-tiation. Consequently, constrained problems are typically reformulated into equivalent unconstrained forms to enable direct application of gradient-based solvers. This reformulation systematically elimi-nates constraints, facilitating seamless integration within differentiable programming frameworks. 

3.2.1 Penalty Reformulation. The equality constraint ùê¥ùë• = ùëè can be enforced using a quadratic penalty ùúÜ ‚Äñùê¥ùë• ‚àí ùëè ‚Äñ22, and the nonnegativity constraint ùë• ‚â• 0 is imposed by projection. This leads to the unconstrained problem: min   

> ùë• ‚â•0

ùëì (ùë• ) + ùúÜ ‚Äñùê¥ùë• ‚àí ùëè ‚Äñ22, (15) which encourages ùê¥ùë• ‚Üí ùëè and ùë• ‚Üí Rùëõ 

> +

. This penalty formulation integrates naturally with PyTorch through torch.norm and torch.clamp, enabling optimizers such as Adam or L-BFGS to incorporate these constraints within an end-to-end differentiable framework. 

3.2.2 Variable Transformation Reformulation. A particular solution ùë¢ of ùê¥ùë• = ùëè together with a null‚Äìspace basis ùêπ ‚àà Rùëõ √ó ( ùëõ ‚àíùëù ) satisfying ùê¥ùêπ = 0 allows any feasible point to be written as 

ùë• = ùêπùëß + ùë¢ . Substituting this parametrization removes the equality constraint and yields: min   

> ùêπùëß +ùë¢ ‚â•0

ùëì (ùêπùëß + ùë¢ ), (16) where nonnegativity is enforced by projecting ùêπùëß +ùë¢ onto Rùëõ  

> +

after each gradient update. The basis 

ùêπ is computed once via SVD [79], after which optimization proceeds over the unconstrained vari-able ùëß , making the approach efficient and fully compatible with PyTorch‚Äôs gradient-based solvers.  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:12 Tao, Tong and Tan

3.2.3 Duality Reformulation. An alternative way to obtain an unconstrained formulation of prob-lem (14) is to introduce dual variables and derive its dual problem. Letting ùúà and ùúÜ ‚â• 0 denote the dual variables, the dual function is: 

ùëî (ùúà, ùúÜ ) = inf  

> ùë•

{ùëì (ùë• ) + ùúà ‚ä§ (ùê¥ùë• ‚àí ùëè ) ‚àí ùúÜ ‚ä§ùë• }

= ‚àíùëè ‚ä§ùúà + inf  

> ùë•

{ùëì (ùë• ) + ( ùúà ‚ä§ùê¥ ‚àí ùúÜ ‚ä§)ùë• }

= ‚àíùëè ‚ä§ùúà ‚àí sup 

> ùë•

{( ùúÜ ‚àí ùê¥ ‚ä§ùúà )‚ä§ùë• ‚àí ùëì (ùë• )} 

= ‚àíùëè ‚ä§ùúà ‚àí ùëì ‚àó (ùúÜ ‚àí ùê¥ ‚ä§ùúà ), (17) where ùëì ‚àó is the Fenchel conjugate [52] of ùëì . The resulting dual problem is: max  

> ùúà, ùúÜ ‚â•0

‚àíùëè ‚ä§ùúà ‚àí ùëì ‚àó (ùúÜ ‚àí ùê¥ ‚ä§ùúà ). (18) This dual formulation is unconstrained and can be handled efficiently by first-order methods in PyTorch. Under Slater‚Äôs condition [27], strong duality ensures that the dual optimum matches the primal optimum and that the primal solution can be recovered from the optimal dual variables. 

3.3 Duality Theory 

The rapid growth of machine learning has driven new optimization methods, revitalized classi-cal ones, and introduced learning-based approaches to problems once viewed as intractable [17]. Yet even as learned models achieve strong primal performance across linear, discrete, nonlinear, and nonconvex settings [83], a central question persists: how close are these solutions to true optimality? Duality offers a principled mechanism for certifying quality, as demonstrated by dual Lagrangian learning [135], which exploits conic structure and expressive models to construct dual-feasible certificates and tight bounds for conic quadratic problems. Building on this perspective, recent work embeds dual principles directly into differentiable architectures, giving rise to deep dual learning‚Äîa paradigm that tightly integrates optimization theory with data-driven pipelines to produce feasible, near-optimal solutions end-to-end. Fioretto et al. [53] initiated this direction by incorporating dual multipliers into neural training and alternating primal‚Äìdual updates, and subsequent extensions span diverse domains, from nonconvex power flow [78] and combinato-rial scheduling [82] to dual-stabilized augmented Lagrangians [12] and dual-centric architectures predicting dual variables and reconstructing primal ones [81]. The following sections introduce the duality theory relevant to cone programming, focusing on Lagrangian duality and Fenchel duality [52, 67], and illustrate these principles through an NNLS example. 

3.3.1 Larangian Duality. Given a closed convex cone K ‚äÜ Rùëõ , its dual cone is K‚àó = { ùë£ ‚àà Rùëõ |„Äàùë£, ùëß „Äâ ‚â• 0, ‚àÄùëß ‚àà K } , the set of all vectors with nonnegative inner products against elements of K.The dual cone is closed and convex, satisfies the biconjugate property (K ‚àó)‚àó = K, and coincides with K when the cone is self-dual. Consider now the cone program (13): min  

> ùë•

ùëê ‚ä§ùë• 

s.t. ùê¥ùë• = ùëè, ùë• ‚àà K . (19) Introducing Lagrange multipliers ùúÜ ‚àà Rùëö for the equality constraint and ùúá ‚àà K ‚àó for the conic constraint, where K‚àó = { ùúá ‚àà Rùëõ | „Äà ùúá, ùë• „Äâ ‚â• 0, ‚àÄùë• ‚àà K } , the Lagrangian is: 

L ( ùë•, ùúÜ, ùúá ) = ùëê ‚ä§ùë• + ùúÜ ‚ä§ (ùëè ‚àí ùê¥ùë• ) ‚àí ùúá ‚ä§ùë•, ùúá ‚àà K ‚àó. (20)  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:13

The corresponding dual problem of (19) is: max  

> ùúÜ, ùúá

ùëè ‚ä§ùúÜ 

s.t. ùëê ‚àí ùê¥ ‚ä§ùúÜ = ùúá, ùúá ‚àà K ‚àó,

(21) where the primal constraint ùë• ‚àà K gives rise to the dual variable ùúá , which lies in the dual cone 

K‚àó.

Example 3.1 (Lagrangian Duality of NNLS). Given A ‚àà Rùëö √óùëõ and b ‚àà Rùëö , the NNLS problem is given by: 

min  

> x
> 1
> 2

‚ÄñAx ‚àí b‚Äñ22

s.t. x ‚â• 0. (22) 

To simplify the notation, defining y = Ax ‚àí b and introducing Lagrange multipliers ùùÄ for y = Ax ‚àí b

and ùùÅ ‚â• 0 for x ‚â• 0, we can rewrite the Lagrangian formulation of (22) :

L ( x, y, ùùÄ , ùùÅ ) = 1 

> 2

‚Äñy‚Äñ22 + ùùÄ ‚ä§ (y ‚àí Ax + b) ‚àí ùùÅ ‚ä§x. (23) 

Then, the corresponding dual function is given by ùëî (ùùÄ, ùùÅ ) = inf x,y L, i.e., ùëî (ùùÄ, ùùÅ ) = inf x,y{ 1 

> 2

‚Äñy‚Äñ22 +

ùùÄ ‚ä§ (y ‚àí Ax ) ‚àí ùùÅ ‚ä§x} + b‚ä§ùúÜ . Therefore, the dual problem is: 

max  

> ùùÄ

‚àí 1 

> 2

‚ÄñùùÄ ‚Äñ22 ‚àí b‚ä§ùùÄ 

s.t. A‚ä§ùùÄ ‚â• 0. (NNLS-Lagrangian-Dual) 

Defining C = {ùùÄ | A‚ä§ùùÄ ‚â• 0}, the dual simplifies to: 

min   

> ùùÄ ‚àà C
> 1
> 2

‚ÄñùùÄ ‚Äñ22 + b‚ä§ùùÄ . (NNLS-Lagrangian-Dual-Shortest) 

Transforming NNLS from its primal form to the dual form yields theoretical and computational benefits. The dual has simpler constraints and a smooth objective suited to first-order methods, enables efficient computation and duality-gap optimality checks, ensures correctness via KKT conditions, and offers better numerical stability while integrating naturally with differentiable programming. 

3.3.2 Fenchel Duality. Given a function ùëì , its Fenchel conjugate is defined as: 

ùëì ‚àó (~) = sup   

> ùë• ‚ààdom (ùëì )

(~‚ä§ùë• ‚àí ùëì (ùë• )) . (24) The Fenchel‚ÄìYoung inequality follows directly: 

ùëì ‚àó (~) ‚â• ~‚ä§ùë• ‚àí ùëì (ùë• ) =‚áí ùëì (ùë• ) ‚â• ~‚ä§ùë• ‚àí ùëì ‚àó (~). (25) It then implies: 

ùëì (ùë• ) ‚â• sup 

> ~

(~‚ä§ùë• ‚àí ùëì ‚àó (~)) = ùëì ‚àó‚àó (ùë• ). (26) If ùëì is closed and convex, equality holds and we obtain the biconjugation identity ùëì ‚àó‚àó = ùëì .Now consider the composite optimization problem: min  

> ùë•

ùëì (ùê¥ùë• ) + ùëî (ùë• ). (27) Introducing an auxiliary variable ùëß allows us to rewrite it as: min  

> ùë•,ùëß

ùëì (ùëß ) + ùëî (ùë• )

s.t. ùê¥ùë• = ùëß. (28)  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:14 Tao, Tong and Tan

The corresponding saddle-point formulation of (28) is: min  

> ùë•,ùëß

max  

> ùë¢

L ( ùë•, ùëß, ùúá ) = ùëì (ùëß ) + ùëî (ùë• ) + ùë¢ ùëá (ùê¥ùë• ‚àí ùëß ), (29) and applying the standard dual construction gives the Fenchel dual problem: max  

> ùë¢

min  

> ùë•,ùëß

ùëì (ùëß ) + ùëî (ùë• ) + ùúá ‚ä§ (ùê¥ùë• ‚àí ùëß )

= max 

> ùë¢

{ ‚àí sup 

> ùë•,ùëß

{ùúá ‚ä§ùëß ‚àí ùëì (ùëß ) + (‚àí ùê¥ ùëá ùë¢ )‚ä§ùë• ‚àí ùëî (ùë• )} }

= max 

> ùë¢

{ ‚àí ùëì ‚àó (ùë¢ ) ‚àí ùëî ‚àó (‚àí ùê¥ ùëá ùë¢ )}. (30) In practice, composite optimization problem (27) is particularly well suited to operator-splitting schemes such as ADMM or PDHG method, both of which exploit the separability and Fenchel structure of ùëì and ùëî . These algorithms will be discussed in subsequent sections. 

Example 3.2 (Fenchel Duality of NNLS). Given A ‚àà Rùëö √óùëõ and b ‚àà Rùëö , the NNLS problem is: 

min  

> x‚â•01
> 2

‚ÄñAx ‚àí b‚Äñ22. (31) 

To streamline notation, define ùëî (Ax ) = 1 

> 2

‚ÄñAx ‚àí b‚Äñ22. Then problem (31) can be expressed as: 

min  

> x‚â•0

ùëî (Ax ) = inf 

> x‚â•0

{‚àíy‚ä§ (Ax ) + ùëî (Ax ) + y‚ä§ (Ax )}

= ‚àí sup 

> x‚â•0

{y‚ä§ (Ax ) ‚àí ùëî (Ax )} + inf 

> x‚â•0

{y‚ä§ (Ax )}

= ‚àíùëî ‚àó (y) + inf 

> x‚â•0

{y‚ä§ (Ax )}, (32) 

which yields the corresponding saddle-point representation: 

max 

> y

{ ‚àí ùëî ‚àó (y) + inf  

> x‚â•0

y‚ä§Ax } = max  

> y

min 

> x‚â•0

{‚àíùëî ‚àó (y) + y‚ä§Ax } . (33) 

Thus the Fenchel dual problem is: 

max  

> y

‚àíùëî ‚àó (y). (34) 

Noting that ùëî (Ax ) = 1 

> 2

‚ÄñAx ‚àí b‚Äñ22, we compute the conjugate as follows: 

ùëî ‚àó (y) = sup 

> x‚â•0

{y‚ä§Ax ‚àí 1 

> 2

‚ÄñAx ‚àí b‚Äñ22

}

= sup 

> x‚â•0

{y‚ä§Ax ‚àí 1 

> 2

‚ÄñAx ‚Äñ22 + b‚ä§Ax ‚àí 1 

> 2

‚Äñb‚Äñ22

}

= sup 

> x‚â•0

{‚àí 1 

> 2

‚ÄñAx ‚àí ( y + b)‚Äñ 22 + 1 

> 2

‚Äñy + b‚Äñ22 ‚àí 1 

> 2

‚Äñb‚Äñ22

}

= sup 

> x‚â•0

{ 1 

> 2

‚Äñy + b‚Äñ22 ‚àí 1 

> 2

‚Äñb‚Äñ22

}

= 1 

> 2

‚Äñy‚Äñ22 + b‚ä§y. (35) 

Therefore, the Fenchel dual problen of (31) becomes: 

max  

> y

‚àíùëî ‚àó (y) = max 

> y

{‚àí 1 

> 2

‚Äñy‚Äñ22 ‚àí b‚ä§y} , (NNLS-Fenchel-Dual) 

which agrees exactly with the dual obtained from Lagrangian duality (NNLS-Lagrangian-Dual) . Fi-nally, the associated saddle-point formulation of (31) becomes: 

max  

> y

min 

> x‚â•0

{y‚ä§Ax ‚àí 1 

> 2

‚Äñy‚Äñ22 ‚àí b‚ä§y} . (NNLS-Fenchel-Saddle-Point)  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:15

3.4 First-Order Methods in Differentiable Programming 

First-order methods update variables using only gradient information [136], avoiding the second-derivative computations required by higher-order schemes such as Newton‚Äôs method [38]. Their simplicity, low per-iteration cost, and scalability make them well suited to large-scale and high-dimensional problems [46]. In differentiable programming, they serve as core components by lever-aging automatic differentiation for end-to-end trainable updates. Accelerated variants further im-prove convergence without added complexity [89], with Nesterov‚Äôs method [104] being a key ex-ample. In this section, we introduce several first-order methods, including Primal‚ÄìDual Gradient Descent (PDG) [10, 121], the Alternating Direction Method of Multipliers (ADMM) [26], and the Primal‚ÄìDual Hybrid Gradient (PDHG) algorithm [31, 158]. We then illustrate and compare these methods, as shown in Figure 3 2, on the NNLS problem (36), where A ‚àà Rùëö √óùëõ and b ‚àà Rùëö :min  

> x
> 1
> 2

‚ÄñAx ‚àí b‚Äñ22

s.t. x ‚â• 0. (36) 

3.4.1 PDG. PDG [43] is a fundamental variant of Gradient Descent (GD) [10, 66, 121]. GD mini-mizes a differentiable objective ùëì (ùë• ) by iteratively stepping in the direction of steepest descent: 

ùë• ùë° +1 = ùë• ùë° ‚àí ùõæ ‚àáùëì (ùë• ùë° ), (37) where ùõæ > 0 is the step size. For convex functions with an ùêø -Lipschitz continuous gradient [27], choosing ùõæ ‚àà ( 0, 1/ùêø ] yields an O( 1/ùëá ) convergence rate, and strong convexity gives linear con-vergence. Owing to its simplicity and efficiency, GD serves as the foundation for stochastic, accel-erated, and proximal variants, as well as constrained and composite optimization. PDG methods solve constrained problems by jointly updating primal and dual variables. Starting from the Lagrangian saddle-point formulation, PDG applies gradient descent to the primal variable and gradient ascent to the dual variable to enforce constraints [85]. Its simplicity, parallel structure, and broad applicability make it a standard tool in convex optimization, game theory, and large-scale distributed learning [102]. For the NNLS problem (36), introducing a nonnegative multiplier 

ùùÅ ‚àà Rùëõ gives the Lagrangian: 

L ( x, ùùÅ ) = 1 

> 2

‚ÄñAx ‚àí b‚Äñ22 ‚àí ùùÅ ‚ä§x. (38) The associated dual function is defined as: 

ùëî (ùùÅ ) = inf  

> x

L ( x, ùùÅ ). (39) In the PDG method, the primal variable x and dual variable ùùÅ are updated simultaneously by taking gradient steps on the Lagrangian (38). The gradients are: 

‚àáxL = A‚ä§ (Ax ‚àí b) ‚àí ùùÅ , ‚àáùùÅ L = ‚àíx. (40) With step sizes ùúè > 0 and ùúé > 0, the updates are: 

xùëò +1 = Œ†Rùëõ 

> ‚â•0

(xùëò ‚àí ùúè (A‚ä§ (Ax ùëò ‚àí b) ‚àí ùùÅ ùëò )), ùùÅ ùëò +1 = Œ†Rùëõ 

> ‚â•0

(ùùÅ ùëò ‚àí ùúé xùëò +1). (41) Here, the primal update reduces the objective, while the dual update enforces the constraint x ‚â• 0. Their coupled iterations converge to a saddle point (x‚àó, ùùÅ ‚àó) satisfying the KKT conditions. The operator Œ†Rùëõ  

> ‚â•0

denotes projection onto the nonnegative orthant, yielding the Projected Gradient Descent (PGD) method [36, 62]. More generally, when the primal variable must lie in a convex set 

C ‚äÜ Rùëõ , the update becomes ùë• ùëò +1 = Œ†{ùë• ‚àà C } (ùë• ùëò ‚àí ùúè (ùëê + ùê¥ ‚ä§~ùëò )) , where Œ†C (ùë• ) = arg min ùë£ ‚àà C ‚Äñùë• ‚àí

ùë£ ‚Äñ22 is the Euclidean projection [93]. This ensures feasibility at every iteration while moving in a descent direction.  

> 2Source code will be available at https://github.com/convexsoft/AITIP. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:16 Tao, Tong and Tan

Algorithm 1 ADMM Algorithm 

Require: Initial values x0, y0, and ùùÄ 0. 

> 1:

repeat  

> 2:

Update the primal variables x, y, and ùùÄ according to (44), (45), and (46), respectively;  

> 3:

until the convergence criteria (47) is satisfied; 

Ensure: Optimal solutions x‚òÖ, y‚òÖ, and ùùÄ ‚òÖ.

3.4.2 ADMM. ADMM [26] is an optimization algorithm that blends the decomposability of dual ascent with the robustness of the method of multipliers. It is particularly well suited for large-scale and distributed convex problems whose objectives decompose into separable components [32]. Owing to its scalability and convergence guarantees, ADMM has become a foundational tool in modern optimization, with applications in statistical learning, signal processing, and networked control [68]. To apply ADMM to the NNLS problem (36), we introduce an auxiliary variable y and rewrite it as: min  

> x
> 1
> 2

‚Äñy‚Äñ22

s.t. Ax ‚àí b = y, x ‚â• 0, (42) where A ‚àà Rùëö √óùëõ , b ‚àà Rùëö , and y ‚àà Rùëö . Introducing a Lagrange multiplier ùùÄ ‚àà Rùëö for Ax ‚àí b = y,the augmented Lagrangian of the problem (42) becomes: 

L ( x ‚â• 0, y, ùùÄ ) = 1 

> 2

‚Äñy‚Äñ22 + ùùÄ ‚ä§ (Ax ‚àí b ‚àí y) + ùúå  

> 2

‚ÄñAx ‚àí b ‚àí y‚Äñ22, (43) where ùúå > 0 is a penalty parameter encouraging satisfaction of the coupling constraint. The ADMM algorithm combines the principles of dual ascent and the method of multipliers by alternating between minimizing the augmented Lagrangian (43) with respect to the primal variables x and y, followed by updating the dual variable ùùÄ . Starting from initial values (x0, y0, ùùÄ 0),the iterations proceed as: 

xùëò +1 = arg min  

> x‚â•0

Lùúå (x, yùëò , ùùÄ ùëò ) = arg min  

> x‚â•0

{( ùùÄ ùëò )‚ä§Ax + ùúå 

2 Ax ‚àí b ‚àí yùëò 22}, (44) 

yùëò +1 = arg min  

> y

Lùúå (xùëò +1, y, ùùÄ ùëò ) = arg min  

> y

{ 1

2 ‚Äñy‚Äñ22 ‚àí ( ùùÄ ùëò )‚ä§y + ùúå 

2 Ax ùëò +1 ‚àí b ‚àí y 22}, (45) 

ùùÄ ùëò +1 = ùùÄ ùëò + ùúå (Ax ùëò +1 ‚àí b ‚àí yùëò +1). (46) Convergence is monitored using the primal residual and the dual residual: 

rùëò +1 = Ax ùëò +1 ‚àí b ‚àí yùëò +1, sùëò +1 = ùúå A‚ä§ (yùëò +1 ‚àí yùëò ). (47) As shown in Algorithm 1, ADMM method stops when both residuals fall below prescribed tol-erances, signaling satisfaction of the KKT conditions. The primal residual measures constraint violation, while the dual residual reflects deviation from stationarity and thus dual optimality. For convex problems, ADMM converges to the optimal solution (x‚òÖ, y‚òÖ, ùùÄ ‚òÖ). As a penalized alternating minimization method, ADMM balances primal and dual progress, supports parallel implementa-tions, and combines the robustness of the method of multipliers with the flexibility of variable splitting, making it well suited to large-scale and distributed convex optimization. 

3.4.3 PDHG. PDHG is a first-order primal‚Äìdual algorithm for large-scale convex and saddle-point problems [31, 158]. Each iteration jointly updates the primal and dual variables via simple prox-imal or gradient steps with linear operator evaluations, using inexpensive projections instead of costly linear solves [50]. PDHG excels in handling sparse and structured operators, is naturally par-allelizable and preconditionable, and has found broad applications in imaging, total variation and  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:17

Algorithm 2 PDHG Algorithm 

Require: Initial x0, y0, and ¬Øx 0 = x0; step sizes ùúè, ùúé > 0 and extrapolation parameter ùúÉ ‚àà [ 0, 1]. 

> 1:

Set iteration counter ùëò ‚Üê 0; 

> 2:

repeat  

> 3:

Dual update: update yùëò +1 according to the PDHG rule in (50);  

> 4:

Primal update: update xùëò +1 according to the PDHG rule in (51);  

> 5:

Extrapolation: compute ¬Øxùëò +1 using (52);  

> 6:

Check convergence criteria; if satisfied, terminate;  

> 7:

Set ùëò ‚Üê ùëò + 1; 

> 8:

until convergence; 

Ensure: Primal‚Äìdual solution (xùëò , yùëò ).

‚Ñì1 regularization, optimal transport, and linear programming. With strong convexity or effective preconditioning, PDHG can be further accelerated for improved convergence speed and robust-ness. Specifically, the general problem addressed by the PDHG method is the generic saddle-point formulation: min  

> x‚ààùëã

max 

> y‚ààùëå

(„ÄàKx , y„Äâ + ùê∫ (x) ‚àí ùêπ ‚àó (y)), (48) where ùê∫ : ùëã ‚Üí [ 0, +‚àû] and ùêπ ‚àó : ùëå ‚Üí [ 0, +‚àû] are proper, convex, lower-semicontinuous func-tions, and ùêπ ‚àó denotes the convex conjugate of a convex l.s.c. function ùêπ . The corresponding primal and dual problems are: min  

> x‚ààùëã

ùêπ (Kx ) + ùê∫ (x), max  

> y‚ààùëå

‚àí(ùê∫ ‚àó (‚àí K‚àóy) ‚àí ùêπ ‚àó (y)). (49) Following the Chambolle‚ÄìPock framework [31], the PDHG iterations (Algorithm 2) take the form: 

yùëõ +1 = prox ùúéùêπ ‚àó (yùëõ + ùúé K¬Øxùëõ ) , (50) 

xùëõ +1 = prox ùúèùê∫ 

(xùëõ ‚àí ùúè K‚àóyùëõ +1) , (51) 

¬Øxùëõ +1 = xùëõ +1 + ùúÉ (xùëõ +1 ‚àí xùëõ ), (52) where K‚àó denotes the adjoint of K, with the extrapolation parameter typically set to ùúÉ = 1, and the proximal operator [111] is: prox ùúéùêπ ‚àó (q) = arg min 

> y

( 1 

> 2

‚Äñy ‚àí q‚Äñ22 + ùúéùêπ ‚àó (y)) . (53) In particular, an equivalent variant of PDHG replaces ¬Øxùëõ +1 with ¬Øyùëõ +1 = yùëõ +1 + ùúÉ (yùëõ +1 ‚àí yùëõ ),which is obtained by interchanging the primal and dual update steps. As an illustrative example, consider again the NNLS problem (36): min  

> x‚â•01
> 2

‚ÄñAx ‚àí b‚Äñ22. (54) Using the Fenchel saddle-point representation derived in (NNLS-Fenchel-Saddle-Point), we ob-tain the equivalent formulation: max  

> y

min 

> x‚â•0

{„ÄàAx , y„Äâ ‚àí 1 

> 2

‚Äñy‚Äñ22 ‚àí b‚ä§y} = max  

> y

min  

> x‚â•0

{„Äà Ax , y„Äâ ‚àí ùê∫ ‚àó (y)} , (55) 

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:18 Tao, Tong and Tan           

> 0 0 00  0 00 
> 0  
> 0  
> 0  
> 0 0
> 0‚Äñ A     ‚Äñ
> 
> 
> 

(a) Objective Value.             

> 0 0 00  0 00 
> 0  
> 0  
> 0  0
> 0  
> 0  
> 0  
> 0  
> 0 0
> ‚Äñx   x ‚Äñ
> ‚ãÜ‚ãÜ  ‚ãÜ 
> 
> 
> 

(b) Distance to Reference Solution.          

> 0 0  00  0  00 
> 0  
> 0  
> 0  0
> 0  
> 0  
> 0  
> 0  
> 0 0
>   
>   
> 
> 

(c) KKT Residual. Fig. 3. Convergence comparison of PDG, ADMM, and PDHG on the NNLS problem. 

where ùêπ (x) = ùúÑ Rùëõ  

> ‚â•0

(x) and ùê∫ ‚àó (y) = 1 

> 2

‚Äñy‚Äñ22 + b‚ä§y. The PDHG update of x associated with (55) is: 

xùëò +1 = prox ùúèùêπ (xùëò ‚àí ùúèùê¥ ‚ä§yùëò )

= arg min 

> x

[ 1 

> 2

‚Äñx ‚àí (xùëò ‚àí ùúèùê¥ ‚ä§yùëò ) ‚Äñ22 + ùúèùêπ (x)]

= arg min 

> x‚â•0

[ 1 

> 2

‚Äñx ‚àí (xùëò ‚àí ùúèùê¥ ‚ä§yùëò ) ‚Äñ22

]

= Œ†Rùëõ  

> ‚â•0

(xùëò ‚àí ùúèùê¥ ‚ä§yùëò ). (56) Similarly, the PDHG update of y is: 

yùëò +1 = prox ùúéùê∫ ‚àó (yùëò + ùúéùê¥ ¬Øxùëò +1)

= arg min 

> y

[ 1 

> 2

‚Äñy ‚àí ( yùëò + ùúéùê¥ ¬Øxùëò +1)‚Äñ 22 + ùúéùê∫ ‚àó (y)]

= arg min 

> y

[ 1 

> 2

(1 + ùúé )‚Äñ y‚Äñ22 ‚àí ( yùëò + ùúéùê¥ ¬Øxùëò +1)‚ä§y + ùúé b‚ä§y]

= yùëò + ùúé (ùê¥ ¬Øxùëò +1 ‚àí b)

1 + ùúé = yùëò + ùúé (ùê¥ (2xùëò +1 ‚àí xùëò ) ‚àí b)

1 + ùúé , (57) where ¬Øxùëò +1 = 2xùëò +1 ‚àí xùëò is the over-relaxed primal iterate. In particular, many enhanced variants of PDHG have emerged. A prominent example is Google‚Äôs Primal-Dual Hybrid Gradient enhanced for Linear Programming (PDLP) [11, 96]. Built on the clas-sical PDHG framework, PDLP tailors primal‚Äìdual updates to the LP saddle-point structure using a two-level scheme with outer restarts and inner PDHG-style iterations. These iterations preserve the projection steps of standard PDHG while incorporating two key improvements through an accept‚Äìreject adaptive step size and a dynamic primal‚Äìdual weighting factor. This design yields strong scalability and convergence, and its differentiable form enables integration as an optimiza-tion layer for end-to-end learning. 

3.5 PyTorch Implementation of Differentiable Programming for Primal‚ÄìDual Cone Programs 

As shown in Figure 1, differentiable programming provides a unified framework for solving cone programs with first-order methods. Primal variables are updated by gradient steps, while dual vari-ables follow projected ascent in the dual space. Dual bounds and duality gaps quantify consistency between primal and dual iterates and guide refinement of the primal solution. Under strong du-ality, the dual problem yields an exact certificate of optimality and supports modern applications such as distributed optimization, sparse modeling, and support vector machines. In this section, we revisit the classical NNLS problem (NNLS) and its large-scale distributed extension to illustrate 

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:19 

how differentiable programming, built on Lagrangian duality, provides a principled framework for analyzing cone programs and implementing them in PyTorch 3.

3.5.1 Classical NNLS Problem Implementation in PyTorch. We detail the NNLS primal‚Äìdual formu-lation and show how PyTorch can be used to implement efficient first-order algorithms for solving it. We further demonstrate how backpropagation enables learning and predicting dual variables, from which the primal NNLS solution can be recovered. 

Example 3.3 (Classical NNLS). Given A ‚àà Rùëö √óùëõ and b ‚àà Rùëö , find x ‚àà Rùëõ  

> +

by solving: 

min  

> x
> 1
> 2

‚ÄñAx ‚àí b‚Äñ22

s.t. x ‚â• 0. (58) 

Rewrite the corresponding dual problem (NNLS-Lagrangian-Dual) to the minimum dual problem: 

min  

> ùùÄ
> 1
> 2

‚ÄñùùÄ ‚Äñ22 + b‚ä§ùùÄ 

s.t. A‚ä§ùùÄ ‚àí ùùÅ = 0, ùùÅ ‚â• 0. (NNLS-Dual-Min) 

The NNLS problem is a convex cone program that satisfies both the Slater condition and the KKT conditions. Consequently, its optimal primal solution x‚àó can be derived from the optimal dual solution. 

(1) ADMM with PyTorch: The augmented Lagrangian of (NNLS-Dual-Min) is given by: 

Lùúå (ùùÄ , ùùÅ , z) = 1 

> 2

‚ÄñùùÄ ‚Äñ22 + b‚ä§ùùÄ + z‚ä§ (A‚ä§ùùÄ ‚àí ùùÅ ) + ùúå  

> 2

‚ÄñA‚ä§ùùÄ ‚àí ùùÅ ‚Äñ22, (59) where z denotes the Lagrange multipliers (i.e., dual variable) and ùúå > 0 is a penalty parameter. By rearranging the Lagrangian function (59), removing constant terms, and defining ùùä = 1 

> ùúå

z, the corresponding augmented Lagrangian in the scaled dual form can be obtained as: 

Lùúå (ùùÄ , ùùÅ , ùùä ) = 1 

> 2

‚ÄñùùÄ ‚Äñ22 + b‚ä§ùùÄ + ùúå  

> 2

‚ÄñA‚ä§ùùÄ ‚àí ùùÅ + ùùä ‚Äñ22, (60) The ADMM iterative updates for solving this problem are given by: 

ùùÄ ùëò +1 = arg min  

> ùùÄ
> 1
> 2

‚ÄñùùÄ ‚Äñ22 + b‚ä§ùùÄ + ùúå  

> 2

‚ÄñA‚ä§ùùÄ ‚àí ùùÅ ùëò + ùùä ùëò ‚Äñ22, (61) 

ùùÅ ùëò +1 = arg min   

> ùùÅ ‚â•0
> ùúå
> 2

‚ÄñA‚ä§ùùÄ ùëò +1 ‚àí ùùÅ + ùùä ùëò ‚Äñ22, (62) 

ùùä ùëò +1 = ùùä ùëò + ( A‚ä§ùùÄ ùëò +1 ‚àí ùùÅ ùëò +1). (63) 

Fig. 4. ADMM with PyTorch. 

The iterative procedure decomposes the problem into simple subproblems with respect to ùùÄ ,

ùùÅ , and ùùä , each admitting a closed-form or efficiently computable update. As shown in the code below, this process is easily implemented in PyTorch to leverage GPU acceleration and parallel computation. With PyTorch‚Äôs efficient tensor operations and built-in projection support (Figure 4), the ADMM-based dual optimization pipeline achieves scalable, high-performance computation for large-scale NNLS problems within the differentiable programming framework.   

> 3Source code will be available at https://github.com/convexsoft/AITIP. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:20 Tao, Tong and Tan
> 1

def nnls_dual_admm (A , b , rho =1.0 , iters =500) :  

> 2

m , n = A . shape  

> 3

AT = A . T  

> 4

L = torch . linalg . cholesky ( torch . eye (m , dtype = A . dtype , device = A. 

device )  

> 5

+ rho *( A@AT ) )  

> 6

lam , mu , v = [ torch . zeros ( sz , dtype = A . dtype , device = A . device ) for sz 

in (m , n , n ) ]  

> 7

for _ in range ( iters ):  

> 8

lam = torch . cholesky_solve (( - b + rho * A@ ( mu - v )) . unsqueeze (1) , L ) 

. squeeze (1)  

> 9

z = AT@lam + v 

> 10

mu = torch . clamp (z , min =0.0)  

> 11

v = v + AT@lam - mu  

> 12

x = torch . clamp ( AT@lam , min =0.0)  

> 13

return lam , x

(2) Learning with PyTorch: An alternative approach to solving the dual problem (NNLS-Dual-Min) is to treat the unconstrained Lagrangian as a trainable loss and use PyTorch to learn the optimal dual solution (Figure 5). As illustrated in the code block below, once reformulated into this uncon-strained form, the objective becomes minimizing the Lagrangian as a differentiable loss. Through reverse-mode automatic differentiation, PyTorch computes and backpropagates gradients to itera-tively update the variables using its built-in gradient-based optimizers until convergence, enabling end-to-end learning of the optimal solution. Specifically, by introducing the associated Lagrange multipliers z, the corresponding Lagrangian (i.e., loss function) of (NNLS-Dual-Min) is: 

L ( ùùÄ , ùùÅ ‚â• 0, z) = 1 

> 2

‚ÄñùùÄ ‚Äñ22 + b‚ä§ùùÄ + z‚ä§ (A‚ä§ùùÄ ‚àí ùùÅ ). (64) 

Fig. 5. Learning with PyTorch.  

> 1

def learn_nnls_ du al _b y _b ac kp r op (A , b , lr =1 e -2 , steps =3000 , eps =1 e -6) :  

> 2

m , n = A . shape  

> 3

lam , mu , z = [ torch . nn . Parameter ( torch . zeros (sz , dtype = A . dtype , 

device = A . device ) ) for sz in (m , n , n ) ]  

> 4

L = lambda : 0.5*( lam@lam ) + b@lam + z@ (A . T@lam - mu )  

> 5

opt_min = torch . optim . Adam ([ lam , mu ], lr = lr )  

> 6

opt_max = torch . optim . Adam ([ z ], lr = lr )  

> 7

for _ in range ( steps ):  

> 8

opt_min . zero_grad () ; L () . backward () ; opt_min . step ()  

> 9

with torch . no_grad () : mu . clamp_ ( min =0.0)  

> 10

opt_max . zero_grad () ; (- L () ) . backward () ; opt_max . step ()  

> 11

if torch . linalg . norm ( A . T@lam - mu ) <= eps : break  

> 12

x = torch . clamp ( A . T@lam , min =0.0) . detach ()  

> 13

return lam . detach () , mu . detach () , z . detach () , x

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Learning to Optimize by Differentiable Programming 111:21           

> 0000 000 000 000 0000 000 000 ! !
> 0000
> 00
> 00 0
> 00
> 000
> 0
> 0 0
> 0 % %%$  %%  %%$!  %% %

!#  ! ! "         

>  
>   
>    
>   
>   

(a) Objective gap.          

> 0000 000 000 000 0000 000 000  
> 000
> 00
> 00
> 00
> 00
> 00
> 00
> 00
> 00 ##"   "  ##

      !  !        

>  
>   
>    
>   
>   

(b) Distance to the reference solution. Fig. 6. Convergence of the learning-based method toward a high-accuracy ADMM reference solution on the NNLS problem, implemented in PyTorch. 

As shown in Figure 6, we conduct a convergence comparison between the learning-based method and an ADMM solver implemented in PyTorch. The ADMM solution is treated as a high-accuracy reference optimum, enabling a clear assessment of how closely and how rapidly the learning-based approach approaches the true optimal solution. This comparison highlights both the optimization dynamics and the approximation quality achieved by the learning-based solver. 

3.5.2 Large-Scale NNLS Problem Implementation in PyTorch. We present the large-scale distributed NNLS problem and demonstrate its parallel, scalable multi-GPU implementation in PyTorch. 

Example 3.4 (Large-Scale Distributed NNLS). Considering the Large-scale NNLS (LNNLS): 

min  

> ùë• ‚â•0

1

2

> ùëÅ

√ï

> ùëñ =1

‚Äñùê¥ ùëñ ùë• ùëñ ‚àí ùëè ùëñ ‚Äñ22, (LNNLS) 

which includes separated objective blocks. The dual problem of (LNNLS) is: 

min 

> ùúÜ ùëñ

1

2

> ùëÅ

√ï

> ùëñ =1

‚ÄñùúÜ ùëñ ‚Äñ22 +

> ùëÅ

√ï

> ùëñ =1

ùëè ‚ä§ 

> ùëñ

ùúÜ ùëñ 

s.t. 

> ùëÅ

√ï

> ùëñ =1

ùê¥ ‚ä§ 

> ùëñ

ùúÜ ùëñ ‚â• 0.

(LNNLS-Dual) 

(1) Multi-GPU ADMM Implementation in PyTorch: Next, we solve the problem (LNNLS-Dual) via ADMM. Introducing local variables ùëß ùëñ ‚àà Rùëõ and a global consensus variable ùëß ‚àà Rùëõ , the dual problem becomes: min   

> ùúÜ ùëñ ,ùëß ùëñ ,ùëß

1

2

> ùëÅ

√ï

> ùëñ =1

‚ÄñùúÜ ùëñ ‚Äñ22 +

> ùëÅ

√ï

> ùëñ =1

ùëè ‚ä§ 

> ùëñ

ùúÜ ùëñ 

s.t. ùê¥ ‚ä§ 

> ùëñ

ùúÜ ùëñ = ùëß ùëñ , ùëß ùëñ = ùëß, ùëß ‚â• 0.

(LNNLS-Dual-rewrite) The corresponding augmented Lagrangian is defined as: 

Lùúå =

> ùëÅ

√ï

> ùëñ =1

( 1

2 ‚ÄñùúÜ ùëñ ‚Äñ22 + ùëè ‚ä§ 

> ùëñ

ùúÜ ùëñ + ùúå 

2 ‚Äñùê¥ ‚ä§ 

> ùëñ

ùúÜ ùëñ ‚àí ùëß ùëñ + ùë¢ ùëñ ‚Äñ22 + ùúå ùëê 

2 ‚Äñùëß ùëñ ‚àí ùëß + ùë£ ùëñ ‚Äñ22

)

, (65) 

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:22 Tao, Tong and Tan 

where ùúå and ùúå ùëê are penalty parameters, and ùë¢ ùëñ , ùë£ ùëñ are scaled dual variables. The iterative update rules for ADMM in the consensus form are then given by: 

ùúÜ ùëò +1 

> ùëñ

= arg min 

> ùúÜ ùëñ

{ 1

2 ‚ÄñùúÜ ùëñ ‚Äñ22 + ùëè ‚ä§ 

> ùëñ

ùúÜ ùëñ + ùúå 

2 ‚Äñùê¥ ‚ä§ 

> ùëñ

ùúÜ ùëñ ‚àí ùëß ùëò ùëñ + ùë¢ ùëò ùëñ ‚Äñ22

}

, (66) 

ùëß ùëò +1 

> ùëñ

= arg min 

> ùëß ùëñ

{ ùúå 

2 ‚Äñùê¥ ‚ä§ 

> ùëñ

ùúÜ ùëò +1 

> ùëñ

‚àí ùëß ùëñ + ùë¢ ùëò ùëñ ‚Äñ22 + ùúå ùëê 

2 ‚Äñùëß ùëñ ‚àí ùëß ùëò + ùë£ ùëò ùëñ ‚Äñ22

}

, (67) 

ùëß ùëò +1 = max 

{ 1

ùëÅ 

> ùëÅ

√ï

> ùëñ =1

(ùëß ùëò +1 

> ùëñ

+ ùë£ ùëò ùëñ ), 0

}

, (68) 

ùë¢ ùëò +1 

> ùëñ

= ùë¢ ùëò ùëñ + ùê¥ ‚ä§ 

> ùëñ

ùúÜ ùëò +1 

> ùëñ

‚àí ùëß ùëò +1 

> ùëñ

, (69) 

ùë£ ùëò +1 

> ùëñ

= ùë£ ùëò ùëñ + ùëß ùëò +1 

> ùëñ

‚àí ùëß ùëò +1. (70) 

Fig. 7. Multi-GPU ADMM Implementation in PyTorch. 

The iterative updates in large-scale dual problems are often complex and computationally de-manding. As illustrated in the code block below, PyTorch enables these updates to run in parallel across multiple GPUs, allowing high-dimensional variable updates to be distributed efficiently. As illustrated in Figure 7, this parallelization greatly improves efficiency and scalability, making large-scale NNLS and related first-order methods practical. This observation aligns with recent evidence that dual iterations can scale efficiently on GPUs when fully vectorized [4].  

> 1

def lnnls_dual_ad mm _b at ch (A , B , rho =1.0 , rho_c =1.0 , iters =1000) :  

> 2

N , m , n = A . shape  

> 3

A_T = A . transpose (1 , 2)  

> 4

L = torch . linalg . cholesky ( torch . eye (m , device = A . device ) + rho * torch 

. bmm (A , A_T ) )  

> 5

Lam = torch . zeros (( N , m ) , device =A . device )  

> 6

Z_i = torch . zeros (( N , n ) , device =A . device )  

> 7

Z = torch . zeros (n , device = A. device )  

> 8

U = V = torch . zeros_like ( Z_i )  

> 9

alpha , beta = rho / ( rho + rho_c ) , rho_c / ( rho + rho_c )  

> 10

for _ in range ( iters ):  

> 11

Lam = torch . cholesky_solve ( (- B - rho * torch . bmm (A , ( U - Z_i )  

> 12

. unsqueeze ( -1) ) ) . unsqueeze ( -1) , L ) . squeeze ( -1)  

> 13

A_Lam = torch . bmm ( A_T , Lam . unsqueeze ( -1) ). squeeze ( -1)  

> 14

Z_i = alpha * ( A_Lam + U ) + beta * ( Z - V)  

> 15

Z = torch . clamp (( Z_i + V ) . mean ( dim =0) , min =0.0)  

> 16

U += A_Lam - Z_i  

> 17

V += Z_i - Z 

> 18

X = torch . clamp ( torch . linalg . lstsq (A ,( B+ Lam ) . unsqueeze ( -1) )  

> 19

. solution . squeeze ( -1) , min =0.0)  

> 20

return Lam , X , Z 

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:23

4 Learning to Optimize by Differentiable Programming: Case Studies 

In this section, we present case studies showing how differentiable programming, combined with duality theory and first-order methods, provides a unified framework for solving Linear Program-ming (LP) [75] and Quadratic Programming (QP) [54]. Differentiable programming generates fea-sible primal solutions while simultaneously solving the dual problem to assess and refine their quality. The applications span power systems (optimal power flow [42]), machine learning verifica-tion (neural network verification [45]), and graph-based learning (Laplacian-regularized minimiza-tion [138]), as summarized in Table 2, illustrating the broad versatility and impact of differentiable programming. 

4.1 Case Study: Stiger Diet Problem 

The Stigler Diet (SD) problem [56, 73] is a classical benchmark in operations research and linear programming. Formulated by George Stigler during World War II, it seeks the lowest-cost diet that satisfies all essential nutritional requirements. Due to the computational limits of the 1940s, Stigler obtained only an approximate solution, and the exact optimum was later computed by Jack Ladman using the then-new simplex method‚Äîa process requiring nine staff members and nearly 120 man-days. With modern computational advances, solvers such as Google OR-Tools [57] and IBM CPLEX [71] now solve this problem in seconds. Below, we review the Stigler Diet Problem and discuss emerging opportunities for applying differentiable programming to this canonical LP task. 

Problem 1 (Stiger Diet Problem). Let x ‚àà Rùëõ denote the quantities of ùëõ food items, and let 

c ‚àà Rùëõ be their unit costs. The nutrient matrix ùê¥ ‚àà Rùëö √óùëõ contains entries ùëé ùëñ ùëó giving the amount of nutrient ùëó in food ùëñ , and b ‚àà Rùëö specifies the minimum daily requirements. The problem can be written in standard LP form: 

min  

> x

c‚ä§x

s.t. ùê¥ x ‚â• b, x ‚â• 0. (SD-Primal) 

The constraint ùê¥ x ‚â• b enforces nutritional adequacy, while x ‚â• 0 ensures nonnegative food quantities. The objective c‚ä§x is the total daily cost to be minimized. 

To form the Lagrangian, we introduce multipliers ùùÄ ‚â• 0 for the constraint b ‚àí ùê¥ x ‚â§ 0 and ùùÇ ‚â• 0for ‚àíx ‚â§ 0. The Lagrangian becomes: 

L ( x, ùùÄ , ùùÇ ) = c‚ä§x + ùùÄ ‚ä§ (b ‚àí ùê¥ x) ‚àí ùùÇ ‚ä§x. (71) The dual function is obtained by taking the infimum of the Lagrangian over all feasible x. This yields ùëî (ùùÄ , ùùÇ ) = b‚ä§ùùÄ when ùê¥ ‚ä§ùùÄ + ùùÇ = c, and ùëî = ‚àí‚àû otherwise. The dual problem is therefore: max  

> ùùÄ ,ùùÇ

b‚ä§ùùÄ 

s.t. ùê¥ ‚ä§ùùÄ + ùùÇ = c,

ùùÄ ‚â• 0, ùùÇ ‚â• 0.

(SD-Dual) Finally, the primal‚Äìdual pair can be equivalently expressed as the following saddle-point prob-lem: min  

> x‚ààRùëõ

max     

> ùùÄ 0,ùùÇ 0

L ( x, ùùÄ , ùùÇ ) = min  

> x‚ààRùëõ

max    

> ùùÄ 0,ùùÇ 0

[c‚ä§x + ùùÄ ‚ä§ (b ‚àí ùê¥ x) ‚àí ùùÇ ‚ä§x]. (72) The saddle point characterizes the primal and dual optima and guarantees strong duality un-der standard regularity conditions. This formulation can be solved numerically using PDHG or  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:24 Tao, Tong and Tan

ADMM, both of which fit naturally into the PyTorch framework for end-to-end differentiable op-timization. These methods iteratively update primal and dual variables and can be implemented as differentiable operators, enabling gradients to flow through parameters such as costs and con-straint matrices. This allows the Stigler Diet Problem to be solved efficiently while integrating seamlessly into differentiable programming, unifying data-driven models with mathematical opti-mization. 

4.2 Case Study: Neural Network Verification 

Deep neural networks have achieved remarkable success in areas such as image recognition and natural language processing but remain vulnerable to adversarial attacks, which are small and imperceptible perturbations that cause misclassification [110]. This vulnerability underscores the importance of Neural Network Verification (NNV) problem [6], which aims to formally prove that models satisfy specific mathematical properties ensuring reliability, safety, and robustness. How-ever, existing verification methods face scalability and computational challenges with limited for-mal guarantees. Differentiable programming offers a promising solution by framing verification as an optimization problem that unites learning with formal optimization principles [45]. Through the use of optimization duality, automatic differentiation, and predictor‚Äìverifier training, differen-tiable programming enables the development of neural networks that are provably robust, compu-tationally efficient, and equipped with formal certificates of correctness. 

Problem 2 (Neural Network Verification Problem). Feed-forward neural networks trained with losses such as cross-entropy or squared error operate on input‚Äìoutput pairs (ùë• nom , ~ true ). The network is modeled as a sequence of layer transformations ‚Ñéùëò , producing activations ùë• ùëò = ‚Ñéùëò (ùë• ùëò ‚àí1)

for ùëò = 1, . . . , ùêæ , mapping the input ùë• 0 to the final output ùë• ùêæ . Verification seeks to ensure that, for all inputs within a prescribed neighborhood ùëÜ in (ùë• nom ), the output satisfies a linear property ùëê ‚ä§ùë• ùêæ + ùëë ‚â§

0, where ùëê and ùëë depend on ùë• nom and the true label ~true . This formulation captures tasks such as adversarial robustness and monotonicity certification. To search for counterexamples, the verification task is cast as the optimization problem: 

max  

> ùë•

ùëê ‚ä§ùë• ùêæ + ùëë 

s.t. ùë• 0 ‚àà ùëÜ in (ùë• nom ),ùë• ùëò +1 = ‚Ñéùëò (ùë• ùëò ), ùëò = 0, . . . , ùêæ ‚àí 1.

(NNV-Primal) 

If the optimal value of (NNV-Primal) is strictly negative, the network is guaranteed to satisfy the specification under all admissible input perturbations. 

Introducing Lagrange multipliers ùúÜ ùëò associated with the equality constraints ùë• ùëò +1 = ‚Ñéùëò (ùë• ùëò ), the Lagrangian is defined as: 

ùêø (ùë•, ùúÜ ) = ùëê ‚ä§ùë• ùêæ + ùëë + 

> ùêæ ‚àí1

√ï

> ùëò =0

ùúÜ ‚ä§

> ùëò

(ùë• ùëò +1 ‚àí ‚Ñéùëò (ùë• ùëò ))

= ùëë + (ùëê ‚ä§ùë• ùêæ + ùúÜ ‚ä§ 

> ùêæ ‚àí1

ùë• ùêæ 

) + 

> ùêæ ‚àí1

√ï

> ùëò =1

(ùúÜ ‚ä§ 

> ùëò ‚àí1

ùë• ùëò ‚àí ùúÜ ‚ä§ 

> ùëò

‚Ñéùëò (ùë• ùëò )) ‚àí ùúÜ ‚ä§ 

> 0

‚Ñé0 (ùë• 0). (73)  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:25

The dual function is ùëî (ùúÜ ) = sup ùë• ùêø (ùë•, ùúÜ ), where the feasible region reflects the admissible input set and propagated activation bounds. The resulting dual problem is: min  

> ùúÜ 0,...,ùúÜ ùêæ ‚àí1

ùëî (ùúÜ ) = ùëë + ùúì 0 (ùúÜ 0) +  

> ùêæ ‚àí1

√ï

> ùëò =0

ùúô ùëò (ùúÜ ùëò ‚àí1, ùúÜ ùëò )

s.t. ùúÜ ùêæ ‚àí1 = ‚àíùëê, 

(NNV-Dual) where ùúô ùëò (ùúÜ ùëò ‚àí1, ùúÜ ùëò ) = sup ùë• ùëò [ùúÜ ‚ä§ 

> ùëò ‚àí1

ùë• ùëò ‚àí ùúÜ ‚ä§ 

> ùëò

‚Ñéùëò (ùë• ùëò )] and ùúì 0 (ùúÜ 0) = sup ùë• 0 ‚ààùëÜ in [‚àí ùúÜ ‚ä§ 

> 0

‚Ñé0 (ùë• 0)] .To solve problem (NNV-Dual), we use a primal‚Äìdual gradient scheme with projected updates. Each layer is assigned step sizes ùúè ùëò > 0 and ùúé ùëò > 0. The primal update performs a gradient ascent step on the Lagrangian followed by projection onto the feasible set: 

ùë• ùë° +1 

> ùëò

= Œ†Cùëò 

(ùë• ùë° ùëò + ùúè ùëò ‚àáùë• ùëò ùêø (ùë•, ùúÜ )) , (74) where Cùëò is the admissible set for ùë• ùëò . The projection Œ†Cùëò enforces feasibility. For instance, for box constraints [ùëô, ùë¢ ], it reduces to elementwise clipping Œ†[ùëô,ùë¢ ] (ùëß ) = min (max (ùëß, ùëô ), ùë¢ ). For the final layer, the update becomes: 

ùë• ùë° +1 

> ùêæ

= Œ†[ùëô ùêæ ,ùë¢ ùêæ ]

(ùë• ùë° ùêæ + ùúè ùêæ 

(ùëê + ùúÜ ùë° ùêæ ‚àí1

) ) . (75) Let ùêΩ ‚Ñéùëò (ùë• ùëò ) denote the Jacobian of ‚Ñéùëò at ùë• ùëò . For piecewise linear mappings such as ReLU, we use the appropriate subgradient or the Jacobian of the active region. Its entries satisfy [ùêΩ ‚Ñéùëò (ùë• ùëò )] ùëñ,ùëó =

ùúï [‚Ñéùëò (ùë• ùëò )] ùëñ /ùúï (ùë• ùëò )ùëó . With this notation, the update for intermediate layers ùëò = 1, . . . , ùêæ ‚àí 1 becomes: 

ùë• ùë° +1 

> ùëò

= Œ†[ùëô ùëò ,ùë¢ ùëò ]

(ùë• ùë° ùëò + ùúè ùëò 

(ùúÜ ùë° ùëò ‚àí1 ‚àí ùêΩ ‚Ñéùëò (ùë• ùë° ùëò )‚ä§ùúÜ ùë° ùëò 

) ) . (76) Next, at the input layer, subject to both box constraints and the admissible set, we have: 

ùë• ùë° +10 = Œ†ùëÜ in ‚à©[ ùëô 0,ùë¢ 0 ]

(ùë• ùë°  

> 0

+ ùúè 0

( ‚àí ùêΩ ‚Ñé0 (ùë• ùë° 

> 0

)‚ä§ùúÜ ùë° 

> 0

) ) , (77) where ùëÜ in = {ùë• : ‚Äñùë• ‚àíùë• nom ‚Äñ‚àû ‚â§ ùúÄ }, implemented by clipping each coordinate to [ùë• nom ‚àíùúÄ, ùë• nom +ùúÄ ].Finally, the dual variables are updated by projected gradient descent on the constraint residuals: 

ùúÜ ùë° +1 

> ùëò

= ùúÜ ùë° ùëò ‚àí ùúé ùëò 

(ùë• ùë° +1 

> ùëò +1

‚àí ‚Ñéùëò (ùë• ùë° +1 

> ùëò

)), ùëò = 0, . . . , ùêæ ‚àí 1. (78) If ùëî (ùúÜ ) < 0, then the condition ùëê ‚ä§ùúô (ùë• 0) + ùëë ‚â§ 0 holds for all ùë• 0 ‚àà ùëÜ in (ùë• nom ), providing a cer-tificate that the network satisfies the specification. In practice, the primal‚Äìdual updates can be implemented efficiently in PyTorch with GPU acceleration, using automatic differentiation and optimized tensor operations for large-scale models. 

‚Ä¢SOS-Based Polynomial and SDP Relaxations for NNV Problem: Beyond first-order pri-mal‚Äìdual approaches, neural network verification can also be framed algebraically by encoding all network constraints within a global optimization model. This viewpoint enables convex relaxation techniques that approximate the network‚Äôs nonlinear and nonconvex behavior with tractable sur-rogates. A prominent example is the family of Sum-of-Squares (SOS) relaxations [87, 157], which reformulate verification as a polynomial program and relax it into a hierarchy of semidefinite programs. Each relaxation level provides a certified lower bound that converges under standard assumptions: positive bounds confirm the property, while nonpositive ones may expose violations. Although theoretically powerful, SOS methods grow rapidly in complexity and are typically ap-plied with care to preserve scalability. A more scalable alternative is the semidefinite relaxation introduced by Raghunathan et al. [118]. This method employs a rank-one lifting of network variables, expressing quadratic interactions in a higher-dimensional space and relaxing only the nonconvex rank constraint. The resulting struc-tured SDP preserves linear relations dictated by the architecture and input set, producing relax-ations that are significantly tighter than linear bounds yet far more tractable than higher-order  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:26 Tao, Tong and Tan

Table 2. Representative Case Studies Integrating Differentiable Programming and First-Order Methods.                        

> Case Study Type Method Mechanism & Differentiable Benefit Implementation
> SD [56, 73] LP ADMM, PDHG
> Mech: Dual splitting.
> Gain: Differentiable for end-to-end learning. D-LADMM [152]
> NNV [45] LP PDG, PGD Mech: Convex relaxation of ReLUs.
> Gain: Differentiable for joint training. CVXPYLayer [40]; Auto_LiRPA [153];
> ùõΩ -CROWN [147]
> OPF [131] QP PDG Mech: Lagrangian structure.
> Gain: Differentiable for controller learning. DeepOPF-U [91]
> LRMP [150] QP ADMM Mech: Graph quadratics.
> Gain: Ddifferentiable for adaptive regularization. CVXPYLayer [40]; AA_DLADMM [47]

SOS hierarchies [29, 35, 97, 100]. A positive SDP certificate verifies the specification, offering a practical balance between accuracy and efficiency. Together, SOS hierarchies and rank-one lift-ing SDPs complement first-order optimization methods, expanding the toolkit for rigorous and scalable neural network verification. 

4.3 Case Study: Optimal Power Flow Problem 

Optimal Power Flow (OPF) problem is a fundamental optimization problem in power systems and is widely regarded as one of the most important operational tasks. The goal of OPF is to deter-mine how to adjust the controllable variables in a power network in order to minimize a chosen objective such as generation cost or transmission losses while strictly enforcing the governing physical laws given by the power flow equations and respecting all engineering limits including voltage magnitudes, generator capacities, and line flow constraints. The OPF formulation is intrin-sically nonlinear and nonconvex, which makes it a challenging problem to solve. Since Carpentier introduced the original formulation in 1962, a large body of research has focused on efficient so-lution strategies. Numerous optimization methods have been explored, including nonlinear pro-gramming, quadratic programming, and Newton based algorithms, each contributing to the de-velopment of modern OPF solution techniques. In this section, we examine how differentiable programming can be applied to solve the OPF problem. 

Problem 3 (Optimal Power Flow Problem). Consider a power network with bus set N and transmission line set E. Let ùë£ ‚àà RùëÅ denote the vector of bus voltages, constrained by lower and upper bounds ùëâ and ùëâ . The network is modeled by a weighted Laplacian conductance matrix ùê∫ ‚àà RùëÅ √óùëÅ ,where each line (ùëñ, ùëó ) ‚àà E has a positive conductance ùëî ùëñ ùëó > 0. For each bus ùëñ , we define the nodal power surrogate ùëù ùëñ (ùë£ ) = ùë£ ùëñ (ùê∫ùë£ )ùëñ . Let ùëë ùëñ denote the demand at load bus ùëñ ‚àà L and ùëù ùëñ the generation capacity limit at generator bus ùëñ ‚àà G . In addition, each line (ùëñ, ùëó ) ‚àà E is subject to a thermal current limit ùêº max  

> ùëñ ùëó

. The optimal power flow problem is then formulated as: 

min   

> ùë£ ‚ààRùëÅ

ùëì (ùë£ ) = ùë£ ‚ä§ùê∫ùë£ =

√ï 

> (ùëñ,ùëó ) ‚àà E

ùëî ùëñ ùëó (ùë£ ùëñ ‚àí ùë£ ùëó )2 (OPF-Primal) 

s.t. ùëâ ‚â§ ùë£ ‚â§ ùëâ , ‚àÄùëñ ‚àà N , (79) 

ùëù ùëñ (ùë£ ) ‚â• ùëë ùëñ , ‚àÄùëñ ‚àà L , (80) 

ùëù ùëñ (ùë£ ) ‚â§ ùëù ùëñ , ‚àÄùëñ ‚àà G , (81) 

ùëî ùëñ ùëó (ùë£ ùëñ ‚àí ùë£ ùëó ) ‚â§ ùêº max  

> ùëñ ùëó

, ‚àÄ( ùëñ, ùëó ) ‚àà E . (82)  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:27

The objective (OPF-Primal) minimizes the total resistive losses of the network. Constraint (79) enforces operational voltage bounds at all buses. Constraints (80) ‚Äì(81) ensure that the nodal power surrogate meets the load demands while respecting generator capacity limits. Finally, constraint (82) imposes thermal current limits on each transmission line, thereby ensuring safe operation of the network. 

Here, we temporarily disregard Constraint (79) on voltage bounds in order to focus on the load balance, generation, and line-flow constraints. For each line (ùëñ, ùëó ) ‚àà E , define the line current surrogate ùëñ ùëñ ùëó (ùë£ ) = ùëî ùëñ ùëó (ùë£ ùëñ ‚àí ùë£ ùëó ), which allows the thermal limit constraint to be written as |ùëñ ùëñ ùëó (ùë£ )| ‚â§ 

ùêº max  

> ùëñ ùëó

. Introducing Lagrange multipliers for these constraints yields the Lagrangian: 

ùêø (ùë£, ùúÜ, ùõæ, ùúá ) = ùë£ ‚ä§ùê∫ùë£ +

√ï 

> ùëñ ‚àà L

ùúÜ ùëñ 

(ùëë ùëñ ‚àí ùëù ùëñ (ùë£ )) +

√ï 

> ùëñ ‚àà G

ùõæ ùëñ 

(ùëù ùëñ (ùë£ ) ‚àí ùëù ùëñ 

) +

√ï 

> (ùëñ,ùëó ) ‚àà E

ùúá ùëñ ùëó 

( |ùëñ ùëñ ùëó (ùë£ )| ‚àí ùêº max 

> ùëñ ùëó

), (83) where ùúÜ ùëñ , ùõæ ùëñ , and ùúá ùëñ ùëó denote the dual variables associated with the load, generation, and line-flow constraints, respectively. Accordingly, the associated saddle-point formulation is given by: min   

> ùë£ ‚ààRùëÅ

max    

> ùúÜ ‚â•0, ùõæ ‚â•0, ùúá ‚â•0

ùêø (ùë£, ùúÜ, ùõæ, ùúá ). (84) For analytical insight, it is convenient to first examine the dual problem obtained when line-flow limit (82) is ignored, that is, when ùúá ‚â° 0. In this simplified setting, the Lagrangian reduces to 

ùêø (ùë£, ùúÜ, ùõæ, 0), and the corresponding dual problem takes the form: max 

> ùúÜ,ùõæ

√ï 

> ùëñ ‚àà L

ùúÜ ùëñ ùëë ùëñ ‚àí

√ï 

> ùëñ ‚àà G

ùõæ ùëñ ùëù ùëñ 

s.t. ùê∫ ‚àí 1 

> 2

diag (ùúÜ ‚àí ùõæ )ùê∫ ‚àí 1

> 2

ùê∫ diag (ùúÜ ‚àí ùõæ )  0,ùúÜ ùëñ ‚â• 0, ùëñ ‚àà L ,ùõæ ùëñ ‚â• 0, ùëñ ‚àà G .

(OPF-Dual) This semidefinite dual highlights the role of the nodal power constraints; in the full model with line-flow limits, we instead work directly with the saddle-point formulation (84). We next employ a projected primal‚Äìdual gradient algorithm to solve the saddle-point formula-tion (84). At each iteration, the primal variable ùë£ is updated by taking a gradient descent step on the Lagrangian, where the gradient is automatically computed via PyTorch‚Äôs autograd. This step is followed by a projection onto the feasible voltage box to enforce operational limits: 

ùë£ ùëò +1 = Œ†[ùëâ , ùëâ ](ùë£ ùëò ‚àí ùúè ‚àáùë£ ùêø (ùë£ ùëò , ùúÜ ùëò , ùõæ ùëò , ùúá ùëò )) , (85) where ùúè > 0 is the primal step size and Œ†[ùëâ ,ùëâ ] (¬∑) denotes the projection operator implemented via project_voltage. The dual variables are updated by projected gradient ascent: 

ùúÜ ùëò +1 

> ùëñ

= [ùúÜ ùëò ùëñ + ùúÇ (ùëë ùëñ ‚àí ùëù ùëñ (ùë£ ùëò +1))]+, ùëñ ‚àà L , (86) 

ùõæ ùëò +1 

> ùëñ

= [ùõæ ùëò ùëñ + ùúÇ (ùëù ùëñ (ùë£ ùëò +1) ‚àí ùëù ùëñ 

)]+, ùëñ ‚àà G , (87) 

ùúá ùëò +1 

> ùëñ ùëó

= [ùúá ùëò ùëñ ùëó + ùúÇ (|ùëî ùëñ ùëó (ùë£ ùëò +1 

> ùëñ

‚àí ùë£ ùëò +1 

> ùëó

)| ‚àí ùêº max 

> ùëñ ùëó

)]+, (ùëñ, ùëó ) ‚àà E , (88) where ùúÇ > 0 is the dual step size, [¬∑] + denotes elementwise projection onto the nonnegative or-thant, and ùëù ùëñ (ùë£ ) = ùë£ ùëñ (ùê∫ùë£ )ùëñ represents the nodal power surrogate. In practice, the gradient ‚àáùë£ ùêø is computed directly using PyTorch‚Äôs autograd, which automatically provides appropriate subgradi-ents for nonsmooth components such as absolute value terms. This enables a streamlined, fully differentiable implementation of the primal‚Äìdual dynamics within a differentiable programming framework.  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:28 Tao, Tong and Tan

4.4 Case Study: Laplacian Regularized Minimization Problem 

In this section, we examine the application of differentiable programming to the Laplacian Regular-ized Minimization Problem (LRMP) [33, 138, 150], which minimizes a convex objective composed of a general convex function and a Laplacian-based smoothness regularizer. As a subclass of convex optimization, LRMP admits various solution strategies depending on its structural properties [27]. 

Problem 4 (Laplacian Regularized Minimization Problem). Given a weighted graph with vertices 1, . . . , ùëõ and edge weights ùë§ ùëñ ùëó ‚â• 0, the regularizer takes the form L ( ùë• ) = √ç(ùëñ,ùëó ) ‚àà E ùë§ ùëñ ùëó (ùë• ùëñ ‚àí

ùë• ùëó )2, which enforces smoothness of the solution across neighboring nodes. Equivalently, this can be expressed using the weighted graph Laplacian ùêø ‚àà Rùëõ √óùëõ , satisfying ùêø = ùêø ‚ä§, ùêø ùëñ ùëó ‚â§ 0 for ùëñ ‚â† ùëó , and 

ùêø 1 = 0 for the all-ones vector 1. The LRMP is then formulated as: 

min   

> ùë• ‚ààRùëõ

ùêπ (ùë• ) = ùëì (ùë• ) + 1 

> 2

ùë• ‚ä§ùêøùë•, (89) 

where ùëì : Rùëõ ‚Üí R ‚à™ {+‚àû} denotes a proper, closed, convex function. 

‚Ä¢ Unconstrained LRMP: We first consider an unconstrained variant of LRMP, which consists of a quadratic data-fidelity term and a smoothness regularizer: min  

> ùëß

ùëû ‚Äñ~ ‚àí ùëß ‚Äñ22 + ùëß ‚ä§ùêøùëß, (LRMP-Primal) where the scalar ùëû > 0 balances the trade-off between data fitting and Laplacian regularization. It is straightforward to verify that the primal problem admits the closed-form solution ùëß ‚àó =

(ùëûùêº + ùêø )‚àí1ùëû~ . However, this expression becomes invalid when the Laplacian matrix ùêø is singular, since ùêø is only positive semidefinite and thus lacks a true inverse. To address this issue, we first define ùë• = ~ ‚àí ùëß and introduce the Lagrange multiplier ùúÜ . The corresponding Lagrangian function of (LRMP-Primal) can then be written as: 

L = ùëûùë• ‚ä§ùë• + ùëß ‚ä§ùêøùëß + ùúÜ ‚ä§ (~ ‚àí ùëß ‚àí ùë• ). (90) Taking the derivatives of the Lagrangian with respect to ùë• and ùëß respectively, we obtain the optimal values ùë• ‚àó = ùúÜ /( 2ùëû ) and ùëß ‚àó = ùêø ‚Ä†ùúÜ /2. In particular, the inverse ùêø ‚àí1 is replaced by the Moore‚ÄìPenrose pseudoinverse ùêø ‚Ä† to ensure that all subsequent derivations remain well-defined over the range of ùêø . Substituting the optimal solutions into the Lagrangian (90) yields: 

L ( ùë• ‚àó, ùëß ‚àó) = ùëû ( ùúÜ  

> 2ùëû

)‚ä§ ( ùúÜ  

> 2ùëû

) + ( ùêø ‚Ä†ùúÜ  

> 2

)‚ä§ùêø ( ùêø ‚Ä†ùúÜ  

> 2

) + ùúÜ ‚ä§ (~ ‚àí ùêø ‚Ä†ùúÜ  

> 2

‚àí ùúÜ  

> 2ùëû

)

= ‚àí 1 

> 4ùëû

‚ÄñùúÜ ‚Äñ22 ‚àí 1 

> 4

ùúÜ ‚ä§ùêø ‚Ä†ùúÜ + ùúÜ ‚ä§~, (91) where (ùêø ‚Ä†)‚ä§ = ùêø ‚Ä† and ùêø ‚Ä†ùêøùêø ‚Ä† = ùêø ‚Ä† since ùêø is symmetric and positive semidefinite. Accordingly, the corresponding dual problem is formulated as: max  

> ùúÜ

‚àí 1 

> 4ùëû

‚ÄñùúÜ ‚Äñ22 ‚àí 1 

> 4

ùúÜ ‚ä§ùêø ‚Ä†ùúÜ + ùúÜ ‚ä§~. (LRMP-Dual) Here, the dual formulation (LRMP-Dual) is particularly amenable to differentiable programming. Its quadratic structure and dependence on the Laplacian pseudoinverse can be evaluated through automatic differentiation and efficient tensor operations. This enables the dual ascent iterations to be implemented directly in PyTorch, providing a scalable alternative to explicit pseudoinverse computation. 

‚Ä¢ Constrained LRMP: Next, we analyze the constrained LRMP. Consider a quadratic optimiza-tion task regularized by a graph Laplacian structure. Let ùê¥ ‚àà Rùëö √óùëõ and ùëè ‚àà Rùëö denote the data  

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:29

matrix and observation vector, and ùë• ‚àà Rùëõ be the decision variable subject to ùë•  0. The problem, referred to as Laplacian-Regularized Nonnegative Least Squares (LR-NNLS), is formulated as: min  

> ùë• 0

1

2 ‚Äñùê¥ùë• ‚àí ùëè ‚Äñ22 + 1

2 ùë• ‚ä§ùêøùë•, (LR-NNLS) where ùêø ‚àà Rùëõ √óùëõ is the graph Laplacian encoding the structural dependencies among components of ùë• . Although the problem in (LR-NNLS) is strongly convex, it remains challenging because the Laplacian ùêø is positive semidefinite yet singular, precluding a closed-form solution. Ignoring the nonnegativity constraint, one can express the minimum-norm solution via the pseudoinverse: 

ùë• ‚òÖ ‚âà [(ùê¥ ‚ä§ùê¥ + ùêø )‚Ä†ùê¥ ‚ä§ùëè ]

> +

, (92) where (ùê¥ ‚ä§ùê¥ + ùêø )‚Ä† denotes the Moore‚ÄìPenrose pseudoinverse of the regularized normal matrix. Despite its simplicity, this formulation is computationally expensive and numerically unstable for large-scale systems due to the spectral degeneracy of ùêø .Let ~ = ùê¥ùë• ‚àí ùëè and introduce the Lagrange multipliers ùúá  0 and ùúÜ ‚àà Rùëö . The corresponding Lagrangian function is then formulated as: 

L ( ùë•,~ ; ùúÜ, ùúá ) = 1 

> 2

‚Äñ~‚Äñ22 + 1 

> 2

ùë• ‚ä§ùêøùë• + ùúÜ ‚ä§ (ùê¥ùë• ‚àí ùëè ‚àí ~) ‚àí ùúá ‚ä§ùë• . (93) Minimizing the Lagrangian with respect to ~ and ùë• yields: 

~‚òÖ = arg min  

> ~

( 1 

> 2

‚Äñ~‚Äñ22 ‚àí ùúÜ ‚ä§~), ùë• ‚òÖ = arg min  

> ùë•

( 1 

> 2

ùë• ‚ä§ùêøùë• + ( ùê¥ ‚ä§ùúÜ ‚àí ùúá )‚ä§ùë• ). (94) Thus, the optimal primal variables are ~‚òÖ = ùúÜ and ùë• ‚òÖ = ùêø ‚Ä† (ùúá ‚àí ùê¥ ‚ä§ùúÜ ), where ùêø ‚Ä† is the Moore‚Äì Penrose pseudoinverse of ùêø . Substituting these into the Lagrangian yields the dual function: 

ùëî (ùúÜ, ùúá ) = ‚àí 1 

> 2

‚ÄñùúÜ ‚Äñ22 ‚àí ùúÜ ‚ä§ùëè ‚àí 1 

> 2

(ùê¥ ‚ä§ùúÜ ‚àí ùúá )‚ä§ùêø ‚Ä† (ùê¥ ‚ä§ùúÜ ‚àí ùúá ), (95) where ùúá  0. Hence, the dual problem is formulated as: max   

> ùúÜ ‚ààRùëö , ùúá 0

‚àí 1 

> 2

‚ÄñùúÜ ‚Äñ22 ‚àí ùúÜ ‚ä§ùëè ‚àí 1 

> 2

(ùê¥ ‚ä§ùúÜ ‚àí ùúá )‚ä§ùêø ‚Ä† (ùê¥ ‚ä§ùúÜ ‚àí ùúá ). (LR-NNLS-Dual) The dual system (LR-NNLS-Dual), though involving both equality and inequality multipliers, remains compatible with differentiable programming frameworks. Its gradients can be computed efficiently using PyTorch‚Äôs automatic differentiation, allowing primal and dual updates to be car-ried out without forming dense factorizations. This offers a practical route to solving large-scale constrained LRMP instances within modern optimization pipelines. Overall, modern differentiable programming frameworks such as PyTorch offer a practical com-putational avenue for solving both the unconstrained and constrained forms of LRMP at scale. The dual objectives derived in (LRMP-Dual) and (LR-NNLS-Dual) involve the Laplacian pseudoin-verse ùêø ‚Ä†, whose numerical handling is typically expensive and sensitive to the singular structure of graph Laplacians. PyTorch alleviates these difficulties through optimized tensor operations, au-tomatic differentiation, GPU acceleration, and support for iterative linear algebra routines. These capabilities enable efficient evaluation of dual objectives and gradients, allowing first-order dual updates to be carried out without relying on explicit matrix factorizations. The resulting dual iter-ates can then be mapped back to the primal variables through the corresponding optimality con-ditions. This computational perspective provides a flexible and scalable mechanism for addressing LRMP problems in modern large-scale settings while fully leveraging the structure exposed by their dual formulations. 

> J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:30 Tao, Tong and Tan 

5 Conclusion 

Differentiable programming provides a powerful foundation for large-scale optimization by repre-senting algorithms as differentiable computational graphs and enabling end-to-end optimization. When combined with duality theory and first-order methods, it introduces principled modeling structure, verifiable guarantees, and new pathways for improving convergence. This tutorial pre-sented the theoretical foundations of differentiable programming, analyzed its integration with cone programming, and demonstrated how first-order methods can be embedded within modern automatic-differentiation frameworks to achieve adaptive and accelerated performance on com-plex, high-dimensional problems. Through case studies in linear programming, optimal power flow, Laplacian regularization, and neural network verification, we illustrated how primal‚Äìdual updates and iterative schemes such as ADMM and PDHG can be efficiently implemented in Py-Torch. Taken as a whole, differentiable programming, duality theory, and scalable first-order meth-ods form a coherent toolkit for building adaptive, efficient, and certifiable optimization pipelines, pointing toward a new paradigm for solving the massive-scale problems that characterize modern applications. 

References 

[1] Mart√≠n Abadi and Gordon D Plotkin. 2019. A simple differentiable programming language. Proceedings of the ACM on Programming Languages 4, POPL (2019), 1‚Äì28. [2] Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter. 2019. Differen-tiable convex optimization layers. Advances in Neural Information Processing Systems 32 (2019). [3] Akshay Agrawal, Shane Barratt, Stephen Boyd, Enzo Busseti, and Walaa M Moursi. 2019. Differentiating through a cone program. arXiv preprint arXiv:1904.09043 (2019). [4] Akshay Agrawal, Stephen Boyd, Deepak Narayanan, Fiodar Kazhamiaka, and Matei Zaharia. 2022. Allocation of fungible resources via a fast, scalable price discovery method. Mathematical Programming Computation 14, 3 (2022), 593‚Äì622. [5] Ali AhmadiTeshnizi, Wenzhi Gao, and Madeleine Udell. 2023. Optimus: Optimization modeling using mip solvers and large language models. arXiv preprint arXiv:2310.06116 (2023). [6] Aws Albarghouthi et al. 2021. Introduction to neural network verification. Foundations and Trends¬Æ in Programming Languages 7, 1‚Äì2 (2021), 1‚Äì157. [7] Brandon Amos. 2019. Differentiable optimization-based modeling for machine learning. (2019). [8] Brandon Amos, Ivan Jimenez, Jacob Sacks, Byron Boots, and J Zico Kolter. 2018. Differentiable mpc for end-to-end planning and control. Advances in Neural Information Processing Systems 31 (2018). [9] Brandon Amos and J Zico Kolter. 2017. Optnet: Differentiable optimization as a layer in neural networks. In Inter-national Conference on Machine Learning . PMLR, 136‚Äì145. [10] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shilling-ford, and Nando De Freitas. 2016. Learning to learn by gradient descent by gradient descent. Advances in Neural Information Processing Systems 29 (2016). [11] David L. Applegate, Mateo D√≠az, Oliver Hinder, Haihao Lu, Miles Lubin, Brendan O‚ÄôDonoghue, and Warren Schudy. 2021. Practical Large-Scale Linear Programming using Primal-Dual Hybrid Gradient. In Advances in Neural Informa-tion Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual . 20243‚Äì20257. [12] Jianchao Bai, Linyuan Jia, and Zheng Peng. 2024. A new insight on augmented Lagrangian method with applications in machine learning. Journal of Scientific Computing 99, 2 (2024), 53. [13] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. 2019. Deep equilibrium models. Advances in Neural Information Processing Systems 32 (2019). [14] Friedrich L Bauer. 1974. Computational graphs and rounding error. SIAM J. Numer. Anal. 11, 1 (1974), 87‚Äì96. [15] Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2018. Automatic differentiation in machine learning: A survey. Journal of Machine Learning Research 18, 153 (2018), 1‚Äì43. [16] Amir Beck. 2017. First-order methods in optimization . SIAM. [17] Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. 2021. Machine learning for combinatorial optimization: a methodological tour d‚Äôhorizon. European Journal of Operational Research 290, 2 (2021), 405‚Äì421. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:31 

[18] A. Bentkamp, R. Fern√°ndez Mir, and J. Avigad. 2023. Verified reductions for optimization. In Tools and Algorithms for the Construction and Analysis of Systems , Vol. 13994. Springer, 74‚Äì92. [19] Quentin Bertrand, Quentin Klopfenstein, Mathurin Massias, Mathieu Blondel, Samuel Vaiter, Alexandre Gramfort, and Joseph Salmon. 2022. Implicit differentiation for fast hyperparameter selection in non-smooth convex learning. 

Journal of Machine Learning Research 23, 149 (2022), 1‚Äì43. [20] Dimitri P Bertsekas. 2014. Constrained optimization and Lagrange multiplier methods . Academic press. [21] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. 2017. Julia: A fresh approach to numerical com-puting. SIAM review 59, 1 (2017), 65‚Äì98. [22] Jeff Bezanson, Stefan Karpinski, Viral B Shah, and Alan Edelman. 2012. Julia: A fast dynamic language for technical computing. arXiv preprint arXiv:1209.5145 (2012). [23] Mathieu Blondel and Vincent Roulet. 2024. The elements of differentiable programming. arXiv preprint arXiv:2403.14606 (2024). [24] Mathieu Blondel, Olivier Teboul, Quentin Berthet, and Josip Djolonga. 2020. Fast differentiable sorting and ranking. In International Conference on Machine Learning . PMLR, 950‚Äì959. [25] Nacime Bouziani, David A Ham, and Ado Farsi. 2024. Differentiable programming across the PDE and Machine Learning barrier. arXiv preprint arXiv:2409.06085 (2024). [26] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. 2011. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends¬Æ in Machine learning 

3, 1 (2011), 1‚Äì122. [27] Stephen P Boyd and Lieven Vandenberghe. 2004. Convex optimization . Cambridge university press. [28] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. 2018. JAX: composable transformations of Python+ NumPy programs. (2018). [29] Robin A Brown, Edward Schmerling, Navid Azizan, and Marco Pavone. 2022. A unified view of SDP-based neural network verification through completely positive programming. In International Conference on Artificial Intelligence and Statistics . PMLR, 9334‚Äì9355. [30] Raafi Careem, G Johar, and Ali Khatibi. 2024. Deep neural networks optimization for resource-constrained envi-ronments: Techniques and models. Indonesian Journal of Electrical Engineering and Computer Science 33, 3 (2024), 1843‚Äì1854. [31] Antonin Chambolle and Thomas Pock. 2011. A first-order primal-dual algorithm for convex problems with applica-tions to imaging. Journal of Mathematical Imaging and Vision 40, 1 (2011), 120‚Äì145. [32] Tsung-Hui Chang, Mingyi Hong, Wei-Cheng Liao, and Xiangfeng Wang. 2016. Asynchronous distributed ADMM for large-scale optimization‚ÄîPart I: Algorithm and convergence analysis. IEEE Transactions on Signal Processing 64, 12 (2016), 3118‚Äì3130. [33] Fei Chen, Gene Cheung, and Xue Zhang. 2024. Manifold graph signal restoration using gradient graph Laplacian regularizer. IEEE Transactions on Signal Processing 72 (2024), 744‚Äì761. [34] Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu, Zhangyang Wang, and Wotao Yin. 2022. Learning to optimize: A primer and a benchmark. Journal of Machine Learning Research 23, 189 (2022), 1‚Äì59. [35] Hong-Ming Chiu, Hao Chen, Huan Zhang, and Richard Y Zhang. 2025. SDP-CROWN: Efficient Bound Propaga-tion for Neural Network Verification with Tightness of Semidefinite Programming. arXiv preprint arXiv:2506.06665 

(2025). [36] Woocheol Choi and Jimyeong Kim. 2025. On the convergence analysis of the decentralized projected gradient descent method. SIAM Journal on Optimization 35, 3 (2025), 1673‚Äì1702. [37] Gonzalo E Constante-Flores, Hao Chen, and Can Li. 2025. Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules. arXiv preprint arXiv:2505.13858 (2025). [38] Ron S Dembo, Stanley C Eisenstat, and Trond Steihaug. 1982. Inexact newton methods. SIAM J. Numer. Anal. 19, 2 (1982), 400‚Äì408. [39] Jelena Diakonikolas, Chenghui Li, Swati Padmanabhan, and Chaobing Song. 2022. A fast scale-invariant algorithm for non-negative least squares with non-negative data. Advances in Neural Information Processing Systems 35 (2022), 6264‚Äì6277. [40] Steven Diamond and Stephen Boyd. 2016. CVXPY: A Python-embedded modeling language for convex optimization. 

Journal of Machine Learning Research 17, 83 (2016), 1‚Äì5. [41] Jean C Digitale, Jeffrey N Martin, and Medellena Maria Glymour. 2022. Tutorial on directed acyclic graphs. Journal of Clinical Epidemiology 142 (2022), 264‚Äì267. [42] Hermann W Dommel and William F Tinney. 2007. Optimal power flow solutions. IEEE Transactions on Power Apparatus and Systems 10 (2007), 1866‚Äì1876. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:32 Tao, Tong and Tan 

[43] Simon S Du and Wei Hu. 2019. Linear convergence of the primal-dual gradient method for convex-concave saddle point problems without strong convexity. In The 22nd International Conference on Artificial Intelligence and Statistics .PMLR, 196‚Äì205. [44] Mirjam D√ºr and Franz Rendl. 2021. Conic optimization: A survey with special focus on copositive optimization and binary quadratic problems. (2021). [45] Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan O‚ÄôDonoghue, Jonathan Ue-sato, and Pushmeet Kohli. 2018. Training verified learners with learned verifiers. arXiv preprint arXiv:1805.10265 

(2018). [46] Pavel Dvurechensky, Shimrit Shtern, and Mathias Staudigl. 2021. First-order methods for convex optimization. EURO Journal on Computational Optimization 9 (2021), 100015. [47] Zeinab Ebrahimi, Gustavo Batista, and Mohammad Deghat. 2024. AA-DLADMM: An accelerated ADMM-based framework for training deep neural networks. arXiv preprint arXiv:2401.03619 (2024). [48] Tobias Ekholm and Yankƒ± Lekili. 2023. Duality between Lagrangian and Legendrian invariants. Geometry & Topology 

27, 6 (2023), 2049‚Äì2179. [49] Ernie Esser, Yifei Lou, and Jack Xin. 2013. A method for finding structured sparse solutions to nonnegative least squares problems with applications. SIAM Journal on Imaging Sciences 6, 4 (2013), 2010‚Äì2046. [50] Ernie Esser, Xiaoqun Zhang, and Tony F Chan. 2010. A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science. SIAM Journal on Imaging Sciences 3, 4 (2010), 1015‚Äì1046. [51] Gabriele Farina. 2024. Lecture 6: Conic Optimization. https://www.mit.edu/~gfarina/2024/67220s24_L06_conic_optimization/L06.pd [52] Werner Fenchel. 2013. On conjugate convex functions. In Traces and Emergence of Nonlinear Programming . Springer, 125‚Äì129. [53] Ferdinando Fioretto, Pascal Van Hentenryck, Terrence WK Mak, Cuong Tran, Federico Baldo, and Michele Lombardi. 2020. Lagrangian duality for constrained deep learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases . Springer, 118‚Äì135. [54] Marguerite Frank, Philip Wolfe, et al. 1956. An algorithm for quadratic programming. Naval Research Logistics Quarterly 3, 1-2 (1956), 95‚Äì110. [55] Yuan Gao and Christian Kroer. 2020. First-order methods for large-scale market equilibrium computation. Advances in Neural Information Processing Systems 33 (2020), 21738‚Äì21750. [56] Susan Garner Garille and Saul I Gass. 2001. Stigler‚Äôs diet problem revisited. Operations Research 49, 1 (2001), 1‚Äì13. [57] Google. 2023. OR-Tools: Google‚Äôs Operations Research Tools . https://github.com/google/or-tools.git [58] Stephen Gould. 2025. Lecture notes on differentiable optimisation in deep learning. [59] Stephen Gould, Richard Hartley, and Dylan Campbell. 2019. Deep declarative networks: A new hope. arXiv e-prints 

(2019), arXiv‚Äì1909. [60] Stephen Gould, Richard Hartley, and Dylan Campbell. 2021. Deep declarative networks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 8 (2021), 3988‚Äì4004. [61] Michael Grant, Stephen Boyd, and Yinyu Ye. 2008. CVX: Matlab software for disciplined convex programming. [62] Harshit Gupta, Kyong Hwan Jin, Ha Q Nguyen, Michael T McCann, and Michael Unser. 2018. CNN-based projected gradient descent for consistent CT image reconstruction. IEEE Transactions on Medical Imaging 37, 6 (2018), 1440‚Äì 1453. [63] Mohamad Azrin Syafiq Halim and Yeak Su Hoe. 2024. Introduction to automatic differentiation and neural differen-tiation equation. Proceedings of the Scientific Mathematics Journal 24 (2024), 81‚Äì89. [64] Nguyen TV Hang, Boris S Mordukhovich, and M Ebrahim Sarabi. 2020. Second-order variational analysis in second-order cone programming. Mathematical Programming 180, 1 (2020), 75‚Äì116. [65] Tyler Hanks, Matthew Klawonn, Evan Patterson, Matthew Hale, and James Fairbanks. 2024. A compositional frame-work for first-order optimization. arXiv preprint arXiv:2403.05711 (2024). [66] Moritz Hardt, Ben Recht, and Yoram Singer. 2016. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning . PMLR, 1225‚Äì1234. [67] Moritz Hardt and Max Simchowitz. 2018. EE 227C: Convex optimization and approximation. University of California, Berkeley. https://ee227c.github.io/ [68] Ang He, Heng Pan, Yueyue Dai, Xueming Si, Chau Yuen, and Yan Zhang. 2024. ADMM for mobile edge intelligence: A survey. IEEE Communications Surveys & Tutorials (2024). [69] Robin J Hogan. 2014. Fast reverse-mode automatic differentiation using expression templates in C++. ACM Trans. Math. Software 40, 4 (2014), 1‚Äì16. [70] Yanyan Hu, Yuchen Jin, Xuqing Wu, and Jiefu Chen. 2021. A theory-guided deep neural network for time domain electromagnetic simulation and inversion using a differentiable programming platform. IEEE Transactions on Anten-nas and Propagation 70, 1 (2021), 767‚Äì772. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:33 

[71] IBM. 2023. What is CPLEX? https://www.ibm.com/docs/en/icos/22.1.1?topic=mc-what-is-cplex [72] Mike Innes, Alan Edelman, Keno Fischer, Chris Rackauckas, Elliot Saba, Viral B Shah, and Will Tebbutt. 2019. A differ-entiable programming system to bridge machine learning and scientific computing. arXiv preprint arXiv:1907.07587 

(2019). [73] E Joannopoulos, F Dubeau, JP Dussault, and C Pomar. 2015. Diet problems. Handbook of Operations Research in Agriculture and the Agri-Food Industry (2015), 397‚Äì417. [74] Richard Johnson. 2025. PyTorch foundations and applications: Definitive reference for developers and engineers . HiTeX Press. [75] Howard Karloff. 2008. Linear programming . Springer Science & Business Media. [76] Calder Katyal. 2024. Differentiable convex optimization layers in neural architectures: Foundations and perspectives. 

arXiv preprint arXiv:2412.20679 (2024). [77] Nikhil Ketkar and Jojo Moolayil. 2021. Introduction to pytorch. In Deep learning with python: learn best practices of deep learning models with PyTorch . Springer, 27‚Äì91. [78] Minsoo Kim and Hongseok Kim. 2024. Unsupervised deep lagrange dual with equation embedding for ac optimal power flow. IEEE Transactions on Power Systems 40, 1 (2024), 1078‚Äì1090. [79] Virginia Klema and Alan Laub. 1980. The singular value decomposition: Its computation and some applications. 

IEEE Trans. Automat. Control 25, 2 (1980), 164‚Äì176. [80] Nikos Komodakis and Jean-Christophe Pesquet. 2015. Playing with duality: An overview of recent primal? dual approaches for solving large-scale optimization problems. IEEE Signal Processing Magazine 32, 6 (2015), 31‚Äì54. [81] James Kotary and Ferdinando Fioretto. 2024. Learning constrained optimization with deep augmented lagrangian methods. arXiv preprint arXiv:2403.03454 (2024). [82] James Kotary, Ferdinando Fioretto, and Pascal Van Hentenryck. 2022. Fast approximations for job shop scheduling: A lagrangian dual deep learning method. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 36. 7239‚Äì7246. [83] James Kotary, Ferdinando Fioretto, Pascal Van Hentenryck, and Bryan Wilder. 2021. End-to-end constrained opti-mization learning: A survey. arXiv preprint arXiv:2103.16378 (2021). [84] Dimitris Kouzoupis, Gianluca Frison, Andrea Zanelli, and Moritz Diehl. 2018. Recent advances in quadratic program-ming algorithms for nonlinear model predictive control. Vietnam Journal of Mathematics 46, 4 (2018), 863‚Äì882. [85] Dmitry Kovalev, Alexander Gasnikov, and Peter Richt√°rik. 2022. Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling. Advances in Neural Information Processing Sys-tems 35 (2022), 21725‚Äì21737. [86] Zeqiang Lai, Kaixuan Wei, Ying Fu, Philipp H√§rtel, and Felix Heide. 2023. ‚àá-Prox: Differentiable proximal algorithm modeling for large-scale optimization. ACM Transactions on Graphics 42, 4 (2023), 1‚Äì19. [87] Jean Bernard Lasserre. 2009. Moments, positive polynomials and their applications . Vol. 1. World Scientific. [88] LeanOpt. 2024. LeanOpt: Optimization Learning Platform. https://leanopt.github.io/ [89] Huan Li, Cong Fang, and Zhouchen Lin. 2020. Accelerated first-order optimization algorithms for machine learning. 

Proc. IEEE 108, 11 (2020), 2067‚Äì2082. [90] Tzu-Mao Li, Micha√´l Gharbi, Andrew Adams, Fr√©do Durand, and Jonathan Ragan-Kelley. 2018. Differentiable pro-gramming for image processing and deep learning in Halide. ACM Transactions on Graphics 37, 4 (2018), 1‚Äì13. [91] Heng Liang and Changhong Zhao. 2023. DeepOPF-U: A unified deep neural network to solve AC optimal power flow in multiple networks. arXiv preprint arXiv:2309.12849 (2023). [92] Hai-Jun Liao, Jin-Guo Liu, Lei Wang, and Tao Xiang. 2019. Differentiable programming tensor networks. Physical Review X 9, 3 (2019), 031041. [93] Jun Liu and Jieping Ye. 2009. Efficient Euclidean projections in linear time. In Proceedings of the 26th Annual Inter-national Conference on Machine Learning . 657‚Äì664. [94] Miguel Sousa Lobo, Lieven Vandenberghe, Stephen Boyd, and Herv√© Lebret. 1998. Applications of second-order cone programming. Linear Algebra and Its Applications 284, 1-3 (1998), 193‚Äì228. [95] Haihao Lu, Zedong Peng, and Jinwen Yang. 2024. Mpax: Mathematical Programming in jax. arXiv preprint arXiv:2412.09734 (2024). [96] Haihao Lu and Jinwen Yang. 2024. On the geometry and refined rate of primal‚Äìdual hybrid gradient for linear programming. Mathematical Programming (2024), 1‚Äì39. [97] Zhi-Quan Luo, Wing-Kin Ma, Anthony Man-Cho So, Yinyu Ye, and Shuzhong Zhang. 2010. Semidefinite relaxation of quadratic optimization problems. IEEE Signal Processing Magazine 27, 3 (2010), 20‚Äì34. [98] Jayanta Mandi, James Kotary, Senne Berden, Maxime Mulamba, Victor Bucarey, Tias Guns, and Ferdinando Fioretto. 2024. Decision-focused learning: Foundations, state of the art, benchmark and future opportunities. Journal of Artificial Intelligence Research 80 (2024), 1623‚Äì1701. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:34 Tao, Tong and Tan 

[99] Charles C Margossian. 2019. A review of automatic differentiation and its efficient implementation. Wiley Interdis-ciplinary Reviews: Data Mining and Knowledge Discovery 9, 4 (2019), e1305. [100] Jared Miller, Yang Zheng, Mario Sznaier, and Antonis Papachristodoulou. 2022. Decomposed structured subsets for semidefinite and sum-of-squares optimization. Automatica 137 (2022), 110125. [101] William S Moses, Valentin Churavy, Ludger Paehler, Jan H√ºckelheim, Sri Hari Krishna Narayanan, Michel Schanen, and Johannes Doerfert. 2021. Reverse-mode automatic differentiation and optimization of GPU kernels via Enzyme. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis .1‚Äì16. [102] Yurii Nesterov. 2009. Primal-dual subgradient methods for convex problems. Mathematical Programming 120, 1 (2009), 221‚Äì259. [103] Yu Nesterov. 2011. Towards non-symmetric conic optimization. Optimization Methods and Software 27, 4‚Äì5 (2011), 893‚Äì917. [104] Yu Nesterov. 2013. Gradient methods for minimizing composite functions. Mathematical Programming 140, 1 (2013), 125‚Äì161. [105] Deniz Oktay, Nick McGreivy, Joshua Aduol, Alex Beatson, and Ryan P Adams. 2020. Randomized automatic differ-entiation. arXiv preprint arXiv:2007.10412 (2020). [106] B. O‚ÄôDonoghue, E. Chu, N. Parikh, and S. Boyd. 2016. Conic optimization via operator splitting and homogeneous self-dual embedding. Journal of Optimization Theory and Applications 169 (2016), 1042‚Äì1068. [107] Aleksandra Pachalieva, Daniel O‚ÄôMalley, Dylan Robert Harp, and Hari Viswanathan. 2022. Physics-informed ma-chine learning with differentiable programming for heterogeneous underground reservoir pressure management. 

Scientific Reports 12, 1 (2022), 18734. [108] Soumen Pal, Manojit Bhattacharya, Snehasish Dash, Sang-Soo Lee, and Chiranjib Chakraborty. 2024. A next-generation dynamic programming language Julia: Its features and applications in biological science. Journal of Advanced Research 64 (2024), 143‚Äì154. [109] Jianming Pan, Zeqi Ye, Xiao Yang, Xu Yang, Weiqing Liu, Lewen Wang, and Jiang Bian. 2024. BPQP: A differentiable convex optimization framework for efficient end-to-end learning. Advances in Neural Information Processing Systems 

37 (2024), 77468‚Äì77493. [110] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and Privacy, SP 2016, San Jose, CA, USA, May 22-26, 2016 . IEEE Computer Society, 582‚Äì597. [111] Neal Parikh, Stephen Boyd, et al. 2014. Proximal algorithms. Foundations and Trends¬Æ in Optimization 1, 3 (2014), 127‚Äì239. [112] Seonho Park and Pascal Van Hentenryck. 2023. Self-supervised primal-dual learning for constrained optimization. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Washington, DC, USA, February 7-14, 2023 .4052‚Äì4060. [113] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch. (2017). [114] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems 32 (2019). [115] Fabian Pedregosa. 2016. Hyperparameter optimization with approximate gradient. In International Conference on Machine Learning . PMLR, 737‚Äì746. [116] Marin Vlastelica Poganƒçiƒá, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. 2019. Differentiation of blackbox combinatorial solvers. In International Conference on Learning Representations .[117] Christopher Rackauckas and Qing Nie. 2017. Differentialequations. jl‚Äìa performant and feature-rich ecosystem for solving differential equations in julia. Journal of Open Research Software 5, 1 (2017), 15‚Äì15. [118] Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. 2018. Semidefinite relaxations for certifying robustness to adversarial examples. Advances in Neural Information Processing Systems 31 (2018). [119] Ben Recht. 2024. The art of deconstraining: A guide to convex optimization in PyTorch. https://www.argmin.net/p/the-art-of-deconstraining. [120] Ben Recht. 2024. Cone programming. https://www.argmin.net/p/cone-programming. [121] Sebastian Ruder. 2016. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747 

(2016). [122] Facundo Sapienza, Jordi Bolibar, Frank Sch√§fer, Brian Groenke, Avik Pal, Victor Boussange, Patrick Heimbach, Giles Hooker, Fernando P√©rez, Per-Olof Persson, et al. 2024. Differentiable programming for differential equations: A review. arXiv preprint arXiv:2406.09699 (2024). [123] Grigory Sapunov. 2024. Deep learning with JAX . Simon and Schuster. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. Learning to Optimize by Differentiable Programming 111:35 

[124] Frank Sch√§fer, Mohamed Tarek, Lyndon White, and Chris Rackauckas. 2021. AbstractDifferentiation. jl: Backend-agnostic differentiable programming in Julia. arXiv preprint arXiv:2109.12449 (2021). [125] Maximilian Schaller, Goran Banjac, Steven Diamond, Akshay Agrawal, Bartolomeo Stellato, and Stephen Boyd. 2022. Embedded code generation with CVXPY. IEEE Control Systems Letters 6 (2022), 2653‚Äì2658. [126] Maximilian Schaller and Stephen Boyd. 2025. Code generation for solving and differentiating through convex opti-mization problems. arXiv preprint arXiv:2504.14099 (2025). [127] Joshua Schneider, David Basin, Frederik Brix, Srƒëan Krstiƒá, and Dmitriy Traytel. 2021. Scalable online first-order monitoring. International Journal on Software Tools for Technology Transfer 23, 2 (2021), 185‚Äì208. [128] Samuel Schoenholz and Ekin Dogus Cubuk. 2020. Jax md: A framework for differentiable physics. Advances in Neural Information Processing Systems 33 (2020), 11428‚Äì11441. [129] C H SCOTT and TR Jefferson. 1989. Conjugate duals for power functions. International Journal of Systems Science 

20, 11 (1989), 2153‚Äì2160. [130] James K Strayer. 2012. Linear programming and its applications . Springer Science & Business Media. [131] Chee Wei Tan, Desmond WH Cai, and Xin Lou. 2014. Resistive network optimal power flow: Uniqueness and algorithms. IEEE Transactions on Power Systems 30, 1 (2014), 263‚Äì273. [132] Chee Wei Tan and Siya Chen. 2026. DIFFRACT: Neuralized utility maximization for wireless networks by differen-tiable programming. In IEEE INFOCOM 2026-IEEE Conference on Computer Communications . IEEE, 1‚Äì10. [133] Bo Tang and Elias B Khalil. 2024. PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming. Mathematical Programming Computation 16, 3 (2024), 297‚Äì335. [134] Ke Tang and Xin Yao. 2024. Learn to optimize-A brief overview. National Science Review (2024), nwae132. [135] Mathieu Tanneau and Pascal Van Hentenryck. 2024. Dual lagrangian learning for conic optimization. CoRR 

abs/2402.03086 (2024). [136] Andrew R. Teel, Jorge I. Poveda, and Justin H. Le. 2019. First-order optimization algorithms with resets and hamil-tonian flows. In 58th IEEE Conference on Decision and Control, CDC 2019, Nice, France, December 11-13, 2019 . IEEE, 5838‚Äì5843. [137] Cuong Tran, Ferdinando Fioretto, and Pascal Van Hentenryck. 2021. Differentially private and fair deep learning: A lagrangian dual approach. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 9932‚Äì9939. [138] Jonathan Tuck, David Hallac, and Stephen Boyd. 2019. Distributed majorization-minimization for Laplacian regu-larized problems. IEEE/CAA Journal of Automatica Sinica 6, 1 (2019), 45‚Äì52. [139] R G√∂khan T√ºreci, Hamdi Daƒüƒ±stanlƒ±, and ƒ∞lkay T√ºrk √áakƒ±r. 2025. Julia Programming for Physics Applications .Springer. [140] Stefan Van Der Walt, S Chris Colbert, and Gael Varoquaux. 2011. The NumPy array: A structure for efficient numer-ical computation. Computing in Science & Engineering 13, 2 (2011), 22‚Äì30. [141] Pascal Van Hentenryck and Kevin Dalmeijer. 2024. AI4OPT: AI institute for advances in optimization. AI Magazine 

45, 1 (2024), 42‚Äì47. [142] Lieven Vandenberghe and Stephen Boyd. 1996. Semidefinite programming. SIAM Rev. 38, 1 (1996), 49‚Äì95. [143] Tim Vieira. 2017. Backprop is not just the chain rule. Blog post, Graduate Descent. https://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/ [144] Chaoming Wang, Sichao He, Shouwei Luo, Yuxiang Huan, and Si Wu. 2025. Integrating physical units into high-performance AI-driven scientific computing. Nature Communications 16, 1 (2025), 3609. [145] Fei Wang, James Decker, Xilun Wu, Gregory Essertel, and Tiark Rompf. 2018. Backpropagation with callbacks: Foundations for efficient and expressive differentiable programming. Advances in Neural Information Processing Systems 31 (2018). [146] Fei Wang, Daniel Zheng, James Decker, Xilun Wu, Gr√©gory M Essertel, and Tiark Rompf. 2019. Demystifying differen-tiable programming: Shift/reset the penultimate backpropagator. Proceedings of the ACM on Programming Languages 

3, ICFP (2019), 1‚Äì31. [147] Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. 2021. Beta-crown: Efficient bound propagation with per-neuron split constraints for neural network robustness verification. Advances in Neural Information Processing Systems 34 (2021), 29909‚Äì29921. [148] Yu Wang, Wotao Yin, and Jinshan Zeng. 2019. Global convergence of ADMM in nonconvex nonsmooth optimization. 

Journal of Scientific Computing 78, 1 (2019), 29‚Äì63. [149] Eric Wong and Zico Kolter. 2018. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning . PMLR, 5286‚Äì5295. [150] Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, and Mung Chiang. 2016. Quantifying political leaning from tweets, retweets, and retweeters. IEEE Transactions on Knowledge and Data Engineering 28, 8 (2016), 2158‚Äì2172. [151] Stephen J Wright and Benjamin Recht. 2022. Optimization for data analysis . Cambridge University Press. J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:36 Tao, Tong and Tan 

[152] Xingyu Xie, Jianlong Wu, Guangcan Liu, Zhisheng Zhong, and Zhouchen Lin. 2019. Differentiable linearized ADMM. In International Conference on Machine Learning . PMLR, 6902‚Äì6911. [153] Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. 2020. Fast and complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. arXiv preprint arXiv:2011.13824 (2020). [154] Jihun Yun, Aur√©lie C Lozano, and Eunho Yang. 2021. Adaptive proximal gradient methods for structured neural networks. Advances in Neural Information Processing Systems 34 (2021), 24365‚Äì24378. [155] Alp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher. 2021. Scalable semidefinite pro-gramming. SIAM Journal on Mathematics of Data Science 3, 1 (2021), 171‚Äì200. [156] Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning, and Cho-Jui Hsieh. 2019. Towards stable and efficient training of verifiably robust neural networks. arXiv preprint arXiv:1906.06316 

(2019). [157] Yang Zheng, Giovanni Fantuzzi, and Antonis Papachristodoulou. 2021. Chordal and factor-width decompositions for scalable semidefinite and polynomial optimization. Annual Reviews in Control 52 (2021), 243‚Äì279. [158] Mingqiang Zhu and Tony Chan. 2008. An efficient primal-dual hybrid gradient algorithm for total variation image restoration. Ucla Cam Report 34, 2 (2008). 

Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009 

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.