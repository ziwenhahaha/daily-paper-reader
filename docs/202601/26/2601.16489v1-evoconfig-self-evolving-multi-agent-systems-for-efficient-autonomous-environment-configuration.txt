Title: EvoConfig: Self-Evolving Multi-Agent Systems for Efficient Autonomous Environment Configuration

URL Source: https://arxiv.org/pdf/2601.16489v1

Published Time: Mon, 26 Jan 2026 01:23:14 GMT

Number of Pages: 13

Markdown Content:
## EvoConfig: Self-Evolving Multi-Agent Systems for Efficient Autonomous Environment Configuration 

Xinshuai Guo 1*, Jiayi Kuang 2âˆ—, Linyue Pan 1, Yinghui Li 3â€ , Yangning Li 1

Hai-Tao Zheng 1â€ , Ying Shen 2â€ , Di Yin 3, Xing Sun 31 Tsinghua University, 2 Sun-Yat Sen University, 3 Tencent Youtu Lab 

gxs25@mails.tsinghua.edu.cn , liyinghuihhh@gmail.com 

Abstract 

A reliable executable environment is the foun-dation for ensuring that large language mod-els solve software engineering tasks. Due to the complex and tedious construction process, large-scale configuration is relatively ineffi-cient. However, most methods always over-look fine-grained analysis of the actions per-formed by the agent, making it difficult to han-dle complex errors and resulting in configura-tion failures. To address this bottleneck, we propose EvoConfig, an efficient environment configuration framework that optimizes multi-agent collaboration to build correct runtime en-vironments. EvoConfig features an expert di-agnosis module for fine-grained post-execution analysis, and a self-evolving mechanism that lets expert agents self-feedback and dynami-cally adjust error-fixing priorities in real time. Empirically, EvoConfig matches the previous state-of-the-art Repo2Run on Repo2Runâ€™s 420 repositories, while delivering clear gains on harder cases: on the more challenging En-vbench, EvoConfig achieves a 78.1% suc-cess rate, outperforming Repo2Run by 7.1%. Beyond end-to-end success, EvoConfig also demonstrates stronger debugging competence, achieving higher accuracy in error identifica-tion and producing more effective repair rec-ommendations than existing methods 1.

1 Introduction 

Large language models (LLMs) have made rapid progress in handling complex software engineer-ing (SWE) tasks (He et al., 2025; Wang et al., 2025c; Xia et al., 2025; Wang et al., 2024a; Kuang et al., 2025a; Lu et al., 2025), leading to the emer-gence of a wide range of code agents such as SWE-Agent (Yang et al., 2024), OpenHands (Wang 

* indicates equal contribution. 

â€  Corresponding authors. 

1We will open-source the code after the paper is published. case 1 

Github Repo    

> for current Python
> version ...
> for current Python
> version ...
> for current Python
> version ...

case 1                     

> No compatib
> numpy<2.0 fou
> for current Pyt
> version ...
> No compatib
> numpy<2.0 fou
> for current Pyt
> version ...
> CASE 1: CASE 1: CASE 1:
> Root Cause:
> No compatible
> numpy<2.0 found ...
> Fix Command:
> pip install numpy==2.0
> Root Cause:
> No compatible
> numpy<2.0 found ...
> Fix Command:
> pip install numpy==2.0
> CASE 1:
> Root Cause:
> No compatible
> numpy<2.0 found ...
> Fix Command:
> pip install numpy==2.0
> CASE 2: CASE 2: CASE 2:
> Root Cause:
> The error might be due
> to a temporary issue ...
> Fix Command:
> ls /repo
> Root Cause:
> The error might be due
> to a temporary issue ...
> Fix Command:
> ls /repo
> CASE 2:
> Root Cause:
> The error might be due
> to a temporary issue ...
> Fix Command:
> ls /repo
> CASE 3: CASE 3: CASE 3:
> Root Cause:
> No module named
> 'django'
> Fix Command:
> pip install django
> Root Cause:
> No module named
> 'django'
> Fix Command:
> pip install django
> CASE 3:
> Root Cause:
> No module named
> 'django'
> Fix Command:
> pip install django
> pip install -e.
> ...
> No matching
> pydantic>=2.0.1 for
> current Python
> version ...
> No matching
> pydantic>=2.0.1 for
> current Python
> version ...
> pip show pydantic
> pip install
> pydantic==1.9.0 ...
> pip show pydantic
> pip install
> pydantic==1.9.0 ...
> Expert Diagnosis

Expert Diagnosis Experience:   

> The error might be due
> to a temporary issue ...
> Feedback

Self -Evolve Process 

Figure 1: Self-Evolving Diagnostic Process. 

et al., 2024b), MetaGPT (Hong et al., 2023), Copi-lot (GitHub, 2021) and Cursor (Anysphere, 2023). As research increasingly shifts toward repository-level software engineering tasks, scalable execution and reliable validation become essential (Wang et al., 2025a; Xie et al., 2024; Liu et al., 2024; Rutherford et al., 2024; Krnjaic et al., 2024). Code agents are no longer required to only generate or modify code, but must also complete end-to-end workflows, including environment construction, testing, and validation, within real code runtime environments. However, a long-overlooked chal-lenge is now becoming increasingly evident: au-tomatically configuring executable environments. Environment setup still depends heavily on human expertise and can be difficult even for experienced developers. Yet a stable, runnable environment is a prerequisite for tackling complex software engi-neering tasks. Therefore, enabling agents to reli-ably configure environments is critical to advanc-ing code agents .In real-world repositories, an agent must au-1

> arXiv:2601.16489v1 [cs.SE] 23 Jan 2026

tonomously complete dependency installation, ver-sion resolution, and test execution under challeng-ing conditions, including unknown dependencies, incomplete documentation, and the coexistence of multiple build tools. Most existing methods for-mulate environment configuration as a sequential decision-making problem (Bouzenia and Pradel, 2025a; Vergopoulos et al., 2025; Hu et al., 2025; Zhang et al., 2025a): the agent observes the cur-rent execution outcome and heuristically proposes the next action. However, these approaches often fail to explicitly address process-level errors that occur during configuration (e.g., cascading de-pendency conflicts, toolchain mismatches, partial installations). Such errors can accumulate across steps and ultimately cause environment construc-tion to fail. Some prior work (Milliken et al., 2025a; Vergopoulos et al., 2025) introduces repair strate-gies that are detached from the original configura-tion context. These methods typically rely on pre-defined, experience-based rules to produce static repair actions, but they lack fine-grained diagno-sis of the specific failure causes in the ongoing configuration process . Consequently, agents are more prone to hallucinated fixes or repetitive trial-and-error behaviors, and may even fall into infinite loops when confronted with complex failures. In addition, the single-agent workflow exacerbates the problem: error-related information and noisy execution traces accumulate over time, which can mislead subsequent decisions and further reduce both success rate and efficiency. To address these challenges, we propose Evo-Config, an efficient environment configuration framework based on self-evolving multi-agent col-laboration. Our core objective is to improve envi-ronment configuration success rates while simul-taneously enhancing process-level error correc-tion capabilities during the configuration pro-cess . Specifically, a main agent is responsible for environment configuration, while expert agents act as diagnostic specialists that perform fine-grained analysis of execution results and autonomously de-termine whether repairs are required, ultimately providing structured and actionable guidance to the main agent. More importantly, we introduce an online self-evolving mechanism that enables expert agents to continuously learn from error correction cases and dynamically adjust their analytical fo-cus and structured suggestions, thereby improving the agentâ€™s ability to resolve complex environment configuration failures. Notably, this self-evolving mechanism does not rely on external memory mod-ules, avoiding additional reasoning overhead and token consumption. We evaluate EvoConfig on multiple real-world open-source repositories, and the results demonstrate that our approach not only improves environment configuration success rates but also significantly enhances process-level error correction during the configuration process. In summary, our main contributions are summa-rized as follows: â€¢ We are the first to propose the multi-agent collaborative framework EvoConfig for auto-mated environment configuration, improving configuration success rates through optimized agent workflows. â€¢ We propose an expert diagnostic module and introduce a self-evolving mechanism to adap-tively enhance the process-level error correc-tion capability of agents in the environment configuration process. â€¢ We conduct extensive evaluations on mul-tiple open-source benchmarks against ad-vanced agent frameworks, demonstrating that EvoConfig achieves state-of-the-art perfor-mance in both environment configuration and process-level error correction. 

2 Formulation 

2.1 Task Definition 

Given a real-world open-source GitHub repository 

R at a specified version, the system is provided with a clean initial execution environment E0. The ultimate goal is to automatically construct a target execution environment E through a sequence of interactive commands, such that unit tests can be successfully executed in the resulting environment. 

2.2 Iterative Configuration Process 

In this work, we model environment configuration as an interactive decision-making process. Specif-ically, at interaction round t, the agent is in the current environment state Et and selects a set of commands from the action space A for execution: 

at = {c1 

> t

, c 2 

> t

, . . . , c kt 

> t

}, at âŠ† A , (1) where each cit denotes an atomic executable com-mand, and kt is the number of commands issued at round t.2After executing the command set at, the system performs a state transition based on the current environment state and the execution outcomes: 

Et+1 = Î´(Et, a t). (2) This process is repeated for at most tmax inter-action rounds, until the test cases are successfully executed or the number of interactions exceeds a predefined maximum threshold. In this work, we place particular emphasis on whether the com-mands generated at each round result in execution errors. Accordingly, the overall optimization ob-jective is to improve the environment configura-tion success rate under a limited interaction budget, while simultaneously enhancing the agentâ€™s capa-bility for process-level error correction. 

3 Method 

This section introduces EvoConfig, a self-evolving multi-agent framework for efficient environment configuration. Given a code repository, EvoCon-fig performs multiple rounds of interaction and decision-making while continuously repairing envi-ronment configuration issues, ultimately generating an executable Dockerfile to build a runnable envi-ronment. EvoConfig consists of three main com-ponents: an environment information extraction module, a main environment configuration module, and an self-evolving expert diagnosis module. 

3.1 Environment Info Extraction Module 

We introduce a lightweight environment informa-tion extraction module that provides the main agent with a small set of high-impact prior signals before interactive configuration begins. The module fo-cuses on extracting stable structural cues that are directly relevant to environment configuration. Formally, given a repository R, the module pro-duces a prior summary: 

P (R) = {M, I, T }, (3) where M , I, and T denote the dependency man-agement strategy, project importability, and test structure, respectively. 

Dependency Management Strategy. The de-pendency management strategy M is inferred from configuration files such as poetry.lock ,

pyproject.toml , and requirements*.txt , guid-ing early installation decisions. 

Project Importability. Project importability I

captures whether the project needs to be installed for tests to run, based on installation metadata, src/ 

layouts, and package structure. 

Test Structure Hypothesis. The test structure hypothesis T describes the presence and location of tests, the inferred test framework, and whether tests import project modules. The prior summary P (R) is injected into the ini-tial prompt words of the main agent to guide the generation of the initial configuration strategy of the main agent with almost no increase in compu-tational cost. 

3.2 Main Environment Configuration Module 

After the environment prior information is ex-tracted, the system enters the core environment configuration stage. Unlike previous approaches, the main agent responsible for environment con-figuration focuses solely on action execution and sequence, without bearing the burden of long-term memory and semantic analysis of execution results. Specifically, at interaction step t, the main agent performs ReAct (Yao et al., 2022) framework rea-soning based on a limited context and generates an action output, which is parsed into a sequence of atomic commands and executed sequentially in the runtime environment. Each command returns standard output and an exit code as execution feed-back for the current step. During this process, the main agent concentrates only on action generation, scheduling, and execution order, and does not di-rectly interpret the semantics of execution results. Instead, the execution context is delegated to the expert diagnosis module for analysis. This design allows the main agent to advance execution in a streaming manner, avoiding the accumulation of large volumes of raw output across multiple interac-tion rounds and receiving only highly summarized analytical feedback. As a result, it effectively miti-gates a key issue in traditional interactive systems, where incorporating large amounts of low-value output directly into the main reasoning context leads to memory inflation and interferes with sub-sequent decision-making. In addition, to maintain reasoning quality while reducing overall overhead, the main agent adopts a strict context management strategy. Combined with the systemâ€™s rollback mechanism, it preserves key command sequences from successful execution rounds as well as structured diagnostic summaries 3Github Repository   

> bash
> commands
> regular
> express path search
> Config Files Code Files
> Repo structure
> Enviroment
> pre -kowledge
> Main Agent Init prompt
> Extraction Stage

Github Repository      

> bash
> commands
> regular
> express path search
> Config Files Code Files
> Repo structure
> Enviroment
> pre -kowledge
> Main Agent Init prompt
> Extraction Stage
> Invoke the actions
> with init prompt in
> the environment.
> Main Agent Toolbox
> You are an expert skilled in
> environment configuration.

... 

> Initial Prompt
> History Cmds: History Cmds: History Cmds: THOUGHT: THOUGHT:

... 

> THOUGHT:

...      

> ACTION: ACTION:
> download
> runtest runtest
> ACTION:
> download
> runtest
> Observation:
> Synthesizer
> do ckerfile.
> Executable Dockerfile
> tests/test_1.py
> Executable Instance
> tests/test_2.py
> tests/test_3.py
> tests/test_2.py
> tests/test_3.py
> Successfully
> run the tests.
> History Memory:
> Expert Agent
> You are an expert debugger for Python environment
> configuration. You will decide whether the
> command succeeded ,failed , or risky .
> Hsitory Memory

... 

> ModuleNotFoundError: No
> module named 'django'
> Feedback

...   

> Execute Output Return Node History Cmds

...    

> 1. cat setup.py
> 2. pip install -e
> Decide:

... ... ... ... THOUGHT 

> failed
> success
> risky
> Diagnosis Result:
> root causes
> fix command

...    

> ðŸ›  Generated Tool
> script test
> generic error
> Execute Status:
> ls /repo
> cat setup.py cat setup.py

...  ...    

> Possible Root Cause
> django mo dule is not
> installed
> Possible Root Cause
> django mo dule is not
> installed
> Possible Root Cause
> django mo dule is not
> installed
> Fix Idea
> pip install diango
> Fix Idea
> pip install diango
> Fix Idea
> pip install diango
> Diagnostic Evidence
> dian go Package(s) not
> found.
> Diagnostic Evidence
> dian go Package(s) not
> found.
> Diagnostic Evidence
> dian go Package(s) not
> found.
> Initialize a Docker
> container with base
> image Python:3.10.
> Multiple turns of
> Main &Expert
> agent interaction

1. Repository Env -info Extraction Module  2.  Main Env Configuration Module  3.  Self -Evolv e Expert  Diagnosis  Module 

> Inspect folder
> structure to choose
> install strategy
> Inspect folder
> structure to choose
> install strategy

Figure 2: Workflow of EvoConfig. A main configuration agent performs interactive environment setup, while 

self-evolving expert diagnostic agent analyzes execution feedback and provide adaptive guidance. The validated command sequence is consolidated into a runnable Dockerfile. 

from the diagnosis module as experience, thereby improving the efficiency of action generation and scheduling during environment configuration. 

3.3 Self-Evolving Expert Diagnosis Module 

During environment configuration, accurate error diagnosis and repair are critical to ensuring success-ful system deployment. To enhance process-level error correction capability, we introduce an expert diagnosis module with a self-evolving mechanism, which explicitly decouples the standard output of execution actions from the primary configuration workflow and assigns it to an independent expert diagnostic agent. The critical function of the expert agent is to assess the outcomes of execution actions and pro-duce fine-grained analytical results. Specifically, based on the executed command, exit code, and standard output, the expert agent categorizes each action into one of three statesâ€” success , failure , or 

potential risk . According to the identified state, it generates corresponding repair commands or risk suggestions, and ultimately outputs a structured diagnostic report. Notably, the expert agent is endowed with the capability of on-the-live tool creation and execu-tion. At each decision step, the agent autonomously determines whether auxiliary tools are needed to support error judgment. Tool creation is subject to strict constraints: each tool must be a single-line executable command used solely for collecting diagnostic evidence rather than performing repairs. The outputs of these tools are treated as diagnostic evidence to strengthen error interpretation and are fed back to the main agent in a structured form. Furthermore, we introduce the concept of a self-evolving mechanism . After each diagnostic cycle, the expert agent incrementally adjusts its internal rules based on feedback signals. These rules pri-marily govern repair suggestion generation, tool creation, and risk assessment. Through continuous evolution driven by historical experience, the ex-pert agent progressively refines its decision-making process and becomes capable of handling increas-ingly complex configuration errors. 

4 Experimental Setup 

We evaluate EvoConfig from two complemen-tary perspectives: environment build success and process-level error correction capability. 

4.1 Environment Build Success Evaluation Dataset and Baselines. We evaluate environ-ment construction on 420 Repo2Run reposito-ries (Hu et al., 2025) and 324 Python repos-itories from EnvBench (Eliseeva et al., 2025), excluding 5 EnvBench repositories larger than 200MB. All experiments follow the Repo2Run pro-tocol and compare EvoConfig with pipreqs (bndr, 2016), LLM Generator , SWE-agent (Yang et al., 2024), and Repo2Run (Hu et al., 2025). We use gpt-4o-2024-05-13 , GPT-3.5-turbo , and 

GPT-4o-mini , with a 2-hour time limit and up to 100 interaction rounds. Additional details are pro-vided in Appendix A. 4Method Backbone DGSR # Successfully EBSR # Successfully Generated Dockerfiles Built Environments                          

> pipreqs (bndr, 2016) -29.8% 125 6.0% 25 LLM generator (Hu et al., 2025) GPT-4o 47.6% 200 22.1% 93 SWE-agent (Yang et al., 2024) GPT-4o 26.9% 113 9.0% 38 Repo2Run (Hu et al., 2025) GPT-4o 100% 420 86.0% 361
> EvoConfig GPT-4o 100% 420 88.1% 370

Table 1: Main results of different baselines in terms of Dockerfile generation and environment build success under the same backbone.              

> Method Backbone EBSR # Successfully Built Environments
> Repo2Run GPT-3.5-turbo 71.0% 230 GPT-4o-mini 40.0% 12
> EvoConfig GPT-3.5-turbo 78.1 %253
> GPT-4o-mini 46.7 %14

Table 2: Performance comparison under different back-bone models on the 324 repositories from EnvBench. Results for gpt-4o-mini are obtained on a randomly sampled subset of 30 repositories. 

Evaluation Metrics. We use two metrics to eval-uate environment construction. DGSR measures the percentage of attempts that generate a runnable Dockerfile that builds without errors, while EBSR 

measures the percentage of attempts that success-fully build executable environments, requiring both a successful Dockerfile build and the ability to exe-cute tests with pytest , regardless of test outcomes. 

4.2 Process Error Correction Evaluation Dataset and Baselines. For process-level error correction evaluation, we use the EnConda-Bench dataset (Kuang et al., 2025b), which is designed to assess an agentâ€™s ability to diagnose errors and recover from failed configuration steps during inter-active execution. We evaluate all 4,201 instances provided by EnConda-Bench and compare Evo-Config against representative baselines, including 

SWE-Agent , OpenHands (Wang et al., 2024b), 

INSTALLAMATIC (Milliken et al., 2025a), and 

Repo2Run (Hu et al., 2025), using GPT-4.1 and 

DeepSeek-V3 (Liu et al., 2023) as the underlying language models. More details about our selected baselines are provided in Appendix B. 

Evaluation Metrics. We follow the evaluation protocol and metrics defined in EnConda-Bench, which measure an agentâ€™s capability from error per-ception to corrective execution. Specifically, the metrics include error classification precision and recall, error description accuracy and fix accuracy. Each agent interacts with the execution environ-ment step by step, generates diagnostic feedback and repair actions upon failure, and is evaluated based on both the correctness of intermediate error handling and the final recovery outcome. 

5 Result Analysis 

5.1 Main Results Environment Construction Success Analysis. 

The results of different baselines are presented in Table 1. Results of all baselines except EvoConfig are taken from the original Repo2Run benchmark to ensure a fair comparison. We observe that EvoConfig achieves an environ-ment building success rate that is comparable to, and slightly higher than, Repo2Run on the origi-nal set of 420 repositories. EvoConfig successfully builds executable environments for 370 reposito-ries (EBSR 88.1%), compared to 361 repositories (EBSR 86.0%) built by Repo2Run. Given the al-ready strong performance of Repo2Run, this im-provement suggests that EvoConfig can recover a small but non-negligible fraction of failure cases that remain challenging for existing environment configuration agents. EvoConfig also maintains a DGSR of 100%, matching Repo2Run and con-firming that robust rollback and verification mech-anisms are preserved, while other baselines fail to consistently guarantee Dockerfile buildability. Table 2 further presents environment building performance under different language model back-bones. For gpt-3.5-turbo , EvoConfig improves EBSR from 71.0% to 78.1%, corresponding to 23 additional repositories successfully configured. We also present results using gpt-4o-mini , evaluated on a randomly sampled subset of 30 repositories due to computational constraints. This result indi-cates that the advantages of EvoConfig generalize 5Method Backbone Perception Feedback Feedback and Action                                                              

> Error type Error description Fix suggestion Pre. Rec. F1 ACC. ACC.
> Code Agent
> SWE-Agent (Yang et al., 2024) GPT-4.1 43.7 83.2 55.3 49.8 30.7 DeepSeek-V3 41.2 70.3 51.9 44.5 27.8 OpenHands (Wang et al., 2024b) GPT-4.1 42.5 72.0 53.2 46.0 29.1 DeepSeek-V3 46.7 93.6 58.7 51.9 33.8
> Environment Configuration Agent
> INSTALLAMATIC (Milliken et al., 2025a) GPT-4.1 37.5 70.4 48.9 45.3 29.1 DeepSeek-V3 40.7 76.8 53.2 49.3 32.5 Repo2Run (Hu et al., 2025) GPT-4.1 44.2 72.3 54.8 48.5 38.6 DeepSeek-V3 46.3 74.2 56.8 44.6 41.2
> EvoConfig GPT-4.1 49.2 75.4 59.7 56.5 39.4 DeepSeek-V3 52.3 77.9 62.6 48.3 45.9

Table 3: Main results across different agents on EnConda-Bench. 

across different model backbones. 

Process-level Error Correction Analysis. We evaluate process-level error correction results on EnConda-Bench in Table 3. EvoConfig demon-strates consistently stronger performance across both error perception and repair-related metrics, indicating improved handling of configuration fail-ures during interactive execution. We observe that code agents such as SWE-Agent and OpenHands show improved error perception compared to generic agents, but their ability to translate diagnosis into effective repair actions re-mains limited. For instance, OpenHands with DeepSeek-V3 achieves an error type F1 score of 58.7 and an error description accuracy of 51.9, while its fix suggestion accuracy is only 33.8, in-dicating a clear gap between error understand-ing and action-level repair. Environment con-figuration agents further improve repair effec-tiveness: INSTALLAMATIC increases fix accu-racy to 32.5, and Repo2Run reaches 41.2 under DeepSeek-V3, demonstrating the benefit of ex-plicitly modeling environment interaction. Evo-Config consistently achieves the strongest perfor-mance across both backbones, reaching error type F1 scores of 59.7/62.6 and fix suggestion accura-cies of 39.4/45.9 under GPT-4.1 and DeepSeek-V3, respectively. These results suggest that EvoCon-fig better aligns fine-grained error analysis with actionable repair guidance, highlighting the value of adaptive, expert-driven diagnosis in improving process-level error correction.       

> Method EBSR # Successfully Built Environments w/o Environment Info Extraction
> 82.0% 82
> w/o Self-Evolving Expert Diagnosis
> 75.0% 75
> EvoConfig 83.0% 83

Table 4: Ablation results of EvoConfig in terms of envi-ronment build success. 

5.2 Ablation Study 

We conduct ablation studies on a randomly sampled set of 100 repositories from EnvBench to examine the contributions of the environment information extraction module and the self-evolving expert di-agnosis module, with additional details provided in Appendix C. As shown in Table 4, removing the self-evolving expert diagnosis module leads to a substantial drop in environment building success rate (EBSR) from 83.0% to 75.0%, while remov-ing the environment information extraction module results in a smaller decrease to 82.0%. The runtime comparison in Figure 3, measured on 30 reposito-ries successfully configured by all variants, further shows that EvoConfig consistently achieves lower average configuration time. In particular, disabling environment information extraction leads to longer execution trajectories, whereas removing expert di-agnosis causes the most significant slowdown and higher variance, indicating repeated and inefficient repair attempts. These results suggest that envi-62 4 6 8 10 

> Frequency
> 8
> 12
> 16
> 20
> 24
> 28
> 32
> Average Time (min)
> w/o Env-Info Extraction
> w/o Self-Evolving Diagnosis
> Ours

Figure 3: Runtime distribution of successful environ-ment builds in the ablation study.          

> Method Times (min) Tokens Cost
> Repo2Run 30.5 495268 $0.33
> EvoConfig 20.9 229531 $0.16

Table 5: Effiency comparison of successful building. 

ronment information extraction mainly improves efficiency, while adaptive diagnosis is critical for robustness and success. To further evaluate the effectiveness of the self-evolving expert diagnosis module at the process level, we conduct a focused ablation study on EnConda-Bench using DeepSeek-V3, with results shown in Figure 4. The evaluation is performed on all instances from a randomly sampled set of 100 repositories, and more detailed analysis is provided in Appendix D. Removing the diagnosis module consistently degrades performance across all stages of error handling, with the error description accu-racy decreasing from 48.3 to 44.1, and fix sugges-tion accuracy declining from 45.9 to 41.0. Notably, the performance gap is most pronounced in fix sug-gestion accuracy, indicating that without adaptive expert feedback, the agent struggles to translate er-ror understanding into effective corrective actions. In contrast, EvoConfig maintains a more consistent perception feedback action performance, indicat-ing that the self evolving diagnostic mechanism enhances error correction ability of agent at the process-level while ensuring environment configu-ration success rate. 

6 Discussion 

6.1 Efficiency and Cost Analysis 

As shown in Table 5, experiments on the 324 En-vBench repositories show that EvoConfig substan-tially improves both aspects: it reduces the average Error description Fix suggestion 

> 16
> 24
> 32
> 40
> 48
> 56
> Accuracy
> w/o Self-Evolving Diagnosis
> Ours

Figure 4: Ablation results of EvoConfig on process-level error correction.       

> Category # Case (%)
> Hardware Insufficiency 23 (32.4%) Config Files Missing 20 (28.2%) Dependency Installation Timeout 10 (14.1%) Unit Tests Missing 5 (7.0%) Runtest Timeout 13 (18.3%)

Table 6: Analysis of failure cases in EvoConfig. 

configuration time per repository from 30.5 min-utes to 20.9 minutes and incurs lower token-level and monetary cost under gpt-3.5-turbo . These gains are largely attributable to EvoConfigâ€™s multi-agent design, which separates execution control from error diagnosis and feedback interpretation, preventing long execution traces from repeatedly entering the main agentâ€™s context and thereby re-ducing redundant reasoning during configuration. 

6.2 Failure Case Study 

We analyze the failure cases of EvoConfig on the EnvBench benchmark, with the distribution sum-marized in Table 6. Most failures are caused by external execution constraints or repository-intrinsic issues rather than limitations of the agent itself. Hardware insufficiency is the most com-mon failure source, accounting for 32.4% of failed cases, followed by missing or incomplete configu-ration information (28.2%), where repositories lack core files such as pyproject.toml , setup.py , or 

requirements.txt . A further portion of failures arises from execution timeouts during dependency installation or test execution, reflecting practical limits imposed by heavy dependencies and long-running tests. 

7 Related Work 

Executable environments as a prerequisite for training and evaluating SWE agents. Ex-7ecutable environments are a prerequisite for repository-level SWE agents, because both train-ing signals and evaluation protocols assume that projects can be built and their verification proce-dures can be executed reproducibly. Accordingly, environment configuration is deeply embedded in popular benchmarks and data pipelines: several widely-used settings rely on manual, repository-specific environment curation, such as SWE-bench (Jimenez et al., 2024), SWE-Flow (Zhang et al., 2025b), SWE-Gym (Pan et al., 2025), and R2E-Gym (Jain et al., 2025). Recent bench-mark and data construction workflows increas-ingly incorporate automated or semi-automated environment synthesis as a critical stage, includ-ing SetupAgent (Vergopoulos et al., 2025), SWE-smith (Yang et al., 2025), SWE-Factory (Guo et al., 2025), SWE-bench-Live (Zhang et al., 2025c), SWE-Compass (Xu et al., 2025a), and SWE-Bench++ (Wang et al., 2025b). Collectively, these trends motivate environment synthesis as a first-class research problem that directly controls the scalability and reliability of executable SWE data. 

Methods for automated environment setup. 

Automated setup methods broadly fall into de-terministic and agentic families (Li et al., 2024a, 2025b,c; Kuang et al., 2025c; Ye et al., 2025). De-terministic approaches implement setup as auto-mated scripts or fixed pipelines, which execute standardized procedures across diverse reposito-ries to maximize reproducibility and reduce per-repository manual effort; a representative example is R2E (Jain et al., 2024), which instantiates exe-cutable test environments via scripted setup pro-cedures. EnvBench (Eliseeva et al., 2025) spans both families: it introduces a benchmark for auto-mated environment setup, includes a determinis-tic shell-script baseline, and also evaluates LLM-based Bash agents under the same task definition and proxy-based verification signals. Template-guided container synthesis constrains Dockerfile structure while leaving repository-specific slots to be filled, improving robustness at scale in SWE-Bench++ (Wang et al., 2025b). Agentic approaches treat setup as interactive search-and-repair: an LLM agent retrieves commands from documen-tation and project artifacts, executes them in a sand-box, diagnoses failures from logs, and iteratively refines the plan, as done in SetupAgent (Vergopou-los et al., 2025) and in the RepoLaunch pipeline of SWE-bench-Live (Zhang et al., 2025c); SWE-Factory adopts multi-agent decomposition and en-vironment reuse to amortize successful configu-rations (Guo et al., 2025). Related systems tar-get key subroutines, including scalable export of runnable Docker environments in Repo2Run (Hu et al., 2025), installation under incomplete docu-mentation in Installamatic (Milliken et al., 2025b), and test execution for arbitrary projects in Exe-cutionAgent (Bouzenia and Pradel, 2025b), while earlier dependency inference in DockerizeMe illus-trates the limits of purely static signals (Horton and Parnin, 2019). 

Benchmarks that evaluate environment setup ability. With the rapid development in the field of LLMs (Li et al., 2025a; Chen et al., 2025b; Huang et al., 2024; Li et al., 2024b; Zhang et al., 2025d; Liu et al., 2022; Du et al., 2024; Li et al., 2022, 2023, 2025e; Xu et al., 2025b; Yu et al., 2024), a complementary line of work elevates environment bootstrapping into a first-class benchmarked capa-bility (Miao et al., 2025; Li et al., 2025d; Chen et al., 2025a). EnvBench (Eliseeva et al., 2025) provides a large-scale benchmark for repository-specific setup across Python and JVM projects and introduces automatic proxy metrics such as missing-import and compilation checks to sup-port scalable evaluation. SetupBench (Arora et al., 2025) formalizes bootstrapping from a bare Linux sandbox with deterministic one-line verification commands, enabling fine-grained analysis of fail-ure modes such as incomplete toolchains and non-persistent modifications. Enconda-bench (Kuang et al., 2025b) moves beyond end-to-end success by scoring process-level trajectories and diagnosing capabilities such as setup planning, error localiza-tion, and feedback-driven repair under realistically perturbed instructions. SWE-Compass (Xu et al., 2025a) incorporates configuration and deployment tasks into a broader agentic coding evaluation suite, contextualizing setup as part of end-to-end agent behavior. Finally, Multi-Docker-Eval (Fu et al., 2025) expands evaluation to multi-language reposi-tories and emphasizes both effectiveness and effi-ciency, including time and resource usage as well as resulting image size. 

8 Conclusion 

In this paper, we propose EvoConfig, a self-evolving multi-agent framework that decouples execution, diagnosis, and repair. By combin-ing lightweight environment information extrac-8tion with adaptive expert diagnosis, EvoConfig improves configuration robustness and efficiency. Experiments on multiple benchmarks demonstrate strong environment building performance with re-duced time and token cost, while process-level eval-uations show improved error understanding and re-pair quality. EvoConfig focuses on enabling test execution, and extending it to reason about test outcomes remains future work. 

Limitations 

EvoConfig focuses on constructing executable en-vironments and improving process-level error cor-rection, but does not reason about test correctness. Our evaluation considers whether unit tests can be executed, rather than the proportion of tests that pass. In practice, test failures may arise from issues beyond environment configuration. While EvoCon-fig enables tests to run reliably, analyzing test out-comes and debugging failing tests remain outside its current scope and are left for future work. 

Ethical Considerations 

Potential Risks Although EvoConfig improves robustness and efficiency in automated environ-ment configuration, several risks remain. The framework depends on execution feedback qual-ity, and noisy or incomplete errors may still affect diagnosis. In addition, the self-evolving mecha-nism may require sufficient feedback to stabilize in early stages. Finally, EvoConfig focuses on build-ing runnable environments and does not guarantee the correctness of test outcomes, which may limit its use in strict functional validation settings. 

Ethical Statement This work focuses on au-tomated environment configuration for open-source software repositories using large language modelâ€“based agents. All experiments are con-ducted on publicly available data and executed in isolated environments, without involving personal, sensitive, or private information. 

LLMs Usage Statement Large language models were used to assist with language polishing and clarity improvement during the writing process; all technical content, experimental design, and conclu-sions were developed and verified by the authors. The proposed method aims to improve research re-producibility and scalability in software engineer-ing and does not introduce new ethical risks beyond those of existing automated development tools. 

References 

Anysphere. 2023. Cursor: An ai-first code editor. 

https://www.cursor.com/ . Official website, ac-cessed 2026-01-05. Avi Arora, Jinu Jang, and Roshanak Zilouchian Moghaddam. 2025. Setupbench: Assessing software engineering agentsâ€™ ability to bootstrap development environments. Preprint , arXiv:2507.09063. bndr. 2016. pipreqs: Generate pip requirements.txt file based on imports of any project. https://github. com/bndr/pipreqs . GitHub repository, accessed 2026-01-05. Islem Bouzenia and Michael Pradel. 2025a. You name it, i run it: An llm agent to execute tests of arbitrary projects. Proceedings of the ACM on Software Engi-neering , 2(ISSTA):1054â€“1076. Islem Bouzenia and Michael Pradel. 2025b. You name it, I run it: An LLM agent to execute tests of arbitrary projects. Proc. ACM Softw. Eng. , 2(ISSTA):1054â€“ 1076. Li-Guo Chen, Zheng Xiao, Yi-Jiang Xu, Rui-Chuan An, Xin Wang, Yang-Ning Li, Ying-Hui Li, Yi-Dong Wang, Zheng-Ran Zeng, Qing Gao, and 1 others. 2025a. Coderankeval: Benchmarking and analyzing llm performance for code ranking. Journal of Com-puter Science and Technology , 40(5):1220â€“1233. Shaoshen Chen, Yangning Li, Zishan Xu, Yongqin Zeng, Shunlong Wu, Xinshuo Hu, Zifei Shan, Xin Su, Jiwei Tang, Yinghui Li, and 1 others. 2025b. Dast: Context-aware compression in llms via dynamic allocation of soft tokens. In Findings of the Association for Computational Linguistics: ACL 2025 , pages 20544â€“ 20552. Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Sri-nath, and 1 others. 2024. Llms assist nlp researchers: Critique paper (meta-) reviewing. In Proceedings of the 2024 conference on empirical methods in natural language processing , pages 5081â€“5099. Aleksandra Eliseeva, Alexander Kovrigin, Ilia Kholkin, Egor Bogomolov, and Yaroslav Zharov. 2025. En-vbench: A benchmark for automated environment setup. In ICLR 2025 Third Workshop on Deep Learn-ing for Code .Kelin Fu, Tianyu Liu, Zeyu Shang, Yingwei Ma, Jian Yang, Jiaheng Liu, and Kaigui Bian. 2025. Multi-docker-eval: A â€˜shovel of the gold rushâ€™ benchmark on automatic environment building for software engi-neering. Preprint , arXiv:2512.06915. GitHub. 2021. Github copilot: Your ai pair program-mer. https://github.com/features/copilot .Online product page, accessed 2026-01-05. 

9Lianghong Guo, Yanlin Wang, Caihua Li, Pengyu Yang, Jiachi Chen, Wei Tao, Yingtian Zou, Duyu Tang, and Zibin Zheng. 2025. Swe-factory: Your automated factory for issue resolution training data and evalua-tion benchmarks. Preprint , arXiv:2506.10954. Junda He, Christoph Treude, and David Lo. 2025. Llm-based multi-agent systems for software engineering: Literature review, vision, and the road ahead. ACM Transactions on Software Engineering and Method-ology , 34(5):1â€“30. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, and 1 others. 2023. Metagpt: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representa-tions .Eric Horton and Chris Parnin. 2019. Dockerizeme: au-tomatic inference of environment dependencies for python code snippets. In Proceedings of the 41st International Conference on Software Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019 ,pages 328â€“338. IEEE / ACM. Ruida Hu, Chao Peng, XinchenWang, Junjielong Xu, and Cuiyun Gao. 2025. Repo2run: Automated build-ing executable environment for code repository at scale. In The Thirty-ninth Annual Conference on Neural Information Processing Systems .Shulin Huang, Shirong Ma, Yinghui Li, Mengzuo Huang, Wuhe Zou, Weidong Zhang, and Haitao Zheng. 2024. Lateval: An interactive llms evaluation benchmark with incomplete information from lateral thinking puzzles. In Proceedings of the 2024 Joint International Conference on Computational Linguis-tics, Language Resources and Evaluation (LREC-COLING 2024) , pages 10186â€“10197. Naman Jain, Manish Shetty, Tianjun Zhang, King Han, Koushik Sen, and Ion Stoica. 2024. R2e: Turning any github repository into a programming agent test environment. In ICLR 2024 Workshop on Large Lan-guage Model (LLM) Agents .Naman Jain, Jaskirat Singh, Manish Shetty, Tianjun Zhang, Liang Zheng, Koushik Sen, and Ion Stoica. 2025. R2e-gym: Procedural environment generation and hybrid verifiers for scaling open-weights SWE agents. In Second Conference on Language Model-ing .Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. 2024. SWE-bench: Can language mod-els resolve real-world github issues? In The Twelfth International Conference on Learning Representa-tions .Aleksandar Krnjaic, Raul D Steleac, Jonathan D Thomas, Georgios Papoudakis, Lukas SchÃ¤fer, An-drew Wing Keung To, Kuan-Ho Lao, Murat Cubuk-tepe, Matthew Haley, Peter BÃ¶rsting, and 1 others. 2024. Scalable multi-agent reinforcement learning for warehouse logistics with robotic and human co-workers. In 2024 IEEE/RSJ International Confer-ence on Intelligent Robots and Systems (IROS) , pages 677â€“684. IEEE. Jiayi Kuang, Haojing Huang, Yinghui Li, Xinnian Liang, Zhikun Xu, Yangning Li, Xiaoyu Tan, Chao Qu, Meishan Zhang, Ying Shen, and 1 others. 2025a. Atomic thinking of llms: Decoupling and explor-ing mathematical reasoning abilities. arXiv preprint arXiv:2509.25725 .Jiayi Kuang, Yinghui Li, Xin Zhang, Yangning Li, Di Yin, Xing Sun, Ying Shen, and Philip S. Yu. 2025b. Process-level trajectory evaluation for envi-ronment configuration in software engineering agents. 

Preprint , arXiv:2510.25694. Jiayi Kuang, Ying Shen, Jingyou Xie, Haohao Luo, Zhe Xu, Ronghao Li, Yinghui Li, Xianfeng Cheng, Xika Lin, and Yu Han. 2025c. Natural language un-derstanding and inference with mllm in visual ques-tion answering: A survey. ACM Computing Surveys ,57(8):1â€“36. Yangning Li, Shaoshen Chen, Yinghui Li, Yankai Chen, Hai-Tao Zheng, Hui Wang, Wenhao Jiang, and Philip S Yu. 2025a. Admtree: Compressing lengthy context with adaptive semantic trees. arXiv preprint arXiv:2512.04550 .Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Philip S Yu, Fei Huang, and 1 others. 2024a. Benchmarking multimodal retrieval augmented gen-eration with dynamic vqa dataset and self-adaptive planning agent. arXiv preprint arXiv:2411.02937 .Yangning Li, Weizhi Zhang, Yuyao Yang, Wei-Chieh Huang, Yaozu Wu, Junyu Luo, Yuanchen Bei, Henry Peng Zou, Xiao Luo, Yusheng Zhao, and 1 others. 2025b. Towards agentic rag with deep rea-soning: A survey of rag-reasoning systems in llms. 

arXiv preprint arXiv:2507.09477 .Yinghui Li, Haojing Huang, Jiayi Kuang, Yangning Li, Shu-Yu Guo, Chao Qu, Xiaoyu Tan, Hai-Tao Zheng, Ying Shen, and Philip S Yu. 2025c. Refine knowledge of large language models via adaptive con-trastive learning. arXiv preprint arXiv:2502.07184 .Yinghui Li, Haojing Huang, Shirong Ma, Yong Jiang, Yangning Li, Feng Zhou, Hai-Tao Zheng, and Qingyu Zhou. 2023. On the (in) effectiveness of large lan-guage models for chinese text correction. arXiv preprint arXiv:2307.09007 .Yinghui Li, Jiayi Kuang, Haojing Huang, Zhikun Xu, Xinnian Liang, Yi Yu, Wenlian Lu, Yangning Li, Xi-aoyu Tan, Chao Qu, and 1 others. 2025d. One exam-ple shown, many concepts known! counterexample-driven conceptual reasoning in mathematical llms. 

arXiv preprint arXiv:2502.10454 .

10 Yinghui Li, Shang Qin, Jingheng Ye, Haojing Huang, Yangning Li, Shu-Yu Guo, Libo Qin, Xuming Hu, Wenhao Jiang, Hai-Tao Zheng, and 1 others. 2025e. Rethinking the roles of large language models in chi-nese grammatical error correction. In Proceedings of the 63rd Annual Meeting of the Association for Com-putational Linguistics (Volume 6: Industry Track) ,pages 553â€“567. Yinghui Li, Qingyu Zhou, Yangning Li, Zhongli Li, Ruiyang Liu, Rongyi Sun, Zizhen Wang, Chao Li, Yunbo Cao, and Hai-Tao Zheng. 2022. The past mis-take is the future wisdom: Error-driven contrastive probability optimization for chinese spell checking. 

arXiv preprint arXiv:2203.00991 .Yinghui Li, Qingyu Zhou, Yuanzhen Luo, Shirong Ma, Yangning Li, Hai-Tao Zheng, Xuming Hu, and Philip S Yu. 2024b. When llms meet cunning texts: A fallacy understanding benchmark for large language models. Advances in Neural Information Processing Systems , 37:112433â€“112458. Dingbang Liu, Fenghui Ren, Jun Yan, Guoxin Su, Wen Gu, and Shohei Kato. 2024. Scaling up multi-agent reinforcement learning: An extensive survey on scal-ability issues. IEEE Access , 12:94610â€“94631. Ruiyang Liu, Yinghui Li, Linmi Tao, Dun Liang, and Hai-Tao Zheng. 2022. Are we ready for a new paradigm shift? a survey on visual deep mlp. Pat-terns , 3(7). Tianyang Liu, Canwen Xu, and Julian McAuley. 2023. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091 .Junru Lu, Jiarui Qin, Lingfeng Qiao, Yinghui Li, Xinyi Dai, Bo Ke, Jianfeng He, Ruizhi Qiao, Di Yin, Xing Sun, and 1 others. 2025. Youtu-llm: Unlocking the native agentic potential for lightweight large lan-guage models. arXiv preprint arXiv:2512.24618 .Chunyu Miao, Henry Peng Zou, Yangning Li, Yankai Chen, Yibo Wang, Fangxin Wang, Yifan Li, Wooseong Yang, Bowei He, Xinni Zhang, and 1 oth-ers. 2025. Recode-h: A benchmark for research code development with interactive human feedback. arXiv preprint arXiv:2510.06186 .Louis Milliken, Sungmin Kang, and Shin Yoo. 2025a. Beyond pip install: Evaluating llm agents for the automated installation of python projects. In 2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER) , pages 1â€“11. IEEE. Louis Milliken, Sungmin Kang, and Shin Yoo. 2025b. Beyond pip install: Evaluating LLM agents for the automated installation of python projects. In IEEE International Conference on Software Analysis, Evo-lution and Reengineering, SANER 2025, Montreal, QC, Canada, March 4-7, 2025 , pages 1â€“11. IEEE. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. 2025. Training software engineering agents and verifiers with SWE-gym. In Forty-second International Con-ference on Machine Learning .Alexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, GarÃ°ar Ingvarsson Juto, Timon Willi, Ravi Hammond, Akbir Khan, Chris-tian Schroeder de Witt, and 1 others. 2024. Jaxmarl: Multi-agent rl environments and algorithms in jax. 

Advances in Neural Information Processing Systems ,37:50925â€“50951. Konstantinos Vergopoulos, Mark Niklas Mueller, and Martin Vechev. 2025. Automated benchmark gen-eration for repository-level coding tasks. In Forty-second International Conference on Machine Learn-ing .Junhao Wang, Daoguang Zan, Shulin Xin, Siyao Liu, Yurong Wu, and Kai Shen. 2025a. Swe-mirror: Scaling issue-resolving datasets by mir-roring issues across repositories. arXiv preprint arXiv:2509.08724 .Lilin Wang, Lucas Ramalho, Alan Celestino, Phuc An-thony Pham, Yu Liu, Umang Kumar Sinha, Andres Portillo, Onassis Osunwa, and Gabriel Maduekwe. 2025b. Swe-bench++: A framework for the scal-able generation of software engineering bench-marks from open-source repositories. Preprint ,arXiv:2512.17419. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xi-angru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, and 1 others. 2024a. Open-devin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741 ,3. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, and 1 others. 2024b. Openhands: An open platform for ai soft-ware developers as generalist agents. arXiv preprint arXiv:2407.16741 .Yanlin Wang, Wanjun Zhong, Yanxian Huang, Ensheng Shi, Min Yang, Jiachi Chen, Hui Li, Yuchi Ma, Qianx-iang Wang, and Zibin Zheng. 2025c. Agents in soft-ware engineering: Survey, landscape, and vision. Au-tomated Software Engineering , 32(2):70. Chunqiu Steven Xia, Zhe Wang, Yan Yang, Yuxiang Wei, and Lingming Zhang. 2025. Live-swe-agent: Can software engineering agents self-evolve on the fly? arXiv preprint arXiv:2511.13646 .Yiqing Xie, Alex Xie, Divyanshu Sheth, Pengfei Liu, Daniel Fried, and Carolyn Rose. 2024. Codebench-gen: Creating scalable execution-based code genera-tion benchmarks. arXiv preprint arXiv:2404.00566 .Jingxuan Xu, Ken Deng, Weihao Li, Songwei Yu, Huaixi Tang, Haoyang Huang, Zhiyi Lai, Zizheng 

11 Zhan, Yanan Wu, Chenchen Zhang, Kepeng Lei, Yi-fan Yao, Xinping Lei, Wenqiang Zhu, Zongxian Feng, Han Li, Junqi Xiong, Dailin Li, Zuchen Gao, and 20 others. 2025a. Swe-compass: Towards unified eval-uation of agentic coding abilities for large language models. Preprint , arXiv:2511.05459. Zhikun Xu, Yinghui Li, Ruixue Ding, Xinyu Wang, Boli Chen, Yong Jiang, Haitao Zheng, Wenlian Lu, Pengjun Xie, and Fei Huang. 2025b. Let llms take on the latest challenges! a chinese dynamic question answering benchmark. In Proceedings of the 31st International Conference on Computational Linguis-tics , pages 10435â€“10448. John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems , 37:50528â€“ 50652. John Yang, Kilian Lieret, Carlos E Jimenez, Alexan-der Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. 2025. SWE-smith: Scaling data for software en-gineering agents. In The Thirty-ninth Annual Con-ference on Neural Information Processing Systems Datasets and Benchmarks Track .Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations .Jingheng Ye, Yong Jiang, Xiaobin Wang, Yinghui Li, Yangning Li, Pengjun Xie, and Fei Huang. 2025. Productagent: Benchmarking conversational product search agent with asking clarification questions. In 

Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track , pages 383â€“398. Tianyu Yu, Chengyue Jiang, Chao Lou, Shen Huang, Xiaobin Wang, Wei Liu, Jiong Cai, Yangning Li, Yinghui Li, Kewei Tu, and 1 others. 2024. Seqgpt: An out-of-the-box large language model for open domain sequence understanding. In Proceedings of the AAAI Conference on Artificial Intelligence , vol-ume 38, pages 19458â€“19467. Jiayi Zhang, Yiran Peng, Fanqi Kong, Yang Cheng, Yi-fan Wu, Zhaoyang Yu, Jinyu Xiang, Jianhao Ruan, Jinlin Wang, Maojia Song, and 1 others. 2025a. Autoenv: Automated environments for measuring cross-environment agent learning. arXiv preprint arXiv:2511.19304 .Lei Zhang, Jiaxi Yang, Min Yang, Jian Yang, Mouxiang Chen, Jiajun Zhang, Zeyu Cui, Binyuan Hui, and Junyang Lin. 2025b. Synthesizing software engineer-ing data in a test-driven manner. In Forty-second International Conference on Machine Learning .       

> Method EBSR Average Times (min) w/o Environment Info Extraction
> 82.0% 22.5
> w/o Self-Evolving Expert Diagnosis
> 75.0% 27.2
> EvoConfig 83.0% 20.5

Table 7: Ablation results of EvoConfig in terms of envi-ronment build success and time cost. Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, and Dongmei Zhang. 2025c. SWE-bench goes live! In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Bench-marks Track .Weizhi Zhang, Yuanchen Bei, Liangwei Yang, Henry Peng Zou, Peilin Zhou, Aiwei Liu, Yinghui Li, Hao Chen, Jianling Wang, Yu Wang, and 1 others. 2025d. Cold-start recommendation towards the era of large language models (llms): A comprehensive sur-vey and roadmap. arXiv preprint arXiv:2501.01945 .

A Environment Build Success Evaluation 

A.1 Baselines 

We evaluate environment configuration perfor-mance using the following representative baselines: â€¢ pipreqs : A static dependency analysis tool that inspects Python import statements to infer required packages and generates a

requirements.txt file, which is then used to construct a Dockerfile. â€¢ LLM Generator : A direct LLM-based ap-proach that parses repository README files and generates executable Dockerfiles without iter-ative interaction. â€¢ SWE-agent : An LLM-based agent with a cus-tom agentâ€“computer interface that supports file inspection, editing, and command execu-tion; its framework is retained while prompts are adapted for environment configuration. â€¢ Repo2Run : A strong agent-based baseline specifically designed for iterative environment configuration through interaction with the ex-ecution environment. 12 Method Backbone Perception Feedback Feedback and Action                   

> Error Type Error Description Fix Suggestion
> Pre. Rec. F1 ACC. ACC. w/o Self-Evolving Expert Diagnosis DeepSeek-V3 43.1 76.2 55.1 44.1 41.0
> EvoConfig DeepSeek-V3 52.3 77.9 62.6 48.3 45.9
> Table 8: Complete ablation experiment results of EvoConfig on process-level error correction.

B Process Error Correction Evaluation 

To evaluate process-level error correction, we com-pare EvoConfig with several representative agent-based baselines that support iterative interaction with the execution environment. â€¢ SWE-agent : An LLM-based agent originally designed for automated bug fixing, which supports file inspection, code editing, and command execution through a custom agentâ€“ computer interface. We adapt its prompts for process-level environment error correction. â€¢ OpenHands : A general-purpose autonomous agent framework for software engineering tasks, used here as a generic baseline to as-sess its ability to correct environment errors through multi-step interaction. â€¢ INSTALLAMATIC : An LLM-driven system that focuses on generating installation and setup commands for resolving dependency-related environment issues, without explicit long-horizon agent planning. â€¢ Repo2Run : A specialized agent-based sys-tem for repository environment configuration. 

C Time Efficiency Analysis 

We analyze environment building success rates (EBSR) and time efficiency of different variants, as reported in Table 7. EvoConfig achieves both the highest success rate (83.0%) and the lowest average configuration time (20.5 minutes), indicating that its improved performance does not come at the cost of increased runtime. In contrast, removing the en-vironment information extraction module slightly reduces the success rate to 82.0% while increasing the average configuration time to 22.5 minutes, sug-gesting that limited environment awareness leads to inefficient trial-and-error and correspondingly longer execution trajectories. Specifically, the success rate drops substantially to 75.0%, accompanied by a significant increase in average configuration time to 27.2 minutes. This observation indicates that static diagnosis strategies not only reduce the likelihood of successful envi-ronment construction but also result in repeated and inefficient repair attempts, thereby prolonging the overall configuration process. Together, these results demonstrate that while environment infor-mation extraction mainly contributes to execution efficiency, adaptive expert diagnosis is crucial for achieving both high configuration success rates and low configuration time. 

D Effect of Self-Evolving Expert Diagnosis 

We evaluate the contribution of the self-evolving expert diagnosis module by comparing the full EvoConfig framework with a variant that removes this component while using the same backbone (DeepSeek-V3). As shown in Table 8, removing self-evolving diagnosis results in a consistent per-formance degradation across all evaluation stages, indicating its critical role in the overall system. From the perception perspective, the absence of self-evolving diagnosis leads to a noticeable drop in error type recognition performance, with the F1 score decreasing from 62.6 to 55.1, mainly due to reduced precision. This suggests that static ex-pert behavior limits the agentâ€™s ability to accurately identify error patterns. Moreover, the accuracy of error description generation also declines from 48.3% to 44.1%, reflecting less precise feedback when adaptive diagnosis is disabled. The impact is further reflected in the action stage, where fix suggestion accuracy drops from 45.9% to 41.0%. Since effective repair actions rely on accu-rate error understanding, these results demonstrate that self-evolving expert diagnosis is an essential component for maintaining coherent perceptionâ€“ feedbackâ€“action alignment in large-scale environ-ment configuration. 13