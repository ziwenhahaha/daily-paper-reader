Title: How Sequential Algorithm Portfolios can benefit Black Box Optimization

URL Source: https://arxiv.org/pdf/2601.16896v1

Published Time: Mon, 26 Jan 2026 01:54:29 GMT

Number of Pages: 12

Markdown Content:
> 1

# How Sequential Algorithm Portfolios can benefit Black Box Optimization 

Catalin-Viorel Dinu, Diederick Vermetten, Carola Doerr 

Abstract —In typical black-box optimization applications, the available computational budget is often allocated to a single algorithm, typically chosen based on user preference with limited knowledge about the problem at hand or according to some expert knowledge. However, we show that splitting the bud-get across several algorithms yield significantly better results. This approach benefits from both algorithm complementarity across diverse problems and variance reduction within individual functions, and shows that algorithm portfolios do NOT require parallel evaluation capabilities. To demonstrate the advantage of sequential algorithm port-folios, we apply it to the COCO data archive, using over 200 algorithms evaluated on the BBOB test suite. The proposed sequential portfolios consistently outperform single-algorithm baselines, achieving relative performance gains of over 14%, and offering new insights into restart mechanisms and potential for warm-started execution strategies. 

I. I NTRODUCTION 

In the context of black-box optimization, a vast collection of algorithms have been developed over the years, each tailored to address specific characteristics or challenges found in certain classes of problems. This naturally leads to complementarity among algorithms: different optimizers are better suited for different problem types, depending on the structure of the instance, the available evaluation budget, and the performance metric being optimized. This diversity gives rise to the classical question: which algorithm should be used when solving a specific optimization problem (or a set of problems)? This is the core of the well-studied algorithm selection problem [1]. Modern solutions typically fall into two categories:  

> •

General-purpose recommendation system, based on ”a priori” knowledge such as variable types or rough budget constraint [2].  

> •

Feature-based selection methods, where part of the eval-uation budget is used to compute problem-specific fea-tures (commonly through Exploratory Landscape Analy-sis (ELA) [3], or other sampling-based approaches [4], and the algorithm is chosen based on these. However, both approaches come with risks. The extracted features might not fully capture the problem’s true structure, or the a priori knowledge may be insufficient or misleading. As such, selecting a single algorithm based on such information may lead to sub-optimal performance. To reduce the risk of selecting a sub-optimal algorithm, a common approach is to allocate a fraction of the total budget     

> Catalin-Viorel Dinu (Catalin-Viorel.Dinu@lip6.fr), Diederick Vermetten (Diederick.Vermetten@lip6.fr) and Carola Doerr (Carola.Doerr@lip6.fr) are with Sorbonne Universit´ e, CNRS, LIP6

to multiple algorithms, rather than relying on a single one. This strategy improves robustness by reducing the likelihood of poor outcomes, albeit at the cost of a more fragmented and potentially less efficient optimization process [5]. One prominent application of this idea is in the use of parallel port-folios, where multiple algorithms are executed concurrently, each receiving an equal portion of the total evaluation budget. This setup has shown strong empirical success across domains such as SAT solving [6], where the collective performance of the portfolio exceeds that of any individual component. While parallel portfolios naturally reduce variance by dis-tributing the risk across multiple algorithms, they also limit the opportunity to capitalize on algorithms that perform ex-ceptionally well when given more budget. This trade-off is especially pronounced in settings where performance improves significantly with longer uninterrupted runs, which parallel portfolios inherently avoid. However, rather than treating algo-rithm selection and parallel portfolios as opposing strategies, we propose to combine their strengths. While this has been tackled in online settings, where budget is allocated based on comparisons of progress between algorithms in the portfolio, e.g., in bet-and-run strategies [7], we focus here on a purely offline strategy without any needed interaction between al-gorithms. Specifically, we ask: How can a fixed evaluation budget be optimally divided among a set of algorithms? 

By allowing heterogeneous budget allocations, we are no longer restricted to uniform or parallel execution. This for-mulation provides a continuum; from allocating the entire budget to a single best-performing algorithm, to finely splitting the budget across several complementary algorithms. This flexibility enables the construction of portfolios that adapt to both performance variation and algorithmic complementarity, making better use of the available evaluation budget. In this paper, we address the budget allocation problem from a feature-free, data-driven perspective. Rather than relying on exploratory features or prior knowledge about the problem landscape, we propose a method that determines how to optimally distribute a fixed evaluation budget across multiple algorithms based solely on existing performance data. This approach is conceptually similar to selecting a single best solver (SBS), but extends it by enabling heterogeneous budget allocation among a portfolio of algorithms. Our results show that leveraging algorithm complementarity (by combining general-purpose and specialized optimizers) can lead to signif-icant performance improvements over the SBS. Furthermore, our analysis highlights the importance of restart strategies, as allocating budget to multiple shorter runs often yields better results than a single long execution. In the following sections, we introduce the portfolio con-

> arXiv:2601.16896v1 [cs.NE] 23 Jan 2026 2

struction problem (Section II), define the performance metric and baselines (Sections II-C–II-D), and present the main experimental results (Section III). We then describe the pro-posed greedy method (Section IV) and explore its extensions, including incremental construction and algorithm set influence (Section V). Our code to reproduce results is available at [8]. II. A LGORITHM PORTFOLIOS 

A. Related Work 

Algorithm portfolios provide a natural framework for lever-aging the complementary strengths of different optimization algorithms. Traditionally, portfolios have been studied in the context of parallel execution, where multiple algorithms are run simultaneously with equal computational budgets. In this setting, portfolios can be constructed either from a curated set of high-performing algorithms [6] or via automated configu-ration procedures that explicitly seek complementarity among the constituent solvers [9], [10]. Over time, these approaches have been adapted to address specific performance metrics and tailored to different problem domains [11], [12]. In sequential settings, portfolio approaches often involve some form of dynamic algorithm selection or chaining, where information from earlier runs guides future decisions [13]. While such strategies may include implicit knowledge transfer, they also benefit from variance reduction effects similar to those of parallel portfolios. Importantly, even in the absence of explicit information sharing, sequential portfolios have been shown to outperform individual state-of-the-art algorithms by exploiting the inherent diversity of algorithm behaviors [14]. 

B. Problem Formulation 

In our context, we will consider portfolio construction as a 

Budget Allocation problem. Specifically, we aim to find the portfolio that maximizes the performance while ensuring that the sum of all assigned evaluation budgets across the portfolio does not exceed a fixed total budget T . This gives rise to the following constrained optimization problem: 

max  

> S

J (F, S)

s.t. X

> ⟨α, b ⟩∈S

b ≤ T. 

Where J is our performance measure to be maximized, F

is the set of problems to be optimized, and S is part of the search space of portfolios, and represented as a multi-set of algorithms a and budgets b: S = {⟨ α, b ⟩ | α ∈ A , b ∈ B} .Note that we use a multi-set representation here, since we can utilize multiple runs of the same algorithm to benefit from the variance reduction this brings. We observe that the structure of our portfolio optimization problem closely resembles the Unbounded Knapsack Problem (UKP) [15], where the goal is to select items (in our case, algorithm–budget pairs) under a total cost constraint in order to maximize some reward. However, a key distinction in our setting is that the performance function J can be highly non-linear, due to the interaction between multiple algorithm-budget pairs across multiple functions. 0.2k       

> 1.0k
> 1.8k
> 2.6k
> 3.4k
> 4.2k
> 5.0k
> 5.8k
> 6.6k
> 7.4k
> 8.2k
> 9.0k
> 9.8k
> 0.01
> 0.11
> 0.21
> 0.31
> 0.41
> 0.51
> 0.61
> 0.71
> 0.81
> 0.91
> Precision ( ε)
> 0.0 0.2 0.4 0.6 0.8 1.0 ̂
> ϕf,α
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Performance  

(a) BBOB F6

CMAES (nevergrad). Steady improvement with budget. 0.2k       

> 1.0k
> 1.8k
> 2.6k
> 3.4k
> 4.2k
> 5.0k
> 5.8k
> 6.6k
> 7.4k
> 8.2k
> 9.0k
> 9.8k
> 0.01
> 0.11
> 0.21
> 0.31
> 0.41
> 0.51
> 0.61
> 0.71
> 0.81
> 0.91
> Precision ( ε)
> 0.0 0.2 0.4 0.6 0.8 1.0 ̂
> ϕf,α
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Performance  

(b) BBOB F14 

CMAES (nevergrad) Early performance plateau. 

Fig. 1: Empirical attainment plots for BBOB functions using CMA-ES from nevergrad. The horizontal axis shows function evaluations ( B), and the vertical axis represents normalized precisions ( E). The red curve represent the performance of a portfolio consisting only of one pair algorithm-budget. 

C. Portfolio Performance 

As performance criterion, we make use of the empirical attainment function [16], [17] to represent an algorithm’s anytime performance. For any function f ∈ F , let f opt denote the global minimum of f . For any algorithm α ∈ A and any function f ∈ F , define f best  

> α

(b) as the best (smallest) function value that can be seen by α on f within a budget of b function evaluations. Given this, we can define the attainment function 

as: 

ϕf,α (b, ε ) = P f best  

> α

(b) − f opt ≤ ε .

We can estimate the attainment function by executing the algorithm R times, since bϕ R→∞ 

−−−−→ ϕ:

bϕf,α (b, ε ) = 1

R

> R

X

> i=1

1( f best    

> α(b)−fopt ≤ε)

.

where 1ξ denotes the indicator function which returns 1 if event ξ happens and 0 otherwise. We discretize both the budget and precision levels to obtain a matrix of empirical values (see Figure 1 for examples). Specifically, we define B = {bi} as the set of discrete budget levels (typically chosen to be evenly spaced) and E = {εj }

as the set of target precision thresholds. For visualization purposes, we normalize the precision axis to lie within the interval [0 , 1] , where a value of 0 corresponds to failure to attain even the coarsest precision level (i.e., max E), and a value of 1 corresponds to successful attainment of the finest precision level (i.e., min E). In this setting, we are essentially evaluating the empirical cumulative distribution function (ECDF) over a range of precision thresholds. A natural way to summarize performance is to compute the average empirical attainment across all precision levels in 

E. Thus, for a single algorithm–budget pair, we define its performance as the mean attainment over all ε ∈ E. An illustration of how this metric evolves with increasing budget is shown in Figure 1, where we observe contrasting algorithm 32.5 5 7.5 10 12.5 15 17.5 20 

Total Budget/1000 

0.30 

0.40 

0.50 

0.60 

0.70 

0.80    

> Performance
> 0.5 0.5
> 0.5 1.5 1.5
> 0.5
> 2
> 0.5
> 2
> 1
> 2.5
> 1
> 3.5
> 0.5
> 4
> 0.5
> 4
> 1
> 4
> 1.5
> 4
> 2
> 4
> 0.5
> 2
> 4.5
> 0.5
> 2
> 4.5
> 1
> 2
> 4.5
> 1.5
> 2
> 4.5
> 2
> 2
> 4.5
> 2
> 2
> 0.5
> 4.5
> 2.5
> 2
> 0.5
> 4.5
> 2.5
> 2
> 1
> 4.5
> 2.5
> 2
> 1.5
> 4.5
> 2.5
> 2
> 1.5
> 0.5
> 7.5
> 2
> 2
> 7.5
> 2.5
> 2
> 7.5
> 2.5
> 2
> 0.5
> 7.5
> 2.5
> 2
> 1
> 7.5
> 2.5
> 1.5
> 2
> 7.5
> 2.5
> 2
> 2
> 7.5
> 2.5
> 2
> 2
> 0.5
> 7.5
> 3
> 2
> 2
> 0.5
> 7.5
> 3
> 2
> 2
> 1
> 7.5
> 3
> 2.5
> 2
> 1
> 7.5
> 3
> 2.5
> 2
> 1.5
> 7.5
> 3
> 2.5
> 2
> 2
> 7.5
> 3
> 2.5
> 2
> 2.5
> 8.5
> 3
> 2.5
> 2
> 2
> 8.5
> 3
> 2.5
> 2.5
> 2
> 8.5
> 3
> 2.5
> 2.5
> 2.5
> 8.5
> 4
> 2
> 2.5
> 2.5
> 8.5
> 4
> 2.5
> 2.5
> 2.5
> BrentSTEPif
> BrentSTEPrr
> DTS-CMA-ES_005-2pop_v26_1model
> FULLNEWUOA
> SLSQP+lq-CMA-ES
> SLSQP-11-scipy_Hansen
> lmm-CMA-ES
> lq-CMA-ES
> tany
> texp

Budget Specific SBS 

Greedy Portfolio 

0

4

8

12 

16 

20 

> Budget/1000

Fig. 2: Performance comparison between the portfolio and baseline algorithm over increasing total budgets, using a subset of COCO’s algorithm set. Background elements represent the composition of the resulting portfolio. We observe that performance gains become more prominent as the total budget increases, and the selected portfolios diversify accordingly. behaviors: steady performance improvements in Figure 1a, and early saturation with performance plateaus in Figure 1b. We now generalize this formulation to the portfolio setting, allowing the portfolio S to contain multiple algorithm–budget pairs. At the same time, we extend the performance metric to account for a set of functions F rather than a single instance. In this context, we are interested in the expected probability of success: the probability that at least one configuration in the portfolio reaches a given precision threshold ε on a function 

f ∈ F . This is equivalent to one minus the joint probability of failure across all configurations in S.To aggregate this performance across the benchmark, we compute the average over all functions in F and all precision thresholds in E. The resulting portfolio performance metric is defined as: 

J (F, S) = 1

|F| |E| 

X 

> f∈F

X

> ε∈E

1 − Y

> ⟨α, b ⟩∈S



1 − bϕf,α (b, ε )

 .

D. Performance Baselines 

Before introducing our results, we begin by analyzing the structure of the problem and identifying a simple yet infor-mative baseline. For any task, a natural baseline is obtained by selecting a single algorithm that performs best on average across all functions in F, and assigning the entire available budget T to it (commonly referred to as the Single Best Solver, SBS, in the algorithm selection context). 

B = max  

> α∈A

J (F, {⟨ α, T ⟩} )

This baseline allows us to evaluate whether more complex portfolios (with multiple smaller-budget configurations) bring a significant advantage over simply committing to the globally strongest single algorithm. The upper bound on portfolio performance arises from the observation that, under our performance metric, algo-rithm–budget pairs can be reused. If a given precision level (target) has a non-zero empirical success probability for at least one algorithm, then by allocating more runs (i.e., more budget) to that pair, the chance of reaching that target in-creases. In the limit, with infinitely many independent repeti-tions, the success probability for such a target approaches 1. This leads to an idealized situation where, for each function and each precision level that is attainable (i.e., where the empirical attainment function is non-zero), we assume perfect success. In other words, the upper bound assumes that for each function, we can select the algorithm that eventually reaches all attainable precisions with probability 1, provided an unconstrained and sufficiently large budget. 

U = 1

|F| ∗ |E| 

X 

> f∈F

max 

> α∈A

X

> ε∈E

1(bϕf,α (b,ε )>0)

!

.

To quantify how much of the theoretically achievable perfor-mance a portfolio captures, we introduce the relative improve-ment over a baseline. The relative improvement is defined as: 

J (F, S) − BU − B

III. M AIN RESULTS 

To truly grasp how portfolio construction can contribute to the development of well-structured algorithm sets that outperform on average individual methods, we consider the full set of 24 functions from the BBOB suite [18]. These functions span a range of characteristics: separable ( F1–F5), low to moderate conditioning ( F6–F9), highly conditioned unimodal ( F10 –F14 ), multimodal with a clear global struc-ture ( F15 –F19 ), and multimodal with weak global structure 41 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24   

> Function ID
> 0.00
> 0.25
> 0.50
> 0.75
> 1.00
> Performance
> per-Function SBS Portfolio BBOB SBS

Fig. 3: The plot shows: (blue) the best-performing algorithm for each function; (ref) the single best algorithm across all functions; and (green) the performance of the constructed portfolio for a total budget of 10,000. Although the portfolio may not outperform the per-function best algorithm on every instance, it achieves a higher overall performance by leveraging complementary strengths across the function set. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24                                                                                             

> Function ID
> lq-CMA-ES
> BrentSTEPif
> SLSQP-11
> scipy_Hansen
> lmm-CMA-ES
> Algorithm
> 0.25 0.32 0.04 0.03 0.31 0.79 0.92 0.42 0.47 0.56 0.58 0.62 0.69 0.49 0.60 0.47 0.33 0.34 0.22 0.26 0.41 0.23 0.49 0.97
> 0.25 0.32 0.94 0.97 0.20 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.07 0.00 0.13 0.08 0.05 0.08 0.26 0.03 0.03 0.21 0.00
> 0.25 0.32 0.00 0.00 0.31 0.20 0.00 0.54 0.47 0.40 0.32 0.28 0.22 0.26 0.00 0.02 0.07 0.04 0.49 0.26 0.40 0.64 0.16 0.00
> 0.25 0.03 0.02 0.01 0.18 0.01 0.08 0.04 0.05 0.04 0.10 0.10 0.09 0.18 0.40 0.38 0.51 0.57 0.21 0.23 0.16 0.10 0.14 0.03
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Normalised Shapley

Fig. 4: Normalized Shapley values indicating the contribution of each algorithm to each individual function in the portfolio. Higher values suggest greater importance of the corresponding algorithm in improving performance for a given function. (F20 –F24 ). This diverse benchmark plays a central role in evaluating gradient-free optimization algorithms and has been widely adopted in the optimization community. To support consistent benchmarking, several archiving ini-tiatives have emerged to collect performance data from a wide range of optimization algorithms. One of the most prominent resources is the COCO archive [19], which includes perfor-mance data for over 250 algorithms contributed by various researchers and institutions. In our study, we restrict our attention to algorithms evaluated on 10-dimensional BBOB functions and for which 20 independent runs were recorded per function (this being the standard protocol). After applying these filters, we obtain a curated set of 234 algorithms. Due to the computational and memory overhead of han-dling such a large-scale dataset, we reduce it to a more manageable subset for meaningful experimentation. Specifi-cally, for each BBOB function, we select the algorithm that achieves the highest average empirical attainment function (EAF) across all budgets and precision levels. This results in a curated set of 20 top-performing algorithms: BIPOP-saACM-k, BrentSTEPrr, DE-BFGS, DTS-CMA-ES 005-2pop v26 1model, FULLNEWUOA, SLSQP+lq-CMA-ES, BrentSTEPif, SLSQP-11-scipy Hansen, lmm-CMA-ES, lq-CMA-ES, tany, texp, NEWUOA, BFGS-M-17, BIRMIN, xNES, BFGS-P-StPt, R-DE-10e5, MLSL, and oMads-Neg. This reduced selection strikes a balance between computa-tional feasibility and diverse algorithmic coverage, enabling more efficient evaluation of portfolio-based methods while maintaining relevance to real-world algorithm selection chal-lenges. We investigate this scenario under varying total budget levels, where T ∈ { 500 · k}k∈1... 40 . For each total budget 

Tk, we define the corresponding intermediary budget set as 

Bk = {500 · j}j≤k. The precision levels are fixed and log-normalized, defined as E = 10 2− x 

> 5
> x∈0... 50

(matching standard ECDF targets from [19]). In Figure 2, we present two overlapping plots. In the foreground, we show the performance of the portfolios dis-covered by our proposed method (see Section IV) (green curve), compared to the baseline performance (red line). In the background, we visualize the composition of the portfolios for each total budget level. We observe that certain algorithms appear consistently in the portfolio structure as the total budget increases, suggesting their general utility across a wide range of budget levels. Interestingly, only 10 algorithms contribute to portfolio for-mation overall, and for any given budget, at most 4 different algorithms are selected. However, some algorithms appear only within narrow budget intervals before disappearing again. While this could partially be attributed to the non-exhaustive nature of our search, it also reflects the nuanced interaction be-tween algorithm behavior and budget allocation. This behavior provides valuable insight into the structure of the problem: it indicates that incrementally building portfolios (by extending a previous configuration as the budget grows) may fail to capture budget-specific dynamics. Certain algorithm–budget 5

combinations are effective only within specific regimes, and a locally greedy or sequential approach may miss such opportu-nities. A more detailed investigation of incremental portfolio construction is presented in Section V-A. An additional observation is that for larger budgets, new algorithms begin to appear in the portfolio. These are often high-performing methods with restart mechanisms, and their repeated inclusion may suggest the potential benefit of tighter restart conditions or more aggressive restarts. Their presence enhances the portfolio’s diversity and adaptability as more budget becomes available. 

Improvement on Individual Function from the Set 

To better understand how the constructed portfolio in-fluences individual function performance, we analyze the scenario where the total budget is fixed at T = 10 ,000 .Specifically, we compare the performance of the portfolio against (i) the best individual algorithm for each function, and (ii) the overall mean performance across all functions. In Figure 3, we report the results of this comparison. We observe that the portfolio does not uniformly outperform the best per-function algorithm. Instead, it strategically bal-ances trade-offs across functions to achieve a stronger overall result. Certain functions (such as F3 and F4) show significant performance gains with the portfolio, while others like F19 ,

F21 , and F22 exhibit moderate improvements. A few functions experience slight performance drops. Nonetheless, the overall average performance increases notably from 0.62 (baseline) to 

0.71 with the constructed portfolio. This effect stems from the complementary structure of the selected portfolio. When examining which algorithms contribute most to the overall performance, quantified via the normalized Shapley values shown in Figure 4, we observe a clear interplay between generalist and specialist strategies. A strong general-purpose algorithm, lq-CMA-ES, serves as the backbone of the portfolio, consistently delivering solid performance across the entire benchmark and closely aligning with the baseline. Around this core, the portfolio integrates specialized algo-rithms that target specific performance gaps:  

> •

BrentSTEPif is highly effective on F3 and F4; 

> •

SLSQP-11-scipy Hansen improves results on F8, F9,

F10 , F19 , F21 , and F22 . 

> •

lmm-CMA-ES contributes strong gains on F15 , F16 , F17 ,and F18 ;This composition demonstrates how the portfolio leverages algorithmic complementarity: rather than relying on one uni-versally strong algorithm, it combines general robustness with targeted specialization to yield better overall performance. While the experiments assume equal importance for all functions and all precision thresholds in the evaluation metric, the proposed methodology is flexible. In practice, performance metrics can be adapted to reflect user-defined priorities, such as emphasizing specific functions or prioritizing tighter precision levels. The portfolio construction strategies proposed in this work remain applicable under such modifications, enabling tailored optimization strategies for a wide range of practical scenarios. IV. M ETHOD 

In the previous section, we demonstrated the advantages of using budget-heterogeneous portfolios for optimizing sets of functions. However, effectively constructing such portfolios is a challenging submodular optimization task. As outlined in Section II-B, this problem is highly complex, involving a large, combinatorial search space with intricate dependencies be-tween algorithm choices, budget allocations, and performance. In this section, we introduce the method that was employed to construct the portfolios presented earlier. We analyze its performance across several controlled scenarios, ranging from simple to more complex settings. This analysis aims to provide a deeper understanding of both the strengths and limitations of the proposed approach. Moreover, we have observed that the optimal portfolio structure is heavily influenced by the total available budget. Solving the problem for a smaller budget does not neces-sarily yield solutions that generalize to larger budgets. This complexity is further amplified by the high dimensionality of the search space, making the problem difficult not only for exact methods but also for iterative heuristic approaches (Appendix A). Algorithms designed for integer domains, such as Genetic Algorithms (Appendix A-A), or for continuous domains, such as CMA-ES (Appendix A-B), often struggle to identify well-performing portfolios in this setting. Moreover, these methods are computationally expensive, as they typically require a large number of evaluations to converge. They are also prone to getting trapped in local optima, especially given the irregular and discontinuous nature of the search space. To better understand the structure and difficulty of the search space, one possible strategy is to perform full enumeration. This approach allows us to assess the distribution and diversity of potential solutions. However, such exhaustive enumeration quickly becomes infeasible in general due to the combinatorial explosion in the number of possible portfolios, which grows exponentially with both the number of algorithms and budget options considered. To conduct this enumeration, we begin by generating all valid sequences of budget values βi ∈⃗ β such that P βi ≤

T , and no additional budget from the set B can be added without exceeding the total budget T . Each of these sequences defines a budget composition of length k. For each such ⃗ β,the number of possible portfolios is given by |A| k, as each budget value can be independently assigned to one of the |A| 

algorithms. This exponential growth illustrates the complexity of the search space, even for relatively small values of T , B,and A.

A. Greedy Local Performance Maximization 

Let us consider an iterative process for constructing aportfolio, where at each iteration we add a new algorithm-budget pair to the current multiset St, resulting in the updated portfolio St+1 = St ⊕ ⟨ α, b ⟩. The goal is to define a selection rule for the next pair to include. A natural greedy strategy is to select the pair that maximizes the performance of the new portfolio: 

max  

> ⟨α, b ⟩∈A×B

J (F, St ⊕ ⟨ α, b ⟩).610                     

> 1
> 0.1
> 0.01
> 0.001
> w
> 1
> 1.5
> 2
> 2.5
> 3
> p
> -147.05 -33.89 26.29 0.00 0.00
> -126.03 -7.20 37.28 0.00 0.00
> -51.17 9.73 39.06 15.84 0.00
> -22.87 28.32 37.09 18.06 0.00
> 12.67 35.08 37.05 18.06 0.00
> −100
> −75
> −50
> −25
> 0
> 25
> 50
> 75
> 100
> Relative
> Improvement (%)

(a) Relative Improvement (%) 10                      

> 1
> 0.1
> 0.01
> 0.001
> w
> 1
> 1.5
> 2
> 2.5
> 3
> p
> 50 46 29 11
> 49 43 13 11
> 47 34 621
> 41 14 521
> 27 9421
> 10
> 20
> 30
> 40
> 50
> Portfolio Size

(b) Portfolio Size 

Fig. 5: Effect of the penalty function π(b) on greedy portfolio construction across different weights w and powers p.However, due to the monotonicity of the performance metric (i.e., adding more budget can never decrease performance), this approach will always prefer selecting the best-performing algorithm with the highest possible budget. As a result, the greedy strategy will prematurely commit to a single algorithm with budget T , effectively preventing any budget splitting or exploration of algorithm diversity. This limitation renders the naive greedy approach ineffective for discovering more balanced or diversified portfolio configurations. As such, we propose a scoring function that includes a penalty term to mildly discourage the repeated use of large budgets, without overly restricting exploration. The proposed score function is defined as: score (t + 1 , ⟨α, b ⟩) = J (F, St ⊕ ⟨ α, b ⟩) − π(b).

where π : N → R is a penalty function applied to the budget component of the candidate pair. In our experiments, we use a simple penalty: 

π(b) = w ·

 bT

p

.

where both p and w are chosen based on a grid search displayed on Figure 5. In Figure 5, we use the same scenario described in Sec-tion IV-B, considering the full set of 24 functions. We observe that a quadratic penalization function with a weight of 0.1 

yields the best performance. This function gently penalizes large budget allocations relative to the total budget T , while also discouraging the use of very small budgets that may be selected prematurely (when their individual contribution is minimal or redundant). This dual effect promotes a more balanced portfolio by avoiding both over-commitment to high-cost pairs and ineffective allocation of budget to low-impact configurations. 

B. Analysis 

In this section, we compare the performance of our proposed method against two baselines: the Single Best Solver (SBS) and a partial enumeration approach. These experiments also help to illustrate the inherent difficulty of the portfolio con-struction problem. We progressively evaluate the effectiveness of our method across increasingly complex scenarios: starting with portfolios tailored to a single function and a single algorithm, then moving to settings with two functions and three algorithms, and finally addressing the most challenging case involving all 24 BBOB functions with either one or three algorithms available. We consider the following experimental setup:  

> •

The set of budgets is defined as B = {200 · k | k ∈

[1 , 50] }; 

> •

The total evaluation budget is T = 10 ,000 ; 

> •

The precision levels are log-normalized and defined as 

E = {10 2− x 

> 10

}x∈0... 100 . 

> •

The functions are from the BBOB suite. We selected three optimization methods that are not neces-sarily the most efficient for each individual function, but rather reflect a more realistic scenario in which practitioners have access to a limited set of standard, widely available algorithms. Specifically, we include two algorithms from the scipy library [20], BFGS [21] and Powell [22], and one from nevergrad [23], CMA-ES [24]. These algorithms are representative of commonly used tools that are easy to deploy when exploring various optimization scenarios. We evaluate the following portfolio building methods:  

> •

Partial enumeration : We exhaustively evaluate all valid portfolios up to a given size. For the case |A| = 1 , we enumerate portfolios of size up to 10 . For |A| = 3 , we limit enumeration to portfolios of size up to 5, as the number of candidate solutions grows exponentially with both portfolio size and the number of algorithms.  

> •

Greedy method : The greedy construction described ear-lier, with a scoring function that includes a mild budget penalty. We will investigate the following scenarios: 

1) Single Function - Single Algorithm 

The first experiment evaluates the difficulty of constructing a portfolio that outperforms the baseline for the simplest setting: a single algorithm ( A = CMA) and a single function ( |F| = 1 )from the BBOB suite. As shown in Figure 6, in most cases, the greedy method successfully finds portfolios that improve over the baseline. Moreover, it even outperforms the partial enumeration approach, which is limited by a maximum port-folio size. This result suggests that high-performing solutions are spread throughout the search space, making it difficult for partial enumeration (especially when constrained in size) to fully capture the optimal configurations. This experiment provides valuable insights into algorithm behavior. Even in the simple case of a single function and a single algorithm, splitting the budget across multiple shorter runs consistently yields better performance. This suggests that the algorithm’s internal restart mechanisms are not sufficiently aggressive (leading to inefficient budget use when runs get trapped in local minima). By distributing the budget across independent runs, the portfolio is better able to explore the search space and avoid stagnation. 

2) Two Functions - Three Algorithms 

Secondly, we investigate the scenario involving pairs of functions (i.e., the simplest function sets of size two) and a set of three algorithms. In Figure 7, we show the average relative 7Partial                          

> Enum
> Greedy
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> Function ID
> 0.0 0.0
> 0.0 0.0
> 20.7 27.9
> 9.9 14.2
> 2.6 -2.6
> 0.0 0.0
> 2.7 -2.0
> 18.8 5.4
> 13.4 7.6
> 0.0 -1.0
> 38.5 67.5
> 46.4 46.4
> 6.9 2.0
> -0.0 -84.3
> 25.0 38.4
> 47.3 57.7
> 46.7 57.3
> 54.7 64.0
> 53.5 48.3
> 11.2 21.7
> 64.8 82.7
> 0.0 0.0
> 7.6 4.6
> 43.8 50.0 100
> 75
> 50
> 25
> 0
> 25
> 50
> 75
> 100
> Relative Improvement (%)

(a) Relative Improvement (%). Partial                           

> Enum
> Greedy
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> Function ID
> 1.0 46.0
> 1.0 38.0
> 10.0 16.0
> 10.0 21.0
> 2.0 2.0
> 1.0 1.0
> 4.0 7.0
> 9.0 4.0
> 10.0 7.0
> 1.0 50.0
> 10.0 48.0
> 10.0 10.0
> 10.0 4.0
> 1.0 29.0
> 10.0 25.0
> 10.0 20.0
> 10.0 16.0
> 10.0 16.0
> 10.0 8.0
> 10.0 24.0
> 10.0 17.0
> 1.0 1.0
> 5.0 11.0
> 10.0 15.0 0
> 10
> 20
> 30
> 40
> 50
> Portfolio Size

(b) Average portfolio size. 

Fig. 6: Portfolio performance and size comparison for the case 

|A| = 1 , |F| = 1 .improvement obtained for all function pairs that include the function indicated on the y-axis. Compared to the single-function case, partial enumeration becomes less effective due to the exponential growth in the number of possible portfolios when more algorithms are involved. More importantly, we observe that the average improvement per pair is often above 30%, highlighting the increasing value of portfolio construction in such settings. This is especially evident when comparing with previous results, suggesting that the benefits of portfolios grow significantly when more algorithms are available. This effect is largely driven by al-gorithm complementarity, having algorithms that specialize in different functions allows for more effective budget splitting. For example, if two algorithms each perform well on one of the two functions, it is often better to divide the budget between them than to commit to just one. Additionally, the observed portfolio sizes reflect not only diversification across algorithms but also internal budget split-ting within each algorithm. This reinforces the insight that multiple independent runs (with varied budget allocations) can outperform single long runs, and further emphasizes the need for stronger internal restart strategies in algorithm design. 

3) All 24 functions - One/Three Algorithms 

Lastly, we consider the most challenging scenario: a func-tion set consisting of all 24 BBOB functions. In Figure 8, we compare the cases of using a single algorithm versus a portfolio of three algorithms. As expected, it becomes increas-ingly difficult to find effective portfolios when using only one algorithm across a diverse set of functions, each exhibiting different behaviors. In contrast, when using three algorithms, we observe improvements of up to 40%, clearly demonstrating the power of algorithm complementarity. Moreover, as the number of functions increases, we also notice a reduction in portfolio sizes. This suggests that, in such diverse settings, allocating the budget across multiple algorithms becomes more critical. However, it also highlights that simply diversifying Partial                          

> Enum
> Greedy
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> Function ID
> 14.7 38.0
> 14.7 39.5
> 13.1 40.8
> 10.7 33.6
> 2.8 56.5
> 0.3 -4.3
> 9.1 31.8
> 11.8 82.0
> 5.6 83.1
> 10.5 59.3
> 18.2 60.5
> 26.3 46.4
> 6.8 56.4
> 5.7 64.1
> 14.3 43.6
> 23.5 54.7
> 23.5 49.1
> 27.7 51.8
> 25.8 48.3
> 12.3 51.9
> 21.9 67.6
> 0.5 30.8
> 11.1 35.5
> 17.6 47.4 100
> 75
> 50
> 25
> 0
> 25
> 50
> 75
> 100
> Relative Improvement (%)

(a) Relative Improvement (%) Partial                           

> Enum
> Greedy
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> Function ID
> 4.0 24.2
> 4.0 20.9
> 4.4 17.3
> 4.3 23.4
> 2.2 24.4
> 1.2 2.6
> 4.1 10.5
> 4.6 12.3
> 4.3 17.6
> 4.0 8.4
> 4.5 11.4
> 4.7 15.9
> 4.3 19.8
> 2.5 16.6
> 4.4 15.0
> 4.6 15.6
> 4.8 14.3
> 4.8 11.7
> 4.5 9.7
> 4.3 16.6
> 4.5 16.6
> 1.3 3.9
> 4.3 12.6
> 4.4 13.0 0
> 10
> 20
> 30
> 40
> 50
> Portfolio Size

(b) Average portfolio size 

Fig. 7: Portfolio performance and size comparison for the case 

|A| = 3 , |F| = 2 , where each function is combined with every other function Partial 

> Enum
> Greedy
> 1 Algos
> 3 Algos

3.0 -2.3 

39.7 39.1 

> 100
> 75
> 50
> 25
> 0
> 25
> 50
> 75
> 100
> Relative
> Improvement(%)

(a) Relative Improvement (%) Partial 

> Enum
> Greedy
> 1 Algos
> 3 Algos

3.0 5.0 

5.0 6.0  

> 0
> 10
> 20
> 30
> 40
> 50
> Portfolio
> Size

(b) Average portfolio size 

Fig. 8: Portfolio performance and size comparison for the case of |F| = 24 .across algorithms is not sufficient: smaller, strategic budget splits within each algorithm (i.e., multiple independent runs with varying budgets) can significantly boost performance. V. A DDITIONAL EXPERIMENTS 

A. Incremental Portfolio Constructions 

Up to this point, we analyzed the portfolio construction problem assuming a fixed total budget. However, another relevant scenario arises when no total budget is predefined, and the goal is to incrementally build better-performing portfolios over time. A natural and straightforward approach in this setting is to define a fixed budget granularity and grow the portfolio step by step. Specifically, we consider a budget set B =

{b, 2b, . . . , T }, where b is the granularity. At each stage Bk =

k · b, we take the previously constructed portfolio for Bk and expand it to Bk+1 by allocating the next b budget units: either to an algorithm already present in the portfolio (reinforcing its use) or to a new algorithm (increasing diversity). In this experiment, we reuse the same setting from Sec-tion III. However, instead of optimizing independently for 82.5 5 7.5 10 12.5 15 17.5 20 

Total Budget/1000 

0.30 

0.40 

0.50 

0.60 

0.70 

0.80    

> Performance
> 0.5 11.5 1.5
> 0.5
> 1.5
> 1
> 2
> 1
> 2
> 1
> 0.5
> 2
> 1
> 1
> 2
> 1
> 1.5
> 2
> 1.5
> 1.5
> 2
> 1.5
> 2
> 2
> 2
> 2
> 2
> 2
> 2
> 0.5
> 2
> 2
> 2
> 1
> 2
> 2
> 2
> 1.5
> 2
> 2
> 2
> 2
> 2
> 2
> 2
> 2.5
> 2.5
> 2
> 2
> 2.5
> 3
> 2
> 2
> 2.5
> 3.5
> 2
> 2
> 2.5
> 4
> 2
> 2
> 2.5
> 4
> 2
> 2
> 2.5
> 0.5
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 1
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 1.5
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 2
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 2.5
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 3
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 3.5
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 4
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 4.5
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 5
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 5.5
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 6
> 4
> 2
> 2
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 6.5
> 4
> 2
> 2.5
> 2.5
> 0.5
> 0.5
> 0.5
> 0.5
> 0.5
> 6.5
> BIPOP-saACM-k
> BrentSTEPrr
> DE-BFGS
> DTS-CMA-ES_005-2pop_v26_1model
> FULLNEWUOA
> SLSQP+lq-CMA-ES

Incremental Portfolio 

Greedy Portfolio 

Budget 

Specific 

SBS 

Incremental Portfolio 

Greedy Portfolio 

Budget 

Specific 

SBS 

0

4

8

12 

16 

20 

> Budget/1000

Fig. 9: Performance comparison between the greedy portfolio, incremental portfolio and baseline algorithm over increasing total budgets, using a subset of COCO’s algorithm set. Background elements represent the composition of the incremental portfolio. We observe that the incremental method fail to find good performing portfolio for small total budgets and it fail to pass the greedy method for higher total budgets each budget level, we grow the portfolio incrementally. In Figure 9, we show the background structure of these evolving portfolios. Overlaid, we compare three performance curves: the baseline (red), the greedy per-budget portfolio (green), and the incrementally constructed portfolio (blue). Compared to the greedy portfolios (Figure 2), we observe that incremental portfolios exhibit much lower algorithmic di-versity, particularly at smaller budgets. They also consistently underperform compared to their greedy counterparts, espe-cially in early stages. This is because, as shown previously, the portfolio composition changes significantly with budget, often in non-smooth or abrupt ways. Thus, incremental strategies may lock into suboptimal trajectories early on, making them unable to adapt to future needs. Moreover, the choice of granularity b plays a critical role. Only algorithms that yield some performance at budget b can enter the portfolio early, effectively filtering out algorithms with long warm-up phases. If b is too small, no algorithm may achieve measurable progress, reducing the process to random selection. Consequently, portfolios built this way are highly sensitive to granularity, and such strategies may miss high-performing solutions that only become competitive at higher budget levels. This behavior further emphasizes the difficulty of con-structing well-performing portfolios. It highlights the strong dependence on the total budget, and shows that portfolios built for smaller budgets do not generalize well to larger budgets, undermining the idea of incrementally extending good solutions. 

B. Influence of the Algorithm Set on Portfolio Performance 

While previous sections focused on a carefully selected set of algorithms, we now aim to evaluate our method in a more realistic and large-scale context. To this end, we 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Overall 

> Number of algorithms
> 50
> 40
> 30
> 20
> 10
> 0
> 10
> 20
> 30
> 40
> 50
> Relative
> Improvement (%)
> Positive improvement
> Negative improvement
> Median
> Mean

Fig. 10: Improvement distribution over 10 random subsets for each number of algorithms ( |A| ∈ [1 , 15] ). Each box shows the median, mean (red dot), and spread of improvements relative to the baseline (SBS). Positive improvements are more frequent and stable as the number of algorithms increases. consider the same COCO dataset and the scenario introduced in Section III. Specifically, we investigate how the composition of the algorithm set A influences the portfolio performance; examining whether, in a fully randomized setting, portfolios still achieve a net positive improvement over the baseline. In this experiment, we randomly sample subsets of algo-rithms of varying sizes ( |A| ∈ [1 , 15] ), drawing multiple sets for each size to ensure statistical robustness. For each sampled set, we apply our greedy portfolio construction method and compare its performance against a simple baseline portfolio. Figure 10 shows the distribution of improvements across these trials. We observe that improvements are easier to obtain as the number of available algorithms increases. On average, our method achieves a relative improvement of 5–10% over 9

the baseline, regardless of algorithm set size. Although these improvements are moderate, the low com-putational cost of the greedy approach makes it particularly attractive in practical settings. This method can serve not only as an efficient standalone strategy, but also as a strong initialization point for more expensive local search heuristics, such as a (1 + 1) Genetic Algorithm seeded with the greedy portfolio. VI. C ONCLUSIONS AND FUTURE WORK 

This work demonstrates the effectiveness of sequential budget-heterogeneous algorithm portfolios as an alternative to traditional single-algorithm selection strategies. Across a range of scenarios (varying in complexity, number of functions, and algorithm availability), we observed consistent gains in performance when using portfolios, particularly due to the complementary strengths of generalist and specialist algo-rithms. However, constructing effective portfolios remains a chal-lenging problem. The underlying search space is combina-torially large and highly sensitive to both the total budget and the structure of the benchmark set. While our greedy method provides a scalable and effective baseline, default black-box optimization techniques often struggle to match its performance, particularly due to local minima and search space fragmentation. A particularly interesting insight is the consistent benefit of distributing the budget across multiple shorter runs of the same algorithm, even in the single-function setting. This indicates sub-optimal designed restart mechanisms and suggests the need for tighter, more adaptive restart strategies in these optimizers. Rather than relying on long, monolithic runs, more granular execution may offer better robustness against premature convergence. Another interesting natural direction is to integrate portfolio methods into algorithm selection frameworks. Unlike standard selection mechanisms that commit to a single algorithm, portfolios can inherently handle uncertainty in function or feature space, potentially offering more robust behavior under uncertainty. Although our portfolios assume sequential and independent evaluation, they open up new directions for more advanced coordination strategies. One promising avenue is to consider knowledge transfer between portfolio components: using the outcome of one algorithm run as a warm start or initialization for the next. This could lead to stronger sequential portfolios, provided care is taken to avoid overfitting or overly narrowing the search space too early. Designing such ”soft chaining” mechanisms, where partial information is shared without full state transfer, could help retain the diversity and robustness properties of portfolios while enhancing performance further. Finally, these methods could be embedded into higher-level automated systems, such as the ngopt wizard in Nevergrad [25], or other AutoML frameworks, offering practical benefits in real-world tuning and adaptation pipelines. REFERENCES [1] J. R. Rice, “The algorithm selection problem,” Adv. Comput. ,vol. 15, pp. 65–118, 1976. [Online]. Available: https://doi.org/10.1016/ S0065-2458(08)60520-3 [2] L. Meunier, H. Rakotoarison, P. K. Wong, B. Roziere, J. Rapin, O. Teytaud, A. Moreau, and C. Doerr, “Black-box optimization revisited: Improving algorithm selection wizards through massive benchmarking,” 

IEEE Transactions on Evolutionary Computation , vol. 26, no. 3, pp. 490–500, 2021. [3] O. Mersmann, B. Bischl, H. Trautmann, M. Preuss, C. Weihs, and G. Rudolph, “Exploratory landscape analysis,” in Proceedings of the 13th annual conference on Genetic and evolutionary computation , 2011, pp. 829–836. [4] G. Cenikj, A. Nikolikj, G. Petelin, N. van Stein, C. Doerr, and T. Ef-timov, “A survey of features used for representing black-box single-objective continuous optimization,” Swarm and Evolutionary Computa-tion , vol. 101, p. 102288, 2026. [5] M. Lindauer, H. Hoos, and F. Hutter, “From sequential algorithm selection to parallel portfolio selection,” in International Conference on Learning and Intelligent Optimization . Springer, 2015, pp. 1–16. [6] L. Xu, F. Hutter, H. H. Hoos, and K. Leyton-Brown, “Satzilla: portfolio-based algorithm selection for sat,” Journal of artificial intelligence research , vol. 32, pp. 565–606, 2008. [7] T. Friedrich, T. K¨ otzing, and M. Wagner, “A generic bet-and-run strategy for speeding up stochastic local search,” in Proceedings of the AAAI Conference on Artificial Intelligence , vol. 31, no. 1, 2017. [8] C. D. Catalin-Viorel Dinu, Diederick Vermetten, “Reproducibility and additional files,” Zenodo , 2026. [9] L. Xu, H. Hoos, and K. Leyton-Brown, “Hydra: Automatically configur-ing algorithms for portfolio-based selection,” in Proceedings of the AAAI Conference on Artificial Intelligence , vol. 24, no. 1, 2010, pp. 210–216. [10] M. Lindauer, H. Hoos, K. Leyton-Brown, and T. Schaub, “Automatic construction of parallel portfolios via algorithm configuration,” Artificial Intelligence , vol. 244, pp. 272–290, 2017. [11] K. Tang, S. Liu, P. Yang, and X. Yao, “Few-shots parallel algorithm port-folio construction via co-evolution,” IEEE Transactions on Evolutionary Computation , vol. 25, no. 3, pp. 595–607, 2021. [12] E. Schede and K. Tierney, “A method for the automated configuration of anytime portfolios of algorithms,” European Journal of Operational Research , 2025. [13] D. Vermetten, H. Wang, K. Sim, and E. Hart, “To switch or not to switch: Predicting the benefit of switching between algorithms based on trajectory features,” in International Conference on the Applications of Evolutionary Computation (Part of EvoStar) . Springer, 2023, pp. 335–350. [14] L. Sch¨ apermeier, “Greedy restart schedules: A baseline for dynamic algorithm selection on numerical black-box optimization problems,” in 

Proceedings of the Genetic and Evolutionary Computation Conference ,2025, pp. 1199–1207. [15] T. Hu, L. Landa, and M.-T. Shing, “The unbounded knapsack prob-lem,” in Research Trends in Combinatorial Optimization: Bonn 2008 .Springer, 2008, pp. 201–217. [16] V. Grunert da Fonseca, C. M. Fonseca, and A. O. Hall, “Inferential performance assessment of stochastic optimisers and the attainment function,” in International Conference on Evolutionary Multi-Criterion Optimization . Springer, 2001, pp. 213–225. [17] M. L´ opez-Ib´ a˜ nez, D. Vermetten, J. Dreo, and C. Doerr, “Using the empirical attainment function for analyzing single-objective black-box optimization algorithms,” IEEE Transactions on Evolutionary Compu-tation , 2024. [18] S. Finck, N. Hansen, R. Ros, and A. Auger, “Real-parameter black-box optimization benchmarking 2009: Presentation of the noiseless functions,” Citeseer, Tech. Rep., 2010. [19] N. Hansen, A. Auger, R. Ros, O. Mersmann, T. Tuˇ sar, and D. Brockhoff, “Coco: A platform for comparing continuous optimizers in a black-box setting,” Optimization Methods and Software , vol. 36, no. 1, pp. 114– 144, 2021. [20] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, ˙ I. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henrik-sen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors, “SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python,” Nature Methods , vol. 17, pp. 261–272, 2020. 10                         

> [21] Y.-H. Dai, “Convergence properties of the bfgs algoritm,” SIAM Journal on Optimization , vol. 13, no. 3, pp. 693–701, 2002. [22] M. J. Powell, “An efficient method for finding the minimum of a function of several variables without calculating derivatives,” The computer journal , vol. 7, no. 2, pp. 155–162, 1964. [23] P. Bennet, C. Doerr, A. Moreau, J. Rapin, F. Teytaud, and O. Tey-taud, “Nevergrad: black-box optimization platform,” Acm Sigevolution ,vol. 14, no. 1, pp. 8–15, 2021. [24] A. Auger and N. Hansen, “Tutorial cma-es: evolution strategies and covariance matrix adaptation,” in Proceedings of the 14th annual con-ference companion on Genetic and evolutionary computation , 2012, pp. 827–848. [25] L. Meunier, H. Rakotoarison, P. Wong, B. Rozi` ere, J. Rapin, O. Teytaud, A. Moreau, and C. Doerr, “Black-box optimization revisited: Improving algorithm selection wizards through massive benchmarking,” IEEE Trans. Evol. Comput. , vol. 26, no. 3, pp. 490–500, 2022. [Online]. Available: https://doi.org/10.1109/TEVC.2021.3108185 [26] J. Paisley, “A simple proof of the stick-breaking construction of the dirichlet process,” Princeton University: Princeton, NJ, USA , 2010. [27] W. Wang and M. A. Carreira-Perpin´ an, “Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application,”
> arXiv preprint arXiv:1309.1541 , 2013.

APPENDIX ATREATING THE PERFORMANCE METRIC AS A BLACK -B OX 

OPTIMIZATION TASK 

In this context, we treat the portfolio optimization task as a black-box optimization problem, where the performance metric J (F, S) acts as an objective function that is expensive to evaluate and lacks analytical gradient information. This setting naturally arises when evaluating the performance of a portfolio requires running multiple stochastic algorithms on a diverse set of benchmark functions. As such, we propose two alternative representations for solutions to the portfolio problem, along with several algo-rithmic strategies designed to operate effectively on these representations. These representations allow us to leverage both discrete and continuous optimization methods to explore the combinatorial space of portfolio configurations under a budget constraint. 

A. Integer Search Space 

To work with a compact and optimization-friendly repre-sentation, we introduce the multiplicity function specific to the multiset S, denoted as: 

μ : A × B → N

This function specifies how many times each pair ⟨α, b ⟩

from the Cartesian product A × B appears in the multiset S.We can equivalently represent this multiset as an integer vector ⃗x ∈ N (|A|·|B| ), where each component corresponds to the multiplicity of a specific algorithm–budget pair: ⃗x = {x ⟨α, b ⟩}⟨α, b ⟩∈A×B , with x⟨α, b ⟩ = μ(⟨α, b ⟩)

Using the integer vector representation introduced earlier, we can express the budget constraint as: 

X

> ⟨α, b ⟩∈A×B

x⟨α, b ⟩b ≤ T

and rewrite the performance function J (F,⃗ x) as: 

1

|F| · |E| 

X 

> f∈F

X

> ε∈E

1 − Y

> ⟨α, b ⟩∈A×B



1 − bϕf,α (b, ε )

x⟨α, b⟩



This representation naturally lends itself to the application of population-based metaheuristics such as Genetic Algo-rithms (GAs). In our implementation, we enforce that all candidate solutions remain valid, i.e., they must satisfy the total budget constraint. After crossover and mutation, any offspring violating the constraint are repaired by decrementing components of ⃗x until feasibility is restored. For the mutation operator , we apply a simple mechanism: with a fixed mutation probability, each gene x⟨α, b ⟩ can be either incremented or decremented by 1, subject to non-negativity. For the crossover operator , any integer-based crossover can be applied. This includes standard operators such as n-point crossover, uniform crossover, or element-wise operations like mean crossover (rounding as necessary to preserve integrality). 

B. Continuous Search Space 

In the continuous search space setting, we first consider how to optimally distribute the total budget among a set of algorithms. One simple and intuitive approach is to assign a fixed proportion of the total budget to each algorithm. For example, allocating 40% to the first algorithm, 50% to the second, and 10% to the third corresponds to the portfolio: 

S∗{(α1, ⌊0.40 · T ⌋) ( α2, ⌊0.50 · T ⌋) ( α3, ⌊0.10 · T ⌋)) }

This representation allows us to explore the entire space of budget allocations among algorithms, enabling exhaustive evaluation of all possible proportional configurations. How-ever, while this proportional representation provides valuable insight into how budget should be divided between algorithms, it overlooks a large portion of the search space. Specifically, scenarios where the budget assigned to an individual algorithm is also split internally (i.e., used at different budget levels). To capture this broader class of solutions, we introduce a constraint on the maximum number of splits allowed per algorithm, denoted k. Each algorithm can thus be assigned up to k different budget levels. We define a continuous solution vector of size k ∗ |A| :⃗x = {x α,i }α∈A ,0≤i<k ,

s.t. X

> α∈A
> k

X

> i=0

xα,i = 1 

Each element xα,i represents a fraction of the total budget to be assigned to the i-th sub-budget level for algorithm α.This representation allows fine-grained encoding of both inter-algorithm and intra-algorithm budget allocation, and can be decoded into valid portfolios accordingly. In order to enable the use of continuous optimizers for solv-ing the portfolio problem, we require a solution representation that satisfies the budget constraint. While this constraint may appear tight, it is possible to transform any continuous solution into a valid one through appropriate normalization. 11 

We consider a raw solution vector ⃗y ∈ [0 , 1] k·|A| , indexed as yα,i , where α ∈ A and i = 1, . . . , k . This vector is then mapped to a normalized vector ⃗x ∈ ∆ k·|A|− 1, indexed similarly as xα,i , using one of the following strategies: 1) Simple normalization :

xα,i = yα,i 

P

> α∈A

Pki=1 yα,i 

This ensures that ⃗x lies on the probability simplex, i.e., all components are non-negative and sum to one. 2) Dirichlet-based stick-breaking transformation [26]: Define D = k · |A| and let ⃗y ∈ [0 , 1] D−1. We compute an intermediate vector ⃗ t ∈ RD−1 as follows: 

t1 = Beta −1(y1; 1 , D )

ti = ti−1 + (1 − ti−1) · Beta −1(yi; 1 , D − i + 1) 

The final normalized components are: 

x1 = t1, xi = ti − ti−1, xD = 1 − tD−1

These values can be reshaped to obtain xα,i as needed. This procedure guarantees that ⃗x ∈ ∆ D−1.3) Euclidean projection onto the simplex [27]: The algorithm proceeds as follows: a) Sort the components of ⃗y in descending order to obtain ⃗u.b) Compute the cumulative sum cssv = Pij=1 uj − 1.c) Find the largest index ρ such that uρ >

> 1
> ρ

Pρj=1 uj − 1



.d) Compute the threshold θ = 1

> ρ

Pρj=1 uj − 1



.e) Set xi = max( yi − θ, 0) .This yields a vector ⃗x that lies on the simplex and is closest to ⃗y in the Euclidean sense. Considering the same scenario as in Section IV-B, we now investigate how the parameter k and the choice of normaliza-tion method affect the performance of the optimizer. As the optimization method, we use the Powell algorithm from SciPy. Figure 11a shows the relative improvement achieved for each combination of k and normalization method, while the Figure 11b reports the corresponding portfolio sizes. We observe that simple normalization becomes unstable as k

increases, leading to volatile performance and unpredictable portfolio sizes. In contrast, Euclidean projection offers consis-tent performance across different values of k, producing stable and compact portfolios. However, the method that strikes the best balance between stability and performance is the Dirichlet-based projection. It consistently yields high performance and remains robust across settings, with the best result obtained at k = 3 . Therefore, for all subsequent continuous optimization experiments, we adopt Dirichlet projection with k = 3 as the default strategy. APPENDIX BFULL ANALYSIS 

In this section, we compare the performance of the various methods introduced in the previous sections. We consider the normalise dirichlet euclidean                     

> 12345678910
> k
> 27.39 25.79 26.11
> 35.03 35.98 34.43
> 29.43 37.18 35.06
> 15.06 35.75 35.63
> 5.44 32.66 33.94
> -19.45 30.56 34.01
> -43.96 28.02 31.65
> -66.54 24.61 34.03
> -86.88 21.91 33.84
> -91.23 19.29 33.61
> −100
> −75
> −50
> −25
> 0
> 25
> 50
> 75
> 100
> Relative Improvement (%)

(a) Relative Improvement (%) normalise dirichlet euclidean                      

> 12345678910
> k
> 3.0 3.0 3.0
> 6.0 6.0 4.7
> 8.4 8.7 5.5
> 10.7 10.9 5.6
> 13.7 12.0 5.8
> 16.0 14.2 5.7
> 18.4 16.2 5.8
> 19.4 18.6 5.5
> 21.4 16.5 5.5
> 22.8 17.0 5.7
> 0
> 10
> 20
> 30
> 40
> 50
> Portfolio Size

(b) Average Portfolio Size 

Fig. 11: Comparison of normalization methods (Simple, Dirichlet, Euclidean) across different values of k.experimental setup from section IV-B and we evaluate the following methods: 1) Parital enumeration : We exhaustively evaluate all valid portfolios up to a given size. For the case |A| = 1 , we enumerate portfolios of size up to 10 . For |A| = 3 , we limit enumeration to portfolios of size up to 5, as the number of candidate solutions grows exponentially with both portfolio size and the number of algorithms. 2) Greedy Portfolio (section IV-A) with a scoring function that includes a mild budget penalty. 3) Incremental Portfolio (section V-A) 4) Genetic algorithms : We consider two evolutionary se-tups: (1 + 1) for local search behavior, and (10 + 20) 

for a population-based strategy. 5) Continuous optimization methods : We use Dirichlet projection with k = 3 as discussed in Section A-B. Two optimizers are tested: Powell (local search) and CMA-ES (population-based). Each heuristic method is allowed a total computational budget of 1000 · |A| fitness evaluations. 

A. Single Function - Single Algorithm 

We begin by analyzing the simplest possible case: a port-folio constructed using only a single algorithm ( |A| = 1 ) and optimized for a single function ( |F| = 1 ). In this setting, we investigate whether splitting the total evaluation budget across multiple shorter runs can outperform allocating the entire budget to a single long run. As shown in Figure 12, even in this minimal configuration, distributing the budget across mul-tiple shorter runs yields significantly better performance. This suggests that internal restart mechanisms in these algorithms are not tight enough, leading to wasted evaluations when a run gets stuck in a poor local optimum. Thus, budget-splitting serves as a simple yet effective way to mitigate stagnation. Figure 12a reveals that the greedy method consistently out-performs other approaches, except for function F14 , where it performs notably worse. Figure 12b further shows that greedy portfolios tend to have larger sizes, reflecting a preference for multiple short runs rather than fewer long ones. In contrast, black-box search methods, such as integer-based Genetic Algorithms and continuous optimization with Dirich-let projections (e.g., CMA-ES, Powell), struggle to compete. 12 Partial                                                                                                                                                  

> Enum
> Greedy
> Incremental
> GA
> (1+1)
> Powell
> (Dirichlet) GA
> (10+20)
> CMA-ES
> (Dirichlet)
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> Function
> 0.0 0.0 0.0 0.0 0.0 0.0 0.0
> 0.0 0.0 0.0 0.0 0.0 0.0 0.0
> 20.7 27.9 31.1 13.6 5.3 13.6 6.2
> 9.9 14.2 9.3 5.8 3.4 5.7 3.3
> 2.6 -2.6 0.0 2.2 -0.1 2.2 1.0
> 0.0 0.0 0.0 0.0 -7.9 0.0 0.0
> 2.7 -2.0 -7.9 0.7 -0.6 0.6 0.2
> 18.8 5.4 0.0 13.0 5.1 13.2 5.4
> 13.4 7.6 0.0 4.6 -0.1 3.6 0.0
> 0.0 -1.0 -1.0 0.0 -0.9 0.0 -0.2
> 38.5 67.5 68.4 39.5 18.1 38.7 18.2
> 46.4 46.4 46.4 36.5 22.0 35.7 22.0
> 6.9 2.0 0.4 2.3 1.5 2.4 2.1
> -0.0 -84.3 -0.0 0.0 -3.6 0.0 0.0
> 25.0 38.4 35.7 18.5 7.7 17.9 8.2
> 47.3 57.7 61.9 40.8 23.2 41.2 23.3
> 46.7 57.3 63.7 41.4 20.9 42.2 20.9
> 54.7 64.0 65.2 48.6 26.5 47.5 26.6
> 53.5 48.3 22.6 43.7 29.7 43.0 29.9
> 11.2 21.7 16.5 6.6 0.8 6.3 2.6
> 64.8 82.7 87.1 49.0 21.8 49.7 23.0
> 0.0 0.0 -19.5 -0.0 -6.4 -0.0 -0.0
> 7.6 4.6 1.3 6.1 5.0 6.2 5.2
> 43.8 50.0 43.8 33.0 14.5 33.0 16.0
> 100
> 75
> 50
> 25
> 0
> 25
> 50
> 75
> 100
> Relative Improvement (%)

(a) Relative Improvement (%). Partial  

> Enum
> Greedy
> Incremental
> GA
> (1+1)
> Powell
> (Dirichlet) GA
> (10+20)
> CMA-ES
> (Dirichlet)
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> Function

1.0 46.0 46.0 6.3 2.9 5.8 2.9 

1.0 38.0 50.0 6.5 2.9 6.0 2.9 

10.0 16.0 17.0 8.0 3.0 8.1 3.0 

10.0 21.0 13.0 6.2 3.0 6.6 3.0 

2.0 2.0 1.0 2.0 2.0 2.0 1.6 

1.0 1.0 1.0 1.0 1.2 1.0 1.0 

4.0 7.0 15.0 3.1 3.0 2.7 2.0 

9.0 4.0 1.0 6.0 3.0 6.4 3.0 

10.0 7.0 1.0 7.0 3.0 6.2 1.8 

1.0 50.0 50.0 1.0 2.4 1.0 1.4 

10.0 48.0 50.0 11.3 3.0 10.9 3.0 

10.0 10.0 10.0 6.5 3.0 6.2 3.0 

10.0 4.0 8.0 3.1 3.0 3.1 2.9 

1.0 29.0 1.0 1.0 1.2 1.0 1.0 

10.0 25.0 17.0 8.1 3.0 7.7 3.0 

10.0 20.0 25.0 8.4 3.0 8.1 3.0 

10.0 16.0 25.0 9.4 3.0 9.5 3.0 

10.0 16.0 17.0 9.5 3.0 8.6 3.0 

10.0 8.0 3.0 7.2 3.0 6.7 3.0 

10.0 24.0 17.0 7.5 3.0 7.3 3.0 

10.0 17.0 24.0 9.1 3.0 9.3 3.0 

1.0 1.0 3.0 1.0 1.7 1.0 1.0 

5.0 11.0 13.0 3.2 2.9 3.4 3.0 

10.0 15.0 10.0 7.8 3.0 7.5 3.0 0 

> 10
> 20
> 30
> 40
> 50
> Portfolio Size

(b) Average portfolio size. 

Fig. 12: Portfolio performance and size comparison for the case |A| = 1 , |F| = 1 .Partial                                                                                                                                                  

> Enum
> Greedy
> Incremental
> GA
> (1+1)
> Powell
> (Dirichlet) GA
> (10+20)
> CMA-ES
> (Dirichlet)
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> Function
> 14.7 38.0 -234.8 39.5 23.2 39.2 32.0
> 14.7 39.5 -1108.1 41.9 25.2 41.8 35.4
> 13.1 40.8 -60.8 35.6 27.7 35.5 33.2
> 10.7 33.6 -57.8 31.9 23.1 32.0 29.9
> 2.8 56.5 40.2 55.6 40.2 55.7 50.2
> 0.3 -4.3 -48.9 1.9 -45.0 2.0 -8.9
> 9.1 31.8 -7.5 33.0 23.9 32.9 32.6
> 11.8 82.0 -249.0 80.2 76.2 80.3 78.7
> 5.6 83.1 -250.2 80.4 72.6 80.4 78.6
> 10.5 59.3 -245.5 51.3 42.6 51.3 43.8
> 18.2 60.5 55.4 50.1 40.4 50.0 41.1
> 26.3 46.4 -1036.3 57.0 46.8 57.0 54.4
> 6.8 56.4 -220.9 53.5 41.4 53.2 46.9
> 5.7 64.1 31.9 54.6 44.1 54.6 48.2
> 14.3 43.6 -79.4 33.9 27.8 33.8 32.4
> 23.5 54.7 6.8 42.7 36.3 42.8 39.2
> 23.5 49.1 -0.8 40.0 38.4 40.0 40.5
> 27.7 51.8 3.3 44.3 42.2 44.4 45.0
> 25.8 48.3 30.8 44.5 39.7 44.5 42.2
> 12.3 51.9 -109.3 43.7 36.6 43.8 40.1
> 21.9 67.6 -16.0 60.3 54.9 60.2 59.4
> 0.5 30.8 68.5 58.6 43.3 58.5 47.3
> 11.1 35.5 -31.6 33.7 21.1 33.8 33.3
> 17.6 47.4 -72.7 35.8 28.2 36.0 32.7
> 100
> 75
> 50
> 25
> 0
> 25
> 50
> 75
> 100
> Relative Improvement (%)

(a) Relative Improvement (%). Partial  

> Enum
> Greedy
> Incremental
> GA
> (1+1)
> Powell
> (Dirichlet) GA
> (10+20)
> CMA-ES
> (Dirichlet)
> 1
> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16
> 17
> 18
> 19
> 20
> 21
> 22
> 23
> 24
> Function

4.0 24.2 20.4 5.5 7.8 5.5 5.5 

4.0 20.9 18.3 5.3 7.8 5.3 5.7 

4.4 17.3 13.4 5.6 7.8 5.6 6.5 

4.3 23.4 13.7 5.5 7.8 5.5 6.2 

2.2 24.4 21.8 5.5 7.9 5.5 6.7 

1.2 2.6 1.6 1.3 3.9 1.3 1.8 

4.1 10.5 11.6 4.1 6.6 4.0 4.4 

4.6 12.3 3.8 5.1 7.9 5.2 5.9 

4.3 17.6 5.3 5.9 8.2 5.7 6.4 

4.0 8.4 18.9 5.7 8.3 5.6 6.9 

4.5 11.4 11.3 6.7 8.3 6.7 7.0 

4.7 15.9 11.9 4.3 7.4 4.3 4.9 

4.3 19.8 15.9 5.8 8.1 5.6 7.1 

2.5 16.6 21.3 6.2 8.3 6.1 7.2 

4.4 15.0 13.9 6.1 8.2 6.1 6.8 

4.6 15.6 18.0 6.1 8.2 6.2 6.0 

4.8 14.3 19.7 6.3 8.3 6.3 7.2 

4.8 11.7 13.8 6.3 8.2 6.3 7.0 

4.5 9.7 6.3 6.0 8.2 6.1 6.4 

4.3 16.6 13.3 5.9 8.5 6.0 7.3 

4.5 16.6 19.0 6.1 8.4 6.1 7.2 

1.3 3.9 10.1 6.4 8.0 6.4 6.2 

4.3 12.6 10.9 4.3 7.5 4.3 4.8 

4.4 13.0 10.2 6.0 8.0 6.0 6.0 0 

> 10
> 20
> 30
> 40
> 50
> Portfolio Size

(b) Average portfolio size. 

Fig. 13: Portfolio performance and size comparison for the case |A| = 3 , |F| = 2 .While they avoid major failures like those seen for greedy on 

F14 or incremental search on F22 , they tend to explore only a limited part of the search space. This is reflected in the smaller portfolio sizes they generate, often failing to identify longer or more diverse portfolios. 

B. Two Functions - Three Algorithms 

The second experiment explores a more complex scenario, involving three algorithms ( |A| = 3 ) and two functions ( |F| =2). In Figure 13a, we observe how various methods perform under this increased complexity. One immediate observation is that the incremental method struggles even more to find competitive portfolios in this setting. In contrast, black-box methods (e.g., Genetic Algorithms and continuous optimiza-tion with Dirichlet projections) become more competitive. This is also reflected in the portfolio sizes shown in Figure 13b, Partial 

Enum 

Greedy 

Incremental 

GA 

(1+1) 

Powell 

(Dirichlet) GA 

(10+20) 

CMA-ES 

(Dirichlet) 

1 Algos 

3 Algos 

3.0 -2.3 3.0 2.1 1.7 2.4 2.1 

39.7 39.1 40.3 34.6 37.2 35.2 39.5 

100 

50 

0

50 

100 

> Relative
> Improvement(%)

(a) Relative Improvement (%) Partial 

Enum 

Greedy 

Incremental 

GA 

(1+1) 

Powell 

(Dirichlet) GA 

(10+20) 

CMA-ES 

(Dirichlet) 

1 Algos 

3 Algos 

3.0 5.0 3.0 2.6 3.0 2.9 3.0 

5.0 6.0 6.0 5.3 8.9 6.3 6.8 

0

10 

20 

30 

40 

50 

> Portfolio
> Size

(b) Average Portfolio Size 

Fig. 14: Portfolio performance and size comparison for the case |F| = 24 .where the greedy method no longer dominates in portfolio length, the sizes are now much closer across methods. Importantly, we also observe a greater overall performance improvement compared to the single-function case. This sug-gests that algorithm complementarity starts to play a stronger role: different algorithms specialize in different functions, and the ability to exploit this diversity leads to more effective portfolios. 

C. All 24 Functions - One/Three Algorithms 

For the final set of experiments, we consider the full set of 24 BBOB functions and compare two scenarios: using a single algorithm ( |A| = 1 ) versus using three algorithms (|A| = 3 ). The main observation is that, when limited to a single algorithm, all methods struggle to construct high-performing portfolios, the diversity of functions makes it difficult to generalize effectively. However, when multiple algorithms are available, the inter-play between them enables significantly better performance, as different algorithms can complement each other across the diverse set of functions. In this case, we see that black-box methods approach the performance of the greedy strategy, a trend also reflected in the similarity in portfolio sizes across methods. Even though the average portfolio size decreases as more functions are added, we still observe that splitting the bud-get across multiple independent runs within each algorithm remains beneficial for achieving strong performance.