<div class="paper-title-row">
<h1 class="paper-title-zh">DeepFedNAS：一个规范化、硬件感知且无预测器的联邦神经架构搜索统一框架</h1>
<h1 class="paper-title-en">DeepFedNAS: A Unified Framework for Principled, Hardware-Aware, and Predictor-Free Federated Neural Architecture Search</h1>
</div>

<div class="paper-meta-row">
<div class="paper-meta-left">
<p><strong>Authors</strong>: Bostan Khan, Masoud Daneshtalab</p>
<p><strong>Date</strong>: 2026-01-21</p>
<p><strong>PDF</strong>: <a href="https://arxiv.org/pdf/2601.15127v1" target="_blank">https://arxiv.org/pdf/2601.15127v1</a></p>
<p><strong>Tags</strong>: <span class="tag-label tag-green">EOH</span> <span class="tag-label tag-green">EAA</span></p>
<p><strong>Score</strong>: 7.0</p>
</div>
<div class="paper-meta-right">
<p><strong>Evidence</strong>: 架构启发式与自动模型设计</p>
<p><strong>TLDR</strong>: DeepFedNAS利用架构启发式和多目标适应度函数实现自动模型设计。</p>
</div>
</div>

<div class="paper-glance-section">
<h2 class="paper-glance-title">速览</h2>
<div class="paper-glance-row">
<div class="paper-glance-col">
<div class="paper-glance-label">Motivation</div>
<div class="paper-glance-content">现有的联邦神经架构搜索面临超网络训练缺乏引导导致模型次优，以及训练后子网搜索过程耗时过长（需数小时）的瓶颈。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Method</div>
<div class="paper-glance-content">采用两阶段法：先利用预计算的帕累托最优架构缓存引导超网络训练，再利用适应度函数作为零成本代理，实现无需精度预测器的秒级子网发现。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Result</div>
<div class="paper-glance-content">在 CIFAR-100 上精度提升达 1.21%，搜索流水线总耗时缩短约 61 倍，单次子网搜索仅需 20 秒。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Conclusion</div>
<div class="paper-glance-content">该框架通过消除昂贵的精度预测器并优化训练流程，使硬件感知的联邦学习模型部署变得即时且实用。</div>
</div>
</div>
<div class="paper-glance-tldr"><strong>TLDR</strong>：DeepFedNAS 是一个高效的联邦神经架构搜索框架，通过多目标适应度函数和帕累托最优课程学习，实现了硬件感知且无需预测器的极速子网搜索。</div>
</div>

---


## 摘要
联邦神经架构搜索 (FedNAS) 旨在为隐私保护的联邦学习 (FL) 自动化模型设计，但目前面临两个关键瓶颈：导致次优模型的无指导超网训练，以及训练后子网发现所需的高昂且耗时数小时的流水线。我们提出了 DeepFedNAS，这是一个新颖的两阶段框架，由一个规范的多目标适应度函数支撑，该函数综合了数学网络设计与架构启发式方法。通过重新设计的超网，DeepFedNAS 引入了联邦帕累托最优超网训练，利用预先计算的高适应度架构帕累托最优缓存作为智能课程来优化共享超网权重。随后，其无预测器搜索方法通过将该适应度函数作为准确率的直接、零成本代理，消除了对昂贵准确率代理模型的需求，从而实现了仅需数秒的按需子网发现。DeepFedNAS 实现了最先进的准确率（例如，在 CIFAR-100 上绝对提升高达 1.21%）、卓越的参数和通信效率，以及训练后搜索流水线总时间约 61 倍的大幅加速。通过将流水线时间从 20 多小时缩短至约 20 分钟（包括初始缓存生成），并实现 20 秒的单次子网搜索，DeepFedNAS 使硬件感知的联邦学习部署变得即时且实用。完整的源代码和实验脚本可在以下网址获取：https://github.com/bostankhan6/DeepFedNAS。

## 速览
**TLDR**：DeepFedNAS 是一个针对联邦神经架构搜索（FedNAS）的统一框架，旨在解决超网络训练不佳和子网搜索耗时过长的问题。它通过多目标适应度函数和帕累托最优缓存引导超网络训练，并利用该函数作为零成本代理实现无预测器的快速搜索。该框架在提升准确率的同时，将搜索流程缩短了约61倍，使硬件感知的联邦学习部署更加高效实用。 \
**Motivation**：现有的 FedNAS 方法面临超网络训练缺乏引导导致模型次优，以及训练后子网发现过程耗时巨大的瓶颈。 \
**Method**：提出联邦帕累托最优超网络训练和无预测器搜索方法，利用预计算的架构缓存作为课程学习，并以适应度函数直接作为准确率代理。 \
**Result**：在 CIFAR-100 上准确率提升达 1.21%，搜索流程提速约 61 倍，单次子网搜索仅需 20 秒。 \
**Conclusion**：DeepFedNAS 通过原理化的设计显著提升了 FedNAS 的效率和性能，为硬件感知的联邦学习提供了即时且实用的部署方案。

---

## Abstract
Federated Neural Architecture Search (FedNAS) aims to automate model design for privacy-preserving Federated Learning (FL) but currently faces two critical bottlenecks: unguided supernet training that yields suboptimal models, and costly multi-hour pipelines for post-training subnet discovery. We introduce DeepFedNAS, a novel, two-phase framework underpinned by a principled, multi-objective fitness function that synthesizes mathematical network design with architectural heuristics. Enabled by a re-engineered supernet, DeepFedNAS introduces Federated Pareto Optimal Supernet Training, which leverages a pre-computed Pareto-optimal cache of high-fitness architectures as an intelligent curriculum to optimize shared supernet weights. Subsequently, its Predictor-Free Search Method eliminates the need for costly accuracy surrogates by utilizing this fitness function as a direct, zero-cost proxy for accuracy, enabling on-demand subnet discovery in mere seconds. DeepFedNAS achieves state-of-the-art accuracy (e.g., up to 1.21% absolute improvement on CIFAR-100), superior parameter and communication efficiency, and a substantial ~61x speedup in total post-training search pipeline time. By reducing the pipeline from over 20 hours to approximately 20 minutes (including initial cache generation) and enabling 20-second individual subnet searches, DeepFedNAS makes hardware-aware FL deployments instantaneous and practical. The complete source code and experimental scripts are available at: https://github.com/bostankhan6/DeepFedNAS