<div class="paper-title-row">
<h1 class="paper-title-zh">具有混合变量类型的基于集成的迁移学习贝叶斯优化的实证研究</h1>
<h1 class="paper-title-en">An Empirical Study on Ensemble-Based Transfer Learning Bayesian Optimisation with Mixed Variable Types</h1>
</div>

<div class="paper-meta-row">
<div class="paper-meta-left">
<p><strong>Authors</strong>: Natasha Trinkle, Huong Ha, Jeffrey Chan</p>
<p><strong>Date</strong>: 2026-01-22</p>
<p><strong>PDF</strong>: <a href="https://arxiv.org/pdf/2601.15640v1" target="_blank">https://arxiv.org/pdf/2601.15640v1</a></p>
<p><strong>Tags</strong>: <span class="tag-label tag-green">EAA</span></p>
<p><strong>Score</strong>: 6.0</p>
</div>
<div class="paper-meta-right">
<p><strong>Evidence</strong>: 高效自动算法选择与优化迁移学习</p>
<p><strong>TLDR</strong>: 本研究针对昂贵的黑盒目标函数，对基于集成的迁移学习贝叶斯优化（TL-BO）进行了深入的实证分析。通过利用相关问题的历史数据，研究探讨了多种流水线组件，并提出了基于正约束正则化回归的集成代理模型加权策略，以及处理迁移学习失效的机制。此外，研究还贡献了三个新的实时TL-BO基准测试。结果表明，热启动初始化和正权重约束是提升TL-BO性能的关键因素。</p>
</div>
</div>

<div class="paper-glance-section">
<h2 class="paper-glance-title">速览</h2>
<div class="paper-glance-row">
<div class="paper-glance-col">
<div class="paper-glance-label">Motivation</div>
<div class="paper-glance-content">旨在利用相关历史数据来提升贝叶斯优化在处理昂贵黑盒函数时的样本效率和性能。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Method</div>
<div class="paper-glance-content">对集成式迁移学习贝叶斯优化组件进行实证研究，并提出一种带正约束的正则化回归加权策略及新的基准测试集。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Result</div>
<div class="paper-glance-content">实验发现热启动初始化和对集成代理模型权重施加正约束能显著改善迁移学习贝叶斯优化的表现。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Conclusion</div>
<div class="paper-glance-content">优化流水线组件（如权重约束和初始化方式）对于实现高效且稳健的迁移学习贝叶斯优化至关重要。</div>
</div>
</div>
</div>

---


## 摘要
贝叶斯优化是一种用于寻找高代价黑盒目标函数全局最优解的样本高效方法。通过将迁移学习方法应用于贝叶斯优化流水线的各个组件，可以利用来自相关问题的历史数据集来提高贝叶斯优化的性能。在本研究中，我们对各种基于集成的迁移学习贝叶斯优化方法和流水线组件进行了实证分析。我们通过贡献一些特定的流水线组件以及三个新的实时迁移学习贝叶斯优化基准，扩展了文献中的先前工作。特别地，我们提出了一种基于正则化回归的集成代理模型预测加权策略，其中权重被约束为正值，并提出了一个相关组件用于处理迁移学习未能提升贝叶斯优化性能的情况。我们发现，通常情况下，有助于提高迁移学习贝叶斯优化性能的两个组件是热启动初始化以及将集成代理模型所使用的权重约束为正值。
## Abstract
Bayesian optimisation is a sample efficient method for finding a global optimum of expensive black-box objective functions. Historic datasets from related problems can be exploited to help improve performance of Bayesian optimisation by adapting transfer learning methods to various components of the Bayesian optimisation pipeline. In this study we perform an empirical analysis of various ensemble-based transfer learning Bayesian optimisation methods and pipeline components. We expand on previous work in the literature by contributing some specific pipeline components, and three new real-time transfer learning Bayesian optimisation benchmarks. In particular we propose to use a weighting strategy for ensemble surrogate model predictions based on regularised regression with weights constrained to be positive, and a related component for handling the case when transfer learning is not improving Bayesian optimisation performance. We find that in general, two components that help improve transfer learning Bayesian optimisation performance are warm start initialisation and constraining weights used with ensemble surrogate model to be positive.