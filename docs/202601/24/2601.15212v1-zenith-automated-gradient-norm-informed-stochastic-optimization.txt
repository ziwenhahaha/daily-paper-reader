Title: ZENITH: Automated Gradient Norm Informed Stochastic Optimization

URL Source: https://arxiv.org/pdf/2601.15212v1

Published Time: Thu, 22 Jan 2026 02:18:58 GMT

Number of Pages: 13

Markdown Content:
# ZENITH: Automated Gradient Norm Informed Stochastic Optimization 

Dhrubo Saha 1

# Abstract 

Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automati-cally, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we intro-duce the ZENITH (Zero-overhead Evolution us-ing Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolu-tion of the gradient norm. Image classification experiments spanning 6 CNN architectures and 6 benchmarks demonstrate that ZENITH achieves higher test accuracy in lower wall-clock time than baselines. It also yielded superior mAP in object detection, keypoint detection, and instance seg-mentation on MS COCO using the R-CNN family of models. Furthermore, its compatibility with regularization enables even better generalization. 

# 1. Introduction 

Modern computer vision relies on deep learning models for classification, detection, and segmentation. These models are optimized via gradient descent, which requires an appro-priate initial learning rate (LR) and manual adjustment of this LR throughout training. Such hyperparameter choices influence both the model’s final performance and the wall-clock time needed for convergence. This tuning challenge is exacerbated by the current trend of training large-scale mod-els on massive datasets. This is because the longer training durations make the manual monitoring and scheduling of LRs more labor-intensive. To alleviate the burden of manual oversight, practitioners employ predefined LR schedules like exponential (Szegedy et al., 2016), polynomial (Chen et al., 2017), and step (Ge et al., 2019) decay, as well as cosine annealing (Loshchilov & Hutter, 2016). These schedules adapt the LR via fixed 

> 1Department of Computer Science, Georgia Institute of Tech-nology, Atlanta, Georgia, USA. Correspondence to: Dhrubo Saha
> <dsaha36@gatech.edu >.
> Preprint. January 22, 2026.

intervals or continuous functions. However, they often prove impractical or suboptimal because the required number of iterations or optimal decay rate are not known in advance. Consequently, identifying the optimal decay rate becomes a costly trial-and-error process that must be repeated for every new task. To address this challenge, researchers have been developing automatic and parameter-free optimizers, which dynamically adjust the LR throughout training. 

1.1. Related Works Coin Betting. One of the first automatic schedulers was CO-COB, which treats weight updates as coin bets and derives the LR from the accumulated reward from previous itera-tions (Orabona & Tommasi, 2017). If the gradients point in a consistent direction, the LR increases to accelerate conver-gence. Conversely, if the gradients exhibit high stochastic noise, the LR remains low to promote stability. Unfortu-nately, noisy gradients are common in mini-batch training, which causes the algorithm to underestimate the ideal LR, delaying convergence. Moreover, COCOB needs 6 times as much memory as vanilla SGD because, in addition to the model weights, it has to store each weight’s maximum observed gradient, sum of absolute gradients, accumulated reward, sum of gradients, and initial betting states. 

Quadratic Loss Approximation. Quadratic approximation methods by Mutschler & Zell (2020) and Fu & Wu (2024) assume that the local loss landscape along the gradient di-rection can be modeled as a quadratic function. In each iteration, the algorithm computes the loss and gradients at the current weights. Then, it takes a probing step in the descent direction and evaluates the loss at this second point. Using the acquired information, it derives the equation for the parabola to locate its vertex, and reaches the vertex in a single step. However, this process introduces a new hy-perparameter, the probing step size, which may need to be tuned in lieu of the LR. Furthermore, the probing mecha-nism needs 1 extra forward pass per iteration (or even 2 in curvature-estimating extensions by Zhu et al. (2021)), which inflates the wall-clock training time. There is also a substantial memory footprint for storing additional copies of the weights. Although Bu & Xu (2024) proposed mitigating these burdens by performing approximations only every 4–8 iterations, this compromises the algorithm’s efficacy. 1

> arXiv:2601.15212v1 [cs.LG] 21 Jan 2026 ZENITH: Automated Gradient Norm Informed Stochastic Optimization

Polyak-Style Interpolation. Polyak-style methods assume that over-parameterized models can achieve near-zero train-ing loss. ALIG (Berrada et al., 2020) and SPS (Loizou et al., 2021) adopt the classical Polyak step size by Polyak (1987). Hence, they set the LR equal to the ratio of the scalar loss to the squared gradient norm ( ηt = L(θt) 

> ||∇L (θt)|| 2

). The LR is large during the high-loss initial phase and diminishes as the loss diminishes. However, this mechanism is sensitive to the absolute scale of the loss function. Factors like the choice of loss function (e.g., Cross-Entropy vs. MSE) and the mag-nitude of regularization can skew this ratio across different tasks. Consequently, the derived LR may become too small, leading to sluggish convergence. Conversely, the LR can become too large, hitting its clipping bound and reverting to constant-LR SGD. Although the L4 method (Rolinek & Martius, 2018) tried to mitigate this instability by scaling the Polyak step size with a reduction factor ( α), tuning α

can be as costly as tuning the LR schedule in vanilla SGD. 

Distance-Aware Estimation. DoG (Ivgi et al., 2023) and DoWG (Khaled et al., 2023) derive the LR by normaliz-ing the distance traveled from initialization by the accumu-lated gradient norms. Similarly, D-Adaptation (Defazio & Mishchenko, 2023) and Prodigy (Mishchenko & Defazio, 2023) estimate the distance to the solution through mono-tonic lower-bound estimations. Unlike SGD, which allows each weight update to be computed independently, these distance-aware methods depend on aggregate statistics that must be calculated globally. This increases the per-iteration wall-clock time, delaying convergence. This delay is exacer-bated by the tendency to yield excessively low LRs in some scenarios. Conversely, excessively high LRs are computed when using regularization because the algorithm mistakes the pull of regularization for evidence that the solution is much farther away. Lastly, these methods are memory-intensive. For instance, D-Adaptation requires 4 times the memory of vanilla SGD, as it must store the current weights, initial weights, and the s and z buffers. 

Overall Issues. Collectively, existing automatic schedulers suffer from several limitations. Firstly, they incur extra computational overhead per iteration, which inflates the wall-clock training time. While prior works solely focus on convergence in terms of iteration counts, wall-clock time is the truly important metric. Secondly, most methods are memory-intensive, requiring multiple auxiliary buffers that scale aggressively with model size. Thirdly, some algo-rithms yield very low LRs, lengthening the training process. Fourthly, some schedulers are sensitive to the absolute scale of the loss function, which limits robustness across tasks, datasets, and model architectures. Fifthly, some methods introduce new hyperparameters that merely shift the tuning burden of the LR schedule rather than eliminating it. Sixthly, some are incompatible with regularization, which is often indispensable for better generalization. Lastly, even when these optimizers achieve near-zero training loss, they often generalize poorly to unseen data. However, the reasons for this problem have remained opaque. 

1.2. Our Contribution 

During training, a quantity that indicates our position in the loss landscape is the gradient norm. In early training stages, this norm is large. It decreases as the optimizer descends into a basin and it becomes tiny near the minimum. This gradient norm has been utilized by Polyak-style optimiz-ers to scale the training loss, whereby the LR is inversely related to the gradient magnitude. In contrast, we intro-duce the ZENITH optimizer, which implements a positive relationship between the LR and the gradient norm rela-tive to the norm’s historical peak or zenith . This ensures the optimizer maintains a high LR to escape local minima and descend into a flatter loss basin during the early train-ing stages. As the gradient norm attenuates, the LR drops commensurately, facilitating stable convergence to a min-imum. This behavior mimics well-known schedules like polynomial decay, exponential decay, and cosine annealing. However, these schedules require prior knowledge of the total iteration count or optimal decay rate, which are sel-dom available in practice. Therefore, these parameters need to be tuned. In comparison, ZENITH automates the decay process by using the evolving signal from the gradient norm. Since gradient computation is an inherent requirement of backpropagation, calculating the resulting L2 norm intro-duces a very small computational overhead. ZENITH im-poses no additional memory overhead because it doesn’t create auxiliary state buffers for each model weight. With-out tuning any hyperparameters, it yields stronger test per-formance than baselines in less wall-clock time. Because ZENITH defines the LR as a normalized fraction of its his-torical zenith, it is invariant to the absolute scale of the loss function. This makes it robust across diverse model archi-tectures and datasets, and compatible with regularization. 

# 2. Methodology 

The algorithm maintains a sliding window of the W most recent gradient L2 norms to compute a smoothed local es-timate of the loss landscape’s steepness. Let Gt denote the norm at training iteration t. The algorithm maintains a First-In-First-Out queue, denoted as Q, with a fixed capacity W .At each iteration, the current norm Gt is appended to Q, and if the size of Q exceeds W , the oldest element is removed. Once the window is fully populated, we compute the rolling mean, μt, of the values in Q:

μt = 1

W 

> W−1

X

> k=0

Q[k] (1) 2ZENITH: Automated Gradient Norm Informed Stochastic Optimization 

The algorithm tracks the historical maximum (or zenith) of this rolling mean, denoted by Zt. Zt serves as a reference point for the highest steepness observed during training and is updated monotonically: 

Zt = max( Zt−1, μ t) (2) The LR ηt is then scaled at each iteration based on the ratio of the current local steepness to the historical zenith, so that the LR anneals as the gradients attenuate relative to their peak magnitude. The update rule is defined as: 

ηt = η0 ·

 μt

Zt



(3) where η0 is the initial LR. η0 and W are the only parameters, and their insensitivity to tuning will be elaborated upon later. The algorithm’s pseudocode is detailed in Appendix A, and a theoretical convergence analysis is provided in Appendix B. Table 1 summarizes the computational time and space over-heads of prior automatic optimizers and ZENITH. To assess the computational time overhead, the wall-clock time per iteration was benchmarked using a VGG-19 model trained on CIFAR-10 for 39063 iterations. The table indicates that ZENITH incurs only a 1% to 2% time penalty relative to vanilla SGD, whereas competing algorithms increase iter-ation time by 15% to 373% . This is because ZENITH’s only computationally significant operation beyond SGD is the calculation of the gradient L2 norm. Furthermore, prior methods often require substantial memory space buffers of up to 5× the model size. Conversely, ZENITH incurs zero per-weight memory overhead, as it only tracks fixed-size scalar statistics that are independent of model dimensions.  

> Table 1. Comparison of computational time and space overheads.

Method Time Space ms / iter Extra (%) (×M odel )

SGD 17.24 0.00 0PAL 25.86 49.99 1LQA 81.45 372.3 5GeN 22.29 29.25 1COCOB 25.95 50.49 5DoG 19.98 15.86 1DoWG 22.06 27.91 1DAdapt 24.16 40.12 3Prodigy 29.14 68.97 4L4 31.31 81.55 2ALIG 20.47 18.72 0

SPS 20.44 18.56 0ZENITH 17.47 1.337 0

# 3. Experiments on Image Classification 

3.1. Experimental Setup 

We conducted image classification experiments across 6 combinations of datasets (LeCun et al., 2002; Deng et al., 2009; Krizhevsky et al., 2009; Bossard et al., 2014; Le & Yang, 2015) and model architectures (Simonyan & Zisser-man, 2014; He et al., 2016; Szegedy et al., 2016; Huang et al., 2017; Yu et al., 2018; Tan & Le, 2019), as detailed in Table 2. Data augmentation was used, including random cropping, horizontal flipping, translations, and rotations. Cross-entropy loss was utilized. Models were trained using a batch size of 128 on an NVIDIA A100 (40 GB) GPU with six data loading workers. Training durations were set for each dataset-architecture pair to ensure sufficient time for baselines to converge (see Table 2). Performance was moni-tored via test accuracy evaluated at the conclusion of each epoch, which is a standard procedure in this research area. Cumulative wall-clock time was recorded for the training phase exclusively, with the testing time being omitted. L2

regularization was not used because some baselines perform poorly with regularization. ZENITH was evaluated against 11 of the 12 baselines dis-cussed in Section 1.1. Due to the absence of an open-source implementation, only QLABGRAD was omitted. All of these methods were recently published in the NeurIPS, ICML, ICLR, AAAI, and AISTATS conferences, and the journal Neural Networks. For optimizers requiring an ini-tial and/or maximum LR (the quadratic, Polyak-style, and ZENITH optimizers), these values were fixed at 0.1. This is because 0.1 is a standard value for image classification us-ing SGD. For other optimizers, we used their recommended initial LR configurations because their performance deterio-rates when forced to use a specific initial LR. Regarding all non-LR parameters of baselines, the recommended config-urations for image classification were used, as specified in the publications and their GitHub repositories. ZENITH’s only non-LR parameter, W , was fixed at 500 throughout all experiments. The PyTorch and TensorFlow implementa-tions of ZENITH, as well as Python notebooks to reproduce the experiments, will be made available upon acceptance.  

> Table 2. Experimental setup for image classification benchmarks.

Exp Dataset Model Time (hr) 

(a) MNIST EfficientNet 1(b) CIFAR-10 VGG19 1(c) CIFAR-100 ResNet50 1(d) Food-101 DLA 5(e) Tiny ImageNet Inception 5(f) ImageNet-100 DenseNet121 10 3ZENITH: Automated Gradient Norm Informed Stochastic Optimization 

Figure 1. Training loss and test accuracy curves against wall-clock time for the 6 image classification experiments. The color-coding for each optimizer is as follows: ALIG , COCOB , D-Adaptation , DoG , DoWG , GeN , L4 , LQA , PAL , Prodigy , SPS , and ZENITH . This color scheme is consistent across all other figures as well. Training loss magnitudes in this figure are scaled by a factor of 10 or 100 for visual clarity. This linear transformation preserves the original curve profiles without any distortions. The loss function was not scaled during the actual training process. Additionally, all curves are smoothed using a simple moving average for visual clarity. 

Table 3. Comparison of test accuracy at convergence and the requisite wall-clock time to achieve it. The best results are in bold green. 

Method MNIST CIFAR-10 CIFAR-100 Food-101 Tiny ImageNet ImageNet-100 Acc (%) Time (min) Acc (%) Time (min) Acc (%) Time (min) Acc (%) Time (hr) Acc (%) Time (hr) Acc (%) Time (hr) PAL 99.51 13.1 91.0 15.0 69.0 23.5 68.4 0.85 57.7 2.41 72.3 3.12 LQA 99.60 58.0 89.0 32.4 65.2 59.1 73.3 2.69 59.2 4.40 1.00 0.20 GeN 99.21 2.24 77.7 30.8 44.0 59.5 44.9 4.97 32.6 4.94 55.0 9.71 COCOB 99.56 4.05 89.9 14.4 64.8 40.9 64.3 1.58 42.7 3.51 68.1 2.83 DoG 99.50 14.0 85.6 31.1 22.8 59.8 67.8 2.86 56.7 4.54 76.4 6.70 DoWG 99.41 3.13 92.4 18.1 70.0 29.8 68.1 1.39 53.3 4.01 76.1 7.21 DAdapt 99.58 6.25 89.6 23.6 69.2 57.1 72.5 2.32 59.6 4.73 76.9 5.59 Prodigy 99.55 4.59 89.3 24.3 67.3 48.3 72.0 2.04 59.2 4.43 78.0 5.63 L4 99.45 7.79 83.5 29.4 57.9 59.1 0.99 0.03 0.61 0.13 68.1 4.69 ALIG 99.48 10.8 85.3 20.1 67.2 16.8 72.3 0.68 59.0 1.56 75.1 1.75 

SPS 99.39 5.45 87.7 8.93 69.8 22.7 73.9 1.33 58.8 2.35 76.6 1.88 ZENITH 99.57 4.39 92.4 13.1 73.8 22.4 75.3 1.40 60.6 2.44 78.2 4.84 

4ZENITH: Automated Gradient Norm Informed Stochastic Optimization    

> Figure 2. (a) Evolution of the L2gradient norm and (b) the corresponding LR used by ZENITH across iterations.

3.2. Experimental Results and Discussion 

Figure 1 compares the training loss and test accuracy evo-lutions of all optimizers as a function of wall-clock time, illustrating ZENITH’s faster convergence or higher peak accuracy. Furthermore, Table 3 demonstrates that ZENITH outperforms baselines across the six experiments when eval-uating the joint objectives of maximizing test accuracy and minimizing the wall-clock time to converge. On MNIST, CIFAR-10, and CIFAR-100, ZENITH is superior to base-lines on both objectives. While ALIG and SPS achieve faster convergence on Food-101, Tiny-ImageNet, and ImageNet-100, this premature convergence compromises their test accuracy. No baseline across any experiment outperformed ZENITH on both objectives simultaneously. 

ZENITH’s LR Scheduling Mechanism. Figure 2(a-b) displays the evolution of L2 gradient norm and the corre-sponding LR adaptation of ZENITH. Figure 2a shows that the gradient norm may initially increase or plateau for some time during the early phase of entering a loss basin. During this period, Figure 2b shows that ZENITH maintains a high LR. As the optimizer approaches a minimum, the gradi-ent norm diminishes. ZENITH uses this signal to perform commensurate LR decay, stabilizing the convergence pro-cess. As the gradient norm approaches 0, the LR decreases concurrently, enabling precise convergence. Interestingly, the LR trajectories in Figure 2b mirror well-known manual schedules. The MNIST trajectory resembles an exponential decay, while the CIFAR-10 and ImageNet-100 trajectories resemble polynomial (linear) decay. The remaining three resemble cosine annealing, where the LR initially reduces slowly, then undergoes a phase of faster decay, and finally stabilizes near a lower bound. However, using these manual schedules requires prior knowledge of the total iteration count or ideal decay rate, which is problematic because these parameters vary greatly across configurations. For example, while both CIFAR-10 and ImageNet-100 exhibit linear decay patterns in Figure 2b, ImageNet-100 needs over twice as many iterations. ZENITH resolves this issue by modulating the LR using gradient signals. This smart scheduling, together with the lower per-iteration wall-clock times shown in Table 1, reduces wall-clock training times. 

ZENITH’s Scale-Invariance. Figure 2(a-b) shows that ZENITH remains robust regardless of the absolute scale of the gradient norm. For instance, MNIST has much lower norms than ImageNet-100. However, because ZENITH modulates the LR based on the ratio of the current norm to its historical peak, it is scale-invariant. This ensures versa-tility across diverse tasks, datasets, and model architectures. 

Minima Sharpness and Generalization. ZENITH’s high test accuracy is partly attributed to its ability to maintain a high LR when necessary, which allows it to converge to 

flatter global minima. It is already well-known in the ML community that higher LRs evade local minima and con-verge to global minima, so that will not be elaborated here. Instead, we will elaborate on the effect of minima sharpness on generalization because this provides novel insights. To create Figure 3(a-b), models were trained using vanilla SGD across a spectrum of constant LRs from 0.004 to 0.7. The sharpness of the minimum points was measured using the top eigenvalue ( λmax ) of the Hessian matrix. Figure 3a shows an inverse relationship between LR and sharpness be-cause higher LRs escaped from sharp basins and converged into flatter regions. Figure 3b shows that within the standard range ( 0.0 − 0.3), these higher LRs correspond to higher relative test accuracy, which means flatter minima were as-sociated with better generalization. To clarify, relative test accuracy pertains to the test accuracy relative to the max-imum achieved for that dataset. However, excessive LRs above 0.3 were detrimental even though the minima were flatter. This was because, even when granted sufficient time to converge, excessive LRs settled at minima with higher training loss. Effectively, the large step sizes prevented the model from fitting to the training data sufficiently. Figure 3c explains why flatter minima yielded better gen-eralization within the standard range. It illustrates that, assuming training loss can be near-zero for both sharp and flat minima, the test loss landscape is shifted relative to the training landscape due to distribution mismatches. Con-sequently, as shown by the red and green lines, a sharp minimum incurs a larger penalty in test loss than a flat mini-mum. This theory aligns with the empirical observations in Figure 3d, which depicts the loss landscapes around the min-5ZENITH: Automated Gradient Norm Informed Stochastic Optimization  

> Figure 3. (a–b) Influence of the LR on minima sharpness and test accuracy for vanilla SGD; (c) conceptual explanation of the generalization benefits of flat minima; (d) loss landscapes around the minima in the CIFAR-100 experiment; (e) accuracy versus minima sharpness across 4 experiments; (f) LR trajectories for distance-aware and Polyak-style baselines; (g–h) sensitivity of optimizers to the choice of initial LR.

ima reached with CIFAR-10. In this visualization, COCOB, which uses a tiny LR, gets trapped in a sharper minimum, resulting in a subpar test accuracy of 89.9%. Similarly, the Polyak-style optimizers (ALIG and SPS) used even smaller step sizes (with maximum LRs of 0.00341 and 0.00967 re-spectively), leading to even sharper minima and lower test accuracies (85.3% and 87.7% respectively). Conversely, ZENITH maintains a larger LR when necessary, converging to a flatter minimum with a superior test accuracy of 92 .4% .Other top-performing baselines include DoWG ( 92 .4% ) and PAL ( 91 .0% ), both of which also converged to flatter min-ima. DoWG’s good performance is attributed to its use of a higher LR of up to 0.25 during training, and likewise for PAL. This inverse correlation between minima sharpness and test accuracy is also consistent across other optimizers and experiments, as summarized in Figure 3e’s scatterplot. While the sharpness is important, the absolute training loss at the minimum influences performance as well. As shown in Figure 3d, high-performing ZENITH, DoWG, and PAL optimizers converged to minima with lower training loss. In contrast, the low LRs used by COCOB, ALIG, and SPS settled in suboptimal local minima. 

Underperformance of Baseline Optimizers. A collective issue with distance-aware estimators is their monotonically increasing LR trajectories. As an example, D-Adaptation’s LR trend is shown in Figure 3f. It contradicts the well-known practice of decaying the LR, without yielding any dis-cernible benefit. Figure 3f also shows that D-Adaptation’s LR choices are frequently inappropriate. Its LR is too low in CIFAR-10 and CIFAR-100, resulting in sharp local minima and poor generalization. However, it performs better on Food-101 where its LR choice surpasses 0.1. Polyak-style interpolators (ALIG and SPS) perform incon-sistently across experiments. While they perform well on Food-101, Tiny ImageNet, and ImageNet-100, they strug-gle on CIFAR-10 and CIFAR-100. This is because the method is sensitive to the absolute scales of the training loss and the L2 gradient norm, which vary greatly across experiments. Figure 3f shows that when SPS is effective, as in Food-101, the ratio of loss to squared L2 gradient norm is enormous, which would have caused divergence if the LR were left unchecked. Fortunately, the 0.1 LR cap prevents this and causes a constant rate of 0.1 early on, similar to ZENITH’s high initial LR. However, the LR is subsequently decayed much faster than with ZENITH. This accelerates convergence, but compromises test accuracy by settling into sharper minima. In the CIFAR-10 and CIFAR-100 experiments, ALIG and SPS are hampered greatly by their scale-variance because the Polyak step-size ratio is too small. For instance, Figure 3f shows that in CIFAR-10, the LR of SPS doesn’t even exceed 0.01. 

Hyperparameter Sensitivity Analysis. Although ZENITH is an automatic LR scheduler, it still needs a reasonable initial LR ( η0) to function effectively. However, this require-ment is not a disadvantage relative to the baselines because Quadratic Loss Approximation and Polyak-Style methods also require an η0. Although Coin-Betting and Distance-Aware Estimators do not take an η0, this characteristic is a liability rather than an asset. This is because these meth-ods often choose suboptimal LRs (e.g. very low rates in 6ZENITH: Automated Gradient Norm Informed Stochastic Optimization  

> Table 4. Comparison of test accuracy and requisite wall-clock time with different regularization magnitudes. Best results are in bold green.

Method CIFAR-100 Food-101 1e-4 2e-4 3e-4 1e-4 2e-4 3e-4 Acc (%) Time (min) Acc (%) Time (min) Acc (%) Time (min) Acc (%) Time (hr) Acc (%) Time (hr) Acc (%) Time (hr) DoWG 68.35 58.0 66.85 56.1 63.47 54.2 70.75 1.57 67.84 1.42 48.10 1.92 DAdapt 62.90 56.5 59.95 44.4 50.21 58.4 69.84 1.42 52.62 0.59 18.26 0.61 ALIG 74.13 59.7 74.69 55.4 74.76 46.3 72.80 1.23 73.16 1.00 74.10 0.91 SPS 73.91 55.8 75.18 51.7 76.95 59.9 75.86 3.40 77.21 3.27 76.80 1.75 

ZENITH 74.90 34.9 76.06 30.6 76.96 35.1 76.42 1.61 77.70 2.15 78.26 1.65 

CIFAR-10 and CIFAR-100 in Figure 3f). Furthermore, their design does not allow a manual LR initialization to correct this poor performance because using a manual initialization deteriorates their performance further. Figure 3(g-h) depicts the sensitivity of ZENITH to different 

η0 from 0.05 to 0.25 in the CIFAR-100 setting. Figure 3g shows that ZENITH effectively decays the LR regardless of the η0. Furthermore, Figure 3h compares ZENITH against the top-performing baselines from the CIFAR-100 setting. It shows that across the entire spectrum of η0, ZENITH yields higher test accuracy. Its accuracy is quite insensitive to η0,as evidenced by the nearly horizontal profile of the curve. The test accuracy for DoWG and D-Adaptation remains completely horizontal because they do not accept an η0.The accuracy of SPS remains unchanged between 0.1 and 

0.25 because its derived LR never exceeds the 0.1 threshold. It is worth noting that ZENITH achieves 74.3% test accuracy with η0 = 0 .2, surpassing the 73.8% reported in Table 3 where 0.1 was used. Increasing η0 from 0.1 toward 0.2 yielded progressively better accuracies. This reinforces that we did not tune ZENITH’s parameters for the experiments to ensure a fair comparison with the baselines. ZENITH’s only non-LR parameter is the window size ( W ), the existence of which is not a weakness because baselines have more non-LR parameters. We did not tune W for the experiments either, and ZENITH outperforms baselines regardless of the choice of W , as detailed in Appendix C. 

Compatibility with Regularization. An advantage of ZENITH over baselines is its robust performance with reg-ularization. In all previous experiments, L2 regularization was not used because several baselines are incompatible with it. We will now investigate the effects of L2 regulariza-tion in the CIFAR-100 and Food-101 settings using the top-performing baselines and ZENITH. Table 4 demonstrates that including regularization enhances ZENITH’s test ac-curacy relative to its non-regularized accuracies in Table 3. Furthermore, increasing the regularization strength from 

1 × 10 −4 to 3 × 10 −4 progressively improved ZENITH’s test performance. For ALIG and SPS on CIFAR-100, using  

> Figure 4. Impact of regularization of strength 2e-4 on LR trajecto-ries in (a) CIFAR-100 and (b) Food-101.

regularization boosts test accuracy compared to not using regularization, but for an artificial reason. Without regular-ization, the scale-sensitivity of these Polyak-style optimizers led to very low LR choices below 0.06 on CIFAR-100. The addition of a regularization term increases the magnitude of the training loss (which is the numerator of the Polyak step size formula), thereby inflating the LR. As illustrated in Figure 4a, the LR of SPS is now constantly overestimated and capped at the 0.1 maximum. As a result, while these optimizers no longer suffer from LR underestimates, they now lack stable convergence. Therefore, their accuracy re-mains inferior to ZENITH’s at all regularization strengths. Distance-aware optimizers showed degraded performance on both datasets when regularization was introduced com-pared to the non-regularized setting. This is because regu-larization grows their LR too rapidly to maintain stability, as shown by D-Adaptation’s behavior in Figure 4(a-b). 

# 4. Experiments on Detection & Segmentation 

Experimental Setup. Experiments were conducted on the MS COCO dataset (Lin et al., 2014), evaluating keypoint detection using Keypoint R-CNN and instance segmenta-tion using Mask R-CNN (He et al., 2017). The ResNet-50 backbone was initialized with ImageNet-pretrained weights, but was not frozen. Other architectural components were randomly initialized. Models were trained with a batch size of 16 on an NVIDIA A100 (80 GB) GPU with 5 data load-7ZENITH: Automated Gradient Norm Informed Stochastic Optimization        

> Figure 5. Training loss and test mAP50 curves against wall-clock time for detection and segmentation experiments. The color-coding for each optimizer is as follows: COCOB ,D-Adaptation ,DoG ,PAL , and ZENITH . Training loss magnitudes in this figure are scaled by a factor of 10 or 100 for visual clarity. This linear transformation preserves the original curve profiles without any distortions. The loss function was not scaled during the actual training process. Curves are also smoothed using a simple moving average for visual clarity.
> Table 5. Comparison of mAP50 across optimizers and the requisite wall-clock time to achieve it. Best results are in bold green.

Method Keypoint Detection Instance Segmentation Box mAP50 (%) Keypoint mAP50 (%) Time (hr) Box mAP50 (%) Mask mAP50 (%) Time (hr) PAL 72.1 78.9 14.4 46.2 44.1 13.5 GeN 62.9 61.7 1.43 29.7 27.2 1.59 COCOB 79.6 81.6 15.8 51.7 47.9 14.8 DoG 80.7 82.3 16.2 55.1 51.9 14.4 DAdapt 80.3 81.8 10.6 57.4 53.6 12.4 

ZENITH 81.3 84.2 13.2 59.3 56.0 14.5 

ing workers. The loss function aggregated classification, localization, and mask/keypoint-specific loss components. Test performance was evaluated three times per epoch (at iterations 2500, 5000, and 7393) using Keypoint mAP50 and Box mAP50 for keypoint detection, and Mask mAP50 and Box mAP50 for instance segmentation. The combined duration for both training and periodic testing was 24 hours. Within this time, the training phase for Keypoint R-CNN and Mask R-CNN was approximately 17 hours and 15 hours, re-spectively. The remaining time was for testing. Cumulative wall-clock time was recorded exclusively for the training phase. L2 regularization was not used. 

Experimental Results. Several baseline optimizers were not suited for keypoint detection and instance segmentation. Some methods, such as LQA, SPS, and ALIG, produced significant overestimates of the LR, defaulting to the maxi-mum LR cap throughout training. Others, such as Prodigy, reverted to constant LRs too, while L4 and DoWG failed to learn at all. Consequently, only five baselines yielded valid results, which are detailed in Figure 5 and Table 5. These results demonstrate that ZENITH achieves superior test performance across all evaluation metrics while con-verging in less wall-clock time than the baselines. Interest-ingly, while PAL decreased the training loss most rapidly, its generalization was the poorest. This is attributed to the higher sharpness of its minima, and PAL’s probing step size hyperparameter may require further tuning for these tasks to converge to flatter minima. The training curves for D-Adaptation are incomplete because divergent gradients ended training prematurely. Its monotonically increasing LR could have contributed to this divergence. 

# 5. Conclusion 

We introduced ZENITH, a scheduler that adapts the LR using the gradient norm history. The algorithm’s efficacy was demonstrated through image classification experiments across six architecture-dataset combinations, as well as on detection and segmentation tasks. In all cases, ZENITH achieved strong test performance and wall-clock efficiency relative to baselines. A deeper analysis revealed that the algorithm’s success stems from its ability to reach flatter minima while maintaining minimal per-iteration computa-tional overhead. Our optimizer’s efficacy has been studied for scheduling the LR in tasks where SGD is standard. It is not intended for tasks that use Adam-style optimizers, which have their own LR adaptation schemes. Given that SGD is the preferred optimizer for a vast array of tasks in computer vision and other domains, the utility of ZENITH is broad. We hope ZENITH will serve as a useful tool for ML practitioners, easing the burden of manual scheduling. 8ZENITH: Automated Gradient Norm Informed Stochastic Optimization 

# Impact Statement 

The ZENITH optimizer automates the learning rate schedul-ing of deep learning (DL) models, reducing the wall-clock time required for convergence. By lowering computational overhead, the algorithm makes DL more accessible to prac-titioners with limited hardware resources. Furthermore, the reduced training durations alleviate the carbon footprint as-sociated with model training. Although accelerated training speeds up DL workflows, it remains the responsibility of practitioners to ensure that trained models do not lead to societal harm. 

# References 

Berrada, L., Zisserman, A., and Kumar, M. P. Training neural networks for and by interpolation. In International conference on machine learning , pp. 799–809. PMLR, 2020. Bossard, L., Guillaumin, M., and Van Gool, L. Food-101– mining discriminative components with random forests. In European conference on computer vision , pp. 446–461. Springer, 2014. Bu, Z. and Xu, S. Gradient descent with generalized new-ton’s method. arXiv preprint arXiv:2407.02772 , 2024. Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A. L. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence , 40(4):834–848, 2017. Defazio, A. and Mishchenko, K. Learning-rate-free learning by d-adaptation. In International Conference on Machine Learning , pp. 7449–7479. PMLR, 2023. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248–255. Ieee, 2009. Fu, M. and Wu, F.-X. Qlabgrad: A hyperparameter-free and convergence-guaranteed scheme for deep learning. In Proceedings of the AAAI conference on artificial intel-ligence , volume 38, pp. 12072–12081, 2024. Ge, R., Kakade, S. M., Kidambi, R., and Netrapalli, P. The step decay schedule: A near optimal, geometrically de-caying learning rate procedure for least squares. Advances in neural information processing systems , 32, 2019. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,pp. 770–778, 2016. He, K., Gkioxari, G., Doll ´ar, P., and Girshick, R. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision , pp. 2961–2969, 2017. Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In 

Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 4700–4708, 2017. Ivgi, M., Hinder, O., and Carmon, Y. Dog is sgd’s best friend: A parameter-free dynamic step size schedule. In International Conference on Machine Learning , pp. 14465–14499. PMLR, 2023. Khaled, A., Mishchenko, K., and Jin, C. Dowg unleashed: An efficient universal parameter-free gradient descent method. Advances in Neural Information Processing Systems , 36:6748–6769, 2023. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Le, Y. and Yang, X. Tiny imagenet visual recognition chal-lenge. CS 231N , 7(7):3, 2015. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceed-ings of the IEEE , 86(11):2278–2324, 2002. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-manan, D., Doll ´ar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision , pp. 740–755. Springer, 2014. Loizou, N., Vaswani, S., Laradji, I. H., and Lacoste-Julien, S. Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence. In International Conference on Artificial Intelligence and Statistics , pp. 1306–1314. PMLR, 2021. Loshchilov, I. and Hutter, F. Sgdr: Stochastic gra-dient descent with warm restarts. arXiv preprint arXiv:1608.03983 , 2016. Mishchenko, K. and Defazio, A. Prodigy: An expedi-tiously adaptive parameter-free learner. arXiv preprint arXiv:2306.06101 , 2023. Mutschler, M. and Zell, A. Parabolic approximation line search for dnns. Advances in neural information process-ing systems , 33:5405–5416, 2020. Orabona, F. and Tommasi, T. Training deep networks with-out learning rates through coin betting. Advances in neural information processing systems , 30, 2017. Polyak, B. T. Introduction to optimization. 1987. 9ZENITH: Automated Gradient Norm Informed Stochastic Optimization 

Rolinek, M. and Martius, G. L4: Practical loss-based step-size adaptation for deep learning. Advances in neural information processing systems , 31, 2018. Simonyan, K. and Zisserman, A. Very deep convolu-tional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vi-sion. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 2818–2826, 2016. Tan, M. and Le, Q. Efficientnet: Rethinking model scal-ing for convolutional neural networks. In International conference on machine learning , pp. 6105–6114. PMLR, 2019. Yu, F., Wang, D., Shelhamer, E., and Darrell, T. Deep layer aggregation. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 2403–2412, 2018. Zhu, Y., Huang, D., Gao, Y., Wu, R., Chen, Y., Zhang, B., and Wang, H. Automatic, dynamic, and nearly optimal learning rate specification via local quadratic approxima-tion. Neural Networks , 141:11–29, 2021. 10 ZENITH: Automated Gradient Norm Informed Stochastic Optimization 

# A. ZENITH algorithm pseudocode 

Algorithm 1 ZENITH Adaptive Learning Rate Schedule 

Input: Initial learning rate η0, Window size W

Initialize: Window queue Q ← ∅ , Zenith Z ← 0, Current LR η ← η0

for each iteration t do 

Compute gradient L2 norm: g ← ∥∇L (θt)∥2

Push g to Q

if size( Q) > W then 

Pop oldest value from Q

end if if size( Q) == W then 

Compute rolling mean: μ ← mean (Q)

Update Zenith: Z ← max( Z, μ )

{Apply Decay }

if Z > 0 then 

η ← η0 · (μ/Z )

else 

η ← η0

end if 

Update optimizer learning rate to η

end if end for 

# B. Theoretical Convergence Analysis 

B.1. Formalization and Assumptions Definition B.1 (ZENITH Algorithm ).Update Rule. The weights are updated via gradient descent: 

θt+1 = θt − ηt∇L (θt) (4) 

Adaptive Step Size. Let gt = ∥∇L (θt)∥2. The step size ηt is computed using a sliding window mean μt over window W

and its historical maximum Zt:

μt = 1

W

> t

X 

> i=t−W+1

gi, Zt = max( Zt−1, μ t) (5) 

ηt = η0 · μt

Zt

(6) where η0 is the initial learning rate. 

Proposition B.2. An important property is that the gradient norm’s current local estimate μt can never exceed its historical maximum Zt by definition. Therefore, for all t ≥ 0:

μt

Zt

≤ 1 = ⇒ ηt ≤ η0 (7) 

Therefore, the step sizes used throughout training are upper-bounded by η0.

The standard assumptions for smooth non-convex optimization (Fu & Wu, 2024) are adopted. The deterministic regime where the gradient signal dominates stochastic noise will be analyzed. 

Assumption B.3. The loss function L is bounded below by a scalar L∗.

L(θ) ≥ L ∗, ∀θ ∈ Rd (8) 

Assumption B.4. The gradient of the loss function is M -Lipschitz continuous: 

∥∇L (x) − ∇L (y)∥2 ≤ M ∥x − y∥2 (9) 11 ZENITH: Automated Gradient Norm Informed Stochastic Optimization 

B.2. Convergence Result Theorem B.5. Under Assumptions B.3 and B.4, if the initial learning rate satisfies η0 ≤ 2 

> M

, then the algorithm converges to a stationary point: 

lim  

> t→∞

∥∇L (θt)∥ = 0 (10) 

Proof. Using the M -Lipschitz smoothness assumption B.4, we invoke the Descent Lemma inequality: 

L(θt+1 ) ≤ L (θt) + ⟨∇L (θt), θ t+1 − θt⟩ + M

2 ∥θt+1 − θt∥2 (11) Substituting the update rule (4) into this inequality: 

L(θt+1 ) ≤ L (θt) − ηt∥∇L (θt)∥2 + M

2 η2 

> t

∥∇L (θt)∥2 (12) Factoring out the term −ηt∥∇L (θt)∥2:

L(θt+1 ) ≤ L (θt) − ηt∥∇L (θt)∥2



1 − M

2 ηt



(13) For the loss to decrease (i.e., L(θt+1 ) ≤ L (θt)), the term in the parenthesis must be non-negative: 

1 − M

2 ηt ≥ 0 = ⇒ ηt ≤ 2

M (14) In accordance with the Boundedness Property ( ηt ≤ η0) (7) , this condition is satisfied for all iterations t if we choose the initial learning rate such that: 

η0 ≤ 2

M (15) This algorithm is theoretically stable because the ηt cannot explode on steep gradients, and it respects the Lipschitz stability bound provided η0 is set appropriately. Recall that the Descent Inequality (13) included the term (1 − M 

> 2

ηt). Using the Boundedness Property ( ηt ≤ η0) (7) , this term can be bounded from below by a constant: 

1 − M

2 ηt ≥ 1 − M

2 η0 (16) Let us define this guaranteed safety margin as C = 1 − M 

> 2

η0. Assuming the initial learning rate is chosen such that 

η0 < 2/M as required in Equation (15) , then C > 0. Substituting this constant C and the definition of ηt (6) into the Descent Inequality (13) yields: 

L(θt+1 ) ≤ L (θt) − Cη 0

 μt

Zt



∥∇L (θt)∥2 (17) Rearranging this inequality to isolate the weighted gradient norm on the left-hand side (LHS): 

μt

Zt

∥∇L (θt)∥2 ≤ L(θt) − L (θt+1 )

Cη 0

(18) Summing the LHS of this inequality over T iterations: 

> T

X

> t=0

μt

Zt

∥∇L (θt)∥2 ≤ L(θ0) − L (θT +1 )

Cη 0

≤ L(θ0) − L ∗

Cη 0

(19) 12 ZENITH: Automated Gradient Norm Informed Stochastic Optimization 

The right side of inequality (19) is finite. Therefore, the sum on the left must be finite: 

lim  

> T→∞
> T

X

> t=0

μt

Zt

∥∇L (θt)∥2 < ∞ (20) This inequality means that the infinite sum of non-negative terms is finite. By the properties of convergent series, the t-th term must approach zero as t → ∞ :

lim 

> t→∞

 μt

Zt

∥∇L (θt)∥2



= 0 (21) We analyze the asymptotic behavior of the components to isolate ∥∇L (θt)∥:• Denominator: By construction, Zt is non-decreasing and bounded. It converges to a finite positive constant: 

lim t→∞ Zt = Z∞ > 0.• Numerator: The sliding window mean μt scales with the local gradient magnitude: μt ≈ ∥∇L (θt)∥.Substituting these limits into the vanishing term condition (21) yields: 

lim 

> t→∞

 ∥∇L (θt)∥

Z∞



∥∇L (θt)∥2 = 0 (22) This simplifies to a cubic relationship: 1

Z∞

lim  

> t→∞

∥∇L (θt)∥3 = 0 (23) Since Z∞ is a finite constant, the gradient norm itself must vanish: 

lim  

> t→∞

∥∇L (θt)∥ = 0 (24) This means that the algorithm converges to a stationary point. 

# C. Effect of the Window Size  

> Figure 6. Effect of the window size hyperparameter on the (a) LR trajectory, (b) test accuracy evolution, and (c) convergence test accuracy.

To evaluate the impact of the window size W , the CIFAR-100 experiment was repeated across six window sizes: 50, 150, 500, 1500, 5000, and 15000. Figure 6a illustrates that a small window size of 50 results in a noisier LR trajectory, which is induced by noisy gradient norms. While increasing W suppresses this noise, it delays LR decay. Despite this delay, Figure 6(b-c) demonstrates that larger window sizes enhance test accuracy slightly. Figure 6c shows that ZENITH outperforms the best baselines across a broad range of window sizes spanning two orders of magnitude. The default value of 

W = 500 utilized in Table 3 was not the best choice according to Figure 6(b-c), but this default value was maintained across the experiments without any task-specific tuning. This ensured a fair comparison with baselines. 13