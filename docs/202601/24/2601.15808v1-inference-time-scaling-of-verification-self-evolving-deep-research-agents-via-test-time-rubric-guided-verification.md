<div class="paper-title-row">
<h1 class="paper-title-zh">验证的推理时扩展：通过测试时准则引导验证实现自进化深度研究智能体</h1>
<h1 class="paper-title-en">Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification</h1>
</div>

<div class="paper-meta-row">
<div class="paper-meta-left">
<p><strong>Authors</strong>: Yuxuan Wan, Tianqing Fang, Zaitang Li, Yintong Huo, Wenxuan Wang, Haitao Mi, Dong Yu, Michael R. Lyu</p>
<p><strong>Date</strong>: 2026-01-22</p>
<p><strong>PDF</strong>: <a href="https://arxiv.org/pdf/2601.15808v1" target="_blank">https://arxiv.org/pdf/2601.15808v1</a></p>
<p><strong>Tags</strong>: <span class="tag-label tag-green">EOH</span> <span class="tag-label tag-green">EAA</span></p>
<p><strong>Score</strong>: 7.0</p>
</div>
<div class="paper-meta-right">
<p><strong>Evidence</strong>: 通过迭代反馈和细化实现自我进化的智能体</p>
<p><strong>TLDR</strong>: 本研究针对深度研究智能体（DRA）提出了一种推理侧验证扩展的新范式。通过构建包含5大类、13小类的失败分类学，开发了DeepVerifier验证器，利用精心设计的准则在推理阶段对智能体输出进行迭代评估与反馈，实现无需额外训练的自我进化。实验表明，该方法显著提升了智能体在复杂任务中的准确率，并开源了高质量验证数据集DeepVerifier-4K以支持开源社区发展。</p>
</div>
</div>

<div class="paper-glance-section">
<h2 class="paper-glance-title">速览</h2>
<div class="paper-glance-row">
<div class="paper-glance-col">
<div class="paper-glance-label">Motivation</div>
<div class="paper-glance-content">现有研究多侧重于训练后的策略增强，缺乏在推理阶段通过自我验证实现智能体性能持续提升的有效手段。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Method</div>
<div class="paper-glance-content">提出基于失败分类学准则的DeepVerifier验证器，通过推理侧的迭代反馈循环引导智能体进行自我修正与优化。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Result</div>
<div class="paper-glance-content">DeepVerifier在元评估中表现优异，使GAIA和XBench-DeepResearch等挑战性任务的准确率提升了8%-11%。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Conclusion</div>
<div class="paper-glance-content">推理侧验证扩展是提升研究智能体可靠性的高效路径，证明了通过准则引导的反馈可以实现智能体的自我进化。</div>
</div>
</div>
</div>

---


## 摘要
深度研究智能体（DRA）的最新进展正在改变自动化的知识发现和问题解决。虽然现有的大多数工作集中在通过后训练来增强策略能力，但我们提出了一种替代范式：在精心设计的评分准则（rubrics）引导下，通过迭代验证策略模型的输出来实现智能体能力的自进化。这种方法实现了验证的推理时扩展（inference-time scaling），即智能体通过评估其生成的答案来产生迭代反馈和改进，从而实现自我提升。我们基于自动构建的 DRA 失败分类法（DRA Failure Taxonomy）推导出评分准则，该分类法将智能体的失败系统地分为五个大类和十三个子类。我们提出了 DeepVerifier，这是一种基于准则的结果奖励验证器（outcome reward verifier），它利用了验证的不对称性，在元评估 F1 分数上比原始的“智能体即评委”（agent-as-judge）和 LLM 评委基准高出 12%-48%。为了实现实际的自进化，DeepVerifier 在测试时推理期间作为一个即插即用的模块集成。验证器生成基于准则的详细反馈，并将其反馈给智能体进行迭代引导（bootstrapping），在无需额外训练的情况下优化响应。在强大的闭源 LLM 支持下，这种测试时扩展在 GAIA 和 XBench-DeepResearch 的挑战性子集上实现了 8%-11% 的准确率提升。最后，为了支持开源发展，我们发布了 DeepVerifier-4K，这是一个包含 4,646 个高质量智能体步骤的精选监督微调数据集，专注于 DRA 验证。这些示例强调反思和自我批评，使开源模型能够开发出强大的验证能力。
## Abstract
Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.