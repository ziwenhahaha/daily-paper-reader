Title: An Empirical Study on Ensemble-Based Transfer Learning Bayesian Optimisation with Mixed Variable Types

URL Source: https://arxiv.org/pdf/2601.15640v1

Published Time: Fri, 23 Jan 2026 01:26:56 GMT

Number of Pages: 48

Markdown Content:
# An Empirical Study on Ensemble-Based Transfer Learning Bayesian Optimisation with Mixed Variable Types 

# Natasha Trinkle RMIT University Huong Ha RMIT University Jeffrey Chan RMIT University January 2026 

Abstract 

Bayesian optimisation is a sample efficient method for finding a global opti-mum of expensive black-box objective functions. Historic datasets from related problems can be exploited to help improve performance of Bayesian optimisa-tion by adapting transfer learning methods to various components of the Bayesian optimisation pipeline. In this study we perform an empirical analysis of vari-ous ensemble-based transfer learning Bayesian optimisation methods and pipeline components. We expand on previous work in the literature by contributing some specific pipeline components, and three new real-time transfer learning Bayesian optimisation benchmarks. In particular we propose to use a weighting strategy for ensemble surrogate model predictions based on regularised regression with weights constrained to be positive, and a related component for handling the case when transfer learning is not improving Bayesian optimisation performance. We find that in general, two components that help improve transfer learning Bayesian optimisation performance are warm start initialisation and constraining weights used with ensemble surrogate model to be positive. 

# 1 Introduction 

# 1.1 Objectives 

The objective of this work is to provide an empirical analysis of approaches using historic data to improve the performance of Bayesian Optimisation. Bayesian Opti-misation (BO) aims to provide a sample efficient approach to optimising black-box objective functions that are typically expensive to evaluate. BO algorithms make use of sequential decision making, active learning and Bayesian decision theory to solve problems where the black-box objective function cannot be optimised using gradient based approaches or assumptions of convexity, but can be evaluated at arbitrary points in the input domain [27, 47]. While BO uses prior assumptions about objective function 1

> arXiv:2601.15640v1 [cs.LG] 22 Jan 2026

characteristics to aid decision making about where to evaluate the expensive objective function for fastest optimisation, it has been suggested that further improvements to the performance of BO may be possible by learning from historic data generated during optimisation of similar processes [1]. While a number of ways of applying transferring learning to enhance performance of BO (using historic data) have been proposed in literature ([1]), in this analysis we focus on ways of using information from historic data to initialise the search for a global optimum, and to construct a surrogate model of the black-box objective function, as was explored in [62, 34, 11]. We aim to analyse in detail the pipeline components presented in literature, as well as proposing several additional methods for certain components of this pipeline. We test on a selection of benchmarks from literature as well as three new real-time benchmarks proposed in this work. 

# 1.2 Justification 

Optimisation of expensive black-box functions is a growing research field with ap-plications in a range of different areas. Examples include hyperparameter tuning in automated machine learning models [47, 13, 6], design engineering processes [13], environmental monitoring using expensive to activate sensors [47, 16], combinatorial optimisation problems such as the travelling salesman problem ([23, 47]), and biologi-cal applications such as molecular or drug design [13]. More recent examples include machine learning [3], robotics [8, 32], chemistry and materials science [52], environ-mental monitoring [20, 57] and energy management [45]. These examples include a number of different types of processes as black-box functions with varying degrees of cost related to evaluating these functions, both in a historic and a real-time context. The different processes contributing to black-box objective functions will also result in different function characteristics. While there are standard sets of prior assump-tions commonly used to tackle these problems, sometimes more specific information in the form of historic observations of related problems is available and can also be included in optimisation strategies being explored [1]. Examples include in AutoML tuning where machine learning algorithm hyperparameters are optimised for many sim-ilar datasets [11, 1], as well as applications such as chemical sciences and data-driven experiment planning, where an analytical approach to optimising experimental design based on previous outcomes can significantly reduce experimental effort [22, 35]. 

# 1.3 Background 

Previous work done on transferring learning from historic datasets to improve perfor-mance of BO has focused on four key steps in the BO pipeline as described by [1]. These include surrogate design [50, 41, 46, 63, 34, 10, 11], acquisition function design [63], initialisation design [34] and search space design and pruning [39]. One approach to surrogate design uses an ensemble of surrogate models consisting of Gaussian Pro-cesses, with each one pre-trained on a separate historic task dataset. Motivation for developing an ensemble-based approach to surrogate design in the context of BO with 2transfer learning was the need to automate the process where by an expert with high levels of experience transfers their knowledge from one problem to another related problem [2, 59] in a way that was computationally tractable [46, 63]. In particular, there was a need to avoid complex operations on large sets of historic data using multi-task kernels with one Gaussian Process, as had been done in earlier work (see [50, 65]). Using an ensemble of surrogate models also enables adaptive weighting of historic tasks as new data becomes available [46, 63]. Weighted predictions from each en-semble surrogate model can be used to construct a surrogate model of the black-box objective function informed by knowledge transferred from these historic related tasks. In the literature there are various ways proposed to combine surrogate model predic-tions in order to model the black-box objective function [62, 34, 11]. The ensemble has also been used to find warm start initialisation data points instead of random ini-tialisation [34]. In this work, we provide an expanded study of previous empirical analyses and ablation studies performed in [62, 34, 11] to provide some suggestions to practition-ers looking for best transfer learning with BO strategies to use with particular bench-marks. We propose and test new weighting strategies that show improved performance on some benchmarks, and include new real-time benchmarks with different variable type and dimensions that enable us to demonstrate comparative performance of dif-ferent approaches on different input domains. In particular, the elements we have added to work done in previous studies include the following: proposal of three new real-time benchmarks to address concerns about OpenML100 datasets (see https: //docs.openml.org/benchmark/ [4, 5]) used to prepare benchmarks proposed in [38, 11], including one based on OpenML-CC18, addition of weighting strategies based on regularised regression (Lasso and Ridge, with and without a postive constraint on the weights) 1 and related strategy for selecting between transfer learning and stan-dard BO modes, and inclusion of a set of ranking plots for each benchmark separately that includes all methods explored in this work using warm start initialisation, as well as standard BO 2. Finally, as part of our empirical analysis, we propose a way to anal-yse a set of historic datasets with a view to gaining insight about comparative locations of minima points and how this may impact performance of BO with transfer learning based on this these historic datasets. 

# 1.4 Summary of Results 

In this work we explore the following questions: 1. How effective are selected ensemble-based transfer learning strategies, including warm start or random initialisation, combined with various weighting strategies 

> 1Regularised regression weighting strategies access a continuous weights search space instead of discre-tised search space as with weighting strategies proposed in [62, 11]
> 2Ranking plots were included in [62] for each benchmark separately, where as [11] combined all bench-marks into one ranking plot, meaning available plots for analysis of separate benchmarks do not currently include all relevant methods.

3in improving performance of Bayesian optimisation as compared to standard Bayesian optimisation (and each other)? 2. How does inclusion of a strategy for automated selection of standard versus transfer learning Bayesian optimisation (based on measurable criteria available during optimisation) affect the overall performance of a Bayesian optimisation pipeline? 3. What kind of insights can be obtained from analysing historic datasets about how effective transfer learning with Bayesian optimisation may be on one particular benchmark as compared to another benchmark? In addressing these questions we are focussing on three components of the transfer learning BO pipeline presented in previous work [34, 62, 11]. These are initialisation design, computing weights for the surrogate models in the ensemble, and strategies for handling the case where surrogate models are not providing positive transfer learning. By performing a set of ablation studies on various combinations of these components, including those proposed in this work, we seek to empirically analyse and compare the benefits of using these various transfer learning strategies on the selected set of nine benchmarks. In general it is observed that warm start initialisation improves the performance of BO, although for some benchmarks there is variation in results depending on weighting strategy. Comparative performance of different weighting strategies, with both warm start initialisation and random initialisation, appears to be benchmark dependent, al-though strategies that constrain weights to be positive mostly perform better than those that allow negative weights. Some benchmarks show similar BO performance across most weighting strategies. However other benchmarks exhibit significant BO perfor-mance improvement for a particular method(s). The best method(s) is not consistent across benchmarks. For eight out of nine benchmarks, there is at least one method that uses warm start initialisation that outperforms all those using random initialisation, confirming that warm start initialisation makes a significant contribution to improving BO performance using transfer learning. However, impact on BO performance result-ing from inclusion of a strategy for automated selection of standard versus transfer learning Bayesian optimisation to handle bad transfer learning is variable, suggest-ing further work could be done to improve this component of the pipeline. There is some relationship noted between results seen in ranking plots and overlap of location of minima across historic datasets for different benchmarks. However, this relationship across benchmarks is not completely clear, possibly due to impact of dimensionality of input domain and ranking plots including range of weighting strategies with varying performance. Overall, results suggest that, while comparative performance of methods is benchmark dependent, there are general trends around best ensemble-based transfer learning strategies for improving BO performance. These include the use of warm start initialisation, and constraining weights used to combine output from ensemble surro-gate models to be positive. 41.5 Guide to Remaining Sections 

The remaining sections of this paper include background information and problem statement in Section 2, a description of methods considered, benchmarks, and exper-imental details in Section 3, followed by the main results in Section 5. Finally, we conclude with a discussion, including observed limitations to approaches explored, and a conclusion in Section 6. 

# 2 Background & Problem Statement 

To provide context for the work done in this project, a brief background about Bayesian Optimisation and commonly used algorithmic components is provided followed by an introduction to transfer learning. 

# 2.1 Bayesian Optimisation 

Bayesian Optimisation (BO), initially developed by [27, 28], was designed to find a global optimum of a stochastic process using Bayes’ theorem, where f denotes the model of the stochastic process function and D1,t denotes a dataset of size t,

P (f |D1: t) ∝ P (D1: t|f )P (f ). (1) It is an iterative process that requires an initialisation dataset to begin, and then loops through two key steps. Key BO steps include training a surrogate model used to model the black-box objective function given available evaluations, and optimisation of an acquisition function, a utility function that quantifies expected gain in making an evaluation at some input, to find the next input for evaluation. The key aim is to improve on more naive approaches to black-box optimisation including random or grid search in finding an optimal location with a limited evaluation budget [25]. The first step in BO is to model the objective function using a surrogate model. A Gaussian Process (GP) is a common and suitable choice of surrogate model because it describes a distribution over functions, but also can be used for point-wise predictions since the marginal distribution over a set of evaluations is a multivariate normal distri-bution [44, 15]. A GP, f (x), is fully specified by mean function, m(x), and covariance function, k(x, x′),

f (x) ∼ GP  m(x), k (x, x′), (2) but can also utilise the marginal joint normal distribution over training, and testing outputs to make point-wise predictions, where y denotes Mtrain objective function evaluations (training outputs), f denotes Mtest test outputs, K(X, X ) is an Mtrain ×

Mtrain matrix of covariances computed using the covariance function on the Mtrain 

training inputs ( X), K(X, X ∗) is an Mtrain × Mtest matrix of covariances on training 5inputs ( X), and test inputs X∗, K(X∗, X ) is the transpose of K(X, X ∗), K(X∗, X ∗)

is the covariance for test inputs, and ζ2I is measurement noise, 

yf



∼ N 0,

K(X, X ) + ζ2I K(X, X ∗)

K(X∗, X ) K(X∗, X ∗)

 !

. (3) In order to construct the surrogate model, a dataset of black-box objective func-tion evaluations is required. Therefore, an initial dataset to use for surrogate model construction is needed for the first iteration of the BO evaluation budget. Frazier et al ([13]) suggests using a random sampling strategy for this purpose. The second step in BO is to construct a global optimisation policy that enables a trade-off between exploration and exploitation using an acquisition function. There are a number of different acquisition functions used in BO. A common acquisition function is upper confidence bound (UCB), first used in [30] and applied to GPs by [48], 

αU CB (x; D, π ) = μ(x) + βσ (x) (4) where π ∈ (0 , 1) indicates the confidence level which informs the value of β and affects exploration-exploitation trade-off. UCB is simply the mean, μ, plus uncertainty around the mean 3. For minimisation problems, lower confidence bound (LCB), μ − βσ 

can be used instead of UCB. In summary, the BO pipeline begins with selection of some initialisation points evaluated on the black-box objective function. This is followed by iterations over con-struction of a surrogate model (GP) used to make predictions for the black-box objec-tive function, and optimisation of an acquisition function that provides a rationale for selecting the next input point to evaluate on the black-box objective function. The next section (2.2) describes approaches in literature to trying to improve BO performance by transferring learning from historic data generated by related tasks. 

# 2.2 Transfer Learning in Bayesian Optimisation 

Since many of the optimisation tasks for which BO is used belong to a group of simi-lar tasks, there exist historic datasets from related problems that have been performed previously and can be exploited using transfer learning during a new optimisation task [1]. For example, when datasets are updated for machine learning tasks, hyperparame-ters need to be re-optimised [1]. The term, source tasks, refer to previously performed tasks which are no longer available for evaluation, but from which there exists a historic dataset. The term, target task, refers to the current optimisation task [40, 1, 66]. One challenge in using transfer learning is how to avoid negative transfer. While lacking a rigorous definition, negative transfer could be considered to occur when the contribu-tion made by datasets from previously performed tasks, given the selected algorithm, results in worse performance on the current task than if only data from the current task      

> 3μand σare predictions from the surrogate model for some input x[15]

6was included [60]. One example of using transfer learning in the context of BO is to warm start the search for optima by selecting promising initialisation points based on historic data rather than randomly generating these points as described in Section 2.1. In [12], an approach to finding warm start initialisation points based on quantification of relat-edness (using metafeatures of task classification datasets) between source and target tasks 4 was proposed. An alternative approach to warm start initialisation was proposed by [34] and used by [11]. In this approach, an ensemble of surrogate models (one per source task) is constructed. Then all inputs in the combined historic datasets are evaluated on each of the ensemble surrogate models. The input point with best aver-age across all task surrogate models is selected as an initialisation point. Algorithm 2 shows how this works to select multiple initialisation points. Another example of transfer learning in the context of BO is ensemble learning. In particular, Schilling et al ([46]) proposed the use of an ensemble of experts consisting of GPs pretrained on historic data from a set of source tasks. As target observations become available, these GPs were further trained on the target dataset. Predictions were made using this ensemble of GPs which is the combination of predictions of each GP expert. Simple weighting schemes were used that ranked all source tasks equally. In another work, [62] proposed a Two-Stage Transfer Surrogate model using pairwise ranking of inputs for weighting (TSTR). This also involved the construction of an en-semble of GP surrogate models, one per historic task, in the first of the two stages. Their weighting scheme involved using Epanechnikov quadratic kernel to compute distance between two datasets, combined with Nadaraya Watson kernel to compute predicted means. In this work [62] used performance rankings 5 and the metric to com-pare hyperparameter datasets. A different approach was explored by [34]. This approach permitted negative weights, and used stochastic gradient descent on a mean squared error loss function to construct the target model as a weighted linear combination of predictions from the en-semble models. Training and validation sets were employed to avoid over-fitting. The work of [10, 11] proposed a Ranking-Weighted Gaussian Process Ensemble (RGPE) and used a linear combination of predictions from ensemble surrogate models with weighting scheme adapted from that proposed by [29] and extending the approach us-ing discordant pairs proposed by [62]. The next section contains a formal statement of the problem with mathematical definitions of the problem space and historic data used for transfer learning.  

> 4In [12], datasets used by the ML algorithm were compared for this purpose. This is only a feasible approach if one has access to these datasets, which may not always be available.
> 5Performance ranking loss function uses number of discordant pairs between two datasets [62].

72.3 Regularised Regression 

In the previous section (2.2), there are several weighting schemes described that were used to linearly combine output from the set of surrogate models in an ensemble to produce a prediction on the target problem. Another approach to weighting ensem-ble surrogate models would be to use regularised regression. Regularised regression involves using mean squared error as a loss function combined with some term to pre-vent over-fitting of the weights. For an in depth description, please see [21, Chapter 3]. Regularised regression is useful in the case of ill-posed problems where the inverse problem does not necessarily have an exact solution [55, 58]. Two popular forms of regularisation are Ridge, which minimises a loss function with an L2 penalty or shrink-age term [21], and Lasso, proposed by [53], which uses an L1 penalty term. The L1 penalty term results in some weights shrinking to 0, resulting in simpler models (see [53] for more details). Equation 5 shows the loss function for Ridge regularised re-gression, and Equation 6 shows the loss function for Lasso regularised regression. In both equations, wi represents the weights used on the i ∈ 1, ..., N features, which are represented as hi,j where j ∈ 1, ..., M represents each of the input points, ytarget,j 

represents the true function output for each of the M input points, and α represents the penalty hyperparameter. 

LRidge = 1

M

> M

X

> j=1

(ytrue,j −

> N

X

> i=1

wihi,j )2 + α

> N

X

> i=1

w2 

> i

(5) 

LLasso = 1

M

> M

X

> j=1

(ytrue,j −

> N

X

> i=1

wihi,j )2 + α

> N

X

> i=1

|wi| (6) The next section provides a formal description of the problem being addressed us-ing BO in this work, including the use of historic datasets for transfer learning. 

# 2.4 Problem Statement 

A single-objective target function is optimised over a mixed-variable search space, described as ϕtarget : H × Z × C → R, where H = H(1) × ... × H (d) is the domain of categorical parameters, Z = Z(1) × ... × Z (dz ) is the domain of discrete (integer) parameters, and C = C(1) × ... × C (d) is the domain of continuous parameters. The number of categorical parameters is dh ≥ 0, the number of discrete parameters is 

dz ≥ 0, and the number of continuous parameters is dc ≥ 1. Let x = [ h, z, c],where h = [ h1, h 2, ..., h dh ], z = [ z1, z 2, ..., z dz ] and c = [ c1, c 2, ..., c dc ] and X =

H(1) × ... × H (dh) × Z (1) × ... × Z (dz ) × C (1) × ... × C (dc) [7]. The optimisation (minimisation) task can be expressed as 8x∗ = arg min 

> x∈X

ϕtarget (x). (7) In the context of BO with transfer learning, it is assumed that, prior to any evalua-tions being made on the target function, there exists a set of N historic datasets which were generated previously (ie. during optimisation) on N related source tasks 6 as in [11]. Source tasks are related to the target task via a shared input domain resulting from some similar process. Source task objective functions are no longer available for evaluation during target task optimisation. Each of the N datasets containing Mi noisy evaluations can be expressed, Dsource = {D 1, ..., DN }, where Di = {xij , y ij }Mi

> j=1

.It is also assumed that all source and target task objective functions can be approx-imated using a surrogate model (GP). For task i, fi ∼ GP (μi, K i), where μi indicates the mean function of fi, and Ki is the kernel (covariance) function. We use an ensemble of surrogate models, inclusive of all source task GPs and a GP to model the target task, 

ftarget , and we denote the ensemble of surrogate models as fens = {fi}Ni=1 ∪ ftarget .

# 3 Methods 

The methods we are presenting for consideration here are broken down into compo-nents of the overall pipeline used for BO with ensemble transfer learning as presented in literature [34, 62, 10, 11]. We also propose four new weighting strategies in Section 3.3, and one new strategy for handling bad transfer learning in Section 3.4. 

# 3.1 Pipeline Overview 

The general pipeline can be seen in Algorithm 1. In this work, we consider methods for initialisation of the target dataset (Section 3.2), construction of the ensemble of surro-gate models, and in particular the weighting strategies used (Section 3.3), and finally, approaches to reverting to standard BO when transfer learning is not improving BO performance (Section 3.4). Equation 8, where fens (xj ) is the output normal distribution for input xj , wi de-notes weights on source surrogate model mean predictions, wtarget is the weight on target surrogate model mean prediction, and variance of fens (xj ), denoted σ2

> target

(xj ),is obtained only from the target surrogate model, 

fens (xj ) ∼ N   NX

> i=1

wiμi(xj ) + wtarget μtarget (xj ), σ 2

> target

(xj ), (8) 

> 6Note that historic datasets, generated during some optimisation process, will not share common input configurations.

9Algorithm 1 Bayesian Optimisation Transfer Learning Pipeline 

Input Dsource = {D 1, ..., DN } (historic data), T (eval budget), n init (number of initialisation points) 

1: Initialisation 

2: fens ← CONSTRUCT ENSEMBLE (Dsource )

3: {xinit j }n init j=1 ← WARM START (fens ) (see Algorithm 2) 

4: {yinit j }n init j=1 = ϕ({xinit j }n init j=1 ) + ϵtarget (objective function evaluation) 

5: Dtarget ← { xinit j , y init j }n init j=1 

6: Specific method initialisation procedures (see Section 3.3) 

7: for i = n init to T do 

8: UPDATE ENSEMBLE (Dtarget , fens )

9: COMPUTE ENSEMBLE WEIGHTS (Dtarget , fens ) (see Section 3.3) 

10: xtarget i = arg min x∈X LCB (x) (see acquisition function optimisation, Sec-tion 2.1) 

11: ytarget i = ϕ(xtarget i ) + ϵtarget (objective function evaluation) 

12: Dtarget = Dtarget ∪ { xtarget i , y target i }

13: end for Output Dtarget , xtarget best 

14: function CONSTRUCT ENSEMBLE ({D i}Ni=1 )

15: Initialise fens 

16: for i = 1 , .., N do 

17: Train GP on Di ({fi ∼ GP (μi, K i)}Ni=1 )

18: Append GP to fens 

19: end for 

20: return fens 

21: end function 

22: function UPDATE ENSEMBLE (Dtarget ,fens )

23: if ftarget ∈ fens then 

24: Remove ftarget 

25: end if 

26: Train GP on Dtarget (ftarget ∼ GP (μtarget , K target ))

27: Append target GP to fens 

28: return fens 

29: end function 

10 describes the construction of the ensemble of GP surrogate models evaluated at a particular input point as a normal distribution. Unlike the standard BO (no transfer learning) setting, where the target objective function is modeled using a single GP con-structed from the current observed dataset, in this transfer learning based BO approach, the target objective function is modeled using an ensemble of GPs that are constructed from the historic datasets to transfer information in the historic datasets to the new tar-get problem. Note that a surrogate model of the target objective function is included, and updated at each iteration. Inclusion of the target surrogate model helps to model the target objective function in cases where the source surrogate models are inadequate [62, 11]. 

# 3.2 Initialisation 

For the initialisation step (Lines 1-6 in Algorithm 1) we compare two approaches to generating initialisation points. The first of these is random generation of a set of 

n init initialisation points using Latin hypercube sampling. The second uses historic datasets to find a set of n init ws warm start initialisation points [34] (see Algorithm 2 for details). Both approaches use the implementation from [11]. We compare how choosing one or other of these initialisation approaches described impacts performance of standard BO (see Section 4 for setup details), and then the seven weighting schemes described in Section 3.3. 

Algorithm 2 Warm Start Initialisation 

Input Xcandidates = {∪{ xi,j }Mi

> j=1

}Ni=1 (union of historic dataset (inputs)), n init ws 

(number of required initialisation points), fens (pre-trained ensemble)  

> 1:

Xinits ← empty list  

> 2:

for p = 1 to n init ws do 

M eanBestEvals ← empty dictionary  

> 3:

for x in Xcandidates do  

> 4:

BestEvals ← empty list  

> 5:

for q = 1 to N do  

> 6:

BestEvals [q] ← min( μq (x), {μk(x)}p−1

> k=1

) 

> 7:

end for  

> 8:

M eanBestEvals [x] ← 1

> N

PNq=1 (BestEvals [q])  

> 9:

end for  

> 10:

Append x to Xinits which corresponds to min val in M eanBestEvals  

> 11:

end for Output Xinits 

11 3.3 Weighting Strategies for the Ensemble of Surrogate Models 

We consider seven different weighting strategies to find {wi}Ni=1 and wtarget for use with the ensemble of surrogate models (Equation 8). The first four weighting strategies involve regularised regression, which formulates the observed objective function val-ues, ytarget , to be equal to a weighted linear combination of predictions from the GPs in the ensemble. Regularised linear regression is used to find the appropriate contributing weight of each GP by minimising a loss function that includes a penalty on the weights as described in Section 2.3, and Equations 5 (for Ridge) and 6 (for Lasso). In literature, [34] uses stochastic gradient descent of the mean squared error loss function with L2 regularisation (equivalent to Ridge regularised regression). We extend this approach, and include not only Ridge regularised regression with an L2 penalty for its suitability to handle ill-posed problems [55, 58], but also Lasso with an L1 penalty for its ability to reduce model complexity by shrinking some weights to 0 [53]. We also include each of these two methods with a constraint on the weights, forcing them to be positive. The intuition behind constraining weights to be positive in the context of transfer learning is that we are using functions from historic tasks we believe to be similar to the target task. In the context of using regression with basis functions in the hypothesis space, we want to be able to positively or negatively weight the basis functions. However, in this context where we use related or similar functions in the hypothesis space, it is unlikely that the reflection of any function ( ϕ(x) ∗ (−1) ) will result in positive transfer of information to the target function. Therefore, constraining weights to be positive helps to avoid negative transfer of information (see Section 2.2). Equations 9 and 10, minimise 1

Mtarget Mtarget 

X

> j=1

(ytarget (xj ) − 

> N+1

X

> i=1

wiμi(xj )) 2 + α 

> N+1

X

> i=1

w2 

> i

(9) subject to wi >= 0 ∀i ∈ { 1, ..., N + 1 }

minimise 1

Mtarget Mtarget 

X

> j=1

(ytarget (xj ) − 

> N+1

X

> i=1

wiμi(xj )) 2 + α 

> N+1

X

> i=1

|wi| (10) subject to wi >= 0 ∀i ∈ { 1, ..., N + 1 }

describe the weighting loss function to be minimised for Ridge and Lasso when weights are constrained to be positive, where wi ∈ 1, ..., N + 1 represent the N + 1 

weights on all N source surrogate models plus 1 target surrogate model in the ensem-ble, μi(xj ) represents the predicted mean value from the ith surrogate model on the 

jth input point ( j ∈ 1, ..., M target where there are Mtarget input points), ytarget (xj )

is the target objective function true evaluation, and α represents the penalty hyperpa-rameter. Also, rather than just computing weightings on training data, we use bootstrap with 1000 samples. We use the abbreviation LaGPE for the Lasso Gaussian Process 12 ensemble methods, and RiGPE for Ridge Gaussian Process ensemble methods. In summary, we test four different regularised regression methods, LaGPE with a positive constraint on the weights, LaGPE without a positive constraint on the weights, RiGPE with a positive constraint on the weights and RiGPE without a positive constraint on the weights, to evaluate the effectiveness of the positive constraint. The remaining three weighting strategies tested have been extracted from approaches proposed in literature. The first of these is a ranking approach, Ranking-Weighted Gaussian Process Ensemble (RGPE) proposed by [11]. In this approach, weights are computed by ranking all source functions based on number of discordant pairs as com-pared to the target objective function. For all possible pairings of input points in the training data, the number of pairings where the surrogate mean prediction function in-crease or decrease is different to the objective target function are summed. Bootstrap with sampling of 1000 is used to compute a distribution over the number of times a par-ticular source surrogate model appears in the group of source models with minimum number of discordant pairings. Equation 11, 

Li,s (fi, Dtarget ) = 

> Mtarget

X

> j=1
> Mtarget

X

> k=1

1(( fi(xtarget j ) < f i(xtarget k )) ⊕(ytarget j < y target k )) ,

(11) shows the loss function used for the number of mis-ranked pairs between bootstrap sample s for the ith source task surrogate model predicted mean function and the target objective function. Equation 12, 

wi = 11000 

> 1000

X

> s=1

 I(i ∈ arg min {L j,s }N +1  

> j=1

)

PN +1  

> k=1

(k ∈ arg min {L k,s }N +1  

> k=1

)



, (12) shows how weights are computed using RGPE. See [11] for more details. The second of these is the Two-Stage Transfer Surrogate Model with Rankings (TSTR) method from [62], which uses the Nadaraya-Watson kernel weighted average to compute weights in the surrogate models in the ensemble as seen in Equation 13 

wi = kp(Xi, Xtarget )

PN +1  

> j=1

kp(Xi, Xtarget ) . (13) The X here indicates the datasets constructed from ensemble surrogate model mean predictions on the target dataset inputs or target dataset. Equation 14, 

kp(Xi, Xj ) = γ

 ||X i − X j || 2

ρ



(14) 

γ(r) = 

( 34 (1 − r2) if r ≤ 10 otherwise 13 is the exact Epanechnikov quadratic kernel used to compute distance between the surrogate model mean prediction evaluations and the target objective function evalua-tions. Again the number of discordant pairs is computed, but this time as the distance between the datasets Xi and Xtarget used in kernel. Here ρ is a bandwidth hyperpa-rameter that needs to be set by the user. The third literature weighting strategy, from [34], is the warm-starting algorithm configuration (WAC). Similar to our regularised regression weighting strategies, WAC instead uses stochastic gradient descent to minimise a mean squared error loss function with L2 regularisation to weight surrogate models in the ensemble. In contrast to all other methods, this method uses prediction uncertainty from all surrogate models in the ensemble in making a prediction 7, rather than only the target GP uncertainty. Additionally, each weighting strategy has different requirements in terms of hyper-parameter tuning and for early iterations. One of the challenges in using a Ridge or Lasso loss function is tuning the penalty coefficient, α. In this work, cross validation is used to learn the best α. The value of α is important in selecting which variables are correlated to the target function and contribute to predictions made on the target function [49]. However, in the context of BO, the dataset size is so small during early iterations that measurement of cross validation error 8 is unreliable. This was observed to lead to overly large estimations of α during early iterations resulting in a weighting of 0 on all source surrogate models. To address this problem, we pre-learn a reasonable guess for α for the target task by using the historic datasets in the following way. Iter-ating through all historic task datasets, we assign one task, i, as the pseudo-target task. Then we compute best α for the historic dataset Di, of that task using an ensemble of surrogate models training on remaining historic datasets, Dsource \ D i. We use Ridge or Lasso regression with Di as the training data and predictions from the ensemble as features, and find best α for historic task i using cross validation. After iterating through all historic tasks, we take the median of the best αs computed for all historic task datasets. See Algorithm 3 in Appendix A.1 for details. For the RGPE weighting strategy, there are no hyperparameter requiring tuning. However, in the implementation in [11], for the third iteration of the evaluation budget weights of 1 

> N

are used for all models in the ensemble. For TSTR the bandwidth hyper-parameter requires tuning. For convenience, we pick one value, 0.1, used as a default in the [11] implementation of this method, and use that for all benchmarks. Conse-quently, it is possible that TSTR may perform better if more tuning of bandwidth were undertaken. Finally, no special tuning or iterations were required for the WAC method.    

> 7Note that only the predicted mean values are used for training the weights. These weights are then used to compute a linear combination of predicted mean and variance for prediction (see GitHub repository from [11]).
> 8In sklearn LassoCV, used in this work, validation set error is computed using R2, which requires at least two validation points. If the number of splits is 3, at least 6 data points are required to use cross validation to find best α[37].

14 3.4 Strategy for Handling Bad Transfer Learning 

This component in the pipeline is designed to prevent the problem of wasting expensive evaluations exploring unfruitful regions of the input domain due to misguidance from source datasets and surrogate models. We look at two different design approaches for this component on a subset of methods described in the weighting strategies section (3.3). The first, a method designed to prevent ”weight dilution” of source surrogate models, was implemented in [11] and is applied here to the RGPE and TSTR methods 9.It involves computing a probability of dropping a particular model from the ensemble based on how it compares to the target surrogate model in the ensemble (comparing number of discordant pairs with the target objective function). Equation 15 provides details, 

pdrop i = 1 −



(1 − t

T )

PSs=1 1(Li,s < Ltarget,s )

S



, (15) where T indicated the evaluation budget, t the current iteration number, S the num-ber of bootstrap samples, and L the number of discordant pairs for surrogate model i

and bootstrap sample s. A key feature of this method is that the probability is tied to iteration number such that when the evaluation budget is exhausted, the probability of dropping a particular source surrogate model will be 1 if number of discordant pairs is greater than for the target GP. The second method, contributed as part of this work, is based on the mean squared error loss, and is applied to the LaGPE and RiGPE weighting strategy methods. It has been designed to not require an additional hyperparameter to tune. In extending the weighting strategies to include handling poor transfer learning, we assume that op-timisation can occur via two modes. There is mode one, using the transfer learning ensemble, or mode two using only the target GP. In this approach the average mean squared error computed during cross validation to find best regularisation hyperparam-eter is used to track quality of hypothesis space (ensemble surrogate models or target GP) in modelling the target objective function. The average mean squared error from the previous two iterations is converted to a probability of changing from current mode to the other mode, and multiplied by an indicator of model performance; 0 if previous iteration yielded the optimal evaluation so far and 1 if not. Hence, while the model is actively optimising, probability of changing mode will be 0. Equation 16, 

> 9In the implementation of these two methods provided by [11], this weight dilution prevention strategy is part of the pipeline. WAC is excluded from this analysis as in the implementation provided this component was not included for this method.

15 pchange = I(ytarget t−1 > min {ytarget k }t−2

> k=1

) ∗



max(0 , (mse t−1 − (mse t−1+mse t−2)2 )

> (mse t−1+mse t−2)2



(16) where 

mse = 1

K

> K

X

> k=1

 1

Mtarget /K 

> Mtarget /K

X

> j=1

(ytarget (xj ) − 

> N+1

X

> i=1

wiμi(xj )) 2



is used to compute this probability, where I represents the indicator function for model performance, and mse is the average computed mean squared error between objective function evaluations and the weighted linear combination of surrogate model mean predictions for each split, in K-fold cross validation at each iteration. One of the key differences between the two approaches, apart from using a dif-ferent loss metric, is that the weight dilution prevention strategy in [11] selectively eliminates source surrogate models individually, where as we propose a more abrupt method where we use all transfer learning BO or all standard BO. We leave a more in depth comparative analysis of these two approaches to another time. 

# 4 Experimental Setup 

This section provides details of experimental set up used by all methods. Then we provide details about the benchmarks we have selected. Finally we discuss metrics we use for evaluation. 

# 4.1 Setup for the Gaussian Process 

All weighting methods tested in this study use an ensemble of GP models. A GP is also the chosen surrogate model for standard BO methods described in Section 3. All GP models used in this study are set up with identical prior mean and kernel functions. We use a prior mean function of zero. The kernel used is a combination of a Mat´ ern kernel with ν = 2 .5 for continuous and integer variables, and a Hamming kernel for categorical variables. Independent, identically distributed Gaussian noise is modelled using a white noise kernel. This implementation follows those used in [33, 11]. 

# 4.2 Other Details 

For all methods use an an evaluation budget of 100 to see effect of certain choices more clearly than in work done previously ([11]). The historic datasets are constructed from 16 50 evaluations from the standard BO run in keeping with [11] implementation. For initialisation of the target dataset, we either use 10 randomly generated initialisaiton points (Latin hypercube sampling), or 2 warm start initialisation points (using historic datasets) [34]. We use LCB as the acquisition function because it is simple to compute and a popular acquisition function in the BO literature (see Section 2.1 for details). The BO package we use for this work is the SMAC package [33] to enable fair comparison with baseline implementations from [11], and SMAC settings were care-fully selected to be consistent with these implementations. All methods use the same kernel in single task GPs and have been adapted to use LCB acquisition function. 

# 4.3 Benchmarks 

In this work, we focus on parameter and hyperparameter tuning of a selection of ma-chine learning, regression and simulation black-box objective functions that can be either easily evaluated, or modelled using a surrogate model to provide approximate evaluations in a reasonable time frame. Suitable benchmarks for transfer learning in a BO context require a set of contextually related tasks with same input domain that each mimic an expensive black-box objective function as described in Section 2.4, while still being feasible in terms of runtime and memory to use for development and test-ing [9]. In the experimental set up proposed in [11] and also used in this work, each benchmark consists of N + 1 tasks, such that 1 task can be selected as the target task, leaving the remaining N as source tasks. We run each method for each of the N + 1 

tasks acting as target task on 15 seeds. Results are averaged over (N + 1) ∗ 15 runs for one method. This approach requires there to be no hierarchy or dependency amongst tasks. We use four different types of benchmarks for validation that meet these criteria. These include grid, surrogate, simulation, and real-time time series benchmarks. The grid benchmark, a neural network benchmark, contains 4 continuous vari-ables and 3 integer variables 10 . It was designed for multi-fidelity optimisation by [67] and adapted to the transfer learning scenario by [11]. It consists of 35 datasets, with each dataset containing 2000 randomly sampled hyperparameter configurations from funnel-shaped MLP nets [17]. For details about variables used in this benchmark please see [11, Appendix D, Table 14] and accompanying code. The five surrogate benchmarks consist of between 2 and 10 variables, with be-tween 2 and 10 variables being continuous, between 0 and 5 variables being integer (discrete), and up to 2 variables being categorical. Each surrogate benchmark con-sists of a set of 38 configuration datasets generated by running five different supervised machine learning algorithms on 38 classification tasks with 2000 different hyperparam-eter configurations. The five machine learning algorithms include svm, glmnet, rpart, 

> 10 For the grid benchmark, only a dataset of 2000 data points per task is available. This is used in place of the objective function. Therefore, recommended evaluation configurations must be match to their nearest data point using a strategy like nearest neighbours.

17 ranger and xgboost. For openml-glmnet there are 2 continuous variables. For openml-svm there are 1 categorical variable, 1 continuous variable, and 1 integer variable that are conditional on the categorical variable, and 1 additional continuous variable. For openml-rpart there is 1 continuous variable and 3 integer variables. For openml-ranger there are 3 continuous variables, 2 categorical variables and 1 integer variable. For openml-xgb (xgboost) there are 10 variables with 1 categorical variable, 3 continuous and 1 integer variable conditional upon the categorical variable, plus 1 integer vari-able and 4 continuous variables 11 . The objective value selected to be used for these benchmarks is auc 12 . The preparation of the surrogate benchmarks from these datasets involved training a Random Forest regression model on the available dataset so that it could be used to approximate the evaluations of the benchmark algorithm used to generate the dataset. Methodology for surrogate model construction implemented by [11] was from [9] 13 .The 38 classification tasks used to generate the 38 configuration datasets for each of the machine learning algorithms were selected from a larger set of 100 classifi-cation datasets available as the OpenML-100 benchmark suite [56, 4]. The subset of 38 datasets were selected to avoid missing values and because they have binary output. Originally proposed and used as a benchmark for BO with transfer learning by [38], the configuration datasets for these benchmarks implemented by [11], was prepared by [26]. Please note that since the proposal of these benchmarks using the OpenML-100 benchmark suite, the OpenML website ( https://docs.openml. org/benchmark/ ) now includes a comment recommending using the OpenML-CC18 benchmark suite rather than OpenML 100 which ”suffers from some teething issues” and ”may obfuscate interpretation of results”. For details about variables used in Openml100 benchmarks implemented by [11], please see Appendix A.2.1. As a result of OpenML recommendation to use the OpenML-CC18 benchmark suite, we propose a benchmark using the RandomForest machine learning algorithm [37] on a subset of OpenML-CC18 datasets [5]. This 10 dimensional benchmark con-sists of 3 continuous variables, 5 integer variables and 2 categorical variables. A ran-dom selection of 38 classification datasets was selected from OpenML-CC18 based on practical considerations around runtime. Input domain for each variable was selected using trial and error to ensure reasonable performance across all tasks. The evaluation metric used as black-box objective function output is accuracy. This metric was chosen to as it is appropriate for classification datasets with differing number of classes and 

> 11 The two benchmarks, openml-svm and openml-xgb, which include conditional hyperparameters, have a set of active hyperparameters that are conditional on the value of a categorical hyperparameter. For example, openml-svm has a categorical variable for kernel type which can take values; ’linear’, ’polynomial’, ’radial’. For ’linear’, no additional variable is required, for ’polynomial’ a degree variable is required, and for ’radial’ a gamma variable is required. The SMAC package used for this work handles these natively [33] according to approach described in [31].
> 12 Performance measures recorded by [26] included auc, accuracy, brier score and runtime. However, auc was used in the implementation by [11].
> 13 Two of the surrogate benchmarks described here, openml-rpart and openml-ranger, did not appear in the publication [11]. However, they are part of the datasets prepared in [26], and were included with methodol-ogy for use as a RandomForest surrogate benchmark implementation in the [11] GitHub repository

18 class imbalance. For a list of OpenML-CC18 task IDs please see Appendix A.2.2, and for more details about variables used please see Table 1. We also propose a benchmark constructed using LassoBench contributed by [68]. This benchmark consists of a version of Lasso regression model that requires one regu-larisation hyperparameter per feature in the regression dataset. In this work we use the implementation provided by [68], but adapt it to our own BO setup, and use data from another source appropriate to our transfer learning experimental requirements. The data we selected for regression is from the California Cooperative Oceanic Fisheries Investigations website ([19]) and consists of a truncated set of features from the Bottle Database. These 10 features include characteristics of sea-water collected including depth, salinity, oxygenation and other nutrients. The predicted variable is water tem-perature. This results in a benchmark with 10 tunable continuous hyperparameters for datasets with 10 features used to model the output variable in regression. This dataset was selected because it can be split into 59 different tasks consisting of different lo-cations off the coast of California from which the sea-water samples were collected. The metric used as black-box objective function evaluation output is validation mean squared error. For more details about variables used, please see Table 2 in Appendix A.2.3. The simulation benchmark, based on linear quadratic regulator (LQR) control of a cartpole, was designed especially for this study to provide an alternative to machine learning benchmarks. It is a 2 dimensional continuous variable benchmark, and the setup is adapted from a a multi-task learning scenario [36], to the transfer learning with BO context being studied in this work. The black-box objective function evaluation output is a cost function depending on the cart position, the pole angle, the voltage, summed over all the time steps of one simulation [36] (see Equation 18). Please see Appendix A.2.4 for more details and Table 3 for variable descriptions. 

# 4.4 Metrics 

In order to evaluate and compare performance of BO with transfer learning using an ensemble of surrogate models on the nine benchmarks described in Section 4.3, we select two performance metrics to construct plots for visual analysis of performance. These are normalised regret (Section 4.4.1) and ranking plots (Section 4.4.2). We also provide some analysis of the historic datasets for each benchmark (see Section 2.4 for description of historic datasets), with a view to gaining insight into relative locations of minima in the input domain for different tasks. For this we require a distance metric to compare distance between input data points (Section 4.4.3). 19 4.4.1 Normalised Regret 

The first evaluation metric used to compare different methods of BO using ensemble-based transfer learning is normalised simple regret 14 [62, 63, 40]. In a transfer learning setting, where we compare function surfaces from related but not identical tasks, differ-ent task objective functions may have a different output range over the selected input domain. Therefore, to compare average performance over all tasks, it is necessary to use normalised simple regret. Other examples of work using this metric include [61, 11, 62, 63, 24]. Plots in Section 5 show mean normalised simple regret over all seed and task combinations for a particular benchmark 15 .

4.4.2 Ranking Plots 

The second evaluation metric plots we use to compare different ensemble-based trans-fer learning with BO methods are ranking plots. They were also used in multiple other works including [46, 62, 10, 63, 11, 40] 16 . The resulting plots can be seen in Section 5. These ranking plots provide a different perspective on comparative performance between methods as compared to the normalised regret plots. While normalised re-gret may be impacted by variable performance across different seeds or tasks for one method compared to another 17 , ranking plots just show the average rank of a particular method over all tasks and seeds, irrespective of how big the difference is (comparing different methods) in performance on a particular seed and task. 

4.4.3 Metrics for Historic Dataset Analysis 

In addition to using metrics to compare performance of different BO with ensemble-based transfer learning methods, we also analyse historic datasets across all tasks with the aim of gaining insight into how comparative location of minima in different tasks impacts the performance of transfer learning with BO. Since we include a range of benchmarks with varying dimension and datatypes, comparison of input locations re-quires a distance metric that can handle different number of dimensions and datatypes. The distance metric we choose is Gower distance [18]. To compute Gower distance, a similarity score between 0 and 1 is first computed between two points by averaging the similarity between each dimension of the two points over all dimensions (see [18] for details). This enables flexibility for different variable types, with exact approach to computing similarity varying for qualitative versus quantitative variables. The simiar-lity score is then converted to a distance proportional to (1 − Si,j )1/2, where Si,j is 

> 14 This is equivalent to average distance to the global minimum (ADTM) used on grid benchmarks [62, 63]
> 15 For benchmarks with 38 tasks, and 15 seeds, this produces 570 seed-tasks over which to average nor-malised regret.
> 16 In this work we prepare these plots by computing rank separately for each seed and task combination and evaluation budget iteration across a set of methods we are comparing. Then for each method we plot the average rank across all seeds and tasks for each iteration.
> 17 If one method exhibits a large amount of variation in performance between different seeds or tasks it will impact the value of average normalised simple regret.

20 similarity between points i and j, as described in [18]. Using Gower distance, we can also obtain a clustering of points using agglomer-ative clustering or spectral clustering [37]. For this work we use agglomerative clus-tering, with distance threshold set to 0.02 and complete linkage. We also use spectral clustering with the same number of clusters discovered during agglomerative cluster-ing to validate clusters discovered with agglomerative clustering [64]. Clustering is experimental and requires validation. There are a number of approaches used, such as verification with other labels [64]. However, in this context, task labels and loca-tion clusters are independent. This is what we are trying to seek information about. Therefore, we use an alternative clustering algorithm as validation. This approach was chosen as a way to approximate overlapping input points that are minima in the historic dataset. It is based on the idea that if clusters are very small, the input points in them are approximately overlapping. If there exists an overlap of minima location between two tasks’ historic datasets (obtained using standard BO), one being a source task and one being the target task, then we assume it is likely that using a surrogate model trained on the source task’s historic dataset in the ensemble will lead to the discovery of that minima location in the target task during ensemble-based transfer learning BO. That means that in theory, transfer learning BO should perform at least as well on the target task as standard BO. Note that this is an analytical tool only, and not one that makes sense on a new problem, since for a real problem, you would not have access to the target task’s expensive black-box objective function historic dataset, but only to source task historic datasets. Using the clusters obtained as an approximation for data points overlapping, we also propose an approach to computing probability of approximate overlap between minima data points from different tasks’ historic datasets. Equation 17 shows how we calculate the probability of at least one overlapping minima between the historic dataset for task i and the union of historic datasets from all other tasks. The probability shown here is the probability of overlap for task i out of the N tasks, averaged over all seeds, where S is number of seeds used in the set of historic datasets and stask i refers to a seeds for task i, and cids (task i,s task i ) is the set of cluster ids in which task i, seed stask i

has at least one minima datapoint (and likewise for task j). An indicator function, I, is 

1 if intersection is the empty set between cluster ids in the task i, seed stask i , and task 

j, seed stask i , and other wise 018 . The product term computes probability of no overlap between task i and other tasks. The probability of at least one overlapping cluster is 1

minus this amount. The average over all seeds for task i is 

poverlap task i = 1

S

> S

X 

> stask i=1

"

1−

> N

Y 

> j=1 ,j ̸=i

PSstask j =1 I {cids (task i,s task i )} ∩ { cids (task j ,s task j )} ∈ ∅ 

S

!# 

.

(17)   

> 18 We are taking the sum over all seeds for task jfor which the intersect of cluster sets is empty.

21 5 Main Results 

This section responds to the three questions posed in Section 1.4. We address the first question in section 5.1. First we analyse the effectiveness of warm start initialisation as compared to random initialisation (Latin hypercube sampling) using normalised regret plots in Section 5.1.1. Then we compare performance of different weighting strategies with both warm start and random initialisation, again using normalised regret plots in Section 5.1.2. Then we address the second question by comparing weighting strategy methods with and without handling of poor transfer learning in Section 5.2. Normalised regret plots addressing this question can be found in Section 5.2.1, and in Section 5.2.2 we include ranking plots to provide further analysis of comparative performance of various methods included in this study. In Section 5.3 we include a box-plot summary of our analysis of historical data in response to the third question. 

# 5.1 Ensemble-Based Transfer Learning Strategies with Bayesian Optimisation 

In this section we present results to address the first question in section 1.4. We exam-ine the effectiveness of the warm start initialisation and compare different weighting strategies. 

5.1.1 Effectiveness of Warm Start 

Firstly we compare the effect of using 2 initialisation points obtained via the warm start algorithm (see Algorithm 2) against the effect of using 10 random initialisation points obtained using Latin hypercube sampling. The number of initialisation points chosen for each method is selected to be consistent with the implementation in [11]. We make this comparison for standard BO, and the ensemble-based transfer learning weighting strategy methods described in Section 3.3 which includes the following: LaGPE with positive weights constraint, LaGPE without positive weights constraint, RiGPE with positive weights constraint, RiGPE without positive weights constraint, RGPE, TSTR and WAC. For standard BO (see Figure 1), it can be observed that for most benchmarks, per-formance of BO is improved by using only 2 warm start initialisation points as com-pared to 10 random initialisation points. This is particularly evident during early it-erations, where the warm start initialisation points minimse normalised regret values considerably. Examples include for nn, openml-rpart, openml-xgb, randomforest and lassobench benchmarks, where the orange line representing warm start initialisation is much lower than the yellow line representing random initialisation. While for bench-marks openml-svm, openml-ranger and cartpole the performance of standard BO with warm start initialisation is not significantly better than with random initialisation, over the 100 iterations, it is comparable. The only benchmark where random initialisation with standard BO significantly out-performed the version with warm start initialisaiton 22 Figure 1: Standard BO plots with different initialisation procedures 

Figure 2: LaGPE plots with different initialisation procedures was openml-glmnet. Figure 2 shows results for the LaGPE weighting strategy with weights constrained to be positive. It can be seen that 6 out of 9 benchmarks (nn, openml-rpart, openml-ranger, openml-xgb, randomforest and lassobench) show improved BO performance over all 100 iterations when warm start initialisation is used in combination with the LaGPE weighting strategy as compared to random initialisation with the LaGPE weight-ing strategy or standard BO. For openml-glmnet, LaGPE with random initialisation is best during early iterations, but the warm start initialisation with LaGPE version works better in later iterations, and performs almost as well as standard BO over the 100 itera-tions. For openml-svm, all 3 methods show similar performance over the 100 iterations, and for cartpole, standard BO was better for most iterations than both LaGPE method versions (with warm start initialisation and with random initialisation), although by the 100th iteration, all 3 are very similar. Similar results can be seen for the LaGPE (without positive weights constraint) weighting strategy in Figure 3. For this method, 5 out of 9 benchmarks, nn, openml-rpart, openml-ranger, openml-xgb and randomforest, show significantly improved op-23 Figure 3: LaGPE (no constraint on weights) plots with different initialisation proce-dures timisation performance using warm start initialisation with LaGPE (without positive weights constraint) as compared to random initialisation with LaGPE (without posi-tive weights constraint) or standard BO. Out of the remaining benchmarks, openml-glmnet, openml-svm and lassobench show best performance for LaGPE (without pos-itive weights constraint) with warm start during initial iterations, but best performance for standard BO during later iterations. For cartpole, standard BO was the best per-forming method for all iterations. Then we look at the Ridge regression weighting strategy methods in Figure 4 with positive weights constraint and Figure 5 without positive weights constraint. For Ridge regression weighting strategy with positive weights constraint, 6 out of 9 bench-marks, nn, openml-rpart, openml-ranger, openml-xgb, randomforest, and lassobench, the warm start initialisation method show better overall optimisation performance than the random initialisation method, or standard BO. For benchmarks openml-glmnet and openml-svm, method without warm start initialisation (RiGPE and standard BO) show better optimisation performance. For cartpole, standard BO was again the best method of optimisation. For Ridge regression weighting strategy without positive weights constraint, again 6 out of 9 benchmarks, nn, openml-svm, openml-rpart, openml-ranger, openml-xgb, and randomforest showed better performance when warm start initialisation was used as compared to random initialisation or standard BO. This time, for lassobench, the warm start initialisation method performed best over the first few iterations, but then the random initialisation and standard BO methods improved more quickly. For openml-glmnet, standard BO was slightly better over most iterations (random initialisation was better initially), and for cartpole, standard BO was better than the RiGPE methods until the very last few iterations. Then we compare the effect of the two initialisation approaches for the RGPE weighting scheme in Figure 6. This time, 7 out of the 9 benchmarks show best perfor-mance over the 100 iterations with the warm start initialisation. For openml-glmnet, 24 Figure 4: RiGPE (positive weights constraint) plots with different initialisation proce-dures 

Figure 5: RiGPE (no weights constraint) plots with different initialisation procedures 25 Figure 6: RGPE plots with different initialisation procedures 

Figure 7: TSTR plots with different initialisation procedures BO performance was better without the warm start initialisation, and cartpole BO per-formance was similar for both. The TSTR method for weighting can be seen in Figure 7. For 8 out of 9 bench-marks, these plots show that the warm start initialisation performs at least as well as other methods. In particular during early iterations, using warm start initialisation sig-nificantly improves performance of BO as compared to using random initialisation. Finally we look at the WAC weighting strategy in Figure 8. Again during early iterations the warm start initilisation improves performance of BO. However, this im-provement only persists throughout the 100 iterations for 2 out of 9 benchmarks, nn and randomforest. For other benchamrks, standard BO performs better than both WAC with warm start initialisation and random initialisation, suggesting that in many cases standard BO may out-perform the WAC weighting strategy method of BO. 26 Figure 8: WAC plots with different initialisation procedures 

Figure 9: Plots comparing different weighting strategies using warm start initialisation 

5.1.2 Comparing Different Weighting Strategies 

In this section we look at how the different weighting strategies described in the the previous section (5.1.1) compare with each other and standard BO. We include two versions of our weighting strategies. In the first version in Figure 9, all weighting strategies use 2 warm start initialisation points. (Please note, these plots have been zoomed in more than the plots in Section 5.1.1.) For 2 out of 9 benchmarks, nn and openml-svm, BO with RGPE (without weight dilution prevention strategy) shows best performance. For another 2 out of the 9 benchmarks, openml-ranger and lassobench, BO using LaGPE (POS) (Lasso with positive weights constraint without alternating strategy) shows best performance, and for 1 out of the 9 benchmarks, cartpole, standard BO was the best optimisation strategy. For the other 4 out of 9 benchmarks, openml-glmnet, openml-rpart, openml-xgb and randomforest, various weighting strategies per-formed similarly and it is difficult to select the best. Also which weighting strategies were best on these 4 benchmarks varied throughout the iterations, with no clear winner overall. Given results overall, methods such as RGPE and LaGPE (POS) that enforce positive weights seem to be better than other methods. 27 Figure 10: Plots comparing different weighting strategies using random initialisation For plots showing comparison of the different weighting strategies when random initialisation ( 10 points) is used (in Figure 10), the majority of methods perform simi-larly overall across most benchmarks. While for lassobench RGPE is better than other methods overall, and for cartpole standard BO is better than other methods overall, for other benchmarks is it difficult to see one method that outperforms the others over the 100 iterations. Since we have already shown in the previous section (5.1.1) that, in general, BO methods using the warm start initialisation perform better, there is no need to further analyse plots showing the random initialisation versions. 

# 5.2 Comparing Methods With and Without a Strategy to Handle Bad Transfer Learning 

In this section we address the second question in section 1.4 by exploring how the in-clusion of a strategy for automated selection of standard BO versus transfer learning BO, based on measurable criteria available during optimisation, affects the overall per-formance of a Bayesian optimisation pipeline. The exact strategy used depends on the weighting strategy component of the pipeline as described in Section 3.4. 

5.2.1 Comparing Methods With and Without a Strategy to Handle Bad Transfer Learning Using Normalised Regret Plots 

For comparing performance with and without the component to handle bad transfer learning, we have included for analysis only the plots for weighting strategies that showed best performance for one or more benchmarks (see Figure 17 in Appendix A.3.1). The relevant plots are found in Figure 11 for LaGPE with positive constraint, Figure 12 for LaGPE without positive constraint and Figure 13 for RGPE. (See Ap-pendix section A.3.1 for plots with RiGPE with positive constraint, RiGPE without positive constraint, and TSTR weighting strategies.) We do this to understand how this component affects performance empirically on the benchmarks included in this study. 28 Figure 11: LaGPE (with positive constraint on the weights) plots with and without strategy to handle bad transfer learning 

Figure 12: LaGPE (without positive constraint on the weights) plots with and without strategy to handle bad transfer learning It can be seen in Figure 11 that the addition of our proposed component to handle bad transfer learning with the LaGPE weighting strategy has little impact on overall performance. For 4 out of 9 benchmarks, openml-rpart, openml-ranger, lassobench and openml-xgb, methods without the alternating strategy perform better overall. For 3 out of the 9 benchmarks, openml-glmnet, randomforest and cartpole, the alternating strategy makes little difference to the performance, and for the remaining 2 bench-marks, nn and openml-svm, the alternating strategy provides small improvements, as compared to methods without, in later iterations only. In Figure 12, showing plots for LaGPE (without positive weights constraint) meth-ods, results are similar as for LaGPE (with positive weights constraint). For 3 out of 9 benchmarks, openml-rpart, openml-ranger, and openml-xgb, methods without the alternating strategy clearly performed better. For 4 out of 9 benchmarks, nn, random-forest, lassobench and cartpole, the alternating strategy made no real difference, and for the remaining 2 out of 9 benchmarks, openml-glmnet and openlm-svm, there was some improvement in methods using the alternating strategy. Again, for methods using the RGPE weighting strategy in Figure 13, the compo-nent in the pipeline to handle bad transfer learning does not significantly improve over-29 Figure 13: RGPE plots with and without strategy to handle bad transfer learning all performance. In 4 out of 9 benchmarks, openml-rpart, openml-ranger, lassobench and openml-xgb, methods without weight dilution prevention perform better over the 100 iterations. For 4 out of 9 benchmarks, nn, openml-svm, randomforest and cartpole, the weight dilution prevention strategy appears to make no difference. Only for 1 out of the 9 benchmarks, openml-glmnet, does the weight dilution prevention strategy im-prove perfomance, and only in later iterations. It is interesting to note that the weight dilution prevention strategy to selectively eliminate source surrogate models (described in Section 3.4), did not appear to perform significantly better than our more abrupt pro-posed method for handling bad transfer learning. While there are definitely regions of optimisation where standard BO will perform better, as can be seen during early itera-tions of openml-glmnet in Figure 13, it is challenging to design an automated approach to detect and exploit this during optimisation. 

5.2.2 Comparing Methods With and Without a Strategy to Handle Bad Transfer Learning Using Ranking Plots 

In analysing the impact of the inclusion of a strategy for handling bad transfer learning in the transfer learning BO pipeline we also examine the ranking plots in Figure 14. These plots give an indication of how different methods compare in relation to each other, on average, without being affected by the size of the difference in normalised regret 19 . It can be seen that in general, results are similar to normalised regret plots, with some exceptions. For 1 out of 9 benchmarks, nn, RGPE (with or without weight dilution prevention) is the best ranking method overall. For 1 out of 9 benchmarks, openml-svm, RGPE without weight dilution, and TSTR with or without weight dilution prevention are the best ranking methods overall. For 1 out of 9 benchmarks, openml-glmnet, TSTR, with or without weight dilution prevention, is the best ranking method. For 2 out of 9 bench-

> 19 For example, if there is one particular task in a benchmark for which a particular method performs much worse on as compared to other tasks, it will affect the average normalised regret (which is affected by the size of difference) more than the averaged rank (which is affected by the rank integer only)

30 Figure 14: Plots comparing different methods’ average rank over evaluation budget (lower rank indicates better performing method) marks, openml-rpart and openml-xgb, LaGPE(POS) and RiGPE(POS), both without the alternating strategy, are the best ranking methods overall. For 2 out of 9 bench-marks, openml-ranger and randomforest, RiGPE(POS) without the alternating strategy is best ranking method. For 1 out of 9 benchmarks, lassobench, LaGPE(POS) with-out alternating strategy is the best ranking method. For cartpole, best ranking method varies between standard BO and RiGPE(POS) without the alternating strategy, depend-ing on iteration number. For all benchmarks it can be seen that during early iterations there is a lot of varia-tion in how different methods rank, where as during later iterations it is clearer which method performs best. Also, for some benchmarks such as randomforest, nn and cart-pole, the majority of methods have similar rank (ie. comparative performance may be more dependent on task number than the method itself), where as for other benchmarks such as openml-ranger and lassobench there is greater variation in method rankings suggesting that for these benchmarks, particular methods do perform differently over-all. General trends include better performance for methods that do not include strate-gies to handle bad transfer learning, the WAC method consistently ranks worse than other methods, and regularised regression methods (LaGPE and RiGPE) perform bet-ter when weights are constrained to be positive. In general, openml-rpart, openml-ranger, openml-xgb and lassobench perform well with methods that use a regularised regression weighting strategy, where as nn, openml-glmnet and openml-svm work well with a ranking based weights strategy. For random-forest results are not consistent for metrics, and while normalised regret models show better performance for RGPE methods, the ranking plot shows Ridge and Lasso re-gression models with positive weights constraint to rank better on average, particularly later during the evaluation budget, the ranking based methods. For cartpole, the value of transfer learning in general is not clear, with both normalised regret and ranking plots showing very competitive performance for the standard BO method. Overall, using positive weights seems to help performance, with positive weights constraint 31 versions of regularised regression, as well as RGPE and TSTR where weights are also guaranteed to be positive, being the best performing weighting strategies. 

# 5.3 Analysis of Historic Datasets 

The third question in Section 1.4 is about trying to predict whether ensemble-based transfer learning methods will improve performance of BO as compared to using stan-dard BO, and is based on only information available in historic datasets. In particular, we seek to gain insight into whether similar locations for minima between source and target functions in a benchmark (or minima discovered in historic datasets) implies better performance using ensemble-based transfer learning methods with BO as com-pared to when minima locations are not similar between source and target functions. Since, when deciding whether to use standard BO or ensemble-based transfer learn-ing BO methods, we assume access only to historic datasets (see Problem Satement in Section 2.4), and not the source functions themselves, we have designed our analysis to use only historic datasets from the benchmarks. We first filter all historic datasets to obtain minima points only, and then iteratively designate each task as target task with remaining tasks as source tasks in order to obtain an averged probability of ap-proximate location overlap of minima between source and target tasks for a particular benchmark using the approach described in Section 4.4.3 and Equation 17. In filtering historic datasets to obtain the minima, we observe that there are multiple minima lo-cations in each seed-task historic dataset where evaluations of the black-box objective function gave equal minimum values for different inputs. The box-plots showing this probability across all benchmarks on the x-axis can be seen in Figure 15. The y-axis is the probability of at least one overlap for a particular task. We chose to use boxplots to visualise the distribution of probabilities over all tasks in each benchmark. We present plots for both clustering algorithms, agglomerative and spectral, used to approximate locations to enable the reader to see how clustering output compares, given that cluster-ing is an experimental method requiring verification (see Section 4.4.3 for discussion about this). For benchmarks such as cartpole, openml-ranger and openml-xgb it can be seen that the probability of any particular task historic dataset having at least one overlap-ping minima with all other historic datasets is roughly similar for all tasks, where as for benchmarks such as openml-glmnet openml-rpart or randomforest, there is more difference between tasks in terms of how well their minima locations relate to other historic dataset minima locations. It can also be seen that, in general, benchmarks, nn, openml-rpart, randomforest, where standard BO performs significantly worse than transfer learning BO methods in both the normalised regret plots (see Figure 9) and the ranking plots (see Figure 14), show at least some tasks with high probability of at least one overlapping minima in the boxplots in Figure 15, suggesting that information in the historic datasets improves BO performance. However, the trend is not consistent for all benchmarks. For example, openml-svm appears to be an exception, with very high probability of overlapping minima, but sim-32 Figure 15: Boxplot showing distribution of probabilities over all tasks across all bench-marks. Benchmarks are ordered along the x-axis with 2 dimensional input on the left, increasing to 10 dimensional input on the right. ilarity in performance between standard BO and transfer learning BO methods in both normalised regret and ranking plots. This may be partly due to effective dimensionality of openml-svm being low (1d or 2d) due to conditionality of the categorical variable (see comments in Section 5.2). Also, for openml-xgb, performance of standard BO is significantly worse than ensemble-based transfer learning BO methods in both nor-malised regret plots (see Figure 9) and the ranking plots (see Figure 14), but the boxplot in Figure 15 suggests low probability of overlapping minima between target and source functions, which would be expected to lead to comparatively worse performance of the ensemble-based transfer learning BO methods as compared to standard BO. Note that for two dimensional benchmarks, openml-glmnet and cartpole, standard BO method is also very competitive with transfer learning BO methods, particularly in later itera-tions of the evaluation budget. In summary, it is evident that our approach to historic data analysis still requires refinement. Use of clustering in this approach is still ex-perimental and consequently may be leading to instability in results (see Section 4.4.3 for details). Scatter plots for clusters by tasks can be found in the Appendix (A.3.2) in Figure 21. 

# 6 Discussion & Conclusion 

# 6.1 Summary 

In this study we aim to empirically compare performance of a range of ensemble-based transfer learning BO pipelines with standard BO. Included are comparisons of specific components of the transfer learning BO pipeline. We explore results over nine bench-marks with dimensions ranging from 2 to 10 , some with mixed variables and others with all continuous variables. We also perform analysis on the historic datasets used in transfer learning to gain insight into the relationship between location of minima in different tasks, and comparative performance of ensemble-based transfer learning BO. We use several different evaluation metrics including normalised simple regret plots, ranking plots, and an approach using Gower distance and clustering to approximate probabilities of overlapping minima between task historic datasets. 33 In analysing results, we observe two key trends in performance of transfer learning BO pipeline components. Firstly, for initialisation of the optimisation process, warm start initialisation using an algorithm proposed in [34] in general improves perfor-mance of transfer learning BO as compared to using random initialisation over an evaluation budget of 100 iterations . Secondly, it is observed that for most benchmarks, 

a weighting strategy that forces weights to be positive is more effective than strate-gies that allow negative weights . Methods that only allowed positive weights include LaGPE (positive weights constraint), RiGPE (positive weights constraint), RGPE and TSTR. Finally, there is an observable trend towards lower dimensional benchmarks, and those with a smaller search space (categorical or integer variables with less than 10 values) exhibiting better BO performance with ranking based loss functions for com-puting best weights, and higher dimensional benchmarks with less categorical or inte-ger variables exhibiting better BO performance with regularised regression approaches to computing best weights. The scatter plot in Figure 16 shows each benchmark in terms of number of dimen-sions (x-axis), against proportion of approximately continuous dimensions (y-axis). The proportion of approximately continuous dimensions is defined as the proportion of variables that have at least 10 possible values (includes continuous variables). In this plot, benchmark points are coloured by category of weighting strategy that worked best overall in later iterations (approximately 30-100) of the ranking plots in Figure 14. According to the ranking plots, overall best BO performance of benchmarks openml-glmnet, openml-svm, and nn is observed when a ranking based weighting strategy is used to compute best weights with ensemble-based transfer learning, and overall best BO performance on benchmarks cartpole, openml-rpart, openml-ranger, openml-xgb, randomforest and lassobench is observed when regularised regression weighting strate-gies are used to compute best weights. There is one benchmark, nn, with 7 dimensions, that appears to contradict the trend noted between dimensions and type of weighting strategy. A possible explanation for this is that it is a grid benchmark. While there are 4 continuous variables, the search space is drastically reduced due to limited number of available evaluations (2000) 20 .

Less clear is the benefit of including a component in the ensemble-based transfer learning with BO pipeline to handle bad transfer learning . While this component is an important part of performance guarantees provided by [11] 21 , in our empirical analysis, it did not appear to provide significant improvement to performance. This was consis-tent with the equivalent component we proposed for use with the regularised regression weighting strategies. Also less clear is exactly which weighting strategy will work with a particular benchmark . While there is a vague trend towards ranking based weighting strategies working with lower dimensional benchmarks (openml-glmnet, openml-svm)  

> 20 This is roughly equivalent to 3 possible values per dimensions for 7 dimensions ( 37= 2187 ), which is much less than the cut off of 10 possible values for approximately continuous variables in the plot. While we have shown proportion of approximately continuous variables for nn benchmark 0.8, inreality it would be less than that due to it being a grid benchmark.
> 21 In [11], a guarantee is provided that the ensemble-based transfer learning with BO will not be slower than some multiple of standard BO.

34 Figure 16: Scatter plot showing relationship between number of dimensions and pro-portion of approximately continuous variables (¿ 10 possible values for integer vari-ables) for benchmarks. (Cartpole is excluded since standard BO worked best.) and/or benchmarks with more categorical variables (randomforest) or discretized input domains (including nn which is a grid benchmark, making it disretized in practice), and regularised regression based weighting strategies working better with benchmarks that have a higher number of dimensions and/or less categorical variables (openml-rpart, openml-ranger, openml-xgb and lassobench), there may be many other factors impact-ing performance and we do not have enough benchmarks to draw any firm conclusions around this. 

# 6.2 Limitations 

An obvious limitation to this work is the lack of mathematical relationship between source and target tasks. We use transfer learning in the hope that it will improve speed of convergence of BO. However, when applying this technique to a new benchmark, there is no guarantee that ensemble-based transfer learning will provide better perfor-mance of BO. Our experimental setup involves averaging over a number seeds and tasks and requires more work to find a way to apply with guarantees for a new problem where we do not have access to all historic tasks for cross validation. Another limitation arising from this is that the insights from our empirical study are not guaranteed to generalize to other benchmarks. While we have found some general trends that may relate to a wider set of benchmarks, more work is required to under-stand how specific pipeline components can be most effectively applied to particular characteristics of objective functions from different fields. Finally, given that these experiments are all averaged over seed task combinations, there is no guarantee that it will work well for a particular target task. The experimen-tal approach used here produces results based on averaging over all available tasks in a benchmark, and 15 seeds for each task. During development, we noted that perfor-35 mance can vary significantly for different tasks and seeds. In developing the approaches proposed here for a real optimisation problem we would like to know whether to use standard BO or ensemble-based transfer learning with BO on a new task for which we only have source historic datasets, and no access to source objective functions. However, without information about how various optimisation methods performed on historic tasks, it would be difficult to guess the best approach. This problem was the motivation for our developing an approach to historic data analysis. While this ap-proach shows some promise, there are still development required to make this approach more reliable. 

# 6.3 Future Work 

In view of the limitations described, we recommend two specific areas of future work. These are firstly the development of a better strategy for detecting and handling bad transfer learning during optimisation. This is particularly challenging with small amounts of data. Secondly, further development of our proposed approach to analysing his-toric data is required to improve accuracy of predictions about effectiveness of transfer learning given access historic (source) datasets only and not historic (source) objective functions. Our method for using clustering may be contributing to inaccuracies in ap-proximating location of minima, and further investigation is required. 

# 7 Acknowledgements 

The data used to construct our proposed benchmark, lassobench, obtained from the California Cooperative Oceanic Fisheries Investigations, is available at their website, 

https://calcofi.org/ .

# A Important Extra Details 

# A.1 Algorithm for Pre-learning Regularisation Hyperparameters A.2 More Benchmark Details 

In this section we provide details of variable name, range, and type for benchmarks. For those benchmarks 

A.2.1 OpenML100 Further Details 

For openml-glmnet benchmark details see [11, Appendix D, Table 9]. For openml-svm benchmark details see [11, Appendix D, Table 10]. 36 Algorithm 3 PreLearnAlpha 

Input Dtask = {D 1, ..., DN } for total N tasks, evaluation budget T 

> 1:

Initialisation 

a. Pre-train ensemble of GPs, fi ∼ GP (μi, K i) for i ∈ 1, ..., N 

b. Initialise empty alphas list  

> 2:

for t = 1,2,...,N do  

> 3:

Let t be pseudo-target task  

> 4:

Training dataset is Dpseudo −target = {xpseudo −target,j , y pseudo −target,j }T 

> j=1
> 5:

Compute best α for Lasso loss function using {fi|i̸ = t} as source functions (see Eq 6) using cross validation on training dataset  

> 6:

Append best α to alphas list  

> 7:

end for Output Median α from alphas list For openml-xgb benchmark details see [11, Appendix D, Table 11]. For openml-rpart benchmark details see [26, Table 1]. For openml-ranger benchmark details see [26, Table 1]. For implementation details of OpenML100 benchmarks see code accompanying [11] at https://github.com/automl/transfer-hpo-framework/ .

A.2.2 RandomForest (OpenML-CC18) Benchmark 

The task IDs for OpenML-CC18 tasks included in our RandomForest benchmark are as follows: 1510, 40668, 41027, 40701, 1468, 1461, 4538, 32, 11, 1063, 40994, 14, 31, 1067, 1475, 1480, 4534, 1068, 22, 46, 469, 1501, 40975, 1494, 1497, 40982, 182, 40984, 1464, 50, 1462, 16, 54, 307, 40978, 23, 44, 40983. Table 1 provides specific details of variables in Randomforest. 

A.2.3 LassoBench Benchmark 

Table 2 provides specific details of variables in our LassoBench benchmark. 

A.2.4 Cartpole Benchmark 

The cartpole benchmark, contributed as part of this work, was adapted from the setup descibed in [36] which used Bayesian optimisation over multiple information sources (simulation and physical model), to the transfer learning with BO context. We construct this benchmark as an example (an extension of a synthetic benchmark), rather than claiming it to be inately useful in the context of control. While [36] uses multi-task learning scenario to swap between evaluations of real and simulated cartpole, this paper uses the parameters provided by the Quanser User Manual, accessed via [43, 42], for 37 Name Range Variable Type n estimators [1,200] integer max depth [1,200] integer min samples split [2,10] integer min samples leaf [1,5] integer max leaf nodes [100,3000] integer min weight fraction leaf [0.0,0.5] continuous ccp alpha [0.0,0.5] continuous min impurity decrease [0.0,0.5] continuous criterion [’gini’, ’entropy’, ’log loss’] categorical max features [None, ’sqrt’, ’log2’] categorical Table 1: Randomforest Benchmark Variables Name Range Variable Type 

α1 [-1,1] continuous 

α2 [-1,1] continuous 

α3 [-1,1] continuous 

α4 [-1,1] continuous 

α5 [-1,1] continuous 

α6 [-1,1] continuous 

α7 [-1,1] continuous 

α8 [-1,1] continuous 

α9 [-1,1] continuous 

α10 [-1,1] continuous Table 2: Lassobench Benchmark Variables the real cartpole in [36], to simulate a range of similar cartpoles with LQR controller. Parameters for similar cartpoles are sampled from a uniform distribution (see [54] for a similar idea with synthetic functions) with ranges as shown in Table 4. The loss function used as the black-box objective function, from [36], is 

J = 1

K

> K−1

X

> K=0

s2 

> k

+ ψ2 

> k

+ ˙s2 

> k

+ 0 .1 ˙ψ2 

> k

+ 10 −5u2

> k

. (18) In this equation, J is used to denote cost, s is the position of the cart, ψ is the angle of the pole and u is the control variable (voltage).Algorithm 4 provides a rough outline for this benchmark, and Table 3 describes the BO variables. The variables, θ1 and θ2

are tuning parameters for the Q = diag (10 θ1 , 1, 1, 0.1) and R = 10 −θ2 LQR weights. * We used [14] control package for this. ** Simulation used cartpole equations derived based on description in [51, Chapter 3] 38 Algorithm 4 Cartpole 

Input cart mass, pole mass, pole length, cart friction, pole friction, evaluation budget 

T , number simulation time steps K 

> 1:

Initialisation  

> 2:

Compute A,B (linearised version of dynamics)  

> 3:

Initialise empty evaluations list, evals  

> 4:

Initialise Q,R (LQR weights)  

> 5:

Initialise simulation state vector, x 

> 6:

for i = 1 , 2, ..., T do  

> 7:

Compute K = LQR (A, B, Q, R )* 

> 8:

loss ← 0 

> 9:

for k = 1 , 2, ..., K do  

> 10:

Update u = K ∗ x 

> 11:

loss + = COMPUTE LOSS (x, u ) (see Equation 18)  

> 12:

Update x = SIMULATION STEP (x, u )**  

> 13:

end for  

> 14:

evals [i] = loss  

> 15:

Update BO surrogate model (standard BO or transfer learning BO)  

> 16:

Update Q,R by optimising acquisition function  

> 17:

end for Output evals 

Name Range Variable Type 

θ1 [-3,2] continuous 

θ2 [1,5] continuous Table 3: Cartpole Benchmark Variables 

# A.3 Some Additional Plots for Main Results 

A.3.1 Additional Normalised Regret Plots for Handling Bad Transfer Learning 

The section contains extra plots for comparing methods with and without a strategy for handling poor transfer learning. RiGPE with positive constraint, RiGPE without positive constraint, and TSTR are included here. See Section 5.2 for related plots com-paring weighting strategies inclusive of strategy for handling poor transfer learning. 

A.3.2 Scatter Plots for Analysis of Historic Data 

Due to the experimental nature of clustering [64], and the absence of suitable labels for validating the use of agglomerative clustering, we also include spectral clustering, with number of clusters set to be the same as that discovered by agglomerative clustering with distance threshold of 0.02 and complete linkage. Scatter plots for each bench-mark with agglomerative clustering on the left and spectral clustering on the right are 39 Figure 17: Plots comparing different weighting strategies using full pipeline 

Figure 18: RiGPE (with positive constraint on the weights) plots with and without strategy to handle bad transfer learning 

Figure 19: RiGPE (no positive constraint on weights) plots with and without strategy to handle bad transfer learning 40 Name Range cart mass [0.1,0.5] pole mass [0.01,0.25] pole length [0.25,0.75] cart friction [0.0001, 0.001] pole friction [0.001, 0.01] Table 4: Cartpole Simulation Parameters 

Figure 20: TSTR plots with and without strategy to handle bad transfer learning shown in Figure 21. In these plots, the x-axis is seed-task, and y-axis is cluster number. While numbering of clusters will not be in identical order using the two clustering al-gorithms, the overall clustering patterns can be seen to be similar for most benchmarks. 41 (a) (b)         

> (c) (d)
> (e) (f)
> (g) (h)
> (i) (j)
> (k) (l)
> (m) (n)
> (o) (p)
> (q) (r)

Figure 21: Scatter plots for Agglomerative Clustering and Spectral Clustering. x-axis is seed-task and y-axis is cluster number. 42 References 

[1] T. Bai, Y. Li, Y. Shen, X. Zhang, W. Zhang, and B. Cui. Transfer learning for bayesian optimization: A survey. arXiv preprint arXiv:2302.05927 , 2023. [2] R. Bardenet, M. Brendel, B. K´ egl, and M. Sebag. Collaborative hyperparameter tuning. In International conference on machine learning , pages 199–207. PMLR, 2013. [3] B. Bischl, M. Binder, M. Lang, T. Pielok, J. Richter, S. Coors, J. Thomas, T. Ull-mann, M. Becker, A.-L. Boulesteix, et al. Hyperparameter optimization: Foun-dations, algorithms, best practices, and open challenges. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery , 13(2):e1484, 2023. [4] B. Bischl, G. Casalicchio, M. Feurer, F. Hutter, M. Lang, R. G. Mantovani, J. N. Van Rijn, and J. Vanschoren. Openml benchmarking suites and the openml100. 

stat , 1050(11):97, 2017. [5] B. Bischl, G. Casalicchio, M. Feurer, F. Hutter, M. Lang, R. G. Mantovani, J. N. van Rijn, and J. Vanschoren. Openml benchmarking suites. arXiv:1708.03731v2 [stat.ML] , 2019. [6] E. Brochu, V. M. Cora, and N. de Freitas. A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchi-cal reinforcement learning. arXiv.org , 2010. [7] S. Daulton, X. Wan, , D. Eriksson, M. Balandat, M. A. Osborne, and E. Bakshy. Bayesian optimization over discrete and mixed spaces via probabilistic reparam-eterization. In Advances in Neural Information Processing Systems 35 , 2022. [8] M. Diessner, K. J. Wilson, and R. D. Whalley. On the development of a practical bayesian optimization algorithm for expensive experiments and simulations with changing environmental conditions. Data-Centric Engineering , 5:e45, 2024. [9] K. Eggensperger, F. Hutter, H. Hoos, and K. Leyton-Brown. Efficient bench-marking of hyperparameter optimizers via surrogates. In Proceedings of the AAAI conference on artificial intelligence , volume 29, 2015. [10] M. Feurer. Scalable meta-learning for bayesian optimization using ranking-weighted gaussian process ensembles. In AutoML Workshop at ICML , 2018. [11] M. Feurer, B. Letham, F. Hutter, and E. Bakshy. Practical transfer learning for bayesian optimization, 2022. [12] M. Feurer, J. Springenberg, and F. Hutter. Initializing bayesian hyperparameter optimization via meta-learning. Proceedings of the AAAI Conference on Artificial Intelligence , 29(1), Feb. 2015. [13] P. I. Frazier. A tutorial on bayesian optimization. arXiv.org , 2018. 43 [14] S. Fuller, B. Greiner, J. Moore, R. Murray, R. van Paassen, and R. Yorke. The python control systems library (python-control). In 60th IEEE Conference on Decision and Control (CDC) , pages 4875–4881. IEEE, 2021. [15] R. Garnett. Bayesian optimization . Cambridge University Press, Cambridge, United Kingdom ;, 2023. [16] R. Garnett, M. A. Osborne, and S. J. Roberts. Bayesian optimization for sensor set selection. In Proceedings of the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks , IPSN ’10, page 209–219, New York, NY, USA, 2010. Association for Computing Machinery. [17] P. Gijsbers, E. LeDell, J. Thomas, S. Poirier, B. Bischl, and J. Vanschoren. An open source automl benchmark. arXiv preprint arXiv:1907.00909 , 2019. [18] J. C. Gower. A general coefficient of similarity and some of its properties. Bio-metrics , pages 857–871, 1971. [19] C. S. T. Group. California cooperative oceanic fisheries investigations. https: //calcofi.org/ , 2025. Accessed: 2025-10-03. [20] M. L. H¨ anel and C.-B. Sch¨ onlieb. Efficient global optimization of non-differentiable, symmetric objectives for multi camera placement. IEEE Sensors Journal , 22(6):5278–5287, 2021. [21] T. Hastie, R. Tibshirani, J. Friedman, et al. The elements of statistical learning, 2009. [22] R. J. Hickman, J. Ruˇ za, H. Tribukait, L. M. Roch, and A. Garc´ ıa-Dur´ an. Equip-ping data-driven experiment planning for self-driving laboratories with semantic memory: case studies of transfer learning in chemical reaction optimization. Re-action Chemistry & Engineering , 8(9):2284–2296, 2023. [23] A. Jaradat, B. Matalkeh, and W. Diabat. Solving traveling salesman problem us-ing firefly algorithm and k-means clustering. In 2019 IEEE Jordan International Joint Conference on Electrical Engineering and Information Technology (JEEIT) ,pages 586–589, 2019. [24] H. S. Jomaa, S. P. Arango, L. Schmidt-Thieme, and J. Grabocka. Transfer learn-ing for bayesian hpo with end-to-end landmark meta-features. In Fifth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems ,2021. [25] D. R. Jones, M. Schonlau, and W. J. Welch. Efficient global optimization of ex-pensive black-box functions. Journal of Global optimization , 13:455–492, 1998. [26] D. K¨ uhn, P. Probst, J. Thomas, and B. Bischl. Automatic exploration of machine learning experiments on openml. arXiv preprint arXiv:1806.10961 , 2018. 44 [27] H. J. Kushner. A versatile stochastic model of a function of unknown and time varying form. Journal of Mathematical Analysis and Applications , 5:150–167, 1962. [28] H. J. Kushner. A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. 1964. [29] A. Lacoste, M. Marchand, F. Laviolette, and H. Larochelle. Agnostic bayesian learning of ensembles. In E. P. Xing and T. Jebara, editors, Proceedings of the 31st International Conference on Machine Learning , volume 32 of Proceedings of Machine Learning Research , pages 611–619, Bejing, China, 22–24 Jun 2014. PMLR. [30] T. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Ad-vances in Applied Mathematics , 6(1):4–22, 1985. [31] J.-C. L´ evesque, A. Durand, C. Gagn´ e, and R. Sabourin. Bayesian optimization for conditional hyperparameter spaces. In 2017 International Joint Conference on Neural Networks (IJCNN) , pages 286–293. IEEE, 2017. [32] X. Li, K. Wu, X. Zhang, and H. Wang. B2opt: Learning to optimize black-box optimization with little budget. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 39, pages 18502–18510, 2025. [33] M. Lindauer, K. Eggensperger, M. Feurer, A. Biedenkapp, D. Deng, C. Ben-jamins, T. Ruhkopf, R. Sass, and F. Hutter. Smac3: A versatile bayesian opti-mization package for hyperparameter optimization. Journal of Machine Learning Research , 23(54):1–9, 2022. [34] M. Lindauer and F. Hutter. Warmstarting of model-based algorithm configura-tion. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018. [35] N. Mahboubi, J. Xie, and B. Huang. Point-by-point transfer learning for bayesian optimization: An accelerated search strategy. Computers & Chemical Engineer-ing , 194:108952, 2025. [36] A. Marco, F. Berkenkamp, P. Hennig, A. P. Schoellig, A. Krause, S. Schaal, and S. Trimpe. Virtual vs. real: Trading off simulations and physical experiments in reinforcement learning with bayesian optimization. In 2017 IEEE International Conference on Robotics and Automation (ICRA) , pages 1557–1563. IEEE, 2017. [37] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011. [38] V. Perrone, R. Jenatton, M. W. Seeger, and C. Archambeau. Scalable hyperpa-rameter transfer learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Pro-cessing Systems , volume 31. Curran Associates, Inc., 2018. 45 [39] V. Perrone, H. Shen, M. W. Seeger, C. Archambeau, and R. Jenatton. Learning search spaces for bayesian optimization: Another view of hyperparameter transfer learning. Advances in neural information processing systems , 32, 2019. [40] S. Pineda-Arango, H. S. Jomaa, M. Wistuba, and J. Grabocka. HPO-B: A large-scale reproducible benchmark for black-box HPO based on openml. Neural Infor-mation Processing Systems (NeurIPS) Track on Datasets and Benchmarks , 2021. [41] M. Poloczek, J. Wang, and P. I. Frazier. Warm starting bayesian optimization. In 

2016 Winter simulation conference (WSC) , pages 770–781. IEEE, 2016. [42] Quanser. Ip01 and ip02 user manual. https://web.itu.edu.tr/ mutlui/IP01_2%20User%20Manual.pdf .[43] Quanser. Single inverted pendulum (sip) user manual. https: //nps.edu/documents/105873337/0/IP01_2+SIP+User+ Manual_511.pdf/3e88a8f6-94df-4697-8b90-4f3b26324f70 .[44] C. E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning .MIT Press, 2006. [45] H. Ren and Y. Sun. Building energy flexibility: modeling and optimization. In 

Building Energy Flexibility and Demand Management , pages 41–62. Elsevier, 2023. [46] N. Schilling, M. Wistuba, and L. Schmidt-Thieme. Scalable hyperparameter op-timization with products of gaussian process experts. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16 , pages 33– 48. Springer, 2016. [47] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2016. [48] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimiza-tion in the bandit setting: No regret and experimental design. In ICML 2010 -Proceedings, 27th International Conference on Machine Learning , pages 1015– 1022, 2010. [49] W. Su, M. Bogdan, and E. Candes. False discoveries occur early on the lasso path. The Annals of statistics , pages 2133–2150, 2017. [50] K. Swersky, J. Snoek, and R. P. Adams. Multi-task bayesian optimization. Ad-vances in neural information processing systems , 26, 2013. [51] R. Tedrake. Underactuated robots. https://underactuated.csail. mit.edu/index.html , 2024. Accessed: 2025-06-26. [52] K. Terayama, M. Sumita, R. Tamura, and K. Tsuda. Black-box optimization for automated discovery. Accounts of Chemical Research , 54(6):1334–1346, 2021. 46 [53] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology , 58(1):267–288, 1996. [54] P. Tighineanu, K. Skubch, P. Baireuther, A. Reiss, F. Berkenkamp, and J. Vino-gradska. Transfer learning with gaussian processes for bayesian optimization. In International Conference on Artificial Intelligence and Statistics , pages 6152– 6181. PMLR, 2022. [55] N. Tikhonov, A. V. Goncharsky, A. V. Stepanov, V. and G. Yagola, A.˙In Numer-ical Methods for the Solution of Ill-Posed Problems , volume 328 of Mathematics and Its Applications , pages 65–79. Springer, Dordrecht, 1995. [56] J. Vanschoren, J. N. Van Rijn, B. Bischl, and L. Torgo. Openml: networked science in machine learning. ACM SIGKDD Explorations Newsletter , 15(2):49– 60, 2014. [57] J. Waczak. Physical Sensing and Physics-Based Machine Learning for Actionable Environmental Insights . The University of Texas at Dallas, 2024. [58] G. Wahba and Y. Wang. Representer theorem. Wiley StatsRef: Statistics Reference Online , 1155:1178, 2019. [59] Z. Wang, G. E. Dahl, K. Swersky, C. Lee, Z. Nado, J. Gilmer, J. Snoek, and Z. Ghahramani. Pre-trained gaussian processes for bayesian optimization. Jour-nal of Machine Learning Research , 25(212):1–83, 2024. [60] Z. Wang, Z. Dai, B. P´ oczos, and J. Carbonell. Characterizing and avoiding nega-tive transfer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 11293–11302, 2019. [61] M. Wistuba and J. Grabocka. Few-shot bayesian optimization with deep kernel surrogates. arXiv preprint arXiv:2101.07667 , 2021. [62] M. Wistuba, N. Schilling, and L. Schmidt-Thieme. Two-stage transfer surro-gate model for automatic hyperparameter optimization. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16 , pages 199– 214. Springer, 2016. [63] M. Wistuba, N. Schilling, and L. Schmidt-Thieme. Scalable gaussian process-based transfer surrogates for hyperparameter optimization. Machine Learning ,107(1):43–78, 2018. [64] H. Xiong and Z. Li. Clustering validation measures. In Data clustering , pages 571–606. Chapman and Hall/CRC, 2018. [65] D. Yogatama and G. Mann. Efficient transfer learning method for automatic hy-perparameter tuning. In Artificial intelligence and statistics , pages 1077–1085. PMLR, 2014. 47 [66] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He. Acomprehensive survey on transfer learning. Proceedings of the IEEE , 109(1):43– 76, 2020. [67] L. Zimmer, M. Lindauer, and F. Hutter. Auto-pytorch: Multi-fidelity metalearn-ing for efficient and robust autodl. IEEE Transactions on Pattern Analysis and Machine Intelligence , 43(9):3079–3090, 2021. [68] S. J. ˇ Sehi´ c Kenan, Gramfort Alexandre and N. Luigi. Lassobench: A high-dimensional hyperparameter optimization benchmark suite for lasso. In Proceed-ings of the 1st International Conference on Automated Machine Learning , pages 2–1, 2022. 48