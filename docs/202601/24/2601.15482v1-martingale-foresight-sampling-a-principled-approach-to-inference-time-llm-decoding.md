<div class="paper-title-row">
<h1 class="paper-title-zh">鞅前瞻采样：一种规范的大语言模型推理阶段解码方法</h1>
<h1 class="paper-title-en">Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding</h1>
</div>

<div class="paper-meta-row">
<div class="paper-meta-left">
<p><strong>Authors</strong>: Huayu Li, ZhengXiao He, Siyuan Tian, Jinghao Wen, Ao Li</p>
<p><strong>Date</strong>: 2026-01-21</p>
<p><strong>PDF</strong>: <a href="https://arxiv.org/pdf/2601.15482v1" target="_blank">https://arxiv.org/pdf/2601.15482v1</a></p>
<p><strong>Tags</strong>: <span class="tag-label tag-green">LNS</span> <span class="tag-label tag-green">EAA</span></p>
<p><strong>Score</strong>: 6.0</p>
</div>
<div class="paper-meta-right">
<p><strong>Evidence</strong>: LLM 解码中原则性的搜索空间剪枝和路径评估</p>
<p><strong>TLDR</strong>: 针对大语言模型自回归解码存在的短视问题及现有前瞻采样方法依赖启发式规则的局限，本文提出了鞅前瞻采样（MFS）框架。该框架将解码建模为随机过程，创新性地引入鞅论中的Doob分解、可选停止定理和收敛定理，实现了路径价值的科学评估、分支剪枝及自适应停止。实验表明，MFS在六项推理任务中不仅显著提升了准确率，还大幅优化了计算效率。</p>
</div>
</div>

<div class="paper-glance-section">
<h2 class="paper-glance-title">速览</h2>
<div class="paper-glance-row">
<div class="paper-glance-col">
<div class="paper-glance-label">Motivation</div>
<div class="paper-glance-content">传统的自回归解码由于逐词生成的特性缺乏全局最优视野，且现有的前瞻采样策略往往依赖于缺乏理论支撑的启发式规则。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Method</div>
<div class="paper-glance-content">提出MFS框架，利用Doob分解衡量路径的可预测优势，结合可选停止定理进行剪枝，并依据鞅收敛定理实现自适应停止。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Result</div>
<div class="paper-glance-content">在六个推理基准测试中，MFS在准确率上超越了现有最先进方法，同时显著提高了推理过程中的计算效率。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Conclusion</div>
<div class="paper-glance-content">MFS为大语言模型推理阶段的解码提供了一个理论完备且高效的随机过程建模方案，证明了概率论原则在优化搜索路径中的有效性。</div>
</div>
</div>
</div>

---


## 摘要
大语言模型（LLM）中的标准自回归解码本质上是短视的，由于其逐标记（token-by-token）生成过程，往往无法找到全局最优的推理路径。虽然像前瞻采样（foresight sampling）这样的推理阶段策略试图通过模拟未来步骤来缓解这一问题，但它们通常依赖于启发式方法来评估路径价值和剪枝搜索空间。本文介绍了鞅前瞻采样（MFS），这是一个将 LLM 解码重新表述为识别最优随机过程问题的规范框架。通过将推理路径的质量建模为随机过程，我们利用鞅理论设计了一种具有理论基础的算法。我们的方法用概率论原理取代了启发式机制：步骤估值源自 Doob 分解定理（Doob Decomposition Theorem），用于衡量路径的可预测优势；路径选择利用可选停止理论（Optional Stopping Theory）对次优候选路径进行规范化剪枝；基于鞅收敛定理（Martingale Convergence Theorem）的自适应停止规则在路径质量被证明收敛时终止探索。在六个推理基准测试上的实验表明，MFS 在准确率上超越了现有最先进的方法，同时显著提高了计算效率。代码将发布于 https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling。
## Abstract
Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.