Title: Designing faster mixed integer linear programming algorithm via learning the optimal path

URL Source: https://arxiv.org/pdf/2601.16056v1

Published Time: Fri, 23 Jan 2026 01:56:09 GMT

Number of Pages: 28

Markdown Content:
# Designing faster mixed integer linear programming algorithm via learning the optimal path 

Ruizhi Liu 1,2 â€ , Liming Xu 1,2 â€ , Xulin Huang 1,2 â€ , Jingyan Sui 1,2,3 ,Shizhe Ding 1,2 , Boyang Xia 1,2 , Chungong Yu 1,2,4 , Dongbo Bu 1,2,4* 1SKLP, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China. 

2University of Chinese Academy of Science, Beijing, 100049, China. 

3School of Computer Science, Liaocheng University, Liaocheng, 252000, China. 

4Central China Research Institute for Artificial Intelligence Technologies, Henan Academy of Sciences, Zhengzhou, 450046, China. *Corresponding author(s). E-mail(s): dbu@ict.ac.cn; Contributing authors: liuruizhi19s@ict.ac.cn; limingxu0713@gmail.com; huangxulin@gs.zzu.edu.cn; suijingyan18@mails.ucas.ac.cn; dingshizhe15@mails.ucas.ac.cn; xiaboyang20@mails.ucas.ac.cn; yuchungong@ict.ac.cn; 

â€ These authors contributed equally to this work. 

Abstract 

Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP prob-lem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpre-dictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algo-rithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solu-tion, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise train-ing paradigm that enhances the modelâ€™s ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with 

1

> arXiv:2601.16056v1 [cs.AI] 22 Jan 2026

significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flex-ible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.  

> Keywords: mixed integer linear programming, neural network, node selection

21 Introduction 

Mixed-Integer Linear Programming (MILP) serves as a fundamental mathematical modeling tool to address complex real-world combinatorial optimization challenges, including vehicle routing [1â€“3], integrated circuit design [4, 5], logistics operations[6, 7] and resource allocation [8â€“10]. However, the inherent complexity of general MILP problems is classified as NP-hard, resulting in exponential growth in computational time as problem dimensions increase. The branch-and-bound algorithm remains the cornerstone for solving MILP problems [11, 12], relying on recursively partitioning the solution space into a search tree. This method systematically explores the opti-mal solution at the leaf nodes of the tree, which has been extensively studied and refined over decades [13â€“15]. Various hand-crafted heuristic strategies have been devel-oped to guide node and variable selection, thereby controlling the size of branching tree and accelerating the solving process. These approaches are typically based on redetermined single- or multi-criteria metrics extracted from branching tree, which introduces limitations that their effectiveness often varies significantly across diverse MILP problems. 

MILP formulation and branch-and-bound algorithm 

Combinatorial optimization problems, e.g., the set covering problem, combinatorial auction problem and capacitated facility location problem depicted in Figure 1.b, can be written in the general MILP form in Figure 1.a [16], where c âˆˆ Rn is the objective coefficient vector that is commonly used to describe the cost function of the original problem. A âˆˆ RmÃ—n and b âˆˆ Rm represent the constraint coefficient matrix and the constraint right-hand-side vector, respectively, and the size of a MILP problem is typically measured by the number of rows ( m) and columns ( n) of the constraint matrix A.When solving the MILP problem, such as the 2000 Ã— 1000 set covering problem in Figure 1.c, the branch-and-bound algorithm will first solve the linear programming relaxation of the original MILP problem, then partition the larger MILP problem into two smaller sub-problems on the variable that does not respect integrality [16]. By executing such binary decomposition iteratively, the algorithm yields a search tree, as shown in Figure 1.d, and searches for the optimal solution to the original MILP problem on newly generated leaf nodes. The nodes containing the optimal solution in their solution space are labeled in red, which form an optimal path from the root node to the node where the optimal solution was found in the branch-and-bound search tree[17]. 

Learning-based enhancement of the branch-and-bound algorithm 

Learning-based approaches have shown promising results in accelerating or even replacing human-designed heuristic rules within the branch-and-bound algorithm. The problem of branching variable selection can be framed as a ranking problem, where the model was trained to learn the order of branching variables directly from the out-comes of strong branching rule [18], or to assign scores to each candidate branching variable [19]. Graph neural networks (GNNs) was used to encode the relationships between vari-ables and constraints in each sub-problem, allowing the model to effectively learn the variable selection of the strong branching rule [20, 21] or to discover improved vari-able selection orders through reinforcement learning [22, 23]. Furthermore, Ding et 3al. [24] introduced a GNN-based learning framework to explore primal heuristics dur-ing the MILP solving process, leveraging tripartite graph representations of nodes to learn how to generate high-quality feasible solutions. Similarly, GNN-based models were also introduced to efficiently explore the neighborhood of an initial feasible solu-tion, aiming to identify better solutions [21, 25, 26]. Newer work also integrated graph neural architecture search algorithms to automatically find the best GNNs for a given NP-hard combinatorial optimization problem [27]. Machine learning has also been successfully applied to discover better cutting plane addition sequences and apply the learned models to accelerate the solving process of different MILP problems [28â€“31]. min ð‘ ð‘‡ ð‘¥ 

s.t. ð´ ð‘¥ â‰¤ ð‘ 

(a) 

(b) 

> Facility
> Candidate Facility
> Customer

Capacitated Facility Location   

> $1000 $850 $300

Combinatorial Auction 

Set Covering                                                                                                       

> 95 âˆ™+83 âˆ™ð‘¥ 1ð‘¥ 2ð‘¥ 3ð‘¥ 4
> ð‘¥ 6ð‘¥ 7ð‘¥ 8ð‘¥ 9
> ð‘¥ 11 â€¦
> ð‘¥ 996 ð‘¥ 997 ð‘¥ 998 ð‘¥ 999 ð‘¥ 1000
> 1âˆ™85 âˆ™
> 1âˆ™84 âˆ™77 âˆ™41 âˆ™
> 32 âˆ™
> 31 âˆ™59 âˆ™12 âˆ™100 âˆ™74 âˆ™
> +++
> ++++
> ++
> ++++
> ð‘¥ 1ð‘¥ 4ð‘¥ 992 ð‘¥ 1000
> ð‘¥ 1ð‘¥ 26 ð‘¥ 985 ð‘¥ 1000
> ð‘¥ 1ð‘¥ 979 ð‘¥ 1000
> ð‘¥ 1ð‘¥ 53 ð‘¥ 920 ð‘¥ 1000
> ð‘¥ 1ð‘¥ 56 ð‘¥ 998 ð‘¥ 1000
> 0âˆ™âˆ’1âˆ™âˆ’1âˆ™+0âˆ™
> âˆ’1âˆ™âˆ’1âˆ™âˆ’1âˆ™+0âˆ™
> 0âˆ™0âˆ™âˆ’1âˆ™
> 0âˆ™âˆ’1âˆ™âˆ’1âˆ™+0âˆ™
> âˆ’1âˆ™âˆ’1âˆ™âˆ’1âˆ™âˆ’1âˆ™
> ð‘¥ 38 âˆ’1âˆ™
> â€¦â€¦â€¦
> â€¦â€¦â€¦
> â€¦â€¦â€¦
> â€¦â€¦â€¦
> â€¦â€¦â€¦
> â‰¤
> â‰¤
> â‰¤
> â‰¤
> â‰¤
> âˆ’1
> âˆ’1
> âˆ’1
> âˆ’1
> âˆ’1
> â€¦

Minimize: 

Subject to:                                   

> ð‘¥ ð‘– âˆˆ{0,1},âˆ€ð‘– âˆˆ1,â€¦,1000
> ð‘¥ 1ð‘¥ 994 ð‘¥ 1000 âˆ’1âˆ™0âˆ™+0âˆ™ð‘¥ 15 +0âˆ™â€¦â€¦â€¦â‰¤âˆ’1
> ð‘¥ 556 âˆ™+
> ð‘¥ 10 43 âˆ™+
> ð‘¥ 993 ð‘¥ 994 ð‘¥ 995 67 âˆ™87 âˆ™100 âˆ™+++
> 2000 constraints

# Total nodes  # #

847  18  829 

(d) 

â€¦

â€¦

â€¦                                               

> ð‘¥ 577 =1
> ð‘¥ 650 =0
> ð‘¥ 91 =0
> ð‘¥ 650 =1
> ð‘¥ 91 =1
> ð‘¥ 41 =0ð‘¥ 41 =1
> ð‘¥ 61 =0
> ð‘¥ 350 =0
> ð‘¥ 790 =0
> ð‘¥ 662 =0
> ð‘¥ 290 =1
> ð‘¥ 577 =0
> ð‘¥ 61 =1
> ð‘¥ 290 =0
> ð‘¥ 181 =1ð‘¥ 181 =0
> ð‘¥ 662 =1
> ð‘¥ 790 =1
> ð‘¥ 350 =1
> Path to the optimal solution
> Nodes on the optimal solution path
> Other nodes

(c) 

Fig. 1 : MILP problems and branch-and-bound tree . a, The general form of MILP problem. b, Three types of NP-Hard MILP problem included in our testing dataset. c, A complex large-scale set covering problem in MILP form. The matrices 

A, b, and cT in this example are highlighted in corresponding colors. d, Part of the branch-and-bound tree obtained by solving the set covering problem in c. Path to the optimal solution are marked in light red. Statistical analysis reveals a severe imbalance in different node types. 4Recent advancements in large language models (LLMs) have emerged as a powerful tool, which was introduced in works to explore the foundation model of solving complex MILP problems. For example, Li et al. [32] introduced MILP-Evolve to improve the generalization capability of LLM-based method by an evolutionary framework while Ye et al. [33] introduced LLM-LNS, which promoted solving large-scale MILP prob-lems using LLM-driven large neighborhood search framework combined with existing heuristics. Awasthi et al. [34] demonstrated that LLMs can balance the flexibility of locally expressed constraints with rigorous global optimization by introducing iter-ated fine-tuning framework where algorithmic feedback progressively refines the LLMâ€™s output. However, to our knowledge, only limited studies have investigated the potential of machine learning-enhanced node selection strategies. Among these, He et al. [17] were pioneers in training support vector machine (SVM)-based node selection and prun-ing models using imitation learning. Building upon this foundation, Yilmaz et al. [35] conducted an extensive investigation into the impact of various node feature combina-tions within a similar experimental framework. Song et al. [36] learned a node selection by imitation learning of shortest paths to good feasible solutions. While their method achieved scale generalization by incrementally increasing the problem size during train-ing, this approach was computationally intensive and potentially compromised the methodâ€™s generalizability. Labassi et al. [37] used the bipartite graph representation from Gasse et al. [20] to encode the state of nodes. They treated the node selection process as a pairwise comparison between two different nodes, training their model to distinguish nodes that contain the optimal solution. However, their experimental validation was limited to small-scale MILP problems that could be readily solved by open-source solvers. Recently, Mattick & Mutschler [38] and Zhang et al. [39] employed the representation of whole branch-and-bound tree to encode the state of nodes and applied reinforce-ment learning to learn the selection of nodes, but the scale of their benchmark MILP instances was also limited. Notably, unlike variable selection, effective node selection in branch-and-bound method requires identifying nodes that contain better feasible solutions to update the global primal bound [16, 40]. Such nodes are exceedingly rare in the search tree, depicted in Figure 1.d, presenting significant challenges of sample imbalance for training robust machine learning models. 

Designing faster mixed integer linear programming algorithm via DeepBound 

We propose a novel approach, named DeepBound, to learn to prioritize the optimal path nodes, which contain the optimal solution to the original MILP problem, in the branch-and-bound tree. To tackle the two critical challenges of node feature encoding and data imbalance faced by learning-based methods in node selection. The core design of DeepBound introduces a multi-level node feature fusion network, complemented by a pairwise training mechanism. This architecture enables DeepBound to process paired node feature data generated during each step of the branch-and-bound procedure, performing cross-feature and cross-node dimension information fusion to enhance its ability to identify nodes belonging to the optimal path. We evaluate the node selection capability of DeepBound on various MILP prob-lem datasets. Integrated with the state-of-the-art SCIP solver [16], our experiments compare the acceleration provided by DeepBound against traditional human-designed heuristics and other learning-based methods when solving practical MILP problems 5(e.g., the set covering problem instance in Figure 1.c). Furthermore, our results demon-strate that the node selection model learned by DeepBound on smaller MILP datasets exhibits strong generalization capabilities, effectively handling more complex problems with increased numbers of variables and constraints. 

## 2 Results Root node 

> Branching
> Node
> Score
> Node
> Score
> Find the
> optimal
> solution!

â€¦Branching 

â€¦ 

> DeepBound node selector
> Nodes with optimal
> solution
> Other nodes
> Fusion Blocks
> Output
> Fusion Blocks
> Output
> Fusion Blocks
> Output

â€¦ Node 

> Score
> Input node
> feature
> Average
> Branching
> Node
> Score

â€¦

> Pairing

Fig. 2 : DeepBound architecture . Overview of using DeepBound for node selection, sorting nodes based on their scores and iteratively selecting the higher-ranked node each time to find the optimal solution. DeepBound utilizes the ensemble learning mechanism of fusion models to process feature vectors of node pairs and provides average score for node ranking. Each fusion block performs node-level and feature-level fusion on the feature vectors of node pairs for node comparison and subsequent node scoring. 

2.1 DeepBound learns the optimal solution path 

DeepBound replaces the original heuristic rules in the branch-and-bound algorithm by scoring newly generated node pairs using a neural network based on multi-level feature fusion (Figure 2). In the branch-and-bound algorithm, each newly generated node from a branching operation, along with all unexplored leaf nodes, is placed in a priority queue. The scores assigned to the nodes by the node selection algorithm determine the 6order of the nodes in the priority queue [16]. When training our DeepBound network model, we use all nodes along the optimal solution path in the branch-and-bound tree of an already solved MILP problem as oracle nodes , and train the DeepBound model to assign higher scores to these nodes. However, such nodes account for only a small proportion of the branch-and-bound tree, leading to a dramatic imbalance problem. 

Mitigating node imbalance via pairing node samples 

To address this imbalance issue, we introduce a pairwise training protocol and a learning-to-rank approach. In this protocol, the oracle nodes from the training data are paired with non-oracle nodes from the same priority queue to form training pairs, converting the original objective into learning a node ranking strategy. Specifically, the DeepBound model is trained to assign higher scores to oracle nodes than to non-oracle nodes within the same training pair. By learning this strategy, the oracle nodes rank higher in the priority queue, allowing the MILP solver to select the oracle nodes earlier from the priority queue, positively accelerating the solving process. 

Enhancing DeepBound via multi-level feature fusion 

In the DeepBound setup, the feature vectors of the two new nodes generated from each new branching operation are input into the model, performing multiple rounds of feature fusion across different dimensions between the nodes and features through a series of concatenated fusion modules (detailed in the Methods, Figure C3). These fusion modules are implemented using different MLPs. Additionally, multiple fusion networks trained through ensemble learning methods are paralleled, providing robust average scores for the node pairs, which will be sent to the priority queue to determine the order of the newly branched node pairs. 

2.2 DeepBound achieves leadership on challenging MILP benchmarks 

Our testing dataset includes three NP-hard MILP problem benchmarks: set covering, combinatorial auction, and capacitated facility location. For each type of MILP prob-lem, we generated two sets of testing instances with varying problem sizes: one set with instances of the same size as those in the training set, and another set with instances larger than those in the training set. These different sizes correspond to varying levels of difficulty in solving the problems. Each testing set contains 100 randomly generated instances. Further details of each MILP problem are included in the Methods. We evaluated different node selection algorithms by combining them with the same full-strong branching (FSB) rule used in SCIP [16]. The total solving time 

was reported in seconds, including the running time for unsolved instances without additional penalties. Additionally, we incorporated the best primal bound time ( bpb-time ) to measure the time taken by the node selector to find the optimal feasible solution in the branch-and-bound tree. After this point, the branch-and-bound solver will use this optimal feasible solution for node pruning. Thus, this metric reflects the ability of the node selection algorithm to identify the optimal feasible solution and accelerate the optimization of the global primal bound. Our analysis further extended to examining the convergence patterns of pri-mal gaps across different node selection algorithms on the three evaluation datasets. The primal gap, which quantifies the disparity between the objective function value of the integer feasible solution at the current node and the known optimal feasible 70 500 1000 1500 2000 2500 3000 3500 

Solving time (s) 

> 0500 1000 1500 2000 2500 3000
> Bpb time (s)

Set Covering (1000 Ã— 1000) 

> BES XGBoost He et al. DeepB ound

(a) 0 200 400 600 800 1000 1200 

Solving time (s) 

> 0200 400 600 800 1000
> Bpb time (s)

Combinatorial Auction (200 Ã— 1000)  

> BES
> XGBoost
> He et al.
> DeepB ound

(b) 0 500 1000 1500 2000 

Solving time (s) 

> 0500 1000 1500 2000
> Bpb time (s)

Capacitated Facility Location (200 Ã— 100)  

> BES XGBoost He et al. DeepB ound

(c) 0 500 1000 1500 2000 2500 3000 3500 

Number of Node 

> 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
> Primal Gap (%)

Set Covering (1000 Ã— 1000) 

> BES XGBoost He et al. DeepB ound

(d) 0 200 400 800 1000 1200 600 

Number of Node 

> 0.0 0.2 0.4 0.6 0.8 1.0
> Primal Gap (%)

Combinatorial Auction (200 Ã— 1000)  

> BES
> XGBoost
> He et al. DeepB ound

(e) 0 250 500 750 1000 1250 1500 1750 2000 

Number of Node 

> 0.0 0.1 0.2 0.3 0.4 0.5
> Primal Gap (%)

Capacitated Facility Location (200 Ã— 100)  

> BES XGBoost He et al. DeepB ound

(f) 15/100 25/100 18/100 42/100 

Set Covering (2000 Ã— 1000) 

> BES wins XGBoost wins He et al. wins DeepBound wins

(g) 22/100 22/100 19/100 37/100 

Combinatorial Auction (300 Ã— 1500)  

> BES wins XGBoost wins He et al. wins DeepBound wins

(h) 18/100 20/100 23/100 39/100 

Capacitated Facility Location (400 Ã— 100)  

> BES wins XGBoost wins He et al. wins DeepBound wins

(i) 

Fig. 3 : Evaluation of DeepBound node selector with MILP test sets of different types and scales. a-c, Evaluation on instances of three different MILP problems. The figures show the distribution of solving time and bpb time for the different node selection methods on 100 randomly generated instances of each type of MILP problem, with the 95% confidence ellipses. d-f, Evaluation of the convergence pattern of primal gap for different node selection methods in solving three different MILP problems, with the mean curves and 95% confidence intervals. g-i, Comparison of the number of different node selection methods that achieve the fastest solving speed (wins) on the same set of 100 large-scale hard problem instances for each type of MILP problem. solution, serves as a crucial metric for evaluating algorithmic performance. The con-vergence pattern of primal gap provides valuable insights into the efficiency of each node selection algorithm to find the optimal solution. 8â€¦

â€¦

â€¦

â€¦

â€¦â€¦

â€¦

â€¦

â€¦

#node = 5 

#node = 50 

DeepB ound 

finds optimal 

solution 

BES 

finds optimal 

solution 

Number of 

node = 79 

Number of 

node = 599 

Primal Gap (%) 

DeepB ound node 

BES node 

Oracle path 

DeepB ound branching tree  BES branching tree 

ð‘¥ 577 = 1

ð‘¥ 650 = 0

ð‘¥ 91 = 0

ð‘¥ 650 = 1

ð‘¥ 91 = 1

ð‘¥ 41 = 0 ð‘¥ 41 = 1

ð‘¥ 61 = 0

ð‘¥ 350 = 0

ð‘¥ 790 = 0

ð‘¥ 662 = 0

ð‘¥ 290 = 1

ð‘¥ 577 = 0

ð‘¥ 61 = 0

ð‘¥ 290 = 0

ð‘¥ 181 = 1ð‘¥ 181 = 0

ð‘¥ 662 = 1

ð‘¥ 790 = 1

ð‘¥ 350 = 1

ð‘¥ 577 = 1

ð‘¥ 650 = 0

ð‘¥ 91 = 0

ð‘¥ 650 = 1

ð‘¥ 91 = 1

ð‘¥ 41 = 0 ð‘¥ 41 = 1

ð‘¥ 61 = 0

ð‘¥ 350 = 0

ð‘¥ 790 = 0

ð‘¥ 662 = 0

ð‘¥ 290 = 1

ð‘¥ 577 = 0

ð‘¥ 61 = 0

ð‘¥ 290 = 0

ð‘¥ 181 = 1ð‘¥ 181 = 0

ð‘¥ 662 = 1

ð‘¥ 790 = 1

ð‘¥ 350 = 1

ð‘¥ 577 = 1

ð‘¥ 650 = 0

ð‘¥ 91 = 0

ð‘¥ 650 = 1

ð‘¥ 91 = 1

ð‘¥ 41 = 0 ð‘¥ 41 = 1

ð‘¥ 61 = 0

ð‘¥ 350 = 0

ð‘¥ 790 = 0

ð‘¥ 662 = 0

ð‘¥ 290 = 1

ð‘¥ 577 = 0

ð‘¥ 61 = 0

ð‘¥ 290 = 0

ð‘¥ 181 = 1ð‘¥ 181 = 0

ð‘¥ 662 = 1

ð‘¥ 790 = 1

ð‘¥ 350 = 1

ð‘¥ 577 = 1

ð‘¥ 650 = 0

ð‘¥ 91 = 0

ð‘¥ 650 = 1

ð‘¥ 91 = 1

ð‘¥ 41 = 0 ð‘¥ 41 = 1

ð‘¥ 61 = 0

ð‘¥ 350 = 0

ð‘¥ 790 = 0

ð‘¥ 662 = 0

ð‘¥ 290 = 1

ð‘¥ 577 = 0

ð‘¥ 61 = 0

ð‘¥ 290 = 0

ð‘¥ 181 = 1ð‘¥ 181 = 0

ð‘¥ 662 = 1

ð‘¥ 790 = 1

ð‘¥ 350 = 1

ð‘¥ 577 = 1

ð‘¥ 650 = 0

ð‘¥ 91 = 0

ð‘¥ 650 = 1

ð‘¥ 91 = 1

ð‘¥ 41 = 0 ð‘¥ 41 = 1

ð‘¥ 61 = 0

ð‘¥ 350 = 0

ð‘¥ 790 = 0

ð‘¥ 662 = 0

ð‘¥ 290 = 1

ð‘¥ 577 = 0

ð‘¥ 61 = 0

ð‘¥ 290 = 0

ð‘¥ 181 = 1ð‘¥ 181 = 0

ð‘¥ 662 = 1

ð‘¥ 790 = 1

ð‘¥ 350 = 1

ð‘¥ 577 = 1

ð‘¥ 650 = 0

ð‘¥ 91 = 0

ð‘¥ 650 = 1

ð‘¥ 91 = 1

ð‘¥ 41 = 0 ð‘¥ 41 = 1

ð‘¥ 61 = 0

ð‘¥ 350 = 0

ð‘¥ 790 = 0

ð‘¥ 662 = 0

ð‘¥ 290 = 1

ð‘¥ 577 = 0

ð‘¥ 61 = 0

ð‘¥ 290 = 0

ð‘¥ 181 = 1ð‘¥ 181 = 0

ð‘¥ 662 = 1

ð‘¥ 790 = 1

ð‘¥ 350 = 1

ð‘¥ 577 = 1

ð‘¥ 650 = 0

ð‘¥ 91 = 0

ð‘¥ 650 = 1

ð‘¥ 91 = 1

ð‘¥ 41 = 0 ð‘¥ 41 = 1

ð‘¥ 61 = 0

ð‘¥ 350 = 0

ð‘¥ 790 = 0

ð‘¥ 662 = 0

ð‘¥ 290 = 1

ð‘¥ 577 = 0

ð‘¥ 61 = 0

ð‘¥ 290 = 0

ð‘¥ 181 = 1ð‘¥ 181 = 0

ð‘¥ 662 = 1

ð‘¥ 790 = 1

ð‘¥ 350 = 1

ð‘¥ 577 = 1

ð‘¥ 650 = 0

ð‘¥ 91 = 0

ð‘¥ 650 = 1

ð‘¥ 91 = 1

ð‘¥ 41 = 0 ð‘¥ 41 = 1

ð‘¥ 61 = 0

ð‘¥ 350 = 0

ð‘¥ 790 = 0

ð‘¥ 662 = 0

ð‘¥ 290 = 1

ð‘¥ 577 = 0

ð‘¥ 61 = 0

ð‘¥ 290 = 0

ð‘¥ 181 = 1ð‘¥ 181 = 0

ð‘¥ 662 = 1

ð‘¥ 790 = 1

ð‘¥ 350 = 1

Oracle path 

Oracle path  Oracle path Fig. 4 : DeepBound accelerates the solving of MILP problems . Comparison of the branch-and-bound trees and the node-primal gap curves between DeepBound and BES when solving identical 2000 Ã—1000 set covering problems. The central panel illustrates the node-primal gap curves, which depict the relationship between the num-ber of already explored nodes and the primal gap at the node. Figures on either side visualize the branch-and-bound tree of DeepBound and BES at different stages of the solving process. The oracle node paths leading to the discovery of the optimal feasible solution within each tree are emphasized in light red shading. 9Finally, in addressing the large-scale, computationally challenging problem sets, not all instances could be completely solved by all selectors within the given computational constraints, we counted the number of instances in which each node selector achieved the fastest solving time ( wins ) among all four node selection algorithms, thereby providing a quantitative measure of relative performance in solving complex problems. 

Faster solving of MILP problems using DeepBound 

In terms of solving time and best-primal-bound time (bpb-time), when tested on instances with similar size and complexity to the training set across all three MILP benchmarks, DeepBound consistently outperforms all other node selection methods. Specially, both the solving time and bpb-time distributions of DeepBound are closer to the origin than the competing approaches, including SCIPâ€™s default best estimate search (BES) node selection rule [16] and two machine learning-based methods [17]. This empirical evidence underscores DeepBoundâ€™s superior ability to identify optimal solutions more rapidly within the branch-and-bound framework, thereby significantly enhancing the efficiency of exact MILP solutions. The results indicate that DeepBound achieves these improvements without compromising solution quality, demonstrating its effectiveness in optimizing the search process for MILP problems. The effectiveness of DeepBound in accelerating the finding of nodes containing optimal solutions can be further demonstrated through an evaluation of primal gap metrics. The primal gap represents the discrepancy between the objective function value of the best-so-far integer feasible solution and the optimal solution within the branch-and-bound framework. For each MILP test set, we analyzed the average convergence behavior of primal gaps across different node selection strategies, incorpo-rating the 95% confidence intervals for all test samples. As illustrated in Figures 3d-f, DeepBound consistently demonstrates superior performance, achieving the fastest pri-mal gap convergence across all three evaluation benchmarks. The algorithm exhibits the most consistent convergence behavior, as evidenced by the smallest confidence interval area across test instances. This robust acceleration in primal gap reduction strongly corroborates the promoting effect of DeepBound in efficiently locating optimal solutions within branch-and-bound method. 

DeepBound generalizes and scales to larger MILP problems 

In generalization tests, as the problem scale increases, the proportion of problems that can be solved in one hour decreases significantly. Despite the substantial increase in problem scale and difficulty, DeepBound performs well on instances larger than those in the training sets and outperforms both other learning-based node selectors and SCIPâ€™s default node selection rule, BES, in terms of the number of wins (Figure 3g-i), which indicates that DeepBound maintains leading performance even facing the more complex MILP problems. An example of the acceleration effect of DeepBound in finding the optimal feasible solution is presented in Figure 4. When solving the same instance of a 2000 Ã—1000 set covering problem, DeepBound is able to explore the branch containing the optimal feasible solution earlier in the branch-and-bound tree. In contrast, the BES method explores nodes more extensively across the branch-and-bound tree and delves deeper into the branch containing the optimal solution at a later stage. This focused explo-ration enables DeepBound to find the optimal feasible solution after solving only 79 nodes (where the primal gap drops to 0), whereas the BES algorithm finds the optimal 10 solution after exploring 599 nodes, resulting in a significantly higher total number of nodes compared to DeepBound. 

2.3 DeepBound accelerates selecting the oracle nodes 

To further analyze how the DeepBound algorithm accelerates the branch-and-bound method in finding the optimal feasible solution, we conducted more detailed experi-ments on the three MILP problems. To quantify the deviation of the search process from the optimal solution path, we define the distance D(ni, n opt ) from the current node ni to the node with optimal solution nopt :

D(ni, n opt ) = len (his (nopt ))+ dif f (his (ni), his (nopt )) âˆ’LCS (his (ni), his (nopt )) . (1) where his (Â·) represents the branching history of a node. The term 

dif f (his (ni), his (nopt )) denotes the number of variables with differing values in the branching histories of two nodes, indicating the number of incorrect node selections. Conversely, LCS (his (ni), his (nopt )) represents the length of the longest common substring between the branching history of two nodes, indicating the number of cor-rect node selections. Incorrect variable values resulting from node selection can cause the current nodeâ€™s solution space to deviate from the optimal solution space, thereby increasing the solving time by creating greater challenges for primal heuristics in the branch-and-bound solver [41]. We performed this optimal solution distance analysis on three MILP benchmark problems: the 1000 Ã—1000 set covering problem, the 200 Ã—1000 combinatorial auction problem, and the 200 Ã—100 capacitated facility location problem. The experimental results were compared between the DeepBound algorithm and three other approaches: the default node selection algorithm BES in the SCIP solver, as well as two other machine learning-based algorithms. 

DeepBound approaches the optimal solution faster 

During the branch-and-bound process, the node distances for all methods exhibit a decreasing trend before the optimal solution is identified. Notably, the DeepBound algorithm demonstrates the most rapid decline in node distance across all three MILP problems (Figure 5a-c), suggesting that the feasible regions of nodes selected by 0 500 1000 2000 2500 3000 1500 

> Number of Node
> 0510 15 20 25
> Distance
> Set Covering (1000 Ã— 1000)
> BES
> XGBoost
> He et al. DeepB ound

(a) 0 200 400 800 1000 1200 600  

> Number of Node
> 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
> Distance
> Combinatorial Auction (200 Ã— 1000)
> BES
> XGBoost
> He et al. DeepB ound

(b) 0 250 500 750 1000 1250 1500 1750 2000  

> Number of Node
> 0510 15 20 25 30
> Distance
> Capacitated Facility Location (200 Ã— 100)
> BES
> XGBoost
> He et al. DeepB ound

(c) 

Fig. 5 : Evaluation of the distance to the optimal solution. a-c, Evaluation of the decreasing trend of the distance for different node selection methods between the current node and the node containing the optimal solution in the branch-and-bound tree before the optimal solution was found. The mean curves and 95% confidence intervals are depicted. 11 DeepBound are closer to the region containing the optimal solution. This allows Deep-Bound to identify the optimal solution more quickly within the branch-and-bound tree compared to other methods. Similar acceleration in selecting the oracle node is also observed in Figure 4, further supporting the efficiency of this approach. In contrast, machine learning-based and heuristic algorithms display inconsistent performance across different problems, with no consistent superiority observed. 0 500 1000 1500 2000 2500 3000 3500 

> Solving time (s)
> 0500 1000 1500 2000 2500 3000
> Bpb time (s) Set Covering (1000 Ã— 1000)
> BES No Pair MLP Pair DeepB ound

(a) 0 500 1000 2000 2500 3000 1500  

> Number of Node
> 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
> Primal Gap (%) Set Covering (1000 Ã— 1000)
> BES No Pair MLP Pair DeepB ound

(b) 

Fig. 6 : Evaluation of ablation models of DeepBound. a, Solving time and bpb time for the DeepBound, BES, DeepBound with single node feature ( No Pair ) and DeepBound with vanilla MLP ( MLP Pair ) on 100 randomly generated instances of set covering 1000 Ã—1000 problem, with the 95% confidence ellipses. b, The comparison of the convergence trend of primal gaps of different node selection methods in solving set covering 1000 Ã—1000 problem. 

Ablation studies on different model components 

To thoroughly analyze the contributions of different components within the Deep-Bound framework, we conducted extensive ablation studies. All experiments were performed using the same 1000 Ã—1000 set covering problem instances as the test set. We configured an unchanged DeepBound network that only takes single-node features as input (denoted as No Pair ) and a MLP network that takes paired node features as input but omits the multi-level feature fusion mechanism (denoted as MLP Pair ). This allowed us to isolate and assess the individual impacts of paired feature input and the feature fusion module. As shown in Figure 6.a, the DeepBound model achieves superior performance, as evidenced by its distributions of solving time and bpb-time being closer to the origin, indicating faster identification of optimal solutions and more efficient exact solving. The primal gap curves in Figure 6.b further confirm this advantage, with DeepBound showing the fastest convergence to zero and maintaining the lowest gap throughout. In contrast, both ablation models and external methods like BES demonstrate inferior performance, and BES additionally suffers from delayed convergence due to its slower discovery of optimal solutions in certain problem instances. 12 2.4 Feature analysis of DeepBound 

We conducted a comprehensive analysis of how different input features contribute to the final node scores, aiming to gain deeper insights into the feature relationships that the DeepBound model learns during node selection. Unlike many existing neural network-based methods for solving MILP problems, which typically rely on direct encoding of raw MILP formulations, we adopt a fundamentally different approach. Specifically, DeepBound leverages a more comprehensive set of engineered features to represent nodes and employs neural networks to learn and score within this feature space. 

Informative features reduce complexity and enable analysis 

Our input features are meticulously designed to capture three key aspects: node-specific attributes, branching variable information, and global characteristics of the branch-and-bound tree (as detailed in the Methods section). This approach provides two significant advantages: first, by constructing a compact and meaningful feature representation, we substantially reduce the complexity and dimensionality of node descriptions, thereby lowering the learning difficulty for the network; second, our fea-ture representation is consistent with the node selection criteria used in traditional MILP solvers, which facilitates both theoretical comparison and empirical analysis of feature importance across diverse MILP problems. These advantages allow us to not only better understand the underlying mechanisms of the model but also to identify which features are most influential in determining node scores. We employed the DeepExplainer from the SHAP (SHapley Additive exPlanations) framework to systematically analyze and visualize the influence of input features on the output of the DeepBound model. The SHAP value of each point indicates whether the feature promotes the modelâ€™s classification of a node as oracle node (positive SHAP value) or non-oracle node (negative SHAP value). The absolute value of the SHAP value reflects the strength of this promotion. The feature analysis for the set cover-ing problem, the combinatorial auction problem and the capacitated facility location problem are presented in Figure 7.a, 7.b, and 7.c, respectively. Additionally, we list the features associated with three commonly used node selection rules in SCIP in Table 1. Given that the DeepBound model operates on paired-node features as input and generates scores for each node in the pair, for each problem in Figure 7, we analyzed the impact of node features on two aspects: the node itself (depicted in the first row of each figure) and its paired sibling node (shown in the second row). 

Table 1: Features used by different node selection heuristic rules.    

> Node selection rule Features
> Depth First Search Node Depth Best First Search Node Lowerbound Node Type (with plunging) Best Estimate Search Node Lowerbound Node Estimate Node Type (with plunging) BranchVar BoundLPDiff BranchVar Pseudocost

13 Fig. 7 : Feature analysis of DeepBound on different MILP problems. The SHAP values of all input features on a, set covering, b, combinatorial auction and 

c, capacitated facility location problem. The color of the data points represents the normalized magnitude of each featureâ€™s SHAP value. The impact of input features on the node itself (first row) and on its paired sibling node (second row) are separately analyzed for each problem. 14 DeepBound learns problem-specific feature patterns 

The experimental results reveal a significant mutual influence between the features of the paired nodes, as evidenced by the strong response of the scoring results to the features of the sibling nodes in the second row. These findings indicate that the paired-node input mechanism employed by the DeepBound method, combined with the feature mixing network architecture, provides a more informative and comprehensive representation compared to single-node input approaches. Manually designed heuristic rules often face the challenge of limited applicabil-ity across diverse MILP problems. Unlike these heuristic node selection rules, which depend on fixed feature sets, the DeepBound model can be trained on problem-specific datasets to identify effective feature combinations tailored to each particular problem structure. For the set covering problem, the modelâ€™s predictions are significantly influ-enced by nodeâ€™s lower bound, estimate, type, the deviation of the branching variable from integer values, and pseudocosts of branching variable. While these features par-tially overlap with those used in BFS and BES rules, but the node depth, which is a focus of DFS, shows minimal impact on the modelâ€™s performance. In contrast, for the combinatorial auction problem, the importance of the type of node decreases, while node depth becomes a much more significant factor. For the capacitated facility loca-tion problem, the contribution of node type is minimal, and the impact of the nodeâ€™s lowerbound and estimate on the node score is also negligible. This cross-problem analysis reveals that existing heuristic rules fail to adequately account for the diverse feature importance across different MILP problem types. By leveraging extensive node data from various problem instances during training, the DeepBound model can identify feature combinations that are particularly effective in solving specific problems. This capability provides valuable insights for develop-ing simpler yet highly effective heuristic rules that can adapt to different problem structures. 

## 3 Discussion 

DeepBound concentrates on learning and replacing the heuristic rules employed during the node selection phase of the branch-and-bound algorithm, instead of attempting to train an end-to-end model of the entire branch-and-bound process. This approach not only mitigates the learning complexity of the neural network model, but also effec-tively accelerates the solving process when integrated with existing branch-and-bound solvers. Unlike conventional neural network-based methods, DeepBound eschews the direct encoding of raw node information and instead employs node features that are collected in a manner akin to heuristic rules yet with a substantially broader scope for feature selection and collection. Our feature analysis reveals that, unlike tradi-tional heuristic rules, DeepBound calculates these features during node selection via neural network inference. By successfully identifying key features while avoiding the limitations of hard-coded feature calculations, DeepBound offers valuable insights for designing new heuristic algorithm. Prior research has explored the application of deep learning techniques to substi-tute specific heuristic rules within the branch-and-bound algorithm. However, many of these approaches have focused exclusively on operations within a single node, neglecting the interdependency between nodes on a larger scale. In contrast, Deep-Bound employs a multi-level feature fusion network to comprehensively capture both intra-node features and inter-node relationships from the nodesâ€™ data. This enhanced 15 information capture is crucial for effective decision-making during the branch-and-bound solving process. Consequently, DeepBound is better equipped to evaluate and encode whether a node contains the optimal solution, which enhances the improve-ment of node selection methods and expands the scope of data-driven substitution of heuristic rules within the branch-and-bound framework. Recent advancements in algorithm design leveraging large language models (LLMs) have emerged as a powerful tool, particularly in addressing complex problems such as the cap set problem, where they have surpassed traditional heuristic approaches. Notably, the successful algorithms developed by LLMs often rely on relatively straight-forward Python code or pseudocode implementations. In contrast, exact algorithms for mixed-integer programming (MIP) problems, such as branch-and-bound methods, are inherently complex and challenging to express in simple code or natural language. This discrepancy presents a promising opportunity to integrate neural networks, trained on problem-solving data, to enhance or replace heuristic search components within the branch-and-bound framework. In the future, a particularly promising research direction is to harness the encoding and representation capabilities of LLMs to either substitute heuristic components or augment exact solving algorithms for MIP problems. By incorporating advanced tech-niques such as reinforcement learning and chain-of-thought, LLMs could be empowered to perform more efficient reasoning and exploration, thereby significantly improving the performance of MIP solvers. 

## References 

[1] Louati, A., Lahyani, R., Aldaej, A., Mellouli, R., Nusir, M.: Mixed integer linear programming models to solve a real-life vehicle routing problem with pickup and delivery. Applied Sciences 11 (20), 9551 (2021) [2] Yang, S., Zhang, R., Ma, Y., Zuo, X.: Adaptive large neighborhood search incor-porating mixed-integer linear programming for electric vehicle routing problem with mobile charging and nonlinear battery degradation. Applied Soft Computing 

175 , 112988 (2025) [3] Pedram, A., Sorooshian, S., Mulubrhan, F., Abbaspour, A.: Incorporating vehicle-routing problems into a closed-loop supply chain network using a mixed-integer linear-programming model. Sustainability 15 (4), 2967 (2023) [4] Cheng, C.-K., Kahng, A.B., Kang, B., Kang, S., Lee, J., Lin, B.: So3-cell: Stan-dard cell layout automation framework for simultaneous optimization of topology, placement, and routing. In: Proc. ICCAD (2025) [5] Liu, G., Zhu, Y., Zhuang, Z., Pei, Z., Gan, M., Huang, X., Guo, W.: A robust multilayer x-architecture global routing system based on particle swarm opti-mization. IEEE Transactions on Systems, Man, and Cybernetics: Systems 54 (9), 5627â€“5640 (2024) [6] Baller, R., Fontaine, P., Minner, S., Lai, Z.: Optimizing automotive inbound logistics: A mixed-integer linear programming approach. Transportation Research Part E: Logistics and Transportation Review 163 , 102734 (2022) [7] Rajak, S., Vimal, K., Arumugam, S., Parthiban, J., Sivaraman, S.K., Kandasamy, 16 J., Duque, A.A.: Multi-objective mixed-integer linear optimization model for sustainable closed-loop supply chain network: A case study on remanufacturing steering column. Environment, development and sustainability 24 (5), 6481â€“6507 (2022) [8] Ahmadian, A., Ponnambalam, K., Almansoori, A., Elkamel, A.: Optimal manage-ment of a virtual power plant consisting of renewable energy resources and electric vehicles using mixed-integer linear programming and deep learning. Energies 

16 (2), 1000 (2023) [9] Lippi, M., Marino, A.: A mixed-integer linear programming formulation for human multi-robot task allocation. In: 2021 30th IEEE International Confer-ence on Robot & Human Interactive Communication (RO-MAN), pp. 1017â€“1023 (2021). IEEE [10] Nematian, J.: A two-stage stochastic fuzzy mixed-integer linear programming approach for water resource allocation under uncertainty in ajabshir qaleh chay dam. Journal of Environmental Informatics 41 (1) (2023) [11] Bengio, Y., Lodi, A., Prouvost, A.: Machine learning for combinatorial optimiza-tion: a methodological tour dâ€™horizon. European Journal of Operational Research 

290 (2), 405â€“421 (2021) [12] Gleixner, A., Bastubbe, M., Eifler, L., Gally, T., Gamrath, G., Gottwald, R.L., Hendel, G., Hojny, C., Koch, T., LÂ¨ ubbecke, M., et al.: The SCIP Optimization Suite 6.0. ZIB-Report 18-26. Zuse Institute Berlin (2018) [13] BÂ´ enichou, M., Gauthier, J.-M., Girodet, P., Hentges, G., Ribi` ere, G., Vincent, O.: Experiments in mixed-integer linear programming. Mathematical Programming 

1(1), 76â€“94 (1971) [14] Achterberg, T., Wunderling, R.: Mixed integer programming: Analyzing 12 years of progress. Facets of Combinatorial Optimization, 449â€“481 (2013) [15] Dakin, R.J.: A tree-search algorithm for mixed integer programming problems. The Computer Journal 8(3), 250â€“255 (1965) [16] Achterberg, T.: Scip: solving constraint integer programs. Mathematical Pro-gramming Computation 1(1), 1â€“41 (2009) [17] He, H., Daume III, H., Eisner, J.M.: Learning to search in branch and bound algorithms. Advances in Neural Information Processing Systems 27 (2014) [18] Khalil, E., Le Bodic, P., Song, L., Nemhauser, G., Dilkina, B.: Learning to branch in mixed integer programming 30 (2016) [19] Alvarez, A.M., Louveaux, Q., Wehenkel, L.: A machine learning-based approx-imation of strong branching. INFORMS Journal on Computing 29 (1), 185â€“195 (2017) [20] Gasse, M., ChÂ´ etelat, D., Ferroni, N., Charlin, L., Lodi, A.: Exact combinato-rial optimization with graph convolutional neural networks. Advances in Neural 17 Information Processing Systems 32 (2019) [21] Nair, V., Bartunov, S., Gimeno, F., Glehn, I., Lichocki, P., Lobov, I., Oâ€™Donoghue, B., Sonnerat, N., Tjandraatmadja, C., Wang, P., et al.: Solving mixed integer programs using neural networks. arXiv preprint arXiv:2012.13349 (2020) [22] Qu, Q., Li, X., Zhou, Y., Zeng, J., Yuan, M., Wang, J., Lv, J., Liu, K., Mao, K.: An improved reinforcement learning algorithm for learning to branch. arXiv preprint arXiv:2201.06213 (2022) [23] Parsonson, C.W., Laterre, A., Barrett, T.D.: Reinforcement learning for branch-and-bound optimisation using retrospective trajectories. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, pp. 4061â€“4069 (2023) [24] Ding, J.-Y., Zhang, C., Shen, L., Li, S., Wang, B., Xu, Y., Song, L.: Accelerating primal solution findings for mixed integer programs based on solution prediction. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, pp. 1452â€“1459 (2020) [25] Paulus, M., Krause, A.: Learning to dive in branch and bound. Advances in Neural Information Processing Systems 36 , 34260â€“34277 (2023) [26] Liu, H., Wang, J., Geng, Z., Li, X., Zong, Y., Zhu, F., Hao, J., Wu, F.: Apollo-milp: An alternating prediction-correction neural solving framework for mixed-integer linear programming. International Conference on Learning Representations 13 

(2025) [27] Liu, Y., Zhang, P., Gao, Y., Zhou, C., Li, Z., Chen, H.: Combinatorial optimiza-tion with automated graph neural networks. arXiv preprint arXiv:2406.02872 (2024) [28] Tang, Y., Agrawal, S., Faenza, Y.: Reinforcement learning for integer program-ming: Learning to cut. In: International Conference on Machine Learning, pp. 9367â€“9376 (2020). Proceedings of Machine Learning Research [29] Huang, Z., Wang, K., Liu, F., Zhen, H.-L., Zhang, W., Yuan, M., Hao, J., Yu, Y., Wang, J.: Learning to select cuts for efficient mixed-integer programming. Pattern Recognition 123 , 108353 (2022) [30] Paulus, M.B., Zarpellon, G., Krause, A., Charlin, L., Maddison, C.: Learning to cut by looking ahead: Cutting plane selection via imitation learning. In: Interna-tional Conference on Machine Learning, pp. 17584â€“17600 (2022). Proceedings of Machine Learning Research [31] Deza, A., Khalil, E.B.: Machine learning for cutting planes in integer program-ming: A survey. arXiv preprint arXiv:2302.09166 (2023) [32] Li, S., Kulkarni, J., Menache, I., Wu, C., Li, B.: Towards foundation mod-els for mixed integer linear programming. International Conference on Learning Representations 13 (2025) 18 [33] Ye, H., Xu, H., Yan, A., Cheng, Y.: Large language model-driven large neighbor-hood search for large-scale MILP problems. International Conference on Machine Learning 42 (2025) [34] Awasthi, P., Gollapudi, S., Kumar, R., Munagala, K.: Combinatorial optimization via llm-driven iterated fine-tuning. arXiv preprint arXiv:2503.06917 (2025) [35] Yilmaz, K., Yorke-Smith, N.: A study of learning search approximation in mixed integer branch and bound: Node selection in scip. AI 2(2), 150â€“178 (2021) [36] Song, J., Lanka, R., Zhao, A., Bhatnagar, A., Yue, Y., Ono, M.: Learning to search via retrospective imitation. arXiv preprint arXiv:1804.00846 (2018) [37] Labassi, A.G., ChÂ´ etelat, D., Lodi, A.: Learning to compare nodes in branch and bound with graph neural networks. Advances in neural information processing systems 35 , 32000â€“32010 (2022) [38] Mattick, A., Mutschler, C.: Reinforcement learning for node selection in branch-and-bound. arXiv preprint arXiv:2310.00112 (2023) [39] Zhang, S., Zeng, S., Li, S., Wu, F., Li, X.: Learning to select nodes in branch and bound with sufficient tree representation. International Conference on Learning Representations 13 (2025) [40] Gasse, M., Bowly, S., Cappart, Q., Charfreitag, J., Charlin, L., ChÂ´ etelat, D., Chmiela, A., Dumouchelle, J., Gleixner, A., Kazachkov, A.M., et al.: The machine learning for combinatorial optimization competition (ml4co): Results and insights, 220â€“231 (2022) [41] Berthold, T.: Primal heuristics for mixed integer programs. PhD thesis, Zuse Institute Berlin (ZIB) (2006) [42] JÂ¨ unger, M., Liebling, T.M., Naddef, D., Nemhauser, G.L., Pulleyblank, W.R., Reinelt, G., Rinaldi, G., Wolsey, L.A.: 50 years of integer programming 1958-2008: From the early years to the state-of-the-art. Springer Science & Business Media (2009) [43] Achterberg, T., Koch, T., Martin, A.: Branching rules revisited. Operations Research Letters 33 (1), 42â€“54 (2005) [44] Achterberg, T., Berthold, T.: Hybrid branching. In: International Conference on Integration of Constraint Programming, Artificial Intelligence, and Operations Research, pp. 309â€“311 (2009). Springer [45] Balas, E., Ho, A.: Set covering algorithms using cutting planes, heuristics, and subgradient optimization: a computational study. Combinatorial Optimization, 37â€“60 (1980) [46] Leyton-Brown, K., Pearson, M., Shoham, Y.: Towards a universal test suite for combinatorial auction algorithms. In: Proceedings of the 2nd ACM Conference on Electronic Commerce, pp. 66â€“76 (2000) 19 [47] CornuÂ´ ejols, G., Sridharan, R., Thizy, J.-M.: A comparison of heuristics and relaxations for the capacitated plant location problem. European Journal of Operational Research 50 (3), 280â€“297 (1991) 20 Appendix A Extended experimental results 

The following pages present more detailed results of various methods in solving dif-ferent MILP instances. All experiments were conducted under identical hardware configurations with Intel(R) Xeon(R) Gold 6238R CPU @ 2.20GHz, except that DeepBound utilized the same CPU with a single NVIDIA A100 (80GB) GPU. 

Table A1 : Policy evaluation on separate instances in terms of number of branch-and-bound nodes, best primalbound time, solving time, and number of wins (fastest method) over number of solved hard instances. For each type of MILP problem, the models are trained on easy instances only. See Section 2.1 for definitions. 

Easy Medium Hard 

Method Nodes Bpb-time Sol-time Nodes Bpb-time Sol-time Nodes Wins 

scip-bes 23 15.40 22.34 418 224.12 394.83 463 15 / 100 XGBoost 24 14.75 21.36 395 147.37 383.46 455 25 / 100 He et al.[17] 24 15.47 21.98 399 158.29 398.71 483 18 / 100 DeepBound (Ours) 23 14.41 21.23 370 118.94 371.39 426 42 / 100 

1000 Ã—500 1000 Ã—1000 1000 Ã—2000 Set Covering 

scip-bes 13 5.91 6.74 88 67.82 93.48 431 22 / 100 

XGBoost 13 5.84 7.12 100 72.56 101.44 451 22 / 100 He et al.[17] 13 5.64 6.90 98 69.04 99.60 418 19 / 100 DeepBound (Ours) 13 5.42 6.67 78 61.16 88.52 310 37 / 100 

100 Ã—500 200 Ã—1000 300 Ã—1500 Combinatorial Auction 

scip-bes 56 14.40 36.45 76 59.30 168.02 74 18 / 100 

XGBoost 61 14.00 37.48 81 55.93 164.97 85 20 / 100 He et al.[17] 63 14.21 37.13 83 57.19 169.33 92 23 / 100 DeepBound (Ours) 55 13.78 36.10 67 52.24 157.56 68 39 / 100 

100 Ã—100 200 Ã—100 400 Ã—100 Capacitated Facility Location 

21 ð‘¥ 58 = 1

ð‘¥ 40 = 0

ð‘¥ 46 = 0

ð‘¥ 40 = 1

ð‘¥ 46 = 1

ð‘¥ 222 = 0 ð‘¥ 222 = 1

ð‘¥ 1059 = 0

ð‘¥ 807 = 0

ð‘¥ 1071 = 0

ð‘¥ 917 = 0

ð‘¥ 439 = 1

ð‘¥ 58 = 0

ð‘¥ 1059 = 0

ð‘¥ 439 = 0

ð‘¥ 1388 = 1ð‘¥ 1388 = 0

ð‘¥ 917 = 1

ð‘¥ 1071 = 1

ð‘¥ 807 = 1

ð‘¥ 140 = 1ð‘¥ 140 = 0

ð‘¥ 991 = 1ð‘¥ 991 = 0

ð‘¥ 58 = 1

ð‘¥ 40 = 0

ð‘¥ 46 = 0

ð‘¥ 40 = 1

ð‘¥ 46 = 1

ð‘¥ 222 = 0 ð‘¥ 222 = 1

ð‘¥ 1059 = 0

ð‘¥ 807 = 0

ð‘¥ 1071 = 0

ð‘¥ 917 = 0

ð‘¥ 439 = 1

ð‘¥ 58 = 0

ð‘¥ 1059 = 0

ð‘¥ 439 = 0

ð‘¥ 1388 = 1ð‘¥ 1388 = 0

ð‘¥ 917 = 1

ð‘¥ 1071 = 1

ð‘¥ 807 = 1

ð‘¥ 140 = 1ð‘¥ 140 = 0

ð‘¥ 991 = 1ð‘¥ 991 = 0

ð‘¥ 58 = 1

ð‘¥ 40 = 0

ð‘¥ 46 = 0

ð‘¥ 40 = 1

ð‘¥ 46 = 1

ð‘¥ 222 = 0 ð‘¥ 222 = 1

ð‘¥ 1059 = 0

ð‘¥ 807 = 0

ð‘¥ 1071 = 0

ð‘¥ 917 = 0

ð‘¥ 439 = 1

ð‘¥ 58 = 0

ð‘¥ 1059 = 0

ð‘¥ 439 = 0

ð‘¥ 1388 = 1ð‘¥ 1388 = 0

ð‘¥ 917 = 1

ð‘¥ 1071 = 1

ð‘¥ 807 = 1

ð‘¥ 140 = 1ð‘¥ 140 = 0

ð‘¥ 991 = 1ð‘¥ 991 = 0

ð‘¥ 58 = 1

ð‘¥ 40 = 0

ð‘¥ 46 = 0

ð‘¥ 40 = 1

ð‘¥ 46 = 1

ð‘¥ 222 = 0 ð‘¥ 222 = 1

ð‘¥ 1059 = 0

ð‘¥ 807 = 0

ð‘¥ 1071 = 0

ð‘¥ 917 = 0

ð‘¥ 439 = 1

ð‘¥ 58 = 0

ð‘¥ 1059 = 0

ð‘¥ 439 = 0

ð‘¥ 1388 = 1ð‘¥ 1388 = 0

ð‘¥ 917 = 1

ð‘¥ 1071 = 1

ð‘¥ 807 = 1

ð‘¥ 140 = 1ð‘¥ 140 = 0

ð‘¥ 991 = 1ð‘¥ 991 = 0

ð‘¥ 58 = 1

ð‘¥ 40 = 0

ð‘¥ 46 = 0

ð‘¥ 40 = 1

ð‘¥ 46 = 1

ð‘¥ 222 = 0 ð‘¥ 222 = 1

ð‘¥ 1059 = 0

ð‘¥ 807 = 0

ð‘¥ 1071 = 0

ð‘¥ 917 = 0

ð‘¥ 439 = 1

ð‘¥ 58 = 0

ð‘¥ 1059 = 0

ð‘¥ 439 = 0

ð‘¥ 1388 = 1ð‘¥ 1388 = 0

ð‘¥ 917 = 1

ð‘¥ 1071 = 1

ð‘¥ 807 = 1

ð‘¥ 140 = 1ð‘¥ 140 = 0

ð‘¥ 612 = 1ð‘¥ 612 = 0

ð‘¥ 58 = 1

ð‘¥ 40 = 0

ð‘¥ 46 = 0

ð‘¥ 40 = 1

ð‘¥ 46 = 1

ð‘¥ 222 = 0 ð‘¥ 222 = 1

ð‘¥ 1059 = 0

ð‘¥ 807 = 0

ð‘¥ 1071 = 0

ð‘¥ 917 = 0

ð‘¥ 439 = 1

ð‘¥ 58 = 0

ð‘¥ 1059 = 0

ð‘¥ 439 = 0

ð‘¥ 1388 = 1ð‘¥ 1388 = 0

ð‘¥ 917 = 1

ð‘¥ 1071 = 1

ð‘¥ 807 = 1

ð‘¥ 140 = 1ð‘¥ 140 = 0

ð‘¥ 612 = 1ð‘¥ 612 = 0

ð‘¥ 58 = 1

ð‘¥ 40 = 0

ð‘¥ 46 = 0

ð‘¥ 40 = 1

ð‘¥ 46 = 1

ð‘¥ 222 = 0 ð‘¥ 222 = 1

ð‘¥ 1059 = 0

ð‘¥ 807 = 0

ð‘¥ 1071 = 0

ð‘¥ 917 = 0

ð‘¥ 439 = 1

ð‘¥ 58 = 0

ð‘¥ 1059 = 0

ð‘¥ 439 = 0

ð‘¥ 1388 = 1ð‘¥ 1388 = 0

ð‘¥ 917 = 1

ð‘¥ 1071 = 1

ð‘¥ 807 = 1

ð‘¥ 140 = 1ð‘¥ 140 = 0

ð‘¥ 612 = 1ð‘¥ 612 = 0

ð‘¥ 58 = 1

ð‘¥ 40 = 0

ð‘¥ 46 = 0

ð‘¥ 40 = 1

ð‘¥ 46 = 1

ð‘¥ 222 = 0 ð‘¥ 222 = 1

ð‘¥ 1059 = 0

ð‘¥ 807 = 0

ð‘¥ 1071 = 0

ð‘¥ 917 = 0

ð‘¥ 439 = 1

ð‘¥ 58 = 0

ð‘¥ 1059 = 0

ð‘¥ 439 = 0

ð‘¥ 1388 = 1ð‘¥ 1388 = 0

ð‘¥ 917 = 1

ð‘¥ 1071 = 1

ð‘¥ 807 = 1

ð‘¥ 140 = 1ð‘¥ 140 = 0

ð‘¥ 612 = 1ð‘¥ 612 = 0

#node = 5 

#node = 50 

#node = 200 

DeepB ound 

finds optimal 

solution at 

node 114 

BES 

finds optimal 

solution at 

node 838 

â€¦ â€¦

â€¦â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

Oracle path  Oracle path 

DeepB ound branching tree  BES branching tree Fig. A1 : DeepBound accelerates the solving of MILP problems . Comparison of the branch-and-bound trees between DeepBound and BES when solving identical 300 Ã—1500 combinatorial auction problem. 22 Oracle path 

ð‘¥ 69 = 1

ð‘¥ 11 = 0

ð‘¥ 59 = 0

ð‘¥ 11 = 1

ð‘¥ 59 = 1

ð‘¥ 57 = 0 ð‘¥ 57 = 1

ð‘¥ 70 = 0

ð‘¥ 76 = 0

ð‘¥ 98 = 0

ð‘¥ 88 = 0

ð‘¥ 6 = 1

ð‘¥ 69 = 0

ð‘¥ 70 = 0

ð‘¥ 6 = 0

ð‘¥ 75 = 1ð‘¥ 75 = 0

ð‘¥ 88 = 1

ð‘¥ 98 = 1

ð‘¥ 76 = 1

ð‘¥ 22 = 1ð‘¥ 22 = 0

ð‘¥ 69 = 1

ð‘¥ 11 = 0

ð‘¥ 59 = 0

ð‘¥ 11 = 1

ð‘¥ 59 = 1

ð‘¥ 57 = 0 ð‘¥ 57 = 1

ð‘¥ 70 = 0

ð‘¥ 76 = 0

ð‘¥ 98 = 0

ð‘¥ 88 = 0

ð‘¥ 6 = 1

ð‘¥ 69 = 0

ð‘¥ 70 = 0

ð‘¥ 6 = 0

ð‘¥ 75 = 1ð‘¥ 75 = 0

ð‘¥ 88 = 1

ð‘¥ 98 = 1

ð‘¥ 76 = 1

ð‘¥ 22 = 1ð‘¥ 22 = 0

ð‘¥ 69 = 1

ð‘¥ 11 = 0

ð‘¥ 59 = 0

ð‘¥ 11 = 1

ð‘¥ 59 = 1

ð‘¥ 57 = 0 ð‘¥ 57 = 1

ð‘¥ 70 = 0

ð‘¥ 76 = 0

ð‘¥ 98 = 0

ð‘¥ 88 = 0

ð‘¥ 6 = 1

ð‘¥ 69 = 0

ð‘¥ 70 = 0

ð‘¥ 6 = 0

ð‘¥ 75 = 1ð‘¥ 75 = 0

ð‘¥ 88 = 1

ð‘¥ 98 = 1

ð‘¥ 76 = 1

ð‘¥ 22 = 1ð‘¥ 22 = 0

ð‘¥ 69 = 1

ð‘¥ 11 = 0

ð‘¥ 59 = 0

ð‘¥ 11 = 1

ð‘¥ 59 = 1

ð‘¥ 57 = 0 ð‘¥ 57 = 1

ð‘¥ 70 = 0

ð‘¥ 76 = 0

ð‘¥ 98 = 0

ð‘¥ 88 = 0

ð‘¥ 6 = 1

ð‘¥ 69 = 0

ð‘¥ 70 = 0

ð‘¥ 6 = 0

ð‘¥ 75 = 1ð‘¥ 75 = 0

ð‘¥ 88 = 1

ð‘¥ 98 = 1

ð‘¥ 76 = 1

ð‘¥ 22 = 1ð‘¥ 22 = 0

ð‘¥ 69 = 1

ð‘¥ 11 = 0

ð‘¥ 59 = 0

ð‘¥ 11 = 1

ð‘¥ 59 = 1

ð‘¥ 57 = 0 ð‘¥ 57 = 1

ð‘¥ 70 = 0

ð‘¥ 76 = 0

ð‘¥ 98 = 0

ð‘¥ 88 = 0

ð‘¥ 5 = 1

ð‘¥ 69 = 0

ð‘¥ 70 = 0

ð‘¥ 5 = 0

ð‘¥ 49 = 1ð‘¥ 49 = 0

ð‘¥ 88 = 1

ð‘¥ 98 = 1

ð‘¥ 76 = 1

ð‘¥ 2 = 0 ð‘¥ 2 = 1

ð‘¥ 83 = 0 ð‘¥ 83 = 1

ð‘¥ 69 = 1

ð‘¥ 11 = 0

ð‘¥ 59 = 0

ð‘¥ 11 = 1

ð‘¥ 59 = 1

ð‘¥ 57 = 0 ð‘¥ 57 = 1

ð‘¥ 70 = 0

ð‘¥ 76 = 0

ð‘¥ 98 = 0

ð‘¥ 88 = 0

ð‘¥ 5 = 1

ð‘¥ 69 = 0

ð‘¥ 70 = 0

ð‘¥ 5 = 0

ð‘¥ 49 = 1ð‘¥ 49 = 0

ð‘¥ 88 = 1

ð‘¥ 98 = 1

ð‘¥ 76 = 1

ð‘¥ 2 = 0 ð‘¥ 2 = 1

ð‘¥ 83 = 0 ð‘¥ 83 = 1

ð‘¥ 69 = 1

ð‘¥ 11 = 0

ð‘¥ 59 = 0

ð‘¥ 11 = 1

ð‘¥ 59 = 1

ð‘¥ 57 = 0 ð‘¥ 57 = 1

ð‘¥ 70 = 0

ð‘¥ 76 = 0

ð‘¥ 98 = 0

ð‘¥ 88 = 0

ð‘¥ 5 = 1

ð‘¥ 69 = 0

ð‘¥ 70 = 0

ð‘¥ 5 = 0

ð‘¥ 49 = 1ð‘¥ 49 = 0

ð‘¥ 88 = 1

ð‘¥ 98 = 1

ð‘¥ 76 = 1

ð‘¥ 2 = 0 ð‘¥ 2 = 1

ð‘¥ 83 = 0 ð‘¥ 83 = 1

ð‘¥ 69 = 1

ð‘¥ 11 = 0

ð‘¥ 59 = 0

ð‘¥ 11 = 1

ð‘¥ 59 = 1

ð‘¥ 57 = 0 ð‘¥ 57 = 1

ð‘¥ 70 = 0

ð‘¥ 76 = 0

ð‘¥ 98 = 0

ð‘¥ 88 = 0

ð‘¥ 5 = 1

ð‘¥ 69 = 0

ð‘¥ 70 = 0

ð‘¥ 5 = 0

ð‘¥ 49 = 1ð‘¥ 49 = 0

ð‘¥ 88 = 1

ð‘¥ 98 = 1

ð‘¥ 76 = 1

ð‘¥ 2 = 0 ð‘¥ 2 = 1

ð‘¥ 83 = 0 ð‘¥ 83 = 1

#node = 5 

#node = 50 

#node = 200 

DeepB ound 

finds optimal 

solution at 

node 199 

BES 

finds optimal 

solution at 

node 567 

â€¦

â€¦

â€¦ â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦â€¦

â€¦â€¦

Oracle path 

DeepB ound branching tree  BES branching tree Fig. A2 : DeepBound accelerates the solving of MILP problems . Comparison of the branch-and-bound trees between DeepBound and BES when solving identical 400 Ã—100 capacitated facility location problem. 23 Appendix B Definitions 

B.1 Node selection rules 

In the process of node selection, finding the node with the optimal solution as soon as possible can tighten the primal-bound of the MILPâ€™s branch-and-bound search tree rapidly, which reduces the size of the search tree by pruning unpromising nodes [16, 42]. Although limited by primal-bound and dual-bound, extremely large number of nodes may be created in the branch-and-bound solving process of practical problems, which consist of thousands or even millions of constraints and variables. In such conditions, the node selection rules are on a mission to guide the branch-and-bound algorithm to nodes that are likely to contain better feasible solution or better LP relaxation which can help to tighten global dual-bound [14]. For the former target, the depth-first-search (DFS) rule greedily selects the child of current solving node, by which may find an integer feasible solution as soon as possible [16]. For the latter target, the best-first-search (BFS) rule is designed to select node with the worst LP relaxation solution to be the next solving node, which can improve global dual-bound at every node selection step [16]. However, on the one hand, although DFS can obtain integer feasible solution very early, it completely neglects the tightening of dual-bound, which results in tediously long solving time. On the other hand, although BFS efficiently reduces redundant LP relaxation search space, it cannot promote the tightening of global primal-bound. In order to take care of both two goals, best-estimate-search (BES) rule will calculate the estimate score of every node, which combines information about the dual bound with the information about integrality of the LP solution. It will select the node with the lowest score to solve, which means higher possibility to improve global dual-bound and to find a better integer feasible solution. In practice, in order to speed up the search for an integer feasible solution, BFS rules or BES rules will be combined with the plunging or diving technology, which quickly explores the children of the current node, trying to find a better integer feasible solution [16]. 

B.2 Branching rules 

The target of branch variable selection algorithm is to select the branch variable that can significantly improve the LP relaxation of sub-problems, by which can tighten the dual bound of the branch-and-bound tree as soon as possible to reduce the calcula-tion of redundant nodes [43]. To this end, full-strong branching (FSB) is designed to calculate the LP relaxation of every potential branching variable to test which one gives the best improvement in the dual-bound before actually branching on any of them. Although FSB can provide a gold standard to branching variable selection, its unbearably time consumption makes it scarcely practicable to solve large-scale MILP problems. In order to improve the computing efficiency, pseudocost branching is pro-posed to utilize the branching history information as an indication of how a branching variable improved the global dual bound before this selection [16]. By combining branching history and measurement if infeasibility, pseudocost branching dramati-cally speeds up branching calculation with the cost of large number of redundant nodes because of sparse branching history information at the beginning of branch-and-bound solving process. To overcome this disadvantage, hybrid branching and reliability branching (RPB) [44] combines FSB branching and pseudocost branching by using the former at the early stage, which provides informative branching history to the lat-ter. In practice, RPB is the default branching rule of SCIP [16] to trade off between LP relaxation efficiency and the size of branch-and-bound search tree. 24 Appendix C Methods 

C.1 Evaluation data sets 

Our dataset consists of benchmarks for three different NP-hard MILP problems: set covering, combinatorial auctions, and capacitated facility location. For the set covering problem, we follow the settings described by Balas and Ho [45], generating instance sets with 1,000 columns (i.e., variables) and 500 (easy), 1,000 (medium), and 2,000 (hard) rows (i.e., constraints). Unlike the model trained on instances with 500 rows in Gasse et al. [20], we train and test on a set covering instance with 1,000 rows because SCIP was unable to generate sufficient node data when solving instances with 500 rows. The combinatorial auction instances were generated following the procedure by Leyton-Brown et al. [46], producing instance sets of varying difficulty: 500 bids with 100 items (easy), 1,000 bids with 200 items (medium), and 1,500 bids with 300 items (hard). Similar to the set covering problem, to generate sufficient node training data, we train our model on instances of 1,000 bids with 200 items and evaluated across all three instance scales. For the capacitated facility location problem [47], we generated instance sets with 100, 200, and 400 customers. The model was trained on a training set of 200-customer instances and tested on all three difficulty levels. For all three problems, the training set consists of 5,000 instances, while the test sets for each difficulty level contain 100 generated problem instances. 

C.2 Training data generation 

To collect the training trajectories of oracle nodes, we solved the original MILP prob-lem twice using SCIP with the same FSB branching rule. The first run was used to obtain the optimal solution to the original problem, and the second run labeled the oracle nodes using the known optimal solution. We employed the FSB branching vari-able selection rule because its branching decisions are independent of node selection history, unlike the RPB branching rule used in previous method. If the RPB rule were used, as in He et al. [17], the branching history would heavily influence the heuris-tic decisions, leading to entirely different branching variable selection results during the second solve, thus introducing inconsistencies. After discovering the optimal solu-tion, SCIP further expands the remaining nodes under global primal and dual bound constraints to prove optimality. All nodes generated during this phase are labeled as non-oracle nodes. To mitigate the impact of the imbalance between positive and negative samples in the training data, we introduce a pairwise training protocol and a ranking-based learning method. On one hand, although oracle nodes are rare in the branch-and-bound search tree, pairing them with non-oracle nodes randomly selected from the same priority queue forms training pairs. This expands the number of effective node pairs and improves the utilization of positive samples in the branch-and-bound tree. On the other hand, pairing one positive sample node with all negative sample nodes in the same priority queue for training creates a data combination that helps the model learn the differences between positive and negative samples. 

C.3 Model architecture 

We use a multi-level feature fusion neural network (fusion model) to learn the repre-sentation of node features, with each fusion module consisting of several concatenated fusion blocks. We notice that previous work utilized graph neural networks or MLPs 25 Algorithm 1 Training Data Generation 

Require: MILP problem instance Q, initial priority queue P = {N 0}

Ensure: Training dataset D(Q)1: Solve Q via branch-and-bound solver  

> 2:

if Sâˆ— found during presolve then â–· S âˆ—: optimal solution  

> 3:

D(Q) â† âˆ…  

> 4:

else  

> 5:

Let N âˆ— denote the node where Sâˆ— is discovered  

> 6:

Let P athN odes be nodes on the path from N0 to N âˆ— 

> 7:

Mark N âˆ— and P athN odes as B â–· B: BPB-Nodes  

> 8:

Re-solve problem Q 

> 9:

while PÌ¸ = âˆ… do  

> 10:

if âˆƒN i âˆˆ B âˆ© P then  

> 11:

D(Q) â† D (Q) âˆª { [( xi, yi), (xj , yj )] }âˆ€N j âˆˆP\{N i} â–· xi: features, yi: labels  

> 12:

end if  

> 13:

Select N â€² = arg max N âˆˆP score (N ) â–· score (Â·): scoring function of solver  

> 14:

Branch N â€² into two children {N â€²

> 1

, N â€²

> 2

} 

> 15:

P â† (P \ {N â€²}) âˆª {N â€²

> 1

, N â€²

> 2

} 

> 16:

end while  

> 17:

end if  

> 18:

return D(Q)MLP node                   

> 1D  Batch Norm 1D Batch Norm
> MLP feat
> Node -level fusion
> Feature
> ++
> Fusion Block
> Node
> â€¦
> Feature
> Node
> Feature -level fusion
> Transpose
> Transpose
> FC +ReLU
> FC +ReLU
> Dropout
> FC +ReLU
> FC +ReLU
> FC +ReLU
> Dropout
> FC +ReLU
> Dropout
> MLP node MLP feat
> ab

Fig. C3 : Fusion block in DeepBound . a, The fusion block schematic demonstrates multi-level feature integration for node pair, where M LP node performs node-level feature aggregation, while M LP f eat enables feature-level fusion. b, Detailed imple-mentations of the core components M LP node and M LP f eat .designed to encode the features or representations of individual nodes without focus-ing on information aggregation and comparison across multiple nodes. Therefore, the fusion block includes two modules: a node-level feature fusion module and a feature-level feature fusion module. These modules are constructed using MLPs with different structures, each introducing varying numbers of dropout layers to prevent overfitting and using residual connections to achieve feature fusion. Additionally, since we use the DeepBound model to encode and score node features, the robustness of the model is critical. We integrate ensemble learning mechanisms into the training of the DeepBound model. The complete DeepBound node selector 26 consists of M independent fusion models, trained by ensemble learning techniques. Inspired by the bagging method, we partition the training data into M subsets, each subset is used to train one independent fusion model. The training hyperparameters are kept consistent in all models. When evaluating the model performance, the final score of the ensemble model is the average of the scores from the M fusion models. To better assess the training results, we apply K-fold cross-validation on the train-ing set. Different partitions of the training data are used to train the ensemble models. The best-performing ensemble model of this process is selected as the final test model. This approach helps mitigate the issue of high variance from individual fusion modelâ€™s outputs and improves the stability of the node selector. 

C.4 Input features 

We use various features of nodes to form a vector that describes the node information. The intrinsic features of a node primarily include the node lower bound, which is the value of the objective function after the nodeâ€™s linear relaxation, and the Node Estimate, which is the estimated value of the node derived from the Best First Search (BFS) rule. Additional features include the node depth and node type (whether it is a child, sibling, or another leaf node of the current solving node). Furthermore, we incorporate features describing the current state of the branch-and-bound tree, such as the global upper and lower bounds, into the input feature vector. 

C.5 Training loss 

We train the DeepBound model using a mean squared error (MSE) loss between predicted scores and ground-truth labels. The loss function is defined as: 

L(Î¸) = 1

n

> n

X

> i=1

(pÎ¸ (fi) âˆ’ yi)2 (C1) where: 

â€¢ n denotes the number of nodes in a training batch 

â€¢ fi represents the feature vector of the i-th node 

â€¢ yi indicates the ground-truth score of the i-th node 

â€¢ pÎ¸ denotes the scoring function of the DeepBound model with parameters Î¸

This objective function directly optimizes the modelâ€™s ability to accurately regress node scores, aligning predictions with the oracle ranking labels. 

C.6 Model inference 

During inference, we replace the node selection module in the SCIP solver with the average node score of multiple fusion models integrated in the DeepBound model. When solving mixed-integer programming problems via the branch-and-bound algo-rithm, the solver selects the highest scoring node from the priority queue containing all unsolved nodes to branch on. The feature vectors of the two newly generated nodes from branching are input as pairs into the DeepBound model. These feature vector pairs are encoded by M independent fusion models, generating M sets of scores, which are then averaged to obtain the final output score for the node pair. This score is subsequently used by the solver to rank the two new nodes when inserting them back into the priority queue, ensuring that all unsolved nodes are 27 ordered in descending score. The branch-and-bound solver, combined with the Deep-Bound model, continues this process until all unsolved nodes in the priority queue have either been solved or pruned according to the global upper bound, ultimately yielding the exact solution to the original mixed-integer programming problem. 28