<div class="paper-title-row">
<h1 class="paper-title-zh">在测试时学习发现</h1>
<h1 class="paper-title-en">Learning to Discover at Test Time</h1>
</div>

<div class="paper-meta-row">
<div class="paper-meta-left">
<p><strong>Authors</strong>: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</p>
<p><strong>Date</strong>: 2026-01-22</p>
<p><strong>PDF</strong>: <a href="https://arxiv.org/pdf/2601.16175v1" target="_blank">https://arxiv.org/pdf/2601.16175v1</a></p>
<p><strong>Tags</strong>: <span class="tag-label tag-green">EOH</span> <span class="tag-label tag-green">EAA</span></p>
<p><strong>Score</strong>: 8.0</p>
</div>
<div class="paper-meta-right">
<p><strong>Evidence</strong>: 通过测试时搜索和强化学习自动发现最先进的解决方案</p>
<p><strong>TLDR</strong>: 提出 TTT-Discover，利用测试时强化学习和搜索来发现科学问题的最优解。</p>
</div>
</div>

<div class="paper-glance-section">
<h2 class="paper-glance-title">速览</h2>
<div class="paper-glance-row">
<div class="paper-glance-col">
<div class="paper-glance-label">Motivation</div>
<div class="paper-glance-content">传统的测试时缩放方法依赖冻结的 LLM，难以针对特定科学问题进行深度持续学习以寻找最优解。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Method</div>
<div class="paper-glance-content">在测试时引入强化学习（RL）和搜索机制，使模型能针对当前问题进行持续训练，并优先处理最有潜力的候选方案。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Result</div>
<div class="paper-glance-content">在 Erdős 数学问题、GPU 内核加速（达 2 倍）、AtCoder 竞赛及单细胞分析中均获 SOTA，且仅需开源模型和数百美元成本。</div>
</div>
<div class="paper-glance-col">
<div class="paper-glance-label">Conclusion</div>
<div class="paper-glance-content">研究表明，测试时训练（TTT）是 AI 驱动科学发现的强大范式，能以较低成本在特定任务上超越闭源前沿模型。</div>
</div>
</div>
<div class="paper-glance-tldr"><strong>TLDR</strong>：提出 TTT-Discover 方法，通过在测试阶段对 LLM 进行强化学习，在数学、GPU 优化、算法和生物学等多个领域刷新了世界纪录。</div>
</div>

---


## 摘要
如何利用人工智能为科学问题发现新的最先进（SOTA）成果？先前的测试时缩放（test-time scaling）工作（如 AlphaEvolve）通过提示冻结的大语言模型（LLM）来进行搜索。我们在测试时进行强化学习，使 LLM 能够继续训练，但现在是利用针对特定测试问题的经验。这种形式的持续学习非常特殊，因为其目标是产生一个极佳的解决方案，而不是平均意义上的许多良好方案，并且旨在解决当前问题而非泛化到其他问题。因此，我们的学习目标和搜索子程序旨在优先考虑最有希望的解决方案。我们将此方法称为“测试时训练以发现”（TTT-Discover）。遵循先前的工作，我们专注于具有连续奖励的问题。我们报告了在数学、GPU 内核工程、算法设计和生物学领域尝试过的每一个问题的结果。TTT-Discover 在几乎所有问题上都创造了新的最先进水平：(i) 埃尔德什（Erdős）最小重叠问题和自相关不等式；(ii) GPUMode 内核竞赛（比现有技术快达 2 倍）；(iii) 过去的 AtCoder 算法竞赛；以及 (iv) 单细胞分析中的去噪问题。我们的解决方案经过了专家或组织者的审查。与之前需要封闭前沿模型的最优结果不同，我们的所有结果都是使用开源模型 OpenAI gpt-oss-120b 实现的，并且可以通过我们公开的代码进行复现。我们的测试时训练运行是使用 Thinking Machines 的 API Tinker 执行的，每个问题的成本仅为几百美元。

## 速览
**TLDR**：本研究提出TTT-Discover方法，旨在利用AI突破科学问题的现有最优解。不同于以往仅通过提示词搜索的测试时缩放方法，该方法在测试阶段引入强化学习，使LLM能针对特定问题进行持续训练。其核心目标是产出单一的最优解而非平均性能。实验表明，该方法在数学、GPU内核工程、算法设计和生物学等多个领域均刷新了世界纪录，且仅需使用开源模型和较低的计算成本即可实现。 \
**Motivation**：旨在克服现有测试时缩放方法中模型参数冻结的局限，通过针对特定问题的持续学习来发现科学问题的最优解。 \
**Method**：提出TTT-Discover，在测试时利用强化学习和专门设计的搜索子程序，使LLM能够根据实时反馈不断优化其解决特定问题的能力。 \
**Result**：在Erdős数学问题、GPU内核优化（提速达2倍）、AtCoder竞赛及单细胞去噪等任务中均取得了新的SOTA结果。 \
**Conclusion**：验证了测试时训练（TTT）在科学发现中的巨大潜力，证明开源模型在特定优化下可超越闭源前沿模型。

---

## Abstract
How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

---

## 论文详细总结（自动生成）

这是一份关于论文《Learning to Discover at Test Time》的结构化深入总结：

### 1. 论文的核心问题与整体含义
*   **研究动机**：科学发现本质上要求产生超出模型训练数据、甚至超出人类现有知识的“分布外（OOD）”想法。
*   **核心问题**：现有的测试时缩放（Test-time Scaling）方法（如 AlphaEvolve）主要依赖于提示（Prompting）冻结的 LLM 进行搜索。这种方式类似于学生只能通过不断尝试来猜答案，而无法真正“内化”新思路。
*   **整体含义**：论文提出在测试阶段对 LLM 进行**强化学习（RL）**，使其能够针对特定的单一问题进行持续训练。这种“测试时训练（TTT）”的目标不是为了泛化，而是为了在当前这一个问题上找到一个极佳的（SOTA）解决方案。

### 2. 论文提出的方法论：TTT-Discover
核心思想是将科学发现建模为一个马尔可夫决策过程（MDP），并在测试时通过 RL 优化模型参数。
*   **关键技术细节**：
    *   **熵目标函数（Entropic Objective, $J_\beta$）**：不同于标准 RL 最大化平均奖励，TTT-Discover 旨在寻找最大值。通过引入指数加权（熵 utility），使目标函数在 $\beta \to \infty$ 时趋向于最大奖励。
    *   **自适应 $\beta$ 调节**：为了防止训练不稳定，论文设计了一种自适应机制，根据 KL 散度约束动态调整 $\beta$，确保权重更新不会偏离原始策略太远。
    *   **PUCT 状态重用（State Reuse）**：借鉴 AlphaZero 的搜索算法，使用基于排名和访问次数的 PUCT 规则从历史缓冲区中选择初始状态，平衡了对高奖励方案的“利用”和对未探索方向的“探索”。
*   **算法流程**：
    1.  初始化缓冲区（如空解或人类已知解）。
    2.  **循环（50步）**：
        *   使用 **PUCT** 选择一个起始状态。
        *   LLM 生成动作（代码+思考过程）。
        *   环境执行代码并评估**连续奖励**（如运行时间的倒数、数学边界值）。
        *   使用**熵目标函数**和 **LoRA** 微调模型参数。
    3.  返回缓冲区中奖励最高的单一解。

### 3. 实验设计
论文在四个极具挑战性的领域进行了实验：
*   **数学**：Erdős 最小重叠问题、自相关不等式、圆堆积问题。
*   **GPU 内核工程**：GPUMode 竞赛中的 TriMul（三角矩阵乘法）和 DeepSeek MLA 内核优化。
*   **算法设计**：AtCoder 启发式竞赛（ahc039, ahc058）。
*   **生物学**：OpenProblems 平台上的单细胞 RNA 测序去噪任务。
*   **Benchmark 与对比方法**：
    *   **基准**：人类专家最高纪录、先前最强 AI（AlphaEvolve, ThetaEvolve, ALE-Agent）。
    *   **对比**：Best-of-N（同等采样预算但不训练）、OpenEvolve（开源进化算法）、以及消融实验（无训练、无重用、标准 RL 等）。

### 4. 资源与算力
*   **模型**：使用开源模型 **OpenAI gpt-oss-120b**（基于 Qwen 架构）。
*   **算力平台**：通过 Thinking Machines 的 **Tinker API** 运行。
*   **训练细节**：
    *   **LoRA**：秩（rank）为 32。
    *   **规模**：每个问题运行 50 个训练步骤，每步生成 512 个样本（总计 25,600 次采样）。
    *   **成本**：每个问题的平均训练成本约为 **500 美元**。
    *   **硬件**：评估过程使用了 NVIDIA H100、A100、H200 以及 AMD MI300X 等高性能 GPU。

### 5. 实验数量与充分性
*   **实验组数**：涵盖了 4 大领域、约 8 个具体科学/工程问题。
*   **消融实验**：针对训练目标（熵 vs 期望）、重用策略（PUCT vs ϵ-greedy vs 无重用）以及测试时训练本身进行了详细对比。
*   **充分性与公平性**：
    *   **公平性**：所有对比均在相同的采样预算（25,600）下进行。
    *   **客观性**：结果由外部专家（如数学教授、GPUMode 组织者、生物学教授）进行独立审查或通过官方竞赛平台验证。
    *   **复现性**：代码和发现的 SOTA 构造均已开源。

### 6. 主要结论与发现
*   **刷新纪录**：在 Erdős 最小重叠问题上发现了新的数学上界；在 GPU 内核优化中比人类专家最快纪录快了 **15%-50%**。
*   **竞赛获胜**：在 AtCoder 竞赛中，AI 生成的算法若在当时提交可获得**第一名**。
*   **开源超越闭源**：证明了通过测试时训练，使用 **120B 的开源模型** 即可超越使用 Gemini 2.0 Pro 等闭源前沿模型构建的系统。
*   **学习优于搜索**：实验证明，单纯增加采样（Best-of-N）很快会遇到瓶颈，而通过 RL 持续改进模型参数能显著提升解的质量。

### 7. 优点
*   **高效性**：相比于大规模预训练，TTT-Discover 是一种极其廉价且高效的“临阵磨枪”方式。
*   **目标明确**：专门为“寻找单一最优解”设计的熵目标函数非常契合科学发现的本质。
*   **跨领域通用**：方法论不依赖特定领域知识，在数学、代码、生物等多个领域表现稳健。
*   **专家背书**：实验结果不仅有数值提升，还得到了相关领域专家的定性认可。

### 8. 不足与局限
*   **奖励函数依赖**：该方法高度依赖于**连续且可验证**的奖励函数。对于奖励稀疏（只有对/错）或无法自动验证的领域（如主观创作），目前难以应用。
*   **上下文窗口限制**：受限于 LLM 的上下文长度（32k），复杂的重用策略（如包含大量思考过程的 Prompt）可能导致信息截断。
*   **生物学验证风险**：虽然在去噪指标（MSE）上获得 SOTA，但专家指出，指标的提升并不一定等同于生物学意义上的新发现，仍需进一步实验。
*   **计算延迟**：虽然成本较低，但 50 步的训练过程仍需数小时，不适用于对实时性要求极高的场景。

（完）
