---
title: Kolmogorov-Arnold Networks Applied to Materials Property Prediction
title_zh: Kolmogorov-Arnold 网络在材料性质预测中的应用
authors: "Ryan Jacobs, Lane E. Schultz, Dane Morgan"
date: 2026-01-27
pdf: "https://arxiv.org/pdf/2601.20097v1"
tags: ["keyword:SR", "query:SR"]
score: 7.0
evidence: KAN作为材料科学的可解释回归模型
tldr: 本研究评估了 Kolmogorov-Arnold 网络 (KAN) 在 33 种材料属性预测中的表现。实验表明，经过优化的 KAN 在预测精度上与随机森林相当，且在反应堆压力容器脆化预测任务中，仅需极少参数即可生成与专家手工模型精度相媲美的闭式表达式。这证明了 KAN 在材料科学领域具有显著的参数效率和可解释性优势，是极具潜力的回归建模工具。
motivation: 旨在探索 KAN 这一新型神经网络架构在材料属性预测中的参数效率、可解释性以及相对于传统机器学习方法的竞争优势。
method: 通过在 33 种材料数据集上对比 KAN 与随机森林的性能，并针对特定物理问题利用 KAN 提取简化的闭式数学表达式。
result: "KAN 在约 65% 的任务中表现优于或等同于随机森林，且能以不到 50 个参数实现与复杂专家模型相当的预测精度。"
conclusion: KAN 为材料科学提供了一种兼具高性能与高可解释性的回归模型，尤其适合在缺乏领域专家知识的情况下发现物理规律。
---

## 摘要
Kolmogorov-Arnold 网络 (KANs) 被提出作为基于多层感知器 (MLP-NNs) 的传统神经网络架构的替代方案。KANs 相对于 MLP-NNs 的潜在优势，包括显著增强的参数效率和更高的可解释性，使其成为监督机器学习问题中一种极具前景的新型回归模型。我们将 KANs 应用于材料性质预测，重点关注由实验和计算数据组成的 33 种不同性质。我们将 KAN 的结果与随机森林进行了比较，后者是一种通常在无需太多优化的情况下就能在广泛的性质预测中表现出色的方法。KANs 的表现分别在约 35%、60% 和 5% 的情况下差于、持平或优于随机森林，且在实践中 KANs 比随机森林更难拟合。通过调整网络架构，我们发现性质拟合的误差通常比标准 KAN 降低了 10-20%，并且通常能得到与随机森林相当的结果。在预测反应堆压力容器转变温度偏移的具体背景下，我们通过将简单 KAN 模型（例如，少于 50 个参数）的预测结果以及由 KAN 拟合建议的闭式表达式，与先前发表的深度 MLP-NNs 和利用脆化物理领域专业知识创建的手动调优模型进行比较，探索了 KANs 的参数效率和可解释能力。我们发现，简单的 KAN 模型及其产生的闭式表达式所产生的预测误差与具有相当参数数量的成熟手动调优模型相当，且在生成过程中基本上不需要领域专业知识。这些发现增强了 KANs 在材料科学机器学习中的潜在适用性，并表明应将 KANs 作为预测材料性质的回归模型进行进一步探索。

## Abstract
Kolmogorov-Arnold Networks (KANs) were proposed as an alternative to traditional neural network architectures based on multilayer perceptrons (MLP-NNs). The potential advantages of KANs over MLP-NNs, including significantly enhanced parameter efficiency and increased interpretability, make them a promising new regression model in supervised machine learning problems. We apply KANs to prediction of materials properties, focusing on a diverse set of 33 properties consisting of both experimental and calculated data. We compare the KAN results to random forest, a method that generally gives excellent performance on a wide range of properties predictions with very little optimization. The KANs were worse, on par, or better than random forest about 35%, 60%, and 5% of the time, respectively, and KANs are in practice more difficult to fit than random forest. By tuning the network architecture, we found property fits often resulted in 10-20% lower errors compared to the standard KAN, and typically gave results comparable to random forest. In the specific context of predicting reactor pressure vessel transition temperature shifts, we explored the parameter efficiency and the interpretable power of KANs by comparing predictions of simple KAN models (e.g., < 50 parameters) and closed-form expressions suggested by the KAN fits to previously published deep MLP-NNs and hand-tuned models created using domain expertise of embrittlement physics. We found that simple KAN models and the resulting closed-form expressions produce prediction errors on par with established hand-tuned models with a comparable number of parameters, and required essentially no domain expertise to produce. These findings reinforce the potential applicability of KANs for machine learning in materials science and suggest that KANs should be explored as a regression model for prediction of materials properties.