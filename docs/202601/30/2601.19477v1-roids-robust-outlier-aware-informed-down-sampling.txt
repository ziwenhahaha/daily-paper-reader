Title: ROIDS: Robust Outlier-Aware Informed Down-Sampling

URL Source: https://arxiv.org/pdf/2601.19477v1

Published Time: Wed, 28 Jan 2026 01:59:21 GMT

Number of Pages: 11

Markdown Content:
# ROIDS: Robust Outlier-Aware Informed Down-Sampling 

# Alina Geiger 

Johannes Gutenberg University Mainz, Germany geiger@uni-mainz.de 

# Martin Briesch 

Johannes Gutenberg University Mainz, Germany briesch@uni-mainz.de 

# Dominik Sobania 

University of Duisburg-Essen Essen, Germany dominik.sobania@uni-due.de 

# Franz Rothlauf 

Johannes Gutenberg University Mainz, Germany rothlauf@uni-mainz.de 

## Abstract 

Informed down -sampling (IDS) is known to improve performance in symbolic regression when combined with various selection strate-gies, especially tournament selection. However, recent work found that IDS‚Äôs gains are not consistent across all problems. Our analysis reveals that IDS performance is worse for problems containing outliers. IDS systematically favors including outliers in subsets which pushes GP towards finding solutions that overfit to outliers. To address this, we introduce ROIDS (Robust Outlier -Aware In-formed Down -Sampling), which excludes potential outliers from the sampling process of IDS. With ROIDS it is possible to keep the advantages of IDS without overfitting to outliers and to compete on a wide range of benchmark problems. This is also reflected in our experiments in which ROIDS shows the desired behavior on all studied benchmark problems. ROIDS consistently outperforms IDS on synthetic problems with added outliers as well as on a wide range of complex real-world problems, surpassing IDS on over 80% of the real-world benchmark problems. Moreover, compared to all studied baseline approaches, ROIDS achieves the best average rank across all tested benchmark problems. This robust behavior makes ROIDS a reliable down-sampling method for selection in symbolic regression, especially when outliers may be included in the data set. 

## Keywords 

Symbolic regression, genetic programming, parent selection, down-sampling 

## 1 Introduction 

Down-sampled selection strategies have been found to enhance the performance and generalization ability of Genetic Programming (GP) in areas like program synthesis [ 12 , 25 ] and symbolic regres-sion [ 16 , 17 ], while simultaneously reducing the computational costs [ 15 , 20 ]. Instead of evaluating the candidate solutions on all available training cases in each selection event, down-sampled selec-tion methods only use a subset of the training cases to evaluate the quality of the candidate solutions in a population. A na√Øve approach is random down-sampling (RDS) [ 20 ], where in each generation a subset of training cases is randomly sampled. 

> Authors‚Äô Contact Information: Alina Geiger, Johannes Gutenberg University, Mainz, Germany, geiger@uni-mainz.de; Martin Briesch, Johannes Gutenberg University, Mainz, Germany, briesch@uni-mainz.de; Dominik Sobania, University of Duisburg-Essen, Essen, Germany, dominik.sobania@uni-due.de; Franz Rothlauf, Johannes Guten-berg University, Mainz, Germany, rothlauf@uni-mainz.de.

Unfortunately, subsets of training cases generated by RDS often contain many similar cases that bring in only little new information, while it potentially excludes other, more rare edge or corner cases which are often much more informative for GP. Ignoring such rare but relevant edge cases for several generations potentially harms the overall performance of RDS [3]. Therefore, informed down-sampling (IDS) [ 3] has been proposed to create diverse subsets, where each case is intended to test dif-ferent niches. To reach this goal, IDS evaluates a fraction of the population on all cases in the training set and then constructs for each case a vector which measures the performance of the eval-uated candidate solutions for this case. As next step, it computes the distance matrix between all vectors and selects the subset of the population that maximizes the pairwise distances in the subset. Based on this procedure, IDS selects a diverse subset of training cases with regards to the behavior of the current population. Originally, IDS was described for program synthesis problems, where GP using IDS is able to find better results [ 3]. It also increases performance in the domain of symbolic regression, although the performance gains are not consistent over the range of different problems. On some real-world regression problems, IDS performs poorly and even worse than RDS [15, 18]. To gain a deeper understanding of the differences in performance, we analyze the behavior of IDS for regression problems in detail. We find that IDS outperforms RDS for problems with rare cases but performs worse when outliers are present, which is often the case in real-world regression problems. To overcome this problem, we propose Robust Outlier-Aware Informed Down-Sampling ( ROIDS )which excludes potential outliers from the sampling process of IDS. With ROIDS, we are able to leverage the advantages of IDS without overfitting to outliers to compete even on real-world regression problems. We compare the performance of ROIDS, IDS, and RDS on a set of synthetic problems with and without outliers as well as for a representative set of real-world regression problems. We find that ROIDS performs significantly better than IDS on all synthetic problems in the presence of outliers and never worse. Moreover, the average ranking of ROIDS across all synthetic problem is better compared to IDS and RDS. Also for real-world problems, we find that ROIDS outperforms IDS on problems where IDS only produces poor results. Further-more, we find that ROIDS outperforms IDS even when we apply an outlier removal technique beforehand. We visualize the problems us-ing UMAPs (Uniform Manifold Approximation and Projection [ 33 ]) 

> arXiv:2601.19477v1 [cs.NE] 27 Jan 2026 Geiger et al.

for dimensionality reduction and observe that ROIDS, as intended, focuses less on potential outliers. Overall, ROIDS achieves a lower error than IDS on over 80% of the studied problems. Again, the average ranking of 1.7 achieved by ROIDS is better compared to the rankings of RDS and IDS, which are 2.5 and 3, respectively. This indicates that ROIDS performs consistently well across different problems. From a computational perspective, ROIDS comes at almost no additional cost, but leads to similar or better search performance than IDS, especially for datasets with outliers. As the implementa-tion of ROIDS is of low complexity and it always achieves a better or comparable performance, we recommend researches as well as practitioners to replace IDS with ROIDS for symbolic regression. Following this introduction, we present the related work in Sect. 2 and an analysis of the behavior of IDS in Sect. 3. In Sect. 4, we describe ROIDS. Section 5 describes our experimental setup, followed by a presentation of our results in Sect. 6. The conclusions are drawn in Sect. 7. 

## 2 Related work 

Down-sampling strategies evaluate candidate solutions using only a subset of the training cases in each generation of an evolutionary run. This reduces the computational cost per generation, which allows the reallocation of the saved evaluations to either a longer search or larger population sizes [7, 22]. A simple yet effective down-sampling strategy is RDS, which randomly samples a fraction of the training cases in each generation. Early work found, that RDS reduces overfitting as well as bloat in GP runs for symbolic regression problems [ 20 ]. More recently, RDS combined with lexicase selection [ 24 , 37 ] increased the problem-solving success in the domain of program synthesis [ 12 , 25 , 36 ]and symbolic regression [ 16 , 17 ]. The main advantage of down-sampled lexicase selection is its ability to evaluate a higher number of candidate solutions with the same computational costs [22, 23]. However, as RDS selects training cases randomly, it does not prefer edge or corner cases which would be relevant for GP. Instead, such cases may be excluded from consideration for several genera-tions, which means that candidate solutions are not be tested for certain important behaviors. Therefore, Boldi et al . [3] proposed IDS to create more diverse subsets of training cases. In detail, every 

ùëô generations, IDS evaluates the performance of a sample of the population using all training cases to identify the cases which are solved by different candidates. If cases are solved by different candi-dates, these cases more likely test distinct behaviors and therefore contribute to a more diverse subset of training cases. The literature confirms these design decisions, as recent work finds that IDS out-performs RDS in the domain of program synthesis [ 3, 4] and has been studied in combination with different selection methods [ 1 , 2]. Moreover, IDS also improves the performance of tournament selection in the domain of symbolic regression [ 18 ]. In combination with IDS, tournament selection performs and behaves similar to lexicase selection but with the benefit of being significantly faster than the lexicase variant [15]. However, recent work by Geiger et al . [15 , 18] observed that for some regression problems IDS performs poorly and even worse than RDS. The reason for this behavior is still unclear. 

> (a) Even distribution of training cases.
> (b) Uneven distribution of training cases.
> (c) Even distribution of training cases and 5% outliers.
> (d) Uneven distribution of training cases and 5% outliers.

Figure 1: Color-coded frequencies of including a training case in the subsets when using IDS. Results are for different variants of the nguyen-6 problem. The top row contains two outlier-free variants of the problem, whereas the bottom row includes 5% outliers. 

## 3 The Limits of IDS in the Presence of Outliers 

In this section, we illustrate the behavior of IDS for several variants of the 2D nguyen-6 problem [ 40 ], as this allows us to demonstrate why and how the existence of outliers in a dataset negatively affects the performance of IDS. This section is for illustrative purposes only, for more details and results, we refer to Sects. 5 and 6. IDS aims at creating a subset of relevant and important training cases. It tries to avoid considering those cases which bring in only little and redundant information about the problem at hand [ 3 ]. For program synthesis problems, Boldi et al . [3] found that IDS includes highly informative edge cases more frequently. This is advantageous for program synthesis problems which are uncom-promising in nature and noise free, meaning a solution needs to solve all training and test cases perfectly. However, in symbolic regression problems the data might be noisy and inhibit outliers. Fitting a solution to this noise and outliers exactly is not desir-able [ 31 ] and including these cases in a subset might have negative effects. Therefore, we exemplarily analyze the behavior of IDS for the 2D nguyen-6 regression problem [ 40 ] with different distributions of training cases and number of outliers (for details of the data generation process, see Sect. 5.1). We apply a GP with IDS for 100 generations to four different problem variants and measure how often the cases are included by IDS in the subset of relevant cases. Figure 1 plots the four different distributions of training cases. Figure 1a is the original problem; Fig. 1b reduces the number of samples for ùë• < 0 mimicking the existence of edge cases. Figs. 1c ROIDS: Robust Outlier-Aware Informed Down-Sampling 

and 1d are analogous with 5% additional outliers. Each dot is col-ored and the color indicates how often this case is included in a subset with IDS over 100 generations using a down-sampling rate of ùëõ = 0.1. We expect that IDS focuses on edge cases by including them more frequently in its subsets. This behavior is in contrast to RDS, where each case would be included with the same probability (uniformly distributed over all training cases; for the example at hand, RDS would sample each case with probability 0.1). In Fig. 1a, where all training cases are evenly distributed, IDS slightly prefers cases that are more to the left and right as well as the inflection point in the middle compared to the cases in between. The edge cases are included much more frequently (around 5 times more often than with RDS). In Fig. 1b, for an uneven distribution of training cases, IDS focuses on the cases with ùë• < 0 in addition to the edge cases. It includes training cases from the under-represented problem region much more frequently compared to the cases lying in dense regions. This clearly highlights the advantage of IDS, as it ensures that cases covering different behaviors are included. In contrast, for RDS, cases from the under-represented problem re-gion would have an equal chance of being sampled than the cases from the dense region ( ‚âà 0.1). Thus, the important edge cases are often omitted for several generations. Consequently, RDS is not sufficiently evaluating candidate solutions on edge cases, which potentially harms the search process. Unlike synthetic problems, real-world regression problems often contain outliers, meaning training cases that significantly deviate from the regular pattern [ 9]. In contrast to edge cases, however, they are not relevant and we want optimization methods to ignore them as their consideration impedes the search process [ 31 ]. To demonstrate the behavior of IDS in the presence of outliers, we added outliers to both variants of the problem (see Figs. 1c and 1d). In Fig. 1c, for an even distribution of cases but with additional outliers, we see that IDS strongly focuses on the outliers and in-cludes them very often in the subsets. For example, the outlier at 

ùë¶ = ‚àí6 is included almost every time. In contrast, RDS would be much less affected by outliers, as the chance of being selected is uniform across all cases. Finally, Fig. 1d illustrates for an uneven distribution with additional outliers, that the benefits of IDS are mitigated because IDS now focuses primarily on the outliers and not on the edge cases. To sum up, the behavior of IDS to over-represent existing outliers in the subsets mitigates the positive effect of IDS to focus on the edge cases. As soon as outliers are present, IDS focuses strongly on outliers not covering the regular data pattern any more. 

## 4 ROIDS: Robust Outlier-Aware Informed Down-Sampling 

To mitigate the limitations of IDS in the presence of outliers we propose ROIDS, an extension of IDS, which keeps the benefits of IDS while mitigating its brittleness to outliers. ROIDS builds on the assumption that outliers, in contrast to edge cases, are fundamen-tally different from the underlying pattern [ 9] and hard to learn without overfitting these cases [ 31 ]. As populations of candidate solutions effectively act as ensembles, such populations are inher-ently less prone to overfitting these outliers [ 27 , 30 , 38 ]. Instead, a 

Algorithm 1 Robust Outlier-Aware Informed Down-sampling 

Require: Parent population ùëÉ , training cases ùëá , current generation 

ùê∫ , down-sample rate ùëõ , parent sampling rate ùúå , scheduled case distance computation parameter ùëô , outlier sensitivity rate ùõæ 

Ensure: Down-sampled training set ùëÅ  

> 1:

if ùê∫ %ùëô == 0 then  

> 2:

ÀÜùëÉ ‚Üê sample ùúå ¬∑ | ùëÉ | candidates from ùëÉ  

> 3:

evaluate each candidate ÀÜùëù ‚àà ÀÜùëÉ on each case ùë° ‚àà ùëá to get all case error vectors ùë† ‚àà ùëÜ  

> 4:

calculate mean error value of each ùë† ‚àà ùëÜ  

> 5:

ÀÜùëá ‚Üê ùëá after removing ùõæ ‚àó | ùëá | cases with highest mean error value  

> 6:

ùê∑ ‚Üê compute distances using case error vectors of ÀÜùëá  

> 7:

end if  

> 8:

ùëÅ ‚Üê sample ùëõ ‚àó | ùëá | cases from ÀÜùëá with farthest first traversal using ùê∑  

> 9:

return ùëÅ 

population should have on average a high mean error on these out-lier cases. ROIDS exploits this property by excluding the cases with the highest mean error across candidates from a possible subset at any given generation. As a result, ROIDS is able to create diverse subsets of training cases without oversampling outliers. Algorithm 1 describes the ROIDS procedure in detail. Similar to IDS [ 3 ], a distance matrix between cases is calculated every ùëô 

generations (line 1 ‚àí 7). This is done by first sampling a fraction ùúå 

of the parent population ùëÉ (line 2). This sampled parent population 

ÀÜùëÉ is then evaluated on the whole training set ùëá to construct a case error vector ùë† ‚àà ùëÜ for each case ùë° ‚àà ùëá (line 3), therefore, ùë† describes the non-aggregated performance of the sampled parent population on a specific case. For example, if ùëá contains 100 training cases and ÀÜùëÉ a sample of 5 candidate solutions, ùëÜ contains 100 case error vectors ùë† of size 5.Contrary to the original IDS algorithm, ROIDS adds an additional step at this point: Prior to the construction of the down-sample, ROIDS identifies possible outliers by collapsing each case error vector to its mean error value and masking those cases from inclu-sion in the down-sample with the highest mean error (line 4 ‚àí 5). Specifically, ùõæ ‚àó | ùëá | cases are excluded from ùëá resulting in ÀÜùëá , where 

ùõæ is the adjustable outlier sensitivity rate in [0, 1). This additional step of ROIDS comes at almost no additional cost in comparison to the original IDS algorithm. Afterwards, the ROIDS algorithm continues as the original IDS algorithm would: A distance matrix ùê∑ is constructed using the pairwise distances between all cases ÀÜùë° ‚àà ÀÜùëá with regards to their case error vectors (line 6). This distance matrix ùê∑ is then used each generation to construct a subset ùëÅ of size ùëõ ‚àó | ùëá | using the farthest first traversal algorithm [ 26 ] to include the most informative cases from ÀÜùëá (line 8) . Finally, the subset ùëÅ is used to evaluate the quality of the can-didate solutions in a population. By including diverse and highly informative cases, the candidates are sufficiently evaluated on dif-ferent problem regions. Further, by excluding potential outliers from the subset, ROIDS guides the search toward a meaningful direction instead of overfitting those outliers. Geiger et al. 

Table 1: Benchmark problems with their dimension, number of training and test cases and the problem type. 

Problem Dimension #Train #Test Type 

nguyen-6 [40] 1 100 100 synthetic 

friedman1 [5, 14] 5 100 100 synthetic 

friedman2 [5, 14] 4 100 100 synthetic 

friedman3 [5, 14] 4 100 100 synthetic 

airfoil [8] 5 1277 226 real-world 

concrete [42] 8 875 155 real-world 

enh [39] 8 652 116 real-world 

housing [21] 13 430 76 real-world 

redwine [11] 11 1359 240 real-world 

yacht [19] 6 261 47 real-world 

## 5 Experimental Setup 

This section describes the used benchmark problems as well as the GP setup and parameter settings. 

## 5.1 Problems 

We first run experiments on synthetic problems that can be ma-nipulated to contain outliers. This controlled setup allows us to test our assumption that IDS performs poorly in the presence of outliers, and to show that ROIDS is more robust in such situations. First, we study the behavior of the down-sampling methods for four variants of the 2D nguyen-6 problem [ 40 ], which is defined as 

ùë¶ = ùë†ùëñùëõ (ùë• ) + ùë†ùëñùëõ (ùë• + ùë• 2). (1) For the first variant with an even distribution, 100 data points were placed evenly across [‚àí 1, 1]. For the second variant with an uneven distribution, 10% of the data points were placed in [‚àí 1, 0]

and 90% in [0, 1]. Additionally, we generated two datasets (with even and uneven distribution) containing outliers by corrupting 5% of the data points with noise from a Gaussian distribution with a mean of zero and a standard deviation four times that of the original 

ùë¶ -values. We use the same test set for all variants of the nguyen-6 

problem, which contains 100 evenly placed data points between 

[‚àí 1, 1] and is outlier free. In addition to that, we generated the friedman1 , friedman2 , and 

friedman3 datasets as described in [ 5 , 14 ] with 100 training points and 100 test points. For each of the datasets we also generated a variant containing outliers by manipulating 5% of the data points as described before. Again, the test set is outlier free. Additionally, we test the performance of ROIDS on a selection of 6 real-world problems that are commonly used in literature [ 16 ,29 , 41 ]. For these problems, we randomly sampled 15% of the data points as a test set and the rest for training. The dimension of each problem, as well as the number of training and test cases is listed in Table 1. The number of training cases varies between 261 for the yacht 

problem to 1, 359 for the redwine problem. The number of features goes up to 13 for the housing problem. 

Table 2: Parameter settings of our GP approach. Parameter Value 

Population size 500 

Generations 500 (100 for nguyen-6 )Primitive set {x, ERC , +, ‚àí, ‚àó, AQ [34 ], sin , cos , neg }

ERC values {‚àí 1, 0, 1}

Selection method Tournament ( ùëõ = 7)Initialization method Ramped half-and-half Maximum tree depth 17 

Crossover probability 80% 

Mutation probability 5% 

Down-sampling rate ùëõ 0.1

Runs 100 

## 5.2 Parameter Setting 

For our experiments, we used the DEAP framework [ 13 ] as foun-dation of our implementation. The GP approach uses standard parameter settings described in Table 2. We initialized the population using ramped half-and-half and set the population size to 500 . The crossover and mutation probabilities are 80% and 5% , respectively. We run our experiments for 500 gener-ations for the Friedman and real-world problems. For the nguyen-6 

problem, we set the generation limit to 100 as it is a much sim-pler problem. We use tournament selection, because previous work found that for symbolic regression problems tournament selection performs similarly to lexicase selection when combined with down-sampling methods, but runs faster [ 15 ]. We set the tournament size to 7. The quality of a candidate solution is measured as its mean squared error (MSE) on the subset of training cases. For all down-sampling methods, we define a down-sampling rate of ùëõ = 0.1 [ 16 ], meaning that only 10% of the training cases are used in each generation to evaluate the quality of the candidate solutions. For IDS and ROIDS, we set the parent sampling rate to 

ùúå = 0.01 and the scheduled case distance computation parameter to ùëô = 10 as suggested by Boldi et al . [3] . For ROIDS, the outlier sensitivity rate is set to ùõæ = 0.05 (an ablation study with results for different ùõæ are shown in Figs. 9 and 13 of the appendix). Additionally, to further motivate the design of ROIDS, we in-cluded a baseline in our real-world experiments that filters outliers from the training set before starting an evolutionary run with IDS. To remove the outliers, we used the Local Outlier Factor (LOF) [ 6], which is a widely used outlier detection method [ 10 ]. We imple-mented LOF with its default parameters in scikit-learn [35]. For each run, we randomly take 15% of the training cases as a validation set. The validation set is used to choose the final solution from all candidates during a run (meaning the candidate with the lowest MSE on the validation set). This candidate is then evaluated on the unseen test set. All experiments are repeated 100 times. For the analysis of the parameter ùõæ (see supplementary material), we performed 30 repeti-tions. ROIDS: Robust Outlier-Aware Informed Down-Sampling 

## 6 Results 

Section 6.1 presents an analysis of the behavior and performance of ROIDS on synthetic regression problems. In Sect. 6.2, we study whether our observations transfer to real-world regression datasets. 

## 6.1 Performance of ROIDS on Synthetic Regression Problems 

Section 3 exemplarily illustrated for the nguyen-6 problem that IDS frequently includes outliers in its subsets and, consequently, is not sufficiently covering the true underlying pattern anymore. Based on this observation, Sect. 4 proposes ROIDS, an outlier-aware variant of IDS that excludes potential outliers from its subsets. In analogy to Sect. 3 which illustrates the behavior of IDS, Fig. 2 visualizes the frequency of case inclusion using ROIDS for the nguyen-6 problem with different distributions of training cases and in the presence of outliers. For an even distribution of training cases (Figs. 2a and 1a), ROIDS behaves similar to IDS including the cases on the edges and the inflection point more frequently compared to RDS (for RDS each case has a probability of approx. 10% to be included for a down-sampling rate of ùëõ = 0.1). For an uneven distribution of training cases (Figs. 2b and 1b), the situation is similar: ROIDS includes cases from the under-represented problem region ( ùë• < 0) as well as edge cases more frequently than RDS. This indicates that ROIDS keeps the advantages of IDS, as it includes highly informative cases more frequently and the subsets consist of cases that cover diverse behaviors. The advantage of ROIDS over IDS for problems with outliers be-comes evident in Figs. 2c and 2d: in the presence of outliers, ROIDS includes the outliers rarely in its subsets, while IDS includes them almost every generation (compare Figs. 1c and 1d). By excluding the outliers, ROIDS is able to focus on the true underlying data pattern and still creates meaningful subsets by including edge cases and cases from under-represented problem regions more frequently. In contrast, IDS fails to sufficiently represent the edge cases in its subsets in the presence of outliers. Next, we analyze how the creation of subsets affects the perfor-mance for the different variants of the nguyen-6 problem. For the best solutions found for the validation cases, Figure 3 shows the distribution of the MSE on the test cases over 100 runs for RDS, IDS, and ROIDS. As expected, for an even distribution without outliers (top left), all down-sampling strategies perform equally well. Intu-itively, if the distribution is even, RDS is also able to sufficiently cover the problem in its subsets as all training cases are equally likely to be included. For an uneven distribution without outliers (top right), both IDS and ROIDS achieve a lower MSE and have less variance in their results compared to RDS. The subsets created by IDS and ROIDS still cover all problem regions equally well even if the original dataset has an uneven distribution. With RDS, the under-represented problem part is also under-represented in its subsets, which harms performance. The situation is different when adding outliers to the nguyen-6 

problem (bottom row of Fig. 3). Then, IDS performs worse than RDS, because it heavily focuses on the outlier cases. In contrast, ROIDS still performs well and similar to RDS. This highlights that ROIDS successfully mitigates the drawbacks of IDS in the presence 

> (a) Even distribution of training cases.
> (b) Uneven distribution of training cases.
> (c) Even distribution of training cases and 5% outliers.
> (d) Uneven distribution of training cases and 5% outliers.

Figure 2: Color-coded frequencies of including a training case in the subsets when using ROIDS. Results are for different variants of the nguyen-6 problem. The top row contains two outlier-free variants of the problem, whereas the bottom row includes 5% outliers. 

Figure 3: Performance of different down-sampling methods for different variants of the nguyen-6 problem. Note that the y-axes of the plots are scaled differently. For better readabil-ity, outliers are not plotted. 

of outliers but is still able to exploit its benefits if the distribution of training cases is uneven. This makes ROIDS a good choice across a wide range of problems. Geiger et al. 

Figure 4: Performance of different down-sampling methods for Friedman problems with and without outliers. Note that the y-axes of the plots are scaled differently. For better read-ability, outliers are not plotted. 

Next, we study the performance of ROIDS in comparison to RDS and IDS for three additional more complex synthetic problems, namely the friedman1 , friedman2 , and friedman3 problems with and without outliers. Fig. 4 shows the MSE on the test cases of the best found solutions for 100 runs. Although IDS performs better than RDS on two out of three Friedman problems without outliers, it performs always worse than RDS in the presence of outliers. In contrast, ROIDS performs reasonably well across all variants of the Friedman problems. Especially in the presence of outliers, ROIDS clearly outperforms IDS on all problems. Table 3 summarizes the median MSE on the test cases for RDS, IDS, and ROIDS for all synthetic problems. Further, we tested for statistically significant differences between ROIDS and IDS using the Mann-Whitney U test [ 32 ], with the significance threshold set to ùõº = 0.05 . ROIDS achieves the lowest MSE on 4 out of 10 problems and the second best on the remaining problems. In contrast to RDS and IDS, ROIDS is never the worst performing method. This is also reflected in the mean ranking of each method. IDS achieves the worst mean ranking with 2.2, RDS the second best with 1.8, and ROIDS the best with 1.6. This confirms not only that ROIDS performs well across different problems but, moreover, that ROIDS always performs significantly better than IDS if outliers are present and never significantly worse otherwise. 

Table 3: Performance of different down-sampling methods for synthetic problems with and without outliers. The me-dian MSE on the test cases over 100 runs is shown. Best re-sults are highlighted in bold. Significant differences between ROIDS and IDS are indicated by an asterisk. 

Problem RDS IDS ROIDS 

nguyen-6 even 0.000 0.000 0.000 

nguyen-6 even + outliers 0.013 0.097 ‚àó0.014 

nguyen-6 uneven 0.001 0.000 0.000 

nguyen-6 uneven + outliers 0.059 0.160 ‚àó0.058 

friedman1 4.625 5.824 5.558 

friedman1 + outliers 8.273 14.955 *7.318 

friedman2 98.886 51.320 78.664 

friedman2 + outliers 209.587 994.823 *261.759 

friedman3 0.051 0.030 0.047 

friedman3 + outliers 0.060 0.106 *0.068 Mean Ranking 1.8 2.2 1.6 

For the interested reader, we visualized the structure of the Fried-man problems with outliers using UMAPs for dimensionality re-duction in Appendix A (see Figs. 10- 12) to show the frequency of case inclusion for the different down-sampling methods. We find that IDS includes the outlier cases very frequently while ROIDS mostly excludes them from the subset creation. Finally, we study how the performance of ROIDS depends on the setting of the outlier sensitivity rate ùõæ . In the supplementary material (Appendix A), Fig. 9 shows how the performance changes for ùõæ ‚àà { 0.025 , 0.05 , 0.1}. We find that for Friedman problems with-out outliers a smaller ùõæ performs slightly better. In the presence of outliers, a ùõæ smaller than the amount of added outliers (here 5% )performs slightly worse. To sum up, in the presence of outliers, ROIDS is able to focus on the true underlying pattern and consequently, guides the search process more successfully than IDS. We found that ROIDS always performs significantly better than IDS if outliers are present. More-over, the mean ranking of ROIDS across all synthetic problems is better compared to RDS and IDS, meaning ROIDS is robust and performs well across a wider range of problems. 

## 6.2 Performance of ROIDS on Real-World Regression Problems 

This subsection analyzes the performance of ROIDS on real-world regression problems to test if the performance benefits observed in Sect. 6.1 also transfer to real-world settings. To motivate our design choice of ROIDS, we include an additional baseline variant called LOF+IDS, which excludes outliers using LOF before performing an evolutionary run with IDS. Figure 5 compares the performance of RDS, IDS, LOF+IDS, and ROIDS for six real-world regression problems over 100 runs. Fur-ther, Table 4 summarizes the median MSE on the test cases of the best found solutions for all down-sampling methods. We find that ROIDS mostly achieves a lower median MSE compared to LOF+IDS. This motivates our design choice for dynamically calculating the ROIDS: Robust Outlier-Aware Informed Down-Sampling 

Table 4: Performance of different down-sampling methods for real-world problems. The median MSE on the test cases over 100 runs is shown. Best results are highlighted in bold. Significant differences between ROIDS and IDS are indicated by an asterisk. 

Problem RDS IDS LOF+IDS ROIDS 

airfoil 36.599 34.462 32.412 31.132 

concrete 63.467 60.646 64.826 60.531 

enh 1.709 0.921 1.804 0.926 

housing 44.202 50.183 46.105 47.080 

redwine 0.424 0.426 0.421 ‚àó0.420 

yacht 1.047 2.010 1.904 ‚àó1.550 Mean Ranking 2.5 3 2.8 1.7 

outliers every ùëô generations based on the average error across the population on each case. In contrast to ROIDS, LOF removes outliers by comparing the local density of each training case to the local density of its ùëò -nearest neighbors [ 6 ]. However, the interpretation of local density is difficult as it depends on the given data distribu-tion [ 28 ], which might also lead to the exclusion of important edge cases. Therefore, we find that ROIDS is better in distinguishing between edge cases and outliers in comparison to LOF+IDS. For the housing , redwine , and yacht problem, IDS achieves a worse median MSE than RDS. Here, the median MSE of ROIDS is always better than that of IDS and ROIDS even significantly outperforms IDS on the redwine and yacht problem. Only on the enh problem, IDS performs best, and notably better than RDS. RDS performs best on two problems, and ROIDS is the best performing method on three problems. Again, ROIDS is never the worst performing method. This is also reflected in its mean ranking of 1.7, which is much better compared to RDS with 2.5, IDS with 3, and LOF+IDS with 2.8. As for the synthetic problems, we analyze the influence of the outlier sensitivity rate ùõæ on ROIDS performance in the supplemen-tary material (App. B). Figure 13 shows the performance of ROIDS for ùõæ ‚àà { 0.025 , 0.05 , 0.1} on all real-world problems. We can see that for real-world problems the performance of ROIDS is robust over a reasonable range of ùõæ and ùõæ = 0.05 performs well across all considered problems. Additionally, we visualize the frequency of case inclusion with IDS and ROIDS for the real-world problems using UMAP as dimen-sionality reduction technique. For the airfoil problem (Fig. 6), we see that all training cases lie in one dense region and that there are no visual outliers present. This probably explains why the per-formance differences between the down-sampling methods are minimal for airfoil . Although IDS focuses on edge cases (see Fig. 6a), RDS probably also covers all problem regions well due to the even distribution of training cases. ROIDS includes the edge cases less frequently compared to IDS (see Fig. 6b). For enh (Fig. 7), we see that there are different clusters. Here, IDS and ROIDS frequently include edge cases from the clusters in their subsets, which probably leads to a better coverage of the problem in their subsets compared to the subsets created with RDS. This is 

Figure 5: Performance of different down-sampling methods for real-world problems. For better readability, outliers are not plotted. 

in line with the finding, that IDS and ROIDS outperform RDS on the enh problem. For yacht (Fig. 8a), we observe that IDS heavily focuses on a few outliers. ROIDS mostly excludes those cases from its subsets (see Fig. 8b). This probably explains why IDS performs notably worse than RDS for the yacht problem and why ROIDS outperforms IDS significantly here. The UMAP visualizations of concrete , housing ,and redwine are in the supplementary material in Appendix B, Fig. 14- 16, were we observe similar behavior as discussed above. To sum up, ROIDS significantly outperforms IDS on two prob-lems, where IDS is worse than RDS. Additionally, ROIDS outper-forms IDS even if an outlier detection algorithm is applied to the training set beforehand. Overall, ROIDS achieves the best average ranking on the real-world problems, which highlights the robust performance compared to IDS, making it a good choice across a wide range of problems. 

## 7 Conclusions and Future Work 

Recent work found that IDS as a down-sampling method for selec-tion is beneficial for problems in the program synthesis domain [ 3 ]as well as for symbolic regression [ 18 ]. Our analysis revealed that this is due to the ability of IDS to build a subset of training cases that better represents underlying patterns even if the data is un-evenly distributed in the training set. However, our analysis also Geiger et al. 

(a) IDS (b) ROIDS 

Figure 6: UMAP Visualization of the airfoil problem. The color scale incidates how often each case is selected using IDS and ROIDS, respectively. The training cases are evenly distributed, which explains that all down-sampling methods perform equally well on this problem. 

(a) IDS (b) ROIDS 

Figure 7: UMAP Visualization of the enh problem. The color scale indicates how often each case is selected under IDS and ROIDS, respectively. The training cases are unevenly distributed across different clusters, which explains why IDS and ROIDS outperform RDS on this problem. 

revealed that the performance of IDS degrades in the presence of outliers by over-representing these unwanted cases in its subsets, consequently impeding search performance. To overcome this limitation, we introduced ROIDS in this paper. ROIDS retains the advantages of IDS and continues to work well on unevenly distributed data points, but, most importantly, performs significantly better in the presence of outliers thanks to its novel outlier-aware down-sampling strategy. In our experiments on the synthetic problems, we find that ROIDS shows exactly the desired behavior: the subsets still fo-cus on edge and corner cases, but outliers are less important and no longer interfere with the evolutionary search. This is also re-flected in the results. Whenever we added outliers to the synthetic problems, ROIDS outperformed the results achieved with IDS. Moreover, the positive aspects of our new approach are also visible when applied to more complex real-world problems, as ROIDS outperforms IDS on over 80% of the benchmark problems. Given that ROIDS consistently achieves the top average rank across the synthetic as well as the real-world benchmark problems, we recommend researches and practitioners to adopt ROIDS as 

(a) IDS (b) ROIDS 

Figure 8: UMAP Visualization of the yacht problem. The color scale indicates how often each case is selected under IDS and ROIDS, respectively. Potential outliers are frequently included by IDS, which explains its low performance. ROIDS includes those cases less frequently leading to a significantly better performance compared to IDS. 

their preferred down-sampling method for selection in GP in the symbolic regression domain. In future work, we will analyze ROIDS on further benchmark problems and develop an adaptive version of ROIDS that automati-cally adapts the outlier sensitivity rate. 

## References 

[1] Ryan Boldi, Ashley Bao, Martin Briesch, Thomas Helmuth, Dominik Sobania, Lee Spector, and Alexander Lalejini. 2023. The Problem Solving Benefits of Down-sampling Vary by Selection Scheme. In Proceedings of the Companion Conference on Genetic and Evolutionary Computation (Lisbon, Portugal) (GECCO ‚Äô23 Companion) . Association for Computing Machinery, New York, NY, USA, 527‚Äì530. [2] Ryan Boldi, Ashley Bao, Martin Briesch, Thomas Helmuth, Dominik Sobania, Lee Spector, and Alexander Lalejini. 2024. Untangling the Effects of Down-Sampling and Selection in Genetic Programming. In ALIFE 2024: Proceedings of the 2024 Artificial Life Conference . MIT Press. [3] Ryan Boldi, Martin Briesch, Dominik Sobania, Alexander Lalejini, Thomas Hel-muth, Franz Rothlauf, Charles Ofria, and Lee Spector. 2024. Informed Down-Sampled Lexicase Selection: Identifying productive training cases for efficient problem solving. Evolutionary computation (2024), 1‚Äì31. [4] Ryan Boldi, Alexander Lalejini, Thomas Helmuth, and Lee Spector. 2023. AStatic Analysis of Informed Down-Samples. In Proceedings of the Companion Conference on Genetic and Evolutionary Computation (Lisbon, Portugal) (GECCO ‚Äô23 Companion) . Association for Computing Machinery, New York, NY, USA, 531‚Äì534. [5] Leo Breiman. 1996. Bagging predictors. Machine learning 24 (1996), 123‚Äì140. [6] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J√∂rg Sander. 2000. LOF: identifying density-based local outliers. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data . 93‚Äì104. [7] Martin Briesch, Dominik Sobania, and Franz Rothlauf. 2023. On the Trade-Off between Population Size and Number of Generations in GP for Program Synthesis. In Proceedings of the Companion Conference on Genetic and Evolutionary Computation . 535‚Äì538. [8] Thomas F. Brooks, D. Stuart Pope, and Michael A. Marcolini. 1989. Airfoil self-noise and prediction . National Aeronautics and Space Administration, Office of Management, Scientific and Technical Information Division. [9] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection: A survey. ACM computing surveys (CSUR) 41, 3 (2009), 1‚Äì58. [10] Zhangyu Cheng, Chengming Zou, and Jianwei Dong. 2019. Outlier detection using isolation forest and local outlier factor. In Proceedings of the conference on research in adaptive and convergent systems . 161‚Äì168. [11] Paulo Cortez, Ant√≥nio Cerdeira, Fernando Almeida, Telmo Matos, and Jos√© Reis. 2009. Modeling wine preferences by data mining from physicochemical properties. Decision support systems 47, 4 (2009), 547‚Äì553. [12] Austin J. Ferguson, Jose Guadalupe Hernandez, Daniel Junghans, Alexander Lalejini, Emily Dolson, and Charles Ofria. 2020. Characterizing the Effects of Random Subsampling on Lexicase Selection. In Genetic Programming Theory ROIDS: Robust Outlier-Aware Informed Down-Sampling 

and Practice XVII . Springer International Publishing, 1‚Äì23. [13] F√©lix-Antoine Fortin, Fran√ßois-Michel de Rainville, Marc-Andr√© Gardner, Marc Parizeau, and Christian Gagn√©. 2012. DEAP: Evolutionary algorithms made easy. 

The Journal of Machine Learning Research 13, 1 (2012), 2171‚Äì2175. [14] Jerome H Friedman. 1991. Multivariate adaptive regression splines. The annals of statistics 19, 1 (1991), 1‚Äì67. [15] Alina Geiger, Martin Briesch, Dominik Sobania, and Franz Rothlauf. 2025. Was Tournament Selection All We Ever Needed? A Critical Reflection on Lexicase Selection. In Genetic Programming , Bing Xue, Luca Manzoni, and Illya Bakurov (Eds.). Springer Nature Switzerland, Cham, 207‚Äì223. [16] Alina Geiger, Dominik Sobania, and Franz Rothlauf. 2023. Down-Sampled Epsilon-Lexicase Selection for Real-World Symbolic Regression Problems. In 

Proceedings of the Genetic and Evolutionary Computation Conference (Lisbon, Portugal) (GECCO ‚Äô23) . ACM, 1109‚Äì1117. [17] Alina Geiger, Dominik Sobania, and Franz Rothlauf. 2024. A Comprehensive Comparison of Lexicase-Based Selection Methods for Symbolic Regression Prob-lems. In European Conference on Genetic Programming (Part of EvoStar) . Springer, 192‚Äì208. [18] Alina Geiger, Dominik Sobania, and Franz Rothlauf. 2025. A performance analysis of lexicase-based and traditional selection methods in gp for symbolic regression. 

ACM Transactions on Evolutionary Learning (2025). [19] J. Gerritsma, R. Onnink, and A. Versluis. 1981. Geometry, resistance and stability of the delft systematic yacht hull series. International shipbuilding progress 28, 328 (1981), 276‚Äì297. [20] Ivo Gon√ßalves, Sara Silva, Joana B. Melo, and Jo√£o M. B. Carreiras. 2012. Random Sampling Technique for Overfitting Control in Genetic Programming. In Genetic Programming . Springer Berlin Heidelberg, 218‚Äì229. [21] David Harrison and Daniel L. Rubinfeld. 1978. Hedonic Housing Prices and the Demand for Clean Air. Journal of environmental economics and management 5, 1 (1978), 81‚Äì102. [22] Thomas Helmuth and Lee Spector. 2020. Explaining and exploiting the advan-tages of down-sampled lexicase selection. In ALIFE 2020: The 2020 Conference on Artificial Life . MIT Press, 341‚Äì349. [23] Thomas Helmuth and Lee Spector. 2021. Problem-Solving Benefits of Down-Sampled Lexicase Selection. Artificial life 27, 3-4 (2021), 183‚Äì203. [24] Thomas Helmuth, Lee Spector, and James Matheson. 2014. Solving Uncompro-mising Problems with Lexicase Selection. IEEE Transactions on Evolutionary Computation 19, 5 (2014), 630‚Äì643. [25] Jose Guadalupe Hernandez, Alexander Lalejini, Emily Dolson, and Charles Ofria. 2019. Random subsampling improves performance in lexicase selection. In 

Proceedings of the Genetic and Evolutionary Computation Conference Companion (GECCO ‚Äô19) . ACM, 2028‚Äì2031. [26] Dorit S Hochbaum and David B Shmoys. 1985. A best possible heuristic for the k-center problem. Mathematics of operations research 10, 2 (1985), 180‚Äì184. [27] Mark E Kotanchek, Ekaterina Y Vladislavleva, and Guido F Smits. 2009. Symbolic regression via genetic programming as a discovery engine: Insights on outliers and prototypes. In Genetic Programming Theory and Practice VII . Springer, 55‚Äì72. [28] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. 2011. Inter-preting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining . SIAM, 13‚Äì24. [29] William La Cava, Thomas Helmuth, Lee Spector, and Jason H. Moore. 2019. A probabilistic and multi-objective analysis of lexicase selection and epsilon-lexicase selection. Evolutionary Computation 27, 3 (2019), 377‚Äì402. [30] Uriel L√≥pez, Leonardo Trujillo, and Pierrick Legrand. 2018. Filtering outliers in one step with genetic programming. In International Conference on Parallel Problem Solving from Nature . Springer, 209‚Äì222. [31] Uriel L√≥pez, Leonardo Trujillo, Yuliana Martinez, Pierrick Legrand, Enrique Naredo, and Sara Silva. 2017. RANSAC-GP: Dealing with outliers in symbolic regression with genetic programming. In European Conference on Genetic Pro-gramming . Springer, 114‚Äì130. [32] Henry B Mann and Donald R Whitney. 1947. On a test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics (1947), 50‚Äì60. [33] Leland McInnes, John Healy, and James Melville. 2018. Umap: Uniform man-ifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426 (2018). [34] Ji Ni, Russ H. Drieberg, and Peter I. Rockett. 2013. The Use of an Analytic Quotient Operator in Genetic Programming. IEEE Transactions on Evolutionary Computation 17, 1 (2013), 146‚Äì152. [35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825‚Äì2830. [36] Dirk Schweim, Dominik Sobania, and Franz Rothlauf. 2022. Effects of the Training Set Size: A Comparison of Standard and Down-Sampled Lexicase Selection in Program Synthesis. In 2022 IEEE Congress on Evolutionary Computation (CEC) .IEEE, 1‚Äì8. [37] Lee Spector. 2012. Assessment of Problem Modality by Differential Performance of Lexicase Selection in Genetic Programming: A Preliminary Report. In Pro-ceedings of the 14th Annual Conference Companion on Genetic and Evolutionary Computation (GECCO ‚Äô12) . ACM, 401‚Äì408. [38] Leonardo Trujillo, Luis Mu√±oz, Uriel L√≥pez, and Daniel E Hern√°ndez. 2019. Un-tapped potential of genetic programming: Transfer learning and outlier removal. In Genetic Programming Theory and Practice XVI . Springer, 193‚Äì207. [39] Athanasios Tsanas and Angeliki Xifara. 2012. Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools. Energy and Buildings 49 (2012), 560‚Äì567. [40] Nguyen Quang Uy, Nguyen Xuan Hoai, Michael O‚ÄôNeill, Robert I McKay, and Edgar Galv√°n-L√≥pez. 2011. Semantically-based crossover in genetic program-ming: application to real-valued symbolic regression. Genetic Programming and Evolvable Machines 12, 2 (2011), 91‚Äì119. [41] Marco Virgolin, Tanja Alderliesten, Cees Witteveen, and Peter A. N. Bosman. 2021. Improving Model-Based Genetic Programming for Symbolic Regression of Small Expressions. Evolutionary Computation 29, 2 (2021), 211‚Äì237. [42] I-Cheng Yeh. 1998. Modeling of strength of high-performance concrete using artificial neural networks. Cement and Concrete research 28, 12 (1998), 1797‚Äì1808. Geiger et al. 

## A Analysis Friedman Problems 

Fig. 9 plots the performance of ROIDS for different outlier sensi-tivity rates ùõæ ‚àà { 0.025 , 0.05 , 0.1}. Results are for all variants of the Friedman problems. For the Friedman problems without outliers, lower values of ùõæ lead to higher performance. This is as expected, as these problems only contain cases that represent the underlying true pattern and should not be removed. However, in real-world settings this is rarely the case. For Friedman problems containing outliers, a ùõæ greater than the percentage of added outliers (here 5% ) leads to better results. This shows that it is important to set ùõæ large enough to remove all existing outliers. A value of ùõæ = 0.05 performs reasonably well across all problem variants. Figures 10-12 show UMAP visualizations of the friedman1 ,

friedman2 , and friedman3 problems with outliers. The color en-codes the frequency of case inclusion by IDS and ROIDS. We observe that outliers are frequently included in the subsets created by IDS (Figs. 10a, 11a, and 12a). In contrast, ROIDS often excludes those cases from its subsets and focuses on including cases that represent the true underlying pattern of the problem (Figs. 10b, 11b, and 12b). 

Figure 9: Analysis of the parameter ùõæ of ROIDS for all variants of the Friedman problems. The performance over 30 runs is shown. For better readability, outliers are not plotted.  

> (a) IDS (b) ROIDS

Figure 10: UMAP Visualization of the friedman1 problem with outliers. The color scale shows how often each case is selected under IDS and ROIDS, respectively.  

> (a) IDS (b) ROIDS

Figure 11: UMAP Visualization of the friedman2 problem with outliers. The color scale shows how often each case is selected under IDS and ROIDS, respectively.  

> (a) IDS (b) ROIDS

Figure 12: UMAP Visualization of the friedman3 problem with outliers. The color scale shows how often each case is selected under IDS and ROIDS, respectively. 

## B Analysis Real-world Problems 

Figure 13 plots the performance of ROIDS for ùõæ ‚àà { 0.025 , 0.05 , 0.1}

for all real-world problems. For the concrete , enh , redwine , and 

yacht problem, ùõæ = 0.05 achieves the lowest MSE. Overall, ùõæ is robust over a reasonable range for real-world problems. Further, we visualize the real-world problems using UMAPs. For the concrete problem (Fig. 14), all training cases lie in one dense region. Here, all down-sampling methods perform equally well. ROIDS: Robust Outlier-Aware Informed Down-Sampling  

> (a) IDS (b) ROIDS

Figure 14: UMAP Visualization of the concrete problem. The color scale shows how often each case is selected under IDS and ROIDS, respectively.  

> (a) IDS (b) ROIDS

Figure 15: UMAP Visualization of the housing problem. The color scale shows how often each case is selected under IDS and ROIDS, respectively.  

> (a) IDS (b) ROIDS

Figure 16: UMAP Visualization of the redwine problem. The color scale shows how often each case is selected under IDS and ROIDS, respectively. 

Figure 13: Analysis of the parameter ùõæ of ROIDS for all real - world problems. The performance over 30 runs is shown. For better readability, outliers are not plotted. 

IDS includes some of the edge cases more frequently (see Fig. 14a), however, this does not impact performance. For the housing problem (Fig. 15), we observe a few visual outlier cases. Those are frequently included in the subsets constructed by IDS (see Fig. 15a), which explains why IDS performs poorly for this problem. In contrast, ROIDS excludes those cases from its subsets most of the time (see Fig. 15b) leading to a better performance compared to IDS. Figure 16 visualizes the distribution of the data points for the 

redwine problem. Here, IDS includes edge cases and potential out-liers more frequently (see Fig. 16a). ROIDS performs significantly better because it includes potential outliers less frequently in its subsets (see Fig. 16b).