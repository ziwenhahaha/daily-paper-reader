Title: TimeSliver : Symbolic-Linear Decomposition for Explainable Time Series Classification

URL Source: https://arxiv.org/pdf/2601.21289v1

Published Time: Fri, 30 Jan 2026 01:34:43 GMT

Number of Pages: 24

Markdown Content:
Published as a conference paper at ICLR 2026 

# TI M E SL I V E R : S YMBOLIC -L INEAR DECOMPOSITION FOR EXPLAINABLE TIME SERIES CLASSIFICATION 

Akash Pandey ∗ Payal Mohapatra ∗ Wei Chen Qi Zhu † Sinan Keten †

Northwestern University, Evanston, IL, USA 

## ABSTRACT 

Identifying the extent to which every temporal segment influences a model’s pre-dictions is essential for explaining model decisions and increasing transparency. While post-hoc explainable methods based on gradients and feature-based attribu-tions have been popular, they suffer from reference state sensitivity and struggle to generalize across time-series datasets, as they treat time points independently and ignore sequential dependencies. Another perspective on explainable time-series classification is through interpretable components of the model, for instance, leveraging self-attention mechanisms to estimate temporal attribution; however, recent findings indicate that these attention weights often fail to provide faithful measures of temporal importance. In this work, we advance this perspective and present a novel explainability-driven deep learning framework, TimeSliver ,which jointly utilizes raw time-series data and its symbolic abstraction to construct a representation that maintains the original temporal structure. Each element in this representation linearly encodes the contribution of each temporal segment to the final prediction, allowing us to assign a meaningful importance score to every time point. For time-series classification, TimeSliver outperforms other temporal attribution methods by 11% on 7 distinct synthetic and real-world multivariate time-series datasets. TimeSliver also achieves predictive performance within 2% of state-of-the-art baselines across 26 UEA benchmark datasets, positioning it as a strong and explainable framework for general time-series classification. 

## 1 INTRODUCTION 

Deep-learning (DL) models such as Convolutional Neural Network (CNN), Long Short-Term Mem-ory (LSTM), and Transformer have proven to be successful as predictive models for time series classification tasks. However, while most DL models offer strong predictive performance, they are not interpretable, limiting our understanding of their decision-making process (Rudin, 2019; Doshi-Velez & Kim, 2017). Interpretable DL models are essential for trust and transparency, particularly in high-stakes domains such as healthcare, law, and finance, where explanations support informed decision-making and regulatory compliance (Rudin, 2019). They also help detect biases in training data, ensure fairer outcomes (Caruana et al., 2015; Molnar, 2020), and facilitate the extraction of new scientific knowledge (Pandey et al., 2025). Over the past few years, several methods have been developed to explain the decisions of DL models. Popular methods like DeepLift and Integrated Gradients attribute predictions via baseline-based backpropagation or path integrals but require careful baseline selection (Shrikumar et al., 2017; Sundararajan et al., 2017). Another method called Grad-CAM, a purely gradient-based approach, attributes importance via output–feature derivatives, but is tailored for CNNs and performs poorly on temporal attribution tasks (Selvaraju et al., 2017; Saha et al., 2024). SHAP-based approaches leverage game-theory-based Shapley scores to provide consistent explanations via a unified framework, but they assume feature independence and scale poorly with dimensionality (Lundberg & Lee, 2017). All these post-hoc interpretability methods face shared challenges of high parametric sensitivity and explanations that often vary significantly across datasets (Turbé et al., 2023). Another set of approaches advocates explainability based on the model’s inherent components . For instance, some works leverage self-attention weights (Wu et al., 2020; Clark et al., 2019; Rogers et al., 

> ∗

Equal contribution 

> †

Equal advising 

1

> arXiv:2601.21289v1 [cs.LG] 29 Jan 2026

Published as a conference paper at ICLR 2026 2021; Vig et al., 2021) in Transformers (Vaswani et al., 2017) as a key tool for model explainability. Grad-SAM (Barkan et al., 2021) enhances this by weighting attention scores with their output gradients. However, due to the non-linearities in Transformers, attention weights often fail to align (unfaithful) with ground-truth attribution (Chefer et al., 2021; Serrano & Smith, 2019; Jain & Wallace, 2019; Wiegreffe & Pinter, 2019). Another instance is a recent Multiple Instance Learning (MIL)-based temporal attribution method (Early et al., 2024), which shows promising results in identifying the importance of each time point. However, it has not been extended to multivariate time-series settings and has limited experimental comparisons only to Grad-CAM and DeepLiftSHAP. Some more recent approaches use self-supervised model behavior consistency (Queen et al., 2023) or the modified information bottleneck (Liu et al., 2024) to compute attribution scores, but they either depend on a pretrained model (Queen et al., 2023) or are computationally complex due to multiple components and hyperparameters (Liu et al., 2024). In protein modeling, COLOR (Pandey et al., 2025) enhances explainability by segmenting protein sequences into motifs Pandey et al. (2024) for representation learning. However, it cannot differentiate between positively and negatively attributing segments, and protein sequences are inherently univariate and composed of categorical variables, unlike multivariate continuous time series data. These limitations (non-linearities leading to unfaithful attributions, inapplicability to multivariate time series, sensitivity to hyperparameters) 

of prior methods motivate us to explore an explainability-driven predictive modeling approach capable of handling multivariate time series with robust attribution capabilities across domains. In this work, we introduce TimeSliver , a novel deep learning model that computes Temporal attribution using S ymbolic– Li near V ector E ncoding for Representation. TimeSliver processes raw (uni- or multi-variate) time series and their symbolic counterparts (via binning) to produce localized, segment-level representations. These representations are then linearly combined into a sequence-length-independent, explainable representation that enables the computation of temporal attribution scores and facilitates insight into the model’s predictions. Our main contributions are as follows:  

> ▶

We propose an explainability-driven deep learning framework , TimeSliver , which learns compact representations through a novel linear composition of symbolic and latent representations of temporal segments to provide temporal importance for multivariate Time Series Classification (TSC) tasks while maintaining state-of-the-art predictive capacity (Section 2.2.3).  

> ▶

TimeSliver provides positive and negative temporal attribution scores to offer a complete explanation of different time points for the model’s prediction (Section 2.2.4).  

> ▶

We evaluate TimeSliver ’s explainability across three diverse real-world applications —audio, sleep-stage classification, and machine fault diagnosis—a s well as on four synthetic datasets, against twelve baseline methods , which place TimeSliver consistently as a top-performing model for identifying positively and negatively influencing temporal segments under various settings (Section 3.1).  

> ▶

We demonstrate TimeSliver ’s competitive predictive performance on 26 multivariate time-series classification tasks from the UEA benchmark (Section 3.2). 

Additional Related Works. Decomposing time-series inputs into human-understandable patterns 

also contributes to explainability. Recent works explore approaches such as shapelet decomposi-tion (Wen et al., 2025b), reinforcement learning-based subsequence selection (Gao et al., 2022a), and abstracted shape representations (Wen et al., 2024). In particular, learnable shapelet-based methods (Wen et al., 2025b; Li et al., 2021a; Qu et al., 2024a; Ma et al., 2020) for encoding subse-quences are popular pattern-based explainable models. These methods are generally better suited for qualitative assessment and exhibit varied performance metrics, making them challenging to benchmark (Wen et al., 2024; 2025b). Another class of self-explainable methods uses neuro-symbolic approaches (Yan et al., 2022) with signal temporal logic (Mehdipour et al., 2020) to output soft-logic predicates at each time step. Architecturally, explainability can also be incorporated through concept bottleneck networks (CBMs) (Koh et al., 2020), which introduce human-understandable concepts as intermediate predictions. However, CBMs typically require dense concept annotations and manual editing, practices often impractical in high-stakes applications. Some recent works address this by proposing data-efficient CBMs (Koh et al., 2020) and exploring their applicability in time-series settings (van Sprang et al., 2024; Wen et al., 2025b). 2Published as a conference paper at ICLR 2026 

## 2 METHODOLOGY 

2.1 PRELIMINARIES AND NOTATIONS 

Although DL models such as 1D CNNs, LSTMs, and Transformers have proven effective for time-series prediction, they often lack explainability, particularly in terms of temporal attribution .

Definition 2.1 (Temporal Attribution-Based Explainability) . Temporal attribution-based explainabil-ity in time series refers to the process of assigning importance scores to each time point in an input sequence by decomposing them into positive and negative contributions. Positive attribution scores quantify how much each time point drives the prediction toward the predicted class, while negative attribution scores quantify how much each time point drives the prediction away from the predicted class. Together, these scores enable identification of which time steps most significantly influence the model’s prediction. 

Problem Statement. Given a dataset D = {(xi, y i)}Ni=1 with N samples, where xi ∈ RL×v is a multivariate time-series input of length L with v features (or number of input channels), and 

yi ∈ { 0, . . . , C −1} is the corresponding class label, we aim to learn an explainable model f by learning a latent representation using parameters θq and then projecting it using a linear layer with parameters θc to predict output logits ˆyi ∈ RC , given as 

ˆyi = f (xi; ( θq, θ c)) ∈ RC .

Based on the optimized parameters (θ∗

> q

, θ ∗ 

> c

), we assign positive and negative temporal attribution scores for each time point k ∈ [1 , L ] in the input sample xi, given as 

{ϕ+( i) 

> k

, ϕ −(i) 

> k

}Lk=1 = fatt 

 xi, (θ∗

> q

, θ ∗ 

> c

), ˆyi

.

The attribution function fatt is derived from the internal representations of the input xi and the outputs of f .2.2 OUR APPROACH 

In this section, we present our explainability-driven novel deep learning model, TimeSliver ,illustrated in Figure 1 which comprises of three key modules: (I) conversion of the raw time-series input xi into temporal segments and learning their representations Q, (II) construction of a latent temporal vector Z using symbolic abstraction of xi, and (III) a linear composition of Q and Z to yield a representation of xi that maintains initial temporal structure. This combined representation is fed to a linear layer , denoted as fcls component, to predict the target label yi. Using the input representations, Q, Z and the logits from fcls , we compute temporal attribution scores via the non-parametric operation fatt described in Section 2.2.4. 

Definition 2.2 (Temporal Segment) . Given a multivariate time series instance xi ∈ RL×v , a temporal segment is defined as a contiguous sub-sequence of xi of length m (with m ≤ L). Formally, a segment is xs = xi[t : t + m] ∈ Rm×v , where t is its start index in xi

2.2.1 MODULE I: L ATENT REPRESENTATION OF TEMPORAL SEGMENTS 

Given a multivariate time-series input xi ∈ RL×v , this module converts xi into κ = L − m + 1 

overlapping latent representations. These representations are obtained using a 1D convolutional operator with kernel size m and stride 1 resulting in κ latent feature vectors in a sequence. Each latent feature vector captures a localized temporal context within the time series. The 1D CNN is parameterized by learnable weights θq and transforms each segment into a q-dimensional latent representation, resulting in a matrix Q ∈ Rκ×q . Formally, this is defined by a learnable mapping, 

g(xi; θq) 7 → Q = [ q1; q2; . . . ; qκ]T , where qj ∈ Rq is the latent vector for the jth segment, enabling end-to-end learning of localized temporal patterns. 2.2.2 MODULE II: S YMBOLIC COMPOSITION -B ASED REPRESENTATION 

In this module, each variate x(j) 

> i

∈ RL, for j ∈ { 0, . . . , v − 1}, is independently discretized into one of n categorical bins using a fixed binning strategy, as proposed by Lin et al. (2007).  

> Code is available at https://github.com/pandeyakash23/TimeSliver

3Published as a conference paper at ICLR 2026 

Figure 1: Overview of TimeSliver : (Module I) temporal segment extraction and latent representation learning (g(xi; θq)); (Module II) symbolic composition of temporal segments; and (Module III) global linear interaction between latent and symbolic representations to generate P , a representation of xi preserving temporal structure. 

P is then passed through a linear layer (h(xi; θc)) to predict yi and used to compute temporal attribution using 

fatt . The right column compares ground truth attribution scores with baseline methods and TimeSliver ,where darker regions indicate positive influence. 

This yields a symbolic matrix si ∈ { 1, . . . , n }L×v , where each element s(t,j ) 

> i

indicates the sym-bolic bin index assigned to the jth variate at time step t. The symbolic representation is formally defined as si = h(xi; n, w ), where h(·) is a deterministic discretization function parameterized by the number of bins n and the compression window size w. In this work, we choose w = 1 .

Figure 2: (a) shows a raw time series in-put, (b) is the symbolic composition ma-trix, Z and (c) shows some sample rows of Z which serve as the Bag-of-Stencils 

to modulate P .

Next, we convert si into a one-hot encoded matrix O ∈

RL×(n·v) by independently applying one-hot encoding to each variate and concatenating the results along the feature dimension. Specifically, for each variate j ∈ { 0, . . . , v − 1},we construct a one-hot matrix O(j) ∈ { 0, 1}L×n, where the tth row O(j) 

> t

corresponds to the one-hot encoding of the symbolic value s(t,j ) 

> i

. The final matrix is formed as: 

O =

h

O(1) ∥ O(2) ∥ · · · ∥ O(v)i

∈ RL×(n·v),

where ∥ denotes concatenation along the column (feature) axis. In alignment with past works (Esmael et al., 2012; Com-bettes et al., 2024; Mohapatra et al., 2025a) noting that using a shared symbolic embedding space across variates can lead to semantic ambiguity and information loss, this structured symbolic encoding ensures that each variate-specific semantic identity is retained. To obtain a segment-wise symbolic representation aligned with the temporal segments extracted in Section 2.2.1, we apply average pooling over the one-hot encoded matrix O ∈

RL×(n·v) using a sliding window of size m and stride 1.This yields a symbolic composition matrix Z ∈ Rκ×(n·v)

as shown in Figure 2b, where each entry Zij captures the normalized frequency of the jth symbolic feature within the 

ith segment as shown in Figure 2c. Formally, this is computed as: 

Zij = 1

m

> m−1

X

> l=0

Oi+l, j , for i ∈ { 0, . . . , κ − 1}, j ∈ { 0, . . . , n · v − 1}, (1) 

where Ot,j denotes the jth one-hot dimension at time step t. Thus, each row Zi: represents the symbolic distribution over the ith temporal segment. 

Remark (Approximate Structural Analogy of Z with Spectral Representation). The Short-Time Fourier Transform (STFT) (Oppenheim et al., 1999) for a segment i (of length m) for an input x

computes the energy at frequency f as, 

Sif = 1

m

> m−1

X

> l=0

x[i + l] e−j2πf l/m 

> 2

4Published as a conference paper at ICLR 2026 providing a localized decomposition of x onto the orthonormal sinusoidal basis {e−j2πf l/m }n−1  

> f=0

,with Sif encoding the segment-level power for each frequency bin f . Analogously, from Equation 1, 

Zij represents the average count of symbolic pattern j within segment i, derived from the columns of 

O, which are orthogonal, approximately paralleling Sif as a segment-level "power" measure. This structural analogy illustrates the similarity between the symbolic-linear composition matrix and spec-trogram representations, both encoding the presence and intensity of discrete components—symbolic patterns and spectral frequencies, respectively—across temporal segments. While this architectural correspondence offers valuable intuition, it is important to note that the underlying mathematical principles of these methodologies are fundamentally distinct. 

Generality of Z with other discretizers, h(·). In this case, we have chosen h(·) based on Lin et al. (2007). We explore other strategies such as Adaptive Brownian Bridge-based Approximation (ABBA) (Elsworth & Güttel, 2020) and Symbolic Fourier Approximation (SFA) (Schäfer & Högqvist, 2012) to construct the categorical representation and follow the same operations to obtain Z. On a synthetic dataset, our explainability scores are almost similar across different discretizers (0.94 AUPRC score, approximately 10% better than the next best model; see Table 1 in Section 3.1). More datasets, evaluation metrics, and further results are given in Sections 3.1 and D.2. This highlights the generality of constructing the composition matrix Z via symbolic representation O for better explainability. In the next section, we show how this matrix enables the construction of a global interaction-based representation that enhances temporal attribution scores. 2.2.3 MODULE III: G LOBAL INTERACTION OF TEMPORAL SEGMENTS 

Modules I and II capture local temporal patterns through segmentation, producing latent segment embeddings Q ∈ Rκ×q and symbolic composition vectors Z ∈ Rκ×(n·v), respectively. Now to predict the target label yi, it is important to consider possible interactions among different segments, and those interactions can be captured by constructing a cross-representation matrix P = Z⊤Q,where P ∈ R(n·v)×q . P aggregates the linear relationships between symbolic and latent segment features, and its size is independent of the sequence length L; thereby assisting in making models with fewer trainable parameters (model details are demonstrated in Appendix B). Each element Pij of the matrix can be expressed as:      

> Pij =
> κX
> k=1
> Zki ·Qkj ,(2)

where Zki is the ith symbolic feature of the kth segment and Qkj is the jth latent feature of the same segment. Thus, each entry in P represents a linear weighted contribution from all temporal segments, providing a global weighted summary of the time-series input. After the construction of P matrix, the temporal ordering is lost. For datasets where the discreminative features are more localized (e.g., localized frequency patterns), Z and Q are sufficient for effective classification. However, for tasks requiring explicit temporal ordering to capture sequential dependencies, we add sinusoidal positional encodings (Vaswani et al., 2017) to xi in Module I (Section 2.2.1) to calculate Q, while Z

remains unaffected. The inclusion of positional encoding for each dataset is determined empirically by comparing predictive performance with and without it. Implementation details are provided in Appendix B.3. 

Supporting Multiple Segment Sizes. The 2D representation P ∈ R(n·v)×q , derived previously, corresponds to a fixed segment size m. However, a single segment size may not capture the diverse temporal patterns needed to accurately predict the output label yi. To address this, we extend the computation of P to multiple segment sizes {m1, m 2, . . . , m |m|}, and stack them along a new axis to obtain a 3D tensor: R ∈ R(n·v)×q×| m|, where each slice P (mℓ) ∈ R(n·v)×q is computed as described in Section 2.2.3, and |m| denotes the number of distinct segment sizes. Although R is a 3D tensor, its dimensions do not reflect a spatial topology; hence, we do not apply any convolutional operations across this representation. 

Intuition of constructing P . Prior works such as SAX-VSM (Senin & Malinchik, 2013) demonstrate the effectiveness of representing each time-series sample as an unordered set ( Bag-of-Words ) of symbolic patterns and leveraging discriminative statistics of symbol occurrences, which enhance predictive performance. Motivated by this, our approach, in contrast, utilizes the collection of segment-wise symbolic occurrences across an input sample—i.e., the rows of Z—as a Bag-of-Stencils (as shown in Figure 2(c)). Through the linear aggregation in Equation 2, each entry of P

5Published as a conference paper at ICLR 2026 aggregates segments weighted by symbolic pattern occurrence (a stencil), which masks the segments where a symbol is absent and enhances the ones where it occurs more frequently, thereby modulating the corresponding latent features from Q. This formulation enables P to capture relevant global discriminative interactions across the sequence linearly. This serves as the foundation for the temporal attribution scoring detailed in the following section while maintaining predictive performance. 

Training TimeSliver . We obtain the predicted logits ˆyi by projecting the representation P to the output space using a linear layer parameterized by θc, given as ˆyi = h(g(xi; θq); θc) ∈ RC . Here, 

g(·; θq) represents the latent representation learning from Module I (Section 2.2.1), and h(·; θc) is the linear projection layer. We optimize the overall model by minimizing the cross-entropy loss: 

(θ∗

> q

, θ ∗ 

> c

) = arg min  

> θq,θ c
> N−1

X

> i=0

L (softmax (ˆ yi), y i) .

2.2.4 CALCULATING TEMPORAL ATTRIBUTION 

Using the optimized parameters (θ∗

> q

, θ ∗ 

> c

), we construct P from the symbolic composition matrix 

Z and learned latent representation Q (Equation 2). The matrix P captures global discriminatory features through linear operations. We then compute the positive and negative temporal attribution scores using the non-parametric function fatt : {ϕ+ 

> k

, ϕ − 

> k

}Lk=1 = fatt 

 P , Z, Q, ˆy. To explain the temporal attribution, we consider the case where |m| = 1 , such that R ≡ P , although it can be extended to |m| > 1 without loss of generality. Let ˆyc denote the logit output corresponding to the predicted class label, c, of the input x. We first compute the influence of the element Pij ∈ P on ˆyc as gij = ∂ ˆyc 

> ∂P ij

. We then define the gradient directionality of gij by σij = sign (gij ), which indicates whether perturbations in Pij are expected to increase ( σij = +1 ) or decrease ( σij = −1) the logit. Based on Equation 2, each Pij can be decomposed into κ components corresponding to κ temporal segments. Therefore, we estimate the normalized positive and negative contributions of the kth (k ∈ [0 , κ − 1] ) segment for a given gij 

and σij as: 

ζ+ 

> k,ij

(gij , σ ij ) = |gij |× ReLU  σij Zki Qkj 



max  

> l

ReLU  σij Zli Qlj 

+ϵ , ζ− 

> k,ij

(gij , σ ij ) = |gij |× ReLU   − σij Zki Qkj 



max  

> l

ReLU   − σij Zli Qlj 

+ϵ ;

(3) 

where ϵ = 10 −18 and used for numerical stability. We further justify the choice of ReLU in assigning positive and negative attribution scores in Table 13 of the Ap-pendix. While determining the contributions, it is crucial that the Zki Qkj terms in Equa-tion 3 remain agnostic to the absolute scale of the terms; otherwise, this can lead to spurious attributions caused by high-magnitude but semantically irrelevant input segments. FreqSum SeqComb-          

> UV
> SeqComb-
> MV
> LOWVAR
> Synthetic Datasets
> 0.0
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0
> AUPRC Score
> (a)
> +
> kwith Z
> +
> kwith raw X
> FreqSum SeqComb-
> UV
> SeqComb-
> MV
> LOWVAR
> Synthetic Datasets
> 0.0
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0
> Accuracy
> (b)
> acc . with Z
> acc . with raw X
> Figure 3: Impact of using raw Xinstead of
> Zon (a) explainability (AUPRC) and (b) predictability (Accuracy).

Our construction of the Z matrix, composed of the frequency of sym-bolic component occurrences within a segment, helps in determining a scale-invariant attribution score. A more formal representation of this property is provided in Appendix A. In Appendix F, we also show that TimeSliver aligns with some key desirable properties for attribution methods (Sundararajan et al., 2017). This design choice is validated through an experiment in which replacing the symbolic composition matrix Z with a dimensionality-matched pro-jection of raw inputs xi ∈ X results in a 17% average drop in explainability (AUPRC) across four synthetic datasets (Figure 3a), while maintaining equivalent predictive accuracy (Figure 3b). Since, P ∈ R(n·v)×q is a 2D matrix, the final positive ( ϕ+ 

> k

) and negative ( ϕ− 

> k

) attribution score of the kth temporal segment in xc is calculated as: 

ϕ+ 

> k

=

> n.v −1

X

> i=0
> q−1

X

> j=0

ζ+ 

> k,ij

and ϕ− 

> k

=

> n.v −1

X

> i=0
> q−1

X

> j=0

ζ− 

> k,ij

(4) 

2.2.5 METRICS TO EVALUATE TEMPORAL ATTRIBUTION 

Evaluating on synthetic dataset. The salient time points in synthetic datasets are known (Queen et al., 2023; Liu et al., 2024) and represented by a binary vector G ∈ { 0, 1}L×1. Temporal attribution 6Published as a conference paper at ICLR 2026 

Table 1: Comparison of mean ±std AUPRC on synthetic datasets. 

Bold : best, underlined: second-best. NA: not applicable                                                                       

> Method FreqSum SeqComb-UV SeqComb-MV LOWVAR
> Random 0.35 ±0.06 0.23 ±0.04 0.22 ±0.04 0.08 ±0.03
> Grad-CAM 0.64 ±0.09 0.61 ±0.02 0.61 ±0.02 0.55 ±0.01
> LIME 0.36 ±0.09 0.26 ±0.07 0.23 ±0.07 0.10 ±0.06
> LIMESegment NA 0.76 ±0.08 NA NA KernelShap 0.37 ±0.06 0.30 ±0.05 0.24 ±0.05 0.12 ±0.05
> Integrated Gradient 0.59 ±0.10 0.36 ±0.16 0.36 ±0.13 0.73 ±0.34
> GradientSHAP 0.54 ±0.09 0.57 ±0.09 0.39 ±0.16 0.50 ±0.20
> DeepLift 0.61 ±0.08 0.61 ±0.03 0.57 ±0.10 0.54 ±0.06
> DeepLiftShap 0.61 ±0.08 0.61 ±0.04 0.58 ±0.09 0.54 ±0.05
> Attention Tracing 0.35 ±0.06 0.24 ±0.06 0.23 ±0.05 0.08 ±0.03
> Grad-SAM 0.67 ±0.03 0.61 ±0.02 0.61 ±0.02 0.54 ±0.01
> COLOR 0.53 ±0.13 0.90 ±0.05 0.72 ±0.13 0.96 ±0.09
> TimeX++ 0.59 ±0.01 0.85 ±0.02 0.76 ±0.01 0.95 ±0.01
> TimeSliver 0.94 ±0.05 0.97 ±0.03 0.94 ±0.01 0.99 ±0.04 10 20 30 40 50 60 70 80 90 100
> Unmasking u%
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0
> Accuracy  e(u)
> Random
> Integrated Gradient
> DeepLift
> TimeSliver

Figure 4: Positive attribution study. Accuracy curves e(u) plotted against the unmasking percentage u% for EEG dataset. 

scores ( ϕ+) are softmax-normalized into probabilities and evaluated against G using the area under the precision-recall curve (AUPRC), where higher values indicate a better method. 

Evaluating on real-world datasets. To quantitatively evaluate temporal attribution scores and compare them against contemporary methods, we adapt masking-based evaluation techniques from prior work in time series (Queen et al., 2023), and protein (Pandey et al., 2025). To evaluate positive attributions, time points are first ranked based on ϕ+. All the time points except the top u% are then masked ( xti=0 if masked) in both the training and test sets. The model is re-trained and evaluated using only the unmasked time points. Given that all datasets are class-balanced (see Appendix C for details), we use accuracy as the evaluation metric. Training quality with partial unmasking is sensitive to the masking method (zeroing or imputation) (Hooker et al., 2019). To ensure a fair comparison across explainable methods, re-training is performed on four different architectures, and the mean accuracies are reported. The value of u is incrementally increased, and with each step, the model is re-trained and the accuracy e(u) on the test data is recorded. The area under the e(u) versus u curve, 

I, calculated as: 

I(U) = 

Z U

> 0

e(u)du, (5) 

is used to quantitatively compare different interpretable models. We use I(100 ) and I(20 ) for the comparison in our experiments. The former captures the entire area under the curve, reflecting overall explainability, while the latter emphasizes the model’s effectiveness in identifying the most critical time points. The higher these values, the more interpretable the method. Unmasking time points from best to worst is more appropriate for time series data, as the discriminative information in time series is often distributed across many timesteps (Queen et al., 2023). To assess negative attributions, we mask the top 2% and 5% of time points with the largest ϕ−. We expect that masking the negatively attributing time points leads to an increase in the predicted class (ˆyc). Therefore, to compare the models in terms of their effectiveness in estimating the negatively attributing time points, we calculate: ∆ˆ yc(u−) = ˆ yc(u−) − ˆyc, where ˆyc(u−) denotes the predicted logit after masking the top u− ∈ { 2% , 5% } of negatively attributing time points. The higher the 

∆ˆ yc(u−) value, the better the model is at identifying the negatively attributing time points for the predicted class. 

## 3 EXPERIMENTAL RESULTS 

We evaluate TimeSliver against twelve temporal attribution methods across 7 datasets, using the explainability metrics from Section 2.2.5. We also report its accuracy on the UEA benchmark to demonstrate predictive performance. 

Datasets. We use four synthetic datasets from Turbé et al. (2023) and Queen et al. (2023): FreqSum, SeqComb-UV, SeqComb-MV, and LowVar which capture a wide variety of temporal dynamics within univariate and multivariate settings (more details in Section B.1 of the Appendix). We leverage three real-world TSC applications: (1) single-channel electroencephalogram (EEG) data for sleep stage classification from 20 healthy individuals, with a sequence length of 3000; (2) the FordA machine fault diagnosis dataset for binary classification (Bagnall et al., 2018), with a sequence length of 500; and (3) an animal sound classification dataset from the Environmental Sound Classification corpus (ESC-50) (Piczak), consisting of 5-second audio clips. The audio data is processed using mel-frequency spectral representation, following standard practice (Piczak). More details about datasets are provided in Appendix B. 7Published as a conference paper at ICLR 2026 

Baselines. TimeSliver is compared with several gradient-, kernel-, and sampling-based post-hoc methods—Grad-CAM (Selvaraju et al., 2017), DeepLIFT (Shrikumar et al., 2017), Integrated Gradi-ents (Sundararajan et al., 2017), GradientSHAP (Lundberg & Lee, 2017), DeepLiftSHAP (Lundberg & Lee, 2017), LIME (Ribeiro et al., 2016), as well as LIMESegment (Sivill & Flach, 2022), a time-series extension of LIME, KernelSHAP (Lundberg & Lee, 2017), and TimeX++ (Liu et al., 2024), an information-bottleneck-based explainable approach for time series data. Note that LIMESegment is applicable only to univariate time series with sequence length < 500 . We also include self-attention-based explainable models—Attention Tracing (Wu et al., 2020), which estimates temporal importance from Transformer attention weights, and Grad-SAM (Barkan et al., 2021), adapted from the language domain—as well as another explainable model from the protein domain, COLOR (Pandey et al., 2025). A Random baseline, assigning uniform attribution scores across time points, is also included. 

Explainability Study. Each dataset has three distinct splits (80% train, 10% valid and 10% test), with three trials per split. For each split, we first train the predictive model using four different backbones: 1D CNN, Transformer, COLOR, and TimeSliver (details in Appendix B). TimeSliver achieves predictive performance within 3–4% of the other backbones, indicating that all models are trained comparably well and enabling a fair comparison. Subsequently, Attention Tracing and Grad-SAM are implemented on the Transformer backbone, while all other explainable methods, except COLOR, are applied to the CNN backbone. We evaluate the temporal attribution scores computed using different explainable methods on all four backbones using the metrics discussed in Section 2.2.5. The results are then averaged across all backbones for each explainable method. 3.1 IMPROVEMENT OF TI M E SL I V E R OVER BASELINES ON EXPLAINABILITY (T EMPORAL 

ATTRIBUTIONS )Table 1 reports the AUPRC values computed as described in Section 2.2.5 for various explainable methods for all four synthetic datasets. TimeSliver achieves an average of 18% improvement over the leading baseline . Qualitative comparisons of TimeSliver ’s temporal attribution scores against ground truth importance scores are provided in Appendix G. To quantitatively evaluate different explainable methods on real-world datasets, we report I(100) and I(20) values, as de-fined in Section 2.2.5, computed across three splits for all three datasets (Table 2 and Appendix D). 

TimeSliver consistently outperforms baselines by 2% in I(20) , demonstrating superior abil-ity to identify key positive time points. To further demonstrate the effectiveness of TimeSliver 

in capturing positive critical time steps, we present the e(u) versus u% curve for the EEG dataset in Figure 4. TimeSliver outperforms the strongest baselines, namely Integrated Gradients and DeepLIFT. Interestingly, Figure 4 shows a sharp accuracy drop when all time steps are unmasked ( e(100) ), revealing negatively contributing segments. Table 3 compares TimeSliver with baselines on computing negative temporal attributions using ∆ˆ yc(u−) (Section 2.2.5). On EEG, TimeSliver 

achieves a mean increase of 0.26 in the predicted logit after masking the top negatively attributing time points, an increase that is 60% higher than the next best baseline. In the audio and FordA 

Table 2: Positive attribution results, with the mean ±std I(100) and I(20) values. Bold : best, underlined :second-best. ↑ denotes higher is better. NA: not applicable 

Method Audio EEG FORD-A 

I(100) ↑ I(20) ↑ I(100) ↑ I(20) ↑ I(100) ↑ I(20) ↑

Random 67 .90 ±0.30 9.06 ±0.02 62 .66 ±1.85 9.33 ±0.38 73 .89 ±1.96 8.89 ±0.25 

Grad-CAM 69 .79 ±0.38 10 .05 ±0.07 67 .23 ±0.96 10 .70 ±0.33 81 .43 ±0.04 11 .07 ±0.04 

LIME 69 .09 ±0.44 9.54 ±0.37 64 .02 ±1.3 10 .55 ±0.33 66 .92 ±0.46 10 .47 ±0.04 

LIMESegment NA NA NA NA 76 .14 ±0.53 10 .57 ±0.11 

KernelShap 72 .26 ±1.71 10 .52 ±0.51 66 .92 ±0.46 10 .47 ±0.04 67 .80 ±1.4 11 .10 ±0.37 

Integrated Gradient 63 .69 ±0.04 8.34 ±0.46 83 .19 ±0.89 14 .24 ±0.29 93 .65 ±0.19 14 .76 ±0.14 

GradSHAP 69 .53 ±0.31 10 .48 ±0.17 63 .76 ±2.27 10 .41 ±0.26 78 .54 ±0.64 11 .44 ±0.03 

DeepLift 63 .35 ±0.39 8.15 ±0.11 80 .43 ±0.70 13 .63 ±0.32 93 .11 ±0.08 14 .41 ±0.07 

DeepLiftShap 70 .55 ±0.40 10 .70 ±0.08 65 .34 ±1.78 10 .66 ±0.34 85 .30 ±0.27 13 .30 ±0.07 

Attention Tracing 69 .15 ±0.49 9.75 ±0.42 63 .16 ±2.42 9.28 ±0.30 76 .47 ±0.36 9.60 ±0.42 

Grad-SAM 69 .00 ±0.12 9.89 ±0.24 62 .11 ±2.92 9.38 ±0.34 74 .17 ±0.01 9.17 ±0.14 

COLOR 71 .46 ±0.67 10 .47 ±0.14 66 .48 ±1.47 10 .85 ±0.34 83 .95 ±0.85 12 .12 ±0.22 

TimeX++ 73 .24 ±0.35 11 .20 ±0.12 74 .10 ±0.49 11 .84 ±0.09 87 .85 ±0.53 13 .83 ±0.18 

TimeSliver 74.30 ±0.68 11.35 ±0.15 83.99 ±0.61 14.52 ±0.15 93.87 ±0.01 14.99 ±0.01 

8Published as a conference paper at ICLR 2026 

Table 3: Negative attribution results showing ∆ˆ yc(u−) . Bold : best; underlined : second-best. ↑ indicates higher is better. NA: not applicable 

EEG Audio FordA Methods ∆ˆ yc(2%) ↑ ∆ˆ yc(5%) ↑ ∆ˆ yc(2%) ↑ ∆ˆ yc(5%) ↑ ∆ˆ yc(2%) ↑ ∆ˆ yc(5%) ↑

Random −0.11 ±0.2 −0.15 ±0.19 −0.25 ±0.27 −0.20 ±0.22 −0.04 ±0.17 −0.08 ±0.25 

Grad-CAM 0.04 ±0.11 0.01 ±0.12 −0.07 ±0.23 −0.12 ±0.28 −0.03 ±0.18 −0.09 ±0.27 

LIME 0.05 ±0.10 0.06 ±0.11 −0.10 ±0.20 −0.10 ±0.19 −0.08 ±0.21 −0.10 ±0.27 

LIMESegment NA NA NA NA 0.02 ±0.07 0.02 ±0.12 

KernelShap 0.06 ±0.11 0.03 ±0.12 −0.17 ±0.35 −0.16 ±0.31 −0.04 ±0.18 −0.13 ±0.23 

Integrated Gradient 0.16 ±0.17 0.17 ±0.20 −0.12 ±0.27 −0.08 ±0.28 0.02 ±0.08 0.02 ±0.07 

GradSHAP 0.00 ±0.17 −0.02 ±0.15 −0.22 ±0.30 −0.18 ±0.31 −0.05 ±0.17 −0.04 ±0.15 

DeepLift 0.11 ±0.17 0.16 ±0.17 −0.15 ±0.33 −0.17 ±0.30 0.00 ±0.16 0.02 ±0.07 

DeepLiftShap 0.03 ±0.16 0.07 ±0.17 −0.15 ±0.31 −0.14 ±0.30 0.00 ±0.10 0.00 ±0.14 

Attention Tracing 0.06 ±0.13 −0.06 ±0.13 −0.13 ±0.27 −0.13 ±0.29 −0.05 ±0.19 −0.11 ±0.17 

Grad-SAM 0.06 ±0.13 0.00 ±0.16 −0.16 ±0.31 −0.11 ±0.26 −0.10 ±0.26 −0.05 ±0.21 

COLOR 0.05 ±0.14 0.03 ±0.14 −0.19 ±0.25 −0.08 ±0.22 −0.04 ±0.16 −0.08 ±0.21 

TimeX++ 0.12 ±0.16 0.11 ±0.12 −0.09 ±0.23 −0.15 ±0.33 −0.01 ±0.17 −0.05 ±0.15 

TimeSliver 0.26 ±0.22 0.26 ±0.19 -0.07 ±0.22 -0.08 ±0.26 0.02 ±0.07 0.02 ±0.07 

datasets, the mean increase in the logit is close to zero, indicating the absence of negatively attributing time points in the majority of samples. 3.2 COMPETITIVE PERFORMANCE OF TI M E SL I V E R ON MULTIVARIATE TIME SERIES 

CLASSIFICATION 

Table 4: Accuracy (acc.) of TimeSliver vs. 16 baselines on 26 UEA datasets. Bold : best, Underlined :second-best.                                                                                                                              

> Type Method Bio. Motion Audio Coord. Misc. All
> Distance-Based DTW_D 46.9 87.5 44.5 95.4 63.9 66.8 DTW_I 47.9 77.1 34.4 90.7 59.2 61.3 DTW_A 45.4 88.1 63.3 95.4 71.3 69.7 Dictionary-Based MUSE 55.7 88.3 51.8 96.0 81.7 72.3 gRSF 49.0 84.3 46.1 88.3 71.7 63.8 CIF 54.0 85.4 55.1 96.2 83.0 71.5 Feature-Based ML MrSEQL 52.3 87.8 47.6 94.2 76.6 69.2 ROCKET 53.8 90.1 48.7 96.6 75.6 70.2 Deep-Learning TapNet 49.9 84.5 51.5 91.5 65.4 64.8 ResNet 48.3 90.6 47.5 97.3 57.2 63.3 IncTime 60.3 96.4 61.6 95.0 69.1 74.3 FCN 56.76 90.8 58.4 97.8 63.4 72.2 TS2Vec 48.57 87.4 53.2 94.7 65.0 68.0 TimesNet 61.06 77.9 42.1 91.3 61.9 67.3 ShapeNet 55.28 86.3 59.3 94.0 53.3 68.2 RLPAM 66.75 89.5 55.1 90.0 67.4 74.3 ShapeConv 61.24 89.0 54.1 95.0 64.8 72.4 SBM 59.8 86.0 45.8 94.1 66.7 70.5 InterpGN 58.73 91.4 52.6 98.3 70.7 73.7
> Ours TimeSliver 66.9 90.9 55.2 93.6 76.1 75.6

We evaluate the predictive performance of 

TimeSliver (fcls ) on 26 datasets from the UEA multivariate time-series classifica-tion archive (Ruiz et al., 2021), which span 8 electrical biosignal (Bio.) datasets, 3audio datasets, 7 accelerometer-based mo-tion datasets, 3 gesture and digit recogni-tion datasets in Cartesian coordinates (Coord.), and other miscellaneous datasets and evalu-ate TimeSliver against five methodologi-cal categories: Distance-based methods (Bag-nall et al., 2016); Dictionary/interval-based 

methods (Schäfer & Leser, 2017); Feature-based ML models; Deep learning models in-cluding ResNet (Wang et al., 2017), Inception-Time (Fawaz et al., 2020), FCN (Karim et al., 2017), TS2vec (Yue et al., 2022), TimesNet (Wu et al., 2022), ShapeNet (Li et al., 2021b), RL-PAM (Gao et al., 2022b), ShapeConv (Qu et al., 2024b), SBM (Wen et al., 2025a), InterpGN (Wen et al., 2025a); and Ensemble-based meth-ods. More details are provided in Section E of the Appendix. We report the predictive performance in Table 4 and observe that: (1) TimeSliver 

demonstrates superior performance on datasets with long sequences (length > 1000), achieving 

an average absolute improvement of 4.4% over the best-performing baselines (details in Ap-pendix E); (2) it improves performance on electrical biosignals by 6.3% on average ; and (3) overall, TimeSliver delivers competitive predictive performance, staying within 2% of the best baselines across diverse applications. To further showcase TimeSliver ’s strong performance, we report its average rank, top-1, and top-3 counts compared to all baselines in Appendix E. To empirically demonstrate TimeSliver ’s ability to capture temporally disjoint yet jointly infor-mative patterns, we construct a synthetic dataset with multiplicative interactions in disjoint segments (Appendix D.3) and evaluate its predictive performance against multiple architectures. As shown 9Published as a conference paper at ICLR 2026 in Appendix D.3, TimeSliver achieves performance within 1% of the baselines, confirming its effectiveness in modeling temporally disjoint interactions. 3.3 UNDERSTANDING THE COMPONENTS OF TI M E SL I V E R 

We assess the impact of TimeSliver ’s core design choices through ablation and sensitivity analysis to gain deeper insights. 

Ablation Study (Figure 5). The framework of Boureau et al. (2010) shows that average pooling lowers feature map resolution, reducing sensitivity to local perturbations and acting as a regularizer. We verify this by removing average pooling after R or replacing it with max pooling, which causes a ∼5% drop in accuracy with negligible impact on explainability. To assess the role of ReLU in Equations 4 in identifying positive and negative attributing time points, we replace ReLU with abs .This assigns equal importance to time points with equal abs (Zki Qkj ) but opposite signs. This modification leads to a 13% drop in explainability, underscoring the importance of ReLU in correctly distinguishing positive and negative contributions.                           

> Figure 5: Impact of model vari-ants on prediction (Predict.) and explainability (Explain.). 0.5 0.6 0.7 0.8 0.9 1.0
> Accuracy
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> AUPRC
> m=1 m=5 m=10 m=20 m=50
> (a)
> 0.70 0.75 0.80 0.85 0.90 0.95
> Accuracy
> 0.90
> 0.91
> 0.92
> 0.93
> 0.94
> 0.95
> 0.96
> q=1 q=4 q=16 q=36 q=64
> (b)
> 0.91 0.92 0.93 0.94
> Accuracy
> 0.90
> 0.91
> 0.92
> 0.93
> 0.94
> 0.95
> 0.96
> n=2 n=4, 25, 60 n=15
> (c)
> 0200 400
> n
> 0.025
> 0.050
> GFLOPs
> 12345
> |m|
> 0.01
> 0.02
> GFLOPs
> (d)
> Figure 6: Effect of (a) segment size m, (b) latent dimension q, and (c) number of bins non predictability (Accuracy) and explainability (AUPRC); (d) GFLOPs variation with nand |m|.

Sensitivity Analysis (Figure 6). The final architecture of TimeSliver is determined by three key hyperparameters: the segment size m, the latent representation dimension q (both defined in Section 2.2.1), and the number of bins n used to discretize raw inputs xi into symbolic representations 

si (described in Section 2.2.2). On the FreqSum dataset, Figure 6a shows that explainability declines for m > 10 , as larger segments can lead the model to over-attribute importance to regions where only a small part is relevant. Conversely, setting m = 1 results in poor predictability and explainability, as the segment is too short to capture meaningful temporal patterns. The effect of the latent dimension q

and the bin count n is minimal beyond values of 4, with both explainability and predictive accuracy remaining stable (see Figures 6b and 6c). Although this analysis is based on FreqSum data, the relative sensitivity trends are expected to generalize across a wide range of real-world datasets. Figure 6d shows that TimeSliver ’s GFLOPs only scale linearly with n ∈ [2 , 500] and |m| ∈ [1 , 5] ,remaining 5–10 times lower than those of Transformers (GFLOP = 0.2), highlighting its efficient scalability. 

## 4 CONCLUSION 

In this work, we presented TimeSliver —a novel deep learning framework that linearly combines raw time series with their symbolic counterparts to construct a global representation facilitating temporal attribution calculation. Our importance scores offer insights into positively and negatively influencing time segments. The effectiveness of TimeSliver is demonstrated by its average improvement of 11% over the best baselines across seven diverse datasets, spanning real-world and synthetic, univariate and multivariate time series with varied temporal dynamics, while maintaining high predictive performance. In the future, it will be interesting to consider human-in-the-loop expert validation (for tasks like sleep-stage classification using EEG) to harness TimeSliver ’s explainability for practical applications. Additionally, TimeSliver ’s principles can be extended to provide feature attribution, identifying which input features are most influential at each time segment, especially by considering a time-frequency representation of time-series data. 10 Published as a conference paper at ICLR 2026 

## ETHICS STATEMENT .

This work does not involve human subjects, sensitive data, or issues related to fairness, discrimination, or legal compliance. TimeSliver is designed to identify influential temporal segments in time series, providing more transparent and interpretable model predictions. By improving explainability, particularly for applications such as healthcare time-series classification, TimeSliver supports responsible and trustworthy deployment of machine learning models. 

## REPRODUCIBILITY STATEMENT 

All source code to reproduce experimental results (with instructions for running the code) is provided in the Supplementary Materials. We use public datasets and include implementation details in the Appendix. All baselines either adopt published hyperparameters or are tuned when unspecified. 

## LLM U SAGE STATEMENT 

The usage of LLMs in this work is limited to paper writing support, language refinement, and experimental data processing. Specifically, LLMs assisted in improving the clarity and coherence of the manuscript, generating LaTeX tables, and formatting results for presentation. Importantly, LLMs were not involved in the design of algorithms, the development of theoretical results, or the execution of experiments, ensuring that all core scientific contributions remain entirely the work of the authors. 

## REFERENCES 

Anthony Bagnall, Jason Lines, Aaron Bostrom, James Large, and Eamonn Keogh. The great time series classification bake off: A review and experimental evaluation of recent algorithmic advances. 

Data Mining and Knowledge Discovery , 31:606–660, 2016. Anthony Bagnall, Jason Lines, William Vickers, and Eamonn Keogh. Time series classification with ensembles of elastic distance measures. Data Mining and Knowledge Discovery , 31:565–592, 2017. Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075 , 2018. Oren Barkan, Edan Hauon, Avi Caciularu, Ori Katz, Itzik Malkiel, Omri Armstrong, and Noam Koenigstein. Grad-sam: Explaining transformers via gradient self-attention maps. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management , pp. 2882– 2887, 2021. Y-Lan Boureau, Jean Ponce, and Yann LeCun. A theoretical analysis of feature pooling in visual recognition. In Proceedings of the 27th International Conference on International Conference on Machine Learning , ICML’10, pp. 111–118, 2010. Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining , pp. 1721–1730, 2015. Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In 

Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 782–791, 2021. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an analysis of BERT‘s attention. In Tal Linzen, Grzegorz Chrupała, Yonatan Belinkov, and Dieuwke Hupkes (eds.), Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. 276–286, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https://aclanthology. org/W19-4828/ .11 Published as a conference paper at ICLR 2026 Sylvain W Combettes, Charles Truong, and Laurent Oudre. Symbolic representation for time series. In 2024 32nd European Signal Processing Conference (EUSIPCO) , pp. 1962–1966. IEEE, 2024. Angus Dempster, François Petitjean, and Geoffrey I Webb. Rocket: Exceptionally fast and accurate time series classification using random convolutional kernels. Data Mining and Knowledge Discovery , 34:1454–1495, 2020. Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning. 

arXiv preprint arXiv:1702.08608 , 2017. Joseph Early, Gavin Cheung, Kurt Cutajar, Hanting Xie, Jas Kandola, and Niall Twomey. Inherently interpretable time series classification via multiple instance learning. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum? id=xriGRsoAza .Steven Elsworth and Stefan Güttel. Abba: adaptive brownian bridge-based symbolic aggregation of time series. Data Mining and Knowledge Discovery , 34(4):1175–1200, 2020. Bilal Esmael, Arghad Arnaout, Rudolf K Fruhwirth, and Gerhard Thonhauser. Multivariate time series classification by combining trend-based and value-based approximations. In Computational Science and Its Applications–ICCSA 2012: 12th International Conference, Salvador de Bahia, Brazil, June 18-21, 2012, Proceedings, Part IV 12 , pp. 392–403. Springer, 2012. Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, et al. Inceptiontime: Finding alexnet for time series classification. Data Mining and Knowledge Discovery , 34:1936–1962, 2020. Ge Gao, Qitong Gao, Xi Yang, Miroslav Pajic, and Min Chi. A reinforcement learning-informed pattern mining framework for multivariate time series classification. In 31st International Joint Conference on Artificial Intelligence (IJCAI) , 2022a. Ge Gao, Qitong Gao, Xi Yang, Miroslav Pajic, and Min Chi. A reinforcement learning-informed pattern mining framework for multivariate time series classification. In 31st International Joint Conference on Artificial Intelligence (IJCAI) , 2022b. Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretability methods in deep neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32. Cur-ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/ paper/2019/file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf .Sarthak Jain and Byron C. Wallace. Attention is not Explanation. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 3543–3556, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1357. URL https://aclanthology. org/N19-1357/ .Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Shun Chen. Lstm fully convolutional networks for time series classification. IEEE access , 6:1662–1669, 2017. Isak Karlsson, Panagiotis Papapetrou, and Henrik Boström. Mr-seql: Multiple representation sequence learner. In 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW) , pp. 1–8, 2016. Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In International conference on machine learning , pp. 5338–5348. PMLR, 2020. Guozhong Li, Byron Choi, Jianliang Xu, Sourav S Bhowmick, Kwok-Pan Chun, and Grace Lai-Hung Wong. Shapenet: A shapelet-neural network approach for multivariate time series classification. In 

Proceedings of the AAAI conference on artificial intelligence , volume 35, pp. 8375–8383, 2021a. 12 Published as a conference paper at ICLR 2026 Guozhong Li, Byron Choi, Jianliang Xu, Sourav S Bhowmick, Kwok-Pan Chun, and Grace Lai-Hung Wong. Shapenet: A shapelet-neural network approach for multivariate time series classification. In 

Proceedings of the AAAI conference on artificial intelligence , volume 35, pp. 8375–8383, 2021b. Jessica Lin, Eamonn Keogh, Li Wei, and Stefano Lonardi. Experiencing sax: a novel symbolic representation of time series. Data Mining and knowledge discovery , 15:107–144, 2007. Jason Lines, Sarah Taylor, and Anthony Bagnall. Time series classification with hive-cote: The hierarchical vote collective of transformation-based ensembles. ACM Transactions on Knowledge Discovery from Data (TKDD) , 12(5):1–35, 2018. Zichuan Liu, Tianchun Wang, Jimeng Shi, Xu Zheng, Zhuomin Chen, Lei Song, Wenqian Dong, Jayantha Obeysekera, Farhad Shirani, and Dongsheng Luo. Timex++: learning time-series explanations with information bottleneck. ICML’24. JMLR.org, 2024. Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems , 30, 2017. Qianli Ma, Wanqing Zhuang, Sen Li, Desen Huang, and Garrison Cottrell. Adversarial dynamic shapelet networks. In Proceedings of the AAAI conference on artificial intelligence , volume 34, pp. 5069–5076, 2020. Noushin Mehdipour, Cristian-Ioan Vasile, and Calin Belta. Specifying user preferences using weighted signal temporal logic. IEEE Control Systems Letters , 5(6):2006–2011, 2020. Matthew Middlehurst, James Large, and Anthony Bagnall. The canonical interval forest (cif) classifier for time series classification. In 2020 IEEE International Conference on Data Mining (ICDM) , pp. 948–953, 2020. Payal Mohapatra, Akash Pandey, Bashima Islam, and Qi Zhu. Speech disfluency detection with contextual representation and data distillation. In Proceedings of the 1st ACM international workshop on intelligent acoustic systems and applications , pp. 19–24, 2022. Payal Mohapatra, Akash Pandey, Yueyuan Sui, and Qi Zhu. Effect of attention and self-supervised speech embeddings on non-semantic speech tasks. In Proceedings of the 31st ACM International Conference on Multimedia , pp. 9511–9515, 2023. Payal Mohapatra, Yueyuan Sui, Akash Pandey, Stephen Xia, and Qi Zhu. Maestro: Adaptive sparse attention and robust learning for multimodal dynamic time series. arXiv preprint arXiv:2509.25278 ,2025a. Payal Mohapatra, Lixu Wang, and Qi Zhu. Phase-driven generalizable representation learning for nonstationary time series classification. Transactions on Machine Learning Research , 2025b. Christoph Molnar. Interpretable machine learning . Lulu. com, 2020. Alan V. Oppenheim, Ronald W. Schafer, and John R. Buck. Discrete-Time Signal Processing .Prentice-hall Englewood Cliffs, 1999. Akash Pandey, Wei Chen, and Sinan Keten. Sequence-based data-constrained deep learning frame-work to predict spider dragline mechanical properties. Communications Materials , 5(1):83, 2024. Akash Pandey, Wei Chen, and Sinan Keten. Color: A compositional linear operation-based represen-tation of protein sequences for identification of monomer contributions to properties. Journal of Chemical Information and Modeling , 2025. Karol J. Piczak. ESC: Dataset for Environmental Sound Classification. In Proceedings of the 23rd Annual ACM Conference on Multimedia , pp. 1015–1018. ACM Press. ISBN 978-1-4503-3459-4. doi: 10.1145/2733373.2806390. URL http://dl.acm.org/citation.cfm?doid= 2733373.2806390 .Eric Qu, Yansen Wang, Xufang Luo, Wenqiang He, Kan Ren, and Dongsheng Li. Cnn kernels can be the best shapelets. In The Twelfth International Conference on Learning Representations , 2024a. 13 Published as a conference paper at ICLR 2026 Eric Qu, Yansen Wang, Xufang Luo, Wenqiang He, Kan Ren, and Dongsheng Li. Cnn kernels can be the best shapelets. In The Twelfth International Conference on Learning Representations , 2024b. Owen Queen, Tom Hartvigsen, Teddy Koker, Huan He, Theodoros Tsiligkaridis, and Marinka Zitnik. Encoding time-series explanations through self-supervised model behavior consistency. Advances in Neural Information Processing Systems , 36:32129–32159, 2023. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. " why should i trust you?" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining , pp. 1135–1144, 2016. Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert works. Transactions of the association for computational linguistics , 8:842–866, 2021. Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature machine intelligence , 1(5):206–215, 2019. Alejandro Pasos Ruiz, Michael Flynn, James Large, Matthew Middlehurst, and Anthony Bagnall. The great multivariate time series classification bake off: a review and experimental evaluation of recent algorithmic advances. Data mining and knowledge discovery , 35(2):401–449, 2021. Avinab Saha, Shashank Gupta, Sravan Kumar Ankireddy, Karl Chahine, and Joydeep Ghosh. Explor-ing explainability in video action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 8176–8181, 2024. Patrick Schäfer and Mikael Högqvist. Sfa: a symbolic fourier approximation and index for similarity search in high dimensional datasets. In Proceedings of the 15th international conference on extending database technology , pp. 516–527, 2012. Patrick Schäfer and Ulf Leser. Multivariate time series classification with weasel+muse. In 2017 IEEE International Conference on Data Mining (ICDM) , pp. 447–456, 2017. Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-ization. In Proceedings of the IEEE international conference on computer vision , pp. 618–626, 2017. Pavel Senin and Sergey Malinchik. Sax-vsm: Interpretable time series classification using sax and vector space model. In 2013 IEEE 13th international conference on data mining , pp. 1175–1180. IEEE, 2013. Sofia Serrano and Noah A Smith. Is attention interpretable? arXiv preprint arXiv:1906.03731 , 2019. Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In International conference on machine learning , pp. 3145– 3153. PMlR, 2017. Torty Sivill and Peter Flach. Limesegment: Meaningful, realistic time series explanations. In 

International Conference on Artificial Intelligence and Statistics , pp. 3418–3433. PMLR, 2022. Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In 

International conference on machine learning , pp. 3319–3328. PMLR, 2017. Hugues Turbé, Mina Bjelogrlic, Christian Lovis, and Gianmarco Mengaldo. Evaluation of post-hoc interpretability methods in time-series classification. Nature Machine Intelligence , 5(3):250–260, 2023. Angela van Sprang, Erman Acar, and Willem Zuidema. Enforcing interpretability in time series transformers: A concept bottleneck framework. arXiv preprint arXiv:2410.06070 , 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017. 14 Published as a conference paper at ICLR 2026 Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, richard socher, and Nazneen Rajani. {BERT}ology meets biology: Interpreting attention in protein language models. In International Conference on Learning Representations , 2021. URL https://openreview.net/forum? id=YWtLZvLmud7 .Zhiguang Wang, Weizhong Yan, and Tim Oates. Time series classification from scratch with deep neural networks: A strong baseline. In 2017 International Joint Conference on Neural Networks (IJCNN) , pp. 1578–1585, 2017. Yunshi Wen, Tengfei Ma, Lily Weng, Lam Nguyen, and Anak Agung Julius. Abstracted shapes as tokens-a generalizable and interpretable model for time-series classification. Advances in Neural Information Processing Systems , 37:92246–92272, 2024. Yunshi Wen, Tengfei Ma, Ronny Luss, Debarun Bhattacharjya, Achille Fokoue, and Anak Agung Julius. Shedding light on time series classification using interpretability gated networks. In The Thirteenth International Conference on Learning Representations , 2025a. Yunshi Wen, Tengfei Ma, Ronny Luss, Debarun Bhattacharjya, Achille Fokoue, and Anak Agung Julius. Shedding light on time series classification using interpretability gated networks. In The Thirteenth International Conference on Learning Representations , 2025b. Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 11–20, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1002. URL https://aclanthology. org/D19-1002/ .Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. arXiv preprint arXiv:2210.02186 ,2022. Zhengxuan Wu, Thanh-Son Nguyen, and Desmond C Ong. Structured self-attention weights encode semantics in sentiment analysis. arXiv preprint arXiv:2010.04922 , 2020. Ruixuan Yan, Tengfei Ma, Achille Fokoue, Maria Chang, and Agung Julius. Neuro-symbolic models for interpretable time series classification using temporal logic description. In 2022 IEEE International Conference on Data Mining (ICDM) , pp. 618–627. IEEE, 2022. Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In Proceedings of the AAAI conference on artificial intelligence , volume 36, pp. 8980–8987, 2022. 15 Published as a conference paper at ICLR 2026 

# APPENDIX 

This appendix provides additional details for " TimeSliver : Symbolic-Linear Decomposition for Explainable Time Series Classification". Additional implementation details for TimeSliver and the backbone models are presented in Section B. Class distribution for the four datasets used in the interpretability study are provided in Section C. Detailed results on interpretability and predictive performance are given in Sections D and E, respectively. 

## A SCALE -I NVARIANCE PROPERTIES OF TI M E SL I V E R 

Remark (Determining Scale-Invariant Attributions). Let Praw = XQ and Psym = ZQ, where 

Q are learned weights. Suppose X = ZD for a diagonal scaling matrix D. Then, for any position t,

∥P(t)raw ∥2 = dt∥P(t)sym ∥2,

where dt is the t-th diagonal entry of D. Thus, only Psym yields attributions invariant to input scaling, and explanations depend solely on the symbolic pattern, not on the magnitude of the input. 

Implication. This property prevents spurious attributions caused by high-magnitude but semantically irrelevant input segments, and is essential for robust and interpretable explanations. The effectiveness of this property is further validated by our ablation study, where replacing the one-hot encoding representation O with raw data x in Equation 1 results in an average 17% decrease in explainability as shown in Figure 3. 

## B ADDITIONAL IMPLEMENTATION DETAILS 

B.1 DATASET DESCRIPTION 

FreqSum is a multivariate time series with randomly embedded sine-wave segments; classes indicate whether the sum of their frequencies exceeds a threshold. As described in (Turbé et al., 2023), each sample in the dataset consists of 6 features and 500 time steps. To simulate realistic temporal dependencies, each feature includes a baseline sine wave with a frequency uniformly sampled from the range [2 , 5] . Two randomly selected features per sample are injected with discriminative sine waves, each supported over 100 time steps, with frequencies drawn from a discrete uniform distribution in the range [10 , 50] . In the remaining four features, a square wave is optionally added with 50% probability, also using frequencies sampled from the same range. The classification task is binary: the model must predict whether the sum of the two discriminative frequencies exceeds a predefined threshold, set to τ = 60 .

SeqComb-UV, SeqComb-MV , and LowVar are generated using the exact technique discussed in Queen et al. (2023). SeqComb-UV is a univariate series with two non-overlapping increasing or decreasing subsequences, with four classes defined by their trend combinations. SeqComb-MV is the multivariate extension of SeqComb-UV. LowVar is a multivariate series with four classes determined by the presence of a low-variance subsequence in a specific channel. 

Audio Dataset. We use a manually curated subset of the ESC-50 audio dataset, focusing exclusively on animal sounds. This subset was selected to leverage the temporal localization of animal sounds, which typically occur within short bursts in the observation window, as opposed to environmental sounds that span the entire duration and yield robust results even with randomly sampled segments. This temporal sparsity makes animal sounds particularly useful for evaluating interpretability methods that rely on temporal attribution. For preprocessing, we extract Mel-frequency cepstral coefficients (MFCCs) from the audio using a Mel spectrogram with 40 Mel bands, employing standard settings such as centered windowing and normalization similar to previous works (Mohapatra et al., 2022; 2023). 

EEG Dataset. This dataset comprises single-channel EEG recordings collected from 20 subjects, with the objective of classifying five sleep stages: wake, N1, N2, N3 (non-REM stages), and REM (rapid eye movement). The temporal structure of EEG signals makes this dataset well-suited for tasks requiring time-series modeling and interpretation. We balance all the classes in the dataset before using it for the study. We follow similar preprocessing as previous work (Mohapatra et al., 2025b). 16 Published as a conference paper at ICLR 2026 

FordA Dataset. We adopt the data preprocessing and train-test splits for the FordA dataset as defined in the MTS-Bakeoff benchmark Ruiz et al. (2021). B.2 DATASET DETAILS 

Information such as the number of variates ( v), maximum sequence length, and dataset splits is provided in Table 5. Table 5: Summary of the four datasets used in the interpretability study. Dataset Num. of Variates, v

Max Seq. Length Train Valid Test FreqSum 6 500 5000 500 500 SeqComb-UV 1 200 5000 1000 1000 SeqComb-MV 4 200 5000 1000 1000 LowVar 2 200 5000 1000 1000 Audio 40 501 280 60 60 EEG 1 3000 5005 1295 3515 Ford-A 1 500 853 106 119 B.3 MODEL DETAILS 

The complete details of TimeSliver for all four datasets are given in Table 6. The selective use of positional encoding shown in the table is determined empirically based on the predictive performance of TimeSliver with and without it. It is also worth noting that positional encoding is only added in Module I (Section 2.2.1), thereby only changing Q and not affecting Z in Equation 1. Additionally, the details for the other three backbones used in the interpretability study are given in Table 7. Table 6: Architecture details of TimeSliver used for different datasets.                                                   

> Dataset Num. of categorical bins, n
> Num. of columns in O,n×v
> Latent vector size, q
> Segment size, m
> Positional Encoding Trainable parameters FreqSum 15 90 36 7✗5,858 SeqComb-UV 20 20 36 [4,7] ✓14,518 SeqComb-MV 10 40 36 [4,7] ✓20,576 LowVar 20 40 36 4✗5,078 Audio 10 400 12 1✗20,110 EEG 25 25 12 10 ✓6,441 Ford-A 70 70 36 10 ✗8,280

Table 7: Number of trainable parameters for different model architectures across datasets. Dataset CNN COLOR Transformer FreqSum 42,378 2,660 46,714 SeqComb-UV 42,076 16,844 361,156 SeqComb-MV 42,268 21,452 361,540 LowVar 42,140 16,880 361,284 Audio 224,938 8,206 370,498 EEG 74,981 43,309 230,805 Ford-A 42,058 26,536 361,090 B.4 TRAINING AND OPTIMIZATION DETAILS 

All experiments are conducted on a server running Ubuntu OS, equipped with NVIDIA RTX A6000 GPUs, using the PyTorch framework. During model training, we employ the Adam optimizer with a 17 Published as a conference paper at ICLR 2026 learning rate ranging from 3 × 10 −4 to 1 × 10 −3. Validation accuracy is used for early stopping and to save the best model checkpoint. B.5 PREDICTIVE RESULTS ON DIFFERENT BACKBONE 

Table 4 presents the predictive performance of the four deep learning models used as backbones in the interpretability study. The CNN backbone is used for all post-hoc interpretability methods, while the Transformer is employed for attention tracing and the Grad-SAM method. COLOR, originally developed for protein sequence design, is inherently interpretable. The predictive performance of 

TimeSliver on the four datasets used in the interpretability study is within 3–4% of the best-performing model. All the post-hoc methods are implemented using the Captum library 0 in PyTorch. Table 8: Accuracy ( mean ±std ) over 3 runs for different predictive backbone and dataset (supporting results for Section 3 in the main paper). Dataset CNN COLOR Transformer TimeSliver 

FreqSum 0.93 ±0.028 0.93 ±0.014 0.95 ±0.0071 0.93 ±0.014 

SeqComb-UV 1.00 ±0.00 1.00 ±0.00 1.00 ±0.00 1.00 ±0.00 

SeqComb-MV 1.00 ±0.00 1.00 ±0.00 1.00 ±0.00 1.00 ±0.00 

LowVar 1.00 ±0.00 1.00 ±0.00 1.00 ±0.00 1.00 ±0.00 

Audio 0.78 ±0.0071 0.80 ±0.021 0.80 ±0.000 0.81 ±0.0071 

EEG 0.78 ±0.019 0.73 ±0.039 0.68 ±0.038 0.72 ±0.042 

FordA 0.92 ±0.0071 0.93 ±0.000 0.83 ±0.0071 0.88 ±0.000 

## C CLASS DISTRIBUTION 

The class distribution for all four datasets is shown in Figure 7, indicating that there is no class imbalance in any of the datasets used in the explainability study. 0 1 2 3 4 5 6 7 8 9

0.00 

0.05 

0.10 

Audio 

0 1 2 3 4

0.0 

0.1 

0.2 

EEG 

0 1

0.0 

0.2 

0.4 

Ford-A 

Class 

> Density

Figure 7: Class distribution among different datasets. 

> 0

https://github.com/pytorch/captum 

18 Published as a conference paper at ICLR 2026 

## D DETAILED EXPLAINABILITY RESULTS 

D.1 EVALUATING POSITIVE TEMPORAL ATTRIBUTION 

Figure 8 shows the mean e(u) versus unmasking percentage ( u%) curves obtained using different interpretability methods, along with their standard deviations. The trend of the curves clearly demonstrates that TimeSliver outperforms the baseline methods in the lower unmasking range (5–20%), highlighting its effectiveness in identifying the most critical time points. 10 20 30 40 50 60 70 80 90 100                      

> Unmasking u%
> 0.5
> 0.6
> 0.7
> 0.8
> Accuracy,  e(u)
> (a)
> Audio
> 10 20 30 40 50 60 70 80 90 100
> Unmasking u%
> 0.6
> 0.8
> 1.0
> Accuracy,  e(u)
> (b)
> Ford-A
> 10 20 30 40 50 60 70 80 90 100
> Unmasking u%
> 0.6
> 0.8
> 1.0
> Accuracy,  e(u)
> (c)
> EEG
> Random
> Grad-CAM
> COLOR
> Attention Tracing
> Grad-SAM
> Integrated Gradient
> GradientSHAP
> DeepLIFT
> DeepLiftSHAP
> TimeSliver

Figure 8: Positive attribution study. Accuracy curves e(u) plotted against the unmasking percentage u% for various methods on three datasets: a) Audio, b) Ford-A, and c) EEG SSC. Each curve represents the mean accuracy over three runs (supporting results for Section 3.1 in the main paper). 

The areas under the curves shown in Figure 8, I(100) and I(20) , are used to quantitatively compare the different interpretability methods. To calculate ∆ˆ yc(u−) (Section 2.2.5) and compare different models in estimating negatively attributing time points, we conduct the study across three different data splits and average the results over all samples. D.2 IMPACT OF OTHER SYMBOLIC REPRESENTATIONS 

Table 9 demonstrates the impact of different discretization functions h(·) (defined in Section 2.2.1) for converting continuous time series into symbolic representations si = h(xi; n, w ). Both ABBA and SFA preserve explainability (AUPRC) and predictability (Accuracy) compared to our default binning approach. However, when we replace the symbolic representation with a higher-dimensional projection of the raw input ( O → xproj ) to calculate Z, explainability drops significantly by 38%, while predictability remains unchanged. This demonstrates that discretizing the continuous time series x into symbolic representation s provides a scale-invariant encoding that ensures uniform treatment of temporal patterns regardless of input magnitude. D.3 TI M E SL I V E R ’S EFFECTIVENESS IN CAPTURING FAR -FIELD INTERACTION 

Although P in Equation 2 is formulated as a linear aggregation of temporal segments, it does not significantly affect the ability of TimeSliver to capture far-field multiplicative interactions. To demonstrate this, we construct a synthetic dataset designed specifically to exhibit strong far-field dependencies. 19 Published as a conference paper at ICLR 2026                

> Symbolic Representation
> Explainability
> AUPRC( ∆% )
> Predictability
> Accuracy( ∆% )
> TimeSliver
> with binning 0.94 ±0.045 0.93 ±0.014
> ABBA 0.94 ±0.048 (0%) 0.93 ±0.05 (0%) SFA 0.93 ±0.068 (-1.0%) 0.92 ±0.02 (-1.0%)
> O → xproj 0.66 ±0.13 (-38.7%) 0.91 ±0.015 (-2.2%)

Table 9: Impact of symbolic representations on explainability and predictability. 

Input Construction. We generate N = 1000 samples, each of length L = 100 . Each sample xi is defined as: 

xi(t) = sin( fit + ϕi) + ηi(t) ·



t − L

2



, t = 1 , . . . , L, (6) where: • fi ∼ U (1 , 10) is a randomly sampled frequency, • ϕi ∼ U (0 , 2π) is a random phase shift, and • ηi(t) is Gaussian noise scaled by the time component  t − L

> 2

 to amplify far-field interac-tions. 

Output Property (Far-Field Interaction). For each sample xi, we define the far-field interaction property: 

pi =

> L/ 2

X

> j=1

xi[j] · xi[L − j + 1] , (7) where xi[j] is the jth element and xi[L − j + 1] is its far-field pair from the opposite end of the sequence. A binary label yi is then assigned: 

yi =

1, if pi > 0,

0, if pi ≤ 0, (8) resulting in a balanced class distribution with a 0:1 ratio of 1:1 .

Results. Table 10 compares the predictive performance of TimeSliver with Transformer, LSTM, and FCN baselines. The results empirically confirm that TimeSliver effectively captures multi-plicative far-field interactions, which we attribute to the fully connected neural network layer present after P in the architecture. 

Metric Transformer LSTM TimeSliver FCN 

Balanced Accuracy 0.68 (0.014) 0.73 (0.04) 0.76 (0.02) 0.77 (0.05) Table 10: Predictive performance on the synthetic far-field interaction dataset. Results are reported as mean (std) accuracy over three runs. 

## E DETAILED PREDICTABILITY RESULTS 

We evaluate TimeSliver against five methodological categories: (1) Distance-based methods, including DTW variants (Bagnall et al., 2016); (2) Dictionary/interval-based methods, such as MUSE (Schäfer & Leser, 2017) and gRFS/CIF (Middlehurst et al., 2020); (3) Feature-based ML 

models like ROCKET (Dempster et al., 2020) and MrSEQL (Karlsson et al., 2016); (4) Deep learning 

models including ResNet (Wang et al., 2017), InceptionTime (Fawaz et al., 2020), FCN (Karim et al., 2017), TS2vec (Yue et al., 2022), TimesNet (Wu et al., 2022), ShapeNet (Li et al., 2021b), RLPAM (Gao et al., 2022b), ShapeConv (Qu et al., 2024b), SBM (Wen et al., 2025a), InterpGN (Wen et al., 2025a); and (5) Ensemble-based approaches such as CBOSS, STC (Bagnall et al., 2016; 2017), RISE, 20 Published as a conference paper at ICLR 2026 TSF, and HC (Lines et al., 2018). The predictive performance of TimeSliver on all 26 UEA datasets, along with the results of the baseline methods, is presented in Table 11. The comparison of 

TimeSliver with baseline methods in terms of average rank, top-1, and top-3 counts is presented in Table 12. 

## F THEORETICALLY DESIRABLE PROPERTIES 

F.1 COMPLETENESS 

To satisfy the completeness axiom, the attribution scores of all temporal segments for class c in input 

x must sum to the change in the predicted logit for class c between x and a baseline xbaseline :

ˆyc(x) − ˆyc(xbaseline ) = 

> κ−1

X

> k=0

ϕk,

where κ is the number of temporal segments and ϕ is the attribution score. In this study, we set 

xbaseline = 0 , so that ˆyc(xbaseline ) = 0 . Hence, we need to show that 

ˆyc(x) = 

> κ−1

X

> k=0

ϕk.

As noted in Section 2.2, TimeSliver transforms P into class logits using just a linear layer ( θc). Thus the predicted logit of class c for input x can be expressed as: 

ˆyc =

> n.ν −1

X

> i=0
> q−1

X

> j=0

wij Pij (9) This implies that 

∂ ˆyc

∂P ij 

= wij (10) Let’s assume that x only consists of positively attributing temporal segments for class c. This implies that • σij = sign (wij ) = +1 for ∀ (i, j ) ∈ { 0, . . . , n.ν − 1} × { 0, . . . , q }.• Zki Qkj >= 0 ∀(i, j, k ) ∈ { 0, . . . , n.ν − 1} × { 0, . . . , q } × { 0, . . . , κ − 1} in Equation 3. Additionally, based on the empirical results presented in Table 13, we can remove max-scaling in Equation 3 by accepting a decrease in AUPRC score by 1.5%. Based on the above conditions, we can rewrite Equation 3 as: 

ζ+ 

> k,ij

(wij ) = wij ×Zki Qkj and ζ− 

> k,ij

(wij ) = 0 (11) Further, the total attribution score of a kth temporal segment can be calculated as: 

ϕ+ 

> k

=

> n.ν −1

X

> i=0
> q−1

X

> j=0

wij ×Zki Qkj (12) Adding the attribution scores of all the temporal segments and using Equation 2 leads to 

> κ−1

X

> k=0

ϕ+ 

> k

=

> n.ν −1

X

> i=0
> q−1

X

> j=0

wij ×

> κ−1

X

> k=0

Zki Qkj =

> n.ν −1

X

> i=0
> q−1

X

> j=0

wij Pij = ˆ yc (13) Equation 13 shows that under the conditions discussed above, TimeSliver satisfies the complete-ness axiom. It is worth noting that this axiom will also be satisfied if there are only negatively attributing segments for class c in x.21 Published as a conference paper at ICLR 2026 

> Problem  DTW_D  DTW_I  DTW_A  MUSE  gRSF  CIF  MrSEQL  ROCKET  CBOSS  STC  RISE  TSF  HC  TapNet  ResNet  Inception Time
> TimeSliver
> ArticularyWordRecognition  98.87  94.31  98.94  98.87  98.21  97.89  98.98  99.56  97.56  97.51  95.73  94.82  97.99  97.13  98.26  99.10  99.33 AtrialFibrillation  23.56  34.67  22.44  74.00  27.56  25.11  36.89  24.89  30.44  31.78  24.44  29.78  29.33  30.22  36.22  22.00  73.00 BasicMotions  95.25  97.17  99.92  100.00  99.83  99.75  94.83  99.00  98.75  97.92  100.00  98.78  100.00  99.08  100.00  100.00  100.00
> Cricket  100.00  95.74  100.00  99.77  97.41  98.38  99.21  100.00  97.55  98.94  97.78  93.15  99.26  97.50  99.40  99.44  98.61 DuckDuckGeese  49.20  29.27  56.67  56.00  44.47  56.00  39.27  46.13  43.07  43.47  50.80  38.87  47.60  58.27  63.20  63.47  56.10 EigenWorms  64.58  44.20  97.85  99.33  83.00  90.33  72.16  86.28  62.80  74.68  81.93  76.62  78.17  83.00  45.45  98.68  89.31 Epilepsy  96.30  67.03  97.37  99.64  97.34  98.38  99.93  99.08  99.83  98.74  99.86  99.83  100.00  96.09  98.16  98.65  98.55 EthanolConcentration  30.15  30.68  29.87  48.64  34.06  72.89  60.18  44.68  39.62  82.36  49.16  45.42  80.68  28.99  28.62  27.92  43.73 ERing  92.91  91.42  92.89  96.89  91.98  95.65  93.19  98.05  84.48  84.28  82.44  89.84  94.26  89.46  87.19  92.10  84.10 FaceDetection  53.28  51.53  – 68.89  55.06  69.17  62.97  69.42  52.32  69.76  51.17  68.95  69.17  52.87  53.13  77.24  69.97 FingerMovements  54.17  55.50  54.93  54.77  54.43  53.90  55.53  55.27  51.03  53.40  52.10  53.17  53.77  51.33  54.70  56.13  65.00
> HandMovementDirection  30.32  26.67  30.72  38.02  32.07  52.21  35.23  44.59  28.87  34.95  28.24  48.51  37.79  32.34  35.32  42.39  46.00 Handwriting  61.21  34.33  60.55  51.85  36.06  35.13  54.04  56.67  49.09  28.77  18.27  36.42  50.41  32.95  59.78  65.74  57.10 Heartbeat  68.88  63.80  69.87  73.59  78.49  76.52  72.52  71.76  72.15  72.15  73.22  72.28  72.18  73.97  63.89  78.62  80.49
> Libras  88.04  78.63  87.85  90.30  75.56  91.67  86.57  90.61  85.26  84.46  81.67  79.72  90.28  83.63  94.11  88.72  83.33 LSST  54.76  49.57  56.96  63.62  58.05  56.17  60.28  63.15  43.62  57.82  50.58  34.31  53.84  46.33  42.94  33.97  68.53
> MotorImagery  56.10  49.63  50.37  52.17  53.80  51.80  53.00  53.13  52.37  50.83  49.83  53.80  52.17  45.37  49.77  51.17  61.90
> NATOPS  82.04  76.07  81.48  87.13  82.37  84.41  86.43  88.54  82.48  84.35  80.59  77.72  82.85  90.30  97.11  96.63  98.89
> PenDigits  99.28  99.22  99.27  98.68  91.27  98.97  97.14  99.56  95.61  97.70  87.47  94.11  97.19  93.65  99.64  99.68  98.10 PEMS-SF  77.05  80.23  78.73  99.85  91.27  99.86  97.15  85.63  96.57  98.40  98.98  96.76  97.98  79.21  81.95  82.83  94.80 PhonemeSpectra  15.39  10.18  – 25.86  15.27  32.87  30.86  28.35  19.43  30.62  26.78  14.52  32.87  22.17  15.39  36.74  29.00 RacketSports  85.64  81.69  85.79  89.56  87.79  89.30  88.73  92.79  88.90  88.09  84.17  88.29  90.64  85.81  91.23  91.69  92.10 SelfRegulationSCP1  81.81  80.63  81.34  73.58  79.74  85.94  82.86  86.55  81.33  84.73  73.17  84.73  86.02  95.68  76.11  84.69  90.00 SelfRegulationSCP2  54.09  48.48  52.43  49.52  50.62  48.87  49.61  51.35  50.62  51.63  50.28  50.62  51.67  56.05  50.24  52.04  62.00
> StandWalkJump  22.00  35.78  25.56  34.67  38.44  45.11  42.00  45.56  36.89  44.00  34.00  33.33  40.67  35.11  30.89  42.00  67.00
> UWaveGestureLibrary  92.28  87.58  91.51  90.39  89.59  92.42  91.32  94.43  86.13  87.03  71.11  85.05  91.31  89.59  88.35  91.23  91.00
> Table 11: Detailed predictive performance comparison across 26 datasets in UEA (supporting results for Section 3.2 in the main paper).

22 Published as a conference paper at ICLR 2026 Table 12: TimeSliver vs. 16 baselines on 26 UEA datasets, evaluated using Average Rank, Top-1 Count, and Top-3 Count. Bold indicates the best result, underlined indicates the second-best. ↑ and ↓

denote that higher and lower values are better, respectively. 

Type Method Average Rank ↓ Top-1 Count ↑ Top-3 Count ↑

Distance-Based DTW_D 16.88 0 0DTW_I 18.65 0 0DTW_A 14.67 1 1Dictionary-Based MUSE 10.50 3 6gRSF 16.88 0 0CIF 11.31 1 3Feature-Based ML MrSEQL 12.88 0 0ROCKET 9.31 4 7Deep Learning TapNet 17.77 1 1ResNet 15.38 1 1IncTime 7.69 4 7FCN 9.92 1 5TS2Vec 16.88 0 0TimesNet 15.92 0 0ShapeNet 12.58 1 2RLPAM 10.23 4 10 ShapeConv 11.19 1 4SBM 11.15 1 3InterpGN 7.08 3 7

Ours TimeSliver 7.00 6 11 

F.2 SENSITIVITY 

To satisfy the sensitivity axiom, any change or perturbation ( δs) to a temporal segment xs = x[t :

t + m] that induces a change in the predicted logit ˆyc must yield a nonzero attribution score for that segment, i.e., 

ˆyc(x)̸ = ˆ yc(x[xs←xs+δs]) =⇒ ϕs̸ = 0 ,

In TimeSliver , the attribution score computation (Equations 3 and 4) involves two key steps after training: • Sensitivity of ˆyc with respect to P . We first estimate gij = ∂ ˆyc 

> ∂P ij

. Although gradient-based methods typically violate sensitivity (Shrikumar et al., 2017), TimeSliver preserves it because ˆyc depends linearly on P (Equation 9). Thus, any perturbation in P that affects ˆyc

necessarily yields gij ̸ = 0 .• Separating positive and negative contributions. In the second step, we use σij =sign( wij ) together with the Zki Qkj terms in Equation 3 to isolate positively and negatively contributing segments via the ReLU operator. Consequently, TimeSliver satisfies sensi-tivity partially : ϕ+ is insensitive to negatively contributing segments (they are zeroed), and 

ϕ− is insensitive to positively contributing segments. F.3 SYMMETRY -P RESERVING 

For a method to be symmetry-preserving, it must assign identical attribution scores ( ϕ+ and ϕ−) to two identical temporal segments in the input sequence. To illustrate that TimeSliver satisfies this property, consider a univariate time-series instance x ∈ RL×1 containing two identical segments 

xs = x[t1 : t1 + m] and x′ 

> s

= x[t2 : t2 + m]. As shown in Equation 3, the attribution score for a segment xi primarily depends on Zi and Qi. Section 2.2.2 establishes that Z depends solely on the one-hot encoding O (Equation 1). Because identical raw segments yield identical one-hot encodings, we have Os = O′

> s

. Likewise, the latent representation qi is obtained from a 1D CNN applied at the 23 Published as a conference paper at ICLR 2026 segment level; therefore, identical segments produce identical latent representations, i.e., qs = q′

> s

.Consequently, both Z and Q are identical for the two segments. By Equation 3, this yields identical attribution scores ζ+ and ζ−, and therefore identical final attributions ϕ+ and ϕ−.

Activation function FreqSum SeqComb-UV SeqComb-MV LOWVAR 

ReLU (with max-scaling) 0.94 ±0.05 0.97 ±0.03 0.94 ±0.01 0.99 ±0.04 

ReLU (w/o max-scaling) 0.91 ±0.06 0.94 ±0.05 0.94 ±0.05 0.99 ±0.01 

Sigmoid 0.91 ±0.07 0.77 ±0.18 0.75 ±0.19 0.99 ±0.02 

Tanh 0.91 ±0.06 0.75 ±0.20 0.70 ±0.17 0.99 ±0.05 

No activation 0.91 ±0.07 0.75 ±0.20 0.70 ±0.17 0.99 ±0.04 

Table 13: Comparison of activation functions across different tasks. 

## G QUALITATIVE ANALYSIS 

Figure 9 presents qualitative results comparing the positive attribution scores computed by 

TimeSliver with the ground truth attribution scores for all synthetic datasets. The strong alignment between TimeSliver ’s attribution scores and the ground truth demonstrates that high attribution scores accurately identify the truly important time points in xi.

Figure 9: Qualitative comparison of temporal attribution scores obtained from TimeSliver with ground truth importance scores on the a) FreqSum, b) SeqComb-UV, c) SeqComb-MV, and d) LOWVAR datasets. 

24