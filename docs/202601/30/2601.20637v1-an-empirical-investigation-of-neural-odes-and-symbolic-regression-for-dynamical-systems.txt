Title: An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems

URL Source: https://arxiv.org/pdf/2601.20637v1

Published Time: Thu, 29 Jan 2026 01:57:08 GMT

Number of Pages: 9

Markdown Content:
# An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems 

Panayiotis Ioannou 

Department of Physics University of Cambridge 

pi225@cam.ac.uk 

Pietro Liò 

Department of Computer Science and Technology University of Cambridge 

pl219@cam.ac.uk 

Pietro Cicuta 

Department of Physics University of Cambridge 

pc245@cam.ac.uk 

Abstract 

Accurately modelling the dynamics of complex systems and discovering their gov-erning differential equations are critical tasks for accelerating scientific discovery. Using noisy, synthetic data from two damped oscillatory systems, we explore the extrapolation capabilities of Neural Ordinary Differential Equations (NODEs) and the ability of Symbolic Regression (SR) to recover the underlying equations. Our study yields three key insights. First, we demonstrate that NODEs can extrapolate effectively to new boundary conditions, provided the resulting trajectories share dynamic similarity with the training data. Second, SR successfully recovers the equations from noisy ground-truth data, though its performance is contingent on the correct selection of input variables. Finally, we find that SR recovers two out of the three governing equations, along with a good approximation for the third, when using data generated by a NODE trained on just 10% of the full simulation. While this last finding highlights an area for future work, our results suggest that using NODEs to enrich limited data and enable symbolic regression to infer physical laws represents a promising new approach for scientific discovery. 

1 Introduction 

Being able to accurately model the dynamics of complex systems or find the differential equations that govern them can help us in understanding those systems better and accelerate scientific discovery. Given the explosion of data and advances in machine learning, a key question arises: how can we best leverage experimental data to infer the dynamics of complex systems? Neural Ordinary Differential Equations (NODEs) [ 1] are a powerful tool for modelling dynamical systems, as their continuous-time nature mirrors that of differential equations. As a result, NODEs have been successfully applied across diverse scientific fields [ 2 , 3]. Much of the existing research has focused on improving NODE architectures [ 4] and assessing their robustness [ 5]. However, a significant gap remains in evaluating their performance under noisy synthetic data or real-world conditions. Specifically, their interpolation and extrapolation capabilities with respect to boundary conditions have been under-explored. This work addresses this gap by systematically investigating NODE performance using noise-infused synthetic data from two dynamical systems that exhibit damped oscillations: the cart-pole [ 6] and a biological model of bacterial adaptation to changing nutrient environments [7]. While NODEs excel at modelling dynamics, their black-box nature limits interpretability. In contrast, Symbolic Regression (SR) [ 8] can discover the exact governing equations; however, it often requires 

Preprint. 

> arXiv:2601.20637v1 [cs.LG] 28 Jan 2026

large datasets, which can be a significant challenge in experimental science. To address this, we investigate a pipeline that uses a trained NODE as a data augmentation tool for SR, assessing its ability to recover ground-truth equations from limited initial data. Our evaluation compares the performance of SR on direct ground-truth data (with and without noise) to its performance on a full dataset generated by a NODE trained on only 10% of the original simulation. 

2 Methods 

This work investigates the extrapolation and interpolation capabilities of NODEs using noisy synthetic data generated from two dynamical systems: the cart-pole [ 6] and a biological system (which we refer to as the Bio-model) [ 7]. We also employ SR to determine if the original equations can be recovered directly from the noisy simulation data, as well as from the output generated by the NODE models. The NODE results were obtained using code adapted from the JAX-based [9] Diffrax library [10]. 

PySR was used for the SR analysis [ 8], and the phaseportrait library [ 11 ] was used to create the phase space plot. All the details to reproduce this work are provided in the remainder of the methods section or in sections A, B and C of the appendix. 

Cart-pole. Equation 1 shows the angle dynamics of the cart-pole system assuming no friction between the cart and the rail [6]: 

d2θ

dt2 = ¨θ = g sin θ + cos θ( −F −mpl ˙θ2 sin θmc+mp ) − μp ˙θmpl

l( 43 − mp cos 2 θmc+mp ) (1) where θ is the angle between the vertical and the pole, ˙θ = dθ 

> dt

, F is the force acting on the cart, l is half of the length of the pole, mp the mass of the pole, mc the mass of the cart, μp is the friction in the articulation between the cart and the pole and g the gravity. Using a forward simulation, equation 1 was solved numerically for many different initial conditions. This was done using the SciPy library, more specifically the integrate.odeint python function. The first data point [ θ0, ω0] was at t = 0. The simulation ran until t = 10 seconds with 0.01 second time-steps. Noise of magnitude -5 to 5% (of the data point value) was added to each point using a uniform distribution. For the results in this report, the simulation was solved using: F = 0, l = 10 cm, mp = 0.1 kg, mc = 1 kg and μp = 0.0008. This corresponds to an unforced cart-pole with friction only between the pole and the cart. Our analysis uses two separate models, each trained on a different set of initial conditions. Model A was trained on 35 boundary conditions, which are all combinations of initial angles from {0,0.6,1.2,1.8,2.4,3.14} and angular speeds from {0,2,4,6,8,10}, excluding the unstable equilib-rium at [0,0]. The training data for Model A consists of 26 points from the first second of simulation for both the angle and angular speed, corresponding to a sampling rate of 25 Hz. The results in Figure 1a were generated by Model B, which was trained exclusively on the set of boundary conditions highlighted by the red box in the figure. Its training data was sampled at the same rate and time points as Model A. 

Bio-model. The system’s evolution is governed by the following ordinary differential equations [ 7 ], where each is presented in both a simplified and an expanded form using only the three state variables (ψA, ϕR, χR) and constants ( ϕRmax , ˜ϵ, k α, A, A 2, τ χ):                                  

> dψA
> dt= ( ϕRmax −ϕR)νf−λ[ψA, ϕ R](1 + ψA) = ( ϕRmax −ϕR)kf−ϕR˜ϵ ψA(ψA+ 1)
> ψA+kα
> (2)
> dϕR
> dt=λ[ψA, ϕ R]( χR−ϕR) = ϕR(χR−ϕR)˜ ϵψA
> ψA+kα
> (3)
> dχR
> dt=1
> τχ
> (ωR[ψA]−χR) = 1
> τχ
> (ψAAψAA+A2
> −χR)(4)

where the simplified form uses λ[ψA, ϕ R] which is a function of ψA and ϕR. The complete model derivation and a comprehensive description of the variables are available in [ 7]. Appendix C provides a brief overview of the variables and constants. The system’s evolution is driven by a shift in nutrient quality, represented by a change in the parameter ν. Each value of ν corresponds to a unique steady state solution for the state variables ( ψA, ϕR, χR). For an example of these values, see Table A1 in the appendix. Experiments begin with the system in a steady state at an initial nutrient quality, νi. At 

t = 0, ν is abruptly changed to a final value, νf , and the system evolves toward a new steady state. This is an up-shift if νf > ν i and a down-shift if νf < ν i. All experiments use a fixed νf = 3.78. 2For the numerical simulations, we solved the model for 8 hours with a 0.01-hour time step and added noise to the data in a way similar to the cart-pole. For Model 2A, the training set consisted of 33 data points per hour over the first 4 hours, generated from νi = 2.22 and 3.465. To generate the results in Figure 3, we trained six models, each with a different sampling rate (5, 10, 20, 33, 50, and 100 data points per hour) using data from the first hour of simulation. These models were trained on 12 distinct shifts (6 up-shifts, 6 down-shifts) with νi values ranging from 0.36 to 7.19 in steps of 0.62. The NODE model for SR was trained on the same 12 shifts, but with a sampling frequency of 10 data points per hour from the first 2 hours of each simulation. 

3 Results and Discussion 

NODE Extrapolation: (A) Cart-Pole. Figure 1a displays a mean squared error (MSE) heatmap for Model B, where each point represents the initial conditions of a time series. As expected, points within the training region (inside the red rectangle) exhibit a low MSE. Surprisingly, we observe low-MSE regions outside these boundaries, with values comparable to the training set. An analysis of the system’s phase space reveals a strong correlation with this phenomenon (Figure 1b). Points on the same phase space trajectory as training data points tend to have a lower MSE. This indicates that NODEs can generalise effectively to regions that share the same dynamical properties as the training set. This finding emphasizes a key insight: when designing a training set for dynamical systems, the goal should be to sample diverse dynamics rather than simply a large number of initial conditions. Figure 2a displays the results of model A on an interpolated, [1.4, 5], initial condition. Trained on data only from the first second, the model accurately captures the dynamics of the system and successfully extrapolates over a duration five times longer than its training period. (B) Bio-model. 

Figure 2b displays the performance of Model 2A, which was trained exclusively on only two up-shift simulations. The model accurately predicts the down-shift ( νi = 5.95 to νf = 3.78) response, with the error for each point being less than 5%, despite having no down-shift data included in the training. 0.0 0.5 1.0 1.5 2.0 2.5 3.0  

> Angle ( rad )
> 0
> 2
> 4
> 6
> 8
> 10
> Angular speed (  rad /s)
> Mean Square Error Heat Map
> 0.00
> 0.01
> 0.02
> 0.03
> 0.04
> 0.05

(a) 0.0 0.5 1.0 1.5 2.0 2.5 3.0   

> Angle (rad )
> 0
> 2
> 4
> 6
> 8
> 10
> Angular speed  (rad / s)
> Cart-Pole. Phase Space

(b) 

Figure 1: The red rectangle in both plots represents the training region. (a) The MSE heatmap for Model B (cart-pole). The low-error zones outside the training region highlight the model’s ability to extrapolate. This occurs because the model learned the underlying dynamics from trajectories within the training set, allowing it to accurately predict other points along the same dynamic path. (b) The cart-pole phase space. The colour represents the magnitude of the angular speed and acceleration. 

The Impact of Training Data Sampling Frequency on Model Performance. Figure 3 shows the MSE for models trained with varying sampling frequencies on only the first hour of data. The most significant finding is the remarkable consistency of the 8-hour MSE, which shows no significant difference across models. This suggests that high-quality long-term predictions can be achieved with a minimal amount of training data. In contrast, the 1-hour MSE for the two lowest sampling frequencies is notably higher. This is a direct result of the extreme sparsity of the training data; with as few as six training points per variable per shift, noise disproportionately affects the model’s fit, leading to a higher interpolation error. The MSE values are averaged over 10 independent runs for each model and across 19 shifts ( νi from 0.98 to 6.57 in steps of 0.31). 

Symbolic Regression. Our SR analysis, presented in Table 1, was conducted on two distinct datasets: ground truth simulation data and data generated by a trained NODE model. First, using ground truth 30 2 4 6 8 10 

> Time (s)
> 15
> 10
> 5
> 0
> 5
> 10
> 15
> Angular speed (  rad /s)
> Extrapolation - Neural ODE Vs Ground Truth
> Ground Truth
> Neural ODE

(a) 0 1 2 3 4 5 6 7 8         

> Time (h)
> 0.08
> 0.10
> 0.12
> 0.14
> 0.16
> 0.18
> 0.20
> 0.22
> R and  R
> Downshift prediction. Rand RVs Time
> Ground Truth R
> Ground Truth R
> Model Prediction R
> Model Prediction R

(b) 

Figure 2: (a) Cart-pole. Model A Time Extrapolation Performance. Model A, trained on data from only the first second, accurately predicts the system’s dynamics for an interpolated initial condition, [1.4, 5]. The model demonstrates strong generalization by successfully extrapolating five times the length of the training data. (b) Bio-model. Results from Model 2A, which was trained solely on two up-shift datasets. The model successfully predicts a down-shift ( νi = 5.95 to νf = 3.78), an event it was never exposed to in training. While small amplitude errors (less than 5%) are observed, the model effectively learns and extrapolates the underlying shift dynamics. data from a single up-shift simulation ( νi = 2.53 to νf = 3.78), we successfully recovered all three target equations when λ was included as an input variable. However, when the analysis was limited to the three main state variables ( ψA, ϕR, χR), only Equation 4 was recovered. One of the primary reasons for this failure is due to the the rational term within λ = 10 .04 · ψAϕR 

> ψA+ka

being effectively masked by the data. With ψA being, on average, 8 times larger than ka, the denominator (ψA + ka)

is dominated by ψA. This makes the rational term ψAϕR 

> ψA+ka

approximately equal to ϕR, hiding the true structure and the constant ka from the SR algorithm. The presence of 5% noise in the ground truth data significantly hindered discovery. As shown in Table 2, SR failed to recover the true structure of Equation (2) , finding only the drastic simplification 

1.410 − λ. The mean of λ over the shift is equal to 1.410 . The target derivatives naturally tend to zero as the system approaches equilibrium, creating a flat fitness landscape; the addition of 5% noise further masks this already-weak signal, making discovery more difficult. Full PySR outputs for this equation and dataset can be found in Table A5 in Appendix D. 5 10 20 33 50 100    

> Sampling Frequency (hour 1)
> 0.4
> 0.6
> 0.8
> 1.0
> 1.2
> 1.4
> 1.6
> 1.8
> Average MSE over different initial conditions  1e 5Average MSE Vs Sampling Frequency
> Up to 1 hour
> Up to 8 hours

Figure 3: Impact of sampling frequency on model performance. The legend shows the time regions for MSE calculation. The 8-hour MSE is consistent across all models, demonstrating that long-term predictions are achievable with sparse data. In contrast, the 1-hour MSE rises sharply for the two lowest sampling frequencies, as the model’s fit is highly sensitive to noise when trained on only a handful of points. Errobars indicate the standard deviation over 10 independent runs. Table 1: Results of our SR analysis. On ground truth data (with λ as input), SR recovered all three governing equations but struggled in the presence of 5% noise, finding only one of the three. On NODE-generated data, SR recovered two of three equations and a good approximation for Equation (2) . Interestingly, it seems that the NODE acts as a denoising filter, enabling better SR performance on the noisy dataset. * The algorithm does not find the correct equation but it finds a decent approxi-mation (see Table 2). 

Equations (with λ)Ground Truth No noise Ground Truth 5% noise 2 ✓ ×

3 ✓ ✓

4 ✓ ×

Equations (with λ)NODE No noise NODE 5% noise 2 ×* ×*3 ✓ ✓

4 ✓ ✓

4Table 2: SR results for Equation (2) . Unlike the other two governing equations (which SR recovered successfully from NODE generated data), this equation’s discovery is sensitive to the data source. While SR finds the true structure from Ground Truth data, it fails on NODE-generated data by omitting one of the low signal terms, and it fails completely on the noisy Ground Truth data. The mean value of λψ A is 0.045. Loss is the MSE between the SR result and the dataset it was trained on.                                                        

> Ground Truth for Equation (2) 2.079 −3.78 ×ϕR−λ−λ×ψA
> Dataset Loss (MSE) Symbolic Regression Result
> Ground Truth (No noise) 2.09 ×10 −82.076 −3.77 ×ϕR−λ−λ×ψA
> Ground Truth (5% noise) 8.12 ×10 −31.410 −λ
> NODE (No noise) 1.91 ×10 −52.024 −3.54 ×ϕR−λ−ψA
> NODE (5% noise) 2.16 ×10 −42.006 −3.67 ×ϕR−λ

SR applied to data from our NODE (trained on ground truth data with or without noise) recovered two of the three governing equations, (3) and (4) . It failed to recover the full form of Equation (2) but nonetheless found good approximations (Table 2). This result also demonstrates the NODE’s ability to act as a denoiser, enabling SR to achieve a better result on noisy data. The SR solution derived from the NODE trained on noisy data fails to identify the −λψ A term. This failure is attributable to a low signal-to-noise ratio: the term’s magnitude ( 0.01 < λψ A < 0.13 , with a mean of 0.045 ) is negligible compared to the equation’s dominant constant, 2.079. Interestingly, PySR compensates for this omitted term by absorbing its mean value into the discovered constant: the algorithm finds 2.006 instead of 2.079. Extended results from some of the SR runs are available in Appendix D. These preliminary findings can be improved, most critically by extending the SR analysis beyond a single-shift simulation to diverse, multi-condition data. Other avenues include improving the NODE model, as its training data was not fully optimised to maximise generalisation in this work, or using more advanced architectures (e.g., Neural CDEs [ 12 ]). The SR search itself could also be strengthened with physical priors (e.g., unit matching) or alternative frameworks like SINDy [13]. 

4 Conclusions and Future Work 

This study investigated the generalization properties of Neural Ordinary Differential Equations (NODEs) and their potential to augment data for symbolic regression (SR). Using noisy, synthetic data from two damped oscillatory systems, we demonstrated that NODEs can learn the underlying dynamics and generalize well to novel initial conditions and time periods, provided the trajectories for the new conditions are dynamically similar to those in the training data. Our results, based on sparse and noisy data, highlight that training on diverse dynamics is crucial for generalization, surpassing the need for dense sampling. We also demonstrated that SR can recover the true governing equations from this noisy data, although its performance is sensitive to the choice of input variables and equation complexity. We tested a pipeline for data-scarce settings: a NODE, trained on only 10% of the data, generated a full dataset for SR. This pipeline successfully recovered two of the three governing equations and found a good approximation for the third. This pipeline is promising, as these outcomes can be improved with future work, such as implementing techniques like unit matching within the SR framework, using more augmented data or exploring other SR frameworks. Ultimately, this research lays the groundwork for a promising scientific discovery tool, particularly for data-scarce domains, by using NODEs to enrich limited experimental data and enable symbolic regression to infer underlying physical laws. 

Acknowledgments and Disclosure of Funding 

P.I. was supported by the Engineering and Physical Sciences Research Council Centre for Doctoral Training in Sensor Technologies for a Healthy and Sustainable Future [EP/S023046/1]. We would like to thank Rossana Droghetti and Marco Cosentino Lagomarsino for helpful discussions regarding the Bio-model. Large language models, such as Gemini and ChatGPT, were used to enhance clarity and improve language use. 5References 

[1] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud, “Neural Ordinary Differential Equations,” in Advances in Neural Information Processing Systems (S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, eds.), vol. 31, Curran Associates, Inc., 2018. [2] G. D. Portwood, P. P. Mitra, M. D. Ribeiro, T. M. Nguyen, B. T. Nadiga, J. A. Saenz, M. Chertkov, A. Garg, A. Anandkumar, A. Dengel, et al. , “Turbulence forecasting via neural ODE,” arXiv preprint arXiv:1911.05180 , 2019. [3] J. Lu, K. Deng, X. Zhang, G. Liu, and Y. Guan, “Neural-ODE for pharmacokinetics modeling and its advantage to alternative machine learning models in predicting new dosing regimens,” 

Iscience , vol. 24, no. 7, 2021. [4] T. Zhang, Z. Yao, A. Gholami, J. E. Gonzalez, K. Keutzer, M. W. Mahoney, and G. Biros, “ANODEV2: A coupled neural ODE framework,” Advances in Neural Information Processing Systems , vol. 32, 2019. [5] H. Yan, J. Du, V. Y. Tan, and J. Feng, “On robustness of neural ordinary differential equations,” 

arXiv preprint arXiv:1910.05513 , 2019. [6] R. V. Florian, “Correct equations for the dynamics of the cart-pole system,” Center for Cognitive and Neural Studies (Coneural), Romania , vol. 63, 2007. [7] R. Droghetti, P. Fuchs, I. Iuliani, V. Firmano, G. Tallarico, L. Calabrese, J. Grilli, B. Sclavi, L. Ciandrini, and M. Cosentino Lagomarsino, “Incoherent feedback from coupled amino acids and ribosome pools generates damped oscillations in growing E. coli,” Nature communications ,vol. 16, no. 1, p. 3063, 2025. [8] M. Cranmer, “Interpretable machine learning for science with PySR and SymbolicRegression. jl,” arXiv preprint arXiv:2305.01582 , 2023. [9] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang, “JAX: composable transformations of Python+NumPy programs,” 2018. [10] P. Kidger, On Neural Differential Equations . PhD thesis, University of Oxford, 2021. [11] H. L. Victor and F. L. Unai, “Make phase portraits in 2d and 3d in Python.” https://github. com/phaseportrait/phaseportrait .[12] P. Kidger, J. Morrill, J. Foster, and T. Lyons, “Neural controlled differential equations for irregular time series,” in Advances in Neural Information Processing Systems (H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, eds.), vol. 33, pp. 6696–6707, Curran Associates, Inc., 2020. [13] S. L. Brunton, J. L. Proctor, and J. N. Kutz, “Discovering governing equations from data by sparse identification of nonlinear dynamical systems,” Proceedings of the National Academy of Sciences , vol. 113, no. 15, pp. 3932–3937, 2016. 6Appendix 

The Appendix includes: (A) details on the training methodology for the Neural ODE models; (B) a description of the setup and parameters used for the symbolic regression (SR) analysis; (C) additional details on the Bio-model, including definitions of key variables, equations, and constants; and (D) the SR outputs from PySR using the data generated from the NODE model that was trained on the ground truth data with 5% noise and for equation 2 also the noisy ground truth data. 

A Neural ODE Training Details 

All models were trained using a standard multi-layer perceptron architecture with two hidden layers, each containing 20 nodes. The Adabelief optimizer from the optax library was used for all training runs. 

Cart-Pole System. For the cart-pole experiments, all models were trained for 100,000 iterations with a learning rate of 0.003. Model A utilized a batch size of 16, whereas Model B used a batch size of 20. 

Bio-model. For the biological system models, Model 2A was trained for 100,000 iterations with a learning rate of 0.001 and a batch size of 1. The Models used to generate the results in Figure 3 were trained for 100,000 iterations with a learning rate of 0.003 and a batch size of 10. To generate the data for the SR analysis, the Neural Ordinary Differential Equation (NODE) models were trained for 150,000 iterations with a learning rate of 0.003 and a batch size of 10. To help prevent convergence to a local minimum, all Bio-model training included an initial 500 iterations using only the first 10% of the data and a learning rate of 0.003. 

B Symbolic Regression Details 

We employed the PySR library (v1.5.9) to perform symbolic regression and identify the governing equations of the system. The search space for candidate equations included a standard set of primary binary operations (addition, subtraction, multiplication, division) and the unary inverse operator, 

inv(x)=1/x . We ran the search for 800 iterations with a population size of 50, setting the maximum equation size to 25 and constant complexity to 2. To serve as the target values for the symbolic regression, the time derivatives of the three state variables ( ψA, ϕR, χR) were calculated from the data using the NumPy gradient function. To improve data fidelity, we truncated the time series by removing the first 10 datapoints (0.1 h) and the final 100 datapoints (1.0 h). This preprocessing mitigates two issues: (1) high numerical error from the finite-difference gradient at the initial points, and (2) uninformative, steady-state data, which introduces a low signal-to-noise ratio that can hinder the SR search. While this approach provided a sufficient approximation, using a more robust gradient estimation technique could further improve the accuracy of our symbolic regression results. 

C Bio-model: Additional information 

A complete description of the model, along with all variables, constants, and the full derivation, is available in [ 7 ]. The model, described by equations 2, 3, and 4 , simulates the adaptive behaviour of 

E. coli bacteria in response to changing nutrient conditions. The system is defined by three key state variables: • ψA: The ratio of free amino acid mass to the total protein mass within the bacteria. • ϕR: The mass fraction of the proteome dedicated to all ribosome-related proteins. • χR: The gene regulatory function, which quantifies the fraction of protein synthesis allocated for ribosome production. The bacterial growth rate, λ, is a key parameter defined by the relationship λ = ϵϕ R. Here, ϵ

represents the translation rate, and is given by the function ϵ = ˜ ϵ ψA 

> ψA+ka

. The constant ka = 0.005 is the amino acid uptake efficiency, and ˜ϵ = 10.04 h −1 is the theoretical maximum elongation rate. The 7system’s behaviour is governed by the nutrient quality parameter, νf , which is solely dependent on the potency of the post-shift nutrient. In this work, we set νf = 3.78 h −1. In the steady state, where the time derivatives are zero, each value of ν corresponds to two solutions for the state variables ( ψA,

ϕR, χR). One of these solutions yields a negative value for ψA, while the other is always positive. Since ψA represents a physical quantity and must therefore be non-negative, the negative solution is not biologically viable. Consequently, there exists only one unique and physically meaningful steady-state solution for each value of ν, which also corresponds to a unique growth rate, λ. Examples of these values are presented in Table A1. The RNA Polymerase allocation on ribosomal genes, ωR, is given by the equation: ωR[ψa] = KG

> KG+[ G]

where [G] is the (p)ppGpp, guanosine tetraphosphate or pentaphosphate, concentration which is given by [G] = CG Ref ( ψa+ka 

> ψa

− 1) . In this formulation, we define the shorthand variables A = KG and 

A2 = CG Ref ∗ ka. For this work, the constants are set to the following values: C = 4.6, GRef =101.46 μM, KG = 14.5 μM, ϕRmax = 0.55 and τx = 1/6 h. Note that the values for the constants KG and GRef differ from those presented in the original reference [ 7 ]. The referenced paper used values from newer experimental data, such as KG = 8.07 μM and GRef = 55.7 μM. Despite these differences, the core results and conclusions of this work remain unchanged. Table A1: Steady-state solutions and their corresponding growth rates. The table presents the unique, biologically viable steady-state values for the state variables ( ψA, ϕR, χR) and their associated growth rates, with each solution determined by a specific value of the parameter ν.At Steady State 

ν growth rate (λ) ψa ϕR and χR

2.53 1.05 0.0233 0.127 3.78 1.41 0.0314 0.163 5.95 1.92 0.0436 0.213 

D Symbolic Regression (PySR) outputs 

The following tables detail the PySR discovery results using data from the NODE model that was trained on ground truth data with 5% noise. Tables A2, A3, and A4 show the outputs for equations (2) , (3) , and (4) , respectively. Table A5 shows the PySR discovery results using the ground truth data with the 5% noise. For clarity, we display equations up to a complexity of 20. The reported 

Loss is the MSE between a discovered equation and the input dataset to the SR, not the ground truth. An asterisk (*) indicates the equation with the highest score, representing PySR ’s optimal trade-off between accuracy and complexity. Bold highlights the equation whose structure is most similar to the true governing equation. Table A2: PySR results for Equation (2) (NODE, 5% noise data). Bold highlights the structure most similar to the true governing equation; * is PySR’s highest score.                                                                                                                                                                 

> Complexity Loss (MSE) Score Equation
> 16.296 ×10 −30.000 ψA
> 24.548 ×10 −30.325 −0.0053729056
> 33.280 ×10 −30.327 ϕR−χR
> 41.109 ×10 −31.084* 1.4028348 −λ
> 66.885 ×10 −40.238 (1 .5672859 −ϕR)−λ
> 84.006 ×10 −40.271 ((1 .7317353 −ϕR)−λ)−ϕR
> 92.160 ×10 −40.618 2.0058749 −(λ+ ( ϕR×3.6670127))
> 11 9.418 ×10 −50.415 (2 .1844785 −(λ+ ( ϕR×4.556726))) −ψA
> 13 6.408 ×10 −50.193 1.8036835 −(((( ϕR×ϕR)×13 .594807) + λ) + ψA)
> 14 2.118 ×10 −51.107 (( ϕR× − 5.776956) −(λ+ ( χR+−2.515695))) ×0.69243747
> 16 1.523 ×10 −50.165 ((( −5.5431213 + χR)×ϕR)−(( χR+−2.4505582) + λ)) ×0.7387968
> 17 1.449 ×10 −50.050 (( ϕR× − 5.272122) −((( χR×0.789321) + −2.398444) + λ)) ×0.74833924
> 18 1.252 ×10 −50.146 (( ϕR×(χR+−5.5318727)) −(( χR+−2.4487023) + λ)) ×(0 .7895749 −ψA)
> 19 1.092 ×10 −50.137 (( ϕR× − 5.1778607) −(λ+ ( −2.376086 + ( χR×0.7470051)))) ×(0 .810564 −ψA)
> ... ... ... ...

8Table A3: PySR results for Equation (3) (NODE, 5% noise data). Bold highlights the structure most similar to the true governing equation; * is PySR’s highest score. 

Complexity Loss (MSE) Score Equation 

1 2.042 × 10 −3 0.000 ψA

3 1.761 × 10 −4 1.225 χR − ϕR

5 3.123 × 10 −5 0.865 (χR − ϕR) × λ

6 2.494 × 10 −5 0.225 (ϕR − χR)/ − 0.7338049 

7 2.412 × 10 −5 0.034 (λ − ϕR) × (χR − ϕR)

8 9.552 × 10 −6 0.926 (χR − ϕR)/(0 .9726795 − χR)

9 9.493 × 10 −6 0.006 (ϕR × − 1.3376828) + ( χR × 1.3615563) 

10 7.311 × 10 −6 0.261 (χR × ψA) − (( ϕR − χR) × 1.2554784) 

11 2.109 × 10 −6 1.243* (( −1.1330749 − χR) × (ϕR − χR)) − − 0.0028907147 

13 1.591 × 10 −6 0.141 (−1.1011528 − χR) × (ϕR + (( ψA × − 0.07154481) − χR)) 

15 6.273 × 10 −7 0.465 (ϕR × − 1.1626418) + ( χR × ((( λ × ψA) × λ) − − 1.1181651)) 

16 6.104 × 10 −7 0.027 (ϕR − (χR × (ψA + 0 .98330754))) × (−0.6877947 − (λ × 0.3857772)) 

17 3.750 × 10 −7 0.487 (ϕR × − 1.1737428) + ((( ψA × (λ × (λ − ψA))) − − 1.1311564) × χR)

18 1.933 × 10 −7 0.663 (χR × (((( λ × λ) + −0.37667426) × ψA) − − 1.1579369)) + ( ϕR × − 1.1897844) 

19 1.800 × 10 −7 0.071 (((( χR × (λ × λ)) − ψA) × ψA) + ( ϕR × − 1.1864315)) − (−1.1489322 × χR)

... ... ... ... 

Table A4: PySR results for Equation (4) (NODE, 5% noise data). Bold highlights the structure most similar to the true governing equation; * is PySR’s highest score. 

Complexity Loss (MSE) Score Equation 

1 3.26 × 10 −2 0.000 ψA

3 3.25 × 10 −2 0.001 χR − ϕR

4 3.13 × 10 −2 0.036 ψA + −0.034593984 

5 1.78 × 10 −2 0.568 (ψA/ϕ R) − χR

7 4.25 × 10 −3 0.715 (ϕR × − 21 .217772) + 3 .486964 

9 2.91 × 10 −3 0.190 (( ψA/(ψA + ϕR)) − χR)/χ R

10 7.57 × 10 −4 1.345 (( ψA/(ψA + ϕR)) − χR) × 4.9153194 

11 9.29 × 10 −5 2.098* (( ψA/(ψA + 0 .16037777)) − χR) × 5.927795 

13 4.36 × 10 −5 0.378 (( ψA/(ψA + 0 .16027561)) − χR) × (4 .554471 + λ)

15 2.95 × 10 −5 0.196 (( λ + λ) + 3 .1795135) × (( ψA/(ψA + 0 .16015515)) − χR)

16 2.93 × 10 −5 0.005 ((( ψA/(ψA + 0 .1601735)) − χR) × 1.8685215) × (λ + 1 .7985661) 

17 2.21 × 10 −5 0.282 (λ + (( λ − χR) + 3 .3858838)) × (( ψA/(ψA + 0 .16015515)) − χR)

18 1.54 × 10 −5 0.364 ((( ψA/(ψA + 0 .16009086)) − χR) × 2.5824206) × (λ + (1 .127972 − χR)) 

20 1.51 × 10 −5 0.010 (( ψA/(ψA + 0 .16009125)) − χR) × (((0 .9849129 + ( λ − χR)) × 2.8481526) − χR)

... ... ... ... 

Table A5: PySR results for Equation (2) (Ground Truth, 5% noise data). Bold highlights the structure most similar to the true governing equation; * is PySR’s highest score. 

Complexity Loss Score Equation 

1 1.185 × 10 −2 0.000 ψA

2 1.018 × 10 −2 0.152 −0.0044986876 

3 9.006 × 10 −3 0.122 ϕR − χR

4 7.639 × 10 −3 0.165* 1.4092975 − λ

6 7.289 × 10 −3 0.023 0.23274253 − (χR × λ)

7 6.872 × 10 −3 0.059 (( ψA/χ R) − χR) − ψA

8 6.736 × 10 −3 0.020 ψA + (0 .19986038 − (λ × χR)) 

9 5.970 × 10 −3 0.121 (ψA − (χR × (χR + ψA))) /ϕ R

10 5.926 × 10 −3 0.007 (ψA − (( χR × χR) × 1.1969346)) /ϕ R

11 5.817 × 10 −3 0.019 (ψA − (χR × (χR + ( χR × χR)))) /ϕ R

12 5.734 × 10 −3 0.014 (ψA − (λ × (( χR × 0.83034015) × χR))) /ϕ R

14 5.609 × 10 −3 0.011 (ψA − ((( χR + ψA) × λ)/(1 .460721 /χ R))) /ϕ R

15 5.608 × 10 −3 0.000 ((( ψA − (χR × (χR × λ))) /ϕ R) × 0.7639032) − − 0.03282497 

16 5.562 × 10 −3 0.008 ((( χR × (( λ × λ) × (−0.09359126 /ϕ R))) × χR) + ψA)/ϕ R

17 5.547 × 10 −3 0.003 (((( ψA − (( χR × λ) × χR)) × 0.1317986) /ϕ R) − − 0.0053570317) /ϕ R

18 5.519 × 10 −3 0.005 (( ψA + ( λ × (( λ × (( −0.080160506 /ϕ R) × χR)) × χR))) /ϕ R) − ψA

19 5.472 × 10 −3 0.009 (ψA + ( χR × ((( λ × (λ × − 0.027622199)) /ϕ R) + (0 .30923742 − χR)))) /ϕ R

... ... ... ... 

9