Title: Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning

URL Source: https://arxiv.org/pdf/2601.14693v1

Published Time: Thu, 22 Jan 2026 01:28:42 GMT

Number of Pages: 9

Markdown Content:
# Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning 

## Jianwen Sun 1,2 , Xinrui Li 1,2 , Fuqing Li 3, Xiaoxuan Shen 1,2* 

> 1

Faculty of Artificial Intelligence in Education, Central China Normal University, Wuhan 430079, China 

> 2

National Engineering Research Center for Educational Big Data, Central China Normal University, Wuhan 430079, China 

> 3

Institute of Collaborative Innovation, University of Macau, Macau 999078, China 

Abstract 

Symbolic Regression aims to automatically identify compact and interpretable mathematical expressions that model the functional relationship between input and output variables. Most existing search-based symbolic regression methods typ-ically rely on the fitting error to inform the search process. However, in the vast expression space, numerous candidate expressions may exhibit similar error values while differing substantially in structure, leading to ambiguous search direc-tions and hindering convergence to the underlying true func-tion. To address this challenge, we propose a novel frame-work named EGRL-SR (Experience-driven Goal-conditioned Reinforcement Learning for Symbolic Regression). In con-trast to traditional error-driven approaches, EGRL-SR intro-duces a new perspective: leveraging precise historical trajec-tories and optimizing the action-value network to proactively guide the search process, thereby achieving a more robust expression search. Specifically, we formulate symbolic re-gression as a goal-conditioned reinforcement learning prob-lem and incorporate hindsight experience replay, allowing the action-value network to generalize common mapping pat-terns from diverse input-output pairs. Moreover, we design an all-point satisfaction binary reward function that encour-ages the action-value network to focus on structural patterns rather than low-error expressions, and concurrently propose a structure-guided heuristic exploration strategy to enhance search diversity and space coverage. Experiments on public benchmarks show that EGRL-SR consistently outperforms state-of-the-art methods in recovery rate and robustness, and can recover more complex expressions under the same search budget. Ablation results validate that the action-value net-work effectively guides the search, with both the reward func-tion and the exploration strategy playing critical roles. 

## Introduction 

Symbolic Regression (SR) aims to discover compact and in-terpretable mathematical expressions that capture the under-lying functional relationships between input and output vari-ables. Given a noise-free dataset D = {(xi, y i)}Ni=1 , where 

xi denotes the input feature and yi the target value, the ob-jective of SR is to identify a function f in a concise closed-form analytical expression such that yi = f (xi) holds for every i ∈ { 1, . . . , N }. Unlike traditional regression meth-ods that rely on pre-specified functional forms, SR conducts 

> *

Corresponding author. 

flexible search in a vast space of symbolic expressions with-out requiring prior assumptions, making it widely applicable in scientific discovery and engineering modeling. However, as the length of expressions increases, the search space ex-pands exponentially, and this problem has been proven to be NP-hard (Virgolin and Pissis 2022). In recent years, advances in neural networks for sequence modeling have encouraged researchers to develop end-to-end methods for expression generation(Valipour et al. 2021; Kamienny et al. 2022; Biggio et al. 2021; Li et al. 2022). While these approaches improve fitting accuracy through large-scale pretraining, the expression space is virtually un-bounded, making it difficult to learn a stable and general-izable distribution. Consequently, the generated expressions often exhibit structurally implausible forms and contain re-dundant symbols. Therefore, in the task of accurately recovering com-plex expressions, researchers increasingly favor search-based symbolic regression methods that incorporate adap-tive mechanisms. Representative approaches include: Ge-netic Programming, which employs the fitting error as a fitness function to evolve expression structures via evo-lutionary operators (Koza 1994; Virgolin, Alderliesten, and Bosman 2019; Burlacu, Kronberger, and Kommenda 2020a); the Equation Learner method, which encodes ex-pressions using special neural networks and jointly opti-mizes both their structure and parameters by minimizing the fitting error (Martius and Lampert 2016; Sahoo, Lampert, and Martius 2018); Monte Carlo Tree Search, each node rep-resents a symbolic expression in prefix form, and the algo-rithm prioritizes exploring branches that are expected to re-duce the fitting error (Sun et al. 2022; Li et al. 2025); and Reinforcement Learning approaches, which formulate ex-pression generation as a sequential decision-making process where a policy network constructs expressions guided by re-ward signals derived from fitting accuracy (Petersen et al. 2019; Mundhenk et al. 2021). In summary, most existing search-based symbolic regres-sion methods rely heavily on fitting error to guide the search direction, yet this strategy exhibits clear limitations in com-plex expression spaces. As illustrated in Figure 2, within the interval [−1, 1] , many structurally distinct expressions can achieve low fitting error, while exhibiting drastically differ-ent behaviors outside the interval. Optimizing solely for er-

> arXiv:2601.14693v1 [cs.LG] 21 Jan 2026

ror often leads to divergent search trajectories without clear direction, and once the search veers off course, the algorithm struggles to backtrack or recover. Consequently, the final ex-pressions tend to fall into one of two undesirable extremes: either they are concise but insufficient accuracy, or exces-sively complex in structure to compensate for error—both of which violate the fundamental objectives of symbolic re-gression. To overcome the limitations of existing symbolic regres-sion methods that overly rely on fitting error for search guidance, we propose a novel framework—Experience-driven Goal-conditioned Reinforcement Learning for Symbolic Regression (EGRL-SR). Built upon Goal-Conditioned Reinforcement Learning (GCRL) (Schaul et al. 2015) and enhanced with Hindsight Experience Replay (HER)(Andrychowicz et al. 2017), EGRL-SR differs fundamentally from error-driven approaches by learning from historical construction trajectories. It generalizes x − y

transformation patterns across different targets y to improve expression generation and actively guide the search process. By focusing on reusable structural patterns instead of local optima tied to specific targets, the method effectively mitigates the pitfalls of fitting-error-based optimization. In this formulation, each input-output pair is treated as a goal-reaching task, where x serves as the starting state and 

y as the intended goal. Furthermore, we design a binary reward function based on All-Point Satisfaction (APSR) to encourage the action-value network to focus on learning consistent x − y mapping pat-terns. This design helps prevent structurally different expres-sions with similar error levels from receiving high rewards simultaneously, which could otherwise misguide the search direction or scatter the optimization trajectory. In addition, we propose a Structure-Guided Heuristic Ex-ploration (SGHE) strategy, which partitions the search space based on expression structures and allocates independent value networks to distinct structural subspaces, enabling more thorough and efficient exploration across structurally diverse regions. The main contributions of this work are as follows: 1. To the best of our knowledge, we propose the first sym-bolic regression framework based on Goal-Conditioned Reinforcement Learning (GCRL), termed EGRL-SR, which integrates Hindsight Experience Replay to over-come the limitations of error-driven search by introduc-ing a novel paradigm guided by pattern induction. 2. To tackle the ambiguity of error-based reward signals and the imbalance in structural search, we introduce two novel components: the All-Point Satisfaction binary re-ward function (APSR) and the Structure-Guided Heuris-tic Exploration strategy (SGHE). 3. Experimental results demonstrate that the proposed method achieves higher recovery rates and greater ro-bustness than existing approaches across multiple public datasets, particularly exhibiting a superior ability to re-cover more complex expressions under the same search budget. Further ablation studies validate the contribu-tions of the action-value network, the reward function, and the exploration strategy. 

## Related work 

GP-based symbolic regression methods 

In the early development of symbolic regression, Genetic Programming (GP) was one of the dominant approaches, as proposed in Koza (Koza 1994). The core procedure in-volves initializing a population of expression trees, evolving their structures through crossover and mutation, and select-ing high-quality individuals for the next generation based on the fitting error. To improve the search efficiency of traditional GP, var-ious enhancement strategies have been proposed (Burlacu, Kronberger, and Kommenda 2020b; Cranmer 2023). For in-stance, Virgolin et al. introduced the concept of semantic backpropagation(Virgolin, Alderliesten, and Bosman 2019), which guides the evolution of expressions by propagating er-rors. In a separate work (Virgolin et al. 2021), they proposed a method that evaluates the cooperative relationships among substructures based on error, organizes them into structural groups, and applies localized replacements during crossover to preserve beneficial components and accelerate conver-gence. 

EQL-based symbolic regression methods 

Differentiable network-based symbolic regression methods construct multi-layer architectures from predefined opera-tors(Martius and Lampert 2016; Sahoo, Lampert, and Mar-tius 2018), and jointly optimize both the structure and pa-rameters by minimizing the fitting error via backpropaga-tion. However, these approaches often lack explicit con-straints on expression simplicity, making them prone to pro-ducing overly complex or redundant models. To mitigate this, researchers have proposed various regularization and pruning strategies(Montazerin et al. 2025; Wu et al. 2024). For example, Tsoi et al.(Tsoi et al. 2025) proposed a dy-namic sparse training method that assigns learnable sparsity thresholds to weights, features, and operators to enable prun-ing and structural optimization. Li et al. (Li et al. 2023), on the other hand, introduced a reinforcement learning-based approach to dynamically adjust the network architec-ture, thereby progressively refining the expression genera-tion process. 

MCTS-based Symbolic Regression methods 

Sun et al.(Sun et al. 2022) proposed a symbolic regres-sion framework based on Monte Carlo Tree Search (MCTS), where a reward function balancing fitting accuracy and spar-sity is used to guide the search. However, due to the reliance on random strategies during the expansion and simulation phases, the overall search efficiency remains low. To address this limitation, Kamienny et al. (Kamienny et al. 2023) and Li et al. (Li et al. 2025) proposed train-ing neural networks to predict the fitness potential of candi-date expressions, thereby directing MCTS to expand toward lower-error regions and refining the networks through feed-back from the search outcomes. Shojaee et al.(Shojaee et al. 2023) further embedded MCTS into the decoding stage of a Transformer model and introduced non-differentiable feed-back—such as symbolic complexity penalties—to optimize expression generation, enhancing the model’s capacity for global structural planning. 

RL-based Symbolic Regression methods 

Symbolic regression tasks are typically limited to input-output pairs without ground-truth expression labels, which renders gradient-based optimization of the generation pol-icy infeasible. To overcome this challenge, reinforcement learning has been increasingly adopted, where fitting error is leveraged as environmental feedback to guide expression generation. A representative approach is Deep Symbolic Re-gression (DSR) (Petersen et al. 2019), which employs an RNN-based policy network to construct expressions in a pre-order fashion. At each step, the current partial expression serves as the state, the next symbol selection is treated as the action, and the reward is defined as the inverse of the fitting error. DSR primarily relies on sampling from the policy net-work to encourage diversity during exploration, which limits its ability to effectively cover complex expression spaces. To improve global coverage of the symbolic expres-sion space, researchers have proposed hybrid approaches that combine reinforcement learning with genetic pro-gramming(Mundhenk et al. 2021), or introduced interac-tive platforms that integrate human feedback(Crochepierre, Boudjeloud-Assala, and Barbesant 2022; Kim, Kim, and Pe-tersen 2021) to compensate for the limitations of purely policy-driven sampling. 

## Methods 

This chapter is organized as follows: we begin by formal-izing the symbolic regression task as a Goal-Conditioned Markov Decision Process (GC-MDP), explicitly defining the state space, action space, and reward function. We then describe the key components of our framework, including the Hindsight Experience Replay (HER) , the optimiza-tion algorithm for the action-value network (Double Duel-ing DQN), and the Structure-Guided Heuristic Exploration (SGHE) strategy. 

Goal-Conditioned Markov Decision Process 

As illustrated in Figure 1, symbolic regression training is framed as an agent–environment interaction process. At each step, the agent follows an ϵ-greedy strategy for ac-tion selection: with probability 1 − ϵ, it exploits prior ex-perience by choosing actions based on the value network; with probability ϵ, it explores the expression space using the Structure-Guided Heuristic Exploration (SGHE) strat-egy. The environment then returns a new state and evaluates whether the target has been reached: if so, the trajectory is marked as successful; if not and the expression length re-mains within the threshold, the agent proceeds to generate further tokens; if the threshold is reached, the failed trajec-tory is reinterpreted as successful via Hindsight Experience Replay (HER). The agent subsequently samples trajectories from the replay buffer to train the value network using a re-inforcement learning algorithm. The complete workflow of the algorithm is presented in Appendix A. 

State The state s captures the agent’s numerical progress during the expression construction process. It is defined as the concatenation of the current intermediate numeric output (denoted as xnow ) and the target output y, i.e., s = xnow ∥y.For single-variable tasks, the initial state is s0 = x∥y; for multi-variable tasks, s0 = 0 ∥y. When the state reaches y∥y,it indicates that the agent has successfully constructed an expression whose numeric evaluation perfectly matches the target y. The sequence of actions along this trajectory con-stitutes the postfix representation of the corresponding ex-pression. 

Action The action space comprises variables, unary oper-ators, and binary operators. At each time step, the agent can choose a variable operate to generate a new node, apply a unary operator to transform an existing node, or use a binary operator to combine two isolated nodes. We adopt a postfix generation strategy, where variables and operands are placed before operators. This design enables the agent to observe its intermediate numeric output at each step and adapt the gen-eration path dynamically based on its proximity to the target. 

Reward In this study, we propose a sparse binary re-ward mechanism called the All-Point Satisfaction Reward (APSR). Under this scheme, the agent receives a reward of 1 only if the constructed expression meets a predefined ac-curacy threshold across all input samples; otherwise, the re-ward is 0. The APSR is designed to overcome two fundamental lim-itations of conventional continuous reward functions based on fitting error. First, a low fitting error does not necessar-ily imply structural correctness—expressions that are struc-turally flawed but highly expressive may still achieve low errors, leading the policy to converge to suboptimal solu-tions. Second, structurally different expressions may yield similar error values, making it difficult for error-based re-wards to distinguish them effectively. This lack of structural guidance hampers policy optimization and poses challenges to stable convergence. 

Hindsight Experience Replay 

In actual search processes, trajectories are guided solely by the current target y, and the replay buffer lacks success-ful experiences associated with alternative targets y′. This hinders the action-value network from learning generaliz-able construction patterns across multiple goals, thus lim-iting its generalization capacity. To address this issue, we introduce Hindsight Experience Replay (HER), which re-places the original target y with an intermediate output actu-ally reached by the agent, thereby yielding successful train-ing samples and providing positive reward signals. Specifically, HER selects several intermediate outputs from each failed trajectory as new goals, recalculates the as-sociated rewards, and adds the relabeled samples to the ex-perience buffer for subsequent learning by the action-value network. This allows the network to encounter a diverse set not success &    

> achieved lenthresh
> not success &
> not achieved lenthresh Value Network Exploration Policy

Agent  Environment  

> update state
> compute reward
> check if done
> ε-greedy exploration strategy
> trajectory
> relabeled experience trajectory
> & original experience trajectory

Replay Buffer  Hindsight Experience Replay                                                               

> relabel failed trajectory
> into successful one
> Start=x=[0.6,0.4,…,0.3]
> Goal=y=[ -0.71, -0.85,…, -0.91] Target Expression Found
> Output:
> state
> [0.35,0.16,…,0.09]
> Double Dueling DQN
> Input ：()
> 222222
> [()( ) 1, ()( ) 1, ][()( ) ,()( ) ,][()( ) ,( ) ()( ) ,]1, ..,
> tttttttt
> ssin x cos x action in sin x cos x rssin x cos x xx action in sin x cos x xx r ssin x cos x x action i in sin x cos x x r tlen formula
> −−++=
> ‖‖‖2

( ( ) )sin x cos x x x +( )sin x xFigure 1: Training framework of EGRL-SR based on agent–environment interaction of goal-conditioned construction paths during training, ex-tract generalizable x − y mapping patterns, and ultimately improve its performance in reconstructing the current target. A detailed description of the algorithm can be found in Ap-pendix A. 

Reinforcement Learning Algorithm — Double Dueling DQN 

Since HER violates the assumption that training data must be generated by the current policy, it is incompatible with on-policy algorithms(Williams 1992; Schulman et al. 2017). To address this, we adopt the off-policy Double Dueling DQN(Mnih et al. 2013; Van Hasselt, Guez, and Silver 2016; Wang et al. 2016), which can effectively leverage both his-torical trajectories and relabeled experiences. Double Dueling DQN enhances Q-value estimation by in-troducing a decomposition into a state-value function V (s)

and an advantage function A(s, a ), which allows the model to more accurately differentiate between state value and action-specific advantage: 

Q(s, a ) = V (s) + A(s, a ) − 1

|A| 

X

> a′

A(s, a ′)

!

(1) In addition, the algorithm employs a double-network archi-tecture that separates action selection from value evaluation, effectively mitigating the overestimation bias common in traditional Q-learning. The target value is computed as: 

y = r + γQ target (s′, arg max  

> a′

Q(s′, a ′)) (2) The network is trained by minimizing the following loss: 

L(θ) = Es,a 

h

(y − Q(s, a ; θ)) 2i

(3) The target network parameters are updated via soft up-dates, gradually tracking the online network with a small step size. 

Structure-Guided Heuristic Exploration Strategy 

Given that EGRL-SR employs an off-policy reinforcement learning algorithm capable of flexibly incorporating external trajectories, we propose a Structure-Guided Heuristic Ex-ploration (SGHE) strategy. SGHE partitions the expression space based on multiple structural attributes, establishing a set of independent exploration directions. Each direction is equipped with its own action-value network and dedicated replay buffer to enable targeted and structurally aware learn-ing. This strategy promotes balanced exploration across struc-turally diverse expressions, mitigating the bias toward low-error expressions confined to specific structural patterns. It facilitates the discovery of target expressions that are struc-turally simple yet exhibit hard-to-model x − y mappings. In contrast to on-policy reinforcement learning-based symbolic regression methods, EGRL-SR supports external structural intervention and directional control over the ex-ploration process, enabling a tight coupling between sym-bolic expression exploration and learning. A detailed theo-retical analysis is provided in Appendix B. Specifically, each exploration direction is defined by a combination of three structural attributes: the number of unary operators, the nesting depth, and the length range of the sub-expression to which the unary operator is applied. With probability ϵ, the agent adheres to the predefined structural constraints—proceeding to generate variable and binary operator nodes, and applying the unary operator only when the sub-expression reaches the required length. -25                                                          

> -5
> 15
> 35
> 55
> -3 -2 -1 0123
> f(x)
> x
> PySR[MSE=0.000017]:
> GOMEA[MSE=0.00073]:
> DSO[MSE=0.00039]:
> DSR[MSE=0.00017]:
> Target Function & EGRL-SR[MSE=0.0]: 32sin( ) sin( ) xxxxxx++++()232
> 0.443 0.443 cos( 1.436) ( ) 0.003 xxxxxxsin x x+++−++−32
> 1.91168 sin( ) xxxx+++sin( )
> 2x
> x e 322
> sin( ) sin( )xxxxx++++

Figure 2: Function plots of the target ex-pression and the corresponding expressions generated by SR algorithms (EGRL-SR, DSR, DSO, GOMEA, PySR). Twenty train-ing points are uniformly sampled from the interval [−1, 1] , and the mean squared error (MSE) quantifies the fitting accuracy within this interval. To assess the generalization ca-pability beyond the original input range, the plotting range is extended to [−3, 3] .0         

> 0.2
> 0.4
> 0.6
> 0.8
> 00.02 0.04 0.06 0.08 0.1
> Recovery
> Noise level
> EGRL-SR DSR DSO PySR GOMEA

Figure 3: Expression recovery rates of EGRL-SR and baseline meth-ods under different noise levels. The recovery rates of Pretraining-based methods (NeSymReS, E2E), Monte Carlo tree search method (TPSR), and Equation Learner methods (EQL-Div, DySymNet) remain at 0%. 0.0             

> 0.5
> 1.0
> 1.5
> 2.0
> 2.5
> 00.2 0.4 0.6 0.8 1
> TtS
> Exploration level
> N3 L1 M1 M2 L2 L3 M3 L15
> ×10 5

Figure 4: Average TtS for similar-length expressions with different structural complexities under varying 

ϵ values. Expressions starting with “N” and “L” come from the Nguyen and Livermore datasets, respectively, with the number indicating their index. “M”-prefixed expressions are manually designed to increase struc-tural diversity at the similar length. 

Expression length (number) EGRL-SR DSR PySR GOMEA DSO TPSR NeSym ReS End to End EQL-Div DySym Net 

≤ 8(4) 100% (0%) 96%(9%) 100%(0%) 40%(49%) 98%(4%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 9–10(4) 100%(0%) 100%(0%) 100%(0%) 50%(58%) 77%(28%) 0%(0%) 17%(19%) 0%(0%) 0%(0%) 0%(0%) 11–12(4) 100%(0%) 77%(32%) 46%(37%) 11%(21%) 25%(18%) 0%(0%) 0% (0%) 0%(0%) 0%(0%) 0%(0%) 13–14(6) 100%(0%) 61%(49%) 47%(39%) 26%(33%) 29%(38%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 15–16(10) 88%(17%) 29%(42%) 30%(36%) 8%(21%) 16%(33%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 17–20(4) 56%(38%) 11%(13%) 48%(41%) 11%(21%) 2%(4%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 21–30(5) 58%(50%) 23%(43%) 0%(0%) 0%(0%) 17%(37%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 

≥ 31 (5) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 0%(0%) 

Table 1: Average exact recovery rates of EGRL-SR and baseline methods on all expressions within each expression length in-terval. The first column indicates the expression length interval (with the number of expressions in each interval in parentheses). The remaining columns present the average exact recovery rates (with standard deviation in parentheses) for each algorithm on all expressions within the corresponding interval. With probability 1 − ϵ, the agent selects the next action purely based on the value network, unconstrained by any external structural heuristics, thereby making full use of ac-cumulated experience. 

## Experiments 

This paper investigates the following five research questions through comparative and ablation studies: RQ1: Given the same search budget, can EGRL-SR reli-ably recover expressions that are typically difficult for exist-ing symbolic regression methods to discover? RQ2: Can EGRL-SR accurately recover expressions un-der noisy conditions by consistently identifying the underly-ing x − y mapping patterns? RQ3: Can the value network trained from experience ef-fectively guide the expression search process? RQ4: Can SGHE improve coverage of the structural search space and facilitate the discovery of structurally sim-ple expressions with hard-to-learn x − y mappings? RQ5: Can APSR prevent the value network from prema-turely converging to suboptimal expressions due to struc-turally incorrect formulas with similarly low error? 

Dataset 

To systematically evaluate the recovery performance of the proposed method across varying structural patterns and lev-els of complexity, we adopt three widely used symbolic regression benchmarks: Nguyen(Uy et al. 2011), Liver-more(Mundhenk et al. 2021), and Keijzer(Keijzer 2003). To stratify the recovery difficulty, all target expressions from these datasets are consolidated and ranked by increasing expression length. Details regarding the expression forms, lengths, sources, and corresponding input sampling ranges contained in the dataset can be found in Appendix C. Target Expressions for Ablation study To assess the crit-ical contributions of the action-value network, exploration strategy, and reward mechanism in guiding expression con-struction, we conduct ablation experiments on eight tar-get expressions that share similar lengths but differ sig-nificantly in structural complexity (as shown in table 2). Among them are three synthetically constructed expressions designed to increase structural complexity. By standardizing expression length, we control for variations in search space size—ensuring that any failure in recovery can be attributed to limitations in structural pattern recognition rather than the confounding effect of a large search space. 

Comparison Algorithms 

To comprehensively assess the performance of our pro-posed method, we compare it against a diverse set of state-of-the-art symbolic regression baselines, spanning the major methodological paradigms: Reinforcement Learning (DSR)(Petersen et al. 2019), Pretraining-based approaches (NeSymReS, E2E)(Biggio et al. 2021; Kamienny et al. 2022), Genetic Programming (GOMEA, PySR)(Virgolin et al. 2021; Cranmer 2023), Equation Learner method (EQL-Div, DySymNet)(Sahoo, Lampert, and Martius 2018; Li et al. 2023), Monte Carlo Tree Search (TPSR)(Shojaee et al. 2023), and RL-GP hybrid method (DSO)(Mundhenk et al. 2021). Details of each baseline are provided in Appendix D. 

Evaluation Metric 

While many studies adopt R2 as the primary evaluation met-ric, this often incentivizes the generation of overly complex expressions that boost accuracy through structural complex-ity rather than genuine model effectiveness. To address this issue, we instead adopt the exact recovery rate of the target expression—including constants—as the evaluation metric. A unified search budget of 1.6 million steps is allocated to each algorithm (corresponding to 8 search directions with 200,000 steps each), and each target expression is evaluated over 12 independent trials. All algorithms adopt a unified op-erator set {+, −, ∗, /, sin, cos, exp, log } for expression re-covery. 

Expression Recovery Rate Evaluation - RQ1 

To evaluate the recovery performance and comparative ad-vantage of the proposed method across expressions of vary-ing complexity, we conduct a series of systematic compara-tive experiments. Details on hyperparameter settings and the computational environment are provided in Appendix E. As shown in table 1, the proposed method exhibits strong recovery performance across most expression length ranges, with its overall average recovery rate consistently exceeding those of the baseline methods. Figure 2 illustrates the expressions generated by the five best-performing algorithms, along with their corresponding function plots, when the target expression is x31 + x21 + x1 +sin( x21) + sin( x1). It can be observed that only EGRL-SR successfully recovers the exact expression, while the expres-sions identified by the other methods—despite achieving very low fitting errors—exhibit structural deviations from the ground-truth target. The results indicate that EGRL-SR, by leveraging struc-tural guidance and experience-driven mechanisms, effec-tively mitigates the bias caused by error-dominated search and enhances the model’s ability to identify underlying x−y

mappings, thereby improving expression recovery perfor-mance. In contrast, other search-based symbolic regression methods tend to overly rely on fitting error, making them susceptible to structurally flawed expressions with decep-tively low fitting errors that represent local optima. The exact recovery rates of symbolic regression meth-ods based on Pretraining, Monte Carlo Tree Search, and Equation Learner method remain close to zero across differ-ent levels of expression complexity. This is likely because pretraining-based approaches rely on synthetic datasets to train expression-generating networks; however, due to the virtually unbounded nature of the expression space, the training data cannot adequately capture structural diversity, limiting the model’s ability to learn generalizable symbolic construction rules. MCTS-based methods, which rely on pretrained networks during node expansion, suffer from sim-ilar distributional mismatch. The suboptimal performance of the Equation Learner method may stem from the absence of effective structural pruning strategies and regularization mechanisms. Per-expression recovery results for all algo-rithms are detailed in Appendix F. 

Robustness to Noise - RQ2 

In real-world applications, observational data often contain noise, and thus the goal of symbolic regression is to re-cover an underlying generalizable function from such per-turbed data. To evaluate whether EGRL-SR remains capable of identifying the core x − y mapping patterns under noisy target outputs, we conducted a systematic noise experiment. Specifically, we added independent Gaussian noise to the dependent variable in the training data, with zero mean and a standard deviation proportional to the root mean square of 

y, gradually increasing the noise level from 0 to 10 −1. The datasets and search budget are consistent with those used in the recovery rate experiments. As illustrated in Figure 3, EGRL-SR consistently achieves superior recovery rates across all noise levels in the expres-sion recovery experiments. Reinforcement learning-based methods such as DSR and DSO maintain relatively sta-ble performance, with recovery rates declining gradually as noise increases—indicating a degree of robustness. How-ever, their overall performance remains inferior to that of EGRL-SR. In contrast, genetic programming methods like PySR and GOMEA suffer a sharp drop in performance upon the introduction of noise, with recovery rates approaching 0%. This is likely because they rely heavily on fitting error as the optimization signal, making them prone to overfitting noisy data. 

Ablation experiment 

Effectiveness of Action-Value Network in Guiding Search - RQ3 To evaluate the search guidance capability of the action-value network, we conducted a controlled ex-periment by introducing perturbations to the ϵ-greedy explo-ration strategy. If the network provides effective guidance, Target Expression HER APSR SGHE HER APSR RE HER nRR SGHE nRR SGHE                                                                     

> N3 : x51+x41+x31+x21+x1100% 100% 100% 100%
> L1 : sin( x21) + x1+13100% 75% 83% 8%
> M1 : sin[( x1−1) 2] + x21100% 8% 100% 100%
> M2 : log( x51+ 2 x31+x1)100% 0% 83% 58%
> L2 : sin( x21) cos x1−2100% 67% 100% 100%
> L3 : sin( x31) cos( x31)−1100% 25% 67% 42%
> M3 : sin( x21+ 1) cos( x21) + x1100% 0% 33% 33%
> L15 : x1/51=exp  15log (x1)83% 0% 50% 8%

Table 2: Exact recovery rates for similar-length expressions with different structural complexities under four configura-tions: (a) the full method (HER + APSR + SGHE); (b) re-placing the structure-guided heuristic exploration (SGHE) with random exploration (HER + APSR + RE); (c) replac-ing APSR with a continuous reward based on normalized RMSE (HER + nRR + SGHE); (d) further removing HER from configuration c (nRR + SGHE). The expression indices (L1, L2, etc.) used in this table correspond to those defined in Figure 4. a lower ϵ value should result in a reduced average num-ber of Trajectories-to-Success (TtS). Specifically, TtS refers to the average number of searches—within the exploration direction where the target expression is successfully recov-ered—required by the action-value network to recover the expression. A smaller TtS indicates higher search efficiency. As shown in Figure 4, the TtS decreases markedly as ϵ de-creases from 1, with the lowest value observed around ϵ=0.2 or 0.4. This trend indicates that the increasing guidance pro-vided by the value network leads to improved search effi-ciency, thereby verifying its essential role in directing the search process. In addition, Figure 4 shows that polynomial expressions (pink line) are the most readily recovered, whereas target expressions involving two unary operators applied to long subexpressions (green and dark orange lines) are the most difficult to recover. This suggests that even when the solu-tion space size is fixed, different expression structures pose significantly different challenges for search and learning. Given the high structural diversity of expressions, nearly ev-ery structure type may contain expressions with low fitting error. If the search is guided solely by fitting error, the algo-rithm tends to favor expressions with lower error in its pre-ferred structures, thereby overlooking the structurally cor-rect ground-truth expressions. 

Evaluation of Structural Exploration Coverage - RQ4 

To evaluate whether SGHE can improve structural space coverage to discover target expressions that are structurally simple yet challenging to recover due to complex x − y

mapping patterns, we conduct a comparative analysis of the recovery performance of SGHE versus random exploration across structurally diverse expressions. As illustrated in table 2, given equal expression lengths, the random exploration strategy can only recover expres-sions with specific structural patterns, suggesting that the action-value network is biased toward certain structures and tends to be trapped in local optima in the absence of struc-tural guidance. 

Evaluation of Reward Function Robustness Against Mis-guidance - RQ5 To evaluate whether the proposed All-Point Satisfaction Reward (APSR) can guide the policy to-ward learning structurally diverse patterns and mitigate the tendency of the model to converge on structurally incorrect yet low-error expressions, we conducted two sets of compar-ative experiments. The first experiment replaces APSR with a continuous re-ward function based on the normalized root mean square error (nRMSE), formulated as r = 11+ nRMSE . The second experiment further removes HER on top of this setup to more closely align with the conventional error-driven search paradigm. As shown in table 2, replacing APSR with reward based on nRMSE leads to a marked decline in overall recovery performance. This decline is likely due to structurally in-correct expressions with low fitting errors receiving reward signals comparable to those of correct trajectories generated by HER, thereby misleading the optimization of the value network. Upon the further removal of HER, performance deterio-rates even more significantly. This is likely because the ab-sence of goal reconstruction deprives the agent of informa-tive trajectories, making it difficult for the value network to capture the mapping patterns between x and diverse target outputs y′. As a result, training is more easily biased by mis-leading error feedback. A few expressions remain unaffected, possibly because the structural subspace defined by SGHE inherently contains few low-error expressions within the given value range. 

## Conclusion and Future Work 

This study introduces a novel symbolic regression frame-work grounded in goal-conditioned reinforcement learning, referred to as EGRL-SR, which establishes a new search paradigm guided by structural pattern induction. However, the current approach represents constants indirectly via com-binations of variables and operators, which substantially in-creases the search space when handling expressions with complex constant terms, thereby impairing algorithm per-formance. Future work will focus on developing effec-tive strategies for recovering expressions involving complex constants within the GCRL framework. 

## References 

Andrychowicz, M.; Wolski, F.; Ray, A.; Schneider, J.; Fong, R.; Welinder, P.; McGrew, B.; Tobin, J.; Pieter Abbeel, O.; and Zaremba, W. 2017. Hindsight experience replay. Ad-vances in neural information processing systems , 30. Biggio, L.; Bendinelli, T.; Neitz, A.; Lucchi, A.; and Paras-candolo, G. 2021. Neural symbolic regression that scales. In International Conference on Machine Learning , 936–945. Pmlr. Burlacu, B.; Kronberger, G.; and Kommenda, M. 2020a. Operon C++ an efficient genetic programming framework for symbolic regression. In Proceedings of the 2020 ge-netic and evolutionary computation conference companion ,1562–1570. Burlacu, B.; Kronberger, G.; and Kommenda, M. 2020b. Operon C++ an efficient genetic programming framework for symbolic regression. In Proceedings of the 2020 ge-netic and evolutionary computation conference companion ,1562–1570. Cranmer, M. 2023. Interpretable machine learning for sci-ence with PySR and SymbolicRegression. jl. arXiv preprint arXiv:2305.01582 .Crochepierre, L.; Boudjeloud-Assala, L.; and Barbesant, V. 2022. Interactive Reinforcement Learning for Symbolic Re-gression from Multi-Format Human-Preference Feedbacks. In IJCAI , 5900–5903. Kamienny, P.-A.; d’Ascoli, S.; Lample, G.; and Charton, F. 2022. End-to-end symbolic regression with transform-ers. Advances in Neural Information Processing Systems ,35: 10269–10281. Kamienny, P.-A.; Lample, G.; Lamprier, S.; and Virgolin, M. 2023. Deep generative symbolic regression with monte-carlo-tree-search. In International Conference on Machine Learning , 15655–15668. PMLR. Keijzer, M. 2003. Improving symbolic regression with inter-val arithmetic and linear scaling. In European Conference on Genetic Programming , 70–82. Springer. Kim, J. T.; Kim, S.; and Petersen, B. K. 2021. An interac-tive visualization platform for deep symbolic regression. In 

Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence ,5261–5263. Koza, J. R. 1994. Genetic programming as a means for programming computers by natural selection. Statistics and computing , 4(2): 87–112. Li, W.; Li, W.; Sun, L.; Wu, M.; Yu, L.; Liu, J.; Li, Y.; and Tian, S. 2022. Transformer-based model for symbolic re-gression via joint supervised learning. In The Eleventh In-ternational Conference on Learning Representations .Li, W.; Li, W.; Yu, L.; Wu, M.; Sun, L.; Liu, J.; Li, Y.; Wei, S.; Deng, Y.; and Hao, M. 2023. A neural-guided dynamic symbolic network for exploring mathematical expressions from data. arXiv preprint arXiv:2309.13705 .Li, Y.; Li, W.; Yu, L.; Wu, M.; Liu, J.; Li, W.; and Hao, M. 2025. Discovering mathematical formulas from data via gpt-guided monte carlo tree search. Expert Systems with Ap-plications , 127591. Martius, G.; and Lampert, C. H. 2016. Extrapolation and learning equations. arXiv preprint arXiv:1610.02995 .Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Play-ing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 .Montazerin, M.; Aawar, M. A.; Ortega, A.; and Srivas-tava, A. 2025. Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression. arXiv preprint arXiv:2506.08267 .Mundhenk, T. N.; Landajuela, M.; Glatt, R.; Santiago, C. P.; Faissol, D. M.; and Petersen, B. K. 2021. Symbolic re-gression via neural-guided genetic programming population seeding. arXiv preprint arXiv:2111.00053 .Petersen, B. K.; Landajuela, M.; Mundhenk, T. N.; San-tiago, C. P.; Kim, S. K.; and Kim, J. T. 2019. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. arXiv preprint arXiv:1912.04871 .Sahoo, S.; Lampert, C.; and Martius, G. 2018. Learning equations for extrapolation and control. In International Conference on Machine Learning , 4442–4450. Pmlr. Schaul, T.; Horgan, D.; Gregor, K.; and Silver, D. 2015. Uni-versal value function approximators. In International con-ference on machine learning , 1312–1320. PMLR. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal policy optimization algorithms. 

arXiv preprint arXiv:1707.06347 .Shojaee, P.; Meidani, K.; Barati Farimani, A.; and Reddy, C. 2023. Transformer-based planning for symbolic regres-sion. Advances in Neural Information Processing Systems ,36: 45907–45919. Sun, F.; Liu, Y.; Wang, J.-X.; and Sun, H. 2022. Symbolic physics learner: Discovering governing equations via monte carlo tree search. arXiv preprint arXiv:2205.13134 .Tsoi, H. F.; Loncar, V.; Dasu, S.; and Harris, P. 2025. Sym-bolNet: neural symbolic regression with adaptive dynamic pruning for compression. Machine Learning: Science and Technology , 6(1): 015021. Uy, N. Q.; Hoai, N. X.; O’Neill, M.; McKay, R. I.; and Galv´ an-L´ opez, E. 2011. Semantically-based crossover in genetic programming: application to real-valued symbolic regression. Genetic Programming and Evolvable Machines ,12(2): 91–119. Valipour, M.; You, B.; Panju, M.; and Ghodsi, A. 2021. Symbolicgpt: A generative transformer model for symbolic regression. arXiv preprint arXiv:2106.14131 .Van Hasselt, H.; Guez, A.; and Silver, D. 2016. Deep rein-forcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence , volume 30. Virgolin, M.; Alderliesten, T.; and Bosman, P. A. 2019. Lin-ear scaling with and within semantic backpropagation-based genetic programming for symbolic regression. In Proceed-ings of the genetic and evolutionary computation confer-ence , 1084–1092. Virgolin, M.; Alderliesten, T.; Witteveen, C.; and Bosman, P. A. 2021. Improving model-based genetic programming for symbolic regression of small expressions. Evolutionary computation , 29(2): 211–237. Virgolin, M.; and Pissis, S. P. 2022. Symbolic regression is NP-hard. arXiv preprint arXiv:2207.01018 .Wang, Z.; Schaul, T.; Hessel, M.; Hasselt, H.; Lanctot, M.; and Freitas, N. 2016. Dueling network architectures for deep reinforcement learning. In International conference on ma-chine learning , 1995–2003. PMLR. Williams, R. J. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Ma-chine learning , 8(3): 229–256. Wu, M.; Li, W.; Yu, L.; Li, W.; Liu, J.; Li, Y.; and Hao, M. 2024. PruneSymNet: A symbolic neural network and pruning algorithm for symbolic regression. arXiv preprint arXiv:2401.15103 .