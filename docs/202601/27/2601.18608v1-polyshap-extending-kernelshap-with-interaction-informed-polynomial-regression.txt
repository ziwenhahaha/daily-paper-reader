Title: PolySHAP: Extending KernelSHAP with Interaction-Informed Polynomial Regression

URL Source: https://arxiv.org/pdf/2601.18608v1

Published Time: Tue, 27 Jan 2026 02:56:51 GMT

Number of Pages: 31

Markdown Content:
# POLY SHAP: E XTENDING KERNEL SHAP WITH INTERACTION -INFORMED POLYNOMIAL REGRESSION 

Fabian Fumagalli 

Bielefeld University 

ffumagalli@techfak.de 

R. Teal Witter 

Claremont McKenna College 

rtealwitter@cmc.edu 

Christopher Musco 

New York University 

cmusco@nyu.edu 

ABSTRACT 

Shapley values have emerged as a central game-theoretic tool in explainable AI (XAI). However, computing Shapley values exactly requires 2d game evaluations for a model with d features. Lund-berg and Leeâ€™s KernelSHAP algorithm has emerged as a leading method for avoiding this exponen-tial cost. KernelSHAP approximates Shapley values by approximating the game as a linear function, which is fit using a small number of game evaluations for random feature subsets. In this work, we extend KernelSHAP by approximating the game via higher degree polynomials, which capture non-linear interactions between features. Our resulting PolySHAP method yields empirically better Shapley value estimates for various benchmark datasets, and we prove that these estimates are consistent. Moreover, we connect our approach to paired sampling (antithetic sampling), a ubiquitous modi-fication to KernelSHAP that improves empirical accuracy. We prove that paired sampling outputs 

exactly the same Shapley value approximations as second-order PolySHAP, without ever fitting a degree 2 polynomial. To the best of our knowledge, this finding provides the first strong theoretical justification for the excellent practical performance of the paired sampling heuristic. 

1 INTRODUCTION 

Understanding the contribution of individual features to a modelâ€™s prediction is a central goal in explainable artificial intelligence (XAI) (Covert & Lee, 2021). Among the most influential approaches are those grounded in cooperative game theory, where the Shapley value (Shapley, 1953) provides a principled way to distribute a modelâ€™s output to its 

d inputs. The intuition behind the use of Shapley values is to attribute larger values to the players of a cooperative game with the most effect on the gameâ€™s value. In XAI applications, players are typically features or training data points and the game value is typically a prediction or model loss. Formally, we represent a cooperative game involving players D = {1, . . . , d } via a value function Î½ : 2 D â†’ R that maps subsets of players to values ( 2D denotes the powerset of D). Shapley values are then defined 1 via the best linear approximation to the game Î½. Concretely, for a subset S âŠ† D, Î½(S) is approximated by a linear function in the binary features 1[i âˆˆ S] for i âˆˆ D. The Shapley values are the coefficients of the linear approximation minimizing a specific weighted â„“2 loss: 

Ï•SV [Î½] := arg min 

> Ï•âˆˆRd:âŸ¨Ï•,1âŸ©=Î½(D)

X

> SâŠ†D

Î¼(S) Î½(S) âˆ’

> d

X

> i=1

Ï•i1[i âˆˆ S]

!2

,

where the non-negative Shapley weights Î¼(S) are given in Equation (2). The constraint that the Shapley values sum to 

Î½(D) enforces what is known as the â€œefficiency propertyâ€, one of four axiomatic properties that motivate the original definition of Shapley values (see e.g, Molnar (2024)). Since the sum above involves 2d terms, exact minimization of the linear approximation to obtain Ï•SV [Î½] is infeasible for most practical games. Over the past several years, substantial research has focused on making the computation of Shapley values feasible in practice (Covert et al., 2020; Covert & Lee, 2021; Mitchell et al., 2022; Musco & Witter, 

> 1

Without loss of generality, we assume Î½(âˆ…) = 0 . Otherwise, we could consider the centered game Î½(S) âˆ’ Î½(âˆ…) which has the same Shapley values. 

1

> arXiv:2601.18608v1 [cs.AI] 26 Jan 2026 ð“ "!"

## â‰ˆ

## â‰ˆ

> ð“ "

ð“ " ! 

> "#
> Game ðœˆ
> ðœˆ (ð‘† #)
> ðœˆ (ð‘† $)
> ðœˆ (ð‘† %)

KernelSHAP 

PolySHAP 

Sampling Extract SV  

> Interactions III

! ð“ # $

|ð‘‡ |   

> $âˆˆ&âˆª(( :!âˆˆ((
> ð“ "!"
> Linear approximatio n
> ð“ "
> Polynomial approximatio n

Figure 1: Both KernelSHAP and PolySHAP fit a function to approximate a sample of game evaluations. While KernelSHAP uses a linear approximation, PolySHAP uses a more expressive polynomial approximation. Finally, both algorithms return the Shapley values (SV) of their respective approximations (trivial for KernelSHAP, see Theorem 4.3 for PolySHAP). 2025; Witter et al., 2025), with KernelSHAP (Lundberg & Lee, 2017) emerging as one of the most widely used model-agnostic methods. From the least squares definition of Shapley values, KernelSHAP can be viewed as a two step process: First, approx-imate the game Î½ with a linear function fit from a sample of game evaluations Î½(S) on randomly selected subsets S.Second, return the Shapley values of the approximation, which, for linear functions, are simply the coefficients of each input. A natural idea is to adapt this framework to fit Î½ with a richer function class that still admits fast Shapley value computation. One such class is tree-based models like XGBoost, which Witter et al. (2025) recently leveraged to approximate the game. When the tree-based approximation is accurate, their Regression MSR estimator produces more accurate Shapley value estimates than KernelSHAP. Butler et al. (2025) also use tree-based models to learn an approximation to the game, but extract Fourier coefficients which can be used to estimate more general attribution values. In the small sample regime with budgets less than 5n, their Proxy SPEX estimator outperforms Kernel SHAP but achieves comparable performance to KernelSHAP for higher budgets. In this work, we introduce an alterative approach called PolySHAP, where we approximate Î½ via a higher degree polynomial in the features 1[i âˆˆ S] for i âˆˆ D, illustrated in Figure 1. For a degree k polynomial, let dâ€² = O(dk)

be the number of terms. We show that, after fitting an approximation with m samples, we can recover the Shapley values of the approximation in just O(dd â€²) time. Across various experiments, we find that higher degree PolySHAP approximations result in more accurate Shapley value estimates (see e.g., Figure 2). Moreover, we prove that the PolySHAP estimates are consistent, concretely that we obtain the Shapley values exactly as m goes to 2d. This is in contrast to RegressionMSR, which needs an additional â€œregression adjustmentâ€ step to obtain a consistent estimator for tree-based approximations (Witter et al., 2025). As a second main contribution of our work, we provide theoretical grounding for a seemingly unrelated sampling strategy called paired sampling , which is known to significantly improve the accuracy of KernelSHAP estimates (Covert & Lee, 2021; Mitchell et al., 2022; Olsen et al., 2024). In paired sampling, subsets are sampled in paired complements S and D \ S.While used in all state-of-the-art Shapley value estimators, the reason for paired samplingâ€™s superior performance is not well understood. Surprisingly, we prove that KernelSHAP with paired sampling outputs 

exactly the same Shapley value approximations as second-order PolySHAP without ever fitting a degree 2 polynomial .This theoretical finding generalizes a very recent result of Mayer & WÂ¨ uthrich (2025), who showed that KernelSHAP with paired sampling exactly recovers Shapley values when the game has interactions of at most degree 2. Because 2the second-order PolySHAP will exactly fit a degree 2 game, their result follows immediately from a special case of ours. However, our finding is more general because it explains why paired sampling is effective for all games, not just those with at most degree 2 interactions. 

Contributions. The main contributions of our work can be summarized as follows: â€¢ We propose PolySHAP , an extension of KernelSHAP that models higher-order interaction terms to approx-imate Î½, and prove it returns the Shapley values as the number of samples m goes to 2d (Theorem 4.3). Moreover, we empirically show that PolySHAP results in more accurate Shapley value estimates than Ker-nelSHAP and Permutation sampling. â€¢ We establish a theoretical equivalence between paired KernelSHAP and second-order PolySHAP (Theo-rem 5.1), thereby explaining the practical benefits of paired sampling. 

2 RELATED WORK 

KernelSHAP Sampling Strategies. Prior work on improving KernelSHAP has focused on refining the subset sam-pling procedure, aiming to reduce variance and improve computational efficiency (Kelodjou et al., 2024; Olsen & Jullum, 2024; Musco & Witter, 2025). Among these enhancements, paired sampling produces the largest improve-ment in accuracy Covert & Lee (2021), yet, until the present work, it was not understood why beyond limited special cases. Another notable enhancement is in the sampling distribution. While it is intuitive to sample subsets propor-tional to their Shapley weights (Equation 2), it turns out that sampling proportional to the leverage scores can be more effective (Musco & Witter, 2025). Paired sampling has also been observed to improve LeverageSHAP (KernelSHAP with leverage score sampling). 

Other Shapley Value Estimators. Beyond the regression-based approach of KernelSHAP, prior Shapley value esti-mators are generally based on direct Monte Carlo approximation Kwon & Zou (2022a); Castro et al. (2009); Kwon & Zou (2022b); Kolpaczki et al. (2024); Li & Yu (2024). These methods estimate the ith Shapley value based on the following equivalent definition: 

Ï•SV  

> i

[Î½] = 1

d

X

> SâŠ†D\{ i}

Î½(S âˆª { i}) âˆ’ Î½(S)

 dâˆ’1

> |S|

 . (1) Permutation sampling, where subsets are sampled from a permutation, is a particularly effective approach (Castro et al., 2009; Mitchell et al., 2022). However, in direct Monte Carlo methods, each game evaluation is used to estimate at most two Shapley values. MSR methods reuse game evaluations in the estimate of every Shapley value, but at the cost of higher variance (Li & Yu, 2024; Witter et al., 2025). A recent benchmark finds that RegressionMSR with tree-based approximations, LeverageSHAP, KernelSHAP, and Permutation sampling are the most accurate (Witter et al., 2025). 

Higher-Order Explanations. Another line of work seeks to improve approximations by explicitly modeling higher-order interactions. kADD -SHAP (Pelegrina et al., 2023) solves a least-squares problem over all interactions up to order 

k, and converges to the Shapley value for k = 2 and k = 3 (Pelegrina et al., 2025). With our results, we simplify 

kADD -SHAP and prove general convergence, where the practical differences are discussed in Section A.4. Relatedly, Mohammadi et al. (2025) propose a regularized least squares method based on the MÂ¨ obius transform (Rota, 1964), which converges only when all higher-order interactions are included. By contrast, PolySHAP converges for any chosen set of interaction terms. Beyond approximation of the Shapley value, Kang et al. (2024) leverage the Fourier representation of games to detect and quantify higher-order interactions. 

3 PRELIMINARIES ON EXPLAINABLE AI AND COOPERATIVE GAMES 

Notation. We use boldface letters to denote vectors, e.g., x, with entries xi, and the corresponding random variable 

Ëœx. The all-one vector is denoted by 1, and âŸ¨Â· , Â·âŸ© is the standard inner product. Given the prediction of a machine learning model f : Rd â†’ R, post-hoc feature-based explanations aim to quantify the contribution of features D to the model output. Such explanations are defined by (i) the choice of an explanation game Î½ : 2 D â†’ R and (ii) a game-theoretic attribution measure, such as the Shapley value (Covert et al., 2021). For a given instance x âˆˆ Rd, the local explanation game Î½x describes the modelâ€™s prediction when restricted to subsets of features, with the remaining features replaced through perturbation. The perturbation is carried out using different imputation strategies, as summarized in Table 1. 3Table 1: Local explanation games Î½x for instance x.

Method Game Î½x(S) Î½x(âˆ…)

Baseline Î½(b) 

> x

f (xS , bD\S ) f (b)

Marginal Î½(m) 

> x

E[f (xS , ËœxD\S )] E[f ( Ëœ x)] 

Conditional Î½(c) 

> x

E[f ( Ëœ x) | ËœxS = xS ] E[f ( Ëœ x)] 

Similarly, global explanation games are con-structed from Î½x by evaluating measures such as variance or risk (Fumagalli et al., 2025). Beyond analyzing features, other variants have been proposed, for instance to characterize properties of individual data points (Ghorbani & Zou, 2019). Like most Shapley value estimators (except e.g., TreeSHAP (Lundberg et al., 2018)), PolySHAP is agnostic to how the game Î½ is defined. 

KernelSHAP. Given a budget of m game evaluations, KernelSHAP solves the approximate least least squares prob-lem: 

Ë†Ï•SV [Î½] := arg min 

> Ï•âˆˆRd:âŸ¨Ï•,1âŸ©=Î½(D)
> m

X

> â„“=1

Î¼(Sâ„“)

p(Sâ„“) Î½(Sâ„“) âˆ’

> d

X

> i=1

Ï•i1[i âˆˆ Sâ„“]

!2

with S1, . . . , S m âˆ¼ p

where the Shapley weight, for subset S âŠ† D is given by 

Î¼(S) := 1

  dâˆ’2

> |S|âˆ’ 1

 if 0 < |S| < d and 0 otherwise. (2) While effective, KernelSHAP is inherently limited to a linear (additive) approximation of Î½ based on the sampled coalitions. 

4 INTERACTION -I NFORMED APPROXIMATION OF SHAPLEY VALUES 

4.1 POLY SHAP I NTERACTION REPRESENTATION 

We introduce PolySHAP, a method for producing Shapley value estimates from a polynomial approximation of Î½. Let the interaction frontier I be a subset of interaction terms 

I âŠ† { T âŠ† D : |T | â‰¥ 2}.

We then extend the linear approximation of Î½ by defining an interaction-based polynomial representation restricted to interactions in I.

Definition 4.1. The PolySHAP representation Ï•I âˆˆ Rdâ€²

with dâ€² = d + |I| is given by 

Ï•I [Î½] := arg min  

> Ï•âˆˆRdâ€²:âŸ¨Ï•,1âŸ©=Î½(D)

X

> SâŠ†D

Î¼(S)

ï£«ï£­Î½(S) âˆ’ X 

> TâˆˆDâˆªI

Ï•T

Y

> jâˆˆT

1[j âˆˆ S]

ï£¶ï£¸

> 2

.

Here, and in the following we abuse notation with Ï•i := Ï•{i} and 1[j âˆˆ i] := 1[j = i] for i, j âˆˆ D.

The PolySHAP representation generalizes the least squares formulation of the Shapley value to arbitrary interaction frontiers I. For each interaction set T âˆˆ I , the approximation contributes a coefficient Ï•T only if all features in T are present in S.

Remark 4.2. The PolySHAP representation directly extends the Faithful Shapley interaction index (Tsai et al., 2023) to arbitrary interaction frontiers. 

In the theorem below, we show how to recover Shapley values from the PolySHAP representation. 

Theorem 4.3. The Shapley values of Î½ are recovered from the PolySHAP representation as 

Ï•SV  

> i

[Î½] = Ï•I 

> i

+ X

> SâˆˆI :iâˆˆS

Ï•I

> S

|S| for i âˆˆ D. (3) In other words, consistent estimation of the PolySHAP representation directly implies consistent estimation of the Shapley value. 44.2 POLY SHAP A LGORITHM 

A natural approximation strategy is to first estimate the PolySHAP representation and then map the result back to Shapley values using Theorem 4.3. Concretely, we approximate the PolySHAP representation by solving 

Ë†Ï•I [Î½] := arg min 

> Ï•âˆˆRd+|I| :âŸ¨Ï•,1âŸ©=Î½(D)
> m

X

> â„“=1

Î¼(Sâ„“)

p(Sâ„“)

ï£«ï£­Î½(Sâ„“) âˆ’ X 

> TâˆˆDâˆªI

Ï•T

Y

> jâˆˆT

1[j âˆˆ S]

ï£¶ï£¸

> 2

(4) with m samples S1, . . . , S m drawn from some distribution p, where dâ€² < m â‰¤ 2d. (When Î½ is clear from context, we write Ë†Ï•I for Ë†Ï•I [Î½].) We then convert Ë†Ï•I into Shapley value estimates via Theorem 4.3. The rationale behind this is approach is that the more expressive PolySHAP representation more accurately represents Î½, which in turn yields more accurate Shapley value estimates. We refer to this interaction-aware extension of KernelSHAP as PolySHAP .In order to produce the PolySHAP solution in practice, we use the matrix representation of the regression problem. Define the sampled design matrix ËœX âˆˆ RmÃ—dâ€²

and the sampled target vector Ëœy âˆˆ Rm. The rows are indexed by 

â„“ âˆˆ [m], and the columns of ËœX are indexed by interactions T âˆˆ D âˆª I . The entries of the sampled design matrix and sampled target vector are given by 

[ ËœX]â„“,T =

s

Î¼(Sâ„“)

p(Sâ„“) Â· 1[T âŠ† Sâ„“] and [Ëœ y]â„“ =

s

Î¼(Sâ„“)

p(Sâ„“) Â· Î½(Sâ„“). (5) In this notation, we may write 

Ë†Ï•I = arg min  

> Ï•âˆˆRdâ€²:âŸ¨1,Ï•âŸ©=Î½(D)

âˆ¥ ËœXÏ• âˆ’ Ëœyâˆ¥22. (6) We would like to apply standard regression tools when solving the problem, so we convert from the constrained problem to an unconstrained reformulation. Let Pdâ€² be the matrix that projects off the all ones vector in dâ€² dimensions i.e., Pdâ€² = I âˆ’ 1 

> dâ€²

1dâ€² 1dâ€² âŠ¤. We have 

arg min  

> Ï•âˆˆRdâ€²:âŸ¨1,Ï•âŸ©=Î½(D)

âˆ¥ ËœXÏ• âˆ’ Ëœyâˆ¥22 = arg min  

> Ï•âˆˆRdâ€²:âŸ¨Ï•,1âŸ©=0

ËœXÏ• + ËœX1 Î½(D)

dâ€² âˆ’ Ëœy

> 22

+ 1 Î½(D)

dâ€²

= Pdâ€² arg min 

> Ï•âˆˆRdâ€²

ËœXP dâ€² Ï• + ËœX1 Î½(D)

dâ€² âˆ’ Ëœy

> 22

+ 1 Î½(D)

dâ€² . (7) PolySHAP is described in pseudocode in Algorithm 1. 

Algorithm 1 PolySHAP 

Require: game Î½x, interaction frontier I, sampling distribution p, sampling budget m > d â€².

1: Define Î½(S) := Î½x(S) âˆ’ Î½x(âˆ…) â–· Center for notational simplicity 

2: {Sâ„“}mâ„“=1 â† SAMPLE (m, p )

3: Construct ËœX and Ëœy â–· Equation (5) 

4: Ë†Ï•I â† SOLVE LEAST SQUARES ( ËœXP dâ€² , Ëœy âˆ’ ËœX1 Î½(D) 

> dâ€²

) + 1 Î½(D)

> dâ€²

5: Ë†Ï•SV â† POLY SHAP TO SV ( Ë†Ï•I ) â–· Equation (3) 

6: return Î½x(âˆ…), Ë†Ï•SV 

Computational Complexity. The computational complexity of PolySHAP can be divided into two components: eval-uating the game for the sampled coalitions, and solving the regression problem followed by extraction of the Shapley values. Evaluating the game requires at least one model call for local explanation games, and highly depends on the application setting. Solving the regression problem scales with O(m Â· dâ€²2 + dâ€²3) , whereas transforming the PolySHAP representation to Shapley values is of order O(d Â· dâ€²). Importantly, this complexity scales linearly with the budget m,and quadratically with the number of regression variables dâ€². In practice, the dominant factor in computational cost is usually the game evaluations, i.e., the model predictions. However, for smaller model architectures, the runtime can be influenced by the number of regression variables. 50 800                                                         

> (20%)
> 1600
> (39%)
> 2400
> (59%)
> 3200
> (78%)
> 4000
> (98%)
> Budget ( m)
> 10 9
> 10 7
> 10 5
> 10 3
> 10 1
> 10 1
> 10 3
> MSE Â± SEM
> Bike ( d= 12) 01500
> (18%)
> 3000
> (37%)
> 4500
> (55%)
> 6000
> (73%)
> 7500
> (92%)
> Budget ( m)
> 10 9
> 10 7
> 10 5
> 10 3
> 10 1
> 10 1
> MSE Â± SEM
> Forest ( d= 13) 03000
> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)
> Budget ( m)
> 10 9
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> MSE Â± SEM
> ResNet18 ( d= 14) 04000
> (6%)
> 8000
> (12%)
> 12000
> (18%)
> 16000
> (24%)
> 20000
> (31%)
> Budget ( m)
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> MSE Â± SEM
> ViT16 ( d= 16) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> MSE Â± SEM
> Cancer ( d= 30) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> MSE Â± SEM
> CG60 ( d= 60) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 3
> 10 2
> 10 1
> 10 0
> 10 1
> MSE Â± SEM
> NHANES ( d= 79) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 1
> 10 0
> 10 1
> 10 2
> 10 3
> 10 4
> MSE Â± SEM
> Crime ( d= 101)

Figure 2: Approximation quality measured by MSE ( Â± SEM) for various sampling budgets m on different games. Adding any number of interactions in PolySHAP improves approximation quality. 4.3 SAMPLING STRATEGIES FOR POLY SHAP PolySHAP uses a distribution p to sample m game evaluations for approximating the least squares objective. Previous work (Lundberg & Lee, 2017; Covert & Lee, 2021) chose p proportional to Î¼(S), which cancels the multiplicative correction term in Equation (4). However, sampling proportionally to leverage scores offers improves estimation quality, and is supported by theoret-ical guarantees (Musco & Witter, 2025). Let X âˆˆ R2dÃ—dâ€²

be the full deterministic matrix (each subset is sampled exactly once with probability 1). The leverage score for the row corresponding to subset S is given by 

â„“S = [ XP D ]âŠ¤

S

 PD XâŠ¤XP D

â€  [XP D ]S (8) where (Â·)â€  denotes the pseudoinverse, and [XP D ]S is the Sth row of XP D .

Theorem 4.4 (Leverage Score Sampling Guarantee (Musco & Witter, 2025)) . Let Ïµ, Î´ > 0. When m = O(dâ€² log dâ€²

Î´ +

dâ€² 1

ÏµÎ´ ) subsets are sampled proportionally to their leverage scores (with or without replacement and with or without paired sampling), the approximation Ë†Ï•I satisfies, with probability 1 âˆ’ Î´,

X

SâŠ†D

Î¼(S)

ï£«ï£­Î½(S) âˆ’ X

T âˆˆDâˆªI 

ËœÏ•I

T

Y

jâˆˆT

1[j âˆˆ S]

ï£¶ï£¸

2

â‰¤ X

SâŠ†D

Î¼(S)

ï£«ï£­Î½(S) âˆ’ X

T âˆˆDâˆªI 

Ï•I

T

Y

jâˆˆT

1[j âˆˆ S]

ï£¶ï£¸

2

.

Musco & Witter (2025) show that â„“S = 1 /  n

|S|

 for KernelSHAP, i.e., leverage score sampling is equivalent to sampling subsets uniformly by their size. For the k-additive interaction frontier, we can directly compute the leverage scores using symmetry and Equation (8), although a closed-form solution remains unknown. In practice, we observed little variation between leverage scores of order 1 and those of higher orders, which is why we recommend using order-1

leverage scores. 4.4 CONSTRUCTION OF INTERACTION FRONTIERS 

The interaction frontier I determines the number of additional variables (columns) in the linear regression problem. Its size must be balanced against the budget m (rows). Since lower-order interaction terms occur more frequently and are thus less sensitive to noise, it is natural to expand these terms first. To this end, we define the k-additive interaction frontier for k = 2 , . . . , d as 

Iâ‰¤k := {S âŠ† D : 2 â‰¤ | S| â‰¤ k} with |I â‰¤k| =

kX

i=2 

di



.

6The k-additive interaction frontier includes all interactions up to order k by sequentially extending the (k âˆ’ 1) -additive interaction frontier with  nk

 sets. It is widely used in Shapley-based interaction indices (Sundararajan et al., 2020; Tsai et al., 2023; Bordt & von Luxburg, 2023). In the following, we refer to PolySHAP using Iâ‰¤k as k-PolySHAP .

Corollary 4.5. The k-PolySHAP representation is equal to order-k Faith-SHAP (Tsai et al., 2023). 

A notable special case of k-PolySHAP is the interaction frontier without interactions: 1-PolySHAP, i.e., without interactions ( I = âˆ…), is equivalent to KernelSHAP. We further show convergence for kADD -SHAP, extending Theorem 4.2 in (Pelegrina et al., 2025). 

Proposition 4.6. kADD -SHAP converges to the Shapley value for k = 1 , . . . , d .

kADD -SHAP is linked to k-PolySHAP, but we recommend PolySHAP in practice, see Section A.4. 

Partial Interaction Frontiers. In high dimensions, the k-additive interaction frontier grows combinatorially with  nk

.With a limited evaluation budget m, including all interaction terms of a given order may yield an underdetermined least-squares system. To address this, we introduce the partial interaction frontier Iâ„“ with exactly â„“ elements: 

Iâ„“ := Iâ‰¤kâ„“ âˆª R , with |I â„“| = â„“, 

where kâ„“ is the largest order such that |I â‰¤kâ„“ | â‰¤ â„“, and R âŠ† I â‰¤kâ„“+1 \ I â‰¤kâ„“ denotes a set of â„“ âˆ’ |I â‰¤kâ„“ | interaction terms of order kâ„“ + 1 . In words, Iâ„“ sequentially covers the k-additive interaction frontier up to kâ„“, and supplements them with a selected subset of the subsequent higher-order interactions. In our experiments, we demonstrate that par-tially including higher-order interactions improves approximation quality, whereas using the full k-additive interaction frontier provides the largest gains. 

5 PAIRED KERNEL SHAP IS PAIRED 2-P OLY SHAP 

A common heuristic when estimating Shapley values is to sample subsets in pairs S and D \ S. A kind of antithetic sampling (Glasserman, 2004), paired sampling substantially improves the approximation of estimators (Covert & Lee, 2021; Mitchell et al., 2022; Olsen & Jullum, 2024). Adding higher order interactions to PolySHAP improves Shapley value estimates, provided we have enough samples (see Figure 2): 3-PolySHAP outperforms 2-PolySHAP, which outperforms KernelSHAP ( 1-PolySHAP). Surprisingly, paired sampling partially collapses this hierarchy (see Figure 3). 

Theorem 5.1 (Paired KernelSHAP is Paired 2-PolySHAP) . Suppose that subsets are sampled in pairs i.e., if S is sampled then so is its complement D \ S, and, the matrix ËœX has full column rank for interaction frontier D and Iâ‰¤2.Then 

Ë†Ï•SV = POLY SHAP TO SV ( Ë†Ï•Iâ‰¤2 )

In words, Shapley values approximated by 2-PolySHAP are precisely the KernelSHAP estimates. 

We prove Theorem 5.1 by explicitly building the approximate solutions of KernelSHAP and 2-PolySHAP. Of particular help is a new technical projection lemma that we also use in the proof of Theorem 4.3. See Section A for the details. 

Generalizing Prior Work. Mayer & WÂ¨ uthrich (2025) recently showed that paired KernelSHAP exactly recovers the Shapley values of games with interactions of at most size 2. This follows immediately from Theorem 5.1, because 2-PolySHAP will precisely a game with order-2 interactions and paired Kernel SHAP will return the same solution. However, Theorem 5.1 is far more generally because it explains why paired sampling performs so well for all games, not just a restricted class. 

Higher Dimensional Extensions. A natural question is whether similar results hold for higher order interactions. Suppose k is an odd number, we find empirically that paired (k + 1) -PolySHAP returns the same approximate Shapley values as paired k-PolySHAP. We conjecture that this pattern holds for all odd k such that 1 â‰¤ k < n . However, it is not obvious how to adapt our proof of Theorem 5.1, since we would need the explicit mapping of k + 1 -PolySHAP representations to k-PolySHAP representations (this is clear when k = 1 , but not so for higher dimensions). 

6 EXPERIMENTS 

70 800                                                          

> (20%)
> 1600
> (39%)
> 2400
> (59%)
> 3200
> (78%)
> 4000
> (98%)
> Budget ( m)
> 10 9
> 10 7
> 10 5
> 10 3
> 10 1
> 10 1
> 10 3
> MSE Â± SEM
> Bike ( d= 12) 01500
> (18%)
> 3000
> (37%)
> 4500
> (55%)
> 6000
> (73%)
> 7500
> (92%)
> Budget ( m)
> 10 9
> 10 7
> 10 5
> 10 3
> 10 1
> 10 1
> MSE Â± SEM
> Forest ( d= 13) 03000
> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)
> Budget ( m)
> 10 9
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> MSE Â± SEM
> ResNet18 ( d= 14) 04000
> (6%)
> 8000
> (12%)
> 12000
> (18%)
> 16000
> (24%)
> 20000
> (31%)
> Budget ( m)
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> MSE Â± SEM
> ViT16 ( d= 16) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 9
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> MSE Â± SEM
> Cancer ( d= 30) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> MSE Â± SEM
> CG60 ( d= 60) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 3
> 10 2
> 10 1
> 10 0
> 10 1
> MSE Â± SEM
> NHANES ( d= 79) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 1
> 10 0
> 10 1
> 10 2
> 10 3
> 10 4
> MSE Â± SEM
> Crime ( d= 101)

Figure 3: Approximation quality measured by MSE ( Â± SEM) for standard (dotted) and paired (solid) sampling. With paired sampling, KernelSHAP achieves the same performance as 2-PolySHAP. Table 2: Explanation games. 

ID d Domain 

Housing 8 tabular ViT9 9 image Bike 12 tabular Forest 13 tabular Adult 14 tabular ResNet18 14 image DistilBERT 14 language Estate 15 tabular ViT16 16 image CIFAR10 16 image Cancer 30 tabular CG60 60 synthetic IL60 60 synthetic NHANES 79 tabular Crime 101 tabular We empirically validate PolySHAP and approximate Shapley values on 

15 local explanation games across 30 randomly selected instances, see Table 2. We evaluate all methods with m samples ranging from d + 1 

to min(2 d, 20000) , and compare PolySHAP against Permutation Sam-pling (Castro et al., 2009), SVARM (Kolpaczki et al., 2024), MSR (Fu-magalli et al., 2023; Wang & Jia, 2023), Unbiased KernelSHAP (Covert & Lee, 2021), RegressionMSR with XGBoost (Witter et al., 2025), and Ker-nelSHAP (1-PolySHAP) with leverage score sampling (Lundberg & Lee, 2017; Musco & Witter, 2025). For tabular datasets, we trained random forests, while for image classifica-tion we used a ResNet18 (He et al., 2016) with 14 superpixels and vision transformers with 3x3 (ViT9) and 4x4 (ViT16) super-patches on ImageNet (Deng et al., 2009), and CIFAR-10 (Krizhevsky et al., 2009). For lan-guage modeling, we used a fine-tuned DistilBert (Sanh et al., 2019) to predict sentiment on the IMDB dataset (Maas et al., 2011) with review excerpts of length 14 . For tabular datasets, the games were defined via path-dependent feature perturbation, allowing ground-truth Shapley val-ues to be obtained from TreeSHAP (Lundberg et al., 2020). For all other datasets, we used baseline imputation and exhaustive Shapley value com-putation. As evaluation metrics, we report mean-squared error (MSE) ,top-5 precision ( Precision@5 ), and Spearman correlation with standard error of the mean (SEM) . Code is available in the supplementary material, and additional details and results, including a runtime analysis, are provided in Section B. 

PolySHAP Variants. For comparability across methods, we sample subsets using order-1 leverage scores, i.e., uni-formly over subset sizes. We further adopt sampling without replacement and distinguish between standard and paired subset sampling. We apply the border trick (Fumagalli et al., 2023), replacing random sampling with exhaustive enu-meration of sizes when the expected samples exceed the number of subsets. We use k-PolySHAP with k âˆˆ { 1, 2, 3, 4},and additionally the partial interaction frontiers that cover 50% of all k-order interactions, denoted by k-PolySHAP (50%) . For high-dimensional settings, we introduce PolySHAP (log) that adds d log(  d

3

) order-3 interactions. 

Higher-order Interactions Improve Approximation. Figure 2 reports the MSE with SEM for selected explanation games and standard sampling. Across different games, we observe that incorporating higher-order interactions in PolySHAP consistently improves approximation quality. However, higher-order PolySHAP requires a larger sampling budget, and hence performance is only plotted for m â‰¥ dâ€². Nevertheless, 2-PolySHAP, and even partial interaction inclusion (e.g., 2-PolySHAP at 50%), still yield notable improvements in approximation accuracy. 8MSR Permutation Sampling 3-PolySHAP (log) RegressionMSR SVARM UnbiasedKernelSHAP 3-PolySHAP (50%) 0 800                                                          

> (20%)
> 1600
> (39%)
> 2400
> (59%)
> 3200
> (78%)
> 4000
> (98%)
> Budget ( m)
> 10 9
> 10 7
> 10 5
> 10 3
> 10 1
> 10 1
> 10 3
> MSE Â± SEM
> Bike ( d= 12) 01500
> (18%)
> 3000
> (37%)
> 4500
> (55%)
> 6000
> (73%)
> 7500
> (92%)
> Budget ( m)
> 10 9
> 10 7
> 10 5
> 10 3
> 10 1
> 10 1
> MSE Â± SEM
> Forest ( d= 13) 03000
> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)
> Budget ( m)
> 10 9
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> MSE Â± SEM
> ResNet18 ( d= 14) 04000
> (6%)
> 8000
> (12%)
> 12000
> (18%)
> 16000
> (24%)
> 20000
> (31%)
> Budget ( m)
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> MSE Â± SEM
> ViT16 ( d= 16) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 9
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> MSE Â± SEM
> Cancer ( d= 30) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> MSE Â± SEM
> CG60 ( d= 60) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> MSE Â± SEM
> NHANES ( d= 79) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 1
> 10 0
> 10 1
> 10 2
> 10 3
> 10 4
> MSE Â± SEM
> Crime ( d= 101)

Figure 4: Approximation quality of PolySHAP variants and baseline methods measured by MSE ( Â± SEM) using paired sampling. With paired sampling, PolySHAP consistently improves upon KernelSHAP when order-3 interactions are included. In higher dimensions ( d â‰¥ 60 ), only a few of these can be modeled, yielding smaller improvements. 

Paired KernelSHAP is 2-PolySHAP. As shown in Theorem 5.1, under paired sampling, KernelSHAP and 2-PolySHAP are equivalent indicated by the overlapping lines. We confirm this empirically in Figure 3. However, there is an important distinction: 2-PolySHAP requires more budget, whereas KernelSHAP can be computed already with d + 1 samples. Lastly, we observe a similar pattern for 3-PolySHAP: Under paired sampling 3-PolySHAP substantially improves its approximation quality and is equivalent to 4-PolySHAP. 

Practical Benefits of PolySHAP. In practice, we adopt paired sampling and benchmark PolySHAP against all base-lines in Figure 4 and Figure 7 in Section B.2. Because of our paired sampling result, the practical benefits of PolySHAP become apparent only when order-3 interactions are included. In low-dimensional settings, the 3-PolySHAP yields the best performance on Housing, Adult, Estate, Forest, and Cancer datasets (see e.g., Figure 4 and Figure 7). In budget-restricted cases, partially incorporating order-3 interactions already provides substantial gains, cf. 3-PolySHAP (50%) and 3-PolySHAP (log). In high-dimensional settings ( d â‰¥ 60 ), however, only a small number of order-3 interactions can be added, resulting in more modest improvements. Among all baselines, only RegressionMSR achieves compa-rable performance, although its performance depends strongly on XGBoost, as indicated by its poor results on CG60. Moreover, RegressionMSR has an inherent advantage since all tabular games rely on tree-based models. 

7 CONCLUSION & F UTURE WORK 

By reformulating the computation of the Shapley value as a polynomial regression problem with selected interaction terms, PolySHAP extends beyond the linear regression framework of KernelSHAP. We demonstrate that PolySHAP provides consistent estimates of the Shapley value (Theorem 4.3), and produces more accurate Shapley value esti-mates (see Figure 2 and Figure 3). Moreover, we show that paired subset sampling in KernelSHAP (Covert & Lee, 2021) implicitly captures all second-order interactions at no extra cost (Theorem 5.1), explaining why paired sampling improves estimator accuracy on games with arbitrary interaction structures. Future work could explore more structured variants of interaction frontier, for example by detecting important inter-actions (Tsang et al., 2020) or leveraging inherent interaction structures in graph-structured inputs (Muschalik et al., 2025). In addition, we empirically find that paired k-PolySHAP produce the same estimates as ( k + 1) -PolySHAP for odd k > 1, but leave the proof for future work. 

ETHICS STATEMENT 

This work introduces a framework for efficient approximation of Shapley values, which are primarily used for explain-able AI. We do not see any ethical concerns associated with this work. 9REPRODUCIBILITY STATEMENT 

Our implementation is based on the shapiq (Muschalik et al., 2024) library, and adds the PolySHAP 

and RegressionMSR approximator class, including changes for the optimized sampling strategies in the 

CoalitionSampler class. We will soon add our methods to shapiq .

REFERENCES 

Sebastian Bordt and Ulrike von Luxburg. From Shapley Values to Generalized Additive Models and back. In Pro-ceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS) , pp. 709â€“745, 2023. Landon Butler, Abhineet Agarwal, Justin Singh Kang, Yigit Efe Erginbas, Bin Yu, and Kannan Ramchandran. Prox-yspex: Inference-efficient interpretability via sparse feature interactions in llms. CoRR , abs/2505.17495, 2025. Javier Castro, Daniel GÂ´ omez, and Juan Tejada. Polynomial calculation of the Shapley value based on sampling. 

Computers & Operations Research , 36(5):1726â€“1730, 2009. doi: 10.1016/j.cor.2008.04.004. Javier Castro, Daniel GÂ´ omez, Elisenda Molina, and Juan Tejada. Improving polynomial estimation of the Shapley value by stratified random sampling with optimum allocation. Computers & Operations Research , 82:180â€“188, 2017. doi: 10.1016/j.cor.2017.01.019. Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) , pp. 785â€“794. ACM, 2016. doi: 10.1145/2939672.2939785. Paulo Cortez and AnÂ´ Ä±bal de Jesus Raimundo Morais. A data mining approach to predict forest fires using meteorolog-ical data. 2007. Ian Covert and Su-In Lee. Improving KernelSHAP: Practical Shapley Value Estimation Using Linear Regression. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS) , pp. 3457â€“3465, 2021. Ian Covert, Scott M. Lundberg, and Su-In Lee. Understanding Global Feature Contributions With Additive Importance Measures. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS) , 2020. Ian Covert, Scott M. Lundberg, and Su-In Lee. Explaining by Removing: A Unified Framework for Model Explana-tion. Journal of Machine Learning Research , 22(209):1â€“90, 2021. doi: 10.5555/3546258.3546467. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 248â€“255, 2009. An Dinh, Stacey Miertschin, Amber Young, and Somya D. Mohanty. A data-driven approach to predicting diabetes and cardiovascular disease with machine learning. BMC Medical Informatics Decisision Making , 19(1):211:1â€“211:15, 2019. doi: 10.1186/S12911-019-0918-5. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations (ICLR) , 2021. H. Fanaee-T and J. Gama. Event Labeling Combining Ensemble Detectors and Background Knowledge. Progress in Artificial Intelligence , 2(2):113â€“127, 2014. doi: 10.1007/s13748-013-0040-3. Matthias Feurer, Jan N. van Rijn, Arlind Kadra, Pieter Gijsbers, Neeratyoy Mallik, Sahithya Ravi, Andreas Mueller, Joaquin Vanschoren, and Frank Hutter. OpenML-Python: an extensible Python API for OpenML. CoRR ,abs/1911.02490, 2020. Fabian Fumagalli, Maximilian Muschalik, Patrick Kolpaczki, Eyke HÂ¨ ullermeier, and Barbara Hammer. SHAP-IQ: Unified Approximation of any-order Shapley Interactions. In Proceedings of Advances in Neural Information Pro-cessing Systems (NeurIPS) , 2023. 10 Fabian Fumagalli, Maximilian Muschalik, Eyke HÂ¨ ullermeier, Barbara Hammer, and Julia Herbinger. Unifying Feature-Based Explanations with Functional ANOVA and Cooperative Game Theory. In Proceedings of the In-ternational Conference on Artificial Intelligence and Statistics (AISTATS) , pp. 5140â€“5148, 2025. Amirata Ghorbani and James Y. Zou. Data shapley: Equitable valuation of data for machine learning. In Proceedings of the International Conference on Machine Learning (ICML) , pp. 2242â€“2251, 2019. Paul Glasserman. Monte Carlo Methods in Financial Engineering . Stochastic Modelling and Applied Probability. Springer, 2004. doi: 10.1007/978-0-387-21617-1. Michel Grabisch and Marc Roubens. An axiomatic approach to the concept of interaction among players in cooperative games. International Journal of Game Theory , 28(4):547â€“565, 1999. doi: 10.1007/s001820050125. Michel Grabisch, Jean-Luc Marichal, and Marc Roubens. Equivalent representations of set functions. Mathematics of Operations Research , 25(2):157â€“178, 2000. doi: 10.1287/moor.25.2.157.12225. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed-ings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770â€“778, 2016. Justin Singh Kang, Yigit Efe Erginbas, Landon Butler, Ramtin Pedarsani, and Kannan Ramchandran. Learning to understand: Identifying interactions via the mÂ¨ obius transform. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS) , 2024. R. Kelley Pace and Ronald Barry. Sparse spatial autoregressions. Statistics & Probability Letters , 33(3):291â€“297, 1997. doi: 10.1016/S0167-7152(96)00140-X. Gwladys Kelodjou, Laurence RozÂ´ e, VÂ´ eronique Masson, Luis GalÂ´ arraga, Romaric Gaudel, Maurice TchuentÂ´ e, and Alexandre Termier. Shaping up SHAP: enhancing stability through layer-wise neighbor selection. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (eds.), Proceedings of the AAAI Conference on Artificial Intel-ligence (AAAI) , pp. 13094â€“13103, 2024. doi: 10.1609/AAAI.V38I12.29208. R. Kohavi. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD) , pp. 202â€“207, 1996. Patrick Kolpaczki, Viktor Bengs, Maximilian Muschalik, and Eyke HÂ¨ ullermeier. Approximating the shapley value without marginal contributions. In Proceeedings of the AAAI Conference on Artificial Intelligence (AAAI) , pp. 13246â€“13255, 2024. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical Report ,2009. Yongchan Kwon and James Zou. Beta shapley: a unified and noise-reduced data valuation framework for machine learning. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS) , pp. 8780â€“8802, 2022a. Yongchan Kwon and James Y. Zou. Weightedshap: analyzing and improving shapley based feature attributions. In 

Proceedings of Advances in Neural Information Processing Systems (NeurIPS) , 2022b. Weida Li and Yaoliang Yu. Faster approximation of probabilistic and distributional values via least squares. In 

Proceedings of the International Conference on Learning Representations (ICLR) , 2024. Scott M. Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS) , pp. 4765â€“4774, 2017. Scott M Lundberg, Gabriel G Erion, and Su-In Lee. Consistent individualized feature attribution for tree ensembles. 

arXiv preprint arXiv:1802.03888 , 2018. Scott M. Lundberg, Gabriel G. Erion, Hugh Chen, Alex J. DeGrave, Jordan M. Prutkin, Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. From local explanations to global understanding with explainable AI for trees. Nature Machine Intelligence , 2(1):56â€“67, 2020. doi: 10.1038/s42256-019-0138-9. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (HLT) , pp. 142â€“150, 2011. 11 Michael Mayer and Mario V WÂ¨ uthrich. Shapley values: Paired-sampling approximations. CoRR , abs/2508.12947, 2025. Rory Mitchell, Joshua Cooper, Eibe Frank, and Geoffrey Holmes. Sampling Permutations for Shapley Value Estima-tion. Journal of Machine Learning Research , 23(43):1â€“46, 2022. Majid Mohammadi, Ilaria Tiddi, and Annette ten Teije. Unlocking the game: Estimating games in mÂ¨ obius represen-tation for explanation and high-order interaction detection. In Proceeedings of the AAAI Conference on Artificial Intelligence (AAAI) , pp. 19512â€“19519, 2025. doi: 10.1609/AAAI.V39I18.34148. Christoph Molnar. Interpretable machine learning: A guide for making black box models explainable . Leanpub, 2024. Maximilian Muschalik, Hubert Baniecki, Fabian Fumagalli, Patrick Kolpaczki, Barbara Hammer, and Eyke HÂ¨ ullermeier. shapiq: Shapley Interactions for Machine Learning. In Proceedings of Advances in Neural Infor-mation Processing Systems (NeurIPS) , pp. 130324â€“130357, 2024. Maximilian Muschalik, Fabian Fumagalli, Paolo Frazzetto, Janine Strotherm, Luca Hermes, Alessandro Sperduti, Eyke HÂ¨ ullermeier, and Barbara Hammer. Exact Computation of Any-Order Shapley Interactions for Graph Neural Networks. In Proceedings of the Conference on Learning Representations (ICLR) , 2025. Christopher Musco and R. Teal Witter. Provably Accurate Shapley Value Estimation via Leverage Score Sampling. In 

Proccedings of the International Conference on Learning Representations (ICLR) , 2025. Lars Henry Berge Olsen and Martin Jullum. Improving the sampling strategy in kernelshap, 2024. Lars Henry Berge Olsen, Ingrid Kristine Glad, Martin Jullum, and Kjersti Aas. A comparative study of methods for estimating model-agnostic Shapley value explanations. Data Mining and Knowledge Discovery , pp. 1â€“48, 2024. Fabian Pedregosa, GaÂ¨ el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine learning in python. Journal of Machine Learning Research , 12:2825â€“2830, 2011. doi: 10.5555/1953048.2078195. Guilherme Dean Pelegrina, Leonardo Tomazeli Duarte, and Michel Grabisch. A k-additive choquet integral-based approach to approximate the SHAP values for local interpretability in machine learning. Artificial Intelligence , 325: 104014, 2023. doi: 10.1016/J.ARTINT.2023.104014. Guilherme Dean Pelegrina, Patrick Kolpaczki, and Eyke HÂ¨ ullermeier. Shapley value approximation based on k-additive games. CoRR , abs/2502.04763, 2025. Michael Redmond. Communities and crime unnormalized, 2011. Gian-Carlo Rota. On the foundations of combinatorial theory: I. theory of mÂ¨ obius functions. In Classic Papers in Combinatorics , pp. 332â€“360. Springer, 1964. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. CoRR , abs/1910.01108, 2019. L. S. Shapley. A Value for n-Person Games. In Contributions to the Theory of Games (AM-28), Volume II , pp. 307â€“318. Princeton University Press, 1953. W Nick Street, William H Wolberg, and Olvi L Mangasarian. Nuclear feature extraction for breast tumor diagnosis. In Biomedical image processing and biomedical visualization , volume 1905, pp. 861â€“870, 1993. Mukund Sundararajan, Kedar Dhamdhere, and Ashish Agarwal. The Shapley Taylor Interaction Index. In Proceedings of the International Conference on Machine Learning (ICML) , pp. 9259â€“9268, 2020. Che-Ping Tsai, Chih-Kuan Yeh, and Pradeep Ravikumar. Faith-Shap: The Faithful Shapley Interaction Index. Journal of Machine Learning Research , 24(94):1â€“42, 2023. Michael Tsang, Sirisha Rambhatla, and Yan Liu. How does This Interaction Affect Me? Interpretable Attribution for Feature Interactions. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS) , pp. 6147â€“6159, 2020. 12 Jiachen T. Wang and Ruoxi Jia. Data banzhaf: A robust data valuation framework for machine learning. In Francisco J. R. Ruiz, Jennifer G. Dy, and Jan-Willem van de Meent (eds.), Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS) , pp. 6388â€“6421, 2023. R. Teal Witter, Yurong Liu, and Christopher Musco. Regression-adjusted monte carlo estimators for shapley values and probabilistic values. CoRR , abs/2506.11849, 2025. I-Cheng Yeh and Tzu-Kuang Hsu. Building real estate valuation models with comparative approach through case-based reasoning. Appied Soft Computing , 65:260â€“271, 2018. doi: 10.1016/J.ASOC.2018.01.029. 13 CONTENTS OF THE APPENDIX 

A Proofs 15 

A.1 Projection Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.2 PolySHAP is Consistent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 A.3 Paired KernelSHAP is Paired 2-PolySHAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.4 kADD -SHAP Converges to the Shapley Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 

B Experimental Details and Additional Results 21 

B.1 Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.2 Additional Results on Approximation Quality using MSE . . . . . . . . . . . . . . . . . . . . . . . . 22 B.3 Approximation Quality using Precision@5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.4 Approximation Quality using Spearman Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.5 Runtime Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 

C Usage of Large Language Models (LLMs) 31 

14 A PROOFS 

A.1 PROJECTION LEMMA 

We introduce the following technical lemma that will be useful in the proofs of Theorem 4.3 and Theorem 5.1. 

Lemma A.1 (Projection Lemma) . Let n â‰¥ d+ > d. Consider a matrix X âˆˆ RnÃ—d with full column rank, a vector 

y âˆˆ Rn, and a real number c âˆˆ R. Let X+ âˆˆ RnÃ—d+ be a matrix where the first d columns are equal to X. Define 

Î²âˆ— 

> +

= arg min   

> Î²âˆˆRd+:âŸ¨Î²,1d+âŸ©=c

âˆ¥X+Î² âˆ’ yâˆ¥22.

Then 

arg min 

> Î²âˆˆRd:âŸ¨Î²,1âŸ©=c

âˆ¥XÎ² âˆ’ yâˆ¥22 = arg min 

> Î²âˆˆRd:âŸ¨Î²,1dâŸ©=c

âˆ¥XÎ² âˆ’ X+Î²âˆ—

> +

âˆ¥22. (9) 

Proof of Lemma A.1. We will first reformulate the constrained least squares problem as an unconstrained problem. Let Pd be the matrix that projects off the all ones vector in d dimensions i.e., P1d = I âˆ’ 1 

> d

1d1dâŠ¤. Similarly, let 

Pd+ = I âˆ’ 1 

> d

1d1dâŠ¤. In general, we will drop the subscript d when the dimension is clear from context. We have 

arg min 

> Î²âˆˆRd:âŸ¨Î²,1âŸ©=c

âˆ¥XÎ² âˆ’ yâˆ¥22 = arg min 

> Î²âˆˆRd:âŸ¨Î²,1âŸ©=0

XÎ² + X1 cd âˆ’ y 22

+ 1 cd

= Pd arg min 

> Î²âˆˆRd

XP dÎ² + X1 cd âˆ’ y 22

+ 1 cd

= Pd(XP d)â€  

y âˆ’ X1 cd



+ 1 cd , (10) where the (Â·)â€  denotes the pseudoinverse, and the last equality follows by the standard solution to an unconstrained least squares problem. Similarly, 

Î²âˆ— 

> +

= arg min   

> Î²âˆˆRd+:âŸ¨Î²,1d+âŸ©=c

âˆ¥X+Î² âˆ’ yâˆ¥22 = Pd+ (X+Pd+ )â€ 



y âˆ’ X+1 cd+



+ 1 cd+

. (11) Let proj XP d = ( XP d)( XP d)â€  be the projection onto XP d. We have 

X arg min 

> Î²âˆˆRd:âŸ¨Î²,1âŸ©=c

âˆ¥XÎ² âˆ’ X+Î²âˆ—

> +

âˆ¥22 = XP d(XP d)â€  

X+Î²âˆ— 

> +

âˆ’ X1 cd



+ X1 cd

= proj XP d



X+



Pd+ (XP d+ )â€ 



y âˆ’ X+1 cd+



+ 1 cd+



âˆ’ X1 cd



+ X1 cd

= proj XP d proj X+Pd+

y âˆ’ proj XP d proj X+Pd+

X+1 cd+

+ proj XP d X+1 cd+

âˆ’ proj XP d X1 cd + X1 cd . (12) Since the column space of XP d is contained in the column space of X+Pd+ , observe that proj XP d proj X+Pd+

=

proj XP d . Then 

(12) = proj XP d y âˆ’ proj XP d X1 cd + X1 cd

= XP d(XP d)â€  

y âˆ’ X1 cd



+ X1 cd

= X arg min 

> Î²âˆˆRd:âŸ¨Î²,1âŸ©=c

âˆ¥XÎ² âˆ’ yâˆ¥22. (13) Since X has full column rank, we have Xâ€ X = I, so multiplying on the left by Xâ€  yields the statement. 15 A.2 POLY SHAP IS CONSISTENT 

In this section, we will prove Theorem 4.3. 

Theorem 4.3. The Shapley values of Î½ are recovered from the PolySHAP representation as 

Ï•SV  

> i

[Î½] = Ï•I 

> i

+ X

> SâˆˆI :iâˆˆS

Ï•I

> S

|S| for i âˆˆ D. (3) 

Proof of Theorem 4.3. Recall dâ€² = d + |I| . Define the target vector y âˆˆ R2d

so that [Ëœ y]S = pÎ¼(S) Â· Î½(Sâ„“). Define the design matrix X âˆˆ R2dÃ—d so that 

[X]S,i = pÎ¼(S) Â· 1[i âŠ† S], (14) and the extended design matrix X+ âˆˆ R2dÃ—dâ€²

so that 

[X+]S,T = pÎ¼(S) Â· 1[T âŠ† S], (15) for T âˆˆ D âˆª I .In this notation, we may write 

Ï•SV [Î½] = arg min 

> Ï•âˆˆRd:âŸ¨1,Ï•âŸ©=Î½(D)

âˆ¥XÏ• âˆ’ yâˆ¥22, (16) and 

Ï•I [Î½] = arg min  

> Ï•âˆˆRdâ€²:âŸ¨1,Ï•âŸ©=Î½(D)

âˆ¥X+Ï• âˆ’ yâˆ¥22. (17) Consider the game Ë†Î½ : 2 D â†’ R where 

Ë†Î½(S) = X  

> TâˆˆDâˆªI :TâŠ†S

Ï•I 

> T

[Î½]. (18) For this game, the target vector is given by Ë†y = X+Ï•I [Î½]. Then its Shapley values are given by 

Ï•SV [Ë† Î½] = arg min 

> Ï•âˆˆRd:âŸ¨1,Ï•âŸ©=Î½(D)

âˆ¥XÏ• âˆ’ Ë†yâˆ¥22

= arg min 

> Ï•âˆˆRd:âŸ¨1,Ï•âŸ©=Î½(D)

âˆ¥XÏ• âˆ’ X+Ï•I âˆ¥22

= arg min 

> Ï•âˆˆRd:âŸ¨1,Ï•âŸ©=Î½(D)

âˆ¥XÏ• âˆ’ yâˆ¥22

= Ï•SV [Î½], (19) where the penultimate equality follows by Lemma A.1. All that remains to compute the Shapley values Ë†Î½. Since we have an explicit representation of Ë†Î½ in terms of its MÂ¨ obius transform in Equation (18), we know its Shapley values are 

Ï•SV  

> i

[Ë† Î½] = X 

> TâˆˆDâˆªI :iâˆˆT

Ï•I 

> T

[Î½]

|T | . (20) by e.g., Table 3 in Grabisch et al. (2000). The statement follows. 16 A.3 PAIRED KERNEL SHAP IS PAIRED 2-P OLY SHAP We introduce some helpful notation, and then use it to restate Theorem 5.1 more formally below. Define dk = Pkâ„“=1 

 dâ„“

. Let ËœXk âˆˆ RmÃ—(dk) be the matrix where the â„“, T entry is given by 

[ ËœXk]â„“,T =

pÎ¼(Sâ„“)

pp(Sâ„“) 1[T âŠ† Sâ„“] (21) where Sâ„“ âŠ† D is the â„“th sampled subset, and T âŠ† D such that |T | = k. Then the matrix ËœXâ‰¤k âˆˆ RmÃ—dk is given by 

ËœXâ‰¤k =  ËœX1 . . . ËœXk

 . (22) Let M2â†’1 âˆˆ RdÃ—d2 be the matrix that projects a 2-PolySHAP to a 1-PolySHAP. The entry corresponding to i âˆˆ D,and S âŠ† D such that |S| â‰¤ 2 is given by 

[M2â†’1]i,S = 1[i âˆˆ S]

|S| . (23) 

Theorem A.2 (Paired KernelSHAP is Paired 2-PolySHAP) . Suppose ËœXâ‰¤2 has full column rank. Further, suppose that both 1-PolySHAP and 2-PolySHAP are computed with the same paired samples i.e., if S is sampled then so is its complement D \ S. Then 

arg min 

> Ï•âˆˆRd:âŸ¨1d,Ï•âŸ©=Î½(D)

âˆ¥ ËœX1Ï• âˆ’ Ëœyâˆ¥22 = M2â†’1 arg min   

> Ï•âˆˆRd2:âŸ¨1d2,Ï•âŸ©=Î½(D)

âˆ¥ ËœXâ‰¤2Ï• âˆ’ Ëœyâˆ¥22. (24) 

In words, the Shapley values of the approximate 1-PolySHAP are exactly the same as those of the approximate 2-PolySHAP. Proof of Theorem A.2. Define 

Ëœz + 1d2

Î½(D)

d2

= arg min   

> Ï•âˆˆRd2:âŸ¨Ï•,1d2âŸ©=Î½(D)

âˆ¥ ËœXâ‰¤2Ï• âˆ’ Ëœyâˆ¥22, (25) where Ëœz is orthogonal to the all ones vector. By Lemma A.1 and the structure Aâ‰¤2 = [ A1 A2], we have 

arg min 

> Ï•âˆˆRd:âŸ¨1d,Ï•âŸ©=Î½(D)

âˆ¥ ËœX1Ï• âˆ’ Ëœyâˆ¥22 = arg min 

> Ï•âˆˆRd:âŸ¨1d,Ï•âŸ©=Î½(D)

ËœX1Ï• âˆ’ ËœXâ‰¤2



Ëœz + 1d2

Î½(D)

d2

 22

. (26) Using Equation 10, we can write Equation 26 explicitly as 

(26) = Pd(Pd ËœXâŠ¤ 

> 1

ËœX1Pd)â€ Pd ËœXâŠ¤

> 1



ËœXâ‰¤2



Ëœz + 1d2

Î½(D)

d2



âˆ’ ËœX11d

Î½(D)

d



+ 1d

Î½(D)

d (27) where Pd = 1 âˆ’ 1 

> d

11 âŠ¤ is the matrix that projects off the all ones direction in d dimensions. Our goal is to show that 

(27) = M2â†’1



Ëœz + 1d2

Î½(D)

d2



. (28) Weâ€™ll begin with the all ones component. Observe that 

Î½(D)

d [M2â†’11]i = Î½(D)

d2

ï£«ï£­

> d

X

> j=1

1[i = j] + X  

> TâŠ†D:|| T|=2

1[i âˆˆ T ]2

ï£¶ï£¸ = Î½(D) 1 + dâˆ’12

d +  d

> 2

 = Î½(D)

d ,

so M2â†’11d2 

> Î½(D)
> d2

= 1d Î½(D) 

> d

.Now it remains to show the equality for the component orthogonal to the all ones direction. Since ËœXâ‰¤2 = ËœX1 ËœX2

 has full column rank by assumption, ËœX1 must have full column rank as well. It follows that 17 (Pd ËœXâŠ¤ 

> 1

ËœX1Pd)( Pd ËœXâŠ¤ 

> 1

ËœX1Pd)â€  = Pd. Then, after multiplying Equations 27 and 28 by (Pd ËœXâŠ¤ 

> 1

ËœX1Pd), it suffices to show that 

Pd ËœXâŠ¤ 

> 1

ËœX1PdM2â†’1Ëœz = Pd ËœXâŠ¤

> 1



ËœXâ‰¤2



Ëœz + 1d2

Î½(D)

d2



âˆ’ ËœX11d

Î½(D)

d



= Pd ËœXâŠ¤ 

> 1

ËœXâ‰¤2Ëœz + Pd



ËœXâŠ¤ 

> 1

ËœXâ‰¤21d2

Î½(D)

d2

âˆ’ ËœXâŠ¤ 

> 1

ËœXâ‰¤11d

Î½(D)

d



. (29) We will first show that the second term on the right hand side is 0. First, notice that 

[ ËœXâ‰¤k1dk ]S = X  

> TâŠ†D:|T|â‰¤ k

âˆšÎ¼S

âˆšpS

1[T âŠ† S] = 

âˆšÎ¼S

âˆšpS

|S|k (30) where |S|k = Pkâ„“=1 

 |S|

> â„“

. Then 

1

dk

[ ËœXâŠ¤ 

> 1

ËœXâ‰¤1dk ]i = X

> S:iâˆˆS

Î¼S

pS

|S|k

dk

. (31) We have |S|2 

> d2

= |S|+(|S| 

> 2

)

> d+

(dk) = |S| 

> d
> 1+( |S|âˆ’ 1) /21+( dâˆ’1) /2

= |S| 

> d

Â· |S|+1  

> d+1

. Together, 



ËœXâŠ¤ 

> 1

ËœXâ‰¤21d2

1

d2

âˆ’ ËœXâŠ¤ 

> 1

ËœX11d

1

d



> i

= X

> S:iâˆˆS

Î¼S

pS

|S|

d

 |S| + 1 

d + 1 âˆ’ d + 1 

d + 1 



= 1

d(d + 1) 

X

> S:iâˆˆS

Î¼S

pS

|S|(|S| âˆ’ d)= 12d(d + 1) 

X

> S

Î¼S

pS

|S|(|S| âˆ’ d) (32) where the last equality follows because the subsets are sampled in paired complements. In particular, for a given pair 

S and D \ S, the item i is in exactly one of them, and the coefficient Î¼S 

> pS

|S|(|S| âˆ’ d) is the same for both. We have shown that every entry is the same, i.e., a scaling of 1, so Pd projects off the entire vector. Finally, it remains to show that 

Pd ËœXâŠ¤ 

> 1

ËœX1PdM2â†’1Ëœz = Pd ËœXâŠ¤ 

> 1

ËœXâ‰¤2Ëœz. (33) It is easy to verify that âŸ¨1d, MËœzâŸ© = âŸ¨1d, ËœzâŸ© = 0 , so PdM2â†’1Ëœz = M2â†’1Ëœz. Therefore, it suffices to prove that 

Pd ËœXâŠ¤ 

> 1

ËœX1M2â†’1 = Pd ËœXâŠ¤ 

> 1

ËœXâ‰¤2.Notice that [ ËœXâŠ¤ 

> 1

ËœX1]i,j = P 

> S:iâˆˆS,j âˆˆSÎ¼S
> pS

where i, j âˆˆ D. Then 

[ ËœXâŠ¤ 

> 1

ËœX1M2â†’1]i,R =

> d

X

> j=1

1[j âˆˆ R]

|R|

X

> S:iâˆˆS,j âˆˆS

Î¼S

pS

= X

> S:iâˆˆS

Î¼S

pSdX

> j=1: jâˆˆS,j âˆˆR

1

|R|

= X

> S:iâˆˆS

Î¼S

pS

|R âˆ© S||R| . (34) Meanwhile, 

[ ËœXâŠ¤ 

> 1

ËœXâ‰¤2]i,R = X

> S:iâˆˆS,R âŠ†[S]

Î¼S

pS

. (35) Clearly, Equations 35 and 34 are equal when |R| = 1 . Now consider the case when |R| = 2 ; we have 

(34) = X

> S:iâˆˆS, |Râˆ©S|=1

Î¼S

pS

12 + X

> S:iâˆˆS, |Râˆ©S|=2

Î¼S

pS

= 14

X

> S:|Râˆ©S|=1

Î¼S

pS

+ X

> S:iâˆˆS,R âŠ†S

Î¼S

pS

. (36) where the last equality follows by sampling in paired complements. In particular, exactly one of the paired samples S

and D âŠ† S will contain item i, and the coefficient Î¼S /p S is the same for both. Finally, because its the same for all i,the projection Pd eliminates the first term. The statement follows. 18 A.4 kADD -SHAP C ONVERGES TO THE SHAPLEY VALUE 

In this section, we prove Proposition 4.6 and discuss the differences between PolySHAP and kADD -SHAP, and its practical implications. We generally recommend to prefer PolySHAP over kADD -SHAP. 

Proposition 4.6. kADD -SHAP converges to the Shapley value for k = 1 , . . . , d .Proof. The kADD approximation algorithm (Pelegrina et al., 2023) is based on the interaction representation (Grabisch et al., 2000) of Î½ given by 

Î½(S) = X 

> TâŠ†D

Î³|T ||Sâˆ©T |ISh (T ) with Î³tr := 

> r

X

> â„“=0

râ„“



Btâˆ’â„“,

where Bt are the Bernoulli numbers and ISh the Shapley interaction index (Grabisch & Roubens, 1999) with 

ISh (S) := X 

> TâŠ†D\S

1(d âˆ’ | S| + 1)  dâˆ’| S||T |

X

> LâŠ†S

(âˆ’1) |S|âˆ’| L|Î½(T âˆª L).

The Shapley interaction index generalizes the Shapley value to arbitrary subsets, and it holds Ï•SV  

> i

[Î½] = ISh (i) for all 

i âˆˆ D. The kADD -SHAP approximation algorithm then restricts this representation to interactions up to order k.

Definition A.3 (kADD -SHAP (Pelegrina et al., 2025)) . The kADD -SHAP algorithms solves the constrained weighted least-squares problem 

IkADD := arg min  

> IâˆˆR
> Pkâ„“=0

(dâ„“)

X

> SâŠ†D

Î¼(S)

ï£«ï£­Î½(S) âˆ’ X  

> TâŠ†D:|T|â‰¤ k

Î³|T ||Sâˆ©T |IT

ï£¶ï£¸

> 2

s.t. Î½(D) âˆ’ Î½(âˆ…) = X  

> TâŠ†D:|T|â‰¤ k



Î³|T ||T | âˆ’ Î³|T |

> 0



IT .

In practice, the least-squares objective is approximated and solved similar to KernelSHAP (Lundberg & Lee, 2017), and the Shapley value estimates that are output are IkADD  

> i

for i âˆˆ D from the approximated least-squares system. 

Our first observation is that the output Ii is the Shapley value of the approximated game, i.e. 

Ï•SV  

> i

[ X  

> TâŠ†D:|T|â‰¤ k

Î³|T | 

> 1[iâˆˆT]

IT ] = Ii.

We will show that the Shapley values of this approximation are the Shapley values of the PolySHAP representation 

Ï•Iâ‰¤k , which then are equal to the Shapley values of Î½ by Theorem 4.3. In contrast to PolySHAP, kADD -SHAP fits a coefficient for the empty set Ï•âˆ…. However, we may rewrite 

ï£«ï£­Î½(S) âˆ’ X  

> TâŠ†D:|T|â‰¤ k

Î³|T ||Sâˆ©T |IT

ï£¶ï£¸

> 2

=

ï£«ï£­Î½(S) âˆ’ Î³00 Iâˆ… âˆ’ X  

> TâŠ†D:0 <|T|â‰¤ k

Î³|T ||Sâˆ©T |IT

ï£¶ï£¸

> 2

,

and thus Iâˆ… is an additive shift of Î½, which does not affect the Shapley values of the approximation, i.e. 

Ï•SV  

> i

[ X  

> TâŠ†D:|T|â‰¤ k

Î³|T ||Sâˆ©T |IT ] = Ï•SV  

> i

[ X  

> TâŠ†D:0 <|T|â‰¤ k

Î³|T ||Sâˆ©T |IT ].

Moreover, we can compute Î³t 

> 0

= Bt and 

Î³ss =

> s

X

> â„“=0

sâ„“



Bsâˆ’â„“ =

> s

X

> â„“=0

sâ„“



Bâ„“ =

> sâˆ’1

X

> â„“=0

sâ„“



Bâ„“ + Bs = 1[s = 1] + Bs,

by the recursion of Bernoulli numbers, and thus 

X

> SâŠ†D:|S|â‰¤ k



Î³|S||S| âˆ’ Î³|S|

> 0



IS = X

> iâˆˆD

Ii,

19 which is already mentioned by Pelegrina et al. (2025)[Proof of Theorem 4.2]. Now, without loss of generality, we can assume that Î½(âˆ…) = 0 , since it does not affect the Shapley values of Î½, and thus the class of approximations is given by 

FkADD := 

ï£±ï£²ï£³S 7 â†’ X  

> TâŠ†D:0 <|T|â‰¤ k

Î³|T ||Sâˆ©T |IT : Ï• âˆˆ Rd+|I â‰¤k | and X

> iâˆˆD

Ii = Î½(D)

ï£¼ï£½ï£¾ .

Lemma A.4. There is an equivalence between the function class FkADD and the class of functions of PolySHAP representation with interaction frontier Iâ‰¤k, i.e. 

FkADD =

ï£±ï£²ï£³S 7 â†’ X 

> TâˆˆDâˆªI â‰¤k

Ï•T

Y

> jâˆˆT

1[j âˆˆ S] : Ï• âˆˆ Rd+|I â‰¤k | and âŸ¨Ï•, 1âŸ© = Î½(D)

ï£¼ï£½ï£¾

Proof. For the game Î½ there exist the two equivalent representations (Grabisch et al., 2000)[Table 3 and 4] 

Î½(S) = X 

> TâŠ†D

Î³|T ||Sâˆ©T |ISh (T ) with Î³tr := 

> r

X

> â„“=0

râ„“



Btâˆ’â„“,

where ISh is the Shapley interaction index (Grabisch & Roubens, 1999), and the MÂ¨ obius representation 

Î½(S) = X 

> TâŠ†D

m(S) Y

> jâˆˆS

1[j âˆˆ S] with m(S) := X

> LâŠ†S

(âˆ’1) |S|âˆ’| L|Î½(L).

Moreover, there exist the two conversion formulas (Grabisch et al., 2000)[Table 3 and 4] 

ISh (S) = X  

> TâŠ†D:TâŠ‡S

1

|T | âˆ’ | S| + 1 m(T ) and m(S) = X  

> TâŠ†D:TâŠ‡S

B|T |âˆ’| S|ISh (T ).

From the conversion formulas it is obvious that 

ISh (S) = 0 , âˆ€S âŠ† D : |S| > k â‡” m(S), âˆ€S âŠ† D : |S| > k. 

Hence, restricting the interaction representation to order k yields the same function class as restricting the MÂ¨ obius representation to order k. Moreover, the constraints are similarly converted, which proves the equivalence. Utilizing Lemma A.4, we obtain that for 

Ik+ 

> ADD

:= arg min  

> IâˆˆR
> Pkâ„“=1

(dâ„“)

X

> SâŠ†D

Î¼(S)

ï£«ï£­Î½(S) âˆ’ X  

> TâŠ†D:|T|â‰¤ k

Î³|T ||Sâˆ©T |IT

ï£¶ï£¸

> 2

s.t. Î½(D) = X

> iâˆˆD

Ii

we have equivalence between the approximations 

X  

> TâŠ†D:|T|â‰¤ k

Î³|T ||Sâˆ©T |Ik+ 

> ADD
> T

= X 

> TâˆˆDâˆªI â‰¤k

Ï•Iâ‰¤k

> T

Y

> jâˆˆT

1[j âˆˆ S],

where Ï•Iâ‰¤k 

> T

is the PolySHAP representation, due to the equivalent function classes parametrized by the vectors Ik+

> ADD

and Ï•Iâ‰¤k . By Theorem 4.3, we know that the Shapley values of this approximation are equal to the Shapley values of 

Î½, and hence, we have 

IkADD  

> i

= Ï•SV  

> i

[ X  

> TâŠ†D:|T|â‰¤ k

Î³|T ||Sâˆ©T |IkADD  

> T

] = Ï•SV  

> i

[ X  

> TâŠ†D:|T|â‰¤ k

Î³|T ||Sâˆ©T |Ik+ 

> ADD
> T

] = Ï•SV  

> i

[Î½],

which concludes the proof and show convergence of kADD -SHAP to the Shapley value. 

Practical difference between kADD -SHAP and PolySHAP. In contrast to PolySHAP, kADD -SHAP was proposed for k-additive interaction frontiers. Moreover, the design matrix of kADD -SHAP is less intuitive, making the PolySHAP formulation a simpler and more transparent alternative. More importantly, ta key practical difference arises from our use of the modified representation Ik+ 

> ADD

as an intermediate step in the proof. While Ik+ 

> ADD

and IkADD yield the same Shapley values when all subsets are evaluated, they diverge under approximation. In particular, unlike PolySHAP, 

kADD -SHAP is affected by the value of Î½(âˆ…), and its least-squares fit includes an additional variable. For these reasons, we recommend PolySHAP in practice over kADD -SHAP. 20 Table 3: Datasets used for tabular explanation games 

Name (ID in bold) Reference License Source 

California Housing (Kelley Pace & Barry, 1997) Public Domain sklearn 

Bike Regression (Fanaee-T & Gama, 2014) CC-BY 4.0 OpenML 

Forest Fires (Cortez & Morais, 2007) CC-BY 4.0 UCI Repo 

Adult Census (Kohavi, 1996) CC-BY 4.0 OpenML Real Estate (Yeh & Hsu, 2018) CC-BY 4.0 UCI Repo Breast Cancer (Street et al., 1993) CC-BY 4.0 shap 

Correlated Groups ( CG60 ) synthetic MIT shap 

Independent Linear ( IL60 ) synthetic MIT shap 

NHANES I (Dinh et al., 2019) Public Domain shap 

Communities and Crime (Redmond, 2011) CC-BY 4.0 shap 

B EXPERIMENTAL DETAILS AND ADDITIONAL RESULTS 

In this section, we provide additional details regarding our experiments and the local explanation game setup (Sec-tion B.1) with additional results on the remaining games using MSE (Section B.2), Precision@5 (Section B.3), and Spearman correlation (Section B.4). Lastly, we report results of the runtime analysis (Section B.5). B.1 EXPERIMENTAL DETAILS 

All experiments were conducted on a consumer-grade laptop with an 11th Gen Intel Core i7-11850H CPU and 30GB of RAM, where we used cuda 2 on a NVIDIA RTX A2000 GPU for inference of the CIFAR10 game. 

Non-tabular Datasets. We used the 30 pre-computed games provided by the shapiq benchmark for the ResNET18 (He et al., 2016), and the vision transformers pre-trained on ImageNet (Deng et al., 2009). We used the pre-computed language game using a DistilBERT (Sanh et al., 2019) model and sentiment analysis on the IMDB dataset (Maas et al., 2011) from the shapiq benchmark. Lastly for the CIFAR10 game, we used a vision transformer (vit-base-patch16-224-in21k) (Dosovitskiy et al., 2021) fine-tuned on CIFAR10 (Krizhevsky et al., 2009), which is publicly available 3.

Datasets. The datasets and their source used for the tabular explanation games are described in Table 3. The Forest Fires 4 and Real Estate 5 were sourced from UCI Machine Learning Repository (UCI Repo), whereas Bike Regres-sion was taken from OpenML (Feurer et al., 2020). The California Housing dataset was sourced from scikit-learn (Pedregosa et al., 2011)[ sklearn ], and the remaining datasets were sourced from the shap 6 library. 

Random forest configuration. We use the standard implementation for RandomForestRegressor and 

RandomForestClassifier from scikit-learn (Pedregosa et al., 2011) with 10 tree instances of maximum depth 10 and fit the training data using accuracy (classification) and F1-score (regression). For all datasets, a 80 /20 

percent train-test-split was executed. 

RegressionMSR. For the RegressionMSR approach, we use XGBoost (Chen & Guestrin, 2016) with its default configuration as a tree-based backbone combined with the MonteCarlo approximator (equivalent to MSR (Witter et al., 2025)) from the shapiq package. 

MSR and Unbiased KernelSHAP. It was shown (Fumagalli et al., 2023)[Theorem 4.5] that MSR (Wang & Jia, 2023) is equivalent to Unbiased KernelSHAP (Covert & Lee, 2021). We use the implementation of Unbiased Ker-nelSHAP provided in the shapiq package. 

> 2https://developer.nvidia.com/cuda-toolkit
> 3https://huggingface.co/aaraki/vit-base-patch16-224-in21k-finetuned-cifar10
> 4https://archive.ics.uci.edu/ml/datasets/forest+fires
> 5https://archive.ics.uci.edu/dataset/477/real+estate+valuation+data+set
> 6https://shap.readthedocs.io/en/latest/

21 0 40         

> (16%)
> 80
> (31%)
> 120
> (47%)
> 160
> (62%)
> 200
> (78%)
> 240
> (94%)
> Budget ( m)
> 10 9
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> MSE Â± SEM

Housing ( d = 8) 0 80          

> (16%)
> 160
> (31%)
> 240
> (47%)
> 320
> (62%)
> 400
> (78%)
> 480
> (94%)
> Budget ( m)
> 10 9
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> MSE Â± SEM

ViT9 ( d = 9) 0 3000        

> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)
> Budget ( m)
> 10 9
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> MSE Â± SEM

Adult ( d = 14) 0 3000          

> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)
> Budget ( m)
> 10 9
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> MSE Â± SEM

DistilBERT ( d = 14) 0 4000      

> (12%)
> 8000
> (24%)
> 12000
> (37%)
> 16000
> (49%)
> 20000
> (61%)
> Budget ( m)
> 10 9
> 10 7
> 10 5
> 10 3
> 10 1
> 10 1
> MSE Â± SEM

Estate ( d = 15) 0 4000      

> (6%)
> 8000
> (12%)
> 12000
> (18%)
> 16000
> (24%)
> 20000
> (31%)
> Budget ( m)
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> MSE Â± SEM

CIFAR-10 ViT16 ( d = 16) 0 4000      

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> MSE Â± SEM

IL60 ( d = 60) Figure 5: Approximation quality measured by MSE ( Â± SEM) for varying budget ( m) on remaining explanation games. Adding interactions in PolySHAP can substantially improve approximation quality 

SVARM Shapley Value Approximation without Requesting Marginals (SVARM) was proposed by (Kolpaczki et al., 2024) and uses stratification of MSR (Castro et al., 2017). We use the implementation of SVARM provided in the 

shapiq package. B.2 ADDITIONAL RESULTS ON APPROXIMATION QUALITY USING MSE In this section, we report approximation quality measured by MSE for the remaining explanation games. Figure 5 reports the MSE for the Housing , ViT9 , Adult , DistilBERT , Estate , and IL60 explanation games. Similar to Figure 2, we observe that PolySHAPâ€™s approximation quality substantially improves with higher-order interactions. Again, this comes at the cost of larger budget requirements, indicated by the delay of the line plots. The Permutation Sampling and KernelSHAP (1-PolySHAP) baseline are consistently outperformed by higher-order PolySHAP, while RegressionMSR yields comparable results. Figure 6 shows the approximation quality of PolySHAP with and without (standard) paired subset sampling. Similar to Figure 3, we observe a strong improvement of 1-PolySHAP due to the equivalence to 2-PolySHAP. The same observation holds for 3-PolySHAP. 22 0 40 

(16%) 

80 

(31%) 

120 

(47%) 

160 

(62%) 

200 

(78%) 

240 

(94%) 

Budget ( m)

10 9

10 8

10 7

10 6

10 5

10 4

10 3

10 2

> MSE Â± SEM

Housing ( d = 8) 0 80 

(16%) 

160 

(31%) 

240 

(47%) 

320 

(62%) 

400 

(78%) 

480 

(94%) 

Budget ( m)

10 9

10 8

10 7

10 6

10 5

10 4

10 3

10 2

10 1

> MSE Â± SEM

ViT9 ( d = 9) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

10 9

10 8

10 7

10 6

10 5

10 4

10 3

> MSE Â± SEM

Adult ( d = 14) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

10 9

10 8

10 7

10 6

10 5

10 4

10 3

10 2

10 1

> MSE Â± SEM

DistilBERT ( d = 14) 0 4000 

(12%) 

8000 

(24%) 

12000 

(37%) 

16000 

(49%) 

20000 

(61%) 

Budget ( m)

10 9

10 7

10 5

10 3

10 1

10 1

> MSE Â± SEM

Estate ( d = 15) 0 4000 

(6%) 

8000 

(12%) 

12000 

(18%) 

16000 

(24%) 

20000 

(31%) 

Budget ( m)

10 5

10 4

10 3

10 2

10 1

10 0

> MSE Â± SEM

CIFAR-10 ViT16 ( d = 16) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

10 5

10 4

10 3

10 2

10 1

10 0

> MSE Â± SEM

IL60 ( d = 60) Figure 6: Approximation quality measured by MSE ( Â± SEM) for standard (dotted) and paired (solid) sampling for remaining local explanation games. Under paired sampling, 2-PolySHAP marginally improves, whereas KernelSHAP substantially improves due to its equivalence to 2-PolySHAP. MSR Permutation Sampling 3-PolySHAP (log) RegressionMSR SVARM UnbiasedKernelSHAP 3-PolySHAP (50%) 0 40 

(16%) 

80 

(31%) 

120 

(47%) 

160 

(62%) 

200 

(78%) 

240 

(94%) 

Budget ( m)

10 9

10 8

10 7

10 6

10 5

10 4

10 3

10 2

10 1

> MSE Â± SEM

Housing ( d = 8) 0 80 

(16%) 

160 

(31%) 

240 

(47%) 

320 

(62%) 

400 

(78%) 

480 

(94%) 

Budget ( m)

10 9

10 8

10 7

10 6

10 5

10 4

10 3

10 2

10 1

> MSE Â± SEM

ViT9 ( d = 9) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

10 9

10 8

10 7

10 6

10 5

10 4

10 3

10 2

> MSE Â± SEM

Adult ( d = 14) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

10 9

10 8

10 7

10 6

10 5

10 4

10 3

10 2

10 1

> MSE Â± SEM

DistilBERT ( d = 14) 0 4000 

(12%) 

8000 

(24%) 

12000 

(37%) 

16000 

(49%) 

20000 

(61%) 

Budget ( m)

10 9

10 7

10 5

10 3

10 1

10 1

> MSE Â± SEM

Estate ( d = 15) 0 4000 

(6%) 

8000 

(12%) 

12000 

(18%) 

16000 

(24%) 

20000 

(31%) 

Budget ( m)

10 5

10 4

10 3

10 2

10 1

10 0

> MSE Â± SEM

CIFAR-10 ViT16 ( d = 16) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

10 5

10 4

10 3

10 2

10 1

> MSE Â± SEM

IL60 ( d = 60) 

Figure 7: Approximation quality of PolySHAP variants and baselines measured by MSE ( Â± SEM) for paired sampling for remaining local explanation games. 23 0 40 

> (16%)
> 80
> (31%)
> 120
> (47%)
> 160
> (62%)
> 200
> (78%)
> 240
> (94%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Housing ( d = 8) 0 80 

> (16%)
> 160
> (31%)
> 240
> (47%)
> 320
> (62%)
> 400
> (78%)
> 480
> (94%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

ViT9 ( d = 9) 0 800 

> (20%)
> 1600
> (39%)
> 2400
> (59%)
> 3200
> (78%)
> 4000
> (98%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Bike ( d = 12) 0 1500 

> (18%)
> 3000
> (37%)
> 4500
> (55%)
> 6000
> (73%)
> 7500
> (92%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Forest ( d = 13) 0 3000 

> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Adult ( d = 14) 0 3000 

> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

ResNet18 ( d = 14) 0 3000 

> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

DistilBERT ( d = 14) 0 4000 

> (12%)
> 8000
> (24%)
> 12000
> (37%)
> 16000
> (49%)
> 20000
> (61%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Estate ( d = 15) 0 4000 

> (6%)
> 8000
> (12%)
> 12000
> (18%)
> 16000
> (24%)
> 20000
> (31%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

ViT16 ( d = 16) 0 4000 

> (6%)
> 8000
> (12%)
> 12000
> (18%)
> 16000
> (24%)
> 20000
> (31%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

CIFAR-10 ViT16 ( d = 16) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Cancer ( d = 30) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

CG60 ( d = 60) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

IL60 ( d = 60) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

NHANES ( d = 79) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Crime ( d = 101) Figure 8: Approximation quality measured by Precision@5 ( Â± SEM) for varying budget ( m) on different games. Adding interactions in PolySHAP can substantially improve approximation quality B.3 APPROXIMATION QUALITY USING PRECISION @5 In this section, we report approximation quality with respect to top-5 precision ( Precision@5 ) for all explanation games from Table 2. In Figure 8 that higher-order interactions also improve the approximation quality regarding the Precision@5 metric. However, the distinction is not as clear as for MSE, since ranking is not considered in the optimization objective. In general, the approximation quality varies across different games, where the low-dimensional tabular explanation games show very good results, in contrast to the more challenging non-tabular games (ViT9, DistilBERT, ResNet18 and ViT16), and high-dimensional games (CG60, IL60, NHANES, and Crime), which require more budget for similar results. In Figure 8 Precision@5 is compared for standard sampling and paired sampling. Again, we observe improvements for 1-PolySHAP and 3-PolySHAP when using paired sampling due to its equivalence to 2-PolySHAP and 4-PolySHAP, respectively. In Figure 10, we report the Precision@5 metric for the PolySHAP variants and the baselines for paired sampling. Again, we observe state-of-the-art performance for PolySHAP and RegressionMSR. PolySHAPâ€™s performance sub-stantially improves, if the budget allows to capture order-3 interactions. 24 0 40 

> (16%)
> 80
> (31%)
> 120
> (47%)
> 160
> (62%)
> 200
> (78%)
> 240
> (94%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Housing ( d = 8) 0 80 

> (16%)
> 160
> (31%)
> 240
> (47%)
> 320
> (62%)
> 400
> (78%)
> 480
> (94%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

ViT9 ( d = 9) 0 800 

> (20%)
> 1600
> (39%)
> 2400
> (59%)
> 3200
> (78%)
> 4000
> (98%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Bike ( d = 12) 0 1500 

> (18%)
> 3000
> (37%)
> 4500
> (55%)
> 6000
> (73%)
> 7500
> (92%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Forest ( d = 13) 0 3000 

> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Adult ( d = 14) 0 3000 

> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

ResNet18 ( d = 14) 0 3000 

> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

DistilBERT ( d = 14) 0 4000 

> (12%)
> 8000
> (24%)
> 12000
> (37%)
> 16000
> (49%)
> 20000
> (61%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Estate ( d = 15) 0 4000 

> (6%)
> 8000
> (12%)
> 12000
> (18%)
> 16000
> (24%)
> 20000
> (31%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

ViT16 ( d = 16) 0 4000 

> (6%)
> 8000
> (12%)
> 12000
> (18%)
> 16000
> (24%)
> 20000
> (31%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

CIFAR-10 ViT16 ( d = 16) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Cancer ( d = 30) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

CG60 ( d = 60) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

IL60 ( d = 60) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

NHANES ( d = 79) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Precision@5 Â± SEM

Crime ( d = 101) Figure 9: Approximation quality measured by Precision@5 ( Â± SEM) for standard (dotted) and paired (solid) sampling. Under paired sampling, 2-PolySHAP marginally improves, whereas KernelSHAP substantially improves due to its equivalence to 2-PolySHAP B.4 APPROXIMATION QUALITY USING SPEARMAN CORRELATION 

In this section, we report approximation quality with respect to Spearman correlation ( SpearmanCorrelation ) for all explanation games from Table 2. Figure 11 reports Spearman correlation of PolySHAP and the baseline methods. Again, we observe consistent im-provements of higher-order interactions in this metric. For high-dimensional settings ( â‰¥ 60) , we further observe that the baselines clearly outperform PolySHAP in this metric. Since we have seen that PolySHAP performs very well in the Precision@5 metric, we conjecture that this difference is mainly due features with lower absolute Shapley values. In Figure 12, we observe a similar pattern as with MSE and Precision@5. Using paired sampling drastically improves the approximation quality of 1-PolySHAP, due to its equivalence to 2-PolySHAP. Since 3-PolySHAP often performs very well in this metric, we do not observe strong differences between 3-PolySHAP and 4-PolySHAP in both sampling settings. In Figure 13, we report SpearmanCorrelation for the PolySHAP variants and the baseline methods under paired sam-pling. Again, we observe state-of-the-art performance for PolySHAP and the RegressionMSR baseline. PolySHAP substantially improves, if the budget allows to capture order-3 interactions. 25 MSR Permutation Sampling 3-PolySHAP (log) RegressionMSR SVARM UnbiasedKernelSHAP 3-PolySHAP (50%) 0 40 

(16%) 

80 

(31%) 

120 

(47%) 

160 

(62%) 

200 

(78%) 

240 

(94%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

Housing ( d = 8) 0 80 

(16%) 

160 

(31%) 

240 

(47%) 

320 

(62%) 

400 

(78%) 

480 

(94%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

ViT9 ( d = 9) 0 800 

(20%) 

1600 

(39%) 

2400 

(59%) 

3200 

(78%) 

4000 

(98%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

Bike ( d = 12) 0 1500 

(18%) 

3000 

(37%) 

4500 

(55%) 

6000 

(73%) 

7500 

(92%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

Forest ( d = 13) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

Adult ( d = 14) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

ResNet18 ( d = 14) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

DistilBERT ( d = 14) 0 4000 

(12%) 

8000 

(24%) 

12000 

(37%) 

16000 

(49%) 

20000 

(61%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

Estate ( d = 15) 0 4000 

(6%) 

8000 

(12%) 

12000 

(18%) 

16000 

(24%) 

20000 

(31%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

ViT16 ( d = 16) 0 4000 

(6%) 

8000 

(12%) 

12000 

(18%) 

16000 

(24%) 

20000 

(31%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

CIFAR-10 ViT16 ( d = 16) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

Cancer ( d = 30) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

CG60 ( d = 60) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

IL60 ( d = 60) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

NHANES ( d = 79) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> Precision@5 Â± SEM

Crime ( d = 101) Figure 10: Approximation quality of PolySHAP variants and baselines measured by Precision@5 ( Â± SEM) for paired sampling 26 0 40 

(16%) 

80 

(31%) 

120 

(47%) 

160 

(62%) 

200 

(78%) 

240 

(94%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Housing ( d = 8) 0 80 

(16%) 

160 

(31%) 

240 

(47%) 

320 

(62%) 

400 

(78%) 

480 

(94%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

ViT9 ( d = 9) 0 800 

(20%) 

1600 

(39%) 

2400 

(59%) 

3200 

(78%) 

4000 

(98%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Bike ( d = 12) 0 1500 

(18%) 

3000 

(37%) 

4500 

(55%) 

6000 

(73%) 

7500 

(92%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Forest ( d = 13) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Adult ( d = 14) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

ResNet18 ( d = 14) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

DistilBERT ( d = 14) 0 4000 

(12%) 

8000 

(24%) 

12000 

(37%) 

16000 

(49%) 

20000 

(61%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Estate ( d = 15) 0 4000 

(6%) 

8000 

(12%) 

12000 

(18%) 

16000 

(24%) 

20000 

(31%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

ViT16 ( d = 16) 0 4000 

(6%) 

8000 

(12%) 

12000 

(18%) 

16000 

(24%) 

20000 

(31%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

CIFAR-10 ViT16 ( d = 16) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Cancer ( d = 30) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

CG60 ( d = 60) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

IL60 ( d = 60) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

NHANES ( d = 79) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Crime ( d = 101) Figure 11: Approximation quality measured by SpearmanCorrelation ( Â± SEM) for varying budget ( m) on different games. Adding interactions in PolySHAP can substantially improve approximation quality 27 0 40 

> (16%)
> 80
> (31%)
> 120
> (47%)
> 160
> (62%)
> 200
> (78%)
> 240
> (94%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

Housing ( d = 8) 0 80 

> (16%)
> 160
> (31%)
> 240
> (47%)
> 320
> (62%)
> 400
> (78%)
> 480
> (94%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

ViT9 ( d = 9) 0 800 

> (20%)
> 1600
> (39%)
> 2400
> (59%)
> 3200
> (78%)
> 4000
> (98%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

Bike ( d = 12) 0 1500 

> (18%)
> 3000
> (37%)
> 4500
> (55%)
> 6000
> (73%)
> 7500
> (92%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

Forest ( d = 13) 0 3000 

> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

Adult ( d = 14) 0 3000 

> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

ResNet18 ( d = 14) 0 3000 

> (18%)
> 6000
> (37%)
> 9000
> (55%)
> 12000
> (73%)
> 15000
> (92%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

DistilBERT ( d = 14) 0 4000 

> (12%)
> 8000
> (24%)
> 12000
> (37%)
> 16000
> (49%)
> 20000
> (61%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

Estate ( d = 15) 0 4000 

> (6%)
> 8000
> (12%)
> 12000
> (18%)
> 16000
> (24%)
> 20000
> (31%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

ViT16 ( d = 16) 0 4000 

> (6%)
> 8000
> (12%)
> 12000
> (18%)
> 16000
> (24%)
> 20000
> (31%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

CIFAR-10 ViT16 ( d = 16) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

Cancer ( d = 30) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

CG60 ( d = 60) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

IL60 ( d = 60) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

NHANES ( d = 79) 0 4000 

> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)

Budget ( m)

> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> SpearmanCorrelation Â± SEM

Crime ( d = 101) Figure 12: Approximation quality measured by SpearmanCorrelation ( Â± SEM) for standard (dotted) and paired (solid) sampling. Under paired sampling, 2-PolySHAP marginally improves, whereas KernelSHAP substantially improves due to its equivalence to 2-PolySHAP B.5 RUNTIME ANALYSIS 

In this section, we analyze the runtime of PolySHAP and the RegressionMSR baseline, since both methods approxi-mate the game values, and subsequently extract Shapley value estimates. Figure 14 reports the runtime in seconds (log-scale) of PolySHAP and RegressionMSR for the spent budget on dif-ferent explanation games. As expected, we observe a linear relationship between the budget m and the computation time in PolySHAP, indicated by the overlapping linear fits (dashed lines). Overall, the computational overhead of the computations executed in RegressionMSR and PolySHAP variants after game evaluations will be negligible in most application settings. 

Complexity of Evaluations. In realistic application settings, the runtime for game evaluations should be considered a main driver of computational complexity of PolySHAP and RegressionMSR. This is verified by the CIFAR10 ViT16 game in Figure 14, a), which requires one model call of the ViT16 for each game evaluation. The computational difference between RegressionMSR and all PolySHAP variants are thereby negligible. For the runtime of the path-dependent tree games, reported in Figure 14, b) the game evaluations require only a single pass through the random forests, which becomes negligible with increasing dimensionality. 28 MSR Permutation Sampling 3-PolySHAP (log) RegressionMSR SVARM UnbiasedKernelSHAP 3-PolySHAP (50%) 0 40 

(16%) 

80 

(31%) 

120 

(47%) 

160 

(62%) 

200 

(78%) 

240 

(94%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Housing ( d = 8) 0 80 

(16%) 

160 

(31%) 

240 

(47%) 

320 

(62%) 

400 

(78%) 

480 

(94%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

ViT9 ( d = 9) 0 800 

(20%) 

1600 

(39%) 

2400 

(59%) 

3200 

(78%) 

4000 

(98%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Bike ( d = 12) 0 1500 

(18%) 

3000 

(37%) 

4500 

(55%) 

6000 

(73%) 

7500 

(92%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Forest ( d = 13) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Adult ( d = 14) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

ResNet18 ( d = 14) 0 3000 

(18%) 

6000 

(37%) 

9000 

(55%) 

12000 

(73%) 

15000 

(92%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

DistilBERT ( d = 14) 0 4000 

(12%) 

8000 

(24%) 

12000 

(37%) 

16000 

(49%) 

20000 

(61%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Estate ( d = 15) 0 4000 

(6%) 

8000 

(12%) 

12000 

(18%) 

16000 

(24%) 

20000 

(31%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

ViT16 ( d = 16) 0 4000 

(6%) 

8000 

(12%) 

12000 

(18%) 

16000 

(24%) 

20000 

(31%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

CIFAR-10 ViT16 ( d = 16) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Cancer ( d = 30) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

CG60 ( d = 60) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

IL60 ( d = 60) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

NHANES ( d = 79) 0 4000 

(<1%) 

8000 

(<1%) 

12000 

(<1%) 

16000 

(<1%) 

20000 

(<1%) 

Budget ( m)

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

> SpearmanCorrelation Â± SEM

Crime ( d = 101) Figure 13: Approximation quality of PolySHAP variants and baselines measured by SpearmanCorrelation ( Â± SEM) for paired sampling 29 Linear Fit Runtime 0 4000 

(6%) 

8000 

(12%) 

12000 

(18%) 

16000 

(24%) 

20000 

(31%) 

Budget ( m)

> 10 0
> 10 1
> 10 2
> Time (seconds)

CIFAR10-ViT16 ( d = 16) a) Real-World ViT Inference Game 0 40               

> (16%)
> 80
> (31%)
> 120
> (47%)
> 160
> (62%)
> 200
> (78%)
> 240
> (94%)
> 280
> (109%)
> Budget ( m)
> 10 1
> Time (seconds)
> Housing ( d= 8) 04000
> (12%)
> 8000
> (24%)
> 12000
> (37%)
> 16000
> (49%)
> 20000
> (61%)
> Budget ( m)
> 10 2
> 10 1
> 10 0
> 10 1
> Time (seconds)
> Estate ( d= 15) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 2
> 10 1
> 10 0
> 10 1
> Time (seconds)
> Cancer ( d= 30) 04000
> (<1%)
> 8000
> (<1%)
> 12000
> (<1%)
> 16000
> (<1%)
> 20000
> (<1%)
> Budget ( m)
> 10 1
> 10 0
> 10 1
> Time (seconds)
> IL60 ( d= 60)

b) Path-dependent Tree Games 

Figure 14: Runtime in seconds (log-scale) of PolySHAP and RegressionMSR for varying budgets ( m) of a) a real-world ViT inference game on CIFAR10, and b) selected path-dependent tree games of varying dimensionality. The runtime increases linearly with the budget m, indicated by the linear fit (dashed line). As expected in practice, the runtime of the real-world ViT inference game on CIFAR10 is dominated by the model calls required for each budget. The computational difference of the approximators are negligible. For path-dependent tree games that require only a single tree traversal per game evaluation, the runtime increases for higher-order PolySHAP due to the increasing number of regression variables with stronger effects in high-dimensional games. 

Complexity of Computation. The computational overhead of RegressionMSR and PolySHAP variants besides the game evaluations is negligible in many application settings. However, there is an impact on runtime for the higher-order k-PolySHAP variants, due to the increasing number of regression variables that yield a polynomial increase of computation time. For the tree-path dependent games in Figure 14, b) this effect is visible due to the very efficient computation of game values. The RegressionMSR method utilizes the XGBoost library (Chen & Guestrin, 2016), which scales well to high-dimensional problems, indicated by the low runtime observed in Figure 14. The runtime of these computations is generally higher than 1- and 2-PolySHAP, but less than 3- and 4-PolySHAP for high-dimensional problems. 30 C USAGE OF LARGE LANGUAGE MODELS (LLM S)

In this work, we used large language models (LLMs) for suggestions regarding refinement of writing, e.g. grammar, clarity and conciseness. 31