Title: KAN-AE with Non-Linearity Score and Symbolic Regression for Energy-Efficient Channel Coding

URL Source: https://arxiv.org/pdf/2601.01598v1

Published Time: Tue, 06 Jan 2026 03:06:07 GMT

Number of Pages: 7

Markdown Content:
# KAN-AE with Non-Linearity Score and Symbolic Regression for Energy-Efficient Channel Coding 

Anthony Joseph Perre, Parker Huggins, and Alphan ≈ûahin Department of Electrical Engineering, University of South Carolina, Columbia, SC, USA Email: {aperre, parkerkh}@email.sc.edu, asahin@mailbox.sc.edu 

Abstract‚ÄîIn this paper, we investigate Kolmogorov-Arnold network-based autoencoders (KAN-AEs) with symbolic regres-sion (SR) for energy-efficient channel coding. By using SR, we convert KAN-AEs into symbolic expressions, which enables low-complexity implementation and improved energy efficiency at the radios. To further enhance the efficiency, we introduce a new non-linearity score term in the SR process to help select lower-complexity equations when possible. Through numerical simulations, we demonstrate that KAN-AEs achieve competi-tive BLER performance while improving energy efficiency when paired with SR. We score the energy efficiency of a KAN-AE implementation using the proposed non-linearity metric and compare it to a multi-layer perceptron-based autoencoder (MLP-AE). Our experiment shows that the KAN-AE paired with SR uses 1.38 times less energy than the MLP-AE, supporting that KAN-AEs are a promising choice for energy-efficient deep learning-based channel coding. Index Terms‚Äîautoencoder, channel coding, energy effi-ciency, Kolmogorov-Arnold network 

I. Introduction Deep learning (DL) has been demonstrated to enhance traditional signal processing techniques in modern wireless communication systems. For example, DL approaches are used to improve channel estimation and recognize modula-tion type in [1] and [2], respectively. Furthermore, DL has been applied to learn joint source and channel coding tasks within end-to-end orthogonal frequency-division multi-plexing (OFDM) systems [1, 2]. Although the application of DL models to wireless communications shows great promise, their adoption in practical systems faces several challenges. One of the main issues lies in mobile device hardware, where constrained memory resources and CPU capabilities limit the efficacy of large DL models at the radios [3]. Complex models with hundreds of thousands of learnable parameters cause memory and timing issues for mobile devices, which in turn leads to increased energy and power consumption [4]. Hence, there is a clear need for more efficient model architectures that can reduce memory usage and computational load without having to sacrifice performance. Multi-layer perceptrons (MLPs) are a fundamental component of various DL architectures. Recently, a novel DL structure called Kolmogorov-Arnold networks (KANs) 

This work has been supported by the National Science Foundation (NSF) through the award CNS-2438837. 

have emerged as an alternative to MLPs [5]. Studies show that KANs outperform MLPs in accuracy while requiring fewer total parameters [5]. The authors of [6] challenge some of the claims made in [5], but confirm that KANs outperform MLPs in symbolic formula repre-sentation under fair comparison. Recently, KANs have seen extensive use in various domains such as physics and time series prediction, particularly for their increased interpretability and symbolic representation capabilities [7, 8]. Of particular interest to this work is the reduced number of parameters required by KANs, which makes them more suitable for resource-constrained environments like mobile devices, where memory and computational resources are limited. Additionally, KANs are highly com-patible with SR, which means that the learned network can be expressed as a combination of simpler symbolic equations. These expressions can improve energy efficiency by reducing the complexity and resource demands of executing the model. In this study, we investigate the use of autoencoders (AEs) for channel coding, which is discussed in sev-eral prior works [9, 10, 11]. In our approach, we replace MLPs in the AE structure with KANs, which we find to demonstrate comparable block error rate (BLER) performance while having fewer total parameters. Once the entire KAN-AE is trained, we use SR to derive equations representing the learned behavior. Furthermore, we introduce a non-linearity score term into the SR process to encourage simpler equations when feasible. Our use of SR, combined with the proposed non-linearity score term, aims to lower the energy consumption of the AE model. By using KANs, we show that it is possible to reduce the energy usage of certain DL architectures without sacrificing performance, which suggests that KANs can be a useful alternative to MLPs for specific DL tasks within wireless communications. Organization: The paper is organized as follows. Sec-tion II presents the system model and provides funda-mental concepts related to KANs. Section III describes the proposed KAN-AE and the metrics used to assess energy efficiency. Section IV shows the BLER performance and compares the energy efficiency of each model. Section V concludes the paper. Notation: The set of real and complex numbers are 

> arXiv:2601.01598v1 [eess.SP] 4 Jan 2026

denoted by R and C, respectively. The complex conjugate of z = a + jb is expressed as z‚àó = a ‚àí jb . The circularly symmetric complex normal distribution with zero mean and variance œÉ2 is represented as CN (0, œÉ 2).The Hermitian of a matrix A is denoted by AH. E[X]

denotes the expected value of X.II. System Model In this section, we discuss preliminaries on KANs and provide our system model on OFDM-based AEs. A. Kolmogorov-Arnold Networks The structure of KANs is inspired by the Kolmogorov-Arnold representation theorem, which establishes that any multi-variate continuous function can be expressed as the sum of several uni-variate continuous functions [12], i.e., 

f (x1, x 2, . . . , x d) = 

> 2d

‚àë

> i=0

Œ¶i

Ô£´Ô£≠

> d

‚àë

> j=1

œïi,j (xj )

Ô£∂Ô£∏ , (1) where œïi,j : [0 , 1] ‚Üí R and Œ¶i : R ‚Üí R. The authors of [5] generalize the inner and outer sums in (1) to accommodate an arbitrary number of layers L as 

KAN( x) = ( Œ¶(L) ‚ó¶ Œ¶(L ‚àí1) ‚ó¶ ¬∑ ¬∑ ¬∑ ‚ó¶ Œ¶(2) ‚ó¶ Œ¶(1) )( x) (2) 

=

> dL‚àí1

‚àë

> iL‚àí1=1

œï(L)

> iL,i L‚àí1

(

¬∑ ¬∑ ¬∑ 

> d1

‚àë

> i1=1

œï(2) 

> i2,i 1

( d0‚àë

> i0=1

œï(1)  

> i1,i 0

(xi0 )

)

¬∑ ¬∑ ¬∑ 

)

,

where œï(l) 

> il,i l‚àí1

(¬∑) is a learnable function in the lth layer connecting the il‚àí1th input neuron to the ilth output neuron, i.e., an edge, dl is the number of neurons in the 

lth layer, Œ¶(l) contains all learnable activation functions at the lth layer as 

Œ¶(l)(¬∑) ‚âú

Ô£ÆÔ£ØÔ£ØÔ£∞

œï(l)1,1 (¬∑) . . . œï(l)1,d l‚àí1 (¬∑)

... . . . ...

œï(l) 

> dl,1

(¬∑) . . . œï(l) 

> dl,d l‚àí1

(¬∑)

Ô£πÔ£∫Ô£∫Ô£ª , (3) and operates on the output of the previous layer a(l‚àí1) ‚àà

Rdl‚àí1 , for ‚àÄl ‚àà { 1, . . . , L }, as 

a(l) = Œ¶(l)(a(l‚àí1) ) ‚âú

Ô£ÆÔ£ØÔ£ØÔ£ØÔ£∞‚àëdl‚àí1 

> il‚àí1=1

œï(l)

> il,i l‚àí1

(

a(l‚àí1) 

> il‚àí1

)

...

‚àëdl‚àí1 

> il‚àí1=1

œï(l)

> il,i l‚àí1

(

a(l‚àí1) 

> il‚àí1

)Ô£πÔ£∫Ô£∫Ô£∫Ô£ª .

Here, an edge refers to a connection between an input and an output neuron that performs some transformation on the input. While the weights on the edges are learnable and the activation functions are fixed in MLPs, the opposite holds for KANs. In [5], the authors express an activation function œï(x)

as a linear combination of B-splines and sigmoid linear unit (SiLU), i.e., 

œï(x) = wb √ó SiLU (x) + ws √ó ‚àë

> i

(ci √ó Bi(x)) , (4) where Bi(x) is a B-spline basis function, composed of piecewise polynomials of degree p and scaled by a learnable weight ci. The parameters wb and ws are also learnable. Each B-spline is defined on a specific grid interval, which is determined by the range of input samples. B. System Model Consider a single-user communication link. Let r = k/n 

be the rate of this communication link, where k is the total number of information bits per message and n is the total number of channel uses. Let sm ‚àà R2k

be a one-hot encoded (OHE) vector representation of the message m. The encoder network œµ(sm) maps sm to the vector se ‚àà Rn, which is then converted to the real and imaginary components of stx ‚àà Cn/ 2, where E[|stx |2] = 1 .We consider Lenc layers at the encoder. For an MLP-based neural network, we have 

a(l)enc = œÉ(l)enc (W(l)enc a(l‚àí1) enc + b(l)enc ) , l = 1 , . . . , L enc ,

(5) where œÉ(l)enc is the element-wise non-linear activation func-tion, and W(l)enc ‚àà Rdl√ódl‚àí1 and b(l)enc ‚àà Rdl are the weight matrix and bias vector, respectively. We note that for both the MLP and KAN models, a(0) enc = sm and 

a(Lenc )enc = se. However, for a KAN-based neural network, by re-expressing (2), we have 

a(l)enc = Œ¶(l)enc (a(l‚àí1) enc ) , l = 1 , . . . , L enc , (6) where Œ¶(l)enc is dl √ó dl‚àí1 function space and a(0) enc = sm. In this study, we use (4) to learn the activation functions in KANs. After encoding at the transmitter, the transmitted symbols stx propagate through a communication channel, where they are distorted by the channel and by zero-mean, circularly symmetric complex additive white Gaussian noise (AWGN). Let h denote the channel coefficient. A received symbol srx can then be expressed as 

srx = hs tx + w , (7) for w ‚àº CN (0, œÉ 2

> n

). Under an AWGN channel, we set 

h = 1 . For a flat-fading Rayleigh channel, such as that observed by an OFDM subcarrier, we instead model h ‚àºCN (0 , 1) and assume that h is known at the receiver. Then, srx are equalized using a minimum mean-squared error (MMSE) equalizer, which yields ÀÜsrx = h‚àósrx 

> |h|2+œÉ2
> n

, where 

ÀÜsrx is the estimated transmitted symbol after equalization. The real and imaginary components of ÀÜsrx ‚àà Cn/ 2 are then converted to a single real-valued vector s d ‚àà Rn.Let Œ¥(sd) denote the decoder network that maps s d to a vector ÀÜsm ‚àà R2k

of logarithmic odds. For Ldec layers, the MLP-based and KAN-based decoders follow the structures Normalization Layer                      

> One  Hot Encoder
> IDFT
> CP+ ùëö
> Subcarrier Mapper
> +
> ‚Ñùùëõ  ‚Üí ‚ÑÇùëõ /2
> DFT
> ‡∑ùùëö
> ‚ÑÇùëõ /2 ‚Üí ‚Ñùùëõ
> Argmax
> +
> +ùê¨ e
> MMSE Equalizer
> +
> +
> +
> CP -
> Subcarrier De -mapper
> ùë† tx ùë† rx ∆∏ùë† rx
> OFDM TX OFDM RX
> . . .
> . . .
> .. .
> .. .
> .. .
> . . .
> +
> +
> . . .
> +
> +
> +
> ++
> +
> +
> . . .
> . . .
> .. .
> .. .
> .. .
> . . .
> +
> +
> . . .
> +
> ùöΩ enc
> (1)ùöΩ enc
> (ùêø enc )
> ùíÇ enc
> (0)ùíÇ enc
> (ùêø enc )ùöΩ dec
> (1)ùöΩ dec
> (ùêø dec )
> ùíÇ dec
> (0)ùíÇ dec
> (ùêø dec )Fig. 1. OFDM transmitter and receiver block diagrams with an (n, k )KAN-AE.

in (5) and (6), respectively. Furthermore, we define a(0) dec =

sd and a(Ldec )dec = ÀÜsm. The detected message ÀÜm is expressed as ÀÜm = arg max 

> ÀÜsm‚ààR2k

ÀÜsm . (8) Conventional single-layer neural networks, also known as perceptrons, can only learn linear decision boundaries, which limits their ability to capture complex non-linear relationships. While MLPs address this limitation by stacking multiple layers, a single KAN layer is capable of modeling complex non-linear patterns due to the flexibility of its learnable activation functions. Fig. 1 illustrates a KAN-AE within an end-to-end OFDM transmitter and receiver, for a given n channel uses and k bits. Although we consider an arbitrary number of KAN layers at the transmitter and receiver, a single layer may be used in both cases. III. Energy-Efficient KAN-based Autoencoder In this work, we aim to reduce the number of learn-able parameters in the encoder and decoder to improve energy efficiency at the transmitter and receiver, while maintaining a low BLER. We exploit that KANs are highly compatible with SR, and introduce a new penalty term in the SR process to discourage less energy-efficient symbolic expressions. The functions are heuristically scored by measuring non-linearity, as discussed in Section III-A and Section III-B. Our approach enables scoring the energy efficiencies of KAN and MLP networks regardless of the implementation, as discussed in Section III-C. A. Quantifying Function Non-linearity To quantify the degree of non-linearity for a function 

f (x) over an interval [a, b ], we propose using a piecewise linear approximation. The idea is to assess the non-linearity of f (x) based on the minimum number of linear segments, N , required to approximate f (x) within aspecified error tolerance, œµ. The number of segments N

then serves as a metric of non-linearity. A larger N

indicates higher non-linearity, while a smaller N implies that f (x) is closer to a linear form over [a, b ].To express the aforementioned metric, consider a set of uniformly spaced partition points a1, a 2, . . . , a N +1 for a1 =

a and aN +1 = b, where [aj , a j+1 ) is the jth sub-interval on which f (x) is linearly approximated. We express the approximation error over the jth sub-interval [aj , a j+1 ) as 

ej =

‚à´ aj+1 

> aj

|f (x) ‚àí œàj (x)|2 dx , (9) where œàj (x) = mj x + kj is the best-fit linear approxi-mation of f (x) over [aj , a j+1 ). To obtain œàj (x), we over-sample f (x) in the jth subinterval and apply least squares regression, with mj and kj as the best-fit slope and in-tercept, respectively. We then measure the approximation error over all sub-intervals as E(N ) = 

> N

‚àë

> j=1

ej . (10) The non-linearity measure of f (x), denoted as Q[f (x)] , is the smallest N that satisfies E (N ) < œµ , i.e., 

Q[f (x)] = arg min  

> N

E(N ) s.t. E(N ) < œµ . (11) If f (x) exhibits greater non-linearity, a larger N will be required to achieve the same approximation accuracy. Conversely, if f (x) is more linear, a smaller N is required. The metric for Q[f (x)] is formulated within the context of SR. Non-linear functions are often computationally in-tensive and energy demanding. Therefore, by determining 

Q[f (x)] , we can estimate the energy cost of approximating 

f (x) and guide SR towards simpler approximations when feasible. Example 1: Let f (x) = |5x| and g(x) = sin(5 x)

be defined on the interval [‚àí1, 1] and assume an error tolerance œµ = 10 ‚àí3. For f (x) and g(x), compute E (N )

using (9) and (10). Repeat this process and increase N

each iteration until the condition in (10) is satisfied. Then, (11) is used to determine the score for each function, yielding Q[f (x)] = 2 and Q[g(x)] = 11 . This is expected, as sin(5 x) is far more oscillatory on [‚àí1, 1] when compared to |5x|, thereby making it more non-linear. B. Symbolic Regression under Non-linearity Constraint Consider an activation function œï(x) ‚àà [a, b ] ‚Üí R and a finite number of candidate functions {fk(x)}Kk=1 (e.g. sin, log, exp). Obtain samples S(œï) = {œï(xi) | xi ‚àà [a, b ]}. Let 

Àúœï(x) = Œ≥ofk(Œ≥ix + Œ≤i) + Œ≤o be an approximation of œï(x)Algorithm 1 Convert œï(x) to symbolic expression Input: S(œï), {fk(x)}Kk=1 

Output: œïsym (x)

R2 

> best

= ‚àí‚àû ; Zbest = ‚àí‚àû ; ÀÜœïk(x) = None; œïsym (x) = 

None for fk(x) in {fk(x)}Kk=1 do 

R2 

> k

= ‚àí‚àû 

for (Œ≥i, Œ≤ i) in grid [‚àí10 , 10] do Set Àúœï(x) = Œ≥ofk(Œ≥ix + Œ≤i) + Œ≤o. Fit Œ≥o, Œ≤o using linear regression with S(œï)

R2[ Àúœï(x)] ‚Üê (12) if R2[ Àúœï(x)] > R 2 

> k

then 

ÀÜœïk(x) = Àúœï(x); R2 

> k

= R2[ Àúœï(x)] 

end end 

Z[ ÀÜœïk(x)] ‚Üê (14) if Z[ ÀÜœïk(x)] > Z best then 

œïsym (x) = ÀÜœïk(x); Zbest = Z[ ÀÜœïk(x)] 

end end return œïsym (x)

given Œ≥i, Œ≤i, Œ≥o, Œ≤o, and fk(x). For each Àúœï(x), we compute the R2 score 

R2[ Àúœï(x)] = 1 ‚àí

‚àëNi=1 [œï(xi) ‚àí Àúœï(xi)] 2

‚àëNi=1 [œï(xi) ‚àí ¬Øœï(xi)] 2 , (12) where ¬Øœï(xi) = E[œï(xi)] . Next, we set 

ÀÜœïk(x) = arg max 

> Àúœï(x)

R2[ Àúœï(x)] , (13) where ÀÜœïk(x) is the best approximation of œï(x) for a given 

fk(x). When determining the symbolic expression œïsym (x)

based on ÀÜœïk(x), we utilize (11) and (12) to create acombined score term Z[ ÀÜœïk(x)] , which is given by 

Z[ ÀÜœïk(x)] = R2[ ÀÜœïk(x)] + ŒªQ[ ÀÜœïk(x)] . (14) Here, Œª weights the non-linearity score term. Using the combined score in (14), we compute 

œïsym (x) = arg max  

> ÀÜœïk(x)

Z[ ÀÜœïk(x)] . (15) In this study, the parameters Œ≥i and Œ≤i maximizing 

R2 for a given Àúœï(x) are determined using a grid search. Also, for each (Œ≥i, Œ≤ i) pair, Œ≥o and Œ≤o are determined using least squares regression, where Œ≥o and Œ≤o are the best-fit slope and intercept of S(œï), respectively. The described approach builds upon [5], with our proposed non-linearity score term added to encourage energy-efficient equations when possible. A thorough outline of the SR procedure is presented in Algorithm 1. C. Scoring MLPs and KANs Based on Non-Linearity Metric For a given MLP network, the total non-linearity score combines the individual scores for linear and non-linear activations. Thus, the total score Q[MLP( x)] is expressed as 

Q[MLP( x)] = 

> L

‚àë

> l=1

[

dl √ó

(

dl‚àí1 + Q[œÉ(l)(x)] 

)] 

, (16) where d0 is the first layer input size. Clearly, the choice of œÉ(l) in each layer affects the total score. Now consider a KAN network, where œï(l) 

> i,j

is the ac-tivation function that connects the ith input to the jth output in the lth layer. The total score Q[KAN( x)] can be determined by treating each learned activation function separately and computing Q[œï(l) 

> i,j

(x)] using the method described in Section III-A. When computing the score for each activation function, we consider the derived symbolic expressions for œï(l) 

> i,j

and not the original B-spline implementation. Summing the individual scores across all activation functions in the KAN network, we obtain 

Q[KAN( x)] = 

> L

‚àë

> l=1
> dl

‚àë

> j=1
> dl‚àí1

‚àë

> i=1

(

a(l) 

> i,j

√ó Q[œï(l) 

> i,j

(x)] 

)

. (17) Here, a(l) 

> i,j

is 0 if œï(l) 

> i,j

is pruned and 1 otherwise. The pruning process is described in Section III-D1. We note that for KANs, the score of each œï(l) 

> i,j

is determined on the grid interval of the activation function. For MLPs, this interval can be chosen based on the domain, range, and boundedness of the activation function in each hidden layer. D. Details for Further Improvements 1) Pruning: To further improve the energy efficiency of KANs, we utilize the pruning methodology in [5]. For a KAN with multiple layers, each neuron‚Äôs importance is determined by incoming and outgoing scores 

I(l) 

> i

= max 

> k

(

|| œï(l‚àí1)  

> i,k

(x)|| 1

)

, O (l) 

> i

= max 

> j

(

|| œï(l+1)  

> j,i

(x)|| 1

)

,

(18) where œï(l‚àí1)  

> i,k

(x) and œï(l+1)  

> j,i

(x) represent activation func-tions on edges to and from the ith neuron in the lth layer. Neurons with both scores above a threshold Œ∑ are retained, and all others are pruned. For KAN layers, we can also consider pruning individual activation functions rather than neurons. In this case, || œï(l‚àí1)  

> i,k

(x)|| 1 is considered for all activation functions, and the edge is pruned if this value is below Œ∑. Pruning will help obtain compact closed-form expressions by removing redundant activation functions, thereby improving energy efficiency. 2) Training: To optimize BLER performance, we em-ploy noise-scheduling, and utilize the modified cross-Algorithm 2 AE training with noise scheduling Input: m, B, Œ±, œÉ2

> max

, œÉ2

> min

, nepochs 

Output: Optimized parameters {Œ∏e, Œ∏ d}

œÉ2 

> n

= œÉ2

> max

for n = 1 to nepochs do Sample {m(i)}Bi=1 from mstx = Encoder (m; Œ∏e)

srx = stx + w, w ‚àº CN (0 , œÉ 2)ÀÜy = Decoder (srx ; Œ∏d)

L ‚Üê (19) Update Œ∏e and Œ∏d using L and Œ±œÉ2 

> n

= œÉ2 

> max

‚àí (n/n epochs ) √ó (œÉ2 

> max

‚àí œÉ2

> min

)

end entropy loss function 

L = ‚àí

> 2k

‚àë

> i=1

Ti log 

(

exp( li)

‚àë2k 

> j=1

exp( lj )

)

, (19) where Ti is the true label for the ith class, and li and lj are the logarithmic odds for the ith and jth decoder output. The model outputs logarithmic odds directly, as there is no softmax layer at the decoder output. In this study, we use the Adam optimizer to adjust the encoder parameters 

Œ∏e and decoder parameters Œ∏d for joint training of the encoder and decoder. Algorithm 2 outlines this process, where B is the batch size, œÉ2 

> min

and œÉ2 

> max

give the noise scheduling range, and Œ± is the learning rate. IV. Numerical Results For numerical experiments, we analyze (24,12) AEs in an end-to-end OFDM system as a proof-of-concept, with plans to use larger block size in future work. For comparison, we consider MLPs with a single input, hidden, and output layer. The hidden layer uses rectified linear unit (ReLU) activation functions, while the output layer has no activations. In this study, we consider MLPs with 150 hidden layer neurons at both the encoder and decoder. For the KAN-AE, we replace the MLPs with a single KAN layer at both the encoder and decoder, where each learnable activation function has 5 learnable control points 

c and uses third-degree polynomial basis functions. To avoid removing key components of the KAN-AE, we use a modest pruning threshold Œ∑ = 10 ‚àí4 at the encoder and 

Œ∑ = 3 √ó 10 ‚àí5 at the decoder. During the SR procedure, we consider an error tolerance œµ = 10 ‚àí2 for the non-linearity score calculation described in Section III-A, and 

Œª = 3 √ó 10 ‚àí2 for the non-linearity score weight given in Section III-B. We create, train, and test the MLP-AE and KAN-AE in Python using the PyTorch machine learning library. Adam is used to train each model in AWGN for 3 √ó 10 4 epochs, where each batch contains 211 randomly selected m, and the learning rate is set to Œ± = 10 ‚àí3. We use Eb/N 0 = 0 

dB to compute œÉ2 

> min

and Eb/N 0 = 6 dB to compute 

œÉ2

> max

. The grid interval for each KAN activation function is updated periodically to fit the training examples. All models are trained using an NVIDIA RTX 3070 GPU. We compare the MLP-AE and KAN-AE to (24,12) Golay code with maximum-likelihood decoding (MLD). Our implementation of MLD for Golay code uses quadrature phase shift keying (QPSK) as the modulation scheme, where E[|stx |2] = 1 . Let sg ‚àà Cn/ 2 be a vector of symbols forming a modulated codeword. We implement the MLD as 

ÀÜc = arg max  

> c

Re {sHg c} , (20) where ÀÜc is the detected modulated codeword and c ‚àà Cn/ 2

is a vector containing a QPSK modulated codeword for the Golay code. For this implementation of MLD, the number of linear operations is n2 √ó 2k, which we use to compute a non-linearity score of 2.359296 √ó 10 6 for (24,12) Golay MLD. A. Block Error Rate Performance To characterize the BLER performance, we perform Monte-Carlo experiments. First, we simulate the BLER in AWGN for the MLP-AE and KAN-AE, and compare it to (24,12) Golay MLD. The BLER curves in Fig. 2a show that the KAN-AE performs similarly to the MLP-AE and (24,12) Golay MLD, with (24,12) Golay MLD slightly out-performing both. Observe that the KAN-AE and MLP-AE perform almost identically. Additionally, in Fig. 2a, we see that pruning has a relatively minor effect on the overall BLER performance for the KAN-AE. We also note that the KAN-based symbolic regression autoencoder (SR-AE) showed no loss in performance compared to the pruned model. We perform another Monte-Carlo experiment under a Rayleigh fading channel and again compare the BLER of the MLP-AE and KAN-AE to (24,12) Golay MLD. In this experiment, the symbols on each subcarrier observe flat-fading due to the use of OFDM, and we assume perfect channel estimation at the receiver. From Fig. 2b, we can see that all models show comparable performance. Additionally, similar to the AWGN channel, pruning has a slight negative effect on BLER performance, with the SR-AE showing nearly identical performance to the pruned model. The simulation results show that the SR-AE performs very similar to the original KAN-AE, thereby indicating that the model accuracy has been maintained. B. Power and Energy Consumption Another experiment is conducted where 5√ó10 3 messages 

m are processed using the MLP-AE, SR-AE, and Golay code, for a fixed 2.5 √ó 10 4 trials. We note that this simulation includes the encoder, channel, and decoder. We monitor the GPU power consumption during inference 0 1 2 3 4 5 6

> Eb/N0 (dB)
> 10 -6
> 10 -5
> 10 -4
> 10 -3
> 10 -2
> 10 -1
> 10 0
> BLER
> (24,12) Golay MLD
> (24,12) MLP-AE
> (24,12) KAN-AE
> (24,12) KAN-AE w/ pruning
> (24,12) SR-AE

(a) Block error rate in AWGN. 0 2 4 6 8 10 12 

> Eb/N0 (dB)
> 10 -4
> 10 -3
> 10 -2
> 10 -1
> 10 0
> BLER
> (24,12) Golay MLD
> (24,12) MLP-AE
> (24,12) KAN-AE
> (24,12) KAN-AE w/ pruning
> (24,12) SR-AE

(b) Block error rate in Rayleigh fading channel. Fig. 2. Performance of different (24,12) coding schemes. 

and compute the energy consumption for each model as the area underneath the power consumption curve. A comparison of the GPU power consumption over time for each scheme can be seen in Fig. 3. Since an NVIDIA RTX 3070 GPU is used in this experiment, the average power draw is very large in all cases; however, in a practical system like a radio or mobile device, the power draw can be significantly reduced at the cost of evaluation speed. Also, we emphasize that the curves seen in Fig. 3 are implementation and hardware dependent. Therefore, it is more appropriate to use the non-linearity score to access relative energy efficiencies of each scheme since it abstracts out the implementation and hardware details. In Fig. 3, we observe that the MLP-AE uses approximately 1.38 times more energy as compared to the KAN-based SR-AE. Here, we note that MLD for Golay code performs the best with respect to energy consumption, which can be explained by the hardware level optimizations of the PyTorch library by which it is implemented. The peak power consumption, total energy consump-0 20 40 60 80 100 120 140 160 180 200 

> Time (s)
> 020 40 60 80 100 120 140 160 180 200
> GPU Power (W)
> MLP-AE
> Golay MLD
> SR-AE

Fig. 3. GPU power consumption of (24,12) coding schemes during operation. Since this is a heuristic measurement, the non-linearity score offers a better, hardware and implementation-independent measure of the energy efficiency. TABLE I MLP-AE vs. SR-AE vs. Golay MLD            

> MLP-AE SR-AE Golay with MLD Peak power 195 W 123 W 145 W Energy consumption 9166.9 J 6644.4 J 2420.0 J Non-linearity score 1.2366 e+ 66.8464 e+ 52.3592 e+ 6

tion, and non-linearity score for the MLP-AE and SR-AE are reported in Table I. To compute the non-linearity score for the MLP-AE, we consider the individual score for each ReLU activation function. Since ReLU is a piecewise linear function with N = 2 , a score of 2 is assigned to each hidden layer activation. The output layer for both MLPs in the AE have no activation, so each output layer activation function is assigned a score of 0. Then, using (16), we calculate the score seen in Table I. Next, consider the KAN-AE, which is pruned and converted to symbolic expressions. Each activation function is considered on its grid interval, which is [0 , 1] for those in the encoder and [‚àí2.2, 2.2] for those in the decoder. Using (17), we calculate the score for the SR-AE seen in Table I. V. Concluding Remarks This study demonstrates that KANs can provide ad-vantages over MLPs in terms of energy efficiency and model size for channel coding tasks. The ability to derive symbolic expressions from KANs allows for simplified, low-complexity equations, which reduces the computational burden during inference. To obtain simpler symbolic expressions, we propose a non-linearity metric, which we then use to score different symbolic expressions and elim-inate unnecessary, highly non-linear activation functions during the SR process. Our results show that KAN-AEs perform similarly to MLP-AEs in AWGN and flat-fading Rayleigh channels, while achieving reduced energy con-sumption when combined with the proposed SR method. This makes KANs a promising option for integrating DL models into energy-constrained devices within practical communication systems. Future work will focus on refining the non-linearity metric, considering larger codes, and performing a sensitivity analysis to examine how the score term impacts the SR process. References 

[1] M. Soltani, V. Pourahmadi, A. Mirzaei, and H. Sheikhzadeh, ‚ÄúDeep learning-based channel estimation,‚Äù IEEE Communica-tions Letters, vol. 23, no. 4, pp. 652‚Äì655, 2019. [2] M. Zhang, Y. Zeng, Z. Han, and Y. Gong, ‚ÄúAutomatic modu-lation recognition using deep learning architectures,‚Äù in Proc. IEEE International Workshop on Signal Processing Advances in Wireless Communications (SPAWC), 2018, pp. 1‚Äì5. [3] Q. Mao, F. Hu, and Q. Hao, ‚ÄúDeep learning for intelligent wire-less networks: A comprehensive survey,‚Äù IEEE Communications Surveys & Tutorials, vol. 20, no. 4, pp. 2595‚Äì2621, 2018. [4] H. Huang, S. Guo, G. Gui, Z. Yang, J. Zhang, H. Sari, and F. Adachi, ‚ÄúDeep learning for physical-layer 5G wireless tech-niques: Opportunities, challenges and solutions,‚Äù IEEE Wireless Communications, vol. 27, no. 1, pp. 214‚Äì222, 2020. [5] Z. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Sol-jaƒçiƒá, T. Y. Hou, and M. Tegmark, ‚ÄúKAN: Kolmogorov-Arnold networks,‚Äù arXiv preprint arXiv:2404.19756, 2024. [6] R. Yu, W. Yu, and X. Wang, ‚ÄúKAN or MLP: A fairer compar-ison,‚Äù arXiv preprint arXiv:2407.16674, 2024. [7] Z. Liu, P. Ma, Y. Wang, W. Matusik, and M. Tegmark, ‚ÄúKAN 2.0: Kolmogorov-Arnold networks meet science,‚Äù 2024. [Online]. Available: https://arxiv.org/abs/2408.10205 [8] C. J. Vaca-Rubio, L. Blanco, R. Pereira, and M. Caus, ‚ÄúKolmogorov-Arnold networks (KANs) for time series analysis,‚Äù arXiv preprint arXiv:2405.08790, 2024. [9] T. O‚ÄôShea and J. Hoydis, ‚ÄúAn introduction to deep learning for the physical layer,‚Äù IEEE Transactions on Cognitive Commu-nications and Networking, vol. 3, no. 4, pp. 563‚Äì575, 2017. [10] A. Felix, S. Cammerer, S. D√∂rner, J. Hoydis, and S. Ten Brink, ‚ÄúOFDM-autoencoder for end-to-end learning of communica-tions systems,‚Äù in Proc. IEEE International Workshop on Signal Processing Advances in Wireless Communications (SPAWC), 2018, pp. 1‚Äì5. [11] D. Wu, M. Nekovee, and Y. Wang, ‚ÄúDeep learning-based autoencoder for m-user wireless interference channel physical layer design,‚Äù IEEE Access, vol. 8, pp. 174 679‚Äì174 691, 2020. [12] A. K. Kolmogorov, ‚ÄúOn the representation of continuous functions of several variables by superposition of continuous functions of one variable and addition,‚Äù Doklady Akademii Nauk SSSR, vol. 114, pp. 369‚Äì373, 1957.