Title: Constructing a Neuro-Symbolic Mathematician from First Principles

URL Source: https://arxiv.org/pdf/2601.00125v1

Published Time: Mon, 05 Jan 2026 01:21:16 GMT

Number of Pages: 17

Markdown Content:
# Constructing a Neuro-Symbolic Mathematician from First Principles 

## Keqin Xie ∗

## 2025-01-01 

Abstract 

Large Language Models (LLMs) exhibit persistent logical failures in complex reasoning due to the lack of an internal axiomatic framework [1]. We propose Mathesis , a neuro-symbolic architecture that encodes mathe-matical states as higher-order hypergraphs and uses a Symbolic Reason-ing Kernel (SRK)—a differentiable logic engine that maps constraints to a continuous energy landscape. By defining a global energy func-tion E(G), where zero energy implies logical consistency, the SRK yields gradient-based signals to train a Hypergraph Transformer Brain, turning proof search into energy minimization. Multi-step deduction is enabled via Monte Carlo Tree Search and Evolutionary Proof Search, guided by learned value functions and semantic unification. 

# 1 Introduction 

Large language models (LLMs) achieve strong performance on linguistic tasks and code generation by modeling the statistical distribution of natural lan-guage [2]. However, they exhibit systematic failures in formal mathematical reasoning, often generating steps that violate basic axioms—so-called “halluci-nations” [1]. This stems from the probabilistic nature of transformer architec-tures, which lack mechanisms for logical verification or enforcement of semantic constraints. Although chain-of-thought (CoT) prompting induces intermediate reasoning steps, it does not ensure their logical validity: the underlying process remains high-dimensional sequence prediction, not symbolic derivation [3]. Neuro-symbolic architectures aim to combine neural pattern recognition with symbolic rigor. For example, AlphaGeometry solves Olympiad-level geometry problems by coupling a generative model with a symbolic deduction engine [4]. Yet conventional neuro-symbolic systems typically employ non-differentiable solvers that act as black boxes, yielding only sparse binary feedback (e.g., “proof valid/invalid”). Without gradient signals from the symbolic component, the neural module cannot be trained directly to satisfy logical constraints. Prior 

> ∗Email: xiekeqin30@gmail.com

1

> arXiv:2601.00125v1 [cs.AI] 31 Dec 2025

efforts toward differentiable logic—such as tensor programs or neural logic ma-chines—struggle to scale beyond small, finite domains due to the unbounded search space of mathematics [5]. We introduce Mathesis , a new architecture that overcomes gradient sparsity through a symbolic reasoning kernel (SRK). The SRK acts as a differentiable “physics engine” for logic: it embeds mathematical hypergraphs into a contin-uous energy landscape where logical consistency corresponds to a zero-energy state. This yields dense, gradient-based feedback for a Hypergraph Transformer Brain , steering its generative policy toward axiom-compliant derivations. Un-like prior approaches, Mathesis encodes mathematical states as higher-order hypergraphs (Section 4), capturing multi-arity relations and nested logical con-nectives with high fidelity. The system integrates this neuro-symbolic core with structured search strategies—including Monte Carlo tree search (MCTS) and evolutionary proof search (EPS)—to enable deliberate, “System 2” reasoning (Section 6). 

# 2 Preliminaries: Representing Mathematics as Hypergraphs 

To facilitate rigorous neuro-symbolic reasoning, we formalize the mathemati-cal workspace as a structured, higher-order heterogeneous hypergraph. This representation distinguishes between syntactic construction (terms) and seman-tic truth (facts), and explicitly handles nested logical structures and variable quantification scopes [6]. 

## 2.1 Definition: Mathematical State Hypergraph 

We define the state of a proof as a tuple tracking structure, truth status, and variable binding scopes. 

Definition 1 (Mathematical State Hypergraph) . A mathematical state is a tuple S = ( G, F), where G = ( V, E ) is a directed higher-order hypergraph. 1. V is the set of nodes, representing mathematical terms (e.g., variables x,constants 0, compound terms x + y). 2. E is the set of hyperedges, representing relations , operations, and logical connectives. 

• To support nested logic (e.g., (A ∧ B) =⇒ C), we adopt a higher-order definition: a hyperedge e ∈ E is an ordered sequence of elements from V ∪ E. That is, an edge can connect nodes or other edges.This structure is essential for capturing the compositional nature of com-plex logical formulas, a challenge also addressed in modern knowledge hypergraph reasoning [7]. 

23. F ⊆ E is the set of Facts. This is a distinguished subset of hyperedges representing assertions currently held to be true within the global context (e.g., axioms, premises, and derived theorems). 

Typing System: We define type mappings ϕV : V → T V and ϕE : E → T E to enforce semantic consistency. 

• Node Types ( TV ): {Variable , Constant , CompoundTerm }.

• Hyperedge Types ( TE ): We distinguish three semantic categories: 

– Constructors ( TCon ): Functional operations that define a term. Inputs are drawn from V , and the output maps to a unique vout ∈ V ,e.g., Sum (va, v b) → vsum .

– Predicates ( TP red ): Atomic logical assertions. (e.g., Equals (va, v b), 

Parallel (l1, l 2)). 

– Connectives ( TConn ): Higher-order logical operators taking edges as inputs. (e.g., Implies (epremise , e conclusion ), And (e1, e 2)). 

Quantification and Scoping: To handle quantification ( ∀, ∃), we introduce Scope Attributes on hyperedges. A quantified statement is represented by a hyperedge equant of type ForAll or Exists .

• equant = ( Vbound , e body )

• Vbound ⊂ V : The set of variables bound by this quantifier. 

• ebody ∈ E: The logical formula (edge) being quantified. 

Example: The statement “ ∀x, (x = x)” is represented by: 1. Term: Node vx (Type: Variable ). 2. Predicate: Edge eeq = ( vx, v x) (Type: Equals ). 3. Quantification: Edge eroot = ( {vx}, e eq ) (Type: ForAll ). 4. Fact Status: eroot ∈ F . Note that eeq is not in F independently; it is only true within the context of the quantifier. 

## 2.2 Problem Formulation 

We frame Automated Theorem Proving (ATP) as a search for a valid derivation path that adds the goal statement to the set of proven facts. 

Definition 2 (Graph Transformation Action) . Let S be the space of valid states. An action is a function a : S → S chosen from a set of admissible rules A (e.g., Modus Ponens, Substitution, Instantiation). An action St+1 = a(St) may: 

• Extend G (construct new terms or logical structures). 

3• Extend F (derive new truths). 

Problem Statement. Given an initial state Spremise = ( G0, F0) encoding ax-ioms and assumptions, and a Goal Proposition represented as a target hyperedge structure Pgoal (or a description thereof), the objective is to find a sequence of actions ( a1, . . . , a n) producing states S0 → · · · → S n such that: 1. Structural Existence: The hypergraph Gn contains a subgraph isomorphic to Pgoal . Let egoal be the hyperedge in Gn corresponding to the root of 

Pgoal .2. Logical Entailment: The goal is recognized as a proven fact: 

egoal ∈ F n

# 3 The Symbolic Reasoning Kernel (SRK) 

The Symbolic Reasoning Kernel (SRK) serves as a differentiable physics engine for formal logic, mapping the discrete syntax of mathematics into a continuous energy landscape. While the Hypergraph Transformer Brain proposes reasoning trajectories, the SRK provides a deterministic verification signal. This archi-tecture transforms the task of finding a proof into the minimization of a global logical energy function E(G)[8]. 

## 3.1 Philosophy and Global Computation 

The SRK is based on the principle that a mathematical state S is logically con-sistent if and only if E(G) = 0. By representing constraints as differentiable energy terms, we provide the generative component with a dense gradient sig-nal, allowing the model to ”sense” the direction of logical consistency. The total energy is aggregated across multiple domain-specific engines as described in Al-gorithm 1.The formal proofs establishing the logical correctness and smoothness of this energy functional are detailed in Appendix A. 

Algorithm 1 SRK Energy Computation 

Require: Mathematical State Hypergraph G = ( V, E ), weight parameters wEnsure: Total logical energy Etotal  

> 1:

Etotal ← 0 

> 2:

for each hyperedge e ∈ E do  

> 3:

Identify domain D ∈ { Matrix , Ideal , Geometry } 

> 4:

Ee ← ComputeEnergy D (e) 

> 5:

Etotal ← Etotal + wD · Ee 

> 6:

end for  

> 7:

return Etotal 

43.2 The Matrix Engine (Linear Algebra) 

The Matrix Engine represents linear operators as tensors within Rd×d. To main-tain generality and support the proof of theorems involving full-rank or invert-ible matrices, the engine avoids restrictive low-rank assumptions that would collapse the representation to partial isometries. Instead, it computes energy based on the residuals of fundamental linear algebraic identities. For any matrix M or set of matrices {A, B, C }, the engine defines the fol-lowing energy terms: 1. Equality and Symmetry: 

Eeq (A, B ) = ∥A − B∥2 

> F

, Esym (A) = ∥A − AT ∥2 

> F

(1) 2. Multiplicative Consistency: 

Emult (A, B, C ) = ∥AB − C∥2 

> F

(2) 3. Orthogonality: To represent the property of being an orthogonal matrix without restricting the singular values of non-orthogonal matrices, we de-fine: 

Eorth (A) = ∥AT A − I∥2 

> F

(3) 4. Invertibility: If a node represents the inverse A−1, the engine enforces: 

Einv (A, A −1) = ∥AA −1 − I∥2 

> F

(4) This formulation ensures that the engine can represent any linear operator in GL (n, R) or Mn(R), providing the necessary flexibility for general theorem proving in linear algebra. 

## 3.3 The Ideal Engine (Algebraic Geometry) 

The Ideal Engine verifies polynomial entailment. Given a set of premise poly-nomials F = {f1, . . . , f s} and a hypothesis h, the engine checks for Ideal Mem-bership: whether h lies in the ideal ⟨F ⟩. This is a sufficient condition for the premises to entail the conclusion. Definition 3.1 (Ideal Membership). A polynomial h is a member of the ideal 

⟨f1, . . . , f s⟩ if there exist witness polynomials g1, . . . , g s such that: 

h =

> s

X

> i=1

gifi (5) The SRK computes the energy as the squared norm of the residual: 

Eideal (h, F ) = h −

> s

X

> i=1

gifi

> 22

(6) 5The formulation of this algebraic check as a loss function is inspired by recent efforts to develop neural solvers for problems in computational algebra[9]. To ensure the search space for witness polynomials gi is mathematically well-defined and computationally bounded, the engine enforces an Effective De-gree Bound. Following the results of Hermann [10], the degrees of gi are con-strained by a function of the degrees of the input polynomials and the number of variables, preventing the ”infinite search” problem inherent in unbounded polynomial construction. Furthermore, while ideal membership is the primary check, the engine ac-counts for the Strong Nullstellensatz when verifying geometric consistency. If the task is to verify that h vanishes on the variety V (F ), the engine searches for witnesses for the radical ideal, checking if hk ∈ ⟨ F ⟩ for some k ∈ N.

## 3.4 The Geometric Engine (Euclidean Geometry) 

The Geometric Engine maps Euclidean predicates to stable polynomial forms, eliminating the singularities associated with division and square roots. By using squared residuals, the engine maintains a strictly non-negative energy surface that is everywhere differentiable [11]. Let D2(A, B ) = ( xA − xB )2 + ( yA − yB )2 be the squared distance between points. 1. Collinearity and Parallelism: Using the squared cross-product of di-rection vectors: 

Ecoll (A, B, C ) = (( xB − xA)( yC − yA) − (yB − yA)( xC − xA)) 2 (7) 

Epara (AB, CD ) = (( xB − xA)( yD − yC ) − (yB − yA)( xD − xC )) 2 (8) 2. Perpendicularity: Using the squared dot product: 

Eperp (AB, CD ) = (( xB − xA)( xD − xC ) + ( yB − yA)( yD − yC )) 2 (9) 3. Congruence and Circles: To avoid square root singularities at the origin, we compare squared distances: 

Econg (AB, CD ) =  D2(A, B ) − D2(C, D )2 (10) 4. Ratio and Similarity: Ratios are verified via cross-multiplication to eliminate division-related instability: 

Eratio (AB, CD, EF, GH ) =  D2(A, B ) · D2(G, H )

− D2(E, F ) · D2(C, D )2 (11) 64 The Hypergraph Transformer Brain 

The “Brain” of the Mathesis architecture is a generative agent responsible for proposing reasoning steps. Unlike the SRK, which is a deterministic physics engine for verification, the Brain is a probabilistic model designed to navigate the immense combinatorial search space of mathematical transformations via learned intuition. 

## 4.1 Model Architecture 

Standard Graph Neural Networks (GNNs) typically flatten higher-order rela-tions into binary edges. However, mathematical expressions are inherently higher-order and non-commutative ; a simple operation like z = x − y con-stitutes a ternary relation where the order of operands defines the semantics. To capture this fidelity, we employ a Hypergraph Transformer that directly models relations as ordered sequences of inputs [12]. 

Message Passing Mechanism We unify nodes V and hyperedges E into a single representational space of entities, X = V ∪ E. This unification is critical because, in higher-order logic, a hyperedge (e.g., an equality A = B) can serve as an argument to another hyperedge (e.g., an implication ( A = B) = ⇒ C). The embedding update at layer l, denoted h(l), occurs in two phases using a specialized Attention mechanism that respects argument ordering: 1. Composition (Children → Parent): Each hyperedge e updates its semantic representation by aggregating information from its constituent arguments. Because mathematical operators are position-sensitive (e.g., x⊖y̸ = y⊖x), we inject positional encodings ϕpos (i) based on each argument’s index i.Let Args( e) = ( u1, u 2, . . . , u k) be the ordered sequence of inputs for edge 

e, where ui ∈ V ∪ E. The update is: 

h(l+1 /2)  

> e

= LN 



h(l) 

> e

+ Attn 



Q = h(l) 

> e

, K = V =

n

h(l) 

> ui

+ ϕpos (i)

oki=1 

 

This ensures the model distinguishes the structural role of every operand. 2. Contextualization (Parents → Children): Every entity u ∈ V ∪ E updates its understanding of its global utility by attending to the hyperedges (par-ents) in which it participates. This allows a sub-expression to receive context from the larger logical statements that consume it. Let Parents( u) = {e ∈ E | u ∈ Args( e)}.

h(l+1)  

> u

= LN 



h(l) 

> u

+ Attn 



Q = h(l) 

> u

, K = V =

n

h(l+1 /2) 

> e

o

> e∈Parents( u)

 

74.2 Action Space 

The Brain interacts with the mathematical state via a discrete action space A,formally defined as the set of all valid operator applications over the graph. An action a ∈ A is a tuple: 

a = (op , τ 1, . . . , τ k)where: 

• op ∈ Ω: An operator from the fixed library of supported functions (e.g., 

Add , Integrate , ModusPonens , ConstructPerpendicular ). 

• τi ∈ V ∪ E: The operands selected from the current graph state. Crucially, the operand domain includes both nodes and hyperedges. This enables the agent to perform: 

• Constructive Actions: Creation of new terms from existing nodes, such as Midpoint(A, B) .

• Logical Actions: Deriving new truths from existing facts. For example, transitivity can be applied to edges a = b and b = c using: 

ApplyTheorem(Transitivity, Edge a=b, Edge b=c) 

## 4.3 Output Policy 

We model the agent’s decision-making as a policy distribution πθ (a|G ). Given the variable arity of operators and the dynamic size of the graph, we employ an autoregressive pointer network[13]. The probability of an action is decomposed into the probability of selecting an operator followed by the sequential selection of its arguments: 

πθ (a|G ) = Pθ (op |G ) ·

> k

Y

> i=1

Pθ (τi|op , τ <i , G)The argument selection Pθ (τi| . . . ) is computed via a pointer attention mech-anism over the embeddings of all valid entities in G, masking invalid types to enforce syntactic correctness (e.g., ensuring the first argument to And is a boolean hyperedge). 

# 5 The Learning Process: Energy-Guided Train-ing 

The fundamental challenge in applying Reinforcement Learning to automated theorem proving is the sparsity of the reward signal. In traditional systems, an agent typically receives a binary reward only upon successfully completing a proof, making credit assignment for intermediate steps intractable. Mathesis overcomes this by leveraging the Symbolic Reasoning Kernel (SRK) to convert logical validity into a continuous, dense reward signal [14]. 85.1 Training Objective 

We formulate the training of the Hypergraph Transformer Brain as a Reinforce-ment Learning (RL) problem. The objective is to learn a policy πθ (a|G ) that generates a sequence of actions minimizing the logical energy of the mathe-matical state. Specifically, the agent seeks to construct a Witness Object—a structured argument (e.g., a set of polynomial coefficients or a geometric con-struction) that algebraically explains the truth of the goal. Let J (θ) be the expected cumulative reward: 

J (θ) = Eτ ∼πθ

" TX

> t=0

γtR(St, a t, St+1 )

#

(12) where γ is a discount factor and τ represents a reasoning trajectory. 

## 5.2 Algorithm 2: GNN Training with SRK 

We employ a policy gradient approach augmented by the physics-based feed-back of the SRK. The core mechanism separates the discrete search for logical structures (performed by the GNN) from the continuous optimization of witness parameters (performed by the SRK). 9Algorithm 2 Energy-Guided Policy Training 

Require: Initial Policy πθ , SRK Energy Function E(·) 

> 1:

for episode = 1 , . . . , M do  

> 2:

Sample problem instance ( Gprem , Ggoal ) 

> 3:

Algebraic Lifting: Convert constraints to polynomial set F0 =

{f1, . . . , f k} and goal h. 

> 4:

Initialize Witness Energy: e0 ← ∥ h∥2 ▷ Initially, no witness exists (gi = 0)  

> 5:

for step t = 0 , . . . , T max do  

> 6:

Discrete Action (Brain):  

> 7:

Sample action at ∼ πθ (·|G t) 

> 8:

Execute at to expand basis: Ft+1 ← Ft ∪ { fnew } 

> 9:

Continuous Optimization (SRK):  

> 10:

Solve for optimal witness coefficients g∗ over basis Ft+1 : 

> 11:

g∗ ← argmin g∥h − P 

> fi∈Ft+1

gifi∥2 

> 12:

Compute Residual Energy: et+1 ← ∥ h − P g∗ 

> i

fi∥2 

> 13:

Reward Calculation:  

> 14:

Rt ← (et − et+1 ) − λcost ▷ Reward energy reduction  

> 15:

Store tuple ( Gt, a t, R t, Gt+1 ) in buffer B 

> 16:

if et+1 < ϵ tol then  

> 17:

RT ← RT + Rsuccess  

> 18:

end if  

> 19:

end for  

> 20:

Policy Update:  

> 21:

Update θ via Proximal Policy Optimization (PPO) [15] on B. 

> 22:

end for 

## 5.3 The Logic of Dense Rewards 

A critical innovation of Mathesis is the resolution of the ”Deductive Paradox.” In standard model checking, a valid theorem implies that the goal is already entailed by the premises, potentially yielding zero energy everywhere. We cir-cumvent this by framing the proof task as Constructive Witness Finding using the Ideal Engine. The problem ”Prove h” is recast as ”Find coefficients gi such that h =P gifi.” 1. Initial State: At t = 0, the witness coefficients are implicitly zero. The initial energy is high: E0 = ∥h∥2 > 0. 2. Action Impact: When the GNN introduces a relevant lemma or auxil-iary construction, it expands the polynomial basis set F . This expansion increases the expressivity of the linear span span( F ). 3. Gradient Feedback: If the new basis element fnew allows for a better approximation of h (i.e., it is algebraically relevant to the goal), the SRK’s 10 optimization step will find a lower residual energy. The term ( et − et+1 )becomes positive, providing an immediate reward signal for that specific reasoning step. This ensures that the agent is rewarded not just for ”being right,” but for 

reducing the algebraic complexity required to verify the truth. 

## 5.4 Addressing the Cold Start Problem 

Random exploration in the infinite action space of mathematics is inefficient. To initialize the policy πθ in a region of high competence, we employ Imita-tion Learning prior to the energy-guided RL phase. We construct a dataset of formal proof traces Dexpert derived from established libraries (e.g., Lean’s Mathlib). The network is pre-trained using Behavior Cloning (BC) to minimize the cross-entropy loss between the predicted action distribution and the expert moves. This ”bootstraps” the Brain with standard reasoning heuristics, which are subsequently refined and verified by the physics-based feedback loop. 

# 6 Principled Reasoning via Guided Search 

The Hypergraph Transformer Brain provides rapid heuristic proposals based on learned mathematical intuition. However, to ensure logical convergence in deep derivation trees, we embed the Brain within a deliberative search framework where the SRK and a learned Value Head Vϕ prune unproductive branches. This coupling allows the system to navigate ”valleys” in the energy landscape where current logical energy is high, but the state is a necessary precursor to a proof [16]. 

## 6.1 Monte Carlo Tree Search (MCTS) 

For sequential deduction, we utilize MCTS to explore derivation paths. The Value Head Vϕ(S) estimates the probability of state S being part of a zero-energy proof, decoupling the search from purely greedy energy minimization. 11 Algorithm 3 MCTS-Mathesis Search 

Require: Root state S0, Policy πθ , Value Head Vϕ

Ensure: Optimal action sequence  

> 1:

for simulation i = 1 . . . N do  

> 2:

Scurr ← S 0, P ath ← []  

> 3:

Selection  

> 4:

while Scurr is not leaf do  

> 5:

a∗ = argmax a



Q(S, a ) + cpuct · πθ (a|S )

√P N (S,·)1+ N (S,a )

 

> 6:

Scurr ← Apply( a∗, Scurr ), P ath. append( Scurr ) 

> 7:

end while  

> 8:

Expansion & Evaluation  

> 9:

if Scurr is not terminal then  

> 10:

v ← Vϕ(Scurr ) ▷ Estimate future feasibility  

> 11:

Expand leaf using πθ (·|S curr ) 

> 12:

else  

> 13:

v ← 1.0 if E(Scurr ) < ϵ else 0 .0 

> 14:

end if  

> 15:

Backpropagation  

> 16:

for S ∈ P ath do  

> 17:

Update N (S) and Q(S) using value v 

> 18:

end for  

> 19:

end for 

## 6.2 Evolutionary Proof Search (EPS) 

For complex constructions requiring the recombination of disparate insights, we employ EPS. Unlike disjoint union operators, our crossover mechanism uti-lizes Semantic Unification to bridge derivation chains.The crossover opera-tor Unify( G1, G2) must identify when different populations have derived identical mathematical terms. We define a Canonicalization Function γ(v) that maps terms to a hash based on their structural definition. For example, the nodes rep-resenting the term ( x + y) and ( y + x) map to the same canonical representation under commutativity axioms. 12 Algorithm 4 Evolutionary Proof Search 

Require: Population P, Fitness f (G) = −E(G)

Ensure: State S where E(S) < ϵ  

> 1:

for generation g = 1 . . . G do  

> 2:

▷ Contextual Selection  

> 3:

Group individuals by their Assumption Set Apremise . Crossover is permitted only within groups to prevent merging contradictory branches (e.g., x > 0 and x < 0).  

> 4:

while |P next | < M do  

> 5:

Select compatible parents G1, G2. 

> 6:

▷ Semantic Unification  

> 7:

Gchild ← G 1 ∪ G 2 

> 8:

for pairs ( u ∈ G 1, v ∈ G 2) do  

> 9:

if γ(u) = γ(v) then  

> 10:

Merge u and v into a single node w. 

> 11:

Redirect all hyperedges incident to u, v to node w. 

> 12:

end if  

> 13:

end for  

> 14:

▷ Guided Mutation  

> 15:

Sample mutation a ∼ πθ (· | G child ) 

> 16:

Pnext .add(Apply( a, Gchild ))  

> 17:

end while  

> 18:

P ← P next  

> 19:

end for 

By merging nodes with identical canonical hashes, the Unify operator physi-cally connects the graph. If G1 proves A → B and G2 proves B → C, the unifica-tion of the node representing term B creates the continuous path A → B → C.The SRK then registers a zero-energy state for the goal C given premise A,achieving a derivation impossible for either parent to discover in isolation. 

# 7 Conclusion 

Mathesis establishes a blueprint for neuro-symbolic architectures where math-ematical logic is treated as a differentiable physical constraint. By unifying a generative Hypergraph Transformer with the Symbolic Reasoning Kernel (SRK), the system moves beyond the probabilistic limitations of standard large language models. The integration of System 2 search algorithms—specifically MCTS with learned value estimation and semantic-unification-based evolution-ary search—provides a rigorous mechanism for navigating the vast combinatorial space of formal proofs. The results presented in this preprint represent the first phase of our ex-perimental validation. We are currently scaling the training of the Hypergraph Transformer Brain using a combination of synthetic datasets generated by the 13 SRK and human-authored formal proofs from the Lean ‘mathlib‘ library. Pre-liminary evaluations on a curated subset of the ‘miniF2F‘ benchmark [17] in-dicate that the energy-guided dense rewards provided by the SRK significantly accelerate the discovery of valid derivation paths compared to sparse-reward baselines. The SRK’s ability to provide stable gradients through polynomial-form predicates (Section 1) has proven critical in maintaining training stability during complex geometric and algebraic constructions. The modular design of Mathesis allows for several avenues of expansion that we intend to explore in subsequent versions of this work: 1. Scaling and Knowledge Transfer: We aim to increase the parame-ter count of the Hypergraph Transformer and expand the SRK’s domain coverage to include real analysis, topology, and number theory. This in-volves defining new energy engines for continuous structures and modular arithmetic. 2. Automated Conjecture Generation: By detaching the SRK from a specific goal Pgoal and allowing the Brain to minimize the global energy of an unconstrained graph, the system can potentially discover new, logically consistent mathematical structures. This “dreaming mode” could lead to the automated generation of non-trivial conjectures. 3. Cross-Domain Application: The principle of logical energy is not lim-ited to mathematics. We anticipate that the Mathesis architecture can be adapted to other logic-intensive domains, such as verified program synthe-sis, where the SRK would enforce type safety and functional correctness, or molecular design, where the constraints are defined by chemical valency and stereochemistry. 

# A Mathematical Foundations of the SRK 

We establish the logical correctness and differentiability of the Symbolic Rea-soning Kernel’s energy functional. The framework is built upon the properties of non-negative real numbers and smooth functions. 

## A.1 Definitions 

Definition 3 (Parameter Manifold) . Let S be a mathematical state. The Pa-rameter Manifold X is the space of all possible concrete realizations of the terms in S (e.g., matrices, points, polynomial coefficients). We assume X has the structure of a smooth ( C∞) manifold. 

Definition 4 (Energy Fact) . A Fact f is a tuple (ϕf , E f ) where: 1. ϕf : X → { True , False } is the Semantic Predicate .2. Ef : X → R is the Energy Kernel .

14 For the system to be valid, every Energy Kernel Ef must satisfy the following axioms for any state x ∈ X:

(A1) Faithfulness: Ef (x) = 0 ⇐⇒ ϕf (x) = True. 

(A2) Non-Negativity: Ef (x) ≥ 0.

(A3) Smoothness: The function Ef is smooth ( Ef ∈ C∞(X)). 

Definition 5 (Total Energy) . For a set of facts F = {f1, . . . , f n}, the Total Energy Functional Etotal : X → R is the sum of individual energies: 

Etotal (x) = 

> n

X

> i=1

Efi (x) (13) 

## A.2 Core Lemma and Theorems 

The system’s validity rests on a fundamental property of non-negative real num-bers. 

Lemma 1 (Vanishing Sum of Non-Negatives) . Let {vi}ni=1 be a sequence of real numbers where vi ≥ 0 for all i. Then: 

> n

X

> i=1

vi = 0 ⇐⇒ ∀ i, v i = 0 

Proof. The ( ⇐) direction is trivial. For ( ⇒), assume for contradiction that P vi = 0 but there exists some vk > 0. Since all other vj ≥ 0, the sum must be P vi ≥ vk > 0. This contradicts the initial assumption. Thus, all vi must be zero. We now prove that minimizing the total energy is equivalent to satisfying the logical specification. 

Theorem 1 (Logical Correctness of the SRK) . For any state x ∈ X, the total energy defined in (13) is zero if and only if every fact in F is logically true. 

Etotal (x) = 0 ⇐⇒ ∀ f ∈ F , ϕ f (x) = True Proof. Let vi = Efi (x) for each fi ∈ F . By Axiom (A2) of Definition 4, each vi

is non-negative. The theorem’s premise, Etotal (x) = 0, is equivalent to P vi = 0. By Lemma 1, this holds if and only if vi = 0 for all i. Applying Axiom (A1) of Definition 4 to each term, the condition vi = Efi (x) = 0 is equivalent to the predicate ϕfi (x) being true. Therefore, the total energy vanishes if and only if all semantic predicates are satisfied. 

Theorem 2 (Differentiability of the SRK) . The total energy functional Etotal 

is a smooth function on the manifold X.Proof. From Axiom (A3) of Definition 4, each individual energy kernel Efi is a smooth function. The space of smooth functions on a manifold, C∞(X), is closed under finite addition. As Etotal is a finite sum of such functions as given by (13), it is also a member of C∞(X). 15 References 

[1] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucina-tion in natural language generation. ACM computing surveys , 55(12):1–38, 2023. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020. [3] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elic-its reasoning in large language models. Advances in neural information processing systems , 35:24824–24837, 2022. [4] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature ,625(7995):476–482, 2024. [5] Tim Rockt¨ aschel and Sebastian Riedel. End-to-end differentiable proving. 

Advances in neural information processing systems , 30, 2017. [6] Emily First, Markus N Rabe, Talia Ringer, and Yuriy Brun. Baldur: Whole-proof generation and repair with large language models (2023). 

arXiv preprint cs.LG/2303.04910 , 2023. [7] Bikram Pratim Bhuyan. Neuro-symbolic knowledge hypergraphs: knowl-edge representation and learning in neuro-symbolic artificial intelligence .PhD thesis, Universit´ e Paris-Saclay; University of Petroleum and Energy Studies . . . , 2025. [8] Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridg-ing deep learning and logical reasoning using a differentiable satisfiability solver. In International Conference on Machine Learning , pages 6545–6554. PMLR, 2019. [9] Hiroshi Kera, Yuki Ishihara, Yuta Kambe, Tristan Vaccon, and Kazuhiro Yokoyama. Learning to compute gr¨ obner bases. Advances in Neural Infor-mation Processing Systems , 37:33141–33187, 2024. [10] Grete Hermann. Die frage der endlich vielen schritte in der theorie der polynomideale. Mathematische Annalen , 95(1):736–788, 1926. [11] Negar Heidari and Alexandros Iosifidis. Geometric deep learning for computer-aided design: A survey. IEEE Access , 2025. [12] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. arXiv preprint arXiv:2301.09505 , 2023. 16 [13] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. 

Advances in neural information processing systems , 28, 2015. [14] Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. 

Advances in Neural Information Processing Systems , 36:21573–21612, 2023. [15] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017. [16] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. 

nature , 550(7676):354–359, 2017. [17] Azim Ospanov, Farzan Farnia, and Roozbeh Yousefzadeh. minif2f-lean re-visited: Reviewing limitations and charting a path forward. arXiv preprint arXiv:2511.03108 , 2025. 17