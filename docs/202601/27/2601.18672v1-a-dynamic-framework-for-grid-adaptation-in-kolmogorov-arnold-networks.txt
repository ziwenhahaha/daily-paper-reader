Title: A Dynamic Framework for Grid Adaptation in Kolmogorov-Arnold Networks

URL Source: https://arxiv.org/pdf/2601.18672v1

Published Time: Tue, 27 Jan 2026 03:12:49 GMT

Number of Pages: 7

Markdown Content:
# A Dynamic Framework for Grid Adaptation in Kolmogorov–Arnold Networks 

Spyros Rigas ∗, Thanasis Papaioannou ∗, Panagiotis Trakadas †, and Georgios Alexandridis ∗∗Department of Digital Industry Technologies, National and Kapodistrian University of Athens, Greece 

> †

Department of Port Management and Shipping, National and Kapodistrian University of Athens, Greece Email: {spyrigas, atpapaioannou, ptrakadas, gealexandri }@uoa.gr 

Abstract —Kolmogorov–Arnold Networks (KANs) have re-cently demonstrated promising potential in scientific machine learning, partly due to their capacity for grid adaptation during training. However, existing adaptation strategies rely solely on input data density, failing to account for the geometric complexity of the target function or metrics calculated during network training. In this work, we propose a generalized framework that treats knot allocation as a density estimation task governed by Importance Density Functions (IDFs), allowing training dynamics to determine grid resolution. We introduce a curvature-based adaptation strategy and evaluate it across synthetic function fitting, regression on a subset of the Feynman dataset and different instances of the Helmholtz PDE, demonstrating that it significantly outperforms the standard input-based baseline. Specifically, our method yields average relative error reductions of 25.3% on synthetic functions, 9.4% on the Feynman dataset, and 23.3% on the PDE benchmark. Statistical significance is confirmed via Wilcoxon signed-rank tests, establishing curvature-based adaptation as a robust and computationally efficient alternative for KAN training. 

Index Terms —Kolmogorov–Arnold networks, grid adaptivity, knot allocation, training dynamics, physics-informed machine learning 

I. I NTRODUCTION 

Kolmogorov–Arnold Networks (KANs) [1] have recently emerged as a promising alternative to Multilayer Perceptrons (MLPs), mitigating several of the shortcomings of MLP-based architectures, such as spectral bias [2], as well as demonstrating improved expressivity and interpretability [3]. These, along with other advantageous properties, have allowed architectures that use them as their primary backbone to find several applications in engineering and industrial domains [4]– [8]. However, their most profound impact has arguably been observed in the domain of scientific machine learning [9], with numerous applications and even state-of-the-art results in fields like Physics-Informed Machine Learning (PIML) [10]– [14] and operator learning [15], [16], among others [17]–[20]. Following the introduction of the original spline-based architecture, referred to as “vanilla” KANs, various stud-ies proposed replacing the B-spline basis with alternative, more computationally efficient functions, such as Chebyshev polynomials [10], ReLU-based [21], or Fourier-based [22] basis functions. While such approaches have seen success in specific problem settings [3], [23], the reliance on grid-independent basis functions precludes the possibility of fine-grained training via grid extension [1], [11], i.e., the capacity to start training with a coarse grid and progressively expand it to introduce more grid points (knots). Moreover, even when the grid resolution is fixed, the ability to adaptively reallocate knots during training – a process known as grid adaptation – has been shown to improve accuracy compared to using static grids [11], [24]. To this end, in this paper we focus on “fully adaptive” basis functions (adopting the terminology introduced in [11]), specifically the standard B-splines. Despite the benefits of full grid adaptivity, the strategies currently employed to determine knot allocation during grid adaptation remain largely restricted to input-based heuristics. In the seminal implementation [1], the authors introduced an adaptive strategy where knots are placed according to the density of each layer’s input points. Recently, [24] proposed the AdaptKAN framework to automate this process via moving histograms of the data; nonetheless, their solution also remains dependent on the input data distribution. In this work, we propose a broader framework for knot allocation that reduces to the aforementioned input-based strategies in special cases, but is also capable of utilizing training dynamics. This is achieved by defining Importance Density Functions (IDFs), which utilize metrics calculated during training on either a global or a per-layer basis. Although a diverse set of IDFs can be defined, in this work, we utilize the concept of curvature to guide knot allocation, drawing inspiration from classical B-spline approximation theory [25]. In particular, the main contributions of our work are the following:  

> •

We introduce a generalized framework for dynamic grid adaptation in KANs, demonstrating that the input-based method introduced in [1] can be obtained as the limiting case of a uniform IDF.  

> •

We propose a curvature-based adaptation strategy to guide knot allocation during training.  

> •

We evaluate our framework on function regression tasks using both a synthetic dataset of custom-curated functions and a subset of the Feynman dataset, as well as four instances of a forward PDE problem. The remainder of this paper is organized as follows. In Section II, we present the vanilla KAN layer, which is the foundational building block of the architectures used in our work. Section III introduces our proposed framework, defining the concept of IDFs and deriving the input-based strategy of 

> arXiv:2601.18672v1 [cs.LG] 26 Jan 2026

[1] as a special case. We also detail the specific curvature-based IDF utilized in this study. Section IV presents the experimental setup, followed by results across three distinct benchmarks: a suite of synthetic functions, a subset of the Feynman dataset [26] and different instances of a forward PDE problem. Finally, Section V summarizes our key findings, discusses current limitations and outlines directions for future research. II. K OLMOGOROV –A RNOLD NETWORKS 

A (vanilla) KAN is a collection of layers where each layer’s output, y ∈ Rnout , is related to its input, x ∈ Rnin , via: 

yj =

> nin

X

> i=1

rji R (xi) + cji G+kX

> m=1

bjim Bm (xi)

!

, (1) with j ∈ { 1, . . . , n out }. In this expression, 

R (x) = x

1 + exp ( −x) (2) is the Sigmoid Linear Unit (SiLU) function, and Bm (·) are B-spline basis functions of order k. The construction of these basis functions relies on a knot vector t, which is generated in two distinct steps. First, the input domain is partitioned into 

G intervals; the distribution of these intervals determines the local resolution of the spline. Second, this primary partition is augmented by extending the vector at its endpoints to ensure the basis functions provide appropriate coverage at the domain boundaries [1]. Importantly, the framework proposed in this work focuses solely on the determination of the primary partition, as the subsequent augmentation is a straightforward, deterministic procedure. As indicated in Eq. (1), for a grid of G intervals and spline order k, the resulting basis contains G + k trainable coefficients. Consequently, the representational power of the network is governed not only by the network depth and width, but also by the specific allocation of the knots in t,which motivates our study. For the purposes of this work, the trainable scaling weights are initialized as cji = 1 , following [1], while the trainable residual and basis weights ( rji and 

bjim ) are initialized following the Glorot-inspired strategy introduced in [27]. III. P ROPOSED FRAMEWORK 

We approach the problem of knot allocation as a density estimation task. Our objective is to determine a knot vector 

t for a given KAN layer such that the density of the grid points mirrors an underlying Importance Density Function (IDF), which characterizes the “significance” of specific re-gions within the layer’s input domain. While the standard KAN implementation inherently assumes that importance is equivalent to the input data probability density function, we generalize this by allowing importance to be derived from the training dynamics of the network itself. 

A. Generalized Knot Allocation 

Let {x(s)}Nb 

> s=1

denote a batch of input coordinates incident to a specific layer, where x(s) ∈ Rnin . For a specific feature dimension d ∈ { 1, . . . , n in }, we seek to distribute knots based on a set of scalar weights {w(s)}Nb

> s=1

, where w(s) assigns a local importance score to the coordinate x(s) 

> d

.We define the discrete empirical IDF as a probability mass function over the input samples: 

P



x(s)

> d



= w(s)

PNb 

> j=1

w(j) . (3) To determine the knot positions, we construct the weighted empirical cumulative distribution function, denoted as ˆFd(z).This is a step function bounded in [0 , 1] :

ˆFd(z) = 

> Nb

X

> s=1

P



x(s)

> d



· I



x(s) 

> d

≤ z



, (4) where I(·) is the indicator function. The knot vector t is obtained by inverting this distribution via the generalized inverse distribution function (quantile function): 

tm = ˆF −1 

> d

(qm) = inf {z ∈ R : ˆFd(z) ≥ qm}, (5) where qm = m/ (G + 1) represents the target quantile for the 

m-th knot. In practice, this inversion is performed by sorting the samples along dimension d such that x(1)  

> d

≤ · · · ≤ x(Nb) 

> d

,computing the cumulative sum of their sorted weights, and selecting the coordinate x(p) 

> d

where the cumulative mass first exceeds the threshold qm.

B. Input-Based Adaptation as a Special Case 

The standard grid adaptation strategy employed in the original KAN implementation [1] allocates knots solely based on the input data distribution. The underlying rationale is that regions with high data density require finer resolution to minimize approximation error. Within our proposed framework, this strategy is retrieved as a special case by selecting a uniform IDF, where the importance weights are constant for all samples: 

w(s) 

> uniform

= 1 , ∀s ∈ { 1, . . . , N b}. (6) Under this condition, the resulting grid has uniform mass with respect to the sample count, thus concentrating knots in regions of high input density. While effective for ensuring that basis functions are active where data exists, this approach ignores the complexity of the function being approximated, potentially allocating excessive resolution to linear regions simply because they are oversampled. C. Curvature-Based Adaptation 

Classical spline approximation theory links optimal knot placement to the derivatives of the target function. Specifically, de Boor [25] demonstrates that the asymptotic knot density should scale with |f (k+1) (x)|1/(k+1) . While for the cubic B-splines utilized in this work ( k = 3) this theoretically points to the fourth derivative, we adopt the curvature as our guiding metric. This choice is motivated by the fact that by allocating knots proportional to curvature, we ensure that the grid densifies in regions exhibiting significant geometric features, while rarefying in linear regions. This serves as a basic implementation of our framework, noting that it is straightforward to define a corresponding IDF for higher-order derivatives. We estimate the local curvature at a sample point via the diagonal of the Hessian of the layer’s response. Specifically, let Φ( x) : Rnin → Rnout denote the mapping learned by the layer. When adapting the grid for the d-th input coordinate, we define the importance weight w(s) 

> curv

as the aggregated second-order partial derivative along that axis: 

w(s) 

> curv

=

> nout

X

> j=1

∂2Φj

∂x 2

> d



x(s)

+ ϵ, (7) where ϵ is a small smoothing constant to ensure strict positiv-ity. In practice, Eq. (7) can be computed exactly in frameworks with automatic differentiation such as JAX [28], otherwise it can be approximated via central finite differences. 

D. An Illustrative Example 

To demonstrate the difference between input-based and curvature-based adaptation, we consider a 1D regression task involving a highly localized feature: f (x) = exp  −200 x2.The training dataset consists of N = 1000 samples drawn from a uniform distribution over [−1, 1] . We train a KAN with architecture [1 , 10 , 1] for 1000 training iterations using a coarse initial grid of G = 3 intervals. At the final epoch, we perform grid extension, increasing the grid resolution to 

G = 10 while simultaneously adapting the knot positions using the two strategies. Figure 1 visualizes the outcome of this experiment. The top row displays the function approximation at the moment of grid update. The middle and bottom rows depict the knot allocation for the input-based and curvature-based strategies, respectively. As expected, the input-based strategy mirrors the uniform data distribution, resulting in an almost equidistant grid that wastes capacity on the linear “tails” of the Gaussian where f (x) ≈ 0. Conversely, the curvature-based strategy identifies the region of high second-derivative magnitude. It condenses the knots near the peak ( x ≈ 0) and the inflection points, providing dense support exactly where the function requires high resolution, while rarefying the grid in the flat regions. We emphasize that this experiment serves primarily as a visual demonstration of the studied strategies; a quantitative evaluation follows in Section IV. 0.0          

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Model Output
> Training Samples
> Input-Based
> Curvature-Based
> Input-Based Knots
> 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
> Model Input
> Curvature-Based Knots
> Fig. 1. Visual comparison of grid adaptation strategies on a 1D sharp Gaussian regression task. (Top) The model predictions from the input-based (dashed red) and curvature-based (dashed blue) KAN models. The training data, sampled from a uniform distribution, is shown in light gray. (Middle) The resulting knot allocation for the input-based strategy. (Bottom) The knot allocation for the proposed curvature-based strategy.

IV. E XPERIMENTAL RESULTS 

In this section, we present experiments across three distinct benchmarks: a suite of custom synthetic functions, a subset of the dimensionless Feynman dataset and four instances of the Helmholtz equation within the PIML setting [29]. Across all benchmarks, we compare two primary configurations: (i) a baseline model using the standard input-based adaptation strategy (equivalent to uniform IDF), and (ii) a model em-ploying the proposed curvature-based adaptation. To isolate the impact of knot allocation, all other hyperparameters remain identical between the two configurations, utilizing the Adam optimizer to minimize the Mean Squared Error (MSE) loss. Training is initiated with a coarse grid of G = 3 intervals. We employ a progressive grid extension schedule, increasing the resolution to G = 6 , G = 9 , and finally G = 12 at predefined intervals throughout the training process. At each extension step, the knot positions are reallocated according to the respective adaptation strategy. To ensure the statistical robustness of our findings, we conduct 3 independent runs for each experiment using different random seeds. Performance is evaluated using the median relative L2 error across the 3 runs with respect to the reference solution defined on a dense grid. Furthermore, to statistically validate our comparisons, we perform a one-tailed Wilcoxon signed-rank test [30]. The null hypothesis posits that the curvature-based strategy does not yield a statistically signifi-cant reduction in error compared to the input-based baseline. All experiments are conducted using the jaxKAN framework [31] on a single NVIDIA GeForce RTX 4090 GPU. For further information about the benchmark functions, see the Reproducibility Statement. A. Benchmark I: Synthetic Functions 

We first define a custom curated benchmark consisting of 10 synthetic functions (from 1 to 6 dimensions), selected to exhibit challenging characteristics such as highly localized features, sharp discontinuities and varying dimensionalities. This suite is designed to explicitly test whether the proposed curvature-based IDF can effectively allocate resolution to “hard” regions of the domain better than the input density. For each function, we sample N = 4000 training points and train a KAN model with a hidden dimension of 10 for 2000 iterations using a constant learning rate of η = 10 −2.The progressive grid updates are performed at iterations 500, 1000 and 1500. Table I presents the final relative L2 error for both strategies. We also report the relative percent im-provement provided by the curvature method, where positive values denote a reduction in error and negative values indicate deterioration. The results indicate that the curvature-based strategy outper-forms the input-based baseline in 9 out of 10 cases, yielding an average relative improvement of 25.31% and reaching a maximum error reduction of over 50% for function f2.Notably, the curvature-based results generally exhibit lower standard deviations compared to the baseline, suggesting that the proposed method offers not only higher accuracy but also improved consistency across independent runs. The one-tailed Wilcoxon signed-rank test yields a p-value of 0.042; since 

p < 0.05 , we reject the null hypothesis, concluding that the curvature-based method provides a statistically significant re-duction in approximation error. Regarding computational cost, the average wall-clock time per run is 15.47s for the input-based method compared to 17.13s for the curvature-based approach. This approximate 10% increase is to be expected, since the IDF in the curvature-based case is more complex than the uniform IDF of the input-based one (compare Eq. (7) to Eq. (6)), and can be considered a negligible trade-off given the substantial gains in predictive performance. 

B. Benchmark II: Feynman Dataset 

While the previous benchmark was specifically curated to highlight the advantages of curvature-based adaptation on                                                                                                                           

> TABLE I COMPARISON OF RELATIVE L2ERROR ON SYNTHETIC FUNCTIONS
> (M EDIAN ±STD OVER 3SEEDS )
> Function Input-Based Curvature-Based Improv.
> f1(3 .4±0.4) ·10 −2(2.9±0.4)·10 −2+14.61%
> f2(2 .4±0.7) ·10 −3(1.0±1.0)·10 −3+57.93%
> f3(1 .3±0.3) ·10 −2(1.0±0.1)·10 −2+25.69%
> f4(2 .9±2.4) ·10 −3(1.9±1.4)·10 −3+35.51%
> f5(1 .1±0.5) ·10 −2(6.7±1.3)·10 −3+36.70%
> f6(2 .4±3.3) ·10 −3(2.1±1.3)·10 −3+11.49%
> f7(8 .6±8.6) ·10 −4(6.5±2.7)·10 −4+25.09%
> f8(6 .7±23 .5) ·10 −3(5.1±1.8)·10 −3+24.71%
> f9(9 .8±3.9) ·10 −3(7.1±3.4)·10 −3+27.80%
> f10 (8.1±5.6)·10 −2(8 .6±3.7) ·10 −2-6.42%

functions with a certain geometric complexity, a robust knot allocation strategy should, at minimum, match the perfor-mance of the baseline on generic regression tasks. To evaluate this generalizability, we turn to a subset of the dimensionless Feynman dataset [26], consisting of 15 physics equations which have been utilized in prior KAN literature [1], [27]. For this benchmark, we employ a KAN architecture with a hidden dimension of 10. Training is conducted for 2000 iterations using a learning rate of η = 10 −3. Consistent with the previous experiment, grid extension and adaptation steps are performed at iterations 500, 1000 and 1500. The comparative results in terms of final relative L2 error are summarized in Table II. The results indicate that the proposed curvature-based strat-egy consistently outperforms the input-based baseline, achiev-ing a lower median error in 13 out of 15 test cases. The two exceptions (I.18.4 and I.27.6) represent cases where both strategies fail to converge to a low-error solution, resulting in negligible differences in performance. While the improvement is less pronounced than in the synthetic benchmark, yielding an average reduction of 9.44% and a maximum of approxi-mately 20% for function I.13.12, this is expected, as physical laws generally yield smoother, well-behaved functions com-pared to the synthetic examples considered in Benchmark I. Nonetheless, the consistency of these gains results in strong statistical significance. The one-tailed Wilcoxon signed-rank test yields a p-value of 1.5 · 10 −4 (p ≪ 0.05 ), leading to a decisive rejection of the null hypothesis. This confirms that even for the Feynman dataset’s functions, curvature-based adaptation provides a statistically significant advantage. Finally, the computational overhead is consistent to previous results, with average training times of 15.36s for the input-based method versus 17.62s for the curvature-based approach.                                                                                                                                                                                  

> TABLE II COMPARISON OF RELATIVE L2ERROR ON FEYNMAN DATASET
> (M EDIAN ±STD OVER 3SEEDS )
> Index Input-Based Curvature-Based Improv.
> I.6.2 (6 .0±1.3) ·10 −1(5.5±1.6)·10 −1+8.48% I.12.11 (3 .4±1.0) ·10 −4(3.2±2.7)·10 −4+7.74% I.13.12 (9 .4±0.5) ·10 −1(7.5±1.4)·10 −1+20.68% I.16.6 (2 .2±0.1) ·10 −2(2.2±0.0)·10 −2+0.81% I.18.4 (1.0±0.0)·10 0(1.0±0.0)·10 00.00% I.27.6 (1.0±0.0)·10 0(1.0±0.0)·10 00.00% I.29.16 (5 .6±1.5) ·10 −3(5.4±0.8)·10 −3+4.08% I.30.3 (5 .5±1.3) ·10 −4(5.2±1.7)·10 −4+5.18% I.40.1 (9 .0±1.7) ·10 −4(7.4±3.0)·10 −4+18.40% II.2.42 (5 .1±2.9) ·10 −4(4.4±1.5)·10 −4+14.36% II.6.15a (1 .8±0.7) ·10 −2(1.7±0.7)·10 −2+8.94% II.11.7 (2 .2±0.3) ·10 −3(1.9±0.4)·10 −3+10.66% II.35.18 (4 .5±2.1) ·10 −4(3.7±6.7)·10 −4+17.51% II.36.38 (2 .1±0.5) ·10 −3(1.8±0.6)·10 −3+14.81% III.17.37 (2 .3±0.3) ·10 −3(2.1±0.3)·10 −3+9.89%

C. Benchmark III: Helmholtz Equation 

To evaluate the proposed framework beyond standard re-gression, we consider the domain of PIML, where KANs have demonstrated significant potential [23]. Specifically, we solve the forward problem for the 2D Helmholtz equation – a standard benchmark that presents non-trivial challenges compared to simpler problems like the 1D heat equation, while remaining solvable without adaptive training techniques [13]. We consider the domain Ω = [ −1, 1] 2 and seek the solution 

u(x, y ) satisfying: 

∆u + k2(x, y )u = f (x, y ), (x, y ) ∈ Ω, (8) 

u(x, y ) = 0 , (x, y ) ∈ ∂Ω, (9) where f (x, y ) is the forcing term corresponding to the true solution utrue (x, y ) = sin( a1πx ) sin( a2πy ), and k(x, y ) = 1 .We investigate the impact of grid adaptation across four distinct parameter configurations representing increasing fre-quency: (a1, a 2) ∈ { (1 , 1) , (1 , 2) , (2 , 2) , (2 , 4) }1.For training, we follow the standard procedure of minimiz-ing a composite physics-informed loss function comprising a PDE residual and a boundary condition term [29]. To this end, we utilize a uniform grid of Nc = 64 × 64 = 4096 collocation points to enforce the PDE residual and Nb = 64 points along each of the four edges ( 256 total) to enforce the Dirichlet boundary conditions. We employ a deeper KAN architecture with two hidden layers of width 6 and train model instances for 5000 iterations with a learning rate of η = 10 −3. The grid extensions and subsequent adaptations are performed at iterations 1000, 2000, and 3000. The quantitative results on this benchmark are visualized in the grouped bar chart of Figure 2, where the bars represent median L2 error relative to the reference solution utrue and the error bars correspond to the standard deviation. Consistent with the previous benchmarks, the curvature-based strategy outperforms the baseline across all investigated parameter configurations. While the logarithmic scale of Fig-ure 2 visually compresses the differences, the quantitative gains are substantial: the proposed method yields relative improvements of 34.20% for (1 , 1) , 3.15% for (1 , 2) , 27.65% for (2 , 2) , and 28.10% for the high-frequency case (2 , 4) . This corresponds to an average reduction in error of 23.27%. Besides the improvement in median accuracy, we again observe that the curvature-based results exhibit lower vari-ance across independent seeds compared to the baseline. This reinforces the hypothesis that curvature-based adaptation leads to more robust training outcomes. Due to the small sample size ( N = 4 configurations), a Wilcoxon signed-rank test is statistically irrelevant; although the curvature method outperforms the baseline in every single instance (yielding the minimum possible p-value of 0.0625 ), this falls just above the substantial threshold of 0.05 . Nonetheless, the consistent superiority across all tests, combined with the clear               

> 1Preliminary experiments indicated that for higher frequencies, e.g., (4 ,4) ,training tends to diverge regardless of the grid adaptation strategy employed. (a1,a2) = 1, 1 (a1,a2) = 1, 2 (a1,a2) = 2, 2 (a1,a2) = 2, 4
> 10 2
> 10 1
> 10 0
> Relative  L2 Error (Median)
> Input-Based
> Curvature-Based
> Fig. 2. Comparative evaluation on the Helmholtz PDE benchmark across four increasing frequency configurations. The grouped bar chart showcases the median relative L2error in logarithmic scale, with error bars indicating the standard deviation over 3 independent runs.

physical intuition – that knots should cluster in regions of high wavefront curvature rather than being distributed uniformly – strongly supports the efficiency of the method. Finally, we note that as the complexity of the training task increases, the relative computational cost of the proposed method diminishes. For this benchmark, which involves deeper networks and 5000 training iterations, the average wall-clock time is 61.21s for the input-based method and 64.30s for the curvature-based approach. This represents an overhead of only ≈ 5% , significantly lower than in previous benchmarks. This trend suggests that for large-scale scientific machine learning tasks involving prolonged training, the additional cost of computing training dynamics becomes negligible relative to the total training time, rendering the two methods practically equivalent in terms of computational expense. V. C ONCLUSION & O UTLOOK 

In this work, we introduced a generalized framework for grid adaptation in Kolmogorov–Arnold Networks, re-framing the problem of knot allocation as a density estimation task governed by Importance Density Functions. We demonstrated that the standard input-based adaptation strategy employed in existing KAN implementations is a special case of this framework corresponding to a uniform IDF. We proposed a curvature-based adaptation strategy that aligns grid resolution with the geometric complexity of the target function. Through experiments on regression and physics-informed PDE solv-ing, we showed that this approach consistently outperforms the standard input-based baseline, achieving significant error reductions and improved training stability with negligible computational overhead. While our results are promising, several limitations of the present study warrant discussion. First, this work primarily established the theoretical framework and evaluated a single alternative IDF (curvature) against the standard input-based method. While curvature is geometrically motivated, it repre-sents only one possible heuristic for importance; other metrics derived from the training process could potentially yield supe-rior results. Moreover, our evaluation was concentrated on the domain of scientific machine learning tasks. While KANs have shown particular promise in this field, they are a fundamental neural architecture applicable to broader domains. We did not evaluate our method on tasks with high-dimensional data such as audio or image processing, partly because KANs have yet to establish state-of-the-art performance in these areas comparable to architectures like Transformers. Finally, the experiments conducted here utilized relatively compact architectures (1-2 hidden layers) appropriate for the regression and PDE tasks at hand. As evidenced by the low relative errors achieved (orders 10 −2 to 10 −4 for regression), these capacities were sufficient. However, it remains to be seen how the proposed method scales to significantly deeper, parameter-heavy architectures required for more complex tasks, although the observed trend of diminishing relative computational cost suggests favorable scaling. The proposed framework opens several avenues for future research. A direct extension involves the design and evaluation of novel IDFs. For instance, importance could be derived from per-sample loss values, directing grid resolution specifically to regions where the network currently struggles to generalize, or via higher-order derivative metrics as suggested by approx-imation theory. Another promising direction lies in the timing of grid updates. Currently, they occur at fixed, predetermined intervals. A more automated approach, similar to what is proposed in the AdaptKAN framework [24], could involve defining “trigger” metrics monitored during training – such as plateaus in the loss landscape or spikes in validation error – to automatically initiate grid adaptation or extension steps. This would move KANs towards fully autonomous self-adaptation, further reducing the need for manual hyperparameter tuning in scientific computing workflows. AI U SAGE STATEMENT 

Large Language Models (LLMs) were used throughout this work for grammar and syntax refinement only; all ideas, technical content and conclusions remain the authors’ work. REPRODUCIBILITY STATEMENT 

The full code (including selected seeds for each exper-iment) and the processed data are publicly available at 

https://github.com/srigas/kan_grid .REFERENCES [1] Z. Liu et al., “KAN: Kolmogorov–arnold networks,” in The Thirteenth International Conference on Learning Representations , 2025. [2] Y. Wang, J. W. Siegel, Z. Liu, and T. Y. Hou, “On the expressiveness and spectral bias of KANs,” in The Thirteenth International Conference on Learning Representations , 2025. [3] A. Noorizadegan, S. Wang, L. Ling, and J. P. Dominguez-Morales, “A practitioner’s guide to Kolmogorov-Arnold networks,” arXiv pre-print ,2025. [4] Y. Peng et al., “Predictive modeling of flexible EHD pumps using Kolmogorov–Arnold networks,” Biomim. Intell. Robot. , vol. 4, no. 4, p. 100184, 2024. [5] M. Abd Elaziz, I. Ahmed Fares, and A. O. Aseeri, “CKAN: Convo-lutional Kolmogorov–Arnold networks model for intrusion detection in IoT environment,” IEEE Access , vol. 12, pp. 134 837–134 851, 2024. [6] S. Rigas, M. Papachristou, I. Sotiropoulos, and G. Alexandridis, “Ex-plainable fault classification and severity diagnosis in rotating machinery using Kolmogorov–Arnold networks,” Entropy , vol. 27, no. 4, 2025. [7] Z. W. et al., “An intrusion detection model based on convolutional Kolmogorov-Arnold networks,” Sci. Rep. , vol. 15, p. 1917, 2025. [8] T. Ansar and W. M. Ashraf, “Comparison of Kolmogorov–Arnold networks and multi-layer perceptron for modelling and optimisation analysis of energy systems,” Energy AI , vol. 20, p. 100473, 2025. [9] Z. Liu, M. Tegmark, P. Ma, W. Matusik, and Y. Wang, “Kolmogorov-Arnold networks meet science,” Phys. Rev. X , vol. 15, p. 041051, 2025. [10] K. Shukla, J. D. Toscano, Z. Wang, Z. Zou, and G. E. Karniadakis, “A comprehensive and FAIR comparison between MLP and KAN represen-tations for differential equations and operator networks,” Comput. Meth. Appl. Mech. Eng. , vol. 431, p. 117290, 2024. [11] S. Rigas, M. Papachristou, T. Papadopoulos, F. Anagnostopoulos, and G. Alexandridis, “Adaptive training of grid-dependent physics-informed Kolmogorov-Arnold networks,” IEEE Access , vol. 12, pp. 176 982– 176 998, 2024. [12] Y. Wang et al., “Kolmogorov–Arnold-informed neural network: Aphysics-informed deep learning framework for solving forward and inverse problems based on Kolmogorov–Arnold networks,” Comput. Meth. Appl. Mech. Eng. , vol. 433, p. 117518, 2025. [13] S. Rigas, F. Anagnostopoulos, M. Papachristou, and G. Alexandridis, “Training deep physics-informed Kolmogorov-Arnold networks,” arXiv pre-print , 2025. [14] N. Cerardi, E. Tolley, and A. Mishra, “Solving the cosmological Vlasov–Poisson equations with physics-informed Kolmogorov–Arnold networks,” Mon. Not. R. Astron. Soc. , vol. 545, no. 4, p. staf2241, 2025. [15] D. W. Abueidda, P. Pantidis, and M. E. Mobasher, “DeepOKAN: Deep operator network based on Kolmogorov Arnold networks for mechanics problems,” Comput. Meth. Appl. Mech. Eng. , vol. 436, p. 117699, 2025. [16] J. Lee et al., “KANO: Kolmogorov-Arnold neural operator,” in The Fourteenth International Conference on Learning Representations , 2026. [17] B. C. Koenig, S. Kim, and S. Deng, “KAN-ODEs: Kolmogorov–Arnold network ordinary differential equations for learning dynamical systems and hidden physics,” Comput. Meth. Appl. Mech. Eng. , vol. 432, p. 117397, 2024. [18] A. Kashefi, “Kolmogorov–Arnold PointNet: Deep learning for prediction of fluid fields on irregular geometries,” Comput. Meth. Appl. Mech. Eng. ,vol. 439, p. 117888, 2025. [19] S. Panahi, M. Moradi, E. M. Bollt, and Y.-C. Lai, “Data-driven model discovery with Kolmogorov-Arnold networks,” Phys. Rev. Res. , vol. 7, p. 023037, 2025. [20] S. Teymoor Seydi, M. Sadegh, and J. Chanussot, “Kolmogorov–Arnold network for hyperspectral change detection,” IEEE Trans. Geosci. Re-mote Sens. , vol. 63, pp. 1–15, 2025. [21] C. C. So and S. P. Yung, “Higher-order-ReLU-KANs (HRKANs) for solving physics-informed neural networks (PINNs) more accurately, robustly and faster,” in 2025 IEEE World AI IoT Congress (AIIoT) , 2025, pp. 1035–1042. [22] J. Zhang, Y. Fan, K. Cai, and K. Wang, “Kolmogorov-Arnold Fourier networks,” 2025. [23] J. D. Toscano et al., “From PINNs to PIKANs: recent advances in physics-informed machine learning,” Mach. Learn. Comput. Sci. Eng ,vol. 1, p. 15, 2025. [24] J. Moody and J. Usevitch, “Automatic grid updates for Kolmogorov-Arnold networks using layer histograms,” arXiv pre-print , 2025. [25] C. De Boor, A practical guide to splines . Springer New York, 1978. [26] S.-M. Udrescu and M. Tegmark, “AI Feynman: A physics-inspired method for symbolic regression,” Sci. Adv. , vol. 6, no. 16, p. eaay2631, 2020. [27] S. Rigas, D. Verma, G. Alexandridis, and Y. Wang, “Initialization schemes for Kolmogorov-Arnold networks: An empirical study,” in The Fourteenth International Conference on Learning Representations , 2026. [28] J. Bradbury et al., “JAX: composable transformations of Python+NumPy programs,” 2018. [29] M. Raissi, P. Perdikaris, and G. E. Karniadakis, “Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,” J. Comput. Phys. , vol. 378, pp. 686–707, 2019. [30] F. Wilcoxon, “Individual comparisons by ranking methods,” Biometrics Bulletin , vol. 1, no. 6, pp. 80–83, 1945. [31] S. Rigas and M. Papachristou, “jaxKAN: A unified JAX framework for Kolmogorov-Arnold networks,” Journal of Open Source Software ,vol. 10, no. 108, p. 7830, 2025.