Title: Data-driven Interpretable Hybrid Robot Dynamics

URL Source: https://arxiv.org/pdf/2512.11900v1

Published Time: Tue, 16 Dec 2025 01:03:17 GMT

Number of Pages: 13

Markdown Content:
# Data-driven Interpretable Hybrid Robot Dynamics 

Christopher E. Mower 1, Rui Zong 1, Haitham Bou-Ammar 1,21Huawei, Noah’s Ark Lab, London, UK 

> 2

University College London, London, UK 

Abstract —We study data-driven identification of interpretable hybrid robot dynamics, where an analytical rigid-body dynamics model is complemented by a learned residual torque term. Using symbolic regression and sparse identification of nonlinear dynamics (SINDy), we recover compact closed-form expressions for this residual from joint-space data. In simulation on a 7-DoF Franka arm with known dynamics, these interpretable models accurately recover inertial, Coriolis, gravity, and viscous effects with very small relative error and outperform neural-network baselines in both accuracy and generalization. On real data from a 7-DoF WAM arm, symbolic-regression residuals generalize substantially better than SINDy and neural networks, which tend to overfit, and suggest candidate new closed-form formulations that extend the nominal dynamics model for this robot. Overall, the results indicate that interpretable residual dynamics models provide compact, accurate, and physically meaningful alternatives to black-box function approximators for torque prediction. 

I. I NTRODUCTION 

Accurate estimation of a robot’s dynamics (i.e., its equations of motion) is crucial for applications such as force control [1] and external torque estimation [2]. However, these dynamics depend on many interacting factors (e.g., mechanical structure, payload, contact conditions), making modeling difficult. As a result, purely model-based approaches often become imprac-tical, requiring considerable expertise to derive the models. In parallel, the emergence of large-scale vision–language– action (VLA) models [3–5] is rapidly changing how we think about robot controllers: instead of hand-designed pipelines, robots are increasingly treated as black-box embodiments that map images and language instructions to low-level actions. However, current VLA datasets and policies are almost ex-clusively built around visual, linguistic, and proprioceptive signals; explicit force or torque measurements are rarely available at scale, and only a small number of recent works explore force- or touch-augmented foundation models [6–8]. These works suggest that explicitly treating force and touch as first-class modalities can substantially improve performance on contact-rich manipulation tasks, but they also highlight a key bottleneck: high-quality force/torque and tactile sensors are expensive, difficult to integrate across many platforms, and often produce noisy, robot-specific signals, so most robots in large datasets simply do not have them. If we could reliably infer internal and external joint torques from readily available signals (e.g. encoder-based kinematics and motor torques) using an interpretable dynamics model, we would effectively equip fleets of otherwise “blind-to-force” robots with vir-tual force sensors at negligible hardware cost. This would both enable scalable collection of contact-rich supervision for future VLA-style policies and provide physically grounded signals (estimated forces and torques) that can be used as additional targets or auxiliary tasks during training, tightening the connection between high-level foundation models and the underlying mechanics of the robot. From a reinforcement learning (RL) perspective, good phys-ical priors over the robot’s dynamics are equally important. A long line of work in legged locomotion has shown that combining deep RL with accurate simulators and carefully designed structure can produce remarkably robust controllers for quadrupeds in the real world [9–12]. In these systems, the policy is typically trained in a high-fidelity rigid-body simu-lator with hand-engineered rewards and contact models, and then transferred to hardware via domain randomization and other robustness techniques. While this pipeline has enabled blind locomotion over challenging terrain [10] and rapid policy training in minutes [11], it remains heavily reliant on accurate but opaque simulators, and the learned policies themselves provide little direct insight into the underlying mechanics. Interpretable models of joint torques and contact forces offer a complementary route: they can serve as structured priors or residual models inside model-based or hybrid RL schemes, provide physically meaningful features and auxiliary targets for value and policy networks, and constrain exploration to behavior that remains consistent with known dynamics. In the context of contact-rich tasks surveyed by Ha et al. [12], such physically grounded priors are crucial for improving sample efficiency, safety, and out-of-distribution generalization, espe-cially when training directly on hardware or across diverse robot morphologies. Together with the rapid growth of AI techniques, these challenges have motivated researchers to explore data-driven approaches based on neural networks. Although such models can outperform traditional model-based methods and adapt to new scenarios, their black-box nature raises concerns about trust and robustness. In contrast, methods that derives explicit mathematical expressions directly from data, without assuming a predefined model structure, e.g,, symbolic/sparse regression. This makes these methods particularly useful for describing physical phenomena that lack explicit mathematical formula-tions or are too complex to derive analytically. Moreover, they have been observed to uncover new, interpretable relationships hidden in the data, making it well suited for approximating complex behaviors such as friction torque, whose precise characterization is difficult due to its dependence on multiple interacting factors. A useful historical analogy is the progression from Ke-

> arXiv:2512.11900v1 [cs.RO] 10 Dec 2025

pler’s and Newton’s descriptions of planetary motion. Kepler’s laws provided an accurate, data-driven description of orbits, but they remained essentially empirical and system-specific. Newton’s formulation, on the other hand, introduced a simple and interpretable law of gravitation that explained Kepler’s observations, generalized far beyond the original data, and ul-timately enabled engineered capabilities such as interplanetary navigation. In a similar spirit, a highly accurate but opaque neural network model of joint torques plays a role akin to Kepler’s descriptive laws, whereas an explicit, low-complexity model of the robot dynamics corresponds more closely to Newton’s formulation: it can be inspected, trusted, and reused for analysis, controller design, and extrapolation beyond the training conditions. This motivates the search for data-driven models of robot dynamics that are not only accurate, but also sparse and interpretable. In this paper, we focus on developing interpretable and accurate dynamic models. Concretely, we model joint-space dynamics as an analytical rigid-body model plus a data-driven residual, and investigate the affect of using symbolic regression and SINDy-style sparse regression on joint-space features to identify closed-form expressions for this residual from data. We validate the approach in simulation on a 7-DoF Franka arm with known dynamics, showing that the recovered expressions accurately reconstruct known dynamics equations and substantially outperform neural-network baselines in both accuracy and generalization. We then apply the same method-ology to real data from a 7-DoF WAM arm, comparing symbolic and sparse models against neural networks, and analyzing how each class trades off in-sample fit, out-of-sample generalization, and interpretability. Throughout, we emphasize models that not only predict joint torques well but also expose physically meaningful structure that can be inspected and potentially reused for tasks such as virtual force sensing, controller design, and physically grounded robot learning. II. P ROBLEM FORMULATION 

For an n-DoF robot, the joint-space dynamics is given by 

τm = τdyn + τext , (1) where τm ∈ Rn are the motor joint torques, τdyn ∈ Rn are the torques due to the internal robot dynamics, and τext ∈ Rn

are external joint torques. A common model used to estimate τdyn is the rigid-body dynamics equations of motion 

τrbd (q, ˙q, ¨q) = M (q)¨ q + C(q, ˙q) ˙ q + τg (q), (2) where M (·) ∈ Rn×n is the inertia matrix, C(·) ∈ Rn×n is the Coriolis matrix, τg (·) ∈ Rn is the gravitational torque, and 

q, ˙q, ¨q ∈ Rn are the joint position, velocity, and acceleration, respectively. For sufficiently rigid/heavy robots whose dynam-ics are dominated by inertial effects, τrbd often provides a good approximation of τdyn . In general, however, unmodeled effects (e.g., friction, actuator dynamics, or flexibilities) can lead to a significant mismatch between the true internal dynamics τdyn 

and the model τrbd , i.e. 

τdyn = τrbd + ϵ, (3) where ϵ ∈ Rn denotes the modeling error such that ∥ϵ∥ is non-negligible. The error can depend on nonlinear relationship with the motion of the joints, coupling effects of the joints, manufacturing mismatches, and potentially many other effects (e.g., temperature). Deriving a model for ϵ by hand is error-prone and can be tedious for researchers. On the other hand, as mentioned above, neural networks have been observed to be good predictors of ϵ, however, they are not interpretable, leading to lack of trust in real world applications. The goal of this work is to discover an interpretable, data-driven model for the error term ϵ in (3). III. R ELATED WORK 

Before studying residual dynamic models, obtaining an accurate rigid-body model is essential. As early as 1986, Atkeson [13] demonstrated that inertial parameters can be identified using linear regression and least-squares techniques, showing on the PUMA manipulator that parameters estimated from the regressor form w = A ϕ produce torque predictions that significantly outperform CAD-based models. Sousa [14] later introduced the notion of enforcing physical feasibility in inertial parameter identification, observing that the physically feasible inertial parameters form a convex set and that these constraints can be expressed as Linear Matrix Inequalities (LMIs). This allows the identification problem to be solved via semidefinite programming while guaranteeing that ϕ corre-sponds to a reliable mass distribution. Their experiments on the WAM manipulator demonstrated a framework for constrained and physically feasible parameter identification. The optimiza-tion problem required to identify these dynamic parameters is often ill-conditioned; consequently, a complementary line of work has focused on generating optimal excitation trajecto-ries [15, 16]. While parameter identification for robotic manipulators is now relatively mature, the same process becomes substantially more challenging for legged robots because the common assumption of zero external torques does not hold. Khor-shidi [17] addresses this difficulty by projecting the robot dynamics into the null space of contact forces with the null-space projector P (q) = Im − Jc(q)†Jc(q), yielding torque equations that are independent of ground reaction forces. They then identify the inertial parameters by solving the constrained optimization problem ˆϕ = arg min ϕ ∥Y ϕ − τm∥2 s.t. ϕ ∈Pphys , where Pphys encodes mass positivity, center-of-mass bounds, and inertia LMIs. The resulting physically consistent model achieves more reliable joints torque predictions than neural-network-based estimators, maintaining low prediction error across a wide range of tasks. A widely adopted strategy for improving robot dynamics models is to learn the residual mismatch between the true dynamics and the nominal model, because this residual is usually much smoother than the full dynamics and can thus be captured effectively with Gaussian Processes (GPs). Car-ron [18] implement this idea by training a GP prior to ap-proximate the model error d(x, u ) in the discrete-time system dynamics xk+1 = Ax k + Bu k + Bd

 d(xk, u k) + ¯d, where the constant offset noise ¯d is estimated online via an extended Kalman Filter (EFK). In their formulation, the GP mean 

μd(x, u ) provides a feedforward compensation to the nominal inverse-dynamics model, while the GP covariance Σd(x, u )

contributes to the predicted state covariance Σx, which is used in chance constraints within the Model Predictive Control (MPC) horizon. Experiments on a compliant 6-DoF robotic arm show substantial tracking improvements compared to both PID and nominal MPC baselines. However, given its intrinsic properties, GP-based residual modeling is fundamentally lim-ited. Because the GP is trained offline and must be sparsified to remain computationally feasible at 1 kHz control rates, this residual model cannot adapt to changing dynamics. Moreover, GP models cannot automatically capture complex physical effects without appropriately designed kernels, which leads to a strong sensitivity to hyperparameter tuning. Its accuracy is constrained by the risk of overfitting, which ultimately highlights the strong dependence of GP-based residual models on the distribution of the training task data. Recently, Scholl used symbolic regression to learn an inter-pretable friction model for the KUKA LWR robot arm [19] where ϵ(j) = τ (j) 

> f

( ˙ q(j), τ (j) 

> g

) and j indicates the model is per joint. By operating the robot at low, near-constant velocities, i.e. ˙q, ¨q, C (q, ˙q) ˙ q ≈ 0n, and assuming zero external torques τext = 0 n they were able to simplify the dynamic model τm = τg (q) + ϵ and thus uncover a model for ϵ

using symbolic regression. Whilst the work by Scholl et al provides early validation that symbolic regression is useful for modeling dynamic effects, the work is limited. Their approach identifies a quasi-static, per-joint friction law only in a narrow operating regime: near-zero accelerations, low velocities, and single-joint motions. As a result, the learned model does not account for dynamic friction phenomena (e.g. hysteresis, pre-sliding) or joint–joint coupling effects, and it strongly relies on an accurate rigid-body model to remove inertial and Coriolis contributions. Moreover, the symbolic expressions are unconstrained outside the identification domain, so there is no guarantee that the inferred friction model extrapolates sensibly to higher velocities, larger accelerations, or more aggressive, multi-joint trajectories. IV. M ETHODS FOR INTERPRETABLE DYNAMICS 

Two common approaches are considered in this work for inferring interpretable dynamics models from motion data. 

A. Symbolic regression 

Symbolic regression is a supervised learning approach in which the goal is to discover an explicit analytic expression that relates input variables to a target quantity, rather than fitting the parameters of a fixed model class. Given a dataset 

D = {(xi, y i)}Ni=1 , symbolic regression searches over aspace F of candidate expressions built from basic operators (e.g. +, −, ×, /, exp , log , sin , . . . ), input variables, and free numerical constants to find a closed-form function f : Rd → R

that explains the data [20]. Formally, one can view symbolic regression as the problem 

bf = arg min  

> f∈F

L f ; D, (4) where L is a loss function that evaluates candidate expressions 

f on the data. In contrast to typical regression techniques, where the functional form is fixed in advance (e.g. linear models, polynomials of fixed degree, neural networks) and only the parameters are optimized, symbolic regression treats both the structure of the expression and its numerical constants as unknowns. Thus, the loss L is typically designed to reflect a trade-off between predictive accuracy and some notion of simplicity, so that the selected expressions are not only accurate on the observed data but also compact and inter-pretable. This is beneficial compared to models such as neural networks, which are often treated as black boxes, since the resulting expressions can be directly inspected and analyzed. Modern tools such as PySR [20] implement this paradigm using heuristic search procedures to explore the large, discrete space F while maintaining this balance between fit quality and model simplicity, and have been used to derive physics models in various applications (e.g. particle physics [21]). 

B. Sparse identification of Nonlinear Dynamics 

Sparse identification of nonlinear dynamics (SINDy) is a data-driven framework for discovering governing equations of dynamical systems from time-series data [22]. Given mea-surements of the system state {xi}Ni=1 and corresponding (estimated) time derivatives { ˙xi}Ni=1 , SINDy assumes that the dynamics can be expressed as a sparse linear combination of candidate nonlinear functions. To this end, one constructs a feature library Θ( X) ∈ RN ×p, whose columns contain nonlinear transformations of the states (and possibly inputs), such as polynomials, trigonometric functions, or other user-specified basis functions. The dynamical model is then written as ˙X ≈ Θ( X) Ξ where ˙X ∈ RN ×d stacks the time derivatives and Ξ ∈ Rp×d is a matrix of coefficients. Identifying the dynamics reduces to solving a regression problem for Ξ with a sparsity-promoting procedure (e.g. sequentially thresholded least squares), so that only a small subset of candidate terms remains active in each column. The resulting models are com-pact and interpretable, since each retained term corresponds to a specific mechanism in the dynamics. In this work, we do not use SINDy to identify an explicit evolution law ˙x = f (x), but instead adapt the same sparse-regression machinery to learn a general input–output mapping 

y = g(x). Conceptually, this amounts to replacing the time-derivative matrix ˙X in the standard SINDy formulation by a target matrix Y collecting the quantities of interest, while keeping the construction of the nonlinear feature library and the sparsity-enforcing regression procedure unchanged. The identification step then becomes 

Y ≈ Θ( X) Ξ , (5) Fig. 1: Visualization of the simulated 7-DoF Franka Robot arm. Note, in our experiments we only use PyBullet to visualize the robot, we use our own simulator definition. so that one recovers a sparse combination of basis functions that best predicts the target signal from the input features. This viewpoint casts our approach as a SINDy-inspired sparse polynomial regression, reusing the standard SINDy compo-nents (feature libraries, sparsity-promoting solvers, and their implementation in PySINDy [23]) in a supervised regression setting rather than for explicit time-derivative modeling. V. E XPERIMENTS 

In this section, we report our experimental findings. 

A. Validation of pipeline in simulation 

A simulator was developed using a fixed-base 7-DoF Franka Emika Panda robot arm, shown in Fig. 1. In these initial exper-iments, our primary objective is to validate the overall pipeline and evaluation infrastructure for the approaches considered in this study. It is important to note that PyBullet [24] is used only as a visualizer in this initial experimental setup; all robot dynamics are simulated using a known analytical rigid-body model of the Franka arm implemented in Pinocchio [25]. This design choice provides a clean benchmark in which modeling errors and low-level simulation artifacts are removed, allowing us to attribute the observed behavior directly to the algorithms under study. Note, the simulator developed in this section assumes no external torques, i.e, τext = 0 7. This means by (1), that predicting τdyn is equivalent to predicting τm.

1) Simulator: The robot is modeled as a 7-DoF manipulator with joint configuration q ∈ R7 and joint velocities ˙q ∈ R7.The joint-space dynamics are written as 

M (q)¨ q + C(q, ˙q) ˙ q + τg (q) = τef f , (6) where M (q) ∈ R7×7 is the joint-space inertia matrix, C(q, ˙q) ˙ q

collects Coriolis and centrifugal terms, and τg (q) is the gen-eralized gravity torque. A diagonal viscous damping model is used, 

τd( ˙ q) = D ˙q, (7) where D = diag (d1, . . . , d 7) ∈ R7×7 is a diagonal matrix with positive diagonal elements, i.e., di > 0 for all i = 1:7 . Given commanded motor torques τm ∈ R7, the effective torques (6) are found by the residual 

τef f = τm − τd. (8) where τd = τd( ˙ q). By including the damping model (7) in (8), we introduce a known, structured contribution to the effective torque that can serve as additional signal for the considered methods to identify and recover from data. The joint accelerations are computed by solving (6) for ¨q,forming the forward dynamics, given by 

¨q = M (q)−1 τef f − C(q, ˙q) ˙ q − τg (q). (9) We use Pinocchio’s efficient implementation of the articulated-body algorithm to compute (9). Given the state (qs, ˙qs) and the effective torque τef f,s at time step s, we first compute ¨qs using (9) and then the state is integrated using a semi-implicit (symplectic) Euler scheme with simulation time step ∆tsim . Thus, the next state is computed using 

˙qs+1 = ˙ qs + ∆ tsim ¨qs,qs+1 = qs + ∆ tsim ˙qs+1 . (10) This yields a deterministic simulator with known dynamics and full access to the true joint state. By substituting (8) into (6) and rearranging for τm, provides us the model that we hope to uncover from motion data. The model we hope to discover is thus given by 

τm = τi + τc + τg + τd (11) where τi = M (q)¨ q, τc = C(q, ˙q) ˙ q, and τg = τg (q).

2) Controller: The low-level controller, used to compute the motor torques τm is a joint-space PID law with gravity compensation. Given the desired joint positions q⋆ ∈ R7 and velocities ˙q⋆ ∈ R7, the motor torques are given by 

τm = τg (q) + τpid ,τpid = Kp

 q⋆ − q + Ki

Z

(q⋆ − q) dt + Kd

  ˙q⋆ − ˙q (12) where τg (q) is the generalized gravity torque computed by Pinocchio, and Kp, K i, K d ∈ R7 are diagonal matrices representing the PID gains. The integral term is updated at the controller (environment) sampling period ∆tenv and is clipped component-wise to prevent windup. This setup provides a clean testbed in which both the plant and the controller share the same perfect model of the robot, isolating the behavior of the proposed methods from modeling errors and sensor noise. 

3) Data generation: In order to test our proposed data-driven approach, we require data collected from the simulator. This section describes the data generation and simulation steps. a) Trajectory generation: To excite the dynamics of all joints, we generate joint-space reference trajectories as sums of sinusoidal signals. For each rollout, we first sample a random initial configuration q0 ∈ R7 inside the joint limits of the Franka arm, with a fixed margin subtracted from each bound to avoid saturating the joints. Formally, if qmin , q max ∈ R7 denote the lower and upper joint limits respectively and 0 < ρ ∈ R

is a fixed margin, we sample 

q0 ∼ U  qmin + ρe, q max − ρe , (13) where U(a, b ) represents a uniform distribution with lower and upper bounds a, b ∈ R7 such that aj < b j for all j = 1:7 , and 

e = [1 , . . . , 1] T ∈ R7 is the vector of ones. We then construct a time grid {tk}Nk=0 over a horizon T with sampling period 

∆tenv . For each joint j = 1:7 , we form a multi-sine signal 

qraw,j (t) = 

> nm

X

> ℓ=1

ajℓ sin  2πf jℓ t + ϕjℓ 

, (14) where nm is the number of sinusoidal modes per joint and the amplitudes ajℓ , frequencies fjℓ , and phases ϕjℓ are drawn in-dependently from uniform distributions over prescribed ranges. The corresponding velocity signal is obtained analytically as 

˙qraw,j (t) = 

> nm

X

> ℓ=1

ajℓ (2 πf jℓ ) cos  2πf jℓ t + ϕjℓ 

. (15) Each pair  qraw,j , ˙qraw,j 

 is then scaled by a single factor such that the resulting joint positions and velocities remain within the (shrunken) position limits and the nominal velocity limits of joint j. Formally, for each joint j we define the available position margin around the initial configuration q0,j 

as 

rj = min  qmax ,j − q0,j , q 0,j − qmin ,j 

 − ρ, (16) where qmin ,j and qmax ,j denote the jth components of qmin 

and qmax , respectively. Let ˙qmax ∈ R7 denote the joint-velocity limits and define 

αpos,j = rj

max k qraw,j (tk) + ε , (17) 

αvel,j = ˙qmax ,j 

max k ˙qraw,j (tk) + ε , (18) with a small 0 < ε ∈ R such that |ε| ≪ 1 to avoid division by zero. The final scaling factor is then 

sj = min  αpos,j , α vel,j 

, (19) and the discrete desired trajectories are obtained as 

q⋆j (tk) = q0,j + sj qraw,j (tk), (20) 

˙q⋆j (tk) = sj ˙qraw,j (tk). (21) This yields discrete desired trajectories q⋆(tk) and ˙q⋆(tk).Using the above procedure and 

b) Simulation: Given desired trajectories q⋆(tk), ˙q⋆(tk)

for k = 0: N , we simulate the closed-loop system consisting of the Franka simulator and the joint-space PID controller described in the previous sections. The initial state is set to 

q0 = q⋆(t0), ˙q0 = ˙ q⋆(t0). (22) At each environment step k = 0: N − 1, the controller computes a motor torque command τm using (12), which is held constant while the plant is integrated for a fixed number of smaller integration sub-steps using the semi-implicit Euler scheme (10). The resulting joint states qk+1 and ˙qk+1 are recorded at the environment rate ∆tenv .For each rollout we thus obtain sequences 

{qk}Nk=0 , { ˙qk}Nk=0 , {τm,k }N −1 

> k=0

, representing the joint positions, joint velocities, and motor torque commands over the horizon, respectively, together with the reference signals q⋆(tk), ˙q⋆(tk) and the time stamps {tk}Nk=0 . Each rollout is stored on disk as a separate trajectory, and adataset is obtained by repeating this procedure for multiple independently sampled reference trajectories. We used the above procedure to generate 10 training tra-jectories and 10 test trajectories. An example of a trajectory generated by the above procedure and resulting rollout in the simulator are shown in Figure 2. 

4) Training and evaluation: The trajectories collected are split into disjoint training and test sets. Each candidate data-driven dynamics approach takes as input the features 

q, ˙q, ¨q, ... 

q , τ i, τ c, τ g (i.e, the joint position, velocity, acceler-ation, jerk, inertia torques, Coriolis/centrifugal torques, and gravity torques respectively), and learns a mapping, such that 

f : R49 → R7, which estimates the motor torques τm,

bτm = f (q, ˙q, ¨q, ... 

q , τ i, τ c, τ g ). (23) Note, the joint accelerations ¨q and jerk ... 

q are estimated using finite-differencing method. The models f are fit on the training set only, and after training, we evaluate each model on both the training and test sets. For each set, we reconstruct the feature matrices and com-pute the corresponding torque predictions bτm. Performance is quantified by a relative root-mean-square error (RMSE) per joint across each dataset. 

5) Methods compared: As mentioned above, in the simu-lated setting, we have τext = 0 in (1), so that τm = τdyn .Using the decomposition in (3), each method either approxi-mates τdyn directly from data, or explicitly uses the rigid-body model (2) and learns an approximation of the error term ϵ.All models take as input the same feature vector 

x = q⊤ ˙q⊤ ¨q⊤ ... 

q ⊤ τ ⊤ 

> i

τ ⊤ 

> c

τ ⊤

> g

⊤ ∈ R49 , (24) constructed from the joint signals and the analytically com-puted inertia, Coriolis/centrifugal, and gravity torques. For the 7-DoF Franka arm used in the experiments, this corresponds to 7 variables of each type. Given x, each model returns an estimate bτm(x). Symbolic-regression and SINDy-based mod-els provide explicit closed-form expressions, whereas neural-network models are treated as black-box baselines. 2.8                

> 2.6
> 2.4
> Joint 1
> Joint Positions (rad)
> 0
> 1
> Joint Velocities (rad/s)
> 20
> 0
> 20
> 40
> Motor Torques (Nm)
> 0.00
> 0.25
> 0.50
> 0.75
> Joint 2
> 2
> 0
> 2
> 40
> 20
> 0
> 0.5
> 1.0
> Joint 3
> 2
> 0
> 2
> 20
> 0
> 20
> 40
> 3.0
> 2.5
> Joint 4
> 1
> 0
> 1
> 0
> 10
> 20
> 0.5
> 1.0
> 1.5
> Joint 5
> 2
> 0
> 2
> 20
> 0
> 2.0
> 2.5
> 3.0
> Joint 6
> 2
> 0
> 2
> 10
> 0
> 10
> 012345
> Time (s)
> 2.0
> 1.5
> Joint 7
> 012345
> Time (s)
> 2
> 0
> 2
> 012345
> Time (s)
> 20
> 0

Fig. 2: Example of a trajectory collected from our idealized simulator based on the 7-DoF Franka robot arm used in our validation experiments. Motor torques were computed using a controller implementing PID with gravity compensation. 

a) Symbolic Regression (SR): The SR baseline directly learns a mapping τsr : R49 → R7 to approximate τdyn (and hence τm) in (3), without explicitly using (2). We stack all training samples into matrices X ∈ RN ×49 and Y ∈ RN ×7,where each row of X is a feature vector x and each row of Y

is the corresponding torque measurement τm. For each joint 

j = 1:7 we fit an independent scalar model 

τsr,j (x) ≈ τm,j , (25) using PySR [20], with a search space of algebraic expres-sions built from the input variables and the binary operators 

{+, −, ∗} (division is excluded). PySR is run in batching mode with batch size 10 4, and the internal selection criterion balances mean-squared error on Y and expression complexity. The result is a set of joint-wise, closed-form expressions τsr,j 

that approximate τdyn,j .

b) Sparse Identification of Nonlinear Dynamics (SINDy): 

The SINDy baseline, based on the work of Brunton et al [22], also approximates τdyn directly, but constrains the model to be a sparse multivariate polynomial in the features. Starting from X and Y as above, we construct a polynomial feature library Θ( X) ∈ RN ×p using a basis of total degree up to 2, including all pairwise interactions and a bias term. We then learn a linear model 

τsindy (x) = W Θ( x) + b, (26) with W ∈ R7×p and b ∈ R7 fitted jointly for all seven outputs using sequentially thresholded least-squares (STLSQ) with a sparsity threshold of 0.01 , regularization parameter α = 10 −4,and at most 100 iterations. The resulting τsindy is an explicit sparse polynomial approximation of τdyn in (3). To implement this approach we utilized the PySINDy library [23]. 

c) Hybrid RBD and SR (r-SR): The r-SR model uses the decomposition in (3) explicitly. For each training sample, we compute τrbd (q, ˙q, ¨q) from the known Franka model and form the residual target 

y = τm − τrbd . (27) We then train seven independent PySR models on (x, y j ),exactly as in the SR baseline, obtaining a residual estimate 

bϵ(x) = bϵ1(x) . . . bϵ7(x)⊤ ∈ R7. (28) At test time, the prediction is 

bτ r-SR  

> m

(x) = τrbd (q, ˙q, ¨q) + bϵ(x), (29) so that r-SR can be interpreted as a symbolic model of the error term ϵ in (3). 

d) Hybrid RBD and SINDy (r-SINDy): The r-SINDy model follows the same residual-learning strategy, but rep-resents ϵ via a SINDy model. After computing τrbd , we form residual targets 

Yres = τm − τrbd (30) and reuse the same polynomial library Θ( X) as in the SINDy baseline. We then fit 

bϵ(x) = Wres Θ( x) + bres , (31) using STLSQ with identical hyperparameters. The hybrid prediction is 

bτ r-SINDy  

> m

(x) = τrbd (q, ˙q, ¨q) + bϵ(x), (32) yielding an interpretable sparse polynomial approximation of the error term in (3). 

e) Hybrid RBD, SINDy, and SR (r-SINDy-SR): The r-SINDy-SR model refines the r-SINDy approximation of ϵ

by adding a second, symbolic residual layer. First, a hybrid RBD–SINDy model 

bτ r-SINDy  

> m

(x) (33) is trained as above and kept fixed. We then define a second residual 

y(2) = τm − bτ r-SINDy  

> m

(x) (34) and train joint-wise PySR models on (x, y (2)  

> j

), obtaining a symbolic correction bϵ(2) (x). The final prediction is 

bτ r-SINDy-SR  

> m

(x) = bτ r-SINDy  

> m

(x) + bϵ(2) (x), (35) which can be viewed as a two-layer approximation of the error term ϵ in (3). 

f) Neural Network (NN): The NN baseline directly parametrizes τdyn by a fully connected neural network. We use a multilayer perceptron 

τnn (x) = fnn (x), (36) with two hidden layers of width 128 and rectified-linear (ReLU) activations. The network is trained on the concatenated dataset using mean-squared error loss between τnn (x) and 

τm, the Adam optimizer with learning rate 10 −4, batch size 

1024 , and 100 training epochs. In the notation of (3), this model implicitly learns τdyn (and hence ϵ) without imposing an explicit structure. 

g) Hybrid RBD and NN (r-NN): Finally, the r-NN model combines the rigid-body model with a neural approximation of the error term ϵ in (3). As in the other hybrid methods, we first compute τrbd and form residual targets 

y = τm − τrbd . (37) We then train an “error network” ferr , with the same archi-tecture as the NN baseline, to map x to an estimate 

bϵ(x) = ferr (x). (38) During training, the loss is evaluated on 

τrbd (q, ˙q, ¨q) + bϵ(x) (39) against the measured τm, so that the network explicitly learns to correct the analytical model. At test time the hybrid predic-tion is 

bτ r-NN  

> m

(x) = τrbd (q, ˙q, ¨q) + bϵ(x), (40) which matches the decomposition in (3) with bϵ represented by a neural network. TABLE I: Training set RMSE per joint for the Franka simu-lation.                                                         

> Joint SR SINDy r-SR r-SINDy r-SINDy-SR NN r-NN 10.002 0.002 0.002 0.002 0.002 0.072 0.041 20.001 0.001 0.001 0.001 0.001 0.092 0.025 30.002 0.002 0.002 0.002 0.002 0.084 0.062 40.002 0.002 0.002 0.002 0.002 0.065 0.041 50.002 0.003 0.003 0.003 0.003 0.138 0.071 60.002 0.003 0.003 0.003 0.003 0.163 0.087 70.003 0.003 0.003 0.003 0.003 0.168 0.111

TABLE II: Test set RMSE per joint for the Franka simulation.                                                         

> Joint SR SINDy r-SR r-SINDy r-SINDy-SR NN r-NN 10.002 0.002 0.002 0.002 0.002 0.687 0.994 20.001 0.001 0.001 0.001 0.001 0.267 0.244 30.002 0.002 0.002 0.002 0.002 0.388 0.628 40.002 0.002 0.002 0.002 0.002 0.300 0.251 50.003 0.003 0.003 0.003 0.003 0.679 0.624 60.003 0.003 0.003 0.003 0.003 1.111 0.424 70.004 0.004 0.004 0.004 0.004 1.356 0.986

6) Results: Table I reports the per-joint relative RMSE on the training set, while Table II shows the corresponding errors on the test set. Across both sets, the SR, SINDy, r-SR, r-SINDy, and r-SINDy-SR models achieve virtually identical performance, with relative RMSEs on the order of 10 −3–10 −2

for all seven joints. In particular, the best-performing method for each joint (shown in bold) is always one of these inter-pretable models, and the numerical differences between them are negligible at the scale of the metric. This indicates that, in the simulated setting where τrbd is computed from the same analytical model used to generate the data, both the purely data-driven (SR, SINDy) and hybrid variants are able to recover the internal dynamics τdyn essentially perfectly. Comparing training and test performance, we observe that the SR, SINDy and all hybrid models generalize very well: their test RMSEs closely match the training RMSEs for every joint. This is consistent with the fact that the simulator defines a smooth, low-noise mapping from the feature vector x to 

τm, which can be represented accurately either as a sparse polynomial or as a compact symbolic expression. The hybrid models do not exhibit a clear systematic advantage over their non-hybrid counterparts in this setting, which is expected since 

τrbd already matches the true rigid-body component of the dynamics and the remaining error term ϵ is relatively small and structured. In contrast, the neural-network baselines perform signifi-cantly worse. On the training set, the NN and r-NN models exhibit relative RMSEs between roughly 0.02 and 0.17 de-pending on the joint, already one to two orders of magnitude larger than those of the interpretable models. On the test set, the degradation is much more pronounced: test RMSEs for the NN and r-NN models reach values between 0.24 and 1.36 

across the seven joints, indicating poor generalization despite having access to the same feature vector x. The hybrid r-NN model slightly improves training errors over the pure NN but does not consistently reduce test errors, suggesting that, in this idealized setting, simply adding a neural residual on top of τrbd does not overcome the optimization and generalization challenges of the black-box model. Table III reports the closed-form expressions identified by the interpretable methods and enables a direct comparison with the target structure in (11) and the error decomposition in (3). For the purely data-driven SR and SINDy models, the learned expressions closely match the desired superposition of inertial, Coriolis/centrifugal, gravity, and viscous contributions. Across all joints, the coefficients multiplying τi,j , τc,j , and τg,j are essentially equal to one (up to small O(10 −3) deviations), indicating that both methods correctly recover the known rigid-body components from data. Moreover, the estimated viscous coefficients ξ(j) on ˙qj agree very well with the nominal damping coefficients dj , with relative discrepancies below 1% 

for all joints. For joints 5 and 6, SR additionally introduces jerk terms ... 

q j with very small magnitude (on the order of 10 −5–10 −4), consistent with the negligible higher-order corrections suggested by the low RMSE values in Tables I and II. The hybrid models further confirm that the dominant un-modeled dynamics in this setup are well captured by a simple viscous term. Both r-SR and r-SINDy recover ϵj as a scalar multiple of ˙qj that is numerically indistinguishable from dj ,with only very small jerk corrections for joints 5 and 6 in the r-SR case. In contrast, the r-SINDy-SR model, which refines the hybrid RBD–SINDy estimate, learns only very small residual structures in terms of accelerations ¨qj (and a single tiny cross term involving ˙q1 for joint 3), with coefficients on the order of 

10 −3 or smaller. Taken together, these results show that once the rigid-body model and a joint-wise viscous damping term are accounted for, there is little systematic structure remaining in the residual, and the interpretable methods converge to compact, physically meaningful expressions that are consistent with the known simulated dynamics. 

7) Discussion: Overall, these results show that in a noise-free environment with a known rigid-body model, interpretable sparsity-promoting methods (SR, SINDy and their hybrid variants) can recover a highly accurate model of τdyn and the error term ϵ in (3), whereas standard neural-network baselines struggle to match their accuracy and generalization. The symbolic models not only achieve low training and test RMSE across all joints, but also recover coefficients that are numerically consistent with the known viscous damping and rigid-body terms, and expose any remaining structure in the residual as small, easily interpretable corrections (e.g., weak jerk or acceleration terms). By contrast, the pure NN and hybrid RBD–NN baselines exhibit clear signs of overfitting in this setting, attaining low training error but substantially worse test performance and offering no direct insight into the underlying physical structure of the dynamics. Taken together, the simulation study validates that, when a reasonable mechanistic model is available and the data are sufficiently informative, interpretable sparse regression can serve as a powerful tool to identify both τdyn and ϵ in (3), combining prediction accuracy with physically meaningful, human-readable models that can be inspected, validated, and potentially modified by a designer. 

B. Analysis on real world data from WAM robot arm 

The previous section provided evidence that interpretable methods can be used to discover dynamics models from data. Also, the experimental pipeline developed was validated. In this section, we wish to apply and analyze these interpretable methods on real world data collected from the 7-DoF WAM robot arm. 

1) Dataset: The dataset for the experiments in this section was sourced from the work of Sousa and Cortes˜ ao [14], available at github.com/cdsousa/wam7 dyn ident. Estimating time derivatives from noisy joint trajectories is delicate, since standard finite-difference schemes tend to amplify measurement noise and can severely degrade the quality of learned models. For the WAM data, we there-fore computed joint velocity, acceleration, and jerk using a smoothed finite-difference method based on the total-variation-regularized differentiation framework of Chartrand [26]. In this approach, the derivative signal is obtained as the minimizer of a small variational problem that balances a least-squares fit to the noisy measurements with a total-variation penalty on the derivative, effectively enforcing a piecewise-smooth derivative that suppresses high-frequency noise while still allowing sharp changes. We apply this procedure independently to each joint trajectory before constructing the feature matrices. The dataset provided by Sousa and Cortes˜ ao [14] contains four robot trajectories, all containing about one minute of robot motion collected at a sampling frequency of 1 kHz. We used three trajectories as part of the training dataset and one as test dataset. An example of the joint positions provided in the dataset, along with the estimated velocities, accelerations and jerk (estimated using the smoothed finite differencing method described above) is shown in Figure 3. 

2) Results: Table IV reports the per-joint relative RMSE on the WAM training set, and Table V shows the corresponding errors on the test set. On the training data, the SINDy-based models (SINDy, r-SINDy, r-SINDy-SR) achieve the lowest RMSE for every joint, with values between roughly 0.03 and 

0.18 , i.e., substantially smaller than those of SR and r-SR (about 0.14 –0.65 ) and somewhat better than the NN and r-NN baselines (about 0.03 –0.33 ). Thus, in terms of in-sample fit, the sparse-regression models remain very effective at capturing the observed joint torques on this system. The generalization behavior on the WAM test set, however, is dramatically different across model classes. The SINDy and SINDy-based hybrid models suffer from severe overfitting: their test RMSEs increase by two to three orders of magnitude compared to training, reaching values between approximately 

4 and 60 depending on the joint. In contrast, the SR and r-SR models exhibit much more stable behaviour, with test RMSEs in the range 0.18 –0.58 that remain close to their training errors. For most joints, the best-performing method on the WAM test set (highlighted in bold in Table V) is therefore SR or r-SR, indicating that the symbolic-regression models provide the most robust out-of-sample performance in this setting. TABLE III: Closed-form models identified by the interpretable methods on a known dynamics model. We used 

[6 .75 , 6.00 , 5.25 , 4.50 , 3.75 , 3.00 , 2.25] ⊤ for the nominal viscous damping coefficients.                                                                                                                                                                                                                                                                                                                           

> jExpected model Identified model Coefficients SR 1(11)
> ξ(1) 1τi, 1 +ξ(1) 2τc, 1 +ξ(1) 3˙q1ξ(1) 1= 1 , ξ (1) 2= 1 , ξ (1) 3= 6 .721
> 2ξ(2) 1τi, 2 +ξ(2) 2τc, 2 +ξ(2) 3τg, 2 +ξ(2) 4˙q2ξ(2) 1= 1 , ξ (2) 2= 1 , ξ (2) 3= 1 , ξ (2) 4= 5 .962
> 3ξ(3) 1τi, 3 +ξ(3) 2τc, 3 +ξ(3) 3τg, 3 +ξ(3) 4˙q3ξ(3) 1= 1 , ξ (3) 2= 1 , ξ (3) 3= 1 , ξ (3) 4= 5 .235
> 4ξ(4) 1τi, 4 +ξ(4) 2τc, 4 +ξ(4) 3τg, 4 +ξ(4) 4˙q4ξ(4) 1= 1 , ξ (4) 2= 1 , ξ (4) 3= 1 , ξ (4) 4= 4 .486
> 5ξ(5) 1τi, 5 +ξ(5) 2τc, 5 +ξ(5) 3τg, 5 +ξ(5) 4˙q5 +ξ(5) 5
> ...
> q5ξ(5) 1≈1.001 , ξ (5) 2= 1 , ξ (5) 3= 1 ,ξ(5) 4≈3.741 , ξ (5) 5≈5.2×10 −5
> 6ξ(6) 1τi, 6 +ξ(6) 2τc, 6 +ξ(6) 3τg, 6 +ξ(6) 4˙q6 +ξ(6) 5
> ...
> q6ξ(6) 1≈1.001 , ξ (6) 2= 1 , ξ (6) 3= 1 ,ξ(6) 4≈2.993 , ξ (6) 5≈1.0×10 −4
> 7ξ(7) 1τi, 7 +ξ(7) 2τc, 7 +ξ(7) 3˙q7ξ(7) 1= 1 , ξ (7) 2= 1 , ξ (7) 3= 2 .244
> SINDy
> 1(11)
> ξ(1) 1τi, 1 +ξ(1) 2τc, 1 +ξ(1) 3˙q1ξ(1) 1= 1 .001 , ξ (1) 2= 1 .001 , ξ (1) 3= 6 .721
> 2ξ(2) 1τi, 2 +ξ(2) 2τc, 2 +ξ(2) 3τg, 2 +ξ(2) 4˙q2ξ(2) 1= 1 , ξ (2) 2= 1 .001 , ξ (2) 3= 1 , ξ (2) 4= 5 .962
> 3ξ(3) 1τi, 3 +ξ(3) 2τc, 3 +ξ(3) 3τg, 3 +ξ(3) 4˙q3ξ(3) 1= 1 .001 , ξ (3) 2= 1 .002 , ξ (3) 3= 1 , ξ (3) 4= 5 .234
> 4ξ(4) 1τi, 4 +ξ(4) 2τc, 4 +ξ(4) 3τg, 4 +ξ(4) 4˙q4ξ(4) 1= 1 , ξ (4) 2= 1 .001 , ξ (4) 3= 1 , ξ (4) 4= 4 .486
> 5ξ(5) 1τi, 5 +ξ(5) 2τc, 5 +ξ(5) 3τg, 5 +ξ(5) 4˙q5ξ(5) 1= 1 .001 , ξ (5) 2= 1 , ξ (5) 3≈0.9997 , ξ (5) 4= 3 .741
> 6ξ(6) 1τi, 6 +ξ(6) 2τc, 6 +ξ(6) 3τg, 6 +ξ(6) 4˙q6ξ(6) 1= 1 .001 , ξ (6) 2= 1 .001 , ξ (6) 3= 1 , ξ (6) 4= 2 .991
> 7ξ(7) 1τi, 7 +ξ(7) 2τc, 7 +ξ(7) 3˙q7ξ(7) 1= 1 .001 , ξ (7) 2= 1 .001 , ξ (7) 3= 2 .244
> r-SR
> 1
> ϵj=dj˙qjξ(1) 1˙q1ξ(1) 1= 6 .721
> 2ξ(2) 1˙q2ξ(2) 1= 5 .962
> 3ξ(3) 1˙q3ξ(3) 1= 5 .235
> 4ξ(4) 1˙q4ξ(4) 1= 4 .486
> 5ξ(5) 1˙q5 +ξ(5) 2
> ...
> q5ξ(5) 1= 3 .742 , ξ (5) 2≈5.2×10 −5
> 6ξ(6) 1˙q6 +ξ(6) 2
> ...
> q6ξ(6) 1= 2 .994 , ξ (6) 2≈1.0×10 −4
> 7ξ(7) 1˙q7ξ(7) 1= 2 .244
> r-SINDy
> 1
> ϵj=dj˙qjξ(1) 1˙q1ξ(1) 1= 6 .721
> 2ξ(2) 1˙q2ξ(2) 1= 5 .962
> 3ξ(3) 1˙q3ξ(3) 1= 5 .235
> 4ξ(4) 1˙q4ξ(4) 1= 4 .486
> 5ξ(5) 1˙q5ξ(5) 1= 3 .741
> 6ξ(6) 1˙q6ξ(6) 1= 2 .991
> 7ξ(7) 1˙q7ξ(7) 1= 2 .244
> r-SINDy-SR  1
> ϵ(2)
> j= 0
> ξ(1) 1¨q1ξ(1) 1≈2.92 ×10 −3
> 2ξ(2) 1¨q2ξ(2) 1≈2.38 ×10 −3
> 3ξ(3) 1¨q3 +ξ(3) 2˙q1ξ(3) 1≈2.74 ×10 −3, ξ (3) 2≈ − 2.74 ×10 −3
> 4ξ(4) 1
> ...
> q4ξ(4) 1≈9.62 ×10 −5
> 5ξ(5) 1¨q5ξ(5) 1≈2.04 ×10 −3
> 6ξ(6) 1¨q6ξ(6) 1≈1.53 ×10 −3
> 7ξ(7) 1¨q7ξ(7) 1≈1.03 ×10 −3

TABLE IV: WAM training set RMSE per joint. 

Joint SR SINDy r-SR r-SINDy r-SINDy-SR NN r-NN 1 0.410 0.103 0.477 0.103 0.103 0.121 0.132 2 0.143 0.027 0.182 0.027 0.027 0.030 0.030 3 0.437 0.057 0.570 0.057 0.057 0.081 0.081 4 0.423 0.039 0.422 0.039 0.039 0.056 0.058 5 0.621 0.177 0.645 0.177 0.177 0.329 0.330 6 0.487 0.076 0.556 0.076 0.076 0.213 0.213 7 0.403 0.090 0.558 0.090 0.090 0.322 0.324 

TABLE V: WAM test set RMSE per joint. 

Joint SR SINDy r-SR r-SINDy r-SINDy-SR NN r-NN 1 0.410 18.566 0.441 18.566 18.566 0.733 0.681 2 0.183 4.106 0.230 4.106 4.106 0.470 0.472 3 0.483 7.782 0.540 7.463 7.463 0.876 0.773 4 0.453 9.423 0.458 9.423 9.423 0.805 0.854 5 0.476 60.189 0.475 59.389 59.388 0.942 1.243 6 0.548 13.280 0.547 13.160 13.160 0.822 0.967 7 0.437 27.754 0.580 27.905 27.905 1.246 1.282 

The neural-network baselines occupy an intermediate regime. On the training set, NN and r-NN attain errors comparable to or slightly larger than the SINDy family, but substantially lower than SR and r-SR. On the test set, their RMSEs increase to roughly 0.47 –1.28 , which is still an order of magnitude smaller than the SINDy-based models but consistently worse than SR and r-SR across all joints. The hybrid r-NN slightly improves over the pure NN for some joints (e.g., joint 1) but degrades performance on others, and does not close the gap to the best SR-based models. The SR approach produced the following equations. 

bτ SR m, 1 = ¨ q1 + ˙ q1(ξ(1) 1 − ˙q21 ) (41) 

bτ SR m, 2 = ¨ q2 − ¨q3 + ˙ q2( ˙ q5 + ξ(1) 2 ) − ˙q23 (42) 

− (( ξ(2) 2 − q4)q2ξ(3) 2 − q6 + τg, 3)

bτ SR m, 3 = ˙ q3 + (¨ q1 + q3q4(ξ(1) 3 ¨q1 + ξ(2) 3 ))( q2 + ξ(3) 3 ) (43) 

bτ SR m, 4 = ( q4 − ξ(1) 4 )ξ(2) 4 q4 + ˙ q4 + ξ(3) 4 (44) 

bτ SR m, 5 = ξ(1) 5 ˙q5 (45) 

bτ SR m, 6 = ( ξ(1) 6 − τg, 6)( ˙ q6 + (¨ q6 + ˙ q3)ξ(2) 6 ) (46) 

bτ SR m, 7 = ˙ q7( ˙ q27 (ξ(1) 7 ˙q27 − ξ(2) 7 ) + ξ(3) 7 ) (47) where ξ(1) 1 = 4.723089 , ξ2(1) = 2.9080215 , ξ(2) 2 =7.1627355 , ξ(3) 2 = 4.039454 , ξ(1) 3 = 0.47727528 , ξ(2) 3 =1.7755635 , ξ(3) 3 = 0 .11157574 , ξ(1) 4 = 3 .4464025 , ξ(2) 4 =1.9739965 , ξ(3) 4 = 1 .8875742 , ξ(1) 5 = 0 .14579542 , ξ(1) 6 =0.26862046 , ξ(2) 6 = −0.12590475 , ξ(1) 7 = 0 .05014042 , ξ(2) 7 =0.17362133 , ξ(3) 7 = 0 .19907075 .2                        

> 0
> 2
> Joint 1
> Joint Positions (rad)
> 1
> 0
> 1
> Joint Velocities (rad/s)
> 2
> 0
> 2
> Joint Accelerations (rad/s²)
> 0
> 10
> Joint Jerks (rad/s³)
> 1
> 0
> 1
> Joint 2
> 1
> 0
> 1
> 2
> 0
> 2
> 0
> 20
> 2
> 0
> 2
> Joint 3
> 1
> 0
> 1
> 5
> 0
> 0
> 20
> 40
> 60
> 0
> 1
> 2
> Joint 4
> 1
> 0
> 1
> 2.5
> 0.0
> 2.5
> 0
> 20
> 40
> 4
> 2
> 0
> Joint 5
> 1
> 0
> 4
> 2
> 0
> 2
> 0
> 20
> 1
> 0
> 1
> Joint 6
> 1
> 0
> 1
> 4
> 2
> 0
> 2
> 0
> 20
> 010 20 30 40 50 60
> Time (s)
> 1
> 0
> 1
> 2
> Joint 7
> 010 20 30 40 50 60
> Time (s)
> 1
> 0
> 1
> 010 20 30 40 50 60
> Time (s)
> 5.0
> 2.5
> 0.0
> 2.5
> 010 20 30 40 50 60
> Time (s)
> 0
> 20
> 40

Fig. 3: An example of one of the trajectories provided by Sousa and Cortes˜ ao [14], along with estimated velocities, accelerations, and jerk. Estimates of the various time derivatives were found using the method proposed by Chartrand [26]. The r-SR approach produced the following equations. 

bϵr-SR  

> m, 1

= −¨q1 − τg, 1 + ξ(1) 1 ˙q1 (48) 

bϵr-SR  

> m, 2

= ( ˙ q2 + ( ξ(1) 2 − q2)( ξ(2) 2 q4 − ξ(3) 2 ) − ξ(4) 2 )ξ(5) 2 (49) 

bϵr-SR  

> m, 3

= q2q3ξ(1) 3 − τg, 3 + ˙ q3 (50) 

bϵr-SR  

> m, 4

= ˙ q4 + ( q4 − ξ(1) 4 )( ξ(2) 4 q4 − ξ(3) 4 ) (51) 

bϵr-SR  

> m, 5

= −τg, 5 + ξ(1) 5 ˙q5 (52) 

bϵr-SR  

> m, 6

= ξ(1) 6 ˙q6 (53) 

bϵr-SR  

> m, 7

= ξ(1) 7 ˙q7 (54) where ξ(1) 1 = 3.2723246 , ξ(1) 2 = 0.016012268 , ξ(2) 2 =

−1.023108 , ξ(3) 2 = −8.626232 , ξ(4) 2 = 0 .50142884 , ξ(5) 2 =3.391348 , ξ(1) 3 = 2.58501 , ξ(1) 4 = 3.1465542 , ξ(2) 4 =1.9603553 , ξ(3) 4 = 0 .58640337 , ξ(1) 5 = 0 .14822637 , ξ(1) 6 =0.24277882 , ξ(1) 7 = 0 .07425557 .The methods using SINDy produced models that are too long to write in the page. Beyond the quantitative comparison in Tables IV and V, the closed-form SR and r-SR expressions for the WAM arm provide additional qualitative insight into the structure of the learned dynamics. For several joints, the models recover familiar physical motifs: joints 1, 5, 6, and 7 are dominated by velocity-dependent terms that resemble viscous or nonlinear friction laws (linear in ˙qj for joints 5–7 in the r-SR residuals, and cubic/quintic in ˙q1 and ˙q7 in the full SR model), while joint 4 features a clear spring-like dependence on q4 of the form (q4 − offset ) q4 combined with a viscous term in ˙q4 and a constant bias torque. These patterns are consistent with the intuition that the nominal rigid-body model accounts for the bulk of the inertial and gravitational effects, whereas the resid-uals capture joint-local friction, compliance, and offset torques that arise from the WAM’s cable transmissions and hardware-specific characteristics. The presence of explicit τg,j terms in several SR and r-SR equations—for instance, τg, 3 appearing in the models for joints 2 and 3, or τg, 5 and τg, 6 modulating the residuals of joints 5 and 6—further indicates that the symbolic learner makes systematic use of the available gravity regressor, effectively re-weighting or partially “undoing” it where the analytical model and the real robot diverge. At the same time, the WAM expressions also expose more complex structures that go beyond the canonical RBD-plus-viscous-friction picture and would be difficult to anticipate a priori, yet remain compact and interpretable. In particular, the SR model for joints 2 and 3 contains configuration- and coupling-dependent factors such as (ξ(2) 2 − q4)q2, and mixed dependencies on q4, q6, and τg, 3, while the r-SR residual for joint 2 exhibits a product ( ˙ q2 + ( ξ(1) 2 − q2)( ξ(2) 2 q4 −

ξ(3) 2 ) − ξ(4) 2 )ξ(5) 2 . These terms suggest nontrivial cross-joint couplings and configuration-dependent biases that are not explicitly encoded in the nominal RBD model, and may be absorbing effects of cable routing, structural compliance, or sensor/actuator nonlinearities. Similarly, the SR expression for joint 6 includes a gravity-modulated friction term (ξ(1) 6 −

τg, 6)( ˙ q6 + (¨ q6 + ˙ q3)ξ(2) 6 ), indicating that the effective damping on joint 6 depends on both posture (through τg, 6) and motion of another joint (via ˙q3), which goes beyond standard jointwise viscous friction models. While some of these couplings may reflect identification artifacts or dataset-specific correlations rather than entirely new physical effects, the fact that they arise as low-complexity terms and still generalize better than both SINDy and neural networks on held-out trajectories makes them valuable hypotheses for further mechanical analysis and model refinement. Finally, we note that all SR and r-SR models on the WAM dataset were learned with a deliberately simple PySR setup: the operator set was restricted to the binary operators 

{+, −, ×} , and all remaining options (evolutionary strategy, complexity–accuracy trade-off, population sizes, etc.) were left at their default values, with no task-specific hyperparameter tuning. In other words, the discovered spring-like terms, cross-joint couplings, and gravity-modulated friction laws emerge under a generic, off-the-shelf configuration. It is plausible that a more targeted search—for example, adjusting the complexity penalty, enriching the operator set with piecewise or non-smooth functions, or biasing the search toward cross-joint features—could either simplify some of the more intricate couplings found here or uncover additional, more physically structured components (such as clearer Coulomb-like friction terms or smoother approximations of stick–slip effects) while retaining or further improving generalization on held-out tra-jectories. 

3) Discussion: Overall, the WAM results highlight a clear gap between in-sample fit and out-of-sample reliability. SINDy and its hybrid variants achieve the lowest training errors but fail to generalize, with test RMSEs increasing by several orders of magnitude and yielding very long expressions that appear to track trajectory-specific artefacts rather than under-lying mechanics. In contrast, SR and r-SR trade some training accuracy for substantially better generalization, attaining the lowest test errors across most joints. As discussed above, the closed-form SR and r-SR mod-els remain compact and physically interpretable, capturing friction-like, spring-like, and gravity-modulated effects as well as nontrivial cross-joint couplings. Importantly, all of these SR and r-SR models were obtained with a simple, off-the-shelf PySR configuration restricted to the operators {+, −, ×} and default evolutionary settings, with no task-specific hyperpa-rameter tuning. This suggests that even a modest symbolic-regression setup can already uncover meaningful structure in realistic robot data. The limited benefit of hybridizing neural networks with τrbd and the poor robustness of the SINDy variants together underline that, in this more realistic setting where the analytical model and function libraries are only approximate, the choice of model class and regularization is at least as important as raw approximation power. VI. C ONCLUSION 

We proposed interpretable, data-driven residual models of robot dynamics that augment an analytical rigid-body dy-namics (RBD) model with a learned error term ϵ. Using symbolic regression (SR) and SINDy-style sparse regression on joint-space features, we obtained compact expressions that, in simulation on a Franka arm with known dynamics, recover inertial, Coriolis, gravity, and viscous terms with relative errors of order 10 −3. In this setting, SR/SINDy and their RBD hybrids match the true dynamics with strong train– test generalization and outperform neural-network baselines while remaining fully interpretable. On real data from a 7-DoF WAM arm, SR models generalize best and consistently outperform both SINDy and neural networks on held-out data, revealing physically meaningful structure such as viscous and nonlinear friction, spring-like terms, gravity reweightings, and low-dimensional cross-joint couplings. 

A. Limitations 

Our method assumes an accurate RBD model and suffi-ciently clean joint-level measurements. Models are trained offline on pre-collected data and evaluated via one-step torque prediction, so their closed-loop impact on performance and safety is unknown. We restrict the feature libraries and func-tion classes to hand-designed polynomials, a basic operator set {+, −, ×} , and per-joint models, which may miss richer friction effects and higher-order couplings. Derivative-based features (velocities, accelerations, jerk) rely on numerical differentiation; although we use smoothed, total-variation-regularized finite differences [26], derivative quality and some learned terms may still be affected by noise, sampling rates, and sensor biases. 

B. Future Work 

Future work will embed interpretable residual models into control and learning pipelines, e.g., as components in impedance or model-predictive controllers, as structured priors in model-based or hybrid reinforcement learning, and as virtual force/torque sensors for contact-rich tasks. We also aim to re-lax reliance on a fixed RBD model by jointly identifying rigid-body parameters and residuals, and by enriching SR/SINDy libraries with more physics-informed, possibly non-smooth operators while preserving sparsity. Finally, we plan to study transfer across robots and data regimes, leveraging shared interpretable structure to generalize dynamics models and support scalable, force-aware extensions of vision–language– action frameworks. REFERENCES 

[1] Maged Iskandar, Christian Ott, Alin Albu-Sch¨ affer, Bruno Siciliano, and Alexander Dietrich. Hybrid force-impedance control for fast end-effector motions. IEEE Robotics and Automation Letters , 8(7):3931–3938, 2023. doi: 10.1109/LRA.2023.3270036. [2] Sichao Liu, Lihui Wang, and Xi Vincent Wang. Sensor-less force estimation for industrial robots using distur-bance observer and neural learning of friction approxima-tion. Robotics and Computer-Integrated Manufacturing ,71:102168, 2021. [3] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Van-houcke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Gre-cia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka 5                                    

> 0
> 5
> 10
> Joint 1
> Motor Torques Comparison
> msr
> 0.0
> 2.5
> 5.0
> 7.5
> Absolute Torque Error
> |msr |
> 20
> 0
> 20
> Joint 2
> msr
> 0.0
> 2.5
> 5.0
> 7.5 |msr |
> 5
> 0
> 5
> Joint 3
> msr
> 0
> 2
> 4|msr |
> 5
> 0
> 5
> Joint 4
> msr
> 0
> 2
> 4
> |msr |
> 0.4
> 0.2
> 0.0
> 0.2
> Joint 5
> msr
> 0.0
> 0.1
> 0.2
> 0.3
> |msr |
> 0.25
> 0.00
> 0.25
> Joint 6
> msr
> 0.0
> 0.1
> 0.2
> 0.3
> |msr |
> 010 20 30 40 50 60
> Time (s)
> 0.1
> 0.0
> 0.1
> Joint 7
> msr
> 010 20 30 40 50 60
> Time (s)
> 0.000
> 0.025
> 0.050
> 0.075 |msr |
> Trajectory rbtlog_traj2

(a) Evaluation on training set. 5                                      

> 0
> 5
> Joint 1
> Motor Torques Comparison
> msr
> 0
> 2
> 4
> Absolute Torque Error
> |msr |
> 20
> 0
> 20
> Joint 2
> msr
> 0
> 5
> 10 |msr |
> 5
> 0
> 5
> Joint 3
> msr
> 0
> 2
> 4|msr |
> 5
> 0
> 5
> Joint 4
> msr
> 0
> 2
> 4
> 6|msr |
> 0.2
> 0.0
> 0.2
> Joint 5
> msr
> 0.0
> 0.1
> 0.2
> |msr |
> 0.25
> 0.00
> 0.25
> Joint 6
> msr
> 0.0
> 0.1
> 0.2
> |msr |
> 010 20 30 40 50 60
> Time (s)
> 0.1
> 0.0
> 0.1
> Joint 7
> msr
> 010 20 30 40 50 60
> Time (s)
> 0.00
> 0.05
> 0.10 |msr |
> Trajectory rbtlog_traj4

(b) Evaluation on test set. 

Fig. 4: Evaluations of our trained model using SR on the real world data collected from the 7-DoF WAM robot arm [14]. Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jas-mine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Cheb-otar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Proceedings of The 7th Con-ference on Robot Learning , volume 229 of Proceed-ings of Machine Learning Research , pages 2165–2183. PMLR, 2023. URL https://proceedings.mlr.press/v229/ zitkovich23a.html. [4] Open X.-Embodiment Collaboration, Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alexander Herzog, Alex Irpan, Alexander Khazatsky, Anant Raj, Anikait Singh, Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bern-hard Sch¨ olkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Di-eter B¨ uchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Giulio Schiavi, Gregory Kahn, Hao Su, Haoshu Fang, Haochen Shi, Heni Ben Amor, Henrik I. Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mor-datch, Ilija Radosavovic, et al. Open x-embodiment: Robotic learning datasets and RT-X models. CoRR ,abs/2310.08864, 2023. doi: 10.48550/arXiv.2310.08864. URL https://arxiv.org/abs/2310.08864. [5] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan P. Foster, Grace Lam, Pannag R. Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. CoRR , abs/2406.09246, 2024. doi: 10.48550/arXiv.2406.09246. URL https://arxiv.org/ abs/2406.09246. [6] William Xie and Nikolaus Correll. Towards forceful robotic foundation models: a literature survey. CoRR ,abs/2504.11827, 2025. doi: 10.48550/arXiv.2504.11827. URL https://arxiv.org/abs/2504.11827. [7] Jialei Huang, Shuo Wang, Fanqi Lin, Yihang Hu, Chuan Wen, and Yang Gao. Tactile-VLA: Unlocking vision-language-action model’s physical knowledge for tactile generalization. CoRR , abs/2507.09160, 2025. doi: 10.48550/arXiv.2507.09160. URL https://arxiv.org/abs/ 2507.09160. [8] Jiawen Yu, Hairuo Liu, Qiaojun Yu, Jieji Ren, Ce Hao, Haitong Ding, Guangyu Huang, Guofan Huang, Yan Song, Panpan Cai, Cewu Lu, and Wenqiang Zhang. Forcevla: Enhancing VLA models with a force-aware MoE for contact-rich manipulation. arXiv preprint arXiv:2505.22159 , 2025. doi: 10.48550/arXiv.2505. 22159. URL https://arxiv.org/abs/2505.22159. NeurIPS 2025. [9] P. Fankhauser, M. Hutter, C. Gehring, M. Bloesch, M. A. Hoepflinger, and R. Siegwart. Reinforcement learning of single legged locomotion. In 2013 IEEE/RSJ Inter-national Conference on Intelligent Robots and Systems (IROS) , pages 188–193. IEEE, 2013. doi: 10.1109/IROS. 2013.6696363. [10] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. Science Robotics , 5 (47):eabc5986, 2020. doi: 10.1126/scirobotics.abc5986. [11] Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes using massively parallel deep reinforcement learning. In Proceedings of the 5th Conference on Robot Learning , volume 164 of 

Proceedings of Machine Learning Research , pages 91– 100. PMLR, 2022. [12] Sehoon Ha, Joonho Lee, Michiel van de Panne, Zhaom-ing Xie, Wenhao Yu, and Majid Khadiv. Learning-based legged locomotion: State of the art and future perspec-tives. International Journal of Robotics Research , 44(8): 1396–1427, 2025. doi: 10.1177/02783649241312698. [13] Christopher G Atkeson, Chae H An, and John M Holler-bach. Estimation of inertial parameters of manipulator loads and links. The International Journal of Robotics Research , 5(3):101–119, 1986. [14] Crist´ ovao D Sousa and Rui Cortesao. Physical feasibility of robot base inertial parameter identification: A linear matrix inequality approach. The International Journal of Robotics Research , 33(6):931–944, 2014. [15] J. Swevers, C. Ganseman, D.B. Tukel, J. de Schutter, and H. Van Brussel. Optimal robot excitation and identifica-tion. IEEE Transactions on Robotics and Automation , 13 (5):730–740, 1997. doi: 10.1109/70.631234. [16] Huanyu Tian, Martin Huber, Christopher E Mower, Zhe Han, Changsheng Li, Xingguang Duan, and Christos Bergeles. Excitation trajectory optimization for dy-namic parameter identification using virtual constraints in hands-on robotic system. In 2024 IEEE International Conference on Robotics and Automation (ICRA) , pages 11605–11611. IEEE, 2024. [17] Shahram Khorshidi, Murad Dawood, Benno Nederkorn, Maren Bennewitz, and Majid Khadiv. Physically-consistent parameter identification of robots in contact. In 2025 IEEE International Conference on Robotics and Automation (ICRA) , pages 677–683, 2025. doi: 10.1109/ICRA55743.2025.11128710. [18] Andrea Carron, Elena Arcari, Martin Wermelinger, Lukas Hewing, Marco Hutter, and Melanie N. Zeilinger. Data-driven model predictive control for trajectory tracking with a robotic arm. IEEE Robotics and Automation Letters , 4(4):3758–3765, 2019. doi: 10.1109/LRA.2019. 2929987. [19] Philipp Scholl, Alexander Dietrich, Sebastian Wolf, Jinoh Lee, Alin-Albu Sch¨ affer, Gitta Kutyniok, and Maged Iskandar. Interpretable robotic friction learning via sym-bolic regression, 2025. URL https://arxiv.org/abs/2505. 13186. [20] Miles Cranmer. Interpretable machine learning for sci-ence with pysr and symbolicregression. jl. arXiv preprint arXiv:2305.01582 , 2023. [21] Manuel Morales-Alvarado, Daniel Conde, Josh Ben-david, Veronica Sanz, and Maria Ubiali. Symbolic regression for precision lhc physics, 2024. URL https: //arxiv.org/abs/2412.07839. [22] Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceed-ings of the National Academy of Sciences , 113(15): 3932–3937, 2016. doi: 10.1073/pnas.1517384113. URL https://www.pnas.org/doi/abs/10.1073/pnas.1517384113. [23] Alan A. Kaptanoglu, Brian M. de Silva, Urban Fasel, Kadierdan Kaheman, Andy J. Goldschmidt, Jared Calla-ham, Charles B. Delahunt, Zachary G. Nicolaou, Kath-leen Champion, Jean-Christophe Loiseau, J. Nathan Kutz, and Steven L. Brunton. Pysindy: A comprehensive python package for robust sparse system identification. 

Journal of Open Source Software , 7(69):3994, 2022. doi: 10.21105/joss.03994. URL https://doi.org/10.21105/joss. 03994. [24] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning, 2016. [25] Justin Carpentier, Guilhem Saurel, Gabriele Buondonno, Joseph Mirabel, Florent Lamiraux, Olivier Stasse, and Nicolas Mansard. The pinocchio c++ library : A fast and flexible implementation of rigid body dynamics algorithms and their analytical derivatives. In 2019 IEEE/SICE International Symposium on System Integra-tion (SII) , pages 614–619, 2019. doi: 10.1109/SII.2019. 8700380. [26] Rick Chartrand. Numerical differentiation of noisy, non-smooth data. International Scholarly Research Notices ,2011(1):164564, 2011.