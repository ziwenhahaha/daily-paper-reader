Title: Towards symbolic regression for interpretable clinical decision scores

URL Source: https://arxiv.org/pdf/2512.07961v1

Published Time: Wed, 10 Dec 2025 01:04:54 GMT

Number of Pages: 15

Markdown Content:
royalsocietypublishing.org/journal/rsta 

# Research 

Preprint / Author Accepted Manuscript (AAM) . This version has been accepted for publication in Philosophical Transactions A . It has not undergone final copyediting, typesetting, or publisher formatting. The final version of record will be accessible from the Royal Society’s website once available. 

Subject Areas: 

xxxxx, xxxxx, xxxx 

Keywords: 

symbolic regression, genetic programming, srbench, clinical decision support 

Author for correspondence: 

William G. La Cava e-mail: William.LaCava@childrens.harvard.edu 

# Towards symbolic regression for interpretable clinical decision scores 

# Guilherme Seidyo Imai Aldeia 1, Joseph D. Romano 3, Fabricio Olivetti de Franca 1, Daniel S. Herman 2, William G. La Cava 4

1Federal University of ABC, SP, Brazil 

2Department of Pathology and Laboratory Medicine, University of Pennsylvania, PA, US 

3Institute for Biomedical Informatics, University of Pennsylvania, PA, US 

4Boston Children’s Hospital, Harvard Medical School, MA, US 

Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models. 

© The Authors. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/ by/4.0/, which permits unrestricted use, provided the original author and source are credited. 

> arXiv:2512.07961v1 [cs.LG] 8 Dec 2025

2 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

# 1. Introduction 

Symbolic Regression (SR) is a supervised regression machine learning method that aims to discover a mathematical expression and its parameters from data. Since SR was first proposed [1], it was applied in various fields, including physics [2 – 6], medical informatics [7 – 9], aerospace engineering [10,11], and material science [12]. The main appeal of SR is its potential to produce intrinsically interpretable solutions, often crucial for scientific discovery in basic sciences or when prediction models must justify their recommendations, e.g. , in health care contexts, c.f. the United States [13] guidelines for clinical decision support software. While promising for healthcare, purely mathematical expressions can be a drawback in practice due to the mathematical literacy they require, making them less attractive than decision trees [14], which are often preferred when interpretability is needed [15]. In the healthcare context, widely used triage scoring systems in the U.S., such as CART [16], are based on physiological parameters and can be promptly evaluated to predict urgent deterioration of patients [17]. Scoring systems require interpretability, consisting of simple decision rules, and have been associated with better hospital outcomes [18]. They are common clinical decision tools because they provide objective, quantifiable measures for decision-making and have historically been developed manually [19]. These systems could be explored with the aid of big data and artificial intelligence [20], especially through the usage of electronic health records (EHR). Contemporary SR algorithms that achieve good results in modeling regression equations for real-world and physics data [21] relies local parameter optimization [22], but a limitation is not being able to incorporate split-wise operations, due to the difficulty of optimizing such parameters with existing optimization methods. This task remains underexplored in SR. We propose a genetic programming symbolic regression algorithm named Brush , aiming to bridge the gap between symbolic prediction models and split-wise operations, along with non-linear parameter optimization. Brush addresses the multi-objective optimization problem of simultaneously maximizing performance and minimizing model complexity. It learns split operations at any point within the expression while maintaining compatibility with non-linear optimization, even in the presence of discontinuities. Figure 1 depicts our proposed algorithm. Variation     

> Initialization of random solutions
> Mutation or Crossover
> Calculate reward
> Sample
> Terminal set Operator set
> Optimization & evaluation
> Selection Survival
> Termination
> NY
> Stop
> criteria
> Sample
> Optimization & evaluation
> Y
> c
> N
> Search space

Figure 1: Brush Overview. Nodes are sampled from the search space to build mathematical expression trees, including those with split operations. A randomly generated population of expressions goes into an evolutionary loop. The set of nodes are used to create variations on the population. After several generations, the final solution is selected from the evolved individuals. Our experiments are two-fold. In the first set of experiments, we validate our method and place it in the context of other SotA approaches. We ran Brush on 122 real-world and 130 

synthetic physics problems from SRBench [21], reporting R2 and accuracy solution of governing physics equations. Brush achieved Pareto-optimal performance in SRBench and, compared to 3 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

other SR methods, produced significantly smaller expressions. It achieved R2 > 0.999 in more than half of the runs, even under high amount of noise. In the second set of experiments, we applied Brush to regression and classification tasks using EHR data from Beth Israel Deaconness Medical Center. We extracted data for 10 , 000 individuals from the MIMIC-IV [23] emergency department table, including vital signs and demographics collected at triage, and applied Brush to the regression task of learning triage scores — namely CART, and MEWS. Brush produced competitive results with much smaller expressions. We then modify the regression task to a classification scenario, aiming to identify patients at high risk of catastrophic deterioration based on vital signs. Brush successfully generated small models with simple split rules to flag high-risk patients. Our results show that Brush is competitive with SotA symbolic regression algorithms while incorporating split-wise equations, and can also support decision-making by generating models that reflect the features and logic of ground-truth systems. The algorithm is promising both as a first-principles modeling tool and as a method for learning interpretable classification models, expanding the practical applications of symbolic regression. 

# 2. Related work 

The idea of mixing decision trees with linear regression models as leaves has been previously explored [24,25], but recent methods have integrated SR with decision trees [26 –28]. PS-Tree [26] builds decision tree-like structures with symbolic regression models as leaves and achieves competitive R2 performance, but at the cost of overly large models. It lacks parameter optimization, relying on randomly generated constants. Another method similar to PS-Tree generates decision trees with symbolic models as leaves [27], performing well on problems with or without required splits. A different approach, SREDT [28], uses splits derived from an SR algorithm. SREDT outperforms traditional decision trees but also lacks parameter optimization. Optimizing free parameters remains challenging, as random constants hinder convergence and fail to reach global optima [29], with evolutionary methods alone converging slowly [30]. This burden can be reduced using optimization methods. Recent SR algorithms apply Levenberg-Marquardt (LM) [31,32] to tune model parameters [33 –36], which has proven effective. Some pre-trained transformer-based methods also struggle with constants, highlighting the need for nonlinear optimization to improve model accuracy [37–39]. Previous work by La Cava et al. [7] modified the FEAT [40] SR algorithm to perform logical comparisons, and it was applied to learn linear combination of meta-features in a logit function for developing computable phenotypes for hypertension. Other SR applications in healthcare include using SR for feature engineering inputs to classifiers for hearth failure prediction [8], or learn meta-features from pediatric patients to predict scores from CT scans [9]. 

# 3. Brush: mixing split-wise functions and parameter optimization 

Brush is a symbolic regression algorithm capable of learning expressions with split-wise operations, without being constrained to a decision tree-like structure. Its splits are compatible with non-linear parameter optimization methods, which are lacking in current algorithms. Additionally, Brush integrates several modern strategies within the genetic programming framework to efficiently explore the search space. This section introduces the split node and its optimization approach, followed by a description of the mechanisms integrated into Brush. Let a dataset be a set of d observed points {(xi, y i)}di=1 , where x is a n-dimensional feature vector, and y is the target value. We denote the feature matrix as X = [x1, x2, . . . , xd]′ ∈ Rn×d,and the vector of target values as y = [y1, y 2, . . . , y d] ∈ Rd. Brush searches for ˆf (X , ˆθ) : Rn → R

such that ˆf (X ) ≈ y, where ˆf is a mathematical expression represented as a tree. 4 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Brush introduces a specialized split node operator , inspired by classical decision trees [14] — a split is a predicate determining whether to evaluate the left or right branch based on a boolean output. During learning, the algorithm exhaustively tests combinations of feature xi

and threshold τ , selecting the configuration [xi∗ > τ ∗] that minimizes the total variance of the target values in the resulting partitions, y[xi∗ >τ ∗] and y[xi∗ ≤τ ∗].Our split node has three branches: one real-valued subtree fc as the condition input, and one subtree each for the true and false cases fT and fF . Each branch is another Brush tree. The conditional compares fc with a learnable weight τ to determine the data flow between fT and 

fF . The optimal τ ∗ minimizes the sum of target variances on either side of the split, effectively performing one-dimensional clustering: 

τ ∗ =min 

> τ

( Var (l)

|l| + Var (r)

|r|

)

(3.1) such that min (fc(X , θ )) < τ < max (fc(X , θ )) (3.2) where l = [ yi : y|fc(xi, θ ) ≤ τ ], (3.3) 

r = [ yi : y|fc(xi, θ ) > τ ]. (3.4) This operator masks the dataset, so downstream operations are applied only to a subset of the training data. Restricting subtrees to a subset of the data allows the resulting subtree to be more precise and enables faster parameter optimization. In Brush, the initial population can optionally consist solely of split nodes, which are then progressively replaced by mathematical expressions through genetic programming whenever predictive performance is improved. Each node is assigned an innate weight, which can be toggled during the search. In the initial population, only terminal weights are toggled on. This extends Operon [36] mechanism that assigns weights exclusively to terminals. Let the node weights and any constants in an expression be adjustable parameters of the function ( θ). Define the residual error as the result of the function ˆf with parameters 

θ ∈ Rp as H(θ) = ̂ f (X , θ ) − y. Then, the optimization problem is done by an iterative process of gradient descent to minimize the mean squared error described by Levenberg-Marquardt [31,32]: 

θ∗ = arg min 

> θ

12 || H(θ)|| 2. (3.5) Since any subtree is a valid Brush tree, we can isolate optimization to subtrees. We propose the following heuristic for optimization with split nodes. First, we optimize the conditional subtree 

fc, then find the best threshold τ ∗ that clusters the data into two groups with minimal variance. Finally, we fit the remaining expression, ignoring the already optimized parameters and passing only the data subset matching the condition to each corresponding branch. We propose a greedy split node , where the conditional is a single feature and the threshold is learned as in decision trees, and a flexible split node , where the evolutionary framework generates a sub-expression and only its threshold is optimized. Figure 2 illustrates (A) tree evaluation, (B) how a split node partitions data across subtrees, and (C) the overall evaluation and optimization process. The split nodes minimize the sum of target variances between the two subtrees. The optimization is done in three steps: first, we optimize the decision criterion (Eq. 3.5) — which may be a single feature with a threshold or a subtree — then determine the optimal threshold for the split (Eq. 3.1), and finally fit the remaining parameters (Eq. 3.5) while keeping the split condition and threshold fixed. Brush uses genetic programming (GP) to optimize overall fitness of a population of candidate models. The fitness is defined as a vector of objectives we want to optimize. Specifically, we use the Mean Squared Error MSE (̂y, y) = 1

> d

∑di=1 (̂y i − yi)2 and the linear complexity to concurrently prioritize simplicity. For a node n with k arguments, the linear complexity is the sum of its children’s linear complexities with the complexity of the node itself cn, that is C(n) = 

cn + ( ∑ka=1 C(a)) , discouraging the use of operators with high complexity values. 5 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

The GP loop starts with a randomly initialized population, then iteratively performs selection, variation, and survival steps. Selection chooses candidates based on fitness to undergo variation ,generating offspring that inherit characteristics from parents. Variation doubles the population size, which returns to its original size during survival , where selection pressure favors better solutions, with probabilities proportional to their fitness. Brush uses ϵ-lexicase [41] for selection, and non-dominated sorting [42,43] (NSGA-II) for survival, which are algorithms proven to yield better convergence performance than other selection and survival mechanisms. Finally, we apply inexact simplification [44] as a post-processing step. This technique identifies and replaces large sub-trees with simpler, approximately equivalent alternatives by comparing prediction vectors. This allows us to compress models while preserving their performance. input:  output:        

> YN
> 1. Fit the decision criteria subtree
> 3. Fit
> outside the criteria subtree
> N
> Condition Y
> or
> Filtered data based on condition
> ABC
> 2. Find that minimizes variance at the conditional childrens

Figure 2: Brush evaluation, split, and optimization. ( A) Evaluation starts at the root, recursively propagating input x and parameters θ, applying each node’s symbol to its children. ( B) Splits can occur anywhere, with the conditional subtree directing data flow based on the condition. ( C)Optimization with splits in three steps: optimize child 1; find the τ minimizing y variance across child 2 and child 3; then fit the rest of the tree, ignoring already fitted parameters. We implemented 6 different mutation operators and a subtree crossover. Toggle weight on/off 

randomly enables or disables a learnable weight on a node. Subtree replaces one node with a random sub-tree, generated with PTC2. Point replaces one random node with a new one of same arity. Delete removes one node, keeping its children. Insert creates a new node. The crossover 

randomly switches two sub-trees between the selected parents. Finally, we extend Brush to classification tasks by introducing a dedicated classifier mode. In this setting, a logistic regression node is fixed at the root of the expression tree. We also enforce the presence of an offset parameter, which can be optimized using the same non-linear optimization methods, without requiring any modifications to the algorithm, and broadens the applicability of Brush to practical problems. 

# 4. Performance on the Symbolic Regression Benchmark 

In our first set of experiments, we demonstrate that Brush can achieve competitive performance on symbolic regression benchmarks. The SR community has adopted SRBench [21] as the standard benchmark for symbolic regression. It is composed of two tracks: black-box and ground-truth 

problems. The former are 122 datasets derived from the Penn Machine Learning Benchmarks (PMLB) [45], while the latter are 130 datasets generated with physics equations from the Feynman Lectures [46,47] compiled by [4], along with 14 nonlinear dynamical systems from Strogatz [48]. The objective in the ground-truth track is to recover the original equation from noisy observations. SRBench performs 10 trials per dataset using a 75 /25 train-test split. For the black-box track, the training is limited at 500 , 000 function evaluations or 48 hours. For the ground-truth track, the limits are 1, 000 , 000 evaluations or 8 hours, with varying levels of noise [0 , 0.1, 0.01 , 0.001] 6 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

added to the target y. In our experiments, Brush used fixed hyperparameters: population size of 

1000 , 100 generations, maximum tree depth of 10 , maximum expression size of 128 , 10 iterations of local optimization, and function set: +, −, ×, ÷, sin , cos , tanh , exp , log , √·, pow , Split. Results for other algorithms were taken from the original SRBench benchmark [49]. Additionally, we ran recent relevant SR methods such as E2E [37], uDSR [38], and TPSR+E2E [39] using their default configurations. Although the inclusion of these methods provide only a rough estimate of their performance, we did it out of completeness given their reported performances, as their raw results are not available, potentially limiting more fair comparisons. 0 5 10 15 20 25  

> R2Test Rank
> 0
> 5
> 10
> 15
> 20
> 25
> Model Size Rank
> AFP
> AFP_FE
> AIFeynman
> AdaBoost
> BSR
> Brush
> DSR
> E2E
> EPLEX
> FEAT
> FFX
> GP-GOMEA
> ITEA
> KernelRidge
> LGBM
> Linear
> MLP
> MRGP
> Operon
> PS-Tree
> RandomForest
> SBP-GP
> TPSR+E2E
> XGB
> gplearn
> uDSR

(a) Pareto plot comparing model size rank (y axis) and R2 score rank ( x axis) for black-box problems. Smaller ranks are better. Points denotes the median, and bars denotes the 95% CI. 0.00 0.25 0.50 0.75 1.00     

> Accuracy Solution
> Operon
> GP-GOMEA
> SBP-GP
> MRGP
> AIFeynman
> Brush
> AFP_FE
> EPLEX
> AFP
> FEAT
> uDSR
> E2E
> gplearn
> ITEA
> DSR
> BSR
> FFX
> TPSR+E2E
> Feynman
> 0.00 0.25 0.50 0.75 1.00
> Accuracy Solution
> Strogatz
> Target Noise
> 0.0
> 0.001
> 0.01
> 0.1

(b) Mean accuracy solution rate for ground-truth problems with varying noise levels, where the rate is defined as the percentage of results with R2 > 0.999 

(four significant figures). Color/shapes indicates noise levels (normal distribution) added to y, and bars denotes the 95% CI. Figure 3: Brush’s SRBench results. Bars denotes the bootstrapped confidence intervals (CI). Brush’s performance on SRBench is summarized in Figure 3. The Pareto plot for the black-box problems shows that Brush produces significantly smaller models than PSTree, and often smaller than Operon. It dominates FEAT in both model simplicity and accuracy, achieving the second-best rank in test R2 score. For the ground-truth track, Brush recovers accurate expressions across varying noise levels. Although split nodes and weighted terminals may prevent exact equation recovery, we observe that over 50% of solutions achieve R2 > 0.999 with four significant figures for all levels of target noise, and Brush achieves mean (SD) of 84 .11( ±25 .77)% accuracy solution with no noise, and 

83 .63( ±25 .55)% at maximum noise with three significant figures ( i.e. R2 > 0.99 ). Brush is robust to noise, unlike methods such as AIFeynman and MRGP, which show a significant drop in accuracy as noise increases. This suggests Brush successfully recovers approximately half of all Feynman and Strogatz equations, regardless of noise level. PS-Tree does not appear in the ground-truth track, as its enforced use of split-wise operations prevents it from modeling exact equations, and their original authors opted out for this track. These results shows that Brush achieves SotA performance across a diverse set of over 250 

datasets. Brush matches the accuracy of traditional nonlinear optimization methods, while allowing for flexible expression structures with decision splits at any node. Overall, Brush demonstrates good potential in regression and physic equation modeling tasks, with robust performance in noisy settings. 7 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

# 5. Methods 

To demonstrate the split nodes utility, we designed a high risk classification experiment based on clinical risk scores derived from the MIMIC-IV-ED v2.2 dataset. MIMIC is a publicly available dataset containing electronic medical records from 2008 to 2019 , collected at Beth Israel Deaconess Medical Center (BIDMC) [23]. These data are deidentified and made available to researchers under a data use agreement following completion of human subjects training. Using the MIMIC-IV-ED pipeline [20], we extracted one simple clinical calculation (Mean Arterial Pressure (MAP)), the Cardiac Arrest Risk Triage (CART) score, and a simplified version of the Modified Early Warning Score (MEWS). While these scores were originally designed for ordinal assessment, we transformed them into binary classification tasks by using thresholds [18] for high risk of catastrophic deterioration classification. For MAP we used the standard formula MAP = 13 SBP + 23 DBP , where SBP and DBP are the systolic and diastolic blood pressure, respectively. The CART score (Table 1) aggregates points from respiratory rate, heart rate, DBP, and age, with a total score ranging from 0 to 57 We considered the threshold of ≥ 12 for high risk. Table 1: CART scoring system [16].                                               

> Score 0468912 13 15 22 Respiratory Rate <21 [21 −24) [24 −26) [26 −29) ≥29
> Heart Rate <110 [110 −140) ≥140
> Diastolic BP (mmHg) ≥49 [40 −50) [35 −40) ≤35
> Age <55 [55 −70) ≥70

The simplified MEWS score (Table 2) combines SBP, heart rate, respiratory rate, temperature, and a responsiveness score (AVPU). We excluded the AVPU component, because it was not accessible in a structured form. We used an interpretive threshold of ≥ 3. The maximum attainable score in our setting is 11 .Table 2: Simplified MEWS scoring system [17].                                                        

> 3210123Systolic BP (mmHg) <71 [71 −81) [81 −101) [101 −200) ≥200
> Heart Rate <41 [41 −51) [51 −101) [101 −111) [111 −130) ≥130
> Respiratory Rate <9[9 −15) [15 −21) [21 −30) ≥30
> Temperature ( ◦C) <35 [35 −38 .5) ≥38 .5

For each score, we trained models to solve both the original regression task and the binary classification task based on the clinical thresholds. We compared Brush with interpretable machine learning (ML) baselines and symbolic regression methods including split-wise operations: PS-Tree, FEAT, decision trees (DT), and logistic regression with L2 regularization (LR L2). We focused comparisons on PS-Tree due to its structural similarity to Brush (both rely on split-wise expressions), and on FEAT due to its successful application in clinical decision-making settings. Hyperparameter settings were as follows. DT had no limit on maximum depth. LR used L2 regularization and the liblinear solver. Random forests were fine-tuned for each run to optimize the number of estimator by searching the parameter grid: n_estimators ∈ { 10 , 100 , 1000 },

min_weight_fraction_leaf ∈ { 0.0, 0.25 , 0.5}, and max_features ∈ { “sqrt” , “log2” , None }.FEAT was tuned with population sizes ∈ { 100 , 500 , 1000 }, generation steps ∈ { 250 , 500 , 2500 },and learning rates ∈ { 0.1, 0.3}. Preliminary experiments with Brush used default population initialization which includes a wide array of operators; these experiments were used to generate example models for clinical interpretation. Subsequent experiments, for which we report aggregate performance metrics, initialized the population with decision tree split nodes, and used the 8 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

same other following fixed hyperparameters: pop_size = 500, max_gens = 100, max_depth = 12, 

max_size = 100, and function set { Add , Sub , Mul , Div , Ceil , Floor , Pow , Log , Min , Max , Split }. We conducted 5 runs of 5-fold cross-validation, totaling 25 runs per method. Each dataset included 10 , 000 samples split in a stratified 75 /25 train-test split. Class imbalance was present across tasks, with positive cases prevalence of 0.09 for CART, and 0.11 for MEWS. Due to this imbalance, we report the Area Under the Precision-Recall Curve (AUPRC) using the average precision score calculation, a more suitable measure for inbalanced class problems. The feature set comprised 77 variables including: triage vital signs (temperature, heart rate, blood pressure), emergency department (ED) visits in the past 30 or 365 days, chief complaints, and Charlson Comorbidity Index (CCI). We excluded post-admission ED vitals, which could act as proxies for triage admission. Even that the triage scores relies on at most 5 features, we kept the entire set to challenge the algorithm’s feature selection when building interpretable models. 

# 6. Results 

We evaluated the performance of all methods on both the scoring and classification problems using the triage scores. In the scoring version, the objective was to predict the clinical score values (MAP, CART, simplified MEWS), formulated as a regression problem. Table 3 presents the average R2 values and model size for each method, along with standard deviations (SD) across the runs. Model size is defined as the number of nodes required to represent the model in a Brush-style expression tree. Table 3: Average R2 and model size for each scoring task ( ± SD).                                                                                                                          

> MAP CART Score Simplified MEWS Score
> R2Size R2Size R2Size RF 0.84 ±0.28 (3 .50 ±4.11) ×10 60.99 ±0.00 (1 .58 ±0.55) ×10 60.98 ±0.01 (1 .19 ±0.77) ×10 5
> DT 0.81 ±0.26 11662 .24 ±89 .93 0.99 ±0.00 194 .68 ±6.93 0.98 ±0.01 206 .44 ±8.40
> LR L2 1.00 ±0.00 303 .00 ±0.00 0.56 ±0.11 303 .00 ±0.00 0.43 ±0.08 303 .0±0.00
> PSTree 1.00 ±0.00 217 .64 ±80 .13 0.87 ±0.18 813 .12 ±141 .54 0.85 ±0.12 815 .36 ±169 .99
> FEAT 1.00 ±0.00 13 .48 ±4.99 0.71 ±0.23 47 .76 ±21 .42 0.65 ±0.10 31 .28 ±20 .13
> Brush 1.00 ±0.00 13 .16 ±4.34 0.82 ±0.26 76 .44 ±15 .17 0.74 ±0.04 45 .00 ±20 .42

Next, we report the classification versions of the CART and simplified MEWS tasks, where models predict whether a patient is at high risk of deterioration based on different triage scores. Table 4 shows the average AUPRC and model size. Due to its lack of native support for classification, PS-Tree is excluded from this comparison. Table 4: Average AUPRC and size ( ± SD) for each of the clinical decision problems.                                                                     

> CART deterioration Simplified MEWS deterioration AUPRC Size AUPRC Size RF 1.00 ±0.01 (8 .39 ±15 .01) ×10 30.99 ±0.00 (2 .22 ±2.56) ×10 4
> DT 0.99 ±0.01 38 .48 ±3.11 0.99 ±0.01 63 .76 ±2.86
> LR L2 0.75 ±0.02 303 .00 ±0.00 0.81 ±0.03 303 .00 ±0.00
> FEAT 0.69 ±0.17 60 .76 ±77 .74 0.89 ±0.03 55 .60 ±27 .82
> Brush 0.99 ±0.01 73 .80 ±15 .18 0.95 ±0.02 61 .84 ±11 .63

We executed the experiments using Brush without split nodes to assess its impact on model performances. Disabling split nodes did not show statistically significant impact on both size and R2 scores for the regression problems, but showed significant degradation to AUPRC for the deterioration classification tasks. On the test set, Brush without split nodes achieves a 9 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

smaller AUPRC compared to its counterpart with split nodes in both CART Deterioration —0.78 ± 0.07 versus 0.99 ± 0.01 (p ≤1.00e-04)— and MEWS Deterioration tasks — 0.88 ± 0.04 

versus 0.95 ± 0.02 (p ≤ 1.00e-04). Using the split nodes didn’t hurt the performance for other problems. This implies the gains are not only in interpretability, but also in classification performance. To further explore the interpretability of Brush models for clinical decision-making, we selected exemplar models from the classification tasks. We note that this model interpretation was conducted using the preliminary Brush settings that did not include split node seeding of the initial population. First, we filtered out models with an AUPRC below 0.90 . We also observed that some models with high AUPRC exhibited a balanced accuracy of 0.5 on the test partition, indicating random performance. Consequently, we excluded models with a balanced accuracy below 0.9. From the remaining models, we selected the smallest, which is visualized in Figure 4. CART deterioration                                                                                          

> 1Logistic (
> 2if ( resprate >=21) :
> 3⌊1.00
> 4else :
> 5|if ( dbp >=50) :
> 6||if ( heartrate >=140) :
> 7||⌊0.39
> 8||else :
> 9|||if ( heartrate >=110) :
> 10 ||||if ( age >=70) :
> 11 ||||⌊0.39
> 12 ||||else :
> 13 ||||⌊-5233.87
> 14 |||else :
> 15 |||⌊1.00 -dbp
> 16 |else :
> 17 ||if ( age >=69) :
> 18 |||if ( age >=71) :
> 19 |||⌊0.39
> 20 |||else :
> 21 |||⌊age -heartrate
> 22 ||else :
> 23 ||⌊-189.94 /o2sat
> 24 )

Simplified MEWS deterioration                                                     

> 1logistic ( -42.07 +
> 2max (
> 3|if ( heartrate >=111) :
> 4|⌊54.05
> 5|else :
> 6||if ( sbp >=200) :
> 7||⌊54.00
> 8||else :
> 9|||if ( temperature >=38.50) :
> 10 |||⌊55.00
> 11 |||else :
> 12 |||⌊resprate
> 13 |) ,
> 14 |(1 +1.16* heartrate -1.05* sbp ) ,
> 15 |n _ h o sp i t a li z a t io n s _ 36 5 d
> 16 )+
> 17 log ( resprate )
> 18 )

Figure 4: Exemplar Brush models edited as Python code for catastrophic deterioration prediction using two different scoring systems CART and simplified MEWS. To complement the tabular results in Table 4, Figure 5 presents barplots of both performance and model size for the classification task. Subfigure 5a displays the R2 scores and complexities for the regression-based scoring tasks, and Subfigure 5b shows the AUPRC scores and complexities for the classification tasks. These plots also include statistical comparisons between Brush and the other methods using the Mann-Whitney-Wilcoxon two-sided test with Holm-Bonferroni correction. We set y axis lower bound to 0.5 in AUPRC plots, and 0.25 in R2 plots. 

# 7. Discussion 

We observe that the MAP score —a weighted sum of two features— showed to be a particular challenge for traditional ML methods. Tree-based models such as RF and DT performed poorly compared to other methods, often producing large models with greater variability. This is likely due to the continuous nature of the output, making it difficult for split-based methods to perform well due to their inherent discretization mechanism with constants as leaves. LR L2 also found large and less interpretable models, failing on the feature selection aspect. PSTree consistently produced overly large models, possibly due to the high dimensionality of the feature space 10 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0.4
> 0.6
> 0.8
> 1.0
> R2 Score Test
> ****
> ****
> ****
> ****
> Score Triage MAP
> ****
> **
> ****
> ****
> ****
> Score CART
> **
> ****
> ****
> ****
> ****
> Score MEWS
> RF
> DT
> LR L2
> FEAT
> Brush
> PSTree
> Model
> 10 1
> 10 2
> 10 3
> 10 4
> 10 5
> 10 6
> 10 7
> 10 8
> Size
> ****
> ****
> ****
> ****
> Score Triage MAP
> RF
> DT
> LR L2
> FEAT
> Brush
> PSTree
> Model
> **
> ****
> ****
> ****
> ****
> Score CART
> RF
> DT
> LR L2
> FEAT
> Brush
> PSTree
> Model
> **
> ****
> ****
> ****
> ****
> Score MEWS

(a) R2 score and model size for predicting the original scoring values. 0.6 

> 0.8
> 1.0
> Average Precision Score Test
> ****
> ****
> ****
> CART Deterioration
> ****
> ****
> ****
> ****
> MEWS Deterioration
> RF
> DT
> LR L2
> FEAT
> Brush
> Model
> 10 1
> 10 2
> 10 3
> 10 4
> 10 5
> Size
> ****
> ****
> ****
> ****
> CART Deterioration
> RF
> DT
> LR L2
> FEAT
> Brush
> Model
> ****
> ****
> MEWS Deterioration

(b) AUPRC and size for predicting deterioration. Figure 5: Metrics and sizes for the clinical decision experiment. The size of the original decision model is denoted as a dashed line. Statistical comparisons are conducted using Mann-Whitney-Wilcoxon two-sided test with Holm-Bonferroni corrections. *: 1.00e-02 < p ≤ 5.00e-02; **: 1.00e-03 < p ≤ 1.00e-02 ***: 1.00e-04 < p ≤ 1.00e-03 ****: p ≤ 1.00e-04. Non-significant comparisons were omitted. combined with its partitioning strategy, which creates numerous splits. Alternatively, PSTree result may be due its reliance on random guessing the coefficients. In contrast, FEAT and Brush produced more compact models with similar or better performance. For the CART and simplified MEWS scoring tasks, we notice that LR L2 shows a drop in R2,while FEAT and Brush achieved superior performance. This suggests that logistic regression, when combined with the feature selection and feature engineering capabilities provided by genetic programming (as used in both symbolic regression methods), can outperform classical logistic regression. For these specific tasks, Brush consistently outperforms FEAT, at the cost of slightly larger models. PSTree achieves higher performance than Brush, but at the cost of models that are more than 20 times larger. Figure 5a provides statistical comparisons, showing that Brush models showed statistically significant differences except when compared to FEAT in the MAP score. It also reveals that the trade-off between AUPRC and model size is overall unfavorable for PSTree due to its extremely large model sizes. Brush models showed relatively low variance across runs when compared to other SR methods, indicating stable behavior. When model predictions are transformed into clinical decision tasks, we observe notable performance differences in Table 4. In particular, both SR methods perform worse than DT on rule-based classification tasks. Compared to FEAT, Brush demonstrates superior performance on CART but yields comparable results on simplified MEWS. These differences are also present in regression tasks; however, Brush and FEAT generate more compact models than shallow methods such as RF, DT, and LR L2. Brush outperforms LR L2 in all cases, and produces smaller models than DT for simplified MEWS. Statistical comparisons in Figure 5b show no significant size difference between Brush and DT for CART, but a significant reduction for MEWS. Overall, we position Brush as a strong compromise between the three main modeling approaches. It approximates DT performance in classification tasks while yielding better regression models; it enhances logistic modeling compared to LR L2 through feature engineering and selection using genetic programming; and it outperforms FEAT and PSTree in different aspects 11 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

by combining split-wise operations with parameter optimization. Allowing specific customization in the trade-off between performance and complexity could yield better task-specific performance. The Brush model uses a logistic root, meaning it outputs a probability estimate for deterioration risk. This probabilistic output can be evaluated using metrics like area under the Precision-Recall Curve or ROC curve, and an optimal classification threshold can be chosen accordingly. For the CART deterioration task, the model depicted in Figure 4 has a size of 62 nodes, an AUPRC of 0.92 , and a balanced accuracy of 0.99 on the test set. The model evaluation begins with a split on respiratory rate ( > 21 ), and can be seen as the most immediate high-risk indicator, identifying high risk immediately — consistent with the original scoring system. If this condition fails, the model proceeds through a nested evaluation of diastolic blood pressure (DBP), heart rate, and age, computing a final probability of deterioration. We can see combinations of DBP and heart rate in the first block of conditionals, which are the two factors other than respiration rate that have the highest scores in the CART scoring system of DBP ≤ 35 and heart rate 

≥ 140 , resulting in a risk higher than the threshold, thus a positive label. Age has a smaller priority for probability modelling, and, in case the patient is not at advanced ages above 70 ,then a computation is made using ‘age - heart rate’. We note that the final branch — when all conditionals fail, meaning a superior bound was set to all tested features —, references O2

saturation, which was not used during label generation and may be an artifact, and could be simplified. This structure mirrors the kind of heuristic reasoning a clinician might follow and resembles the CART scoring system, although in a compact, rule-based form. The Brush model for simplified MEWS deterioration prediction had an AUPRC of 0.96 and a balanced accuracy of 0.91 on the test set, with a model size of 68 nodes. Impressively, it identified the relevant features aligned with the actual simplified MEWS scoring system from a pool of 77 

possible features, and learned threshold values consistent with the actual clinical model. The model uses a Max function to return a fixed output associated with high-risk prediction if any of the vital signs (heart rate, SBP, or temperature) are critically high. If none of these are critical, the model considers a linear combination of heart rate and SBP, which identifies a combination of relative tachycardia and hypotension, and number of previous admissions. Finally this is added to the logarithm of respiratory rate. This algorithm integrates risk across vital signs in several ways, such that it is simpler than the MEWS score which treats each element as independent. 

# 8. Conclusions 

In this paper we propose Brush, a Multi-Objective symbolic regression algorithm specially designed for solving problems where split-wise functions are desired. The novelty introduced by Brush is integrating split-wise functions with non-linear optimization methods, also combining several state-of-the-art components into a genetic programming framework, namely ϵ-lexicase selection, NSGA-II survival, weighted nodes, fast simplification methods, and classification support. Brush contributes to building interpretable and high-performing models. We shown that Brush achieves Pareto-optimal performance on a collection of 122 real world datasets, and successfully retrieves more than half of the Feynman and Strogatz problems, even in noisy scenarios. These results highlight its potential as a tool for deriving interpretable and high-performing heuristics in healthcare and other domains where interpretability is essential. Applied to real-world electronic health records data, Brush consistently identified relevant features and produced compact decision models with near-perfect predictive performance. Overall, the models are simple, interpretable, and captures the essential structure of clinical scoring with minimal deviation, missing only some corner cases. Brush effectively balances interpretability, performance, and stability —rarely achieved simultaneously in clinical modeling tools. We notice some limitations. First, Brush was not fine tuned, which could lead to better results, but was out of the scope of this paper. Second, Brush minimizes linear complexity based on arbitrarily defined complexities for each node, which could be tweaked or derived from prior distributions observed in different fields. 12 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

## Data availability 

SRBench [21] experiments and results are available at GitHub [49]. MIMIC-IV-ED dataset [50] is available at Physionet https://physionet.org/content/mimic-iv-ed/2.2/ . MIMIC-IV-ED pipeline [51] is available at GitHub [52]. Brush source code [53] is hosted at GitHub https: //github.com/cavalab/brush/ . Our experiments are available at GitHub https://github.com/ cavalab/brush_paper_experiments .

## Acknowledgments 

W.G.L., D.S.H, and G.S.I.A. are supported by Patient Centered Outcomes Research Institute (PCORI) ME-2020C1D-19393. The statements in this work are solely the responsibility of the authors and do not necessarily represent the views of the Patient-Centered Outcomes Research Institute (PCORI), its Board of Governors or Methodology Committee. F.O.F. is supported by Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq) grant 301596/2022-0. G.S.I.A. is supported by Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES) finance Code 001 and grant 88887.802848/2023-00. J.D.R. is supported by US National Libraries of Medicine grant R00-LM013646. 

# References 

1. Koza JR. 1994 Genetic programming as a means for programming computers by natural selection. Statistics and computing 4, 87–112. 2. Udrescu SM, Tan A, Feng J, Neto O, Wu T, Tegmark M. 2020 AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. In Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H, editors, Advances in Neural Information Processing Systems vol. 33 pp. 4860–4871. Curran Associates, Inc. 3. Lalande F, Matsubara Y, Chiba N, Taniai T, Igarashi R, Ushiku Y. 2023 A Transformer Model for Symbolic Regression towards Scientific Discovery. In NeurIPS 2023 AI for Science Workshop .4. Udrescu SM, Tegmark M. 2020 AI Feynman: A physics-inspired method for symbolic regression. Science Advances 6, eaay2631. (10.1126/sciadv.aay2631) 5. Makke N, Chawla S. 2024 Interpretable scientific discovery with symbolic regression: a review. 

Artificial Intelligence Review 57 , 1–32. (10.1007/s10462-023-10622-0) 6. Angelis D, Sofos F, Karakasidis TE. 2023 Artificial Intelligence in Physical Sciences: Symbolic Regression Trends and Perspectives. Archives of Computational Methods in Engineering 30 ,3845–3865. (10.1007/s11831-023-09922-z) 7. La Cava WG, Lee PC, Ajmal I, Ding X, Solanki P, Cohen JB, Moore JH, Herman DS. 2023 A flexible symbolic regression method for constructing interpretable clinical prediction models. 

npj Digital Medicine 6, 107. (10.1038/s41746-023-00833-8) 8. Wilstrup C, Cave C. 2022 Combining symbolic regression with the Cox proportional hazards model improves prediction of heart failure deaths. BMC Medical Informatics and Decision Making 22 , 1–7. (10.1186/s12911-022-01943-1) 9. Virgolin M, Alderliesten T, Bel A, Witteveen C, Bosman PAN. 2018 Symbolic regression and feature construction with GP-GOMEA applied to radiotherapy dose reconstruction of childhood cancer survivors. In Proceedings of the Genetic and Evolutionary Computation Conference GECCO ’18 p. 1395–1402 New York, NY, USA. Association for Computing Machinery. (10.1145/3205455.3205604) 10. La Cava W, Danai K, Spector L, Fleming P, Wright A, Lackner M. 2016 Automatic identification of wind turbine models using evolutionary multiobjective optimization. 

Renewable Energy 87 , 892–902. Optimization Methods in Renewable Energy Systems Design (https://doi.org/10.1016/j.renene.2015.09.068) 11. La Cava W, Danai K, Lackner M, Spector L, Fleming P, Wright A. 2015 Automatic Identification of Closed-Loop Wind Turbine Dynamics via Genetic Programming. In Dynamic Systems and Control Conference vol. 57250 p. V002T21A002. American Society of Mechanical Engineers. 13 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12. Wang Y, Wagner N, Rondinelli JM. 2019 Symbolic regression in materials science. MRS Communications 9, 793–805. (10.1557/mrc.2019.85) 13. Food, Administration D. 2022 Clinical Decision Support Software - Guidance for Industry and Food and Drug Administration Staff. https://www.fda.gov/regulatory-information/search-fda-guidance-documents/clinical-decision-support-software. 14. Breiman L, Friedman J, Stone C, Olshen R. 1984 Classification and Regression Trees . Taylor & Francis. 15. Rudin C, Chen C, Chen Z, Huang H, Semenova L, Zhong C. 2022 Interpretable machine learning: Fundamental principles and 10 grand challenges. Statistics Surveys 16 , 1 – 85. (10.1214/21-SS133) 16. Churpek MM, Yuen TC, Park SY, Meltzer DO, Hall JB, Edelson DP. 2012 Derivation of a cardiac arrest prediction model using ward vital signs. Critical care medicine 40 , 2102–2108. 17. Subbe CP, Kruger M, Rutherford P, Gemmel L. 2001 Validation of a modified Early Warning Score in medical admissions. Qjm 94 , 521–526. 18. Tan ADA, Permejo CC, Torres MCD. 2022 Modified early warning score vs cardiac arrest risk triage score for prediction of cardiopulmonary arrest: a case–control study. Indian Journal of Critical Care Medicine: Peer-reviewed, Official Publication of Indian Society of Critical Care Medicine 26 , 780. 19. Guidetti V et al.. 2024 Symbolic Regression for Transparent Clinical Decision Support: A Data-Centric Framework for Scoring System Development. In CEUR WORKSHOP PROCEEDINGS vol. 3741 pp. 604–614. 20. Xie F, Zhou J, Lee JW, Tan M, Li S, Rajnthern LS, Chee ML, Chakraborty B, Wong AKI, Dagan A et al.. 2022 Benchmarking emergency department prediction models with machine learning and public electronic health records. Scientific Data 9, 658. 21. La Cava W, Orzechowski P, Burlacu B, de Franca F, Virgolin M, Jin Y, Kommenda M, Moore J. 2021 Contemporary Symbolic Regression Methods and their Relative Performance. In Vanschoren J, Yeung S, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks vol. 1. Curran. 22. Aldeia GSI, Zhang H, Bomarito G, Cranmer M, Fonseca A, Burlacu B, La Cava WG, de França FO. 2025 Call for Action: towards the next generation of symbolic regression benchmark. arXiv preprint arXiv:2505.03977 .23. Johnson AE, Bulgarelli L, Shen L, Gayles A, Shammout A, Horng S, Pollard TJ, Hao S, Moody B, Gow B et al.. 2023 MIMIC-IV, a freely accessible electronic health record dataset. 

Scientific data 10 , 1. 24. Gama J. 2004 Functional Trees. Machine Learning 55 , 219–250. (10.1023/b:mach.0000027782.67192.13) 25. Rusch T, Zeileis A. 2013 Gaining insight with recursive partitioning of generalized linear models. Journal of Statistical Computation and Simulation 83 , 1301–1315. 26. Zhang H, Zhou A, Qian H, Zhang H. 2022 PS-Tree: A piecewise symbolic regression tree. Swarm and Evolutionary Computation 71 , 101061. (https://doi.org/10.1016/j.swevo.2022.101061) 27. Doquet G. 2025 Unified Piecewise Symbolic Regression. In Xue B, Manzoni L, Bakurov I, editors, Genetic Programming pp. 190–206 Cham. Springer Nature Switzerland. 28. Fong KS, Motani M. 2024 Symbolic Regression Enhanced Decision Trees for Classification Tasks. In Proceedings of the AAAI Conference on Artificial Intelligence vol. 38 pp. 12033– 12042. 29. Wang HF, Wu KY. 2004 Hybrid genetic algorithm for optimization problems with permutation property. Computers & Operations Research 31 , 2453–2471. (https://doi.org/10.1016/S0305-0548(03)00198-9) 30. Chen C, Luo C, Jiang Z. 2017 Elite bases regression: A real-time algorithm for symbolic regression. In 2017 13th International conference on natural computation, fuzzy systems and knowledge discovery (ICNC-FSKD) pp. 529–535. IEEE. 31. Levenberg K. 1944 A method for the solution of certain non-linear problems in least squares. 

Quarterly of applied mathematics 2, 164–168. 14 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

32. Marquardt DW. 1963 An algorithm for least-squares estimation of nonlinear parameters. 

Journal of the society for Industrial and Applied Mathematics 11 , 431–441. 33. Aldeia GSI, de França FO. 2022 Interaction-Transformation Evolutionary Algorithm with Coefficients Optimization. In Proceedings of the Genetic and Evolutionary Computation Conference Companion GECCO ’22 p. 2274–2281 New York, NY, USA. Association for Computing Machinery. (10.1145/3520304.3533987) 34. Kommenda M, Burlacu B, Kronberger G, Affenzeller M. 2019 Parameter identification for symbolic regression using nonlinear least squares. Genetic Programming and Evolvable Machines 21 , 471–501. (10.1007/s10710-019-09371-3) 35. Worm T, Chiu K. 2013 Prioritized Grammar Enumeration: Symbolic Regression by Dynamic Programming. In Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation GECCO ’13 p. 1021–1028 New York, NY, USA. Association for Computing Machinery. (10.1145/2463372.2463486) 36. Burlacu B, Kronberger G, Kommenda M. 2020 Operon C++: an efficient genetic programming framework for symbolic regression. In Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion GECCO ’20 p. 1562–1570 New York, NY, USA. Association for Computing Machinery. (10.1145/3377929.3398099) 37. Kamienny PA, d’Ascoli S, Lample G, Charton F. 2022 End-to-end Symbolic Regression with Transformers. In Oh AH, Agarwal A, Belgrave D, Cho K, editors, Advances in Neural Information Processing Systems pp. 1–13. 38. Landajuela M, Lee CS, Yang J, Glatt R, Santiago CP, Aravena I, Mundhenk T, Mulcahy G, Petersen BK. 2022 A Unified Framework for Deep Symbolic Regression. In Koyejo S, Mohamed S, Agarwal A, Belgrave D, Cho K, Oh A, editors, Advances in Neural Information Processing Systems vol. 35 pp. 33985–33998. Curran Associates, Inc. 39. Shojaee P, Meidani K, Farimani AB, Reddy CK. 2023 Transformer-based Planning for Symbolic Regression. In Thirty-seventh Conference on Neural Information Processing Systems .40. La Cava W, Singh TR, Taggart J, Suri S, Moore JH. 2018 Learning concise representations for regression by evolving networks of trees. arXiv preprint arXiv:1807.00981 .41. La Cava W, Spector L, Danai K. 2016 Epsilon-Lexicase Selection for Regression. In Proceedings of the Genetic and Evolutionary Computation Conference 2016 pp. 741–748. arXiv:1905.13266 [cs] (10.1145/2908812.2908898) 42. Poli R, McPhee NF, Koza JR. 2008 A Field Guide to Genetic Programming . [S.I.]: [Lulu Press], lulu.com. 43. Deb K, Pratap A, Agarwal S, Meyarivan T. 2002 A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation 6, 182–197. (10.1109/4235.996017) 44. Imai Aldeia GS, De França FO, La Cava WG. 2024 Inexact Simplification of Symbolic Regression Expressions with Locality-sensitive Hashing. In Proceedings of the Genetic and Evolutionary Computation Conference GECCO ’24 p. 896–904 New York, NY, USA. Association for Computing Machinery. (10.1145/3638529.3654147) 45. Romano JD, Le TT, La Cava W, Gregg JT, Goldberg DJ, Chakraborty P, Ray NL, Himmelstein D, Fu W, Moore JH. 2021 PMLB v1.0: an open-source dataset collection for benchmarking machine learning methods. Bioinformatics 38 , 878–880. (10.1093/bioinformatics/btab727) 46. Feynman R, Leighton R, Sands M. 2006 The Feynman Lectures on Physics . Number vol. 2 in The Feynman Lectures on Physics. Pearson/Addison-Wesley. 47. Feynman R, Leighton R, Sands M. 2015 The Feynman Lectures on Physics, Vol. I: The New Millennium Edition: Mainly Mechanics, Radiation, and Heat . Number vol. 1 in The Feynman Lectures on Physics. Basic Books. 48. Strogatz SH. 2018 Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering . CRC press. 49. La Cava W, Cranmer M, de Franca FO, Orzechowski P, Burlacu B, kahlmeyer94, Marco, Zhang H, Boisbunon A, McDermott J, Matsubara Y, Bouter A, Kartelj A, Jin Y. 2025 

cavalab/srbench .15 royalsocietypublishing.org/journal/rsta Phil. Trans. R. Soc. A 0000000 

> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

50. Johnson A, Bulgarelli L, Pollard T, Celi LA, Mark R, Horng S. 2023 MIMIC-IV-ED. (10.13026/5NTK-KM72) 51. Gupta M, Gallamoza B, Cutrona N, Dhakal P, Poulain R, Beheshti R. 2022 An Extensive Data Processing Pipeline for MIMIC-IV. In Proceedings of the 2nd Machine Learning for Health symposium vol. 193 Proceedings of Machine Learning Research pp. 311–325. PMLR. 52. mehak25, Gallamoza B, UDpranjal, Cutrona N. 2024 healthylaife/MIMIC-IV-Data-Pipeline .53. Aldeia G, La Cava W, Romano J. 2025 cavalab/brush .