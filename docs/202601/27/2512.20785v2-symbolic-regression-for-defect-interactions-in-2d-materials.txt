Title: Symbolic regression for defect interactions in 2D materials

URL Source: https://arxiv.org/pdf/2512.20785v2

Published Time: Tue, 13 Jan 2026 03:14:46 GMT

Number of Pages: 25

Markdown Content:
# Symbolic regression for defect interactions in 2D materials 

# Mikhail Lazarev ∗1 and Andrey Ustyuzhanin 3,2 1HSE University, Myasnitskaya Ulitsa, 20, Moscow, Russia, 101000 

2Institute for Functional Intelligent Materials, National University of Singapore, 4 Science Drive 2, Singapore 117544 

3Constructor University, Campus Ring 1, Bremen, 28759, Germany January 13, 2026 

Abstract 

Machine learning models have become firmly established across all scientific fields. Ex-tracting features from data and making inferences based on them with neural network models often yields high accuracy; however, this approach has several drawbacks. Symbolic regres-sion is a powerful technique for discovering analytical equations that describe data, pro-viding interpretable and generalizable models capable of predicting unseen data. Symbolic regression methods have gained new momentum with the advancement of neural network technologies and offer several advantages, the main one being the interpretability of results. In this work, we examined the application of the deep symbolic regression algorithm SEG-VAE to determine the properties of two-dimensional materials with defects. Comparing the results with state-of-the-art graph neural network-based methods shows comparable or, in some cases, even identical outcomes. We also discuss the applicability of this class of methods in natural sciences. 

Keywords— 2D materials, Machine Learning, Symbolic Regression, GNNs, Interpretabil-ity. 

> ∗

Corresponding author: mvlazarev@hse.ru 

1

> arXiv:2512.20785v2 [cs.LG] 12 Jan 2026

# 1 Introduction 

Since ancient times, humanity has sought to understand dependencies in observable effects and phenomena around it. The new discoveries that have driven us to the technological advancements of today were made manually through an iterative process of theory building and practical vali-dation. These theories, typically expressed in the language of mathematics, can be represented through formulas or symbols; for instance, the theory of classical electrodynamics is fully de-scribed by four equations. This analogy is not unique, as other fields or branches of science also have compact symbolic representations to describe specific effects. Abstractly, the development of scientific knowledge methods can be depicted in 4 paradigms as shown in Figure 1 (top sec-tion). The lower part of Figure 1 illustrates the types of numerical modeling methods for physical systems across different time and spatial scales. 

Figure 1: Upper part: Science paradigm over time. Taken from [1]. Lower part: Simulation methods based on physics theories. Taken from [2]. 2Today, neural network-based technologies have permeated virtually all areas of life and sci-ence, reaching new heights in process automation and the search for dependencies in observed phenomena. On one hand, modern hardware enables the training of extremely large neural net-work algorithms, which have become the gold standard of quality in many fields. However, they function as black boxes, producing results that are often impossible to interpret. In science, in-terpretability is essential due to the need to uncover complex relationships within data. Thus, for neural network methods to be widely applicable, we must either develop algorithms to interpret them or design inherently interpretable algorithms. The interpretability of model predictions is crucial for many areas of potential application, such as medicine, autonomous vehicles, and finance—fields where errors could have significant consequences. A promising solution lies in symbolic regression methods, which integrate neural network approaches under the hood but yield interpretable final results. Symbolic regression is a form of regression that searches for formulas in the space of mathe-matical expressions. The goal is to find an optimal expression, one that best explains the training data, while often requiring that the formula be as concise as possible to ensure interpretability. Such an interpretable model offers several advantages over black-box models; assuming the dis-covered formula accurately captures the system’s underlying laws, it can be extrapolated beyond the training data. The primary objective of symbolic regression is to uncover mathematical dependencies that describe observed data. While the topic of symbolic regression is not new, it predates the widespread use of neural networks. Historically, evolutionary programming methods such as genetic algorithms [3] were used to generate formulas that best fit the data. There even ex-isted commercial products, like Eureqa [4], which applied symbolic regression algorithms and were state-of-the-art at the time. However, today, a diverse range of neural network-based symbolic regression models has emerged, outperforming previous evolutionary algorithm-based approaches. The variety of symbolic regression algorithms is reviewed in appendices 9. 

# 2 Related Work 

In natural science, symbolic regression has emerged as a powerful method for uncovering ana-lytical relationships within complex datasets, providing interpretable models that align closely with underlying physical laws. In material science, symbolic regression (SR) is implemented for some tasks as well. For example, in [5] can be found a comprehensive review of the application of SR in materials science can be found, where a big part of examples has genetic algorithms (GA) under the hood. Traditionally, GAs are still popular for SR tasks, for example, in [6], authors show that generated equations perform better in this case than well-established empiri-cal equations for Lennard–Jones fluid descriptors. This is another evidence of SR application in the same field [7]. Genetic programming (GP) remains competitive for certain physics modeling tasks and can be combined with graph-based inductive biases; for example in [8] demonstrate GP-based symbolic models for particle-laden flows and compare them against graph-network baselines. Discovering interatomic potentials from data with SR based on GA was demonstrated in [9]. Another use case where SR was implemented to establish a symbolic dependence form of a band gap in NaCl-type compounds [10]. In [11] GA GA-based SR was compared in terms of accuracy with other machine learning (ML) methods and demonstrated good performance; however, the final equation forms usually contain inner functions that usually do not appear in physical models. An interesting study of atomic-scale modeling with SR for 2D materials [12] in this particular case, low-dimensional TiO 2. Authors use SR to study automatically designed molecular descriptors that capture the essential chemical interactions, such as those influenced by 3external electric fields. Combined with machine learning, specifically random forest algorithms, symbolic regression achieves high accuracy in predicting adsorption energies. SR for searching new descriptors also was used in [13]. In [14] SR has been used to discover fast, accurate, and transferable many-body potentials (e.g., for Cu), reducing simulation cost while retaining inter-pretability; such closed forms enable larger design-space exploration and quick what-if analyses in process and device modeling. In [15] integrative SR pipelines for multicomponent perovskite oxides provide interpretable formulas used to screen large compositional spaces for stable phases before costly Density functional theory (DFT) or experiments. In the paper [16] authors demon-strated another approach to utilize SR, instead of use of an operator’s library like in GA-SR, where they use a feature generation algorithm and select features based on multiple criteria, thus selecting a few out of thousands of features, and use them to present finale equation in a compact form akin to a linear function. Many other examples can be found in the literature where symbolic formulas are needed for some purposes like in [17], where the goal was to find a formula that can be integrated into the simulation tool. These examples illustrate how closed-form, human-readable equations function as “design rules” that can be embedded into screening loops, surrogate models, and process windows, complementing black-box ML. 

Figure 2: One crystal can be represented in various ways using different representations. These representations include: a graph containing atom and bond weights, Coulomb matrix, diffraction fingerprint, using topological descriptors and etc. Taken from [18] with permission. Most of the symbolic regression methods discussed above rely on genetic algorithms (GA) for formula construction. As in machine learning models, selecting the correct descriptors is crucial for symbolic regression algorithms. A large feature space or an extensive set of arguments for the target formula results in a vast search space, which often leads to either a loss of interpretability to achieve the best quality metric or unsatisfactory outcomes. Every in silico object under study must be represented in a machine-readable format, and substantial datasets must be available for training machine learning models. A crystal can be represented in various ways, some of which are illustrated in Figure 2. A 4graph representation of a crystal is intuitive and arguably the most common approach, with several neural algorithms in the form of graph neural networks (GNNs) specifically designed for crystal structures in this format [19]. It is essential to account for the periodic nature of the crystal lattice when constructing the graph. Graph neural networks (GNNs) operate on graphs by iteratively updating learnable embeddings of nodes (and optionally edges and global attributes) via message passing. At each layer, a node collects messages from its neighbors using an aggregation operator (e.g., sum/mean/attention) and combines them with its current state to produce an updated representation; after several layers, a readout/pooling operation produces graph-level predictions. In atomistic and crystal-property prediction, nodes typically encode atomic descriptors (e.g., element type/embedding, valence-related or periodic-table features), edges encode pairwise geometric information (e.g., interatomic distance expanded in a radial basis, bond type, neighbor shell), and a global state may store system-level metadata (e.g., charge, pressure/temperature, or composition statistics). In abstract terms, a GNN can be represented as shown in Figure 3. 

Figure 3: Convolutional GNN (a) Example of graph structure (b) For each node, a computational graph is constructed, where each layer contains aggregation functions with shared weights across that layer (c) Upscaled vision of сonvolutional GNN. One of the most powerful graph neural network (GNN) architectures for crystals is MEGNet family [20], which has also performed well on this dataset, making it a suitable choice for training on sparse representations. A schematic of the MEGNet architecture is shown in Figure 4, with its defining feature being the update mechanism for the vector representations of graph nodes, edges, and a global representation vector. However, GNNs suffer from a lack of interpretability or face overfitting challenges on small datasets, and most of the neural network (NN) approaches struggle to generalize to data outside the training distribution. In [21], reported gains can reflect overfitting: an out-of-distribution (OOD) benchmark showed that random splits artificially inflate performance due to dataset redundancy and that state-of-the-art GNNs generalize poorly to unseen chemistries/structures. 5Figure 4: An overview of a MEGNet module. The initial graph is represented by the set of atomic attributes V = vi, bond attributes E = {(ek, r k, s k)}, and global state attributes u. In the first update step, the bond attributes are updated. Information flows from atoms that form the bond, the state attributes, and the previous bond attribute to the new bond attributes. Similarly, the second and third steps update the atomic and global state attributes, respectively, by information flow among all three attributes. The final result is a new graph representation. Taken from [20]. Today, a wide range of crystal databases are accessible [22], but due to the diversity of crystal types and target properties, it is often the case that few or no structures are available for specific tasks in open databases—particularly with experimental data. Given the cost and complexity of obtaining experimental data, most databases predominantly contain results from physical simulations. In our example, we also use a synthetic dataset, specifically a new dataset of two-dimensional materials with two types of defects: substitutions and vacancies. This dataset was computed using density functional theory (DFT) [23]. The types and details of the vacancies are listed in Table 1. Material Substitutions Vacancies MoS 2 S → Se; Mo → W Mo; S WSe 2 Se → S; W → Mo W; Se h-BN B → C; N → C B; N GaSe Ga → In; Se → S Ga; Se InSe In → Ga; Se → S In; Se BP P → N PTable 1: Point defect types present in the datasets [23] It has been demonstrated that a neural network with this architecture predicts crystal prop-erties more accurately than others available at the time. However, like other neural network approaches, it lacks interpretability and generalizability. For studying defects in crystals, a sparse representation—constructing a graph of defects as shown in Figure 5—proved more ef-6fective. As shown in [24], this approach significantly improves model accuracy for predicting formation energy, although it does not yield similar improvements for predicting the highest occupied–lowest unoccupied molecular orbital (HOMO-LUMO) gap. 

Figure 5: Left side: pristine MoS 2 crystal. Right side: it’s a sparse representation (defects only). Neural networks often function as a black box; however, scientific inquiry demands a clear and precise understanding of observable physical parameters. Here we want to fill the gap and recover physically constrained expressions for 2D defect properties from a small dataset and demonstrate its generalizability. We therefore seek interpretable, closed-form expressions that (i) encode physically plausible interaction kernels, (ii) generalize across different kinds of structures, and (iii) remain reproducible. The next section formalizes this task and introduces our SEGVAE-based method tailored to these requirements. 

# 3 SEGVAE for 2D materials 

Uncovering the laws governing atomic interactions within a crystal lattice will not only enable the prediction of crystal properties in an interpretable form but will also pave the way for designing crystals with tailored properties. We require an SR method that (i) explicitly encodes physical priors, (ii) controls expression complexity, and (iii) remains reproducible under small, noisy DFT datasets. Among genetic programming GP SR, deep/RL SR (e.g., Deep Symbolic Optimization (DSO)), and LLM-driven SR, we adopt SEGVAE [25] because it combines predicate-guided hypothesis restriction, struc-ture pre-training, a Bank of Best Formulas, and Pareto selection over accuracy vs. complexity. Our method can be viewed within the estimation-of-distribution algorithm (EDA) paradigm for genetic programming: instead of relying solely on hand-crafted variation operators, we iteratively fit a probabilistic model to high-performing expressions and sample new candidates from this model. This perspective connects our work to recent advances in EDA-GP, such as DAE-GP by Wittenberg et al. [26], which uses a denoising autoencoder to model and resample promising programs. While we share the EDA principle, our implementation differs in using a VAE-based generative model with an explicit latent distribution and in incorporating domain knowledge through predicate-based filtering of candidate expressions. SEGVAE is algorithm, based on a variational autoencoder architecture with Long Short-Term Memory (LSTM) [27] as decoder and encoder. Key SEGVAE aspects include: (1) Noise Robustness: Unlike traditional symbolic regression models, SEGVAE demonstrates 7robustness against high noise levels, achieving performance levels comparable to DSO on standard benchmarks. (2) Prior Knowledge Integration: SEGVAE allows users to encode domain-specific knowledge through predicate conditions, improving search efficiency and the relevance of generated expres-sions. This effectively narrows the search space, increasing the likelihood of finding meaningful symbolic representations. (3) Pre-Training on Formula Structures: Pre-training enables the model to generate struc-turally valid formulas, facilitating faster and more accurate optimization for the target data without requiring extensive recalibration. (4) Bank of Best Formulas (BBF): SEGVAE maintains a repository of optimal formulas, refining the variational autoencoder (VAE) over successive training epochs and allowing it to converge on the best-fit expressions. We use a variational autoencoder for sequences in Polish notation (operators precede operands). The encoder and decoder are single-layer LSTMs with 64 hidden units; the latent code is 128-dimensional—a size that performed best in ablations while keeping training stable. We cap expression length at 30 tokens. SEGVAE takes as input a tokenized structure. Tokens represent operators, including basic arithmetic and functions like trigonometric and logarithmic operations, while variables and constants are also incorporated as tokens. These choices follow the original SEGVAE implementation and its ablation guidance, but are fixed here to our materials setting for reproducibility. We used the same token list [’add’, ’sub’, ’mul’, ’div’,’sqrt’,’cos’,’exp’,’pow’] for SEGVAE train process and inference, where ’add’ is ’+’, ’sub’ is ’-’, ’mul’ is ’ ·’, ’div’ is ’/’, ’sqrt’ is ’ √’,’cos’ is kosinus function, ’exp’ is an exponential function, ’pow’ is power operator. Token list is one of the hyperparameters to train SEGVAE; the rest we set by default, as in [25]. Since for scientific data, some prior knowledge on the functional dependency is usually avail-able, the SEGVAE easily applies it to search for formulas. The SEGVAE framework distinguishes itself by incorporating domain knowledge directly through predicates, which act as structural fil-ters within the training process. These predicates constrain the expression search space to reflect anticipated functional forms, aligning formula generation with known physical or functional con-straints. We applied only general constraints on the final formula. In our case, (i) definedness on the working domain, (ii) no NaN/Inf, and (iii) physically reasonable behavior at large sep-arations. These constraints sharply reduce the search space and improve convergence on our task. The training of SEGVAE involves two main steps: pre-training and the main training cycle. In pre-training, the model is exposed to random sequences of tokens to learn valid formula structures. The main training phase involves sampling candidate formulas, filtering them based on defined predicates, and evaluating them for accuracy on a target dataset using mean squared error (MSE). The best-performing formulas are stored in a “Bank of Best Formulas” and used to fine-tune the VAE, gradually focusing the model on generating high-fidelity expressions that fit the target data. After training, SEGVAE generates candidate expressions that fit the target dataset, balancing between formula accuracy and complexity. A complexity metric is calculated for each candidate based on token types, helping to identify expressions that offer simplicity without sacrificing fit quality. SEGVAE outputs a Pareto front of formulas based on mean absolute error (MAE) and complexity available for used to choose the optimal function in terms of explainability and accuracy. Final expressions are selected from a Pareto frontier of accuracy versus complexity, allowing for informed selection based on the user’s interpretability needs. In this work, the final equation is selected using a two-stage criterion: (i) numerical selection: we first identify the simplest expression whose validation MAE is within 5% of the best MAE on the Pareto front 8(ii) expert screening: among near-optimal expressions, we prefer formulas that satisfy additional physics-motivated sanity checks (e.g., definedness on the full domain, absence of NaN/Inf, and physically reasonable asymptotic behavior at large separations). When multiple candidates sat-isfy these criteria, we report the most compact one for readability. This selection principle (accuracy–complexity trade-off with preference for interpretable, physically meaningful forms) is common in physics-oriented symbolic regression; see, e.g., [28], where closed-form relations are chosen with emphasis on small complexity, interpretability, and reflecting underlying physical mechanisms. One single structure of MoS 2 8x8 superlattice contains 192 atoms, thus in total 576 variables. To construct generalizable and interpretable formulas for formation energy per site and HOMO-LUMO gap, we do a similar trick of dimensionality reduction as in the sparse representation paper [24]. As a general formula frame search for formation energy, we construct for pairwise interaction functions as an analogy to Coulomb interaction. In our setting (pairwise defect inter-actions in 2D), SEGVAE focuses the search on physically plausible kernels (decaying/oscillatory components) and improves data efficiency. 

Ef ormation = X

> i∈N

Ei + 12

X

> i,j ∈N

Vi,j (r) (1) We define formation energy per site as in [23]. 

Eper site = Ef ormation 

N (2) Where N is a number of defects in the superlattice. The frame formula for the HOMO-LUMO gap : 

EHOMO-LUMO gap = min  

> i,j ∈N

(Ei,j (r)) (3) We model the HOMO–LUMO gap using a “worst-case” (minimum) aggregation over pairwise contributions. This choice reflects that the global gap in a defective supercell can be governed by the most strongly gap-reducing defect interaction / localized defect state. Thus, we postulate two main assumptions: We use Euclidean distance between the defects and consider interaction as a sum of pair defect interactions. For each defect interaction type, we select a small dataset with two defect structures of interest types. Figure 6 illustrates schematically the interaction formula search for a simple interaction type. First, interested target values were calculated with DFT. The dataset for symbolic regression represents the distance between the defects at all possible positions, and as a target value, formation energy per site and HOMO-LUMO gap. 

Ef ormation = const + V (r) (4) 9Figure 6: The SEGVAE application, architecture, and training scheme. Upper side: MoS 2 with defects example. For readability and interoperability, we report the simplest expression within 5% of the best MAE on the Pareto front; in a few cases, this means not reporting the single best-MAE expres-sion, while keeping the accuracy essentially unchanged. Here is an example of the formation energy per defect site as a function of Mo and S vacancy defects distance found by SEGVAE. 

Eper site (x) = 4 .895 + ( cos (0 .5) · cos (8 − 2x) − 1) · x2 · e−x (5) This formula shows how big the impact of vacancy interactions is on formation energy pre-diction, the graphical form with data points is illustrated in Figure 7(a). While a set of discrete DFT-computed values for defect formation energy as a function of sep-aration provides empirical insights, the discovery of an explicit analytical expression via symbolic 10 regression elevates the result to the level of a physically meaningful hypothesis. The resulting symbolic expression includes a product of an oscillatory term and an exponentially decaying envelope — a structure remind well well-known physical interaction potentials such as the Rud-erman–Kittel–Kasuya–Yosida (RKKY) interaction [29] and Friedel oscillations [30]. The general form of RKKY interaction is: 

H(Rij ) = Ii · Ij

4

|∆kmkm |2m∗

(2 π)3R4 

> ij

ℏ2 [2 kmRij cos(2 kmRij ) − sin(2 kmRij )] , (6) Where H represents the Hamiltonian, R i j is the distance between the nuclei i and j, Ii is the nuclear spin of atom i, ∆kmkm is a matrix element that represents the strength of the hyper-fine interaction, m* is the effective mass of the electrons in the crystal, and km is the Fermi momentum. Although Friedel oscillations are formally defined as spatial modulations in the electron den-sity δρ (r) ∝ cos(2 kF r + ϕ)/r d induced by an impurity or defect, where kF is Fermi wave vector, 

ϕ is phase, r is the distance from the defect. In systems with interacting defects, the Friedel-type modulation of the electronic environment can lead to oscillatory variations in the formation en-ergy as a function of defect separation. These forms are characteristic of quantum interference and screening effects in many-body electron systems. This similarity suggests that the interac-tion between defects in the 2D lattice is not purely electrostatic or local but is mediated through the delocalized electronic structure of the host material. Indeed, in monolayer transition metal dichalcogenides (TMDs) such as MoS 2 and WSe 2, recent experimental and theoretical studies have demonstrated Friedel-like oscillations and long-range oscillatory strain fields around atomic defects [31] [32], supporting the notion that quantum interference plays a role in defect–defect coupling. In Figure 7(b-c) we plot the pairwise contribution to the HOMO–LUMO gap as a function of the defect separation r together with the SEGVAE fit. For the HOMO–LUMO gap we obtain the following pairwise kernel (an example for the same interaction as in Figure 7(b). 

Egap (r) ≈ 0.3601 − 0.99 5r + 10 cos   43 r + 4  + r

(r + 4) 2 + 6(6 − r)2 . (7) As a representative example of the discovered closed-form kernels, for the W substitution and S vacancy defects SEGVAE yields (Figure 7(с)): 

Egap (r) ≈ 1.1611 + 8

r − 10 6−r − e6 + 0.0485 

r . (8) For WSe 2 and other materials we follow the same procedure and repeat it for each interaction type. The different fit quality across Figure 7(a–c) can be explained by both physics and the modeling assumptions. Vacancy–vacancy formation energy typically varies smoothly with sepa-ration and is well captured by a pairwise distance-based interaction kernel; the resulting decay-ing/oscillatory form is consistent with screened long-range electronic responses (RKKY/Friedel-type behavior) discussed above. In contrast, the HOMO–LUMO gap is more sensitive to defect-induced electronic states and their reordering/hybridization, which can lead to regime changes and clustered target values; therefore, a single smooth function of distance may not capture all variations equally well. Finally, substitution–vacancy interactions are affected by additional latent factors not explicitly modeled here (e.g., whether defects reside in the same or different atomic planes, local relaxations, and chemical asymmetry). Since we intentionally restrict de-scriptors to distance and defect types for interpretability, these latent degrees of freedom appear as irreducible scatter, lowering the apparent fit quality in Figure 7(b) and (c). 11 Therefore, the symbolic model derived here does not simply interpolate the data but reveals a functional form aligned with known physical principles, suggesting that symbolic regression can serve as a hypothesis generation mechanism — potentially uncovering new physical laws in systems where the governing interactions are not yet fully understood. 

Figure 7: (a) Energy formation per site dependence on distance between Mo and S vacancies in MoS 2 (b) Band Gap dependence from distance between Mo and S vacancies in MoS 2 (c) Energy formation per site dependence from distance between W substitution and S vacancies in WSe 2.Data presented as circles. Note that panels (a–c) use different y-axis ranges (order-of-magnitude differences). 

# 4 Results and comparison 

Each selected SEGVAE formula is relatively simple and interpretable. For simplicity in this study, we use only distance and atom types to construct the final function. However, accuracy still can be improved, for example in S (or Se) MoS 2 (WSe 2) layer we did not distinguish between the upper or lower layers in the structure, but we know that defects interact differently if they are in one plane or in both. This can be taken into account by adding more interaction classes and separating cases where defects are on the same or different planes. Or in another way, perform additional coefficient optimization inside formulas. In total, for each interaction type, we had up 12 to 20 structures to learn pairwise interaction laws in functional form. Thus, the total number of structures for MoS 2 and WSe 2 was less than 300 structures to learn all interactions presented in the dataset. 

Figure 8: Prediction results for energy formation per site for MoS 2 and WSe 2. Left panels: Low-density data where a number of defects per structure is less or equal to 3. Right panels: Results on high-density defect structures where the number of defects is between 4 and 25. The general formula for formation energy per site obtained by substituting all the interaction laws found in Equation 1 is general for any structure with any type of defects in the data set. Test results on high (more than 3 defects in supercell) and low density (less or equal to 3 defects in supercell) datasets, presented in Figure 8, right and left side correspondingly. Generalization 13 on higher density defects data gives higher MAE, but still better results than all GNNs on full structure representation. Figure 9 presents the results of our approach for HOMO-LUMO gap prediction based on Equation 3. We see that generalizing to data with a higher density of defects, the MAE error increased for both targets, and the same trend is true for GNNs. The increase in error is associated with the increasing influence of many-particle interactions with increasing defect density. The clustered structure of band-gap values in Figure 9 can also reflect distinct electronic regimes associated with different defect species (e.g., vacancies vs substitutions and the chemical identity of the missing/substituting atom). Different defect types introduce defect states at different energies and with different localization, which can shift the gap into separate value ranges. Combined with the ‘min’ aggregation in Equation 3, this naturally leads to a multi-regime target and explains why a single smooth distance-only kernel may underfit band-gap trends compared to formation energies. 14 Figure 9: Prediction results for Band Gap for MoS 2 and WSe 2. Left panels: Low-density data where a number of defects per structure is less or equal to 3. Right panels: Results on high-density defect structures where the number of defects is between 4 and 25. Table 2 summarizes the accuracy of the proposed pairwise-only symbolic model against GNN-based baselines on the same benchmark. For formation energy per site, the symbolic approach achieves errors comparable to MEGNet trained on sparse representations on the low-density subset, while remaining competitive in the high-density regime. For the HOMO–LUMO gap, the symbolic model performs competitively with SchNet, GemNet, and CatBoost across most settings, and is comparable to MEGNet on the full representation for several materials; the largest discrepancy is observed for WSe 2 in the low-density case. Importantly, our symbolic interactions are identified using only a small number of two-defect 15 structures, after which the model is evaluated on low- and high-density defect configurations via aggregation. This highlights the ability of the proposed physically motivated construction to extrapolate from sparse pairwise data to more complex defect environments. Reported values for Symbolic are mean(std) over the 10 benchmark folds, matching the baselines’ evaluation protocol.                                                                                                                                                                                                                                                                                       

> Formation energy per site MAE, meV; lower is better Material Density SchNet GemNet MEGNet CatBoost MEGNet(Sparse) Symbolic
> MoS 2high 321 ±100 535 ±206 136 ±22 136 ±523 ±550 .0±5.8
> WSe 2high 536 ±123 575 ±181 112 ±33 162 ±623 ±474 ±9
> h-BN high 1442 ±68 697 ±315 496 ±229 363 ±17 127 ±16 295 ±38
> GaSe high 245 ±12 230 ±41 107 ±25 103 ±448 ±798 .0±7.3
> InSe high 268 ±19 247 ±26 95 ±27 137 ±535 ±270 ±7
> MoS 2low 65 ±544 ±14 58 ±11 12 .6±0.44±113 .0±0.6
> WSe 2low 85 ±22 42 ±965 ±16 16 .3±0.86±130 .0±1.3
> HOMO – LUMO gap MAE, meV; lower is better Material Density SchNet GemNet MEGNet CatBoost MEGNet(Sparse) Symbolic
> MoS 2high 204 ±121 174 ±111 54 ±471 ±439 ±481 ±10
> WSe 2high 186 ±177 268 ±182 47 ±3106 ±638 ±455 ±6
> h-BN high 244 ±24 227 ±6233 ±4208 ±3260 ±14 240 ±31
> GaSe high 309 ±83 196 ±11 178 ±8173 ±4194 ±11 201 ±18
> InSe high 214 ±69 178 ±22 156 ±7155 ±1167 ±15 170 ±14
> MoS 2low 187 ±180 46 ±42 30 ±226 .7±0.85.7±0.218 .0±0.5
> WSe 2low 236 ±224 64 ±46 32 ±518 .3±0.68.1±0.644 ±2

Table 2: Accuracy comparison on the benchmark for formation energy per site and HOMO–LUMO gap. For our method Symbolic, we report MAE with standard deviation across 10-fold splits of the benchmark (MAE(std)). All baseline results are taken from [24] as reported. Lower is better. 

To probe the impact of residual many-body effects, we additionally consider a lightweight three-defect correction on MoS 2 in the low-density regime. Specifically, we keep the discovered symbolic pairwise expressions fixed and estimate a single additive intercept using the train-ing portion of each fold (closed-form); we then report mean ±std across folds. This post-hoc calibration does not require any additional symbolic searches. A systematic extension of this three-defect construction to all materials and density regimes is left for future work. For structures containing exactly three defects a, b, c , we construct the pairwise-only predic-tion as 

ˆypair = fab (rab ) + fac (rac ) + fbc (rbc ),

where each f (·) is the symbolic interaction identified from two-defect data. We then apply the simplest three-defect correction 

ˆythree-defect = ˆ ypair + c, 

with a single intercept c fitted on the training split of each fold. The test MAE is computed on the corresponding held-out split, and we report mean ±std across the 10 benchmark folds. On this subset, the three-defect correction reduces the formation-energy MAE from 13 .0±0.6

to 4.3 ± 0.9 meV. This brings the error close to the GNN baselines reported for MoS 2 low density in Table 2. 16 5 Conclusion 

Symbolic regression is a fascinating method for deriving functional dependencies, allowing users to construct formulas that describe specific effects with defined physical or semantic constraints. To keep this usable in practice, we contain formula growth via predicate-guided grammars, di-mensional/symmetry checks, and Pareto selection over error vs. complexity, with a final symbolic simplification/constant refitting; and we reduce reliance on user intuition through predefined op-erator libraries and filtering under the same predicates. With well-structured data, symbolic regression can yield interpretable results that outperform many methods, including state-of-the-art graph neural networks, as we demonstrated in our example. By using SEGVAE as a symbolic regressor and breaking down the formula for the dependency of crystal properties on defect po-sitions into pairwise interactions, we achieved MAE metric results that surpass MEGNet and closely approach MEGNet’s performance when applied to data in a sparse representation. Also, our approach can be used for material design tasks by placing defects in certain positions to modify physical properties. This enables an inverse–defect–placement workflow: given a target window for formation energy or the HOMO–LUMO gap, we search defect configurations using the SR surrogate (milliseconds per query). We have demonstrated such a coupling of SR with a genetic optimizer in [33] where we integrated the symbolic method into a genetic algorithm to generate defects in MoS 2.A key advantage of this approach lies in the interpretability of the result and computational speed, which can exceed that of GNNs by orders of magnitude and use less data consumption to learn functional dependencies. However, there are certain limitations. To accommodate new types of defects, one must derive formulas for new interactions, which grow quadratically with the number of defect types. Introducing new materials also requires crafting new formulas, as these are generally unique for each material. Moreover, the formulas themselves are not unique but are shaped by the subjective choices, intuition, and experience of the user. On the other hand, GNNs also require retraining, as they lack generalization capabilities for new materials and defects. Our findings confirm the viability of deep symbolic regression methods for certain tasks in the natural sciences, showing that in some cases, the results can surpass established ML approaches. 

# 6 Acknowledgements 

The authors thank Pengru Huang from the National University of Singapore for running DFT simulations of new structures with two defects. 

# 7 Funding 

The work was supported by the grant for research centers in the field of AI provided by the Ministry of Economic Development of the Russian Federation in accordance with the agreement 000000С313925P4E0002 and the agreement with HSE University № 139-15-2025-009 

# 8 Author contributions 

ML implemented SEGVAE, conducted computational experiments, interpreted the results, and wrote the manuscript. AU supervised the work. All authors contributed to the debate and analysis of the data and approved the final version. 17 9 Data availability 

To fit pairwise interactions, we generated new structures that were not presented in the original dataset. We generated new structures h-BN, GaSe, InSe with all kinds of possible pairs defect configurations (see Table 1) on 8x8 supercell. Structures were relaxed using Vienna Ab initio Simulation Package (VASP) DFT simulation package [34]. The relaxation results are available in DOI:10.5281/zenodo.15806883. 

# Appendix 

Variety of Symbolic regression algorithms is a reflection of the multitude of neural network types available; their adaptation for symbolic regression tasks offers a wide range of algorithmic options, each with its own advantages and limitations. This convergence of approaches is schematically represented in Figure 10. 

Figure 10: ML and GP methods Several symbolic regression approaches have demonstrated outstanding results, with one of the most well-known being the model for predicting physics formulas from Feynman’s physics textbook [35]. This algorithm was later improved, resulting in [36]. In [37] AIFeynman algorithm in combination with an ordinary differential equation solver was used to discover physical laws from video on toy examples. PySR paper [38] presents an approach that combines GNNs with symbolic regression to un-cover explicit physical equations. By embedding strong inductive biases into GNNs and employ-ing symbolic regression on the learned components, they demonstrated the ability to rediscover known physical laws and even derive a new analytic expression in a cosmology application. Un-der the hood, as a symbolic regressor, Eureqa was used. This work showcases how deep learning 18 models, traditionally considered "black boxes," can be made interpretable, enhancing both gen-eralization and the potential for new discoveries in fields ranging from physics to astrophysics. A series of papers [39] and the latest version [40] were dedicated to an MLP-inspired approach. Informed Equation Learner (iEQL) that integrates domain-specific knowledge into symbolic re-gression models. Unlike traditional machine learning models that yield dense, uninterpretable representations, iEQL uses structured building blocks with a custom set of activation functions to derive compact, interpretable equations. This approach includes handling atomic functions with singularities (e.g., logarithm and division) and applying domain-dependent structured sparsity, allowing for more effective scaling to real-world scientific and engineering applications. Through experiments on simulated and real-world datasets, the iEQL demonstrated the ability to learn interpretable models with high predictive accuracy, especially in industrial applications like mod-eling power loss in electric machines and torque in combustion engines. The variety of deep learning algorithms for various types of data spawns different SR ap-proaches. One recent work [41] treats the SR task as an image-to-expression translation problem. Instead of relying solely on searching mathematical spaces, Symbolic Expression Transformer SET represents sampled data as images and uses transformer-based encoding to convert visual patterns into symbolic expressions. SET introduces a large-scale dataset, carefully designed with non-overlapping training and test sets, but limited to two variables. Another advanced deep learning approach to symbolic regression is introduced in [42]. Deep Symbolic Regression (DSR), which uses a reinforcement learning framework to recover exact mathematical expressions from data. Unlike traditional methods that focus on average perfor-mance, DSR applies a risk-seeking policy gradient to optimize for best-case outcomes, allowing it to identify highly accurate, interpretable models. This approach employs an autoregressive recur-rent neural network (RNN) to construct expressions token by token, integrating domain-specific constraints in real time to avoid invalid formulations. By outperforming genetic programming and commercial tools like Eureqa on benchmark tasks, DSR demonstrates the potential of com-bining deep learning with symbolic methods to advance scientific discovery. DSR has recently been updated by introducing a genetic programming component [43] (DSO). Example of utilizing NLP methods in [44], where transformer architecture was implemented, the authors propose to use pre-trained transformers [45] to predict symbolic expression. This approach demonstrates comparable results with DSR, but compared with DSO on common benchmarks, the quality of the results is lower. With the advent of large language models (LLMs), these too have been adapted for symbolic regression tasks. One of the pioneering works in this direction is [46]. Central to SymbolicGPT is its order-invariant embedding method, which uses a T-net [47] to convert datasets of varying sizes into robust representations, enabling the model to generate accurate expressions without relying on fixed data orders. Additionally, the model employs a generative pre-trained transformer to produce equation "skeletons" with constants optimized separately using the BFGS algorithm, streamlining computational load. The latest method [48] was proposed last year, aiming to accelerate formula generation while achieving quality comparable to DSO. This was accomplished by using a GPT architecture to generate tokens, with features extracted using the SetTransformer [49], and a decoder that produces the sequence of the DSR training process. The authors demonstrated that their method could quickly generate formulas, and while its accuracy trails behind DSR and DSO, it outperforms SNIP [50] and NeSymReS [44] in precision. However, a significant drawback of these GPT-based methods is the lack of accessible source code and model weights, preventing result validation and application to real-world problems. An elegant approach the worth mentioning is based on reimagining the MLP structure: Kol-mogorov Arnold Networks (KAN), implemented in [51]. KANs redefine the traditional neural network structure by replacing fixed activation functions on nodes with learnable univariate func-19 tions on edges, modeled as splines. The difference is schematically shown in Figure 11. This architecture reduces the curse of dimensionality by decomposing complex functions into simpler, one-dimensional components, allowing KANs to efficiently scale in high-dimensional settings. Unlike conventional MLPs, KANs are inherently interpretable, with symbolic simplification and regularization enabling clear visualization of learned functions, as shown in Figure 12. Demon-strated in scientific tasks across mathematics and physics, KANs not only achieve high accuracy similar to MLPs but also offer insights into underlying laws and relationships. In symbolic regression, the development of benchmarks that align with real-world scientific discovery is crucial to advancing the field. In [52], address this need with Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery, introducing a new set of symbolic regression for scientific discovery (SRSD) datasets that incorporate physics-inspired formulas with realistic variable sampling ranges and added complexity through dummy variables. This approach reflects real-world scenarios, allowing models to demonstrate robustness and feature selection in noisy environments. To assess accuracy beyond traditional metrics, the authors propose Normalized Edit Distance (NED), which measures structural similarity between pre-dicted and true equations, providing a more nuanced evaluation that correlates well with human judgment. In parallel to the recent success of neural symbolic regression, genetic programming based symbolic regression has also seen noticeable progress in the last 3–5 years. First, several modern GP-SR frameworks were proposed with an emphasis on scalability, modularity, and practical usability. For example, Operon is a high-performance GP framework with efficient expression encoding and parallel offspring generation, and Bingo is a flexible GP-SR framework with swap-pable components (fitness, selection, constant optimization), designed to be easily adapted to different SR tasks [53, 54]. Such toolchains make it easier to reproduce experiments and to compare GP baselines under consistent settings. Second, the GP community went beyond the “classic” subtree crossover/mutation by develop-ing stronger variation and tuning mechanisms. A representative example is GP-GOMEA, where mixing is guided by learned dependencies between parts of expressions; recent work also studied coefficient mutation inside GP-GOMEA to improve constant fitting and rediscovery of the un-derlying equations on SR benchmarks [55]. Another active direction is to guide GP search with semantics and constraints. For example, semantic schema-based GP proposes locality/semantic guidance for a more reliable search in symbolic regression [56]. Moreover, formal constraint handling has been studied in counterexample-driven GP, where candidate expressions are au-tomatically checked against user-defined constraints and violating examples are generated to refine evaluation and steer the search away from infeasible regions [57]. This direction is closely related to our predicate filtering (definedness checks, NaN/Inf rejection, and physically reason-able behavior), although we implement constraints as lightweight predicates compatible with a VAE-guided pipeline. 20 Figure 11: MLP vs KAN. Taken from [51] 

Figure 12: KAN for symbolic regression. Taken from [51] 21 References 

[1] Gabriel R Schleder et al. “From DFT to machine learning: recent approaches to materials science–a review”. In: Journal of Physics: Materials 2.3 (2019), p. 032001. [2] Kui Lin and Zhanlong Wang. “Multiscale mechanics and molecular dynamics simulations of the durability of fiber-reinforced polymer composites”. In: Communications Materials 

4.1 (2023), p. 66. [3] Dominic P Searson, David E Leahy, and Mark J Willis. “GPTIPS: an open source genetic programming toolbox for multigene symbolic regression”. In: Proceedings of the Interna-tional multiconference of engineers and computer scientists . Vol. 1. Citeseer. 2010, pp. 77– 80. [4] Michael Schmidt and Hod Lipson. “Distilling free-form natural laws from experimental data”. In: science 324.5923 (2009), pp. 81–85. [5] Yiqun Wang, Nicholas Wagner, and James M Rondinelli. “Symbolic regression in materials science”. In: MRS Communications 9.3 (2019), pp. 793–805. [6] Filippos Sofos et al. “A combined clustering/symbolic regression framework for fluid prop-erty prediction”. In: Physics of Fluids 34.6 (2022), p. 062004. [7] Todd M Alam et al. “Symbolic regression development of empirical equations for diffusion in Lennard-Jones fluids”. In: The Journal of Chemical Physics 157.1 (2022), p. 014503. [8] Julia Reuter et al. “Graph networks as inductive bias for genetic programming: Symbolic models for particle-laden flows”. In: European Conference on Genetic Programming (Part of EvoStar) . Springer. 2023, pp. 36–51. [9] Bogdan Burlacu et al. “Symbolic Regression in Materials Science: Discovering Interatomic Potentials from Data”. In: arXiv preprint arXiv:2206.06422 (2022). [10] Changxin Wang et al. “Symbolic regression in materials science via dimension-synchronous-computation”. In: Journal of Materials Science & Technology 122 (2022), pp. 77–83. [11] Christian Loftis et al. “Lattice thermal conductivity prediction using symbolic regression and machine learning”. In: The Journal of Physical Chemistry A 125.1 (2020), pp. 435–450. [12] Jiachi Xie and Lei Zhang. “Machine learning and symbolic regression for adsorption of atmospheric molecules on low-dimensional TiO2”. In: Applied Surface Science 597 (2022), p. 153728. [13] Baicheng Weng et al. “Simple descriptor derived from symbolic regression accelerating the discovery of new perovskite catalysts”. In: Nature communications 11.1 (2020), p. 3513. [14] Alberto Hernandez et al. “Fast, accurate, and transferable many-body interatomic poten-tials by symbolic regression”. In: npj Computational Materials 5.1 (2019), p. 112. [15] Zhaosheng Zhang, Yingjie Zhang, and Sijia Liu. “Integrative approach of machine learning and symbolic regression for stability prediction of multicomponent perovskite oxides and high-throughput screening”. In: Computational Materials Science 236 (2024), p. 112889. [16] Eibar Flores et al. “Learning the laws of lithium-ion transport in electrolytes using symbolic regression”. In: Digital Discovery 1.4 (2022), pp. 440–447. [17] Evgeniya Kabliman et al. “Application of symbolic regression for constitutive modeling of plastic deformation”. In: Applications in Engineering Science 6 (2021), p. 100052. [18] Shunning Li et al. “Encoding the atomic structure for machine learning in materials sci-ence”. In: Wiley Interdisciplinary Reviews: Computational Molecular Science 12.1 (2022), e1558. 22 [19] Xuan Zhang et al. “Artificial intelligence for science in quantum, atomistic, and continuum systems”. In: arXiv preprint arXiv:2307.08423 (2023). [20] Chi Chen et al. “Graph networks as a universal machine learning framework for molecules and crystals”. In: Chemistry of Materials 31.9 (2019), pp. 3564–3572. [21] Sadman Sadeed Omee et al. “Structure-based out-of-distribution (OOD) materials property prediction: a benchmark study”. In: npj Computational Materials 10.1 (2024), p. 144. [22] Abdalaziz Rashid et al. “Review on automated 2D material design”. In: 2D Materials 

(2024). [23] P Huang et al. “Unveiling the complex structure-property correlation of defects in 2D materials based on high throughput datasets. npj 2D Mater”. In: Appl . Vol. 7. 1. 2023, pp. 1–10. [24] Nikita Kazeev et al. “Sparse representation for machine learning the properties of defects in 2D materials”. In: npj Computational Materials 9.1 (2023), p. 113. [25] Sergei Popov et al. “Symbolic expression generation via variational auto-encoder”. In: PeerJ Computer Science 9 (2023), e1241. [26] David Wittenberg, Franz Rothlauf, and Christian Gagn´ e. “Denoising autoencoder genetic programming: strategies to control exploration and exploitation in search”. In: Genetic Programming and Evolvable Machines 24.2 (2023), p. 17. [27] S Hochreiter. “Long Short-term Memory”. In: Neural Computation MIT-Press (1997). [28] Dimitrios Angelis, Filippos Sofos, and Theodoros E Karakasidis. “Reassessing the transport properties of fluids: A symbolic regression approach”. In: Physical Review E 109.1 (2024), p. 015105. [29] Melvin A Ruderman and Charles Kittel. “Indirect exchange coupling of nuclear magnetic moments by conduction electrons”. In: Physical Review 96.1 (1954), p. 99. [30] J Friedel. “XIV. The distribution of electrons round impurities in monovalent metals”. In: 

The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 43.337 (1952), pp. 153–189. [31] Chia-Hao Lee et al. “Deep learning enabled strain mapping of single-atom defects in two-dimensional transition metal dichalcogenides with sub-picometer precision”. In: Nano let-ters 20.5 (2020), pp. 3369–3377. [32] Stephen R Power and Mauro S Ferreira. “Indirect exchange and Ruderman–Kittel–Kasuya– Yosida (RKKY) interactions in magnetically-doped graphene”. In: Crystals 3.1 (2013), pp. 49–78. [33] HE Karlinski and MV Lazarev. “Prediction of Defect Structure in MoS by Given Proper-ties”. In: Moscow University Physics Bulletin 79.Suppl 2 (2024), S866–S871. [34] Georg Kresse and J¨ urgen Furthm¨ uller. “Efficient iterative schemes for ab initio total-energy calculations using a plane-wave basis set”. In: Physical review B 54.16 (1996), p. 11169. [35] Silviu-Marian Udrescu and Max Tegmark. “AI Feynman: A physics-inspired method for symbolic regression”. In: Science Advances 6.16 (2020). doi : 10.1126/sciadv.aay2631 .eprint: https://advances.sciencemag.org/content/6/16/eaay2631.full.pdf . url :

https://advances.sciencemag.org/content/6/16/eaay2631 .[36] Silviu-Marian Udrescu et al. “AI Feynman 2.0: Pareto-optimal symbolic regression ex-ploiting graph modularity”. In: Advances in Neural Information Processing Systems 33 pre-proceedings (NeurIPS 2020) . Dec. 2020. 23 [37] Silviu-Marian Udrescu and Max Tegmark. “Symbolic pregression: Discovering physical laws from distorted video”. In: Physical Review E 103.4 (2021), p. 043307. [38] Miles D. Cranmer et al. “Discovering Symbolic Models from Deep Learning with Inductive Biases”. In: CoRR abs/2006.11287 (2020). arXiv: 2006.11287 . url : https://arxiv.org/ abs/2006.11287 .[39] Georg Martius and Christoph H. Lampert. “Extrapolation and learning equations”. In: 

CoRR abs/1610.02995 (2016). arXiv: 1610.02995 . url : http://arxiv.org/abs/1610. 02995 .[40] Subham Sahoo, Christoph Lampert, and Georg Martius. “Learning Equations for Extrap-olation and Control”. In: Proceedings of the 35th International Conference on Machine Learning . Ed. by Jennifer Dy and Andreas Krause. Vol. 80. Proceedings of Machine Learn-ing Research. PMLR, Oct. 2018, pp. 4442–4450. url : https://proceedings.mlr.press/ v80/sahoo18a.html .[41] Jiachen Li, Ye Yuan, and Hong-Bin Shen. “Symbolic expression transformer: A computer vision approach for symbolic regression”. In: arXiv preprint arXiv:2205.11798 (2022). [42] Brenden K Petersen et al. “Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients”. In: International Conference on Learning Rep-resentations . 2021. url : https://openreview.net/forum?id=m5Qsh0kBQG .[43] T Nathan Mundhenk et al. “Symbolic regression via neural-guided genetic programming population seeding”. In: arXiv preprint arXiv:2111.00053 (2021). [44] Luca Biggio et al. “Neural symbolic regression that scales”. In: International Conference on Machine Learning . PMLR. 2021, pp. 936–945. [45] Ashish Vaswani et al. “Attention is All you Need”. In: Advances in Neural Information Pro-cessing Systems . Ed. by I. Guyon et al. Vol. 30. Curran Associates, Inc., 2017. url : https: //proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .[46] Mojtaba Valipour et al. “Symbolicgpt: A generative transformer model for symbolic re-gression”. In: arXiv preprint arXiv:2106.14131 (2021). [47] Charles R Qi et al. “Pointnet: Deep learning on point sets for 3d classification and segmen-tation”. In: Proceedings of the IEEE conference on computer vision and pattern recognition .2017, pp. 652–660. [48] Yanjie Li et al. “Generative Pre-Trained Transformer for Symbolic Regression Base In-Context Reinforcement Learning”. In: arXiv preprint arXiv:2404.06330 (2024). [49] Juho Lee et al. “Set transformer: A framework for attention-based permutation-invariant neural networks”. In: International conference on machine learning . PMLR. 2019, pp. 3744– 3753. [50] Kazem Meidani et al. “Snip: Bridging mathematical symbolic and numeric realms with unified pre-training”. In: arXiv preprint arXiv:2310.02227 (2023). [51] Ziming Liu et al. “Kan: Kolmogorov-arnold networks”. In: arXiv preprint arXiv:2404.19756 

(2024). [52] Yoshitomo Matsubara et al. “Rethinking symbolic regression datasets and benchmarks for scientific discovery”. In: arXiv preprint arXiv:2206.10540 (2022). [53] Bogdan Burlacu, Gabriel Kronberger, and Michael Kommenda. “Operon C++ an efficient genetic programming framework for symbolic regression”. In: Proceedings of the 2020 ge-netic and evolutionary computation conference companion . 2020, pp. 1562–1570. 24 [54] David L Randall et al. “Bingo: a customizable framework for symbolic regression with ge-netic programming”. In: Proceedings of the genetic and evolutionary computation conference companion . 2022, pp. 2282–2288. [55] Marco Virgolin and Peter AN Bosman. “Coefficient mutation in the gene-pool optimal mixing evolutionary algorithm for symbolic regression”. In: Proceedings of the Genetic and Evolutionary Computation Conference Companion . 2022, pp. 2289–2297. [56] Zahra Zojaji, Mohammad Mehdi Ebadzadeh, and Hamid Nasiri. “Semantic schema based genetic programming for symbolic regression”. In: Applied Soft Computing 122 (2022), p. 108825. [57] Iwo B la˛dek and Krzysztof Krawiec. “Counterexample-driven genetic programming for sym-bolic regression with formal constraints”. In: IEEE Transactions on Evolutionary Compu-tation 27.5 (2022), pp. 1327–1339. 25