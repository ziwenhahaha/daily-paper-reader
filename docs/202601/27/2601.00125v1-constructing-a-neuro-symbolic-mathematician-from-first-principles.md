# Constructing a Neuro-Symbolic Mathematician from First Principles
# 从第一性原理构建神经符号数学家

**Authors**: Keqin Xie \
**Date**: 2025-12-31 \
**PDF**: https://arxiv.org/pdf/2601.00125v1 \
**Tags**: <span class="tag-label tag-green">SR</span> \
**Score**: 7.0 \
**Evidence**: 演化证明搜索与符号推理内核 \
**TLDR**: 一种利用符号推理内核和演化搜索进行数学推导的神经符号架构。

---

## 速览
**TLDR**：提出 Mathesis 架构，通过将数学推理转化为超图上的能量最小化过程，构建具备公理化逻辑能力的神经符号数学家。 \
**Motivation**：针对大语言模型因缺乏内部公理化框架而在复杂逻辑推理中表现出的持续性失败问题。 \
**Method**：采用高阶超图表示数学状态，并利用可微逻辑引擎将逻辑约束映射为能量函数，结合蒙特卡洛树搜索进行证明搜索。 \
**Result**：通过梯度信号训练超图 Transformer，成功将多步演绎推理转化为受语义统一引导的能量最小化搜索过程。 \
**Conclusion**：该研究证明了结合符号逻辑与神经网络的神经符号架构能有效提升机器的数学推理与证明能力。

---


## 摘要
由于缺乏内部公理化框架，大语言模型（LLMs）在复杂推理中表现出持续的逻辑失败。我们提出了 Mathesis，这是一种神经符号架构，它将数学状态编码为高阶超图，并使用符号推理内核（SRK）——一种将约束映射到连续能量景观的可微逻辑引擎。通过定义全局能量函数 E(G)（其中零能量意味着逻辑一致性），SRK 产生基于梯度的信号来训练超图 Transformer 大脑，从而将证明搜索转化为能量最小化。通过蒙特卡洛树搜索和演化证明搜索实现多步演绎，并由学习到的价值函数和语义统一进行引导。
## Abstract
Large Language Models (LLMs) exhibit persistent logical failures in complex reasoning due to the lack of an internal axiomatic framework. We propose Mathesis, a neuro-symbolic architecture that encodes mathematical states as higher-order hypergraphs and uses a Symbolic Reasoning Kernel (SRK)--a differentiable logic engine that maps constraints to a continuous energy landscape. By defining a global energy function E(G), where zero energy implies logical consistency, the SRK yields gradient-based signals to train a Hypergraph Transformer Brain, turning proof search into energy minimization. Multi-step deduction is enabled via Monte Carlo Tree Search and Evolutionary Proof Search, guided by learned value functions and semantic unification.