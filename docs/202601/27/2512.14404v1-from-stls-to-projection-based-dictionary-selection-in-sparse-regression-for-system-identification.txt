Title: From STLS to Projection-based Dictionary Selection in Sparse Regression for System Identification

URL Source: https://arxiv.org/pdf/2512.14404v1

Published Time: Wed, 17 Dec 2025 02:04:52 GMT

Number of Pages: 32

Markdown Content:
# From STLS to Projection-based Dictionary Selection in Sparse Regression for System Identification âˆ—

## Hangjun Cho â€  Fabio V.G. Amaral â€¡ Andrei A. Klishin Â§

## Cassio M. Oishi â€¡ Steven L. Brunton â€ 

## December 17, 2025 

Abstract 

In this work, we revisit dictionary-based sparse regression, in particular, Sequential Threshold Least Squares (STLS), and propose a score-guided library selection to provide practical guidance for data-driven modeling, with emphasis on SINDy-type algorithms. STLS is an algorithm to solve the â„“0 sparse least-squares problem, which relies on splitting to efficiently solve the least-squares portion while handling the sparse term via proximal methods. It produces coefficient vectors whose components depend on both the projected reconstruction errors, here referred to as the scores, and the mutual coherence of dictionary terms. The first contribution of this work is a theoretical analysis of the score and dictionary-selection strategy. This could be understood in both the original and weak SINDy regime. Second, numerical experiments on ordinary and partial differential equations highlight the effectiveness of score-based screening, improving both accuracy and interpretability in dynamical system identification. These results suggest that integrating score-guided methods to refine the dictionary more accurately may help SINDy users in some cases to enhance their robustness for data-driven discovery of governing equations. 

MSCcodes 37M10, 62J99, 65L09, 93B30 

keywords SINDy, STLS, Hard thresholding, Sparse Regression, System Identification, Equation Discovery, Backward Variable Selection, Sum of Squares 

# 1 Introduction 

System identification from time-series data remains a longstanding and important challenge. Least squares problems have appeared in linear modeling [ 1], especially in signal processing [ 2] and statistics [ 3]. In this context, several seminal works have leveraged sparse regression for reconstruction, leading to the development of the method of compressed sensing [ 4, 5, 6] and sparse optimization [ 7]. Recently, data-driven approaches â€“ particularly those based on symbolic regression â€“ have received significant attention [ 8, 9]. Within the framework of compressed sensing which typically consider underdetermined systems [ 4, 5], several system identification methods have been proposed [10, 11]. In the context of sparse optimization for overdetermined systems, Sparse Identification of Nonlinear Dynamics (SINDy), a dictionary-based sparse regression method, has demonstrated numerical performance, particularly in applications to fluid 

> âˆ—

Submitted to the editors DATE. 

Funding: The work by H. Cho was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (RS-2023-00253171), C. Oishi was supported by National Council for Scientific and Technological Development (CNPq), grants 305383/2019-1 and 307228/2023-1, F. Amaral and C. Oishi were supported by the SÃ£o Paulo Research Foundation (FAPESP) process numbers 2013/07375-0, 2021/13833-7, 2023/06035-2, 2021/07034-4. The authors acknowledge support from the National Science Foundation AI Institute in Dynamic Systems (grant number 2112085). 

> â€ 

AI Institute in Dynamic Systems, Department of Mechanical Engineering, University of Washington, Seattle, WA 98195, United States (cho.1363@osu.edu, sbrunton@uw.edu) 

> â€¡

Departamento de MatemÃ¡tica e ComputaÃ§Ã£o, Faculdade de CiÃªncias e Tecnologia, Universidade Estadual Paulista â€œJÃºlio de Mesquita Filhoâ€, Presidente Prudente, Brazil (fabio.amaral@unesp.br, cassio.oishi@unesp.br) 

> Â§

Department of Mechanical Engineering, University of Hawaiâ€˜i at MÂ¯ anoa, Honolulu 96822, United States (ak-lishin@hawaii.edu) 

1

> arXiv:2512.14404v1 [stat.ML] 16 Dec 2025

2. Search sub-dictionary sequence with minimal score 

at each sparsity level 

Â·x = Ïƒ(y âˆ’ x)

Â·y = Ïx âˆ’ xz âˆ’ y

Â·z = xy âˆ’ Î²z

Lorenz system 

1. Data 

generation   

> {(x(ti),y(ti),z(ti))}N
> i=1

3. Pattern observation 

and fi ltering sub-dictionary                                                                

> e.g.Dsub=
> |||
> ğ’Ÿ
> xğ’Ÿ yâƒ—zx
> |||
> Dâˆ–Dsub=
> |||||||
> ğ’Ÿ
> 1ğ’Ÿ zğ’Ÿ x2ğ’Ÿ y2ğ’Ÿ z2âƒ—xyâƒ—yz
> |||||||
> D=
> 1x(t1)y(t1)z(t1)x2(t1)y2(t1)z2(t1)xy(t1)yz(t1)zx(t1)
> 1x(t2)y(t2)z(t2)x2(t2)y2(t2)z2(t2)xy(t2)yz(t2)zx(t2)âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
> 1x(tN)y(tN)z(tN)x2(tN)y2(tN)z2(tN)xy(tN)yz(tN)zx(tN)

Score (Dsub; D, Â·y) = âˆ¥(ğ’« D âˆ’ ğ’« Dâˆ–Dsub) Â·yâˆ¥

âˆ¥ Â·yâˆ¥

Time derivative and dictionary Â·y D

ğ’Ÿ 

1 ğ’Ÿ x ğ’Ÿ y ğ’Ÿ z ğ’Ÿ x2 ğ’Ÿ y2 ğ’Ÿ z2âƒ— xyâƒ— yzâƒ— zx 

> Level 2:

a2 = Score ([ ğ’Ÿ z2 | ğ’Ÿ x2]) < Score ([ ğ’Ÿ z2 | ğ’Ÿ 1]), Score ([ ğ’Ÿ z2 | ğ’Ÿ x]), â€¦, Score ([ ğ’Ÿ z2 |âƒ— zx])âƒ— a2 /a1 Dfiltered = [ ğ’Ÿ x ğ’Ÿ yâƒ— zx]âƒ— 

> Level 1:

a1 = Score ([ ğ’Ÿ z2]) < Score ([ ğ’Ÿ 1]), Score ([ ğ’Ÿ x]), ğ’œ , Score ([âƒ— zx]) 

> Level 3:

a3 = Score ([ ğ’Ÿ z2 | ğ’Ÿ x2 |âƒ— zy]) < Score ([ ğ’Ÿ z2 | ğ’Ÿ x2 | ğ’Ÿ 1]), â€¦, Score ([ ğ’Ÿ z2 | ğ’Ÿ x2 |âƒ— zx])

a3/a2Figure 1: A schematic of the scoring procedure and its data pattern over a dataset generated by the Lorenz system, which includes the time history of the states (x, y, z ). At each stage, we compute the score of sub-dictionaries and select the one with the smallest score, starting with single items, then this procedure is repeated recursively with the inclusion of the previously selected items. Here, PA is the projection map to the column space of a given matrix A. A sharp increase in minimal scores at 8th step indicates that the set of the removed items may contain an important item. We filter out the first seven terms from the dictionary. A regular regression with the reduced dictionary yields the final model for Ë™y. The parameters are Ïƒ = 10 , Ï = 26 , Î² = 2 .66667 , and the initial state is (x(0) , y (0) , z (0)) = ( âˆ’8, 8, 27) .flows [ 12 ], and is supported by convergence theory [ 13 ]. While Least Absolute Shrinkage and Selection Operator (LASSO) offers an approach to system identification, it results that errors occur early on the LASSO path [ 14 ], leading to false discoveries. Sequential Threshold Least Squares (STLS), the algorithm used in the papers [ 12 , 13 ], serves as an effective alternative to LASSO with improved performance [ 15 ]and has become a common choice for SINDy implementations. One challenge in dictionary-based learning is building or pruning the library, particularly for terms with small coefficients. Indeed, STLS may eliminate functionally important terms because their coefficients are small. For example, in a system undergoing a Hopf bifurcation, even a small coefficient may have a critical impact on the stability. Also, STLS focuses only on the argmin , that is, on finding the coefficients that minimize the reconstruction error after thresholding. It does not address how these minimal errors relate to observable patterns in the sequence of scores across sparsity levels. This implies that selection of the threshold is crucial to the result, and users typically select it through trial and error. In this paper, we present a useful scoring-based tool for helping SINDy users. Through this tool, users may observe how low projected model reconstruction error can get for a given dictionary, indicating how well the dictionary explains the signal. Thus, this tool may be a pruning method to refine the dictionary more accurately. Moreover, our tool may prevent STLS from thresholding terms with small coefficients. Indeed, we demonstrate the power and limitations of our tool theoretically and numerically. A schematic overview is in Figure 1. One may observe clear empirical data patterns that help to determine an optimal number â€“ an optimal sparsity level â€“ of dictionary items. 21.1 Related work 

## 1.1 Related work 

In statistics, the square of the score we will use is called a sum of squares [ 16 ]. Those quantities are generalized F-values and are used for model selection (See Section 3.1). Including those, the pruning variables problem has been extensively studied in the statistics literature, particularly in the context of overcomplete dictionaries. Our use case, however, is distinct. We consider a regime that is only mildly overcomplete and arises in a different context, where nonlinear correlations exist among the dictionary columns. These features make the problem different from the classical setting. Our regressor resembles the Stepwise Sparse Regressor (SSR) [ 17 ] with different criteria for eliminating one item at each iteration. In their paper, the cross-validation score, which is the average reconstruction error obtained through k-fold cross-validation, was used to select the optimal sparsity level automatically (See Section 2.4). Likewise, we observe the data pattern of scores, which represent errors in fully projected signals, and may set a sparsity level. Beyond these, we also highlight recent developments in SINDy and related frameworks. On recent progress in general, we refer to [ 18 , 19 , 20 ]. Here we only review several related papers among lots of great developments in literature. In Ref. [ 21 ], the authors replaced thresholding based on on the coefficient magnitude with l0 sparsity penalized Bayesian inference it order to investigate how the identified models depend on sparsity, amount of data, and noise. A key observation was that the results produced by SINDy depend significantly on the choice of sparsity penalty and the identified models change discontinuously by adding or removing terms. To explain this phenomenon, the authors employed tools from statistical mechanics, introducing a notion of free energy associated with dictionary items. Based on this perspective, they proposed an alternative method called Z-SINDy. The SINDy variants incorporate statistical tools for uncertainty quantification and model selection. Ensemble-SINDy (E-SINDy) leverages bagging and bootstrap techniques to improve robustness to noise [22 ] with theoretical foundations [ 23 ]. k-fold cross-validation has also been used to do automatic dictionary selection [ 17 ], and Bayesian-SINDy has been developed within a Bayesian framework [ 24 ]. Model selection in SINDy can be formalized with criteria based on information theory [ 25 ]. Additional robustness on noisy data can be achieved by Simultaneous Identification and Denoising of Dynamical Systems (SIDDS) that introduces an auxiliary variable in the Least Squares Operator Inference (LSOI) problem [ 26 ], and Derivative-based SINDy (D-SINDy) [27] that uses a projection-based denoising approach. Since we solve linear regression, some optimization strategies were used in Sparse Relaxed Regularized Regression (SR3) [ 15 , 28 ] and the Conditional gradient-based approach CINDy, which adopts an integral formulation of LASSO [ 29 ]. Adam-SINDy employs Adam optimization for dictionary refinement [ 30 ]. Efforts to refine dictionary selection include SINDy with sensitivity analysis (SINDy-SA), which compares reconstructed coefficient vectors from least squares and ridge regression to select dictionary items [31]. Extensions to parametric systems have also been developed [ 32 ], along with frameworks for systems with control inputs [ 33 ]. Further methodological variations include SINDy-PI, which identifies implicit differential equations [ 34 ], and SINDy-BVP, designed to handle boundary value problems [ 35 ]. Stochastic diffusions have also been considered on stochastic differential equations [ 36 , 37 ]. SINDy has been extended to discrete-time mappings such as the PoincarÃ© map [ 38 ], and to Reinforcement Learning contexts through SINDy-RL [ 39 ]. Beyond regression-based improvements, SINDy has also been integrated with neural architectures, which detect intrinsic coordinates within an autoencoder framework [40]. 

## 1.2 Contributions and Paper Structure 

Our main contributions are the following: (i) Based on the score, we propose dictionary selection algorithms: exhaustive version (ESR) and greedy one (GBSR) . We also provide a theoretical comparison between STLS and the score-based dictionary selection method. (ii) To improve the results and performance, we apply the weak formulation, as described in Section 3.3. (iii) We implement noise robustness tests, a PDE example, and an unsupervised dataset to illustrate the advantages and limitations of our method, especially in cases where STLS fails to detect small 3coefficients. 

Paper Structure. This paper is organized as follows. Section 2 introduces notation and briefly reviews related methodologies. Section 3 presents our proposed score-based method, derived from the coefficient formula of STLS. Section 4 provides a theoretical result, which gives a detailed relationship between scoring and STLS. In Section 5, several numerical simulations show both the strengths and limitations of the proposed scoring methods. Finally, Section 6 summarizes our work and outlines future research directions. For readability, all proofs are collected in Appendix C. Code for all of these examples can be found at https://github.com/fabioamaral08/libselection .

# 2 Background 

In this section, we first summarize several notation to be used throughout the paper, and then review STLS, Pareto curves, Stepwise Sparse Regressor, Weak-SINDy and PDE-FIND. 

## 2.1 Notation 

We summarize the notation used throughout the paper, beginning with notation related to the dictionary. Let d1, d 2, . . . , d n be given dictionary functions (often called variables in statistics or dictionary elements in signal processing). We denote the unordered set of these items by D = {d1, d 2, . . . , d n}, and the ordered tuple by âƒ— D = ( d1, d 2, . . . , d n). Given a time sequence (tj )jâˆˆ[m], we define the evaluation vector corresponding to the function di as di = di(t1) Â· Â· Â· di(tm)T. We then construct the data matrix 

D âˆˆ RmÃ—n whose ith column is di, i.e., D = d1 Â· Â· Â· dn

.We now introduce general notation used throughout the paper. For N âˆˆ N, define [N ] := {1, . . . , N }

and [N ]0 := {0} âˆª [N ]. For a finite set S, |S| denotes its cardinality. For a vector x = ( x1, . . . , xd) âˆˆ Rd,

â„“p (p â‰¥ 0) is defined as: 

âˆ¥xâˆ¥0 := |{ i âˆˆ [d] : xiÌ¸ = 0 }| , âˆ¥xâˆ¥p := 

> d

X

> i=1

|xi|p, p > 0.

The support of a vector x âˆˆ Rd is defined as supp (x) = {i âˆˆ [d] : xiÌ¸ = 0 }. A vector is called k-sparse when 

| supp( x)| â‰¤ k.For a matrix A = [ a1| Â· Â· Â· | am] âˆˆ RnÃ—m, let Aâ€  denote its Moore-Penrose pseudoinverse. The notation 

[a1| Â· Â· Â· | Ë†ai| Â· Â· Â· | am] denotes the matrix obtained by omitting the ith column. For an index subset S âŠ‚ [m] with |S| = s, a matrix A âˆˆ RnÃ—m, AS denotes the submatrix consisting of columns of A indexed by S. Similarly, for x âˆˆ Rm, xS âˆˆ Rs denotes the subvector containing the components indexed by S.1

In Rd, we use âŸ¨Â· , Â·âŸ© and âˆ¥ Â· âˆ¥ to denote the standard inner product and Euclidean norm, respectively. That is, for x = ( xi), y = ( yi) âˆˆ Rd, âŸ¨x, yâŸ© = Pdi=1 xiyi and âˆ¥xâˆ¥ = pâŸ¨x, xâŸ© = âˆ¥xâˆ¥2. For a matrix 

A âˆˆ RnÃ—m, its operator norm âˆ¥Aâˆ¥op is defined as âˆ¥Aâˆ¥op = sup {âˆ¥ Ax âˆ¥/âˆ¥xâˆ¥ : x âˆˆ Rm, âˆ¥xâˆ¥ = 1 }.For a set S of functions, span (S) denotes the linear span of the elements of S. For a matrix A, span (A)

denotes its column space. The orthogonal projection onto a function space S is denoted by PS . For a matrix A, PA denotes the orthogonal projection onto its column space, and for a nonzero vector a, Pa

denotes the projection onto a.

## 2.2 Sequential Threshold Least Square 

For a given time series dataset x = [ x(ti)] 1â‰¤iâ‰¤m âˆˆ Rm, we approximate an empirical time derivative as y â‰ˆ [ Ë™x(ti)] 1â‰¤iâ‰¤m âˆˆ Rm, typically generated by a finite time difference methods. We prepare a set of dictionary functions {d1, . . . , d n}, and consider the associated linear problem Dc = y where y âˆˆ Rm

and D âˆˆ RmÃ—n with m > n . We also assume the dictionary matrix, D = [ d1| Â· Â· Â· | dn], has full-rank. The 

> 1For these notations, we refer to [13].

42.2 Sequential Threshold Least Square 

algorithm STLS compares a given threshold Î» > 0 and coefficients of the projection vector of y to the column space of D.

â€¢ (STLS) The algorithm proceeds through the following iterative scheme with a thresholding parameter 

Î» > 0 [13]: 

Î¾0 = Dâ€ y, Si = {j âˆˆ [N ] : |Î¾ij | â‰¥ Î»}, i = 0 , 1, . . . , Î¾i+1 = argmin 

> supp( Î¾)âŠ‚Si

âˆ¥DÎ¾ âˆ’ yâˆ¥2.

(2.1) STLS generates a sequence of coefficients (Î¾k) such that the cost function Î¾ 7 â†’ âˆ¥ y âˆ’ DÎ¾ kâˆ¥22 + Î»2âˆ¥Î¾kâˆ¥0

decreases with respect to k. Moreover, the sequence converges to a local minimizer of the cost function [13]. We now recall one basic lemma: 

Lemma 1. For each i = 1 , . . . , n ,

|[Dâ€ y]i| = âˆ¥(PD âˆ’ P D[n]\{ i} )yâˆ¥2/âˆ¥di âˆ’ P D[n]\{ i} diâˆ¥2.

Remark 1. In the first iteration of STLS with the threshold Î», indices i satisfying the following will vanish: 

âˆ¥(PD âˆ’ P D[n]\{ i} )yâˆ¥2 < Î» âˆ¥di âˆ’ P D[n]\{ i} diâˆ¥2. (2.2) Remark 1 gives us an intuitive explanation of STLS. Let an one-dimensional finite time sequence x be given and let y = f (x) + g(x) for some linearly independent function f and g. If we have the dictionary matrix D = D(x) = [ f (x)|g(x)|h(x)] for some h, which linearly independent to f and g, and this matrix has full-rank, it is natural to expect the following result 

min  

> ÎâˆˆR3

âˆ¥y âˆ’ D Â· Îâˆ¥ =

ï£®ï£°110

ï£¹ï£» .

And also, we expect STLS with an appropriate threshold yields the same answer. Indeed, it does since 

P[f (x)|g(x)|h(x)] y = P[f (x)|g(x)] y = f (x) + g(x) and (2.2) . In general cases with a dictionary matrix 

D = [ f1(x)| Â· Â· Â· | fn(x)] , if y âˆˆ span (D[n]\{ j0}), [Dâ€ y]j0 = 0 < Î» for any Î» > 0 and the index j0 will not survive in the first iteration of STLS. In these examples, the quantities of the form âˆ¥(PD âˆ’ P D[n]\{ i} )yâˆ¥ in (2.2) play important role. We refer to these as scores and provide their definition in Section 3.1. One may wonder the relationship between the model reconstruction error to these quantities, which are distinguishable when the dictionary doesnâ€™t fully explain the signal: 

âˆ¥y âˆ’ P D[n]\{ i} yâˆ¥22 âˆ’ âˆ¥P D[n]\{ i} y âˆ’ P D yâˆ¥22 = âˆ¥y âˆ’ P D yâˆ¥22 > 0. (2.3) When analyzing the STLS algorithm, we focus only on the first step because the same pattern emerges regardless of which dictionary terms are eliminated - whether one or multiple. This implies that at each iteration, the algorithm repeatedly faces the same problem of solving a least squares system using the pseudoinverse, as described in the following lemma: 

Lemma 2 ([41] Lemma 3.4) . For an index set S âŠ‚ [n], if 

Î¾ = argmin 

> zâˆˆC
> supp( z)âŠ‚S

âˆ¥y âˆ’ Dz âˆ¥2,

then 

(Dâˆ—(y âˆ’ DÎ¾ )) S = 0.

Thus, Î¾ = Dâ€  

> S

y.

52.3 Pareto curve 

## 2.3 Pareto curve 

A Pareto curve is a graph of one-norm Ï„ versus corresponding residual for solution to LASSO [ 7] with parameter Ï„ [41]. Precisely, this is represented by the function Ï• : Râ‰¥0 â†’ Râ‰¥0 defined as: 

Ï•(Ï„ ) = âˆ¥y âˆ’ Dc Ï„ âˆ¥2, cÏ„ = argmin 

> x
> âˆ¥xâˆ¥1â‰¤Ï„

âˆ¥y âˆ’ Dx âˆ¥2, Ï„ â‰¥ 0.

This curve is convex and decreasing, and so connects a basis pursuit denoise problem to a LASSO problem [42]. In our setting, we consider an â„“0-analogue, which is discrete rather than continuous, obtained by recording the residual relative to the projected target vector at each sparsity level. 

## 2.4 Stepwise Sparse Regressor 

The Stepwise Sparse Regressor (SSR) iteratively removes one dictionary element at each iteration and employs cross validation to determine the optimal sparsity level [ 17 ]. Without the cross validation part, the algorithm is as follows: 

Î¾0 = Dâ€ y, S0 = âˆ…,Si+1 = Si âˆª  argmin 

> jâˆˆ[n]\Si

|Î¾ij | , i = 0 , 1, . . . , Î¾i+1 = argmin 

> supp( Î¾)âŠ‚Si+1

âˆ¥DÎ¾ âˆ’ yâˆ¥2.

(2.4) One typically observes a data pattern on the cross-validation scores, with a drastic increase at the optimal sparsity level. SSR has been shown to be effective in pruning unimportant dictionary elements from a large dictionary when applied to (stochastic) ordinary differential equations. As in [17], we denote the coefficient vector by Î¾i =: SSR( D, Î¾ )i. The definition of the cross-validation score is provided in Section 5, where it is compared with our proposed score. 

## 2.5 Weak-SINDy 

Weak formulations of dynamical systems makes a Galerkin-based model selection method [ 43 , 44 , 45 , 46 ]. Let a time-series dataset {Ui}0â‰¤iâ‰¤n with a time sequence {ti}0â‰¤iâ‰¤n be given. We assume that the dataset satisfies Ui = x(ti) for a solution u of the ODE Ë™x = f (x(t)) where f is a continuously smooth vector field. If Ï• is a real-valued test function which has a (connected) compact support, denoting a (time) interval 

(a, b ), in the real-line. By the integration by parts, we have an equality: 

Z ba

 Ï•â€²(t)x(t) + Ï•(t)f (x(t)) dt = 0 .

This equality gives us a different discretization to identify a system. For a time sequence (ti) such that 

t0 â‰¤ a < b â‰¤ tn, the integral equity implies 

> n

X

> i=0

Ï•â€²(ti)x(ti) + Ï•(ti)f (x(ti)) â‰ˆ 0.

Now if we have multiple test functions Ï•1, . . . , Ï• k and f is of the form Dx with a dictionary matrix D and a coefficient vector x, then we can make a least square problem to minimize âˆ¥Gx âˆ’ bâˆ¥2 where G = Î¦ D

and b = âˆ’Î¦x.Designing test function is essential, so we follow the selection in Ref. [ 43 ]: Ï•i(t) has of the form 

C(t âˆ’ a)p(b âˆ’ t)q . Therefore, hyperparameters are the degree of the polynomial, p and q, the number of test functions, k, and the length of compact support for each test function b âˆ’ a.For a weighted least square formulation with a regularizer, we refer to the paper [ 43 ]. In this paper, we only adopt the simplified form. 62.6 PDE-FIND 

## 2.6 PDE-FIND 

Sparse identification in symbolic regression can be applied to datasets generated by partial differential equations [ 47 ]. Dictionaries typically are composed with time and spatial derivative. PDE version of weak SINDy also was developed [ 48 ]. Here we describe a simplified version of them, following notation in the paper [48]. Our spatiotemporal ambient space is R1 Ã—Rd. For an open bounded subset Î© in Rd, let a spatiotemporal dataset {Ui}0â‰¤iâ‰¤n be given on the spatial grid X âŠ‚ Â¯Î© over a time sequence {ti}0â‰¤iâ‰¤n âŠ‚ [0 , T ] . We assume that the dataset satisfies Ui = u(X, t i) for a solution u of the PDE 

âˆ‚âˆ‚t u(x, t ) = DÎ±1

g1(u(x, t )) + DÎ±2

g2(u(x, t )) + Â· Â· Â· + DÎ±S

gS (u(x, t )) , x âˆˆ Î©, t âˆˆ (0 , T ).

for some set of unknown true functions {gi}iâˆˆ[S]. Here, we used the multi-index notation to write the partial differentiation: 

Î±i = ( Î±i

> 1

, . . . , Î± id+1 ) âˆˆ Nd+1 , DÎ±s

u(x, t ) = âˆ‚Î±s

> 1+Â·Â·Â· +Î±sd+1

âˆ‚x Î±s 

> 1
> 1

Â· Â· Â· âˆ‚x Î±sd 

> d

âˆ‚t Î±sd+1 

u(x, t ).

Our goal is to identify this differential equation using a dictionary whose elements are spatiotemporal derivatives of a family of functions (fj )jâˆˆ[m] (called as the trial functions). Again we assume that 

{gi} âŠ‚ span jâˆˆ[m](fj ) so that the PDE can be rewritten by 

âˆ‚âˆ‚t u(x, t ) = 

> S

X

> s=1
> m

X

> j=1

w(sâˆ’1) m+j DÎ±s

fj (u), (2.5) where the vector w âˆˆ RmS is the coefficients, which is assume to be sparse. Flattening the snapshots Ui at each time ti gives us a setting for a sparse linear regression, namely PDE-FIND [47]. On the other hand, with a smooth test function Ïˆ(x, t ) âˆˆ L2(Î©) which compactly supported in 

Î© Ã— (0 , T ), one may get the weak formulation of the dynamics (2.5) as follows: 

âˆ’

 âˆ‚âˆ‚t Ïˆ, u 



> L2(Î©)

=

> S

X

> s=1
> m

X

> j=1

D

(âˆ’1) |Î±s|DÎ±s

Ïˆ, f j (u)

E

> L2(Î©)

.

Likewise the ODE version, we use multiple test functions Ïˆ1, . . . , Ïˆ k to deduce a linear system of the form b = Gw where b = ( b1, . . . , b k) âˆˆ Rk, G = ( Gi,j ) âˆˆ RkÃ—mS , bâ„“ = âˆ’ âˆ‚âˆ‚t Ïˆâ„“, u L2(Î©) , Gâ„“,j =

âŸ¨(âˆ’1) |Î±s|DÎ±s

Ïˆâ„“, f j (U )âŸ©L2(Î©) . Weak SINDy for PDE works over this setting [ 48 ]. In particular, the authors took k translated functions over one reference test function Ïˆ; Ïˆ(x, t ) = Ïˆ(x âˆ’ xâ„“, t âˆ’ tâ„“), so that the inner products could be rewritten in the form of convolutions. Leveraging the Fourier transform and its property, one may reach out a reduced linear system again. Solving the least square problem over the given dataset to find the coefficient vector effectively works for several examples [48]. 

# 3 Scoring-Based Dictionary Pruning Method 

In this section, we introduce scores and our dictionary-selection method. Let y âˆˆ Rm be a given target vector, and we seek to construct a model for y using a dictionary matrix D = D(x) = [ d1(x)| Â· Â· Â· | dn(x)] âˆˆ RmÃ—n whose columns are generated by the dictionary items 

di âˆˆ L2(Rd; R) = L 2. Here, the space L2 is equipped with the empirical measure Pni=1 Î´xi and Î´xi is the Dirac measure. Our prior assumptions for learning dynamics are dealt with in Appendix A. 

## 3.1 Score 

We define the (projected) score of each dictionary item di as follows: Score (di; D, y ) = (PD âˆ’ P D[n]\{ i} )y L2

âˆ¥yâˆ¥L2

.

73.1 Score 

Since we will compare such scores, we impose normalization by their denominator, which similar to the concept of the fraction of variance unexplained. If y were in span (D[n]\{ i}), then the score of di would be zero. This score quantifies how informative a given dictionary item is relative to the entire dictionary D.As we have seen in Remark 1, the STLS algorithm eliminates indices corresponding to low-score items. Thus, the smallness of the score assigned to a dictionary function can be interpreted as a sign for its potential removal in STLS. We regard a dictionary item as non-informative to explain the signal when it has a small score relative to others. On the other hand, it is well-known that significant variables appear uncorrelated until conditioned on other variables. Thus, pruning dictionary items based on the numerator in the score formula may not help explain the signal. One natural extension of the formula from the proximity of the partially projected signal to the fully projected signal onto the dictionary matrix may tackle this matter since it considers all possible combinations of sub-dictionaries. Again, we consider a sub-dictionary non-informative for explaining the signal when it has a small (generalized) score relative to others. Let Dsub = Dsub (x) be a submatrix of D whose columns are selected from D. Denote the remaining columns, i.e., the (column) complement of Dsub , by D \ Dsub . The score of Dsub given D is defined as: Score (Dsub ; D, y ) = (PD âˆ’ P D\Dsub )y L2

âˆ¥yâˆ¥L2

.

This score quantifies how informative the sparse sub-dictionary Dsub is relative to the entire dictionary D.We now list several basic properties of the scores. 

Lemma 3. The scores lie in the interval [0 , 1] :

0 â‰¤ Score( Dsub ; D, y ) â‰¤ 1

One relationship between the score sequence and the sub-dictionary score is as follows: we fix a permutation Ïƒ : [ n] â†’ [n] and a sub-dictionary Dsub = {dÏƒ(1) , . . . , d Ïƒ(â„“)} âŠ‚ D satisfying |D sub | = â„“ â‰¤ n.Let h1, . . . , hn be the orthonormalization of dÏƒ(n), . . . , dÏƒ(2) , dÏƒ(1) . Then, we have 

âˆ¥(PD âˆ’ P Dsub )yâˆ¥22 = |âŸ¨ hnâˆ’â„“+1 , y âŸ©| 2 + Â· Â· Â· + |âŸ¨ hn, y âŸ©| 2

= âˆ¥yâˆ¥22



(Score( dÏƒ(â„“); D[n]\{ Ïƒ(1) ,...,Ïƒ (â„“âˆ’1) }, y )) 2 + Â· Â· Â· + (Score( dÏƒ(1) ; D, y )) 2

. (3.1) Thus, we have Score (Dsub ; D, y )2

> (3.1)

= (Score( dÏƒ(â„“); D[n]\{ Ïƒ(1) ,...,Ïƒ (â„“âˆ’1) }, y )) 2 + Â· Â· Â· + (Score( dÏƒ(1) ; D, y )) 2.

(3.2) One trivial inequality is as follows: Score (di; D, y ) < Score (Dsub ; D, y ), âˆ€ di âˆˆ D sub . (3.3) This score is related to the extra-sum-of-squares method in statistic. Here, we refer to [ 16 ]. For a model of the form y = DÎ² + Îµ, D âˆˆ RmÃ—n, the regression sum of squares of the this (full) model is given by SS R(Î²) := ( Dâ€ y)T DT y = yT PD y = âˆ¥P D yâˆ¥22.

For a partition Î² = [ Î²T 

> 1

| Î²T 

> 2

]T with Î²1 âˆˆ Rnâˆ’r , Î² 2 âˆˆ Rr , we have the reduced model y = Dsub Î²1 + Îµ. In this case, the regression sum of squares for this (reduced) model is given by SS R(Î²1) = ( Dâ€ 

> sub

y)T DTsub y = âˆ¥P Dsub yâˆ¥22

so that 2

SS R(Î²2|Î²1) := SS R(Î²) âˆ’ SS R(Î²1) = âˆ¥P D y âˆ’ P Dsub yâˆ¥22.                        

> 2These quantities lead the statistic F0given by
> F0:= SS R(Î²2|Î²1)/r
> MS Res
> ,MS Res := yTyâˆ’SS R(Î²)
> mâˆ’n=âˆ¥yâˆ’ P Dyâˆ¥22
> mâˆ’n.
> This quantity effectively used for variable selections in statistics. Also, it is known that when yfollows a Gaussian distribution, each SS Rfollows a Ï‡2distribution (e.g. [16] Appendix C.3.4). However, this statistical context is beyond our scope.

83.2 Dictionary selection 

## 3.2 Dictionary selection 

In this section, we introduce two stepwise regressors, which filter non-important terms based on scores: one exhaustive and the other reductive. The exhaustive one is analogous to drawing a Pareto curve, which plots the score (serving a role similar to the reconstruction error) against the â„“0 norm of the coefficient vector. In particular, it can be interpreted as the â„“0 counterpart of the basis pursuit denoise problem: 

max 

> â„“âˆˆ[m]
> |aâ„“|<Îµ

â„“ where aâ„“ = min 

> Dsub âŠ‚D |D sub |=â„“

Score( Dsub ; D, y ). (3.4) Here, the admissible tolerance parameter Îµ > 0 determines the sparsity level for model construction, but it will not be considered in the following iterative schemes of this section; instead, its value will be determined empirically in the numerical section. 

3.2.1 Exhaustive Stepwise Regressor 

We propose a sequential procedure to identify an optimal sub-dictionary from a given dictionary: Under the same setting in Subsection 3.1, we seek a sequence of sub-dictionaries {D i} satisfying, for i = 1 , . . . , m ,

D0 = D, Di = D \ A i, Ai := argmin 

> Dsub âŠ‚D |D sub |=i

Score( Dsub ; D, y ), ai := Score( Ai; D, y ). (ESR) Here, Ai might not be uniquely determined, especially when a score value is zero. We select a dictionary 

Di among the sequence for SINDy where the associated score ai is small. In this paper, we refer to this approach as an exhaustive stepwise regressor. We note that, unlike this search, which reduces the sparsity level incrementally, STLS eliminates multiple terms at once. Discarding low-scoring items gives an error bound for the trajectory reconstruction (See Appendix B). This scoring could be energy-like formulated. We refer to Z-SINDy in Appendix 4.4 and D.1. On the other hand, rather than discarding one by one, one may weigh each coefficient along its projection-based reconstruction weight. We refer to D-SINDy in Section D.2. 

3.2.2 Greedy Backward Stepwise Regressor 

Computing all possible sub-dictionary scores is computationally expensive. Thus, we propose an inclusive stepwise regressor, which is computationally cheaper. For a given dictionary matrix D = [ d1| Â· Â· Â· | dm] and target vector y, we define an algorithm that iteratively computes a score for each dictionary element, removes the one with the lowest score, and updates the dictionary. This procedure is formally defined as follows: 

j0 = argmin 

> iâˆˆ[m]

Score (di; D, y ), J0 = [ m] \ { j0},ji = argmin 

> â„“Ì¸âˆˆ Jiâˆ’1

Score (D{j1,...,j iâˆ’1}âˆª{ â„“}; D, y ), Ji = Jiâˆ’1 \ { ji}, i = 1 , . . . , n. (GBSR) We denote the counterpart of the sequence (ai) defined in the previous section by (bi):

bi := Score( Ji; D, y ), i = 0 , 1, . . . , n. (3.5) One may see this way a backward variable selection method based on the sum of squares regression in statistic. Also, our way reminds us of the SSR introduced in Ref. [ 17 ]. On the other hand, we visit the Orthogonal Matching Pursuit (OMP) using the score in Appendix section D.3, which is a well-known greedy search for signals. Note that this algorithm (GBSR) may not yield the minimal sub-dictionary score, i.e. possibly 

Score( {dj0 , . . . , djâ„“âˆ’1 }; D, y )Ì¸ = min 

> JâŠ‚[n]
> |J|=â„“

Score( DJ ; D, y ).

93.3 Practical Implementation 

However, when y âˆˆ span (Dsub ) âŠ‚ span (D), the procedure GBSR arrives at a sub-dictionary that achieves reasonably low score while maintaining maximal sparsity. Assume the case and let |D sub | = k < n = |D| ,then 

Score( DS ; D, y ) = 0 , âˆ€ S âŠ‚ D \ D sub .

And Anâˆ’k = D \ D sub . Indeed, the discussion in this paragraph can be formalized as Theorem 1. 

Remark 2. One might wonder the forward one; adding dictionary items one-by-one. We call this Greedy Forward Stepwise Regressor (GFSR). In our numerical simulation, the performance of GFSR is worse than GBSR. See Appendix E. 

## 3.3 Practical Implementation 

Since our discussion is just on linear regression, our methods could be combined with different methods, e.g. E-SINDy or weak-SINDy. Especially, in this paper, we focus on the weak formulation. The weak-SINDy [ 43 ]provides a different formulation of regression for system identification. Although this demands additional hyperparameters, there are advantages on computational cost and noise robustness, so we adopt this as our default method in numerical tests section. Empirically, combining all coordinates in search rather than one coordinate search is more useful. We summarize this combination in the pseudo-code (Procedure 1). 

Procedure 1 Library Selection Algorithm  

> 1:

Input: Time sequence t = ( ti), Data x = ( x(ti)) i, dictionary D = D(x), test function matrix Î¦ = Î¦( t),time-derivative of test function Î¦â€² = Î¦ â€²(t) 

> 2:

Set: Ë™x â†time derivative approximation  

> 3:

if Use Weak Form then  

> 4:

Compute transformed response: y â† Î¦â€²y, D â† DÎ¦ â–· Weak formulation  

> 5:

end if  

> 6:

Score type: Choose type A (One coordinate) or B (All coordinate)  

> 7:

Set coordinate: j, y â† Ë™x[: , j ] 

> 8:

if Exhaustive Stepwise Regressor then  

> 9:

for each level i = 1 , 2, . . . , do  

> 10:

Ai â† argmin |D sub |=i

> Dsub âŠ‚D

Score( Dsub ; D, y ) â–· Refer to (ESR)  

> 11:

ai â† Score( Ai; D, y ) 

> 12:

Apply STLS with D \ A 

> 13:

end for  

> 14:

else if Greedy Backward Stepwise Regressor then  

> 15:

Set: A0 = âˆ… 

> 16:

for each level i = 1 , 2, . . . , do  

> 17:

di â† argmin diâˆˆD\A iâˆ’1 Score( Aiâˆ’1 âˆª { di}; D, y ) â–· Refer to (GBSR)  

> 18:

Ai â† A iâˆ’1 âˆª { di} 

> 19:

ai â† Score( Ai; D, y ) 

> 20:

end for  

> 21:

end if  

> 22:

Output: {ai}, {A i}

# 4 Theoretical result: Score and STLS 

In this section, we study mathematical properties of our iterative schemes. Especially, we reveal relationships to STLS. 10 4.1 â„“0-minimization 

## 4.1 â„“0-minimization 

It is well-known that the SINDy algorithm makes a sequence which decreases the objective function 

x 7 â†’ âˆ¥ Dx âˆ’ yâˆ¥22 + Î±2âˆ¥xâˆ¥0 [ 13 ]. We could make a similar statement. For a given target vector y and a dictionary matrix D, let {Î¾i}i=0 be a sequence generated by the greedy searching. If one score of an item has small enough, say Score (di; D, y ) < Îµ 

> âˆ¥yâˆ¥2

and dÎ± > Îµ for some threshold Î±, then 

âˆ¥DÎ¾ i âˆ’ yâˆ¥2 + Î±âˆ¥Î¾âˆ¥0 > âˆ¥D[n]\{ i}Î¾i+1 âˆ’ yâˆ¥2 âˆ’ Îµ + Î±âˆ¥Î¾âˆ¥0

= âˆ¥D[n]\{ i}Î¾i+1 âˆ’ yâˆ¥2 + Î±âˆ¥Î¾i+1 âˆ¥0 + ( dÎ± âˆ’ Îµ)

> âˆ¥D[n]\{ i}Î¾i+1 âˆ’ yâˆ¥2 + Î±âˆ¥Î¾i+1 âˆ¥0.

Generally, if one takes the maximum absolute value of coefficients, whose items are discarded, as the parameter Î±, then the objective function decreases. This is summarized as the following statement without a proof, which is a direct result from [13]: 

Proposition 1. Suppose a long-thin matrix D = [ d1| Â· Â· Â· | dn] âˆˆ RmÃ—n, which has full-rank and satisfies 

âˆ¥Dâˆ¥op = 1 , is given. Let y âˆˆ Rm be a target vector. Let the sequence {si}i=ki=0 and {Si}i=ki=0 be generated by kth iteration of the algorithm (GBSR) . Then, the sequence {xi}i=k+1  

> i=0

generated by the iteration (4.1) 

decreases the map F (x) = âˆ¥Dx âˆ’ yâˆ¥22 + Î±2âˆ¥xâˆ¥0 where Î± = max iâˆˆ[k]0 |xisi |.

x0 = Dâ€ y, xi+1 = argmin 

> supp( x)âŠ‚Si

âˆ¥Dx âˆ’ yâˆ¥2, i = 0 , . . . , k. (4.1) 

## 4.2 Data pattern on a supervised dataset: ideal case 

We will see a pattern in the sequences (ai) and (bi) defined in Section 3.2; once a sub-dictionary loses an important item then the corresponding score drastically increases. Based on this observation, we may choose an empirical sparsity level and perform linear regression for model selection. In this case, in terms of the problem (3.4) , the admissible error bound Îµ may be automatically selected. However, if there is no combination composed with a given dictionary to explain the target vector, then it would be hard to make a stop criterion. In this case, we design a preprocessing step for SINDy that closely aligns with STLS (See Proposition 3), thereby allowing a controlled adjustment of the solution path. In an ideal setting, we would observe a clear data pattern that separates important from non-important items when the iteration scheme (ESR) is applied. This observation provides an intuitive justification for why our preprocessing method is effective in identifying the underlying system. For a system Ë™x(t) = F (x(t)) = Pki=1 cidi(x(t)) and Î iciÌ¸ = 0 , set Dsub = {d1, . . . , d k} âŠ‚ D =

{d1, . . . , d n} with n > k . For a time sequence (ti), let xi = x(ti) and di = [ di(tj )] j . Let yi = F (xi). In this ideal situation, a sequence ai generated by (ESR) will be of the form ai = 0 for i = 1 , . . . , n âˆ’ k but 

aiÌ¸ = 0 for i > n âˆ’ k. So the best way to pick up a dictionary is to select Di0 where ak = 0 for all k â‰¤ i0.

â€¢ When i < n âˆ’ k, any subset A âŠ‚ D \ D sub satisfying |A| = i implies Score (Ai; D) = 0 . Obviously, there are multiple candidates for Ai for each i < n âˆ’ k. However, any sub-dictionary D \ A may generate the same result as STLS. 

â€¢ When i = n âˆ’ k, Anâˆ’k = D \ D sub = âˆªnâˆ’ki=1 Ai. It is because Score( Anâˆ’k; D) = 0 .

â€¢ i = n âˆ’ k + 1 case is a little different since any candidate of Anâˆ’k+1 must contain, at least, one important dictionary item due to the Pigeonhole principle. With one important item di0 (i0 â‰¤ k),

âˆ¥P D y âˆ’ P Dsub \di0 yâˆ¥2 = |ci0 |âˆ¥ (I âˆ’ P Dsub )di0 âˆ¥2.

A lower bound of the score of Dsub could be derived from the mutual incoherence between dictionary items: there exists a positive number m such that for any index j,

âˆ¥P S dj âˆ¥2 < m < âˆ¥dj âˆ¥2, âˆ€S = {d1, . . . , Ë†dj , . . . , d n}.

11 4.3 Alignment to STLS 

## 4.3 Alignment to STLS 

Since the ground-truth dynamics is generally unknown for a given time series dataset, we use STLS as a reference criterion and examine when the results produced by the iterative schemes (ESR) and (GBSR) align with those of STLS. While the alignment holds under strong assumptions, numerical experiments suggest that the behavior persists beyond this regime. The following proposition demonstrates that if a dictionary item survives the first iteration of STLS, then any sub-dictionary containing this item will not yield an exceptionally low score. Therefore, a score-based preprocessing step for STLS, which selects sub-dictionaries with low scores, ideally preserves the outcome of the original STLS procedure. 

Proposition 2. Suppose for all i = 1 , . . . , n , there exists a constant Ï‰ > 0 such that 

Ï‰ < âˆ¥di âˆ’ P D[n]\{ i} diâˆ¥2, i = 1 , . . . , n. (4.2) 

If the score of a sub-dictionary matrix Dsub with Dsub âŠ‚ D is sufficiently small with respect to a threshold 

Î» > 0, then the first iteration of STLS with the threshold deletes the sub-dictionary items. 

When the coefficients from the first iteration of STLS are clearly separated into two groups by the threshold with a large difference between them, the sub-dictionary sequence generated by (ESR) converges to the same result as STLS. 

Proposition 3. Suppose that all dictionary items are normalized; âˆ¥diâˆ¥2 = 1 for all i = 1 , . . . , n . Assume a dictionary D = {d1, . . . , d n} satisfies the followings: 1. PD y = Pni=1 cidi = Dâƒ— c, y = P D y +âƒ—e, âˆ¥âƒ—eâˆ¥ â‰¤ Îµ,2. S0 = {i âˆˆ [n] : |ci| â‰¥ Î»1}, |S0| = k < n , [n] \ S0 = {i âˆˆ [n] : |ci| â‰¤ Î»2}, |S0| = k < n ,3. For any T âŠ‚ [n] satisfying |T | = n âˆ’ k, |T âˆ© S0| = â„“ â‰¤ k,

(2 n âˆ’ 2k âˆ’ â„“)Î»2 + 2 Îµ â‰¤ â„“pR(G,âƒ— c T âˆ©S0 )Î»1 (4.3) 

where R( G,âƒ— c T âˆ©S0 ) is the Rayleigh quotient for the matrix G = DTT âˆ©S0 (I âˆ’ PD[n]\T )DT âˆ©S0 and the vector âƒ—c T âˆ©S0 .Then, Dnâˆ’k = S0. Here, Dnâˆ’k is defined in (ESR) .

Remark 3. One example of a dictionary satisfying this assumption in this proposition is when the dictionary is nearly orthogonal. 

Now we are ready to see the result on the procedure (GBSR). 

Theorem 1. Suppose the same settings in Proposition 3. Furthermore, assume that for all i = 1 , . . . , n ,there exists a constant 0 < Ï‰ such that 

Ï‰ < âˆ¥di âˆ’ P D[n]\{ i} diâˆ¥2 =: Ï‰i, (2 k âˆ’ 1) Î»2 + 2 Îµ â‰¤ Î»1Ï‰. (4.4) 

Then, Jkâˆ’1 = Anâˆ’k. Here, Jkâˆ’1 is defined in (GBSR) .

Remark 4. In the proof, one may get the equality {j0, . . . , j kâˆ’1} = [ n] \ S0 without the condition (4.3) .

## 4.4 Trade-off between score and sparsity 

One may consider a score-based minimization problem with a sparsity penalty over sub-dictionaries: for some Î± > 0,

min  

> Dsub âŠ‚D

Score (Dsub ; D) + Î±|D \ D sub |.

Interestingly, this formulation resembles the structure of Z-SINDy, particularly in the regime of large sample sizes, a recently developed approach within the framework of statistical mechanics. While we do not pursue this minimization further in the present paper, we note its connection to Z-SINDy, which addresses a similar problem. A brief discussion is provided in Appendix section D.1. 12 5 Numerical simulation 

In this section, we numerically test the score in different scenarios, both for ODEs and PDEs. As noted earlier, all simulations are conducted under the weak formulation since we obtained better results than the standard form. We begin with the ODEs systems (Table 1) as a show case of how score works in classical benchmark (Lorenz System) and how it can improve the system identification with SINDy in the case with small coefficients (Hopf bifurcation) and a case where SINDy fails to identify the correct system (Pitchfork bifurcation). Then we test how the score behaves with noise, both for ODEs and PDEs. The PDEs equations appeared in Ref. [ 48 ]. Finally, we present an unsupervised dataset from a viscoelastic flow in Section 5.3. 

## 5.1 ODEs Systems 

In this section, we apply the score for the equations on Table 1. We test our stepwise algorithm, which seek to iteratively exclude irrelevant terms in the library until we get only relevant terms. We can see in Figure 2 that the correct terms of each equation were kept until the end of filtering, discarding all irrelevant terms first. Table 1: Examples of ODEs Tested ODE Form Parameters Used                                                                    

> Lorenz System
> ï£±ï£´ï£²ï£´ï£³
> Ë™x=Ïƒ(yâˆ’x),
> Ë™y=x(Ïâˆ’z)âˆ’y,
> Ë™z=xy âˆ’Î²z Ïƒ= 10 , Ï = 26 , Î² =83;
> Initial condition: (âˆ’8,8,27)
> Tâˆˆ[0 ,10] ,âˆ†t= 0 .01
> Hopf Bifurcation
> (
> Ë™x=Î¼x âˆ’Ï‰y âˆ’x(x2+y2),
> Ë™y=Ï‰x +Î¼y âˆ’y(x2+y2)
> Î¼=âˆ’10 âˆ’5, Ï‰ = 1;
> Initial condition: (5 ,0)
> Tâˆˆ[0 ,100] ,âˆ†t= 0 .01
> Pitchfork Bifurcation
> (
> Ë™x=Î¼x âˆ’x3,
> Ë™y=âˆ’yÎ¼= 0 .5;
> Initial condition: (âˆ’1.5,1)
> Tâˆˆ[0 ,10] ,âˆ†t= 0 .01

We also observe a pattern in the scores over the iterations for the trajectory of each coordinate of the system. Figure 3 shows the score of each iteration and, when the process filter out a relevant term for the equation, there is a "jump" in the score, which is a indicative of optimal sparsity. We could not automatically identify this jump in the score, so the selection of optimal sparsity is made empirically by the observation of the relative scores. 

5.1.1 Scores 

For the purpose of comparison, we introduce two other scores: Pareto-score and cross validation scores. Recall the definition of our (projective) score and then it is natural to directly use the reconstruction error in the definitions. So we denote it by Pareto-score: 

Pareto( Dsub ; D, y ) = y âˆ’ P D\Dsub y L2

âˆ¥yâˆ¥L2

(5.5) We also denote by ci the Pareto-score counterpart of the sequence (ai) defined in Section 3.2. For the k-fold cross validation score (See Section 2.4), we use the same notation Î´ as in Ref. [ 17 ]. For a given dataset x = [ x(ti)] 1â‰¤iâ‰¤m, we split the (sampling) index set [m] into k disjoint equivalent subsets Aj

satisfying âˆªAj = [ m]. Denoting the restriction of a vector (resp. matrix) y onto Aj by y|Aj , we define the 13 5.1 ODEs Systems 

(a) Lorenz System sin (2 x)

> z2
> cos (1 z)
> sin (1 y)
> sin (1 z)
> cos (2 z)
> cos (2 x)
> sin (2 y)
> cos (1 x)
> cos (1 y)
> sin (1 x)
> cos (2 y)
> 1
> z3
> sin (2 z)
> xz 2
> yz
> x2z
> x2
> yz 2
> x2y
> y2z
> y3
> xy 2
> x3
> y2
> xyz
> xz
> z
> xy
> x
> y
> Dictionary items
> 0
> 5
> 10
> 15
> 20
> 25
> 30
> Filtering order (0, 1, 2, 3, ...)

Score history (White: filtered) 

> 1.000
> 0.000
> 0.000
> 0.001
> 0.001
> 0.005
> 0.010
> 0.050
> 0.100
> 0.500
> Score

(b) Hopf System xy 

> x2
> 1
> y2
> xy 2
> x3
> x2y
> y3
> x
> y
> Dictionary items

0

2

4

6

8 Filtering order (0, 1, 2, 3, ...) 

> Score history (White: filtered)
> 1.000
> 0.000
> 0.000
> 0.001
> 0.001
> 0.005
> 0.010
> 0.050
> 0.100
> 0.500
> Score

(c) Pitchfork Bifurcation xy 

> 1
> y3
> x2
> y2
> xy 2
> x2y
> y
> x
> x3
> Dictionary items

0

2

4

6

8 Filtering order (0, 1, 2, 3, ...) 

> Score history (White: filtered)
> 1.000
> 0.000
> 0.000
> 0.001
> 0.001
> 0.005
> 0.010
> 0.050
> 0.100
> 0.500
> Score

Figure 2: Each row represents a comparison between items, color-coded in the iteration step of GBSR. To generate the plots, we iteratively evaluate sub-dictionaries of increasing sparsity. At sparsity level 1 (top row), we compute the total score for each sub-dictionary by summing the scores over all state variables. The sub-dictionary with the lowest total score, bâ€²

0, is placed at the leftmost position. At each successive sparsity level (e.g., level 2 for the second row), we evaluate only those sub-dictionaries that contain the previous selection with bâ€²

iâˆ’1, identify the one with the smallest score ai, and place it accordingly. This continues until the desired sparsity level is reached. For each system, we analyze the score sequence (bâ€²

i) to detect a sudden increase and determine the optimal sparsity level. The last few terms retained are the most important: 5 for the Lorenz system, 6 for Hopf, and 3 for Pitchfork. 

ith cross validation score as 

Î´2[i] = 1

k

kX

â„“=1 

âˆ¥Î¾|Aâ„“ âˆ’ D|Aâ„“ Â· SSR( D|Bâ„“ , y |Bâ„“ )iâˆ¥22, (5.6) 14 5.1 ODEs Systems 

(a) Lorenz System 0 5 10 15 20 25 30 

Filtering Order       

> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> Score
> Library Terms
> yx
> Ë™x
> xz xy
> Ë™y
> xy z
> Ë™z

Score History for Each Coordinate 

(Lorenz System)     

> coordinate x
> coordinate y
> coordinate z

(b) Hopf System 0 2 4 6 8

Filtering Order        

> 10 13
> 10 11
> 10 9
> 10 7
> 10 5
> 10 3
> 10 1
> Score
> Library Terms
> x3
> yxy 2
> x
> Ë™x
> xx2yy3
> y
> Ë™y

Score History for Each Coordinate 

(Hopf System)   

> coordinate x
> coordinate y

(c) Pitchfork Bifurcation 0 2 4 6 8

Filtering Order       

> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> Score
> Library Terms
> x3
> x

Ë™x

> y

Ë™y

Score History for Each Coordinate 

(Pitchfork bifurcation)   

> coordinate x
> coordinate y

Figure 3: Minimum score sequence (bi) defined in (3.5) of the algorithm GBSR for each coordinate of the ODE systems. For each variable, a noticeable increase in the score occurs when a correct term is removed from the library, indicating the point of optimal sparsity. where Bn = âˆªjÌ¸ =nAj .To observe the data patterns, we will use the relative scores of the form ai/a iâˆ’1, bi/b iâˆ’1, and Î´[i]/Î´ [iâˆ’1] .

5.1.2 Lorenz System (without noise) 

The Lorenz system is widely used as a benchmark for Sparse Identification of Nonlinear Dynamical Systems (SINDy) due to its chaotic nature and the challenge it poses in identifying the underlying governing equations from data. For the library, we used a set of polynomial basis functions up to degree 3, including all possible monomials of the form xiyj zk where i+j +k â‰¤ 3 combined with trigonometric functions with 2 periodicities, resulting in a total of 32 functions: 

Î˜( X) =  1 x y z Â· Â· Â· z3 sin( x) cos( x) sin( y) Â· Â· Â· sin(2 z) cos(2 z)  (5.7) Even with a large library, the scoring method is able to identify the correct terms. An interesting case arises for the y-coordinate: although the relevant library term ( y) is correctly ranked above the incorrect items, its score is noticeably lower than those of the other true items. This may cause one to incorrectly eliminate one true dictionary item for a unsupervised system with only empirical observation of the score curve. A similar result was obtained in ZSINDy where the same y is first to be identified incorrectly under noise [21]. 15 5.1 ODEs Systems 

5.1.3 Hopf bifurcation 

We revisit the Hopf bifurcation example in Ref. [ 12 ] (Table 1), where Ï‰ > 0 is a fixed parameter that determines the rotational frequency, and Î¼ is a bifurcation parameter that controls the growth or decay of amplitude. The asymptotic behavior depends on Î¼. When Î¼ > 0, limit cycles appear. When Î¼ = 0 , the system undergoes a Hopf bifurcation. The origin is a center: trajectories neither spiral in nor out but form closed orbits (non-asymptotic periodic motion). When Î¼ < 0, the origin becomes a globally asymptotically stable spiral (attractor). All trajectories decay toward the origin regardless of initial conditions. When Î¼ < 0 but with small magnitude, short-time trajectories appear similar to those at Î¼ = 0 .However, their asymptotic behaviors differ. STLS may detect such linear terms if an appropriate threshold is chosen. In this example, in order for SINDy to correctly identify the system, the threshold must be smaller than the Î¼ parameter, which was chosen to be small ( Î¼ = âˆ’10 âˆ’5). In contrast, the score-based strategy may perform robustly regardless of the threshold. Here, the dictionary is monomials with degrees less than or equal to 3; D = {xiyj : i + j â‰¤ 3}. Without the pre-processing, the SINDy with threshold Î» = 5 Ã— 10 âˆ’6 yields the following system: 

ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³

Ë™x = âˆ’0.003086 + 0 .038567 x âˆ’ 1.013777 y + 0 .390146 x2 âˆ’ 0.015740 xy 

âˆ’0.104667 y2 âˆ’ 1.159224 x3 âˆ’ 0.348013 x2y âˆ’ 1.443307 xy 2 + 0 .277868 y3

Ë™y = 7.3 Ã— 10 âˆ’6 + 0 .999047 x + 0 .000333 y âˆ’ 0.009260 x2 + 0 .000258 xy 

+0 .002534 y2 + 0 .003947 x3 âˆ’ 0.992273 x2y + 0 .011203 xy 2 âˆ’ 1.006687 y3.

(5.8) This model includes many incorrect terms with coefficients larger than those in the true system, which causes SINDy to select spurious terms in the final model. In contrast, the proposed pre-processing of the library does not require any thresholding. The user can identify relevant terms by examining the score behavior for each coordinate, as shown in Figure 3, and then perform regression using only the selected terms. When the weak formulation is applied, STLS correctly identifies the relevant terms. However, in scenarios where the user lacks prior knowledge of the true terms, an inappropriately high threshold may be chosenâ€”potentially larger than some true coefficientsâ€”which again results in an incorrect system. 

5.1.4 Pitchfork Bifurcation 

While the filtering procedure are sensitive to the weak formulation parameters (e.g., polynomial degree, number of test functions, and support size), making careful hyperparameter tuning essential, we found that an appropriate choice of weak formulation parameters allowed the score-based filtering to recover the correct terms for a single trajectory, whereas SINDy alone could not identify the correct solution without filtering. Furthermore, this example shows that our scores outperfom other scoring strategies, as illustrated in Figure 4. In our tests, we used a number of test functions equal to twice the number of library terms. The polynomial degree of the test functions was set to 17. The support size for the test functions was determined following the method proposed in [ 43 ], which leverages Fourier modes to capture large variations in the systemâ€™s dynamics within each test function interval. We observed that small changes in any of these hyperparameters significantly affect the results of both the filtering and SINDy procedures, making careful hyperparameter tuning essential. With this choice of weak formulation hyperparameters, SINDy produces the following system: 

(

Ë™x = 1 .7661 + 0 .500 x âˆ’ 3.531 x2 âˆ’ 1.000 x3 + 2 .747 x2y

Ë™y = 4 .0871 âˆ’ 1.000 y âˆ’ 8.173 x2 + 6 .357 x2y

This model includes the correct terms with accurate coefficients but also contains additional spurious terms. It is important to note that the results are highly sensitive to the choice of hyperparameters. Therefore, it is possible that an optimal selection of these parameters could lead SINDy to recover the exact system without extra terms. A practical note is that using multiple trajectories generated under different simulation parameters may mitigate this sensitivity. In this setting, both SINDy and the score-based method are able to recover the correct system, and their performance becomes much less dependent on hyperparameter choices. 16 5.1 ODEs Systems 10 9    

> 10 7
> 10 5
> 10 3
> 10 1
> Coordinate  x
> Score
> Correct terms
> xx3

Absolute score  

> Our
> Pareto
> Cross-Validation
> 10 1
> 10 0
> 10 1
> 10 2
> 10 3
> 10 4
> 10 5

Relative Score                

> 02468
> Filtering Order (n)
> 10 12
> 10 10
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0
> Coordinate  y
> Score
> Correct term
> y
> 02468
> Filtering Order (n)
> 10 2
> 10 0
> 10 2
> 10 4
> 10 6
> 10 8
> 10 10
> 10 12

Pitchfork Bifurcation: Comparison of 3 scores 

Figure 4: Comparison of our score with the Pareto score and cross-validation score. Only our method successfully identify the correct terms of the system for this setting of weak formulation hyperparameters. The first column shows the absolute score obtained by each method, while the second column presents the score ratio between steps n and n âˆ’ 1 (relative score). The top row corresponds to the results for the coordinate x, and the bottom row shows results for the coordinate y. Our score (blue circle) exhibit a clear peak in the relative score, which enables identification of the optimal sparsity. In contrast, the Pareto score (green square) and the cross-validation score (magenta triangle) shows this peak behavior only for the y-coordinate, offering no consistent criterion for determining optimal sparsity across all coordinates. 

5.1.5 Noisy data 

One important aspect is how the method behaves with noisy data. SINDy variations such as Weak SINDy (WSINDy) [ 43 ] and Ensemble SINDy (ESINDy) [ 22 ] are examples of methods that deal with noise in the data, making them more useful in practial applicaions. The noisy data Xnoise is obtained by adding a gaussian noise to the original data X according to: 

Xnoise = X + Î·

r P X2

> i

N N (0 , 1) 

where Î· is the relative noise level. For each noise level, 100 samples were generated. The stepwise regressor are built under the heuristic assumption that Ai âŠ‚ A i+1 , where Ai is defined by Eq. (GBSR) . However, the sharp increasing pattern was not observed in the presence of noise, indicating that exhaustive search (ESR) is the recommended method in such cases. To test the correct identification of terms after filtering, we evaluated the ability to recover the terms for the y-coordinate of the Lorenz system under noisy conditions. This coordinate was chosen because the contribution of its correct term to the score is relatively low, as shown in Figure 3 (the y term has a lower score compared to the others). We performed an exhaustive search over all possible combinations of three functions from the library and considered the identification successful only when all three correct terms from the original equation were selected. The results are presented in Figure 5. We also tested the identification of the correct library using all coordinates, searching for the best combination of five terms. This resulted in 100% correct identification, likely because the correct terms for the other variables have significantly higher scores than the incorrect ones. 17 5.2 PDE-FIND 0 1 2 3 4 5 6 7 8 9 10 

% noise 

0

2

4

8

16                                                   

> Test Function Degree
> 100% 98% 47% 7% 1% 0% 0% 0% 0% 0% 1%
> 100% 100% 97% 76% 48% 28% 22% 8% 2% 2% 1%
> 100% 100% 98% 80% 59% 42% 28% 22% 4% 5% 2%
> 100% 100% 99% 84% 65% 46% 28% 23% 6% 5% 1%
> 100% 100% 99% 86% 67% 52% 25% 22% 9% 7% 2%

Success Rate of Filtering (Exhaustive Search) 

> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

Figure 5: Each row presents the success rates of recovering the correct terms associated with the y

coordinate of the weak-formulated Lorenz system using score-based exhaustive search. Each column indicates the level of additive noise. We also highlight the use of the weak formulation with high-degree polynomials as test functions. In this case, the number of test functions was four times the number of library terms. There are some hyperparameters in the weak formulation that could be optimized for improved performance. Additionally, the computational cost is reduced due to the decrease in matrix size, from the number of time points to the number of test functions. 

## 5.2 PDE-FIND 

In this section, we apply our scoring to some time-evolution-ary PDEs. The simulation shows that the exhaustive search for the correct number of library terms in PDEs is successful . However, a major limitation is that PDE libraries are significantly larger than their ODE counterparts, making exhaustive search considerably more computationally expensive. For example, in the Reaction-Diffusion (RD) case, each field contains 7 correct terms, and the full library consists of 42 terms, rendering a complete combinatorial search infeasible. In contrast, the PDE filtering using a stepwise regressor proved to be much more effective, achieving a success rate above 70% across all cases, even under high noise levels as shown in Figure 6. This is particularly advantageous for PDEs, where larger libraries are common and stepwise search is less computationally intensive. 

## 5.3 Proper Orthogonal Decomposition on viscoelastic flow 

We apply our method to the dataset from [ 49 ], which modeled the Proper Orthogonal Decomposition (POD) modes of a numerical simulation of viscoelastic flow in the four-roll mill setup [50] using SINDy. In Ref. [ 49 ], the authors applied POD to a dataset generated by the viscoelastic flow simulation and took two time-temporal modes of them. The two modes produce a planar curve which is reconstructed via SINDy method. On the other hand, Weak SINDy application over POD modes in fluid dynamics also was studied [ 45 ]. We use the same dataset of Ref. [ 49 ] in the present paper: for a fixed non-dimensional parameter value of W i = 3 .5, Figure 7 shows the temporal evolution of the first two modes. Starting from the origin, the dynamics evolve toward a limit cycle. In their work, the authors used a function library containing linear and cubic terms, excluding quadratic terms as these negatively impacted the results. Here, we show how our scoring procedure provides a principled justification for this choice by automatically filtering out quadratic terms. The system discovered by SINDy is given by: 18 5.3 Proper Orthogonal Decomposition on viscoelastic flow 

Table 2: PDEs used for testing the selection PDE Form 

Inviscid Burgers 

âˆ‚tu = âˆ’ 12 âˆ‚x(u2)

Kuramoto-Sivashinsky (KS) 

âˆ‚tu = âˆ’ 12 âˆ‚x(u2) âˆ’ âˆ‚xx u âˆ’ âˆ‚xxxx u

Nonlinear SchrÃ¶dinger (NLS) 

(

âˆ‚tu = 12 âˆ‚xx v + u2v + v3

âˆ‚tv = âˆ’ 12 âˆ‚xx u âˆ’ uv 2 âˆ’ u3

Reaction-Diffusion (RD) 

(

âˆ‚tu = 110 âˆ‚xx u + 110 âˆ‚yy u âˆ’ uv 2 âˆ’ u3 + v3 + u2v + uâˆ‚tv = 110 âˆ‚xx v + 110 âˆ‚yy v + v âˆ’ uv 2 âˆ’ u3 âˆ’ v3 âˆ’ u2v

Viscoelastic Flow (16-Roll) 

ï£±ï£´ï£²ï£´ï£³

âˆ‡ Â· u = 0 

âˆ‚tu + u Â· âˆ‡ u = âˆ’âˆ‡ p + Î²Re âˆ‡2u + 1 

> Re

âˆ‡ Â· Ï„ + f

âˆ‚tC + ( u Â· âˆ‡ )C = ( âˆ‡u) C + C (âˆ‡u)T âˆ’ 1

> Wi

 C âˆ’ I0 1 2 3 4 5 6 7 8 9 10 

% noise 

Inviscid 

Burgers 

KS 

NLS 

RD                                         

> PDE Models
> 100% 89% 93% 97% 94% 97% 91% 95% 95% 94% 95%
> 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100%
> 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 99%
> 100% 100% 97% 91% 78% 79% 71% 75% 80% 79% 74%

Success Rate of Filtering (Stepwise Regressor) 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

Figure 6: Each row presents the success rates of recovering the correct terms associated with a particular coordinate of the weak-formulated test system using score-based stepwise search. Each column indicates the level of additive noise. 

 Ë™a1

Ë™a2



=

 Ïµ1 Ïµ2

âˆ’Ïµ2 Ïµ1

  a1

a2



+

Î´1a21 + Î´2a22 + Î´3a1a2 Î´4a22

âˆ’Î´4a21 Î´2a21 + Î´1a22 âˆ’ Î´3a1a2

  a1

a2



(5.9) 19 0.4 0.2 0.0 0.2 0.4 

> mode #1
> 0.4
> 0.2
> 0.0
> 0.2
> 0.4
> mode #2

First 2 POD modes from 16 roll 

> Initial Point

Figure 7: 16-Roll data example. On the left we have the full order dada representation. On the right the planar curve is a trajectory from two time-temporal POD modes. The red dot denotes the starting point of the trajectory. The graph seemingly tends to a limit cycle. a21 a22

> a1a2
> a1a32
> 1
> a31 a2
> a41
> a22
> a21
> a42
> a21 a2
> a1a22
> a32
> a31
> a2
> a1
> Dictionary items
> 0
> 2
> 4
> 6
> 8
> 10
> 12
> Filtering order (0, 1, 2, 3, ...)
> Score history (White: filtered)
> 1.000
> 0.000
> 0.000
> 0.001
> 0.001
> 0.005
> 0.010
> 0.050
> 0.100
> 0.500
> Score

Figure 8: A comparison between items, color-coded in the iteration step of GBSR for POD modes of viscoelastic flow simulation. We apply our filtering considering a polynomial library with degree upmost 4. The filtering order in Figure 8 shows how the quadratic and fourth order term were removed first, leaving the linear and cubic term at the end, although it is hard to see the data pattern we expected in Figure 9. It might be due to the non-existence of a ground dynamics consisting with dictionary functions. 

# 6 Conclusion 

In this paper, we proposed a score-based variable selection method that enables the identification of important terms even when their coefficients are small. Over the rationale of Pareto curve, we studied the connection between coefficient and score, thereby clarifying how STLS relates to our score framework. We investigated the power and limitation of our approach through various numerical examples, and also conducted noise tests for the practical utility of our approach for SINDy users. In particular, we implemented scoring to weak SINDy formulation. SINDy user may get help to select threshold or may use our scoring as a pre-processing step for SINDy (See Appendix D.4). While the concept of score-based selection is not new, our contribution lies in exploring it within the context of dynamical system identification and uncovering connections to existing methods. 20 0 2 4 6 8 10 12 14 

Filtering Order (n) 

# 10 3

# 10 1

> Our Score ( bn)

# Score History for each coordinate 

# (POD-Viscoelastic Data) 

coordinate a1

coordinate a2  

> 10 0
> 10 1bn
> bn1

Relative Score Figure 9: Stepwise regression scores computed using our proposed score for the POD modes of a viscoelastic simulation. The filtering order suggests that quadratic terms are less relevant to the system. However, no clear "jump" in the score is observed, which complicates the identification of an optimal sparsity level. The issue of system identification with small coefficients may be mitigated. Our method replaces threshold-based selection a alternative based on score-guided filtering, that only requires a discrete integer hyperparameter: the number of terms, avoiding the need to fine-tune a real-valued threshold. As a result, the burden of parameter tuning is reduced, and systems with small but important terms become more recoverable in practice. Our results suggest that the pattern of error or score variation across sparsity levels contains important structural information. In particular, the presence of distinct "jumps" in the score can indicate the right model complexity, which guides the discrete hyperparameter mentioned before. Finally, our framework offers compatibility with various SINDy variants and extensions. The method may integrate into pipelines for system discovery, e.g. implicit differential equation identification, SINDy-PI [34 ], or unsupervised dataset where right dictionary functions are not clear, as exemplified using the work of [49]. Several open problems and improvements remain for future work. These include developing an automatic selection algorithm, deriving optimal conditions in Proposition 3, conducting statistical analysis of score distributions under noise, explore parametric systems with multiple simulation at once, and aligning model selection strategies with asymptotic system behaviors. 

# Acknowledgments 

The work by H. Cho was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (RS-2023-00253171), F. Amaral and C. Oishi would like to thank the financial support given by the SÃ£o Paulo Research Foundation (FAPESP) process numbers 2013/07375-0, 2021/13833-7, 2023/06035-2, 2021/07034-4, and the National Council for Scientific and Technological Development (CNPq), grants 305383/2019-1 and 307228/2023-1. The authors acknowledge support from the National Science Foundation AI Institute in Dynamic Systems (grant number 2112085).. We thank Joel Tropp for pointing us to interesting literature. H. Cho thanks Zackary Nicoulau, Nick Zolman and Doris Voina for helpful discussions. 21 A Assumption on learning dynamics 

In this section, we introduce several assumptions underlying the learning of dynamical systems â€“ SINDy and discuss their limitations: The existence of a well-defined (continuous) ground-truth dynamical system; The assumption that the projected dynamics onto the function space spanned by the dictionary does not diverge. Before proceeding, we adopt an assumption for learning dynamics similar to that used in Ref. [51]. 

Assumption 1. For a given time-series dataset, there exists an underlying dynamical system governed by differential equations with a continuously differentiable, time-independent vector field. This is assumed throughout the present paper. Moreover, we do not consider cases involving partial observations (on supervised datasets). The accuracy of the best approximation depends on the finite difference scheme used to approximate the derivative. Specifically, we have âˆ¥ Ë™xâˆ— âˆ’ Ë™xâˆ¥â„“âˆ = O(âˆ†) where âˆ† is the (uniform) time step. Thus, even if one obtains a well-constructed dictionary D(x) and a coefficient vector Î¾ such that âˆ¥ Ë™x âˆ’ D(x) Â· Î¾âˆ¥â„“âˆ â‰¤âˆ¥ Ë™x âˆ’ D(x) Â· Î¾âˆ¥â„“2 = m Â· O(âˆ†) , the resulting error still satisfies âˆ¥ Ë™xâˆ— âˆ’ D(x) Â· Î¾âˆ¥â„“âˆ = O(âˆ†) . Defining 

âˆ¥ Ë™xâˆ— âˆ’ D(x) Â· Î¾âˆ¥â„“âˆ =: âˆ†âˆ, this implies that for t âˆˆ [t1, t m], xâˆ—(t) âˆ’ xâˆ—(t1) âˆ’ R tt1 D(x(s)) Â· Î¾ds = ( t âˆ’ t1) Â· âˆ†âˆ.Thus, our objective is to determine the optimal Î¾ such that the error remains at the order of O(âˆ†) .However, we will not further discuss this limitation in this paper. On the other hand, Tran and Ward [ 52 ]proved the exact recovery of chaotic behavior from an â„“1 minimization problem under this reformulation. To overcome such limitations, several studies have proposed methods that avoid the direct computation of numerical derivatives. One notable approach is the weak formulation, which led to the development of Weak SINDy [ 43 , 44 ]. By introducing test functions, this framework provides greater flexibility when dealing with noisy or sparse datasets. Another method, RK4-SINDy [ 53 ], incorporates the structure of the Rungeâ€“Kutta integration scheme to bypass derivative estimation altogether, embedding finite difference constraints directly into the model identification process. These derivative-free or derivative-relaxed approaches reveal a more fundamental learning limitation. Given a prescribed dictionary, learning the coefficients amounts to approximating the projection PD Ë™x of the true derivatives onto the span of the dictionary, rather than recovering Ë™x itself. To proceed with our analysis under this viewpoint, we make the following assumption: 

Assumption 2. Along the notation in the second paragraph in Introduction, the system z = z(t) governed by Ë™z(t) = Pni=1 ËœÎ¾idi(z(t)) does not blow-up in finite-time. 

# B Recovery of dynamics 

In this section, we study how, in the low-score regime, one can bound the difference between the trajectories generated by the time derivative projected onto the full dictionary and those generated by its projection onto the complement of a low-score sub-dictionary. In this section, we use perturbation theory to derive a theoretical bound on phase differences in relation to the score. Suppose the state space is compact in an ambient Euclidean space. According to the classical ODE theory, the perturbed trajectory xÎµ(t), governed by Ë™x = f (x) + Îµg (x) and xÎµ(t0) = x0, from x0(t),governed by Ë™x = f (x) and x0(t0) = x0, could be described in an explicit form. KaszÃ¡s and Haller [ 54 ]found that for any Î´ > 0, there exists Îµ0 > 0, such that for Îµ < Îµ 0,

|xÎµ(t) âˆ’ x0(t)| â‰¤ Îµ

Z t

> 0

Î›ts(x0(s)) ds + Î´



âˆ¥g(x0(Â·)) âˆ¥L2([ t0,t ])] , t â‰¥ t0, (B.1) where Î›ts(x0(s)) is the maximal eigenvalue of the Gram matrix of the fundamental matrix Î¦( t; s, x 0(s)) to the dynamics Ë™x = f (x).3

Let U âŠ‚ Rd be a region, and let h : U â†’ U be a continuously differentiable and bounded vector field. Consider a system given by Ë™xâˆ—(t) = h(xâˆ—(t)) on 0 â‰¤ t â‰¤ T with the initial condition xâˆ—(0) = p âˆˆ U ,and suppose xâˆ—(t) âˆˆ U for all t âˆˆ [0 , T ]. Let D = {d1, . . . , d n} âŠ‚ C (U ) be a dictionary of scalar-valued continuous functions on U . For each d âˆˆ D , we assume that the function is normalized along  

> 3The original statement uses the maximum norm of g(x0(Â·)) .

22 the trajectory {xâˆ—(t) : 0 â‰¤ t â‰¤ T }, meaning that âˆ¥d(xâˆ—(Â·)) âˆ¥L2[0 ,T ] = 1 for any d âˆˆ D . We denote by 

D(x) = {d1 â—¦ x, . . . , d n â—¦ x}, the set of scalar functions obtained by evaluating the dictionary elements along a trajectory x : [0 , T ] â†’ U . So, D(x) âŠ‚ L2[0 , T ]. Suppose the dictionary D(xâˆ—) approximates h well (along the trajectory xâˆ—, in the sense that âˆ¥h â—¦ xâˆ— âˆ’ P D(xâˆ—)(h â—¦ xâˆ—)âˆ¥L2[0 ,T ] â‰ª 1.Denote Dsub := D \ { di1 , . . . , d iâ„“ } âŠ‚ D , where dn âˆˆ D is a specific dictionary element. Let 

PD(xâˆ—)(h â—¦ xâˆ—) = 

> n

X

> k=1

c(1)  

> k

dk â—¦ xâˆ—, PDsub (xâˆ—)(h â—¦ xâˆ—) = 

> â„“

X

> k=1

c(2)  

> ik

dik â—¦ xâˆ—.

Consider the following two systems: 

Ë™x1(t) = 

> n

X

> k=1

c(1)  

> k

dk(x1(t)) =: F1(x1(t)) ,

Ë™x2(t) = 

> â„“

X

> k=1

c(2)  

> ik

dik (x2(t)) =: F2(x2(t)) , 0 â‰¤ t â‰¤ T, x1(0) = x2(0) = p âˆˆ U. 

Assume that the system governing x2 is an Îµ-perturbed version of the system governing x1. That is, let 

f = F1 and f + Îµg = F2, where âˆ¥g â—¦ xâˆ—âˆ¥L2[0 ,T ] = 1 , following the notation from the previous paragraph. 4

This situation is typical in the SINDy framework: coefficients corresponding to important terms (i.e., the survivors) remain relatively stable across iterations, while non-essential termsâ€“those that will be eliminatedâ€“tend to have small coefficients and are more sensitive to perturbations. Then, (B.1) implies that 

|x1(t) âˆ’ x2(t)| â‰¤ 

Z t

> 0

Î›ts(x1(s)) ds + Î´



âˆ¥g â—¦ x1âˆ¥L2[0 ,T ], 0 â‰¤ t â‰¤ T, 

provided that Îµ is sufficiently small. The right factor in the right-hand-side approximates âˆ¥g â—¦ xâˆ—âˆ¥L2[0 ,T ] = 1 

if x1 approximates xâˆ—.5 According to the result in Ref. [ 54 ], one may numerically choose Î´ = 0 , and the resulting integral does exhibit tighter than exponential growth. Therefore, we may argue that the trajectory x2, governed by the sparser system, remains close to the trajectory x1, with a theoretical bound on the difference. On the other hand, the assumption 

âˆ¥(PD(xâˆ—) âˆ’ P Dsub (xâˆ—))( h â—¦ xâˆ—)âˆ¥L2[0 ,T ] = âˆ¥F1 â—¦ xâˆ— âˆ’ F2 â—¦ xâˆ—âˆ¥L2[0 ,T ] = Îµ â‰ª 1

may be achieved when the dictionary element dn has a small score in the discretized setting. Therefore, to obtain a theoretical bound on the data-fitting error with a sparser sub-dictionary, one may remove the dictionary element with the lowest score. 

# C Collection of Proofs 

In this appendix, we list the proofs for all propositions in the present work. We will use the following lemma without mentioning throughout this section. 

Lemma 4 ([ 55 ] Theorem 8) . Let PD1 and PD2 be projections on L2(M). P = PD1 âˆ’ P D2 is projection if and only if span (D1) âŠ‚ span (D2). Then P : L 2(M) â†’ L, where span (D2) = span (D1) âŠ• L, namely L is the orthogonal complement of span( D1) in span( D2).              

> 4In the referenced work [ 54 ], the restriction âˆ¥gâ—¦xâˆ—âˆ¥L2[0 ,T ]= 1 is not explicitly imposed. To the best of the authorsâ€™ knowledge, the essential condition is that the perturbation term gis independent of the parameter Îµ > 0. Therefore, our normalization assumption is harmless and does not affect the generality of the analysis.
> 5To simplify the analysis, one may assume that hâˆˆspan( D).

23 C.1 Proof for Lemma 1 

## C.1 Proof for Lemma 1 

Proof. This can be deduced from the inverse formula of a partitioned matrix (e.g. Appendix C.3.5 [ 16 ]). Here, we provide another proof. We fix an index i0 âˆˆ [n]. We swap the i0th column to the last position in D, denoting the resulting matrix as Ë†D = [ Ë†d1| Â· Â· Â· | Ë†dn]. Then, 

P Ë†D y = Ë†D Ë†Dâ€ y =

> n

X

> i=1

[ Ë†Dâ€ y]i Ë†di. (C.1) The orthonormalization of (Ë†d1, . . . , Ë†dn) generates the orthogonal matrix 

Ë†U = [ u1| Â· Â· Â· | un]

where u1 = Ë†d1/âˆ¥Ë†d1âˆ¥2, ui = ( Ë†di âˆ’ fi( Ë†d1, . . . , Ë†diâˆ’1)) /âˆ¥Ë†di âˆ’ fi( Ë†d1, . . . , Ë†diâˆ’1)âˆ¥2, and fi( Ë†d1, . . . , Ë†diâˆ’1) = 

P Ë†U{1,...,i âˆ’1} di for i = 2 , . . . , n . We observe that 

P Ë†U y = Ë†U Ë†U â€ y =

> nâˆ’1

X

> i=1

[ Ë†U â€ y]iui + [ Ë†U â€ y]nun =

> nâˆ’1

X

> i=1

ci Ë†di + [ Ë†U â€ y]n

âˆ¥Ë†dn âˆ’ fn(Ë†d1, . . . , Ë†dnâˆ’1)âˆ¥

Ë†dn, (C.2) for some sequence (ci)nâˆ’1 

> i=1

. We recall the geometric property of the orthonormalization, P Ë†D â‰¡ P Ë†U .Thus, from (C.1) and (C.2) , we have [ Ë†Dâ€ y]n = [ Ë†U â€ y]n/âˆ¥Ë†dn âˆ’ fn( Ë†d1, . . . , Ë†dnâˆ’1)âˆ¥ since Ë†di are linearly independent. 6 Recalling that Ë†D was generated by swapping columns, we have âˆ¥Ë†dn âˆ’ fn( Ë†d1, . . . , Ë†dnâˆ’1)âˆ¥ =

âˆ¥di0 âˆ’ P D[n]\{ i0} di0 âˆ¥ and also the following 

|[ Ë†U â€ y]n| = âˆ¥[ Ë†U â€ y]nunâˆ¥ = âˆ¥P un yâˆ¥ = âˆ¥(P Ë†U âˆ’ P Ë†U[n]\{ n} )yâˆ¥

= âˆ¥(P Ë†D âˆ’ P Ë†D[n]\{ n} )yâˆ¥ = âˆ¥(PD âˆ’ P D[n]\{ i0} )yâˆ¥.

Combining altogether, we have the conclusion. â– 

## C.2 Proof for Lemma 3 

Proof. From the trivial relationship span( D \ Dsub ) âŠ‚ span( D), we have 

PD PD\Dsub = PD\Dsub PD = PD\Dsub 

and so the map PD âˆ’ P D\Dsub is a projection. Since the operator norm of a projection map is less than 1,we have Score (Dsub ; D) â‰¤ PD âˆ’ P D\Dsub op â‰¤ 1.

â– 

## C.3 Proof for Proposition 2 

Proof. Suppose the sub-dictionary score is sufficiently small, Score( Dsub ; D) < Î»Ï‰/ âˆ¥yâˆ¥2. Then, 

Score( di; D) (3.3) 

â‰¤ Score( Dsub ; D) â‰¤ Î»Ï‰/ âˆ¥yâˆ¥2. (C.3) The condition (4.2) and the above (C.3) imply (2.2): 

âˆ¥yâˆ¥2 Score( di; D) < Î» âˆ¥di âˆ’ P D[n]\{ i} diâˆ¥2.

â– 

> 6

According to the formula for pseudo-inverse matrices of block matrices, it is the same as the following: [ Ë†Dâ€ y]n =[PâŠ¥

> Ë†Dsub

dn]â€ y = [( I âˆ’ P Ë†Dsub )dn]â€ y = [dnâˆ’(P Ë†Dsub dn)] T y   

> âˆ¥dnâˆ’(PË†Dsub dn)âˆ¥2

.

24 C.4 Proof for Proposition 3 

## C.4 Proof for Proposition 3 

Proof. Without loss of generality, let S0 = {1, 2, . . . , k }. By definition of S0, |ci| â‰¥ Î»1 for i â‰¤ k and 

|ci| < Î» 2 for i â‰¥ k + 1 . Our claim is to show Anâˆ’k = {dk+1 , . . . , d n}. In other words, 

Score( D[n]\S0 ; D) â‰¤ Score( D[n]\T ; D)

for T âŠ‚ [n], |T | = k, T âˆ© S0Ì¸ = âˆ…, T âˆ© ([ n] \ S0)Ì¸ = âˆ…. (C.4) Fix such T with |T âˆ© S0| = â„“. We compute the denominators of the scores in (C.4) through 

PDS0 y =

> k

X

> i=1

cidi +

> n

X

> i=k+1

ciPD[n]\S0 di + PD[n]\S0âƒ—e, 

PD y âˆ’ P DS0 y =

> n

X

> i=k+1

ci(I âˆ’ P D[n]\S0 )di âˆ’ P D[n]\S0âƒ—e. 

We obtain 

âˆ¥P D y âˆ’ P DS0 yâˆ¥2 â‰¤ (n âˆ’ k)Î»2 + Îµ. (C.5) Second, we compute 

PD[n]\T y = X

> iâˆˆ[n]\T

cidi + X

> iâˆˆT

ciP[n]\T di + PD[n]\Tâƒ—e

PD y âˆ’ P D[n]\T y = X

> iâˆˆT

cidi âˆ’ X

> iâˆˆT

ciP[n]\T di âˆ’ P D[n]\Tâƒ—e

= X 

> iâˆˆTâˆ©S0

ci(I âˆ’ P [n]\T )di + X 

> iâˆˆT\S0

ciPspan( D[n]\T )âŠ¥ di âˆ’ P D[n]\Tâƒ—e. 

Thus, we have 

âˆ¥P D y âˆ’ P D[n]\T yâˆ¥2 â‰¥ âˆ¥ (I âˆ’ PD[n]\T )DT âˆ©S0âƒ—c T âˆ©S0 âˆ¥2 âˆ’ (n âˆ’ k âˆ’ â„“)Î»2 âˆ’ Îµ

â‰¥ pR( G,âƒ— c T âˆ©S0 )âˆ¥âƒ—c T âˆ©S0 âˆ¥2 âˆ’ (n âˆ’ k âˆ’ â„“)Î»2 âˆ’ Îµ

â‰¥ â„“pR( G,âƒ— c T âˆ©S0 )Î»1 âˆ’ (n âˆ’ k âˆ’ â„“)Î»2 âˆ’ Îµ

(C.6) where G is the Gram matrix DTT âˆ©S0 (I âˆ’ PD[n]\T )DT âˆ©S0 , and R(G,âƒ— c T âˆ©S0 ) is the Rayleigh quotient for the matrix G and the vector âƒ—c T âˆ©S0 . Combining the condition (4.3) , estimate (C.4) and (C.6) , we prove (C.4). â– 

## C.5 Proof for Theorem 1 

Proof. We use the mathematical induction in n = 0 , . . . , k âˆ’ 1.(n = 1 ) We show dj0 âˆˆ [n]\S0. Suppose not. Then, |cj0 | â‰¥ Î»1. Recalling the condition (4.4) (; Î»1Ï‰ â‰¥ Î»2)and the fact Ï‰i â‰¤ 1, we have 

|cj0 |Ï‰j0 â‰¥ Î»1Ï‰ â‰¥ Î»2 â‰¥ Î»2Ï‰i, âˆ€ i = 1 , . . . , n. 

On the other hand, score is determined by |ci|Ï‰i thanks to Lemma 1. Thus, we have the contradiction to the minimal condition of dj0 in (GBSR). (n = m â‰¤ k âˆ’ 1) Assume that {dj0 , . . . , djmâˆ’1 } âŠ‚ [n] \ S0. Our claim is to show djm âˆˆ [n] \ S0. Suppose not. By the construction (GBSR), we have 

Score( D{j0,...,j m}; D) â‰¤ Score( D{j0,...,j mâˆ’1,i 0}; D), i0 âˆˆ [n] \ { j0, . . . , j mâˆ’1, j m}. (C.7) On the other hand, for im âˆˆ [n] \ (S0 âˆª { j0, . . . , j mâˆ’1}) and Im := {j0, . . . , j mâˆ’1, i m},

PD[n]\Im y = X

> iâˆˆ[n]\Im

cidi + X

> iâˆˆIm

ciPDIm di + PD[n]\Imâƒ—e, 

25 PD y âˆ’ P D[n]\Im y = X

> iâˆˆIm

ci(I âˆ’ P D[n]\Im )di âˆ’ P D[n]\Imâƒ—e. 

These directly yield the following: 

âˆ¥P D y âˆ’ P DIm yâˆ¥2 â‰¤ (m + 1) Î»2 + Îµ (C.8) On the other hand, projections to the sub-dictionary generated by Jm give us 

PDJm y = X

> iâˆˆJm

cidi + X

> iâˆˆ[n]\Jm

ciPDJm di + PDJmâƒ—e, 

PD y âˆ’ P DJm y = X

> iâˆˆ[n]\Jm

cidi âˆ’ X

> iâˆˆ[n]\Jm

ciPDJm di âˆ’ P DJmâƒ—e, 

= cjm (I âˆ’ P DJm )djm + X

> iâˆˆ[n]\Jmâˆ’1

ciPspan( DJm )âŠ¥ di âˆ’ P DJmâƒ—e. 

So, we obtain 

Î»1âˆ¥(I âˆ’ P DJm )djm âˆ¥ âˆ’ mÎ» 2 âˆ’ Îµ â‰¤ âˆ¥P D y âˆ’ P DJm yâˆ¥2. (C.9) Note that the condition (4.4) 1 implies âˆ¥(I âˆ’ P DJm )djm âˆ¥ > Ï‰ , since span (DJm ) âŠ‚ span (D[n]\{ jm}).Together with the estimates (C.8) and (C.9), and the condition (4.4) 2, we obtain 

âˆ¥P D y âˆ’ P DIm yâˆ¥2 < âˆ¥P D y âˆ’ P DJm yâˆ¥2.

This contradicts the minimality assumption (C.7). We showed that {j0, . . . , j kâˆ’1} = [ n] \ S0. The conclusion comes from Proposition 3. â– 

# D Literature 

## D.1 Z-SINDy 

Z-SINDy defines the free energy for each sub-dictionary and finds the minimal energy regime, using Bayesian inference within a statistical mechanical approach to sparse equation discovery [ 21 ]. In this subsection, we see a similarity between two minimization problems. Z-SINDy computes the free energy FÎ³ of a sub-dictionary Dsub = Î³FÎ³ = âˆ’ log Z0 âˆ’ |Î³|

2 log(2 Ï€Ï 2) + 12 log det CÎ³ âˆ’ 12Ï2âƒ— V TÎ³ Câˆ’1

> Î³

âƒ— VÎ³ + Î» T

âˆ†t |Î³|

where Z0 is the evidence of an empty set, Ï is the noise resolution, matrix of dictionary VÎ³ = DTsub y and empirical correlation matrix CÎ³ = DTsub Dsub . Here, we solve the following minimization problem: arg min  

> Î³

FÎ³ = arg max  

> Dsub âŠ‚D

yT PDsub y âˆ’ c1|Dsub | âˆ’ c2 log(det CÎ³ )= arg min  

> Dsub âŠ‚D

âˆ¥y âˆ’ P D\Dsub yâˆ¥2 + c1|Dsub | + c2 log(det CÎ³ ) (D.1) Recall the tradeoff to sparsity in Subsection 4.4. Suppose y âˆˆ span (D(x)) . Then, the minimization problem is 

min  

> Dsub âŠ‚D

(PD(x) âˆ’ P Dâˆ’D sub (x))y + Î»|D \ D sub |

= min  

> Dsub âŠ‚D

y âˆ’ P Dâˆ’D sub (x)y 2 + Î»|D \ D sub | = max 

> S=Dâˆ’D sub

yT PS y + Î»|S|

where we used PDâˆ’D sub (x) = P2 

> Dâˆ’D sub (x)

= PT 

> Dâˆ’D sub (x)

and ignored constant terms. Since PS = SS â€ , we have 

max 

> S=Dâˆ’D sub

yT SS â€ y + Î»|S| = max 

> S=Dâˆ’D sub

(ST y)T (ST S)âˆ’1ST y + Î»|S|.

26 D.2 D-SINDy 

## D.2 D-SINDy 

In the paper [ 27 ], the authors combined two algorithms; Projection based State Denoising (PSDN) and Iteratively reweighted second order cone program (IRW-SOCP). While our method deletes dictionary items with minimal scores each iteration, D-SINDy imposes a weight for each item along its importance. The authors use an architecture similar to iteratively reweighted Lasso (IRW-Lasso), which solves the Lasso â„“1-regularization problem: minimize câˆ¥Ac âˆ’ yâˆ¥2 + Î»âˆ¥W câˆ¥1.

Here, W is a diagonal matrix such that Wii = ( |ci| + Îµ)âˆ’1 and c is the coefficients vector obtained in the previous iteration. In D-SINDy, their weight vector is c = PD y. In other words, it emphasizes smallness of coefficients of projected vectors. 

## D.3 Orthogonal Matching Pursuit 

The Matching Pursuit (MP) is a sparse approximation algorithm [ 56 ]. The Orthogonal Matching Pursuit (OMP) is a well-known extension of MP [ 57 ]. For a dictionary matrix D = [ d1| Â· Â· Â· | dn] which is normalized columnwise and a target signal y, we want to find sparse coefficient vector x which is the best matching projections of y onto the span of dictionary. For a threshold Î´ > 0, it detects subindex sets Sk and a sequence (xk)

R0 = y, S0 = âˆ…,Î³k = argmax 

> i / âˆˆSkâˆ’1

|âŸ¨ di, R kâˆ’1âŸ©| , Sk = Skâˆ’1 âˆª { Î³k}, xk = argmin 

> supp( x)âŠ‚Sk

âˆ¥Dx âˆ’ yâˆ¥2,Rk = y âˆ’ Dx k, k = 1 , . . . , min  

> i

{i : |Ri| < Î´ }.

At each iteration step, it finds the maximum of the following for a greedy searching 

|âŸ¨ di, R kâŸ©| = |âŸ¨ di, y âˆ’ P DSk yâŸ©| = |âŸ¨ di, (I âˆ’ P DSk )yâŸ©| = |âŸ¨ (I âˆ’ P DSk )di, y âŸ©| (D.2) where we used the projection property (I âˆ’ P DSk )T = I âˆ’ P DSk . Also, 

|âŸ¨ di, R kâŸ©| = âˆ¥(PDSk âˆª{ i} âˆ’ P DSk )yâˆ¥2âˆ¥(I âˆ’ P DSk )diâˆ¥2

= Score( di; DSk âˆª{ i})âˆ¥yâˆ¥2âˆ¥(I âˆ’ P DSk )diâˆ¥2

(D.3) where we used Lemma 1 and DSk âˆª{ i} is a super-matrix DSk and its last column is di. Precisely, from the block matrix pseudoinverse formula, we have 

[PDSk âˆª{ i} y]k+1 = ( P âŠ¥ 

> DSk

di)â€ y = |âŸ¨ (I âˆ’ P DSk )di, y âŸ©| âˆ¥di âˆ’ PDSk diâˆ¥22

. (D.4) We combine (D.2) and (D.4) to obtain (D.5), which implies (D.3). 

Score( di; DSk âˆª{ i})âˆ¥yâˆ¥2

âˆ¥di âˆ’ P DSk diâˆ¥2

= [PDSk âˆª{ i} y]k+1 = (P âŠ¥ 

> DSk

di)T y

âˆ¥di âˆ’ PDSk diâˆ¥22

. (D.5) As a comparison, if the score of an item is small at each iteration for OMP, then OMP may not take the item (provided that mutual incoherences between dictionary items are similar). 

## D.4 Screening process 

This procedure addresses the case with many variables whose numbers exceed the number of sampling points, safely discarding variables to facilitate speed for the LASSO problem. Several methods have been developed such as Sure Independence Screening which selects the top features most correlated with the target variable [ 58 , 59 ] and SAfe Feature Elimination (SAFE) which safely discards features guaranteed to 27 0 2 4 6 8 10              

> Noise Level (%)
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> Coefficient Error
> Coefficient Error: WSINDy Filtered
> WSINDy Full lib
> WSINDy Filtered (27% of terms)
> WSINDy Filtered (50% of terms)
> WSINDy Filtered (75% of terms)
> 0246810
> Noise Level (%)
> Coefficient Error: WSINDy Filtered (y-coordinate)
> WSINDy Full lib
> WSINDy Filtered (25% of terms)
> WSINDy Filtered (50% of terms)
> WSINDy Filtered (75% of terms)

Figure 10: Coefficient errors of STLS results over filtered dictionaries in the weak formulation at each noise level. The left figure presents results based on filtered sub-dictionaries that use all coordinate scores; that is, the removal order is determined by the sum of scores across coordinates. The right figure shows the result based on the y coordinate only. have zero coefficients in the LASSO solution, based on a conservative bound involving the regularization parameter [60]. In the present paper, we do not use their method, but use the structure of the schemes. As an application of scoring, we propose a preprocessing step of STLS which (unsafely) discard dictionary items in advance but safely under certain conditions. We tested whether the filtering approach can aid in constructing a base dictionary for STLS. In this context, we treat the filtered dictionary terms as the base dictionary within the weak formulation of STLSâ€”a process analogous to a screening step. A natural question arises: â€œHow many terms should be filtered out to improve upon standard STLS?â€ We provide a partial numerical answer to this question and also discuss the limitations of our screening-like strategy. In Figure 10, we compare WSINDy results with dictionaries filtered n percents by scoring from the based dictionary. Power of scoring in noise test appears when we filter more than half amount of items over the base dictionary. On the other hand, the other cases do not this is because the significant variables appearing uncorrelated until conditioned on other variables. 

# E Greedy Forward Stepwise Regressor 

In this section, we numerically show that GBSR is better than GFSR, which is given as follows: 

j0 

> f

= argmin 

> iâˆˆ[m]

Score (D[n]\{ i}; D, y ), J0 

> f

= {j0 

> f

},jif = argmin 

> â„“Ì¸âˆˆ Jiâˆ’1
> f

Score (D[n]\(Jiâˆ’1 

> fâˆª{ â„“})

; D, y ), Jif = Jiâˆ’1 

> f

âˆª { jif }, i = 1 , . . . , n. (GFSR) A natural question arises: "Why remove items from a library instead of building it up incrementally?" Given the assumption that the underlying system is sparse, a removal-based approach (i.e., filtering) could, in principle, require more iterations. In our numerical experiments, we tested a forward selection strategy using our scoring method. However, as shown in Figure 11 for the Lorenz system, the initial selection is often random and highly prone to error, leading to poor identification of the correct terms. One potential solution to this problem is to begin the process with an exhaustive search over com-binations of multiple terms. However, this strategy introduces complications for systems in which some equations contain only a single relevant termâ€”such cases would be missed by multiâ€“term initialization. 28 REFERENCES y2

> 1
> xz 2
> y
> x
> xz
> z
> xy
> xyz
> y2z
> y3
> cos (1 y)
> cos (1 x)
> cos (2 x)
> sin (2 y)
> sin (2 z)
> sin (1 y)
> cos (1 z)
> sin (1 x)
> cos (2 z)
> sin (1 z)
> cos (2 y)
> sin (2 x)
> x2z
> z2
> z3
> x3
> xy 2
> yz
> x2y
> yz 2
> x2
> Dictionary items
> 0
> 5
> 10
> 15
> 20
> 25
> 30
> Filtering order (0, 1, 2, 3, ...)
> Score history (White: filtered)
> 1.000
> 0.000
> 0.000
> 0.001
> 0.001
> 0.005
> 0.010
> 0.050
> 0.100
> 0.500
> Score

Figure 11: Ranking the library terms for Lorenz system (1) using the score (E) . In the very first selection step, an incorrect term is chosen, which compromises the remainder of the selection process. 

# References 

[1] Ã…. BjÃ¶rck, Numerical Methods for Least Squares Problems, Society for Industrial and Applied Mathematics, 1996. doi:10.1137/1.9781611971484 .[2] S. Mallat, A wavelet tour of signal processing, Elsevier, 1999. [3] M. J. Wainwright, High-dimensional statistics: A non-asymptotic viewpoint, Vol. 48, Cambridge university press, 2019. [4] D. Donoho, Compressed sensing, IEEE Transactions on Information Theory 52 (4) (2006) 1289â€“1306. 

doi:10.1109/TIT.2006.871582 .[5] E. J. CandÃ¨s, J. K. Romberg, T. Tao, Stable signal recovery from incomplete and inaccurate measurements, Communications on Pure and Applied Mathematics 59 (8) (2006) 1207â€“1223. doi: https://doi.org/10.1002/cpa.20124 .[6] J. A. Tropp, Just relax: Convex programming methods for identifying sparse signals in noise, IEEE transactions on information theory 52 (3) (2006) 1030â€“1051. [7] R. Tibshirani, Regression shrinkage and selection via the lasso, Journal of the Royal Statistical Society Series B: Statistical Methodology 58 (1) (1996) 267â€“288. [8] J. Bongard, H. Lipson, Automated reverse engineering of nonlinear dynamical systems, Proceedings of the National Academy of Sciences 104 (24) (2007) 9943â€“9948. doi:10.1073/pnas.0609476104 .[9] M. Schmidt, H. Lipson, Distilling free-form natural laws from experimental data, science 324 (5923) (2009) 81â€“85. [10] W.-X. Wang, R. Yang, Y.-C. Lai, V. Kovanis, C. Grebogi, Predicting catastrophes in nonlinear dynamical systems by compressive sensing, Phys. Rev. Lett. 106 (2011) 154101. doi:10.1103/ PhysRevLett.106.154101 .[11] M. Naik, D. Cochran, Nonlinear system identification using compressed sensing, in: 2012 Conference Record of the Forty Sixth Asilomar Conference on Signals, Systems and Computers (ASILOMAR), 2012, pp. 426â€“430. doi:10.1109/ACSSC.2012.6489039 .[12] S. L. Brunton, J. L. Proctor, J. N. Kutz, Discovering governing equations from data by sparse identification of nonlinear dynamical systems, Proceedings of the National Academy of Sciences 113 (15) (2016) 3932â€“3937. doi:10.1073/pnas.1517384113 .29 REFERENCES 

[13] L. Zhang, H. Schaeffer, On the convergence of the sindy algorithm, Multiscale Modeling & Simulation 17 (3) (2019) 948â€“972. doi:10.1137/18M1189828 .[14] W. Su, M. Bogdan, E. Candes, False discoveries occur early on the lasso path, The Annals of statistics (2017) 2133â€“2150. [15] P. Zheng, T. Askham, S. L. Brunton, J. N. Kutz, A. Y. Aravkin, A unified framework for sparse relaxed regularized regression: Sr3, IEEE Access 7 (2018) 1404â€“1423. [16] D. C. Montgomery, E. A. Peck, G. G. Vining, Introduction to linear regression analysis, John Wiley & Sons, 2021. [17] L. Boninsegna, F. NÃ¼ske, C. Clementi, Sparse learning of stochastic dynamical equations, The Journal of Chemical Physics 148 (24) (Mar. 2018). doi:10.1063/1.5018409 .[18] A. Ghadami, B. I. Epureanu, Data-driven prediction in dynamical systems: recent developments, Philosophical Transactions of the Royal Society A 380 (2229) (2022) 20210213. [19] S. L. Brunton, J. N. Kutz, Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control, Cambridge University Press, 2019. doi:10.1017/9781108380690 .[20] J. J. Bramburger, Data-Driven Methods for Dynamic Systems, Society for Industrial and Applied Mathematics, 2024. doi:10.1137/1.9781611978162 .[21] A. A. Klishin, J. Bakarji, J. N. Kutz, K. Manohar, Statistical mechanics of dynamical system identification, Phys. Rev. Res. 7 (2025) 033181. doi:10.1103/4d98-tdlp .[22] U. Fasel, J. N. Kutz, B. W. Brunton, S. L. Brunton, Ensemble-sindy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control, Proc. R. Soc. A 478 (20210904) (2022). doi:10.1098/rspa.2021.0904 .[23] L. Gao, U. Fasel, S. L. Brunton, J. N. Kutz, Convergence of uncertainty estimates in ensemble and bayesian sparse model discovery, arXiv preprint arXiv:2301.12649 (2023). [24] L. Fung, U. Fasel, M. Juniper, Rapid bayesian identification of sparse nonlinear dynamics from scarce and noisy data, in: Proceedings A, Vol. 481, The Royal Society, 2025, p. 20240200. [25] N. M. Mangan, J. N. Kutz, S. L. Brunton, J. L. Proctor, Model selection for dynamical systems via sparse regression and information criteria, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 473 (2204) (2017) 20170009. [26] J. M. Hokanson, G. Iaccarino, A. Doostan, Simultaneous identification and denoising of dynamical sys-tems, SIAM Journal on Scientific Computing 45 (4) (2023) A1413â€“A1437. doi:10.1137/22M1486303 .[27] J. Wentz, A. Doostan, Derivative-based sindy (dsindy): Addressing the challenge of discovering governing equations from noisy data, Computer Methods in Applied Mechanics and Engineering 413 (2023) 116096. doi:https://doi.org/10.1016/j.cma.2023.116096 .[28] K. Champion, P. Zheng, A. Y. Aravkin, S. L. Brunton, J. N. Kutz, A unified sparse optimization framework to learn parsimonious physics-informed models from data, IEEE Access 8 (2020) 169259â€“ 169271. doi:10.1109/ACCESS.2020.3023625 .[29] A. Carderera, S. Pokutta, C. SchÃ¼tte, M. Weiser, Cindy: Conditional gradient-based identification of non-linear dynamicsâ€“noise-robust recovery, Journal of Computational and Applied Mathematics (2021). [30] S. Viknesh, Y. Tatari, A. Arzani, Adam-sindy: An efficient optimization framework for parameterized nonlinear dynamical system identification, arXiv preprint arXiv:2410.16528 (2024). [31] G. T. Naozuka, H. L. Rocha, R. S. Silva, R. C. Almeida, Sindy-sa framework: enhancing nonlinear system identification with sensitivity analysis, Nonlinear Dynamics 110 (3) (2022) 2589â€“2609. 30 REFERENCES 

[32] Z. G. Nicolaou, G. Huo, Y. Chen, S. L. Brunton, J. N. Kutz, Data-driven discovery and extrapolation of parameterized pattern-forming dynamics, Phys. Rev. Res. 5 (2023) L042017. doi:10.1103/ PhysRevResearch.5.L042017 .[33] U. Fasel, E. Kaiser, J. N. Kutz, B. W. Brunton, S. L. Brunton, Sindy with control: A tutorial, in: 2021 60th IEEE Conference on Decision and Control (CDC), 2021, pp. 16â€“21. doi:10.1109/CDC45484. 2021.9683120 .[34] K. Kaheman, J. N. Kutz, S. L. Brunton, Sindy-pi: a robust algorithm for parallel implicit sparse identification of nonlinear dynamics, Proceedings of the Royal Society A 476 (2242) (2020) 20200279. [35] D. E. Shea, S. L. Brunton, J. N. Kutz, Sindy-bvp: Sparse identification of nonlinear dynamics for boundary value problems, Physical Review Research 3 (2) (2021) 023255. [36] M. Wanner, I. MeziÄ‡, On higher order drift and diffusion estimates for stochastic sindy, SIAM Journal on Applied Dynamical Systems 23 (2) (2024) 1504â€“1539. doi:10.1137/23M1567011 .[37] M. Jacobs, B. W. Brunton, S. Brunton, J. N. Kutz, R. V. Raut, Hypersindy: Deep generative modeling of nonlinear stochastic governing equations (2024). URL https://openreview.net/forum?id=B4XM9nQ8Ns 

[38] J. J. Bramburger, J. N. Kutz, PoincarÃ© maps for multiscale physics discovery and nonlinear floquet theory, Physica D: Nonlinear Phenomena 408 (2020) 132479. doi:10.1016/j.physd.2020.132479 .[39] N. Zolman, C. Lagemann, U. Fasel, J. N. Kutz, S. L. Brunton, SINDy-RL for interpretable and efficient model-based reinforcement learning, Nature Communications 16 (1) (2025) 10714. doi: 10.1038/s41467-025-65738-4 .[40] K. Champion, B. Lusch, J. N. Kutz, S. L. Brunton, Data-driven discovery of coordinates and governing equations, Proceedings of the National Academy of Sciences 116 (45) (2019) 22445â€“22451. 

doi:10.1073/pnas.1906995116 .[41] S. Foucart, H. Rauhut, A Mathematical Introduction to Compressive Sensing, Springer New York, 2013. doi:10.1007/978-0-8176-4948-7 .[42] E. Van Den Berg, M. P. Friedlander, Probing the pareto frontier for basis pursuit solutions, Siam journal on scientific computing 31 (2) (2009) 890â€“912. [43] D. A. Messenger, D. M. Bortz, Weak sindy: Galerkin-based data-driven model selection, Multiscale Modeling & Simulation 19 (3) (2021) 1474â€“1497. doi:10.1137/20M1343166 .[44] B. P. Russo, M. P. Laiu, Convergence of weak-sindy surrogate models, SIAM Journal on Applied Dynamical Systems 23 (2) (2024) 1017â€“1051. doi:10.1137/22M1526782 .[45] B. P. Russo, M. P. Laiu, R. Archibald, Streaming compression of scientific data via weak-sindy, SIAM Journal on Scientific Computing 47 (1) (2025) C207â€“C234. doi:10.1137/23M1599331 .[46] D. Messenger, A. Tran, V. Dukic, D. Bortz, The weak form is stronger than you think, SIAM News 57 (8) (2024). [47] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven discovery of partial differential equations, Science Advances 3 (4) (2017) e1602614. doi:10.1126/sciadv.1602614 .[48] D. A. Messenger, D. M. Bortz, Weak sindy for partial differential equations, Journal of Computational Physics 443 (2021) 110525. doi:https://doi.org/10.1016/j.jcp.2021.110525 .[49] C. M. Oishi, A. A. Kaptanoglu, J. N. Kutz, S. L. Brunton, Nonlinear parametric models of viscoelastic fluid flows, Royal Society Open Science 11 (10) (2024) 240995. 31 REFERENCES 

[50] B. Thomases, M. J. Shelley, Transition to mixing and oscillations in a stokesian viscoelastic flow., Physical review letters 103 9 (2009) 094501. URL https://api.semanticscholar.org/CorpusID:586707 

[51] T. Berry, S. Das, Limits of learning dynamical systems, SIAM Review 67 (1) (2025) 107â€“137. 

doi:10.1137/24m1696974 .[52] G. Tran, R. Ward, Exact recovery of chaotic systems from highly corrupted data, Multiscale Modeling & Simulation 15 (3) (2017) 1108â€“1129. doi:10.1137/16M1086637 .[53] P. Goyal, P. Benner, Discovery of nonlinear dynamical systems using a rungeâ€“kutta inspired dictionary-based sparse regression approach, Proceedings of the Royal Society A 478 (2262) (2022) 20210883. [54] B. KaszÃ¡s, G. Haller, Universal upper estimate for prediction errors under moderate model uncertainty, Chaos: An Interdisciplinary Journal of Nonlinear Science 30 (11) (2020) 113144. doi:10.1063/5. 0021665 .[55] S. Szedmak, R. Huusari, T. H. Duong Le, J. Rousu, Scalable variable selection for two-view learning tasks with projection operators, Machine Learning 113 (6) (2024) 3525â€“3544. [56] S. Mallat, Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE Transactions on Signal Processing 41 (12) (1993) 3397â€“3415. doi:10.1109/78.258082 .[57] Y. Pati, R. Rezaiifar, P. Krishnaprasad, Orthogonal matching pursuit: recursive function approxima-tion with applications to wavelet decomposition, in: Proceedings of 27th Asilomar Conference on Signals, Systems and Computers, 1993, pp. 40â€“44 vol.1. doi:10.1109/ACSSC.1993.342465 .[58] J. Fan, J. Lv, Sure independence screening for ultrahigh dimensional feature space, Journal of the Royal Statistical Society Series B: Statistical Methodology 70 (5) (2008) 849â€“911. [59] J. Fan, J. Lv, A selective overview of variable selection in high dimensional feature space, Statistica Sinica 20 (1) (2010) 101. [60] L. El Ghaoui, V. Viallon, T. Rabbani, Safe feature elimination in sparse supervised learning, Tech. Rep. UC/EECS-2010-126, EECS Dept., University of California at Berkeley (September 2010). 32