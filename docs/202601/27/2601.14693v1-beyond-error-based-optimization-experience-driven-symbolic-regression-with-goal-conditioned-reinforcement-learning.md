# Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning
# 超越基于误差的优化：基于目标条件强化学习的经验驱动符号回归

**Authors**: Jianwen Sun, Xinrui Li, Fuqing Li, Xiaoxuan Shen \
**Date**: 2026-01-21 \
**PDF**: https://arxiv.org/pdf/2601.14693v1 \
**Tags**: <span class="tag-label tag-green">SR</span> <span class="tag-label tag-blue">SR</span> \
**Score**: 10.0 \
**Evidence**: 基于强化学习的符号回归创新 \
**TLDR**: 提出一种目标条件强化学习框架，以提高符号回归的搜索效率。

---

## 速览
**TLDR**：提出 EGRL-SR 框架，通过目标条件强化学习和经验驱动策略，超越传统的误差驱动搜索，提升符号回归的准确性和鲁棒性。 \
**Motivation**：传统的基于误差的搜索方法在面对结构迥异但误差相似的表达式时，容易出现搜索方向模糊和收敛困难的问题。 \
**Method**：将符号回归建模为目标条件强化学习问题，利用事后经验回放、全点满足二值奖励函数及结构引导的启发式探索来优化动作价值网络。 \
**Result**：在公开基准测试中，EGRL-SR 在恢复率和鲁棒性上均优于现有最先进方法，且在相同搜索预算下能恢复更复杂的表达式。 \
**Conclusion**：实验证明动作价值网络能有效引导搜索，且所设计的奖励函数和探索策略对提升符号回归性能至关重要。

---


## 摘要
符号回归旨在自动识别紧凑且可解释的数学表达式，以建模输入和输出变量之间的函数关系。大多数现有的基于搜索的符号回归方法通常依赖拟合误差来指导搜索过程。然而，在广阔的表达式空间中，许多候选表达式可能表现出相似的误差值，但在结构上却存在巨大差异，这导致了模糊的搜索方向，并阻碍了向底层真实函数的收敛。为了应对这一挑战，我们提出了一个名为 EGRL-SR（经验驱动的目标条件强化学习符号回归）的新型框架。与传统的误差驱动方法不同，EGRL-SR 引入了一个新的视角：利用精确的历史轨迹并优化动作值网络（action-value network）来主动引导搜索过程，从而实现更稳健的表达式搜索。具体而言，我们将符号回归建模为一个目标条件强化学习问题，并结合了事后经验回放（hindsight experience replay），使动作值网络能够从多样的输入输出对中泛化出通用的映射模式。此外，我们设计了一个全点满足二元奖励函数，鼓励动作值网络关注结构模式而非低误差表达式，并同时提出了一种结构引导的启发式探索策略，以增强搜索的多样性和空间覆盖率。在公共基准测试上的实验表明，EGRL-SR 在恢复率和稳健性方面始终优于现有最先进的方法，并且在相同的搜索预算下可以恢复更复杂的表达式。消融实验结果验证了动作值网络能有效引导搜索，且奖励函数和探索策略都发挥了关键作用。
## Abstract
Symbolic Regression aims to automatically identify compact and interpretable mathematical expressions that model the functional relationship between input and output variables. Most existing search-based symbolic regression methods typically rely on the fitting error to inform the search process. However, in the vast expression space, numerous candidate expressions may exhibit similar error values while differing substantially in structure, leading to ambiguous search directions and hindering convergence to the underlying true function. To address this challenge, we propose a novel framework named EGRL-SR (Experience-driven Goal-conditioned Reinforcement Learning for Symbolic Regression). In contrast to traditional error-driven approaches, EGRL-SR introduces a new perspective: leveraging precise historical trajectories and optimizing the action-value network to proactively guide the search process, thereby achieving a more robust expression search. Specifically, we formulate symbolic regression as a goal-conditioned reinforcement learning problem and incorporate hindsight experience replay, allowing the action-value network to generalize common mapping patterns from diverse input-output pairs. Moreover, we design an all-point satisfaction binary reward function that encourages the action-value network to focus on structural patterns rather than low-error expressions, and concurrently propose a structure-guided heuristic exploration strategy to enhance search diversity and space coverage. Experiments on public benchmarks show that EGRL-SR consistently outperforms state-of-the-art methods in recovery rate and robustness, and can recover more complex expressions under the same search budget. Ablation results validate that the action-value network effectively guides the search, with both the reward function and the exploration strategy playing critical roles.
