Title: Symbolic Regression for Shared Expressions: Introducing Partial Parameter Sharing

URL Source: https://arxiv.org/pdf/2601.04051v1

Published Time: Thu, 08 Jan 2026 02:00:48 GMT

Number of Pages: 12

Markdown Content:
SYMBOLIC REGRESSION FOR SHARED EXPRESSIONS INTRODUCING PARTIAL PARAMETER SHARING 

VIKTOR MARTINEK AND ROLAND HERZOG 

Abstract. Symbolic regression aims to find symbolic expressions that de-scribe datasets. Due to better interpretability, it is a machine learning par-adigm particularly powerful for scientific discovery. In recent years, several works have expanded the concept to allow the description of similar phenomena using a single expression with varying sets of parameters, thereby introducing categorical variables. Some previous works allow only “non-shared” (category-value-specific) parameters, and others also incorporate “shared” (category-value-agnostic) parameters. We expand upon those efforts by considering multiple categorical variables, and introducing intermediate levels of parame-ter sharing. With two categorical variables, an intermediate level of parameter sharing emerges, i. e. parameters which are shared across either category but change across the other. The new approach potentially decreases the number of parameters, while revealing additional information about the problem. Us-ing a synthetic, fitting-only example, we test the limits of this setup in terms of data requirement reduction and transfer learning. As a real-world symbolic re-gression example, we demonstrate the benefits of the proposed approach on an astrophysics dataset used in a previous study, which considered only one cate-gorical variable. We achieve a similar fit quality but require significantly fewer individual parameters, and extract additional information about the problem. 

1. Introduction 

Symbolic regression (SR) is a machine learning paradigm that distills symbolic expressions from data. Other machine learning methods assume a fixed functional form and seek to identify its parameters. In contrast, SR searches for the func-tional form and its associated parameters aiming for more concise representations. As symbolic expressions of this form have long been used describe and study phe-nomena, and as they are more human interpretable, SR is a uniquely powerful tool for scientific discovery, system identification, and explainable artificial intelligence. However, optimizing the functional form is not as straightforward as identifying parameters of a fixed functional form. In fact, [29] proved SR to be NP-hard. Still, there are many effective approaches to SR, many of which are emerging in recent years. [10] introduced SR itself and proposed an evolutionary-inspired method, i. e. genetic programming (GP). In simple terms, a population of randomly generated expressions are randomly combined and changed. Inferior expressions are discarded 

Date : January 8, 2026. 

Key words and phrases. symbolic regression, symbolic machine learning, multi-view symbolic regression, class symbolic regression, factor variables. This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foun-dation) – HE 6077/14-2 – within the Priority Programme “SPP 2331: Machine Learning in Chem-ical Engineering”.  

> 1
> arXiv:2601.04051v1 [cs.LG] 7 Jan 2026 2V. MARTINEK, AND R. HERZOG

in favor of better ones, as determined by the optimization objectives that balance fit quality and expression complexity. Beyond [10], GP methods have been continually improved in the last 30 plus years and are still among the best approaches to SR [12, 7]. There are numerous other methods including, but not limited to, exhaustive search [8, 1], reinforcement-learning-based methods [20, 23, 13], and amortized approaches [2, 28]. Although some are well-poised to, these have not yet eclipsed GP-based methods in all aspects. SR has successfully been applied for scientific discovery in many domains, e. g., astrophysics [19, 5, 14], material science [31, 30, 3], and thermodynamics [21, 6, 33, 26]. Some problems may benefit from, or even necessitate, the incorporation of categorical variables. 1 Categorical variables allow different continuous datasets to be combined that either describe the same phenomenon measured under differ-ent conditions, or describe different but similar phenomena. Categorical variables may be incorporated into symbolic expressions by using a single, “shared” func-tional form, but different sets of parameters for each categorical variable value. For example, [17] developed a single expression linking the residual entropy to the viscosity of 127 fluids, where the fluids represent one categorical variable. This is achieved by sharing some parameters across all fluids and identifying some for each fluid individually. This correlation was, however, developed without the use of SR. Intuitively, finding a model capable of describing multiple similar phenomena increases the chances of if it being meaningful and that it can generalize beyond the provided category values. The chances of the model valid for 127 fluids being valid for a 128 th fluid are high. In some cases, categorical variables may hide unwanted complexity or compress not yet understood interrelations of missing independent variables capable of ex-plaining the distinct categorical values, as also discussed in [11]. For example, the ideal gas law is p = ρ RM T , where R is the ideal gas constant, M is the substance-specific molar mass, and the p, ρ, T are the variables pressure, density, and tem-perature. If the substance-specific molar masses were unknown, we could develop a shared expression defining each substance as a category value. For the mentioned viscosity correlation, it is not fully clear which continuous variables may explain the different behavior. In a sense, sharing a functional form across category values regularizes the func-tional form [24]. This may prevent “symbolic overfitting”, improve generalization performance, and increase robustness to noise. Some of the prior approaches use a fully distinct set of parameters for each category value, while others also allow category-agnostic or “shared” parameters. Sharing some of the parameters in be-tween the category values, and thereby reducing their number, further enhances all benefits mentioned, as also discussed in [25]. Furthermore, this approach also leads to transfer learning to some degree, where knowledge is reused across phenomena [32, 15]. In recent years, several works have explored incorporating one categorical vari-able into SR in such a (or a similar) manner with some differences in their ap-proaches [15, 27, 24, 11, 25] (and [22, 4]). [11] is closest to the present one. Focusing on finding common behavior across similar systems, they introduce one categorical   

> 1In this work, we use the term “categorical variables” in favor of “nominal variables”, as they allow simpler phrasing and thus clearer understanding. We do not imply any ordering of the category values. SYMBOLIC REGRESSION FOR SHARED EXPRESSIONS 3

variable using category-value-specific parameters in the discussed manner. Both fully-shared and category-value-specific parameters are allowed. Multiple categori-cal variables are discussed, but deemed unfeasible, as the interactions could not be accounted for, and the datasets would become too sparse. They also compare this varying-parameter-sets-approach to one-hot-encoding, which is another method to including categorical variables. One-hot-encoding is deemed less interpretable, as the categorical influences are distributed across the resulting expression. In [15, 27, 24, 11] (and [22, 4]), various approaches are introduced which each incorporate only one categorical variable. While [24, 22, 15] allow for category-value-specific parameters only, [11, 27, 4] include also parameters shared across all category values. [25] compares [24, 27, 4, 22] on a common benchmark with real-world problems containing one categorical variable. They discuss benefits and drawbacks and give recommendations for future research in this promising field. The option of using more than one categorical variables is briefly mentioned in [25], but they do not discuss adapting the methods to accommodate for those. Problems with two or more categorical variables can be approached using cate-gory-value-combinations as a single, condensed categorical variable. However, this ignores a crucial part of the problem, and fails to fully exploit the potential to share and reduce parameters. To the best of our knowledge, all prior works but one ex-plored approaches capable of incorporating only one categorical variable. Only [11] could in principle handle multiple categorical variables separately. However, their approach cannot account for the interactions. In the present work, we demonstrate a novel approach that uses three types of parameter sharing, i. e. “fully-shared” (global), “partially-shared” (category-value-specific), and “non-shared” (category-value-combination-specific). In the following section, the proposed approach is introduced, while its benefits are demonstrated in Section 3. Finally, a conclusion is drawn in Section 4. 2. Method 

In this section, for simplicity’s sake, the proposed approach is shown for a two-category example. However, the approach is applicable to arbitrarily many cate-gorical variables. 2.1. Concept. To explain the concept, we introduce an example problem. The problem has two categorical variable U (uppercase) and L (lowercase). Category U

has the four category values A, B, C, D, while L has the three values a, b, and c. Thus, in the datasets, there are four times three possible category-value-combinations. The core of our approach is the use of three distinct levels of parameter sharing: fully-shared, partially-shared, and non-shared, which depend on neither, either, or both category values, respectively. The highest level of sharing is represented by fully-shared parameters, which are the same across all category-value-combinations. Non-shared parameters are different for each category-value-combination, and thus, do not share parameters. The intermediate sharing level is realized by partially-shared parameters. Those are shared across either of the two categories, but depend on the other category. This approach allows us to distinguish category-agnostic ef-fects, effects of either category, while also correcting for category-value-combination interactions. Figure 2.1 illustrates the sharing concepts for the example at hand. 4 V. MARTINEK, AND R. HERZOG             

> ABCD
> a
> b
> c
> Aa
> Ab
> Ac
> Ba
> Bb
> Bc
> Ca
> Cb
> Cc
> Da
> Db
> Dc
> Aa
> Ab
> Ac
> Ba
> Bb
> Bc
> Ca
> Cb
> Cc
> Da
> Db
> Dc
> ABCD
> a
> b
> c
> Aa
> Ab
> Ac
> Ba
> Bb
> Bc
> Ca
> Cb
> Cc
> Da
> Db
> Dc
> ABCD
> a
> b
> c
> Aa
> Ab
> Ac
> Ba
> Bb
> Bc
> Ca
> Cb
> Cc
> Da
> Db
> Dc
> ABCD
> a
> b
> c
> Aa
> Ab
> Ac
> Ba
> Bb
> Bc
> Ca
> Cb
> Cc
> Da
> Db
> Dc

Figure 2.1. Illustrative example of a two-category problem with all possible category-value-combinations. Non-shared parameters are illustrated in the top-left, fully-shared ones in the top-right, and partially-shared ones in the bottom two illustrations. As an intuitive example, if U were patient groups and L were medications, a parameter partially shared across the patient types may account for the effects of a particular medicine on all patient types. A parameter partially shared across medications may account for a particular patient-types reaction to medication in general. Finally, non-shared parameters would correct for binary interactions of particular medications and particular patient types, e. g., allergies. The approach is reminiscent of mixed-effect models or group-contribution methods. 2.2. Evaluation and Parameter Identification. The implementation is par-tially similar to Kronberger et al. [11] with some extra considerations regarding the new types of parameter sharing. Shared parameters pshared hold one real number and are minimized using all data. The partially-shared parameters depending on category U , i. e., ppartial ,U , contain as many real values as there are category values 

u in category U , and mutatis mutandis, the same holds for partially-shared parame-ters depending on other categories. Non-shared parameters pnonshared are optimized for each category-value-combination (u, l ) individually, and thus, they contain as many real numbers as there are unique category-value-combinations (u, l ).For each category pair (u, l ) and each data point i belonging to that pair, the observed value yu,l,i is predicted with (2.1) ˆyu,l,i = f  xU,l,i ; pshared , p partial ,U,u , p partial ,L,l , p nonshared ,(u,l )

.

The least-squares fitting objective becomes a double loop over the categorical vari-ables around another loop for the data points Iu,v in the category-value combina-tion (u, l )

(2.2) Minimize X

> u∈U

X

> l∈L

X

> i∈I u,v

 yu,l,i − ˆyu,l,i 

2 w.r.t. P, 

where P =  pshared , p partial ,U , p partial ,L , p nonshared 

.SYMBOLIC REGRESSION FOR SHARED EXPRESSIONS 5

To illustrate, we introduce the shared expression (2.3) ˆyu,l,i = pshared · v1 + ppartial ,U,u · v21 + ppartial ,L,l · v31 + pnonshared ,(u,l ) · v41 ,

where pshared = 100 is a fully-shared parameter, ppartial ,U = {A 7 → 10 , B 7 → 20 , C 7 →

30 , D 7 → 40 } is a partially-shared parameter depending on the categorical variable U ,

ppartial ,L = {a 7 → 1, b 7 → 2, c 7 → 3} is a partially-shared parameter depending on L,and pnonshared = {Aa 7 → 0.01 , Ab 7 → 0.02 , Ac 7 → 0.03 , Ba 7 → 0.04 , Bb 7 → 0.05 , Bc 7 →

0.06 , Ca 7 → 0.07 , Cb 7 → 0.08 , Cc 7 → 0.09 , Da 7 → 0.1, Db 7 → 0.11 , Dc 7 → 0.12 } is a non-shared parameter different for each category-value-combination (u, l ).Table 2.1 illustrates which values each parameter assumes depending on the category-value-combination using Equation (2.3) for a simple dataset. The sparsity pattern of the Jacobian (consisting of the partial derivatives of the predictions ˆyu,l,i 

w.r.t. the parameters P ) required for identifying the parameters of this example is shown in Figure 2.2. This sparsity pattern can be exploited during differentiation to avoid a time and memory complexity of O(nk ), where n is the number of data points and k denotes the number of “individual” parameters 2 (1 + 4 + 3 + 12 = 20 

in this case), to achieve a time and memory complexity of O(nm ), where m is the number of parameters ( 1 + 1 + 1 + 1 = 4 in this case). 

Table 2.1. Showcasing which values the different types of param-eters assume depending on the category-value-combinations using Equation (2.3) and a simple dataset. 

u l i v1 pshared ppartial ,U,u ppartial ,L,l pnonshared ,(u,l ) y

A a 1 1 100 10 1 0.01 111.01 

A b 1 1 100 10 2 0.02 112.02 

A c 1 1 100 10 3 0.03 113.03 

B a 1 1 100 20 1 0.04 121.04 

B b 1 1 100 20 2 0.05 122.05 

B c 1 1 100 20 3 0.06 123.06 

C a 1 1 100 30 1 0.07 131.07 

C b 1 1 100 30 2 0.08 132.08 

C c 1 1 100 30 3 0.09 133.09 

D a 1 1 100 40 1 0.1 141.1 

D b 1 1 100 40 2 0.11 142.11 

D c 1 1 100 40 3 0.12 143.12 2.3. Incorporation into Symbolic Regression. Regardless of the SR algorithm, the terminal set has to be expanded to accommodate the three types of parameters. The partially-shared parameters are further divided into as many different ones as there are categorical variables, as they behave differently and contain a different number of individual parameters according to which category they depend on. This may affect several parts of the respective algorithm or implementation. Here, we briefly mention some of the changes required for a GP method. 

• Evaluation and parameter identification: Some pointers are provided above.  

> 2We refer to the real-valued parameters contained in each of the “high-level” parameters as individual parameters. 6V. MARTINEK, AND R. HERZOG

Figure 2.2. Sparsity pattern of the Jacobian required for identi-fying the parameters of Equation (2.3) using the example of Ta-ble 2.1. The circular points correspond to shared parameters, squares correspond to partially-shared parameters, and triangles pertain to non-shared parameters. 

• Expression generation: The three new types need to be included during expression generation. 

• Mutations: Mutations which may concern terminals, e. g., a “point muta-tion” that changes a type of node to a different type of node, should be adapted for the new terminal set. 

• Representation: A new string representation is required for the different shared parameter types, e. g., C1 for parameters depending on categorical variable 1, and CI for parameters depending on categorical-variable-value combination ( I for interaction). 

• Simplification: Some simplification rules for normal parameters, e. g., 1 * 1 → 1, are not valid for the new parameter types, e. g., C1 * C2 ↛ C1 .

• Complexity measures: As lower levels of sharing introduce more individual parameters, this should be reflected in the complexity measure directly, or be penalized using an additional complexity measure. (optional) 

• Grammar: Implementations utilizing grammar need to update their gram-mar to include the new terminal set. (optional) 

• Saving the individual parameter values: It has been shown that fewer pa-rameter identification iterations are sufficient, if the values of the parame-ters are saved [9, 11]. (optional) 

• IO: Beyond string representation, it is useful to make the individual param-eter values accessible in the results, as refitting in non-trivial for non-expert users. Further, as the problem is likely multi-modal, reproducing the result may fail. (optional) 

• Data splitting: Data splitting, e. g., for training and testing data, needs to be adapted to ensure a stratified split for category-value-combinations. (optional) Many SR implementations allow reusing parts of the expression within itself, often referred to as directed acyclic graphs. This may be particularly useful for SYMBOLIC REGRESSION FOR SHARED EXPRESSIONS 7

shared expression, as it enables using a shared parameter in several places, and thus, potentially reduces the total number of individual parameters further. 2.4. Sharing is Caring. The presented approach with three types of parameters sharing is applicable to problems with two or more categorical variables. However, in the cases with more than two categorical variable, new intermediate levels of sharing emerge. For example, for a three-category problem, there may be effects that mostly depend on the combination of values in two of the categories, and thus may be shared across the third. Generalizing to arbitrarily many levels of sharing may be beneficial depending on the problem. 3. Application 

The proposed approach is implemented on the basis of the open-source SR pack-age TiSR [18, 16]. To demonstrate it, first, we further expand on the previously introduced example Equation (2.3) with the two categories U and L. We design an example to gauge the parameter sharing capabilities, and thus, the data sharing and transfer learning limits in terms of data reduction. Equation (2.3), along with its parameters, remains the same. For v1 of each of the twelve category-value-com-binations, eight data points are randomly and uniformly sampled in the range of 

[−20 , 20] , resulting in a total of 96 data points. During the inner loop of a “proces-sion”, a random data point is moved from the training dataset to the test dataset. All individual parameters p are then randomly perturbed with (3.1) pperturbed = p + 0 .1 · p · r with r ∼ N (0 , 1) 

before they are re-identified and the prediction is compared to the test set. For one procession, this is repeated until the minimum data requirements are no longer satisfied. Excerpts of four of a total of 100 conducted processions are shown in Table 3.1. As can be seen in Table 3.1, the test data are predicted well, despite very few data points in many of the category-value-combinations. For many category-value-combinations, only one data point is required, as long as there are sufficient data in other category-value-combinations. For example, in ID 3:26, seven out of twelve category-value-combinations require only a single data point. In this example, there are 20 individual parameters, which act like 48 parame-ters. Thus, in the best case, as can be seen in procession 2, only a minimum of 

20 data points are required to identify the parameters. However, as can be seen in processions 1, 3, and 4, the minimum data requirements are no longer fulfilled despite the availability of more than 20 points. This stems the fact that not all data can be used to identify any parameters. For each non-shared parameter, one point is required in each category-value-combination. For a partially-shared parameter, at least one additional point is required in each of the categories, on which they depend. And finally, one additional point in any category-value-combination is required to identify a shared parameter. Whether the data distribution across the category-value-combinations is suffi-cient according to those conditions is indicated in the “req.” column of Table 3.1. As evident in the table, as soon as the minimum data requirements are no longer satisfied, the test data can no longer be predicted well, which is reflected in all of 8 V. MARTINEK, AND R. HERZOG 

Table 3.1. Excerpts of four processions, each randomly, itera-tively moving points from the training dataset to the test dataset, perturbing the parameters, reidentifying them, and determining the mean squared error (mse test ) between the predictions and the test dataset. The ID is made up of a procession number and the number of total data points, the columns Aa . . . Dc show the num-ber of data points in each category, and the “req.” column indicates whether the minimum data requirements are fulfilled. ID Aa Ab Ac Ba Bb Bc Ca Cb Cc Da Db Dc mse test req. 1:96 8 8 8 8 8 8 8 8 8 8 8 8 N/A yes 1:90 8 7 8 8 8 8 5 8 8 8 6 8 2e-25 yes 1:60 7 3 5 7 6 4 3 6 6 5 3 5 3e-24 yes 1:30 2 2 1 3 4 1 1 5 5 4 1 1 2e-22 yes 1:22 2 1 1 2 3 1 1 2 5 2 1 1 2e-11 yes 1:21 2 1 1 2 3 1 1 2 5 1 1 1 1e6 no 2:20 1 1 2 1 4 1 2 1 1 2 2 2 7e-10 yes 2:19 1 1 2 1 4 1 2 1 1 1 2 2 1e4 no 3:26 3 1 1 1 4 1 1 4 5 1 3 1 3e-23 yes 3:25 2 1 1 1 4 1 1 4 5 1 3 1 1e-2 no 4:48 5 6 3 5 6 7 4 5 3 1 1 2 2e-23 yes 4:47 5 6 3 5 6 7 4 5 3 1 1 1 6e3 no the 100 conducted processions. Of course, sufficient data for the parameter iden-tification alone does not guarantee a successful refitting, as it also depends on the distribution of the v1 values in the training dataset. As a real-world example, we adopt an example from [24, 25], i. e., the astrophysics dataset. This dataset contains two categorical variables, which the authors flattened to one by considering their combination as a single categorical variable. It describes the change of radiation flux of supernovae (three datasets) over time using two types of photometric filters (two bands), which results in three times two (equals six) unique category-value-combinations. In [24], the authors show three resulting expressions for this problem with three, four, and five parameters and R2-values of 0.99 , 0.987 , and 0.992 , respectively. How-ever, as they only contain the present-studies-equivalent of non-shared parameters, each of those carry six individual parameters, resulting in 18 , 24 , and 30 individual parameters. We adopt their dataset and preprocessing, but treat the two categories sepa-rately. The function set is set to 

{+, -, *, /, ˆ, exp , log , square , sqrt },

and the selection objectives are 1 − R2, the complexity (number of operators and operands), and the number of individual parameters. We limited the maximum allowed complexity during the search to 15 , but show only expressions up to a complexity of 12 to allow direct comparisons to [24]. SYMBOLIC REGRESSION FOR SHARED EXPRESSIONS 9

Table 3.2. Excerpt of expressions found for the supernovae dataset along with their R2-values, complexity (number of opera-tions and operands), and the number of individual parameters. Pa-rameters occurring multiple times are counted once (row 4 and 5). The non-shared parameters are denoted by Ci, the partially-shared parameters depending on the type of band are shown as Cband ,and the partially-shared parameters depending on the dataset are shown as Cdataset .Expression R2 complexity # of individual parameters 

C(Ci−v1)2·(0.147+ Cv1

> band ,2

) 

> band ,1

0.991 12 11 

C  

> Ci−v1
> v1−Cband ,2
> 2
> band ,1

0.988 10 10 

C(Cband ,2+v1)2·(0.146+ Cv1

> band ,3

) 

> band ,1

0.983 12 7

Cv1·(Cband ,1−(Cdataset +0 .954 v1 )2) 

> band ,1

0.982 12 6

Cv1− 

> v11.22 √1.22 v1
> band

0.977 12 3Table 3.2 shows five selected expressions of the Pareto front resulting from the expression search conducted with the proposed approach. All the shown expression have much fewer individual parameters than the ones shown in [24], while retain-ing a similar R2-value. For example, the expression in the first row of Table 3.2 uses one shared parameter, two parameters depending on the type of band, and one non-shared parameter, which results in only eleven individual parameters. De-spite using six fewer individual parameters, its R2-value ( 0.991 ) is better than the one from three-parameter expression from [24] ( 0.99 ). Furthermore, the proposed approach also yields additional information about the problem, as the effects of different categories, as well as category-specific and category-agnostic effects, may be understood by analyzing the expressions. 4. Conclusion 

A novel approach to incorporating multiple categorical variables into SR is pro-posed. It improves upon previous approaches if two or more categorical variables are present, by using three types of parameter sharing: fully-shared, partially-shared, and non-shared parameters. Those are able to identify category-agnostic effects, distinguish category-specific effects, and correct for category-value-combination-interactions. This potentially identifies additional information about the problem, reduces the number of individual parameter, and the number of required data. In a synthetic, fitting-only example, we gauge the limits of the parameter, and thus, data sharing and transfer learning capabilities, and show that very few data in any category-value-combination are required, as long as there are sufficient data in the other category-value-combinations. We also demonstrate the proposed ap-proach on a real-world astrophysics dataset used in a related study [24, 25] that only considered one categorical variable. The resulting expressions improve upon 10 V. MARTINEK, AND R. HERZOG 

prior results by using significantly fewer individual parameters while retaining a similar fit quality, and extracting additional information about the problem. The proposed approach will be merged into the publicly available branch of TiSR in the future. 

References 

1. Bartlett, D. J.; Desmond, H.; Ferreira, P. G. Exhaustive symbolic regression. 

IEEE Transactions on Evolutionary Computation 28, 950–964. doi: 10.1109/ tevc.2023.3280250 (2024). 2. Biggio, L.; Bendinelli, T.; Neitz, A.; Lucchi, A.; Parascandolo, G. Neural sym-bolic regression that scales Proceedings of the 38th International Conference on Machine Learning (eds Meila, M.; Zhang, T.) 139 (PMLR, 2021), 936–945. arXiv: 2106 . 06427 . https :/ /proceedings . mlr . press/v139/biggio21a . html .3. Burlacu, B.; Kommenda, M.; Kronberger, G.; Winkler, S. M.; Affenzeller, M. Genetic Programming Theory and Practice XIX 1–30 (Springer Nature Singapore, 2023). doi: 10.1007/978-981-19-8460-0 1 .4. Cranmer, M. PySR: Fast & parallelized symbolic regression in Python/Julia 

2020. doi: 10.5281/zenodo.4041459 .5. Cranmer, M.; Sanchez-Gonzalez, A.; Battaglia, P.; Xu, R.; Cranmer, K.; Spergel, D.; Ho, S. Discovering symbolic models from deep learning with inductive bi-ases arXiv: 2006.11287 .6. Frotscher, O.; Martinek, V.; Fingerhut, R.; Yang, X.; Vrabec, J.; Herzog, R.; Richter, M. Proof of concept for fast equation of state development using an integrated experimental-computational approach. International Journal of Thermophysics 44. doi: 10.1007/s10765-023-03197-z (2023). 7. Imai Aldeia, G. S.; Zhang, H.; Bomarito, G.; Cranmer, M.; Fonseca, A.; Burlacu, B.; La Cava, W. G.; Olivetti de França, F. Call for action: towards the next generation of symbolic regression benchmark Proceedings of the Ge-netic and Evolutionary Computation Conference Companion (ACM, 2025), 2529–2538. doi: 10.1145/3712255.3734309 . arXiv: 2505.03977 .8. Kammerer, L.; Kronberger, G.; Burlacu, B.; Winkler, S. M.; Kommenda, M.; Affenzeller, M. Genetic Programming Theory and Practice XVII 79–99 (Springer International Publishing, 2020). doi: 10.1007/978- 3- 030- 39958-0 5 .9. Kommenda, M.; Kronberger, G.; Winkler, S.; Affenzeller, M.; Wagner, S. Ef-fects of constant optimization by nonlinear least squares minimization in sym-bolic regression Proceedings of the 15th Annual Conference Companion on Ge-netic and Evolutionary Computation (ACM, 2013), 1121–1128. doi: 10.1145/ 2464576.2482691 .10. Koza, J. R. Genetic Programming. On the Programming of Computers by Means of Natural Selection (The MIT Press, 1992). 11. Kronberger, G.; Kommenda, M.; Promberger, A.; Nickel, F. Predicting friction system performance with symbolic regression and genetic programming with factor variables Proceedings of the Genetic and Evolutionary Computation Conference (ACM, 2018). doi: 10.1145/3205455.3205522 .REFERENCES 11 

12. La Cava, W.; Orzechowski, P.; Burlacu, B.; Olivetti de Franca, F.; Virgolin, M.; Jin, Y.; Kommenda, M.; Moore, J. Contemporary symbolic regression methods and their relative performance Proceedings of the Neural Information Process-ing Systems Track on Datasets and Benchmarks (eds Vanschoren, J.; Yeung, S.) 1 (2021). https :/ /datasets - benchmarks - proceedings . neurips . cc/ paper/2021/hash/c0c7c76d30bd3dcaefc96f40275bdc0a-Abstract-round1. html .13. Landajuela, M.; Lee, C. S.; Yang, J.; Glatt, R.; Santiago, C. P.; Aravena, I.; Mundhenk, T.; Mulcahy, G.; Petersen, B. K. A unified framework for deep symbolic regression Advances in Neural Information Processing Systems (eds Koyejo, S.; Mohamed, S.; Agarwal, A.; Belgrave, D.; Cho, K.; Oh, A.) 35 (Cur-ran Associates, Inc., 2022), 33985–33998. https:/ /proceedings.neurips.cc/ paper files/paper/2022/hash/dbca58f35bddc6e4003b2dd80e42f838-Abstract-Conference.html .14. Lemos, P.; Jeffrey, N.; Cranmer, M.; Ho, S.; Battaglia, P. Rediscovering orbital mechanics with machine learning. Machine Learning: Science and Technology 

4, 045002. doi: 10.1088/2632-2153/acfa63 (2023). 15. Loh, C.; Schneegass, D.; Tian, P. A search for the underlying equation gov-erning similar systems arXiv: 1908.10673 .16. Martinek, V. Thermodynamics-informed symbolic regression (TiSR). A tool for the thermodynamic equation of state development 2023. doi: 10 . 5281/ zenodo.8317546 . https:/ /github.com/scoop-group/TiSR/ .17. Martinek, V.; Bell, I.; Herzog, R.; Richter, M.; Yang, X. Entropy scaling of vis-cosity IV—application to 124 industrially important fluids. Journal of Chemi-cal & Engineering Data 70, 727–742. doi: 10.1021/acs.jced.4c00451 (2025). 18. Martinek, V.; Frotscher, O.; Richter, M.; Herzog, R. Introducing thermodynamics-informed symbolic regression – a tool for thermodynamic equations of state development arXiv: 2309.02805 .19. Matchev, K. T.; Matcheva, K.; Roman, A. Analytical modeling of exoplanet transit spectroscopy with dimensional analysis and symbolic regression. The Astrophysical Journal 930, 33. doi: 10.3847/1538-4357/ac610c (2022). 20. Michishita, Y. Alpha Zero for physics: application of symbolic regression with Alpha Zero to find analytical methods in physics. Journal of the Physical So-ciety of Japan 93. doi: 10.7566/jpsj.93.074005 . arXiv: 2311.12713 (2024). 21. Muzny, C. D.; Huber, M. L.; Kazakov, A. F. Correlation for the viscosity of normal hydrogen obtained from symbolic regression. Journal of Chemical & Engineering Data 58, 969–979. doi: 10.1021/je301273j (2013). 22. Olivetti de França, F.; Kronberger, G. Improving genetic programming for symbolic regression with equality graphs Proceedings of the Genetic and Evolu-tionary Computation Conference (ACM, 2025), 989–998. doi: 10.1145/3712256. 3726383 . arXiv: 2501.17848 .23. Petersen, B. K.; Landajuela Larma, M.; Mundhenk, T. N.; Prata Santiago, C.; Kim, S. K.; Kim, J. T. Deep symbolic regression: recovering mathematical expressions from data via risk-seeking policy gradients International Confer-ence on Learning Representations, ICLR 2021 (2021). arXiv: 1912 . 04871 .

https:/ /openreview.net/forum?id=m5Qsh0kBQG .12 REFERENCES 

24. Russeil, E.; Olivetti de França, F.; Malanchev, K.; Burlacu, B.; Ishida, E. E. O.; Leroux, M.; Michelin, C.; Moinard, G.; Gangler, E. Multi-view symbolic regres-sion arXiv: 2402.04298 .25. Russeil, E.; Olivetti de França, F.; Malanchev, K.; Moinard, G.; Cherrey, M. 

Exploring multi-view symbolic regression methods in physical sciences arXiv: 

2509.10500 .26. Sotiriadou, S. G.; Antoniadis, K. D.; Assael, M. J.; Martinek, V.; Huber, M. L. Correlation for the viscosity of methane (CH4) from the triple point to 625 K and pressures to 1000 MPa. International Journal of Thermophysics 

47. doi: 10.1007/s10765-025-03690-7 (2025). 27. Tenachi, W.; Ibata, R.; François, T. L.; Diakogiannis, F. I. Class symbolic regression: gotta fit ’em all. The Astrophysical Journal Letters 969, L26. doi: 10.3847/2041-8213/ad5970 . arXiv: 2312.01816 (2024). 28. Valipour, M.; You, B.; Panju, M.; Ghodsi, A. SymbolicGPT: a generative transformer model for symbolic regression arXiv: 2106.14131 .29. Virgolin, M.; Pissis, S. P. Symbolic regression is NP-hard arXiv: 2207.01018 .30. Wang, G.; Wang, E.; Li, Z.; Zhou, J.; Sun, Z. Exploring the mathematic equa-tions behind the materials science data using interpretable symbolic regression. 

Interdisciplinary Materials 3, 637–657. doi: 10.1002/idm2.12180 (2024). 31. Wang, Y.; Wagner, N.; Rondinelli, J. M. Symbolic regression in materials science. MRS Communications 9, 793–805. doi: 10.1557/mrc.2019.85 (2019). 32. Weiss, K.; Khoshgoftaar, T. M.; Wang, D. A survey of transfer learning. Jour-nal of Big Data 3. doi: 10.1186/s40537-016-0043-6 (2016). 33. Yang, X.; Frotscher, O.; Richter, M. Symbolic-regression aided development of a new cubic equation of state for improved liquid phase density calculation at pressures up to 100 MPa. International Journal of Thermophysics 46. doi: 10. 1007/s10765-024-03490-5 (2025).