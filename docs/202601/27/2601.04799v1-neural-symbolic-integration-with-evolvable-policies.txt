Title: Neural-Symbolic Integration with Evolvable Policies

URL Source: https://arxiv.org/pdf/2601.04799v1

Published Time: Fri, 09 Jan 2026 01:58:47 GMT

Number of Pages: 18

Markdown Content:
# Neural-Symbolic Integration with Evolvable Policies 

Marios Thoma 1,2 0000-0001-7364-5799, Vassilis Vassiliades 1 0000-0002-1336-5629 and Loizos Michael 2,1 

Abstract 

Neural-Symbolic (NeSy) Artificial Intelligence has emerged as a promising approach for combining the learning capabilities of neural networks with the interpretable reasoning of symbolic systems. However, existing NeSy frameworks typically require either predefined symbolic policies or policies that are differentiable, limiting their applicability when domain expertise is unavailable or when policies are inherently non-differentiable. We propose a framework that addresses this limitation by enabling the concurrent learning of both non-differentiable symbolic policies and neural network weights through an evolutionary process. Our approach casts NeSy systems as organisms in a population that evolve through mutations (both symbolic rule additions and neural weight changes), with fitness-based selection guiding convergence toward hidden target policies. The framework extends the N EURO LOG architecture to make symbolic policies trainable, adapts Valiant’s Evolvability framework to the NeSy context, and employs Machine Coaching semantics for mutable symbolic representations. Neural networks are trained through abductive reasoning from the symbolic component, eliminating differentiability requirements. Through extensive experimentation, we demonstrate that NeSy systems starting with empty policies and random neural weights can successfully approximate hidden non-differentiable target policies, achieving median correct performance approaching 100%. This work represents a step toward enabling NeSy research in domains where the acquisition of symbolic knowledge from experts is challenging or infeasible. 

Keywords 

Neural-Symbolic Integration, Evolutionary Learning, Abductive Reasoning, Symbolic Policy Induction. 

1 Introduction 

In recent years, Artificial Intelligence (AI) has seen rapid integration into everyday life, with technologies such as computer vision, personalized recommendation systems, facial recognition, and Large Language Models (LLMs) becoming increasingly prevalent. However, despite the many advantages of Deep Learning and Neural Networks (NNs), their “black-box” nature (Liang et al. 2021) presents a significant challenge for developing trustworthy and explainable AI systems. Neural-Symbolic AI (NeSy) (Besold et al. 2021; Hitzler and Sarker 2021) addresses this challenge by combining the learning capabilities of NNs with the inherent explainability of symbolic AI. The goal is to develop robust AI systems capable of perception-based learning and logical reasoning, thereby enhancing their interpretability, generalizability, and transparency (d’Avila Garcez and Lamb 2023). A recent notable example of such integration is DeepMind’s AlphaGeometry (Trinh et al. 2024), which combines an LLM with a symbolic reasoning engine to solve geometry problems from the International Mathematical Olympiad, achieving scores nearing those of human gold-medalists. A central challenge in Neural-Symbolic AI concerns the acquisition of symbolic policies 1 when domain expertise or background knowledge is not available or difficult to obtain. Existing approaches address this challenge through various strategies: some methods learn differentiable approximations of symbolic policies through gradient-based optimization (e.g., Dai et al. 2019; Serafini and d’Avila Garcez 2016), while others assume fixed symbolic structures that guide neural learning without imposing differentiability constraints (e.g., Tsamoura et al. 2021). However, a critical gap remains in the concurrent learning of non-differentiable symbolic policies alongside neural components, without relying on predefined symbolic policies or differentiability assumptions. In this work, we propose a framework that addresses this gap by enabling NeSy systems to concurrently learn symbolic policies and optimize neural weights through an evolutionary process. Our key contribution is the demonstration that non-differentiable symbolic policies can be learned from scratch (starting with empty policies), while simultaneously training neural networks, without requiring either predefined knowledge or gradient-based policy learning. Building on Valiant’s Evolvability framework (Valiant 2009) and broader work on integrating coachable policies with neural architectures (Michael 2023), we treat NeSy systems as organisms in an evolutionary population, where mutations introduce new symbolic rules and adjust neural weights, and fitness-based selection guides the system toward approximating hidden target policies. Through extensive experimentation involving 150 runs across 30 randomly generated target policies, this study shows that NeSy systems starting with empty symbolic policies and randomly initialized neural weights can successfully approximate hidden non-differentiable target policies. The   

> 1CYENS Centre of Excellence, Nicosia, Cyprus
> 2Open University of Cyprus, Nicosia, Cyprus
> Corresponding author:
> Marios Thoma, Dimarchou Lellou Demetriadi 1, Nicosia, 1016, Cyprus. Email: m.thoma@cyens.org.cy

1

> arXiv:2601.04799v1 [cs.LG] 8 Jan 2026

evolutionary approach reliably achieves a median final correct performance of near 100% correct performance, establishing its viability for concurrent neural and symbolic learning in settings without predefined knowledge or differentiability assumptions. Although the approach attains high correctness, it does so at a high computational cost when compared to an end-to-end neural baseline. The phrase “price of interpretability/explainability” has been used to quantify various trade-offs in machine learning, including predictive or reward gaps (Bertsimas et al. 2019; Garcia et al. 2024), degradation of clustering objectives under explainability constraints (Dasgupta et al. 2020; Laber and Murtinho 2021), and differences in financial return under regulatory constraints (Dessain et al. 2023). In this work, we operationalize the price as additional computational resources and longer training required to obtain explicit, human-readable symbolic policies relative to end-to-end neural baselines that achieve similar predictive performance more efficiently, as reported in Section 4.6. The remainder of this paper is organized as follows. We begin by providing the necessary background information and detailing the proposed framework in Sections 2 and 3. We present our empirical evaluation and findings in Section 4, followed by their analysis in Section 5. Finally, we relate our work to the existing literature in Section 6, before offering conclusions and directions for future work in Section 7. 

2 Background & Preliminaries 

The framework we propose draws upon three key concepts in the literature to enable the concurrent learning of neural and symbolic components: (i) the NeSy framework N EURO LOG (Tsamoura et al. 2021), (ii) Valiant’s Evolvability framework (Valiant 2009), and (iii) Machine Coaching (Michael 2019). We briefly discuss these concepts below, as well as our contributions that make their use together possible (for more details, we direct the reader to the original papers). 

2.1 Machine Coaching 

The original N EURO LOG framework used the Prolog lan-guage to represent symbolic policies (see Section 2.3), which are not inherently mutable or elaboration tolerant (McCarthy 1998), thus posing challenges for their integration with the evolutionary learning mechanism used in our proposed frame-work (described in Section 2.2). To overcome this limitation, we adopt the semantics of Machine Coaching (MC) (Michael 2019), an argumentation-based learning framework that structures symbolic knowledge into policies composed of prioritized sets of rules, making them inherently mutable and elaboration tolerant, properties that enable their integration with neural architectures in evolvable systems (Michael 2023). Specifically, following MC’s semantics, binary concepts in the universe of discourse are represented using propositional logic, in the form of atoms . Literals are instances of atoms, positive (e.g. “ a”) or negative (e.g. “ -a ”) (MC adopts an open-world assumption, where the absence of an atom in perception does not imply its negation). In turn, a rule is a logical construct composed of a sequence of literals forming @KnowledgeBase R1 :: -p1, -p2, -p3 implies h; R2 :: -p1, -p2, -p3, p4 implies -h; R3 :: -p5, -p6 implies -h; R4 :: -p5, -p6, p7 implies h; R5 :: -p5, -p6, p7, p8 implies -h;  

> Figure 1. Example symbolic policy structure using Machine Coaching semantics. The policy consists of prioritized rules (increasing in priority from top to bottom), where each rule has a body (sequence of literals) and head (single literal). This example demonstrates the type of non-differentiable shallow propositional policies that our evolutionary framework learns to approximate. This policy is used throughout the paper to demonstrate the framework’s operation.

its body , and a single literal forming its head . If all the literals in a rule’s body are verified to be true given a context of literals, the rule’s head is concluded to be true as well. E.g., rule “ a, b implies c ”, given the context {a, b}, will conclude that “ c” holds. A policy organizes a set of rules into a prioritized sequence, with later rules having higher priority. Given a context of literals, a policy’s decision is determined by the highest-priority rule that is triggered by the context. This decision-making process can yield correct or wrong predictions, but also abstentions , when no rule is triggered by the context. An example of such a policy structure is shown in Figure 1. MC allows new rules to be added to the end of a policy, where they automatically have higher priority than existing rules and can override them. This property makes rule additions suitable as symbolic mutations in our evolutionary framework. Although rules in a policy can form chains, where the conclusion of one rule serves as input to subsequent rules, in this study we restrict our attention to shallow propositional policies , i.e. policies with no chained rules, where all rules have the special atom head (or -head ) as their head. 

2.2 Evolvability Framework 

Valiant’s Evolvability framework (Valiant 2009), shown to be a restricted case of Probably Approximately Correct (PAC) learnability (Valiant 1984), formalizes evolution as a learning mechanism seeking to approximate a hidden target function. It does so by casting functions as organisms in a population, evolving through mutation and survival-of-the-fittest, to gradually approximate the target function. Adapting this to a NeSy context, we treat NeSy systems as NeSy organisms .In the original framework, each organism in the population is a mutated offspring derived from a parent organism of the preceding generation. To accommodate the dual nature of NeSy organisms, we implement mutation mechanisms for both neural and symbolic components: (i) neural mutations ,which involve the offspring inheriting the neural network weights from their parent, or starting with randomly initialized weights; and (ii) symbolic mutations , which involve the offspring’s symbolic policy being expanded by adding a new rule, building upon the parent ’s existing policy, thus incrementally evolving the organism’s symbolic reasoning capabilities. 2Evolution proceeds in distinct generations. At the start of each generation, the parent organism reproduces to create a population of offspring (the parent itself is not part of the generation), which due to the mutations is variable. Each organism in the population is evaluated for fitness against a dataset created using the hidden target function. We adopt the same mechanism, with the only difference being that NeSy organisms are first trained on a training dataset labeled using a hidden target symbolic policy , then evaluated for fitness using a validation dataset. We employ a relative fitness metric that compares each offspring’s performance against the validation dataset to their parent ’s performance. The selection of the f ittest organism follows the mechanism of the original framework: offspring are grouped according to the relation of their relative fitness to a fixed threshold parameter t. An organism oi is categorized as 

detrimental , neutral , or beneficial , if their relative fitness 

fi is in (−∞ , −t), [−t, +t], or (+ t, +∞), respectively. If available, a beneficial offspring oi is selected using a form of fitness proportionate selection, with probability f ki / P 

> j

f kj ,where the exponent k is a fixed nonlinearity parameter. If beneficial offspring are not available, a neutral offspring oi

is selected uniformly at random. Finally, if the other two groups are empty 2, the offspring oi with the highest fi from the detrimental group is selected, breaking any ties at random. The selected organism is added to the lineage of f ittest 

organisms and becomes the founder of the next generation. Past work by Markos et al. (2022) demonstrated the effectiveness of this evolutionary framework for learning symbolic policies in conjunction with Machine Coaching semantics. Building on this foundation, our work extends the approach to the neural-symbolic setting by incorporating the N EURO LOG architecture described below, enabling the concurrent evolution of both symbolic policies and neural network weights. 

2.3 NEURO LOG NeSy Framework 

The NEURO LOG framework (Tsamoura et al. 2021) compositionally integrates neural and symbolic systems by treating them as “black-boxes” incorporated as independent 

modules in a unified architecture. The architecture consists of a SymbolicModule (containing a symbolic policy), a NeuralModule (containing a NN), and a predefined 

Translator function that enables communication between the modules (Figure 2). The framework’s compositional nature and semantic agnosticism allowed us to easily adapt it to our proposed evolutionary approach. The N EURO LOG framework assumes that the symbolic policy is given, but makes no assumptions about its differen-tiability. It bypasses differentiability requirements through 

abductive reasoning (Kakas 2017), a form of backward reasoning that enables training the NeuralModule ’s neural network using the SymbolicModule ’s symbolic policy. A policy p consists of logical expressions over a set of atoms A

(binary/boolean concepts). The policy supports both forward and backward reasoning: given input atoms I ⊆ A, forward reasoning produces output atoms O = deduce (p, I ) such that p ∪ I |= O. Conversely, given output atoms O, backward reasoning produces one or more sets of input atoms I ∈

abduce (p, O ) such that p ∪ I |= O. Any conjunction of    

> NEURO LOG
> Neural Module
> Symbolic Module
> Translator Input Output/Label Loss calculation
> Figure 2. The N EURO LOG NeSy architecture, as proposed by Tsamoura et al. (2021).

atoms from abduce (p, O ) forms an abductive proof for the given outcome (or label) O.Atoms in A correspond to both the NeuralModule ’s output neurons and the SymbolicModule ’s binary concepts (further discussed in Section 2.1). During training, the NeuralModule learns via abductive feedback from the 

SymbolicModule through semantic loss calculation (Xu et al. 2018). Given a training instance with neural input and its label, the system first generates all possible abductive proofs for the label. These proofs are compiled into a Sentential Decision Diagram (SDD) (Darwiche 2011), which enables efficient Weighted Model Count (WMC) calculation (Chavira and Darwiche 2008). The WMC incorporates the neuron activation values produced when the NeuralModule 

processes the neural input, weighting each atom in the abductive proofs. The semantic loss is then computed as the negative logarithm of the WMC. 

3 Proposed Framework 

Bringing together the ideas discussed in the previous section, our proposed framework consists of two major components: an extensively modified and elaborated NEURO LOG architecture, and an evolutionary algorithm, which, when combined, allow the concurrent training of both neural and symbolic components of the NeSy system. 

3.1 Extended NEURO LOG Architecture 

We further formalize the N EURO LOG framework to introduce a learning mechanism to the SymbolicModule , to make it trainable in an evolutionary setting. The original framework employs the notions of deduction 

(forward reasoning) and abduction (backward reasoning), to allow for the transparent integration between the neural and symbolic modules. We expand this to include the notion of induction (learning), i.e. a way for both modules to be trainable, which was already possible for the 

NeuralModule , but not for the SymbolicModule . We do so by requiring both modules to expose three methods: 

deduce() , abduce() and induce() , for the three notions mentioned above, respectively 3.Forward reasoning in the system is achieved by chaining the deduce() methods of the two modules, using the Translator to facilitate their communication, as demonstrated in Figure 3 (black arrows). A concrete example of this forward reasoning process with actual MNIST input and a symbolic policy is shown later in Figure 4. For the SymbolicModule , the induce() method provides a way to modify the symbolic policy in the module, so that new knowledge is acquired. Although our framework does not make any assumptions about the symbolic policy semantics used, in this study we adopt the MC semantics, 3Symbolic Induction  Neural Induction NeSy Deduction    

> NeuroLog
> NeuralModule
> + abduce()
> + deduce()
> + induce()
> SymbolicModule
> + abduce()
> + deduce()
> + induce()
> Translator
> + neural_to_symbolic()
> + symbolic_to_neural()
> Input
> Loss Calculation
> Label
> New
> Rule Figure 3. The extended N EURO LOG architecture enabling concurrent neural and symbolic learning. Black arrows show forward reasoning (deduction): the

deduce() methods of NeuralModule and SymbolicModule are chained via the Translator to produce predictions. Red arrows show neural induction: the NeuralModule trains via backpropagation using abductive feedback from the SymbolicModule ’s abduce() method, which generates training signals from symbolic rules. Blue arrows show symbolic induction: new rules are added to the SymbolicModule ’s policy via its induce() method during evolutionary mutations. 

as discussed in Section 2.1. Thus, the SymbolicModule induce() method maps to the addition of new rules to a symbolic policy (Figure 3, blue line). We employ the inference engine Prudens (Markos and Michael 2022), which implements MC, to interpret the symbolic policies. Prudens 

is capable of both deduction and abduction with such policies, and thus powers both deduce() and abduce() methods of the SymbolicModule . Since the abduction process is computationally costly, the proposed framework implements extensive caching of abductive results. In NeuralModule , the induce() method directly maps to the use of backpropagation in NNs, once training loss has been computed using abductive feedback from the 

SymbolicModule abduce() method (Figure 3, red line). Training of the system, similar to classical NN training, occurs across multiple epochs, with the full training dataset used in each epoch. As explained previously, instances of the extended NEURO LOG architecture are cast as organisms in an evolutionary setting. Although training of an organism’s 

NeuralModule is done primarily through abductive feedback from the SymbolicModule , as explained in Section 2.3, when the organism is still learning its symbolic policy, said abductive feedback is incomplete (especially early in the evolutionary process). To investigate whether additional training signals could improve convergence during these early generations, a self-supervised learning mechanism was introduced to the NeuralModule in the form of reconstruction loss. This auxiliary loss component provides a training signal that is independent of the symbolic policy’s completeness, potentially helping guide neural learning when abductive feedback is limited. The details of the implementation and the results of the ablation study for different loss configurations are presented in Sections 4.2 and 4.5. 

3.2 Evolutionary Process 

The goal of the evolutionary process is to approximate a NeSy system containing a hidden target policy p, starting with a NeSy system with an empty policy, while concurrently optimizing its neural weights. To do so, an exemplar set 

bp is constructed using the target policy p, and is then split randomly into training ( bptrain ), validation ( bpval ), and testing ( bptest ) subsets. bptrain and bpval are used during the evolutionary process, while bptest is held out and used to objectively evaluate the evolutionary process once it is finished. Algorithm 1 outlines the evolutionary process, which uses instances of our modified N EURO LOG architecture as NeSy organisms. At gen = 0 , an array ℓ, intended to hold the lineage of f ittest organisms, is seeded with a single organism with an empty symbolic policy and randomly initialized neural weights. Each subsequent generation starts with the 

f ittest organism of the previous generation reproducing to create a population ( pop gen ) of mutated offspring. Specifically, for the symbolic mutations, all offspring inherit the symbolic policy of their parent , modified as follows: (i) S0 (clone mutation): an offspring inherits an exact copy of the symbolic policy of its parent ; (ii) S+ (addition 

mutation): based on a random context x drawn from 2A (since each atom can be positive or negative), two offspring are created by adding the new rule “ x implies head ” or “ x

implies -head ” to their policy; (iii) S↓ (simplification 

mutation): an offspring is created for each j, by adding the rule “ body −j implies head ” to their policy, where “body implies head ” is the latest rule added to their 

parent ’s policy, and body −j is body minus its j-th literal. Similarly, we consider the inheritance or not of the neural weights w from parent to offspring as neural mutations, specifically: (i) Npw: an offspring that inherits w from their parent; (ii) Nrw: an offspring that starts with randomly initialized w. Enough offspring are created to accommodate all possible combinations of symbolic and neural mutations (the two types of mutation are independent; thus all their combinations are possible, e.g. an NpwS0 offspring is a symbolic clone with inherited neural weights, while an NrwS+

offspring is a symbolic addition mutation with randomly initialized neural weights). Subsequently, all organisms in pop gen are trained using 

bptrain for a number of epochs. Then, their relative fitness is calculated, which is a measure of progress for each offspring from its parent, calculated by a point-by-point comparison of the change in deductions of the offspring vs the parent on 

bpval , rewarding or giving penalties depending on the direction of change, using the score matrix in Table 2 (Appendix A). Then, based on the parameters t and k, a single organism is selected as the f ittest in pop gen , using the mechanism 4Input                                

> image …
> Neural Module
> @KnowledgeBase
> R1 :: -p1 ,-p2 ,-p3 implies h ;
> R2 :: -p1 ,-p2 ,-p3 ,p4 implies -h;
> R3 :: -p5 ,-p6 implies -h;
> R4 :: -p5 ,-p6 ,p7 implies h ;
> R5 :: -p5 ,-p6 ,p7 ,p8 implies -h;
> Symbolic Module
> Translator  -p1
> p8
> …Output
> -hFigure 4. Example of forward reasoning in the extended N EURO LOG architecture with a concrete target policy. The input MNIST image sequence is processed by the

NeuralModule (CNN), producing symbolic atom predictions via the Translator , which the 

SymbolicModule uses with its policy (the same policy shown in Figure 1) to generate the final output. This policy represents one of the randomly generated target policies used in the experiments. 

Algorithm 1: The evolutionary process algorithm. 

input: Datasets bptrain , bpval & bptest . Parameters t, k &

maxgen . 

> 1

At gen = 0 , initialize array ℓ to hold lineage of fittest organisms, with a single organism with empty policy and randomly initialized NN weights;  

> 2

for gen = 1 to maxgen do  

> 3

parent = ℓ[gen − 1] ; /* Parent fittest organism of previous gen */  

> 4

Populate array pop gen with offspring of parent 

with all combinations of neural ( Npw, Nrw) and symbolic ( S0, S+, S↓) mutations;  

> 5

Train all organisms in pop gen using bptrain ; 

> 6

Using bpval , compute relative fitness fi of each organism oi in pop gen vs parent ; 

> 7

Based on t, split pop gen into benef icial , neutral ,and detrimental groups;  

> 8

if benef icial is not empty then  

> 9

Select f ittest from benef icial using fitness proportionate selection with exponent k; 

> 10

else if neutral is not empty then  

> 11

Select f ittest from neutral uniformly at random;  

> 12

else  

> 13

Select f ittest as the organism with highest fi

in detrimental ; 

> 14

ℓ[gen ] = f ittest ; 

> 15

Clear pop gen ; 

> 16

if ff ittest ≥ 99% then  

> 17

break /* Early stopping */  

> 18

Test all organisms in ℓ using bptest ;described in Section 2.2, and added to the lineage ℓ, to become the founder of the next generation. The evolutionary process continues until (i) a specific number of generations is exceeded ( maxgen ); or (ii) an early stopping criterion is triggered (e.g., a threshold of correct performance on 

bpval is reached). Once the evolutionary process is over, the organisms in ℓ are evaluated using bptest . The final organism in ℓ represents the result of the concurrent training of the neural and symbolic components of a NeSy system, the goal of our proposed framework. 

3.3 Technical Contributions 

The use of an evolutionary algorithm as a coach involves the repeated mutating (coaching) and training of several NeSy “organisms” in successive generations, until a NeSy organism that successfully approximates a hidden symbolic policy emerges. Preliminary implementations revealed that this process presented significant computational challenges due to two major bottlenecks: the sequential nature of training NeSy organisms during experiments, and the computational cost of loss calculation during training. Without optimization, training the populations of organisms required for even a single generation was substantially slower than practical, making the experiments described in this study challenging to execute at scale. The following subsections detail the technical contributions we developed to address these computational bottlenecks. The effectiveness of these optimizations, which collectively enabled the large-scale experimentation presented in Section 4.2, is empirically validated in Section 4.4. 

3.3.1 Parallel Training of Neural Networks on a Single GPU A common challenge in training neural networks (NNs) is the VRAM capacity limitation of a single GPU, often necessitating the use of GPUs with larger VRAM, or the adoption of multi-GPU training strategies. Conversely, the NeSy evolutionary experiments presented in this study involve the training of numerous, relatively small NNs, each embedded within a NeSy organism. Sequential training of these NNs presents significant computational challenges due to the large number of organisms. To address this, we developed a parallel training methodology enabling the concurrent training of multiple NNs on a single GPU. Parallel training was implemented using PyTorch’s 

multiprocessing module. A significant engineering consideration was ensuring the serializability of all inter-process objects, a requisite of the Python programming environment, so they could be safely passed to other processes. 

3.3.2 Operation Caching During Semantic Loss Calcula-tion The training of NeSy organisms, and particularly their 

NeuralModule , is based on the calculation of semantic loss (Xu et al. 2018), a process that can be computationally intensive. This intensity stems from the intricate steps involved in deriving the loss from the abductive feedback of the symbolic component. As described in Section 2.3, for each training instance the SymbolicModule generates abductive proofs, compiles them into a Sentential Decision Diagram (SDD) (Darwiche 2011), and computes the Weighted Model Count (WMC) (Chavira and Darwiche 2008) by traversing the SDD through Depth-First Search (DFS) while 5integrating neural activation values. The repeated generation of abductive proofs and, more critically, the construction and traversal of SDDs for every batch of training data, represent significant computational overhead. To mitigate this computational bottleneck, our framework implements an operation caching strategy. The core idea is to take advantage of the repetitive nature of calculations within and across training batches. During the first encounter with a specific set of abductive proofs (and thus a specific SDD structure) for a given label within a training batch, the initial DFS operation required for WMC calculation is performed. The sequence of operations and the structure of this traversal are then cached as a computational graph. For all subsequent instances within the same batch that require the WMC for the same SDD structure (but typically with different neural activation values), this pre-compiled computational graph is reused. This caching mechanism substantially reduces training times. A key advantage is the significant reduction in CPU-GPU interaction. Traditionally, the symbolic reasoning steps (abduction, SDD construction, DFS) would reside on the CPU, requiring data to be passed back and forth to the GPU where neural computations and loss backpropagation occur. By caching the computational graph of the WMC calculation, which can be parameterized by the neural outputs, the core of the semantic loss computation can be executed directly and repeatedly on the GPU. This minimizes the costly data transfers and synchronization points between the CPU and GPU, leading to a more streamlined and efficient use of GPU resources. 

4 Experiments & Results 

This section includes key aspects of our empirical investigation of the proposed framework. Additional details are included in Appendix A; code for reproducing all experiments is publicly available (see Supplementary Materials). 

4.1 Target Policies & Datasets 

The target policies used in the experiments were randomly generated, first by fixing A to a set of 8 binary concepts 

{a1, . . . , a 8} for use in contexts and rule bodies, and then randomly generating a set P of target policies, using atoms from A (an example target policy is shown in Figure 1). The random policy generator developed by Markos et al. (2022) was used. Importantly, the target policies used are non-differentiable due to their discrete nature and the logical implications that define their structure, which does not allow for gradient-based optimization methods to be applied directly. Datasets were constructed as follows: to create an exemplar set bp for each target policy p ∈ P , data instances (or contexts) consisting of 8 atoms each were constructed by sampling uniformly at random from 2A (since each atom can be positive, negative, or unobserved, but unobserved atoms were not included in the data instances), and labeling each data instance according to p, and filtering out those on which p

abstained. Each training instance was then converted to a pictorial representation, by using randomly picked images of handwritten digits from the MNIST dataset (Lecun et al. a1 -a2 a3 a4 -a5 -a6 a7 -a8    

> ,label =head

# { }a1 a2 -a3 a4 -a5 -a6 -a7 a8   

> ,label =-head

# { }           

> Figure 5. Two training instances from a bptrain dataset labeled by the symbolic policy in Figure 1. Each instance consists of a sequence of 8 MNIST digit images representing atoms: digits with value 1() represent positive atoms, digits with value 2()represent negative atoms. The label ( head or -head ) is determined by applying the symbolic policy’s rules to the atom sequence.

1998) to represent their individual atoms, using the following convention: MNIST digits with a numerical value of 1 ( )were used to represent positive atoms and 2 ( ) negative atoms. Thus, for example, the context {a1 , . . . , -a8 } would be represented as { , . . . , }. Figure 5 shows two examples of full-context training data instances, labeled using the policy in Figure 1. bptrain and bpval used images from the train subset of MNIST, while bptest images from the test MNIST subset. 

4.2 Experimental Setup 

For the experiments, a set P of 30 randomly generated target policies was used. For each policy, the experiments were repeated 5 times using different randomly generated datasets (30 × 5 = 150 total experiments). The datasets had the following sizes: bptrain 20000 data instances, bpval 2000 , and 

bptest 2000 . The population of each generation was generated using all possible combinations of symbolic ( S0, S+, S↓) and neural ( Npw, Nrw) mutations. Experimentation showed that the evolutionary process could be considerably shortened by introducing multiple S+ mutations per generation, so 5 S+

mutations were used per generation. A convolutional neural network (CNN) architecture was used in NeuralModule s, composed of 2 convolutional layers, followed by 3 fully-connected layers (Figures 4 and 13). Since the data instances consisted of sequences of 8 MNIST images, each image was passed through the CNN sequentially, and the output vectors for each were concatenated into a single output vector (the same base encoder was used for the end-to-end baseline described in Section 4.6, with additional output layers for direct binary classification). The neural weights of Nrw offspring were randomly initialized using Xavier initialization (Glorot and Bengio 2010). The Adam optimizer (Kingma and Ba 2017) was used, with a learning rate of 0.001 .The semantic loss was calculated following the methodol-ogy in Section 2.3, incorporating the optimization techniques we detail in Section 3.3.2. The weighted model counting (WMC) was performed using the PySDD library (wannesm 2024). As discussed in Section 3.1, to investigate whether additional training signals could improve convergence when abductive feedback is limited, a reconstruction loss component was incorporated. For this reconstruction loss, a mirroring de-convolution decoder was used to reconstruct the input MNIST images using the NN output vector as input, employing the mean squared error (MSE) for its calculation. To constrain the decoder to reconstruct only 6No multiproc. 1 2 3 4 5 6

Number of processes 

> 1000
> 1200
> 1400
> 1600
> 1800
> 2000
> 2200
> Total experiment training times (s)
> Operation caching
> No
> Yes

1Figure 6. Box plots showing the impact of operation caching and parallel processing on total experiment training times, in experiments involving the training of multiple NeSy organisms. Each box represents 20 experiments (10 symbolic policies, each run twice), with each experiment involving the training of 100 NeSy organisms (in a non evolutionary setting). The x-axis indicates the number of parallel processes used (where each process corresponds to one CPU) on a single GPU, while the y-axis shows the total experiment training time in seconds. Blue boxes represent training without operation caching in semantic loss calculation, while orange boxes show training with operation caching enabled. Results demonstrate that operation caching consistently reduces training time across all parallelization configurations. The “No multiproc.” condition represents sequential training using a single CPU without the multiprocessing framework, while 1 process represents using a single process within the multiprocessing environment. The similarity between these configurations confirms that the multiprocessing environment itself introduces minimal overhead. Additionally, increasing the number of parallel processes substantially decreases total experiment duration, with efficiency gains plateauing at approximately 5 parallel processes. 

one version of each MNIST digit (numerical values {1, 2}), a Gumbel-Softmax operation (Jang et al. 2017; Maddison et al. 2017) was applied to the NN output vector before being given to the decoder. To evaluate different approaches to combining these loss components, ablation experiments were conducted comparing three semantic:reconstruction loss ratios: 1:0 (semantic loss only), 1:1 (equal weighting), and 1:10000 (heavily weighted reconstruction loss), as detailed in Section 4.5. For the evolutionary process, the upper limit maxgen =500 was used, and the early stopping criterion was set for when the correct performance on bpval was ≥ 99% . The threshold parameter t = 0 and the non-linearity parameter 

k = 2 were used. The organisms were trained for 5 epochs using bptrain , with a batch size of 2000. As shown in Figure 7, this batch size was selected because it achieves the best overall balance between training time per epoch and cumulative training efficiency, providing significant performance gains over smaller batch sizes while avoiding the diminishing returns observed at larger batch sizes. 

4.3 Main Results 

Figure 8 shows a typical example of an experimental run (further examples are included in Appendix B). The upper subplot shows the performance against bpval and bptest of the lineage of f ittest organisms selected by the evolutionary process. As a typical example, the correct performance (blue line) of the organisms gradually increases, with a corresponding decrease in abstain performance (green line). The wrong performance (red line) increases slightly early in the process, but eventually falls back to near zero. Figure 9 shows the aggregated results of all experiments on their respective testing subsets bptest . The top subplot shows performance metrics across training progress: correct performance (blue) rises steadily from near-zero to approaching ∼99% median by the end of training, while abstain performance (green) follows an inverse trajectory, decreasing from initial dominance to near-zero as symbolic policies develop. Wrong performance (red) remains relatively low throughout, with median values staying below 15% and eventually declining as correct performance improves. The middle and bottom subplots show the corresponding decreases in both semantic and reconstruction losses. The reduction in reconstruction loss reflects neural learning, while the decrease in semantic loss indicates successful joint optimization of both components. 

4.4 Performance Optimization Validation 

To empirically validate the effectiveness of the technical contributions described in Section 3.3, we conducted a series of experiments measuring their impact on computational performance. The validation experiments trained 100 NeSy organisms across 20 independent runs (10 symbolic policies, each run twice) under various configurations of parallel processing and operation caching. The total training times were recorded for each configuration, with results presented in Figure 6. Operation caching during semantic loss calculation provides substantial speedup across all parallelization configurations. Comparing uncached (blue boxes) with cached (orange boxes) results in Figure 6 shows that caching the WMC computational graph consistently and significantly reduces training times. This shows that repeated WMC calculations represent a major computational bottleneck that caching effectively addresses. Parallel training of multiple neural networks on a single GPU yields additional performance gains. The results show that increasing parallelization from sequential execution to 7Figure 7. Training time comparison across different batch sizes (500, 1000, 2000, 10000) with and without operation caching during semantic loss calculation. For each batch size, the top subplot shows training time per epoch (with median and Q1–Q3 ranges indicated by solid lines and shaded areas, respectively), while the bottom subplot shows cumulative training time (mean) across epochs. Batch size 2000 achieves the best overall balance, providing the lowest training time per epoch while maintaining efficient cumulative training time, making it the optimal choice for the main experiments. 0.0                                                           

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Performance
> Correct ( ̂ptest )Correct ( ̂pval )Abstain ( ̂ptest )Abstain ( ̂pval )Wrong ( ̂ptest )Wrong ( ̂pval )
> 0.0
> 5.0
> 10.0
> Semantic loss  Semantic loss
> 0.0
> 0.5
> 1.0
> Recon. loss  Reconstruction loss
> 510 15 20 25 30 35
> Generations
> BBBBBBBBBBBBBBBBDBBBBBBBBBBBBBBBBBNB
> S+(Addition mutation) S0(Clone mutation) S↓(Simplification mutation) Npw(NN weights inherited) Nrw(NN weights not inherited) Homogeneous rule added
> 1

Figure 8. Results of a single experimental run: columns represent generations, showing the f ittest organism’s performance and training history per generation. The 1st subplot shows correct, abstain, and wrong performances on bpval and bptest . The 2nd and 3rd subplots show the semantic and reconstruction losses, respectively. Symbolic and neural mutations are differentiated by background colors and hatch patterns, with homogeneous rule additions also marked by a hatch pattern. The letters on the top x-axis indicate the fitness group the generation’s f ittest organism was selected from (beneficial, neutral, or detrimental groups). Generation 0 is omitted, since the initial organism is not trained due to an empty policy. 

approximately 5 parallel processes substantially decreases total experiment duration. Beyond this point, efficiency gains plateau, likely due to GPU resource saturation and the overhead of managing concurrent processes. The comparable performance between sequential training (“No multiproc.”) and single-process execution (1 process) confirms that the multiprocessing framework itself introduces minimal overhead. These validation results demonstrate that both optimiza-tions are essential for the feasibility of the large-scale evolutionary experiments presented in this paper. For the main experiments reported in Section 4.2, we adopted a configuration of 4 parallel processes per GPU, balancing computational efficiency with resource utilization based on the observed performance characteristics. 

4.5 Ablation Study: Loss Ratio Comparison 

As motivated in Section 3.1, the reconstruction loss was introduced to provide an additional training signal when abductive feedback is limited during early evolutionary generations. To evaluate whether this auxiliary signal improves convergence, ablation experiments compared three different semantic:reconstruction loss ratios: 1:0 (semantic loss only), 1:1 (equal weighting), and 1:10000 (heavily weighted reconstruction loss). Results are shown in Figure 10, which aggregates results by generation number (unlike Figure 9, which interpolates results to a unified scale). The same general trend observed in individual experiments (e.g., Figure 8) holds across all loss ratio configurations: during the evolutionary process, there is a gradual increase in correct performance with a simultaneous reduction 80.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> Performance

Correct (Median) 

Correct (Mean) 

Correct (Q1–Q3) 

Abstain (Median) 

Abstain (Mean) 

Abstain (Q1–Q3) 

Wrong (Median) 

Wrong (Mean) 

Wrong (Q1–Q3) 

0

5

10 

> Semantic loss

Semantic Loss (Median) 

Semantic Loss (Mean) 

Semantic Loss (Q1–Q3) 

0 20 40 60 80 100 

0.2

0.4

> Recon. loss

Recon. Loss (Median) 

Recon. Loss (Mean) 

Recon. Loss (Q1–Q3) 

Training progress % 

1Figure 9. Aggregated results of all experiments on their respective bptest subsets. Since experiments concluded at different generation numbers, results were interpolated to a unified 1–100 scale to enable meaningful aggregation across all runs. Shaded areas depict the interquartile range (Q1–Q3) for each metric. 0.0 

> 20.0
> 40.0
> 60.0
> 80.0
> 100.0
> Performance

Semantic:Reconstruction = 1:0 

> Correct (Median)
> Correct (Q1–Q3)
> Abstain (Median)
> Abstain (Q1–Q3)
> Wrong (Median)
> Wrong (Q1–Q3)

Semantic:Reconstruction = 1:1 

> Correct (Median)
> Correct (Q1–Q3)
> Abstain (Median)
> Abstain (Q1–Q3)
> Wrong (Median)
> Wrong (Q1–Q3)

Semantic:Reconstruction = 1:10000                

> Correct (Median)
> Correct (Q1–Q3)
> Abstain (Median)
> Abstain (Q1–Q3)
> Wrong (Median)
> Wrong (Q1–Q3)
> 0.0
> 2.0
> 4.0
> 6.0
> 8.0
> 10.0
> 12.0
> Semantic Loss
> Semantic Loss (Median)
> Semantic Loss (Q1–Q3)
> Semantic Loss (Median)
> Semantic Loss (Q1–Q3)
> Semantic Loss (Median)
> Semantic Loss (Q1–Q3)
> 0100 200 300 400 500
> 10 −5
> 10 −3
> 10 −1
> 10 1
> 10 3
> Recon. Loss (Log scale)
> No reconstruction loss (ratio = 0)
> 0100 200 300 400 500
> Recon. Loss (Median)
> Recon. Loss (Q1–Q3)
> 0100 200 300 400 500
> Recon. Loss (Median)
> Recon. Loss (Q1–Q3)

Generations 

> 1

Figure 10. Aggregated results comparing three semantic:reconstruction loss ratios (1:0, 1:1, and 1:10000) on their respective bptest 

subsets. Results are aggregated by generation number: for each generation, data from all experiments that reached that generation are combined. Since experiments terminate at different generations, later generations aggregate fewer experiments. The top subplot shows performance metrics (correct, abstain, and wrong predictions), while the middle and bottom subplots show semantic and reconstruction losses, respectively. Shaded areas depict the interquartile range (Q1–Q3) for each metric. 

in abstention performance. Due to the greedy nature of the evolutionary algorithm, which always selects a single organism as the f ittest of each generation, there is an increase in wrong performance early in the evolutionary process, which later decreases to near zero as correct performance increases. To assess whether different loss ratios affect evolutionary efficiency, we compared cumulative correct performance across all generations for each experiment run (giving one summary value per experiment), grouped by loss ratio. This cumulative metric captures the overall evolutionary efficiency: configurations that reach high performance faster accumulate higher cumulative scores, even if final performance is similar. Statistical analysis using Dunn’s non-parametric test revealed no statistically significant differences across the three loss ratio configurations (all pairwise p > 0.05 ), as detailed in Table 1. This indicates that the reconstruction loss component, despite being motivated by theoretical considerations about 9Table 1. Dunn’s test p-values for cumulative correct performance comparisons between semantic:reconstruction loss ratios (1:0, 1:1, 1:10000). No significant differences were detected (all values > 0.05 ). 

Ratio 0 1 10000 0 1.00 0.78 0.87 1 0.78 1.00 0.91 10000 0.87 0.91 1.00 incomplete abductive feedback, does not provide measurable benefit for convergence. The implications of this finding are discussed in Section 5. 

4.6 Baselines 

Due to the specific nature of the policies used in this study, direct comparison with other NeSy frameworks in the literature was not feasible. Frameworks such as MetaAbd (Dai and Muggleton 2021), NSIL (Cunnington et al. 2022), and NeuralFastLAS (Charalambous et al. 2023) employ different policy structures, learning paradigms, and assumptions about knowledge representation that make them incompatible with our evolutionary approach to learning non-differentiable shallow propositional policies. Instead, we compared our approach against an end-to-end neural baseline; specifically, the same CNN architecture used in the NeuralModule of our NeSy organisms, but trained directly on the labeled data without any symbolic component. The end-to-end neural baseline was evaluated on the same 150 datasets used for the NeSy experiments (30 target policies × 5 random dataset generations each, as described in Section 4.2), with identical training/validation/test splits. The baseline was trained using the Adam optimizer with learning rate 0.001 and identical batch sizes to the NeSy organisms. The baseline network receives sequences of 8 MNIST digit images as input and is trained to predict the binary label directly through supervised learning, without any intermediate symbolic reasoning. While the base CNN encoder architecture remains identical to that used in the NeuralModule (processing each of the 8 images sequentially to produce n-dimensional output vectors), the end-to-end baseline includes additional layers for direct binary classification: the concatenated encoder outputs are flattened (producing an 8n-dimensional vector) and passed through two fully-connected layers (64 neurons with ReLU activation, then 2 output neurons) followed by softmax, enabling direct prediction of head or -head without symbolic reasoning. Figure 11 shows the progression of the training of the end-to-end neural baseline over 100 epochs. Although median performance reaches > 98% accuracy on all sets, the results exhibit substantial variance between networks, with occasional convergence failures contributing to the large interquartile ranges visible in the figure. 

5 Discussion 

The results presented in Section 4.3 demonstrate that the proposed evolutionary framework successfully enables NeSy systems to learn non-differentiable symbolic policies while concurrently training neural networks. Starting from empty symbolic policies and randomly initialized neural 0.0      

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Accuracy
> Train Acc. (Median)
> Val. Acc. (Median)
> Test Acc. (Median)
> 020 40 60 80 100
> Epochs
> 0.0
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> Loss  Train Loss (Median)
> Val. Loss (Median)

Figure 11. Training progression of the end-to-end neural baseline over 100 epochs across 150 datasets (30 target policies 

× 5 random dataset generations each). The baseline uses the same CNN architecture as the NeSy organisms’ 

NeuralModule but is trained directly on labeled data without symbolic components. Networks were trained using cross-entropy loss with the Adam optimizer. Performance metrics show accuracy on bptrain (blue), bpval (green), and bptest (red), with median accuracy > 98% on all sets. The lower subplot shows training and validation losses. Shaded areas depict the interquartile range (Q1–Q3) for each metric. The large variation in ranges shows that while most networks learn very well (median accuracy approaching 100%), occasionally networks fail to converge effectively, resulting in lower accuracy and higher losses. 

weights, the evolutionary process consistently discovers policies that approximate hidden targets, with a median correct performance approaching 99% by the end of training (Figure 9). The characteristic pattern observed across experiments—correct performance rising steadily while abstentions decrease (Figure 8)—indicates that the evolutionary process progressively builds symbolic policies that cover an increasing portion of the input space. The simultaneous decrease in both losses confirms effective learning. Reconstruction loss decrease indicates neural learning through self-supervision, while semantic loss decrease reflects successful joint optimization, as this measure depends on both neural network outputs and the symbolic policy’s abductive feedback. These findings validate the central hypothesis of this work: that non-differentiable symbolic policies can be learned from scratch through evolutionary search, without requiring predefined symbolic knowledge or gradient-based policy optimization. The fact that the majority of experiments converge to near-perfect correct performance demonstrates the viability of this approach for concurrent neural-symbolic learning. Although the majority of the experiments followed this successful convergence pattern, in some cases the evolutionary process entered what we term a stuck state , i.e. a local optimum where organisms achieve partial correct performance on training and validation data, but fail to improve further across subsequent generations. Figure 12 shows such an example: the evolutionary process falls into a stuck state at generation 3. The stuck state persists 10 0.0                                                                                                        

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Performance
> Correct ( ̂ptest )Correct ( ̂pval )Abstain ( ̂ptest )Abstain ( ̂pval )Wrong ( ̂ptest )Wrong ( ̂pval )
> 0.0
> 5.0
> 10.0
> Semantic loss  Semantic loss
> 0.0
> 0.5
> 1.0
> Recon. loss  Reconstruction loss
> 510 15 20 25 30 35 40 45 50 55 60 65 70
> Generations
> BBBNNNNNNNNNNNNNNNBNBBBBBNDBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBNNDNBBBB
> S+(Addition mutation) S0(Clone mutation) S↓(Simplification mutation) Npw(NN weights inherited) Nrw(NN weights not inherited) Homogeneous rule added
> 1

Figure 12. Experimental run showing the stuck state for part of the evolutionary process. 

until generation 21, whereupon it is broken, allowing the evolutionary process to sequentially select a lineage of organisms that reach a correct performance of > 99% at generation 74. Stuck states result from the addition of a homogeneous rule —a rule whose body contains literals that all share the same sign—to an organism’s policy via S+ or S↓

mutations. Examples of homogeneous rules include “ a1 ,

a2 implies head ” (all positive atoms) or “ -a1 , -a2 implies head ” (all negative atoms). When such rules are added to a policy, the NeuralModule sometimes converges to a trivial solution where it classifies all input images into the same category (all as digit 1 or all as digit 2), regardless of the actual input content. This uniform classification causes the homogeneous rule to fire consistently, leading the organism to produce the same prediction ( head or -head )for all data instances. Consequently, the organism achieves misleadingly elevated relative fitness by “correctly” predicting the proportion of bptrain and bpval labeled with the same atom ( head or -head ) that its NeuralModule uniformly predicts. This high fitness leads to preferential selection of such organisms, trapping the evolutionary process in a local optimum (a more detailed technical explanation is given in Appendix B). The stuck state phenomenon, combined with the challenge of training neural networks when symbolic policies are incomplete during early evolutionary generations, motivated the introduction of reconstruction loss as a self-supervised learning mechanism. As mentioned in Section 3.1, when organisms possess minimal symbolic policies, the abductive feedback available for calculating semantic loss is limited, potentially hindering effective neural learning. The framework incorporated a reconstruction loss component with Gumbel-Softmax regularization to provide an additional training signal during these challenging phases. However, the results of the ablation study reported in Section 4.5 demonstrate that the semantic loss alone is sufficient for successful convergence, indicating that abductive feedback from the symbolic module, even when policies are initially incomplete, provides an adequate training signal for the neural networks. This simplifies the proposed framework: the reconstruction loss component, despite being initially incorporated on the basis of theoretical motivations, is not necessary for achieving strong performance. The framework can operate effectively with semantic loss alone, reducing architectural complexity and eliminating the computational overhead associated with the decoder network and reconstruction loss calculation. Ultimately, given enough generations, the majority of stuck experiments eventually break out of the stuck state. This is evident in Figure 9, where at the end of training, the 

median correct performance approaches 100% , while the corresponding mean performance trails slightly behind (while still following an upward trend), indicating a very small percentage of stuck experiments at the end of training. The interquartile range (Q1–Q3) shown in the shaded regions of Figure 9 shows variance across experiments, which arises from the stochastic nature of both the evolutionary process (random rule generation and selection) and the neural network training (weight initialization and batch sampling). This variance reflects the inherent exploration-exploitation trade-off in evolutionary search: some lineages discover effective symbolic policies more quickly through fortuitous mutations, while others require more generations to escape suboptimal regions of the search space. Given the strong performance of the end-to-end neural baseline described in Section 4.6, a natural question arises: 

Why pursue the evolutionary NeSy approach when the end-to-end neural baseline achieves comparable or superior performance with substantially lower computational cost? 

The baseline achieves a median accuracy exceeding 98% with a single network trained for 100 epochs, while the evolutionary framework requires training multiple organisms across many generations, representing a significant difference in computational expense. This disparity reflects the computational price of interpretability mentioned in the Introduction, understood as the additional computational resources and longer training times required to obtain human-readable symbolic policies alongside neural components. The value proposition of this trade-off lies in the interpretable symbolic policies that emerge from the evolutionary process. Unlike the black-box nature of pure neural networks, the learned symbolic policies provide 11 explicit, inspectable logical rules that explain the system’s reasoning. These policies can be examined by domain experts without machine learning expertise, modified if necessary, and verified for consistency with domain knowledge. The symbolic component offers transparency that is fundamentally absent in end-to-end neural approaches, regardless of their predictive performance. This trade-off becomes justifiable in domains where interpretability and trust are critical. In safety-critical applications (medical diagnosis, autonomous systems), legal and regulatory contexts (credit decisions, criminal justice), and scientific domains that require explainable models, the ability to inspect and validate reasoning processes may outweigh computational efficiency concerns. When decisions must be explained to stakeholders, audited for compliance, or validated by domain experts, symbolic policies provide a level of transparency that end-to-end neural baselines cannot match. The additional computational cost provides verifiable symbolic policies that enable accountability and validation by domain experts, which may be essential in such contexts. 

6 Related Work 

Early approaches to neural-symbolic integration focused on making symbolic reasoning differentiable to enable gradient-based learning. Serafini and d’Avila Garcez (2016) introduced Logic Tensor Networks (LTNs), mapping logical constructs to differentiable tensor operations, while Evans and Grefenstette (2018) proposed ∂ILP, embedding symbolic rules as differentiable components amenable to backpropagation through forward-chaining deduction implemented as differentiable computation. These foundational works demonstrated that symbolic reasoning could be integrated with deep learning through differentiability, although both required the symbolic component to support gradient propagation. More recent work has advanced joint learning of neural perception and symbolic reasoning. Mao et al. (2019) introduced the Neuro-Symbolic Concept Learner (NS-CL), demonstrating how disentangled perception and symbolic reasoning yield transparent, compositional visual understanding through joint acquisition of visual concepts, word meanings, and semantic parsing. Dai and Muggleton (2021) proposed MetaAbd, which jointly learns subsymbolic perception and symbolic reasoning from raw data within an expectation-maximization framework, successfully inducing reusable logic programs with advantages in predictive accuracy and data efficiency over end-to-end neural baselines. Building on this paradigm, Cunnington et al. (2022, 2024) developed NSIL, which alternates between neural learning of latent concepts and Answer Set Programming hypothesis learning, later extending it to handle unlabeled raw data. Charalambous et al. (2023) introduced NeuralFastLAS, training neural networks alongside learned rule posteriors with semantic loss, achieving high accuracies with reduced training times. Daniele et al. (2023) proposed Deep Symbolic Learning (DSL), which departs from relying on given symbolic knowledge by jointly learning perception and symbol composition functions within a differentiable pipeline, making discrete symbolic choices through policy functions inspired by reinforcement learning. Complementary advances in automatic predicate invention have emerged from Sha et al. (2023), who introduced NeSy-π for automatic predicate invention from visual scenes, and Barbiero et al. (2023), whose Deep Concept Reasoner generates and evaluates fuzzy logic rules from concept embeddings without explicit concept supervision, demonstrating semantically consistent reasoning learned directly from embeddings. Integration approaches have diversified across methodolo-gies. D ´ıaz-Rodr ´ıguez et al. (2022) fused deep learning with domain knowledge graphs through XAI-informed training aligning feature attributions with knowledge. Pryor et al. (2023) introduced NeuPSL, extending probabilistic soft logic with neural predicates for end-to-end gradient training. Zhong et al. (2023) integrated large language models into abductive learning through ChatABL, transforming visual features into natural language logical facts for LLM-based reasoning. Recent surveys have provided systematic perspectives on the field. Vermeulen et al. (2023) categorized NeSy tasks and demonstrated that probabilistic logic-programming approaches achieve superior performance at higher computational cost, while Marra (2024) proposed the NeSy recipe, a unified two-phase pipeline facilitating systematic comparison and design across approaches. While these approaches have advanced neural-symbolic integration, most assume predefined symbolic structures, rule templates, or background knowledge. Our work addresses this limitation through evolutionary learning, starting with empty policies and progressively building symbolic knowledge through mutation and selection without requiring differentiability or predefined structures. This evolutionary approach to concurrent neural and symbolic learning offers a complementary direction for NeSy systems, particularly in domains where symbolic knowledge acquisition from experts is challenging. 

7 Conclusions & Future Work 

In this study, we propose a new framework that facilitates the simultaneous training of both neural and symbolic components within NeSy systems. Our extensive experimentation validates the effectiveness of this approach, demonstrating that NeSy systems can start with empty symbolic policies and randomly initialized neural weights, and progressively evolve to closely approximate hidden target policies. Arguably, the most important contribution of our work is that it demonstrates that it is feasible to learn non-differentiable policies, while simultaneously training neural networks in NeSy systems. The demonstrated capability for evolvable policies within NeSy systems may be a first step towards facilitating research in areas where symbolic knowledge acquisition from domain experts is challenging. Although our ablation study on reconstruction loss demonstrated its dispensability, future work could explore the individual contributions of other framework components to performance metrics. Additionally, it will be explored whether evolution can be replaced by other search methods, for example through the use of less greedy versions of the evolutionary process that allow the retention of different forms diversity in the population (e.g., quality diversity optimization techniques, see Chatzilygeroudis et al. 2021; Mouret and 12 Clune 2015; Vassiliades et al. 2018). Another research avenue would be to experiment with larger target policies (in terms of number of binary concepts), and the use of the proposed framework with data from real-world scenarios. 

Funding 

This project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme, under Grant agreement No 739578, complemented by the Government of the Republic of Cyprus through the Directorate General for European Programmes, Coordination and Development. 

Acknowledgements 

We are grateful to Kyriacos Mosphilis and Vasileios Markos for their insightful discussions throughout the project. 

Supplemental material 

The source code, experimental configurations, and data generation scripts for reproducing all experiments presented in this paper are publicly available at https://github.com/CYENS/ evolvable-nesy .

Notes 

1. The term “policy” is used to refer to logic-based knowledge ,referred to variously in the literature as “theory” (Tsamoura et al. 2021), “program” (Gaunt et al. 2017), “knowledge base” (Michael 2019), among others. 2. The original Evolvability framework guarantees that the neutral group will be nonempty by means of clone mutations (Sec. 3.2), but in our case this is not guaranteed, due to the randomness introduced by the NeuralModule and its interaction with the 

SymbolicModule .3. It should be noted that the abduce() method does not map to any meaningful functionality in the NeuralModule , but was included for symmetry’s sake. 

References 

Barbiero P, Ciravegna G, Giannini F, Zarlenga ME, Magister LC, Tonda A, Lio P, Precioso F, Jamnik M and Marra G (2023) Interpretable Neural-Symbolic Concept Reasoning. In: 

Proceedings of the 40th International Conference on Machine Learning . PMLR, pp. 1801–1825. Bertsimas D, Delarue A, Jaillet P and Martin S (2019) The Price of Interpretability. doi:10.48550/arXiv.1907.03419. Besold TR, d’Avila Garcez A, Bader S, Bowman H, Domingos P, Hitzler P, K ¨uhnberger KU, Lamb LC, Lima PMV, De Penning L, Pinkas G, Poon H and Zaverucha G (2021) Chapter 1. Neural-Symbolic Learning and Reasoning: A Survey and Interpretation. In: Hitzler P and Sarker MK (eds.) Frontiers in Artificial Intelligence and Applications . IOS Press. ISBN 978-1-64368-244-0 978-1-64368-245-7, pp. 1–51. doi:10.3233/FAIA210348. Charalambous T, Aspis Y and Russo A (2023) Neural-FastLAS: Fast Logic-Based Learning from Raw Data. doi:10.48550/arXiv.2310.05145. Chatzilygeroudis K, Cully A, Vassiliades V and Mouret JB (2021) Quality-Diversity Optimization: A Novel Branch of Stochastic Optimization. In: Pardalos PM, Rasskazova V and Vrahatis MN (eds.) Black Box Optimization, Machine Learning, and No-Free Lunch Theorems , volume 170. Cham: Springer International Publishing. ISBN 978-3-030-66514-2 978-3-030-66515-9, pp. 109–135. doi:10.1007/978-3-030-66515-9 4. Chavira M and Darwiche A (2008) On Probabilistic Inference by Weighted Model Counting. Artificial Intelligence 172(6): 772– 799. doi:10.1016/j.artint.2007.11.002. Cunnington D, Law M, Lobo J and Russo A (2022) Inductive Learning of Complex Knowledge from Raw Data. In: AAAI Fall Symposium , volume 3332. Arlington, Virginia, USA: CEUR Workshop Proceedings, pp. 1–21. Cunnington D, Law M, Lobo J and Russo A (2024) Neuro-Symbolic Learning of Answer Set Programs from Raw Data. doi:10.48550/arXiv.2205.12735. Dai WZ and Muggleton SH (2021) Abductive Knowledge Induction From Raw Data. doi:10.48550/arXiv.2010.03514. Dai WZ, Xu Q, Yu Y and Zhou ZH (2019) Bridging Machine Learning and Logical Reasoning by Abductive Learning. In: Wallach H, Larochelle H, Beygelzimer A, dAlch ´e-Buc F, Fox E and Garnett R (eds.) Advances in Neural Information Processing Systems , volume 32. Vancouver, Canada: Curran Associates, Inc., pp. 2811–2822. Daniele A, Campari T, Malhotra S and Serafini L (2023) Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions. doi:10.48550/arXiv.2208.11561. Darwiche A (2011) SDD: A New Canonical Representation of Propositional Knowledge Bases. In: Twenty-Second International Joint Conference on Artificial Intelligence . pp. 819–826. Dasgupta S, Frost N, Moshkovitz M and Rashtchian C (2020) Explainable k-means clustering: Theory and practice. In: XXAI Workshop . Vienna, Austria, pp. 1–8. d’Avila Garcez A and Lamb LC (2023) Neurosymbolic AI: The 3rd wave. Artificial Intelligence Review 56(11): 12387–12406. doi:10.1007/s10462-023-10448-w. Dessain J, Bentaleb N and Vinas F (2023) Cost of Explainability in AI: An Example with Credit Scoring Models. In: Longo L (ed.) Explainable Artificial Intelligence . Cham: Springer Nature Switzerland. ISBN 978-3-031-44064-9, pp. 498–516. doi:10.1007/978-3-031-44064-9 26. D´ıaz-Rodr ´ıguez N, Lamas A, Sanchez J, Franchi G, Donadello I, Tabik S, Filliat D, Cruz P, Montes R and Herrera F (2022) EXplainable Neural-Symbolic Learning (X-NeSyL) methodol-ogy to fuse deep learning representations with expert knowledge graphs: The MonuMAI cultural heritage use case. Information Fusion 79: 58–83. doi:10.1016/j.inffus.2021.09.022. Evans R and Grefenstette E (2018) Learning Explanatory Rules from Noisy Data. Journal of Artificial Intelligence Research 61: 1–64. doi:10.1613/jair.5714. Garcia GGP, Steimle LN, Marrero WJ and Sussman JB (2024) Inter-pretable Policies and the Price of Interpretability in Hyperten-sion Treatment Planning. Manufacturing & Service Operations Management 26(1): 80–94. doi:10.1287/msom.2021.0373. Gaunt AL, Brockschmidt M, Kushman N and Tarlow D (2017) Differentiable Programs with Neural Libraries. In: Proceedings of the 34th International Conference on Machine Learning .PMLR, pp. 1213–1222. Glorot X and Bengio Y (2010) Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics . JMLR Workshop and Conference Proceedings, pp. 249–256. 

13 Hitzler P and Sarker MK (eds.) (2021) Neuro-Symbolic Artificial Intelligence: The State of the Art , Frontiers in Artificial Intelligence and Applications , volume 342. IOS Press. ISBN 978-1-64368-244-0 978-1-64368-245-7. doi:10.3233/FAIA342. Jang E, Gu S and Poole B (2017) Categorical Reparameterization with Gumbel-Softmax. doi:10.48550/arXiv.1611.01144. Kakas AC (2017) Abduction. In: Sammut C and Webb GI (eds.) 

Encyclopedia of Machine Learning and Data Mining , 2nd edition. Boston, MA: Springer US, pp. 1–8. Kingma DP and Ba J (2017) Adam: A Method for Stochastic Optimization. doi:10.48550/arXiv.1412.6980. Laber ES and Murtinho L (2021) On the price of explainability for some clustering problems. In: Proceedings of the 38th International Conference on Machine Learning . PMLR, pp. 5915–5925. Lecun Y, Bottou L, Bengio Y and Haffner P (1998) Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE 86(11): 2278–2324. doi:10.1109/5.726791. LeNail A (2019) NN-SVG: Publication-Ready Neural Network Architecture Schematics. Journal of Open Source Software 

4(33): 747. doi:10.21105/joss.00747. Liang Y, Li S, Yan C, Li M and Jiang C (2021) Explaining the Black-Box Model: A Survey of Local Interpretation Methods for Deep Neural Networks. Neurocomputing 419: 168–182. doi:10.1016/j.neucom.2020.08.011. Maddison CJ, Mnih A and Teh YW (2017) The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables. doi:10.48550/arXiv.1611.00712. Mao J, Gan C, Kohli P, Tenenbaum JB and Wu J (2019) The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision. doi:10.48550/arXiv.1904.12584. Markos V and Michael L (2022) Prudens: An Argumentation-Based Language for Cognitive Assistants. In: Governatori G and Turhan AY (eds.) Rules and Reasoning , Lecture Notes in Computer Science. Cham: Springer International Publishing. ISBN 978-3-031-21541-4, pp. 296–304. doi:10.1007/978-3-031-21541-4 19. Markos V, Thoma M and Michael L (2022) Machine Coaching with Proxy Coaches. In: Kuhlmann I, Mumford J and Sarkadi S (eds.) Proceedings of the 1st Workshop on Argumentation & Machine Learning , CEUR Workshop Proceedings , volume 3208. Cardiff, Wales: CEUR, pp. 45–64. Marra G (2024) From Statistical Relational to Neuro-Symbolic Artificial Intelligence. Proceedings of the AAAI Con-ference on Artificial Intelligence 38(20): 22678–22678. doi:10.1609/aaai.v38i20.30294. McCarthy J (1998) Elaboration Tolerance. In: Common Sense 98 ,volume 98. London, U.K., p. 2. Michael L (2019) Machine Coaching. In: IJCAI 2019 Workshop on Explainable Artificial Intelligence . Macau, China, pp. 80–86. Michael L (2023) Chapter 11. Autodidactic and Coachable Neural Architectures. In: Hitzler P, Sarker MK and Eberhart A (eds.) 

Compendium of Neurosymbolic Artificial Intelligence , Frontiers in Artificial Intelligence and Applications , volume 369. IOS Press. ISBN 978-1-64368-406-2 978-1-64368-407-9, pp. 235– 248. doi:10.3233/FAIA230143. Mouret JB and Clune J (2015) Illuminating search spaces by mapping elites. doi:10.48550/arXiv.1504.04909. Pryor C, Dickens C, Augustine E, Albalak A, Wang W and Getoor L (2023) NeuPSL: Neural Probabilistic Soft Logic. doi:10.48550/arXiv.2205.14268. Serafini L and d’Avila Garcez A (2016) Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge. doi:10.48550/arXiv.1606.04422. Sha J, Shindo H, Kersting K and Dhami DS (2023) Neural-Symbolic Predicate Invention: Learning Relational Concepts from Visual Scenes. In: NeSy 2023, 17th International Workshop on Neural-Symbolic Learning and Reasoning , volume 3432. Certosa di Pontignano, Siena, Italy: CEUR Workshop Proceedings, pp. 1–15. Trinh TH, Wu Y, Le QV, He H and Luong T (2024) Solving Olympiad Geometry Without Human Demonstrations. Nature 

625(7995): 476–482. doi:10.1038/s41586-023-06747-5. Tsamoura E, Hospedales T and Michael L (2021) Neural-Symbolic Integration: A Compositional Perspective. Proceedings of the AAAI Conference on Artificial Intelligence 35(6): 5051–5060. Valiant LG (1984) A Theory of the Learnable. Communications of the ACM 27(11): 1134–1142. doi:10.1145/1968.1972. Valiant LG (2009) Evolvability. Journal of the ACM 56(1): 1–21. doi:10.1145/1462153.1462156. Vassiliades V, Chatzilygeroudis K and Mouret JB (2018) Using Centroidal Voronoi Tessellations to Scale Up the Multidimensional Archive of Phenotypic Elites Algorithm. 

IEEE Transactions on Evolutionary Computation 22(4): 623– 630. doi:10.1109/TEVC.2017.2735550. Vermeulen A, Manhaeve R and Marra G (2023) An Experimental Overview of Neural-Symbolic Systems. In: Bellodi E, Lisi FA and Zese R (eds.) Inductive Logic Programming , Lecture Notes in Computer Science. Cham: Springer Nature Switzerland. ISBN 978-3-031-49299-0, pp. 124–138. doi:10.1007/978-3-031-49299-0 9. wannesm (2024) PySDD. Xu J, Zhang Z, Friedman T, Liang Y and Van den Broeck G (2018) A Semantic Loss Function for Deep Learning with Symbolic Knowledge. In: Proceedings of the 35th International Conference on Machine Learning . Stockholm, Sweden: PMLR, pp. 5502–5511. Zhong T, Wei Y, Yang L, Wu Z, Liu Z, Wei X, Li W, Yao J, Ma C, Li X, Zhu D, Jiang X, Han J, Shen D, Liu T and Zhang T (2023) ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT. doi:10.48550/arXiv.2304.11107. 

14 A Empirical Setup (Details) 

The relative fitness during the evolutionary process was calculated using the score matrix shown in Table 2. The CNN architecture used in the experiments is shown in Figure 13. This base encoder architecture was used in both the NeSy organisms’ NeuralModule and the end-to-end baseline. Since each data instance was made up of eight MNIST images, the same NN was used eight times sequentially to obtain predictions for all input images. For the NeSy organisms, the encoder outputs n neurons per image (8 atoms), producing 8n outputs total that are concatenated and used as symbolic atom activations. For the end-to-end baseline, these 8n concatenated outputs are instead flattened and passed through two additional fully-connected layers: a hidden layer with 64 neurons and ReLU activation, followed by an output layer with 2 neurons (representing head and 

-head ), with softmax applied for binary classification. Each experiment was run in a Rocky Linux 8.5 environment, using Python v3.11.6, with 4 CPU cores and 48GB RAM, and an Nvidia RTX A5000 GPU with 24GB VRAM.  

> Table 2. Score matrix used for the calculation of relative fitness.

Offspring 

Correct Abstain W rong 

> Parent

Correct 0 −1 −1

Abstain 1 0 −1

W rong 1 1 0

B Results (Details) 

B.1 Aggregated Results 

Detailed aggregated results at the final generation for all experiments are shown in Table 3.  

> Table 3. Aggregated results at the end of all experiments on
> bptest .

Median Mean SD Correct 0.992 0.944 0.109 Abstain 0.000 0.002 0.003 Wrong 0.005 0.053 0.110 

B.2 Stuck State 

Figures 17 to 20 show the results of individual experimental runs that exhibit the stuck state for part of their evolutionary process; in all these cases the stuck state was eventually broken. During some evolutionary runs, organisms entered stuck states (local optima where they achieve partial correct performance on training and validation data, but fail to improve further across subsequent generations). Organisms become stuck after a symbolic S+ or S↓ mutation adds a 

homogeneous rule to their policy. A homogeneous rule is a rule whose body contains literals that all share the same sign, such as “ a1 , a2 , a3 implies head ” (all positive atoms) or “ -a1 , -a2 , -a3 implies head ” (all negative atoms). The addition of such rules triggers an adverse interaction between the homogeneous symbolic structure and the sequen-tial neural architecture. Specifically, the NeuralModule 

converges to a trivial solution where it classifies all input images into the same category (all as digit 1 or all as digit 2), regardless of the actual input content. This uniform classification causes the homogeneous rule in the symbolic policy to fire consistently for all data instances, causing the 

SymbolicModule to always produce the same prediction (head or -head ). Consequently, the organism achieves misleadingly high relative fitness by “correctly” predicting the proportion of bptrain and bpval labeled with the atom ( head or 

-head ) that its NeuralModule uniformly outputs. This high fitness leads to preferential selection of such organisms during the evolutionary process. The stuck state persists because it is not often that subsequent mutations produce offspring that exceed the local optimum fitness achieved by organisms with homogeneous rules. The pattern described above is due to an adverse interaction between the NN architecture used in the NeuralModule ,and incomplete hypothesis policies that give one-sided signals to it during training, due to the addition of homogeneous rules to the hypothesis policies. Since each image of each atom is processed by the NN sequentially (see Figure 4), the signals given by the homogeneous rule sometimes lead the NN to always predict the same value regardless of input. However, the occurrence of the pattern is limited to ∼15% 

of the experiments at any given time, and the percentage decreases as the evolutionary process continues, indicating that the pattern is eventually broken in the majority of the experiments. This is evident in Figure 9, where at the end of training the median correct performance approaches 100% ,while the corresponding mean performance trails slightly behind (while still following an upward trend), indicating a certain number of experiments that conclude while still stuck. Empirically, it was observed that most instances of stuck experiments eventually break out of the stuck state, given enough generations. Figure 12 shows an example of such an experimental run, that becomes stuck early in the evolutionary process, but eventually becomes unstuck, and finally reaches almost 100% correct performance. 

B.3 Individual Experiment Plots 

Figures 14 to 16 show the results of typical experimental runs, while Figures 17 to 20 show the results of individual experimental runs that exhibit the stuck state for part of their evolutionary process. 15 Convolution Convolution Max-Pool Dense 1@28x28 8@64x64 24@48x48 24@16x16 1x256 1x128 1x3 Figure 13. CNN architecture used in the experiments. Diagram created using NN-SVG (LeNail 2019). 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> Performance

Correct ( ̂ptest ) Correct ( ̂pval ) Abstain ( ̂ ptest ) Abstain ( ̂ pval ) Wrong ( ̂ptest ) Wrong ( ̂pval )

0.0 

5.0 

10.0 

> Semantic loss

Semantic loss 

0.0 

0.5 

1.0 

> Recon. loss

Reconstruction loss 

5 10 15 20 25 30 35 40 45 50 55 60 65 70 

Generations                                                                         

> BBBBBBBBBBBBBBBBBBBBBDNNNNNNNNNNNNNNNNNNNNBBBNNNDNBDNNNNBBBBBNNBNNNNNNNNB

S+ (Addition mutation) S0 (Clone mutation) S↓ (Simplification mutation) Npw (NN weights inherited) Nrw (NN weights not inherited) Homogeneous rule added 

1

Figure 14. Typical experimental run. 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> Performance

Correct ( ̂ptest ) Correct ( ̂pval ) Abstain ( ̂ ptest ) Abstain ( ̂ pval ) Wrong ( ̂ptest ) Wrong ( ̂pval )

0.0 

5.0 

10.0 

> Semantic loss

Semantic loss 

0.0 

0.5 

1.0 

> Recon. loss

Reconstruction loss 

5 10 15 20 25 30 35 40 45 50 55 60 65 70 

Generations                                                                         

> BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBDNBBBBBBBNNBBBBBBBBNNNNDNNNNNBBNBDNNNNNBBB

S+ (Addition mutation) S0 (Clone mutation) S↓ (Simplification mutation) Npw (NN weights inherited) Nrw (NN weights not inherited) Homogeneous rule added 

1

Figure 15. Typical experimental run. 

16 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> Performance

Correct ( ̂ptest ) Correct ( ̂pval ) Abstain ( ̂ ptest ) Abstain ( ̂ pval ) Wrong ( ̂ptest ) Wrong ( ̂pval )

0.0 

5.0 

10.0 

> Semantic loss

Semantic loss 

0.0 

0.5 

1.0 

> Recon. loss

Reconstruction loss 

5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 

Generations                                                                           

> BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBNBNBBBBNNNNNBBBB

S+ (Addition mutation) S0 (Clone mutation) S↓ (Simplification mutation) Npw (NN weights inherited) Nrw (NN weights not inherited) Homogeneous rule added 

1Figure 16. Typical experimental run. 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> Performance

Correct ( ̂ptest ) Correct ( ̂pval ) Abstain ( ̂ ptest ) Abstain ( ̂ pval ) Wrong ( ̂ptest ) Wrong ( ̂pval )

0.0 

5.0 

10.0 

> Semantic loss

Semantic loss 

0.0 

0.5 

1.0 

> Recon. loss

Reconstruction loss 

5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 

Generations                                                                                                                                      

> BBBBNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNBNBBBDBNDBDBBDBNDBBDBBDBDBBBBBBBBBBBBBBBBNDBBBBBBBBBBBBBBBBBBDBBBBBBBBBBBBBBBNNNNDDNNBNDBNBBBBBB

S+ (Addition mutation) S0 (Clone mutation) S↓ (Simplification mutation) Npw (NN weights inherited) Nrw (NN weights not inherited) Homogeneous rule added 

1

Figure 17. Experimental run showing the stuck state for part of the evolutionary process. 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> Performance

Correct ( ̂ptest ) Correct ( ̂pval ) Abstain ( ̂ ptest ) Abstain ( ̂ pval ) Wrong ( ̂ptest ) Wrong ( ̂pval )

0.0 

5.0 

10.0 

> Semantic loss

Semantic loss 

0.0 

0.5 

1.0 

> Recon. loss

Reconstruction loss 

5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 

Generations                                                                                                                                                                  

> BBBBBBBBBBNNNNNNNNNNBDBBBNDNNNNNNNNNNNNBBDBDBBBBDNBBBDDBDDBDBDBDDBNBNDBDBDDBBDDBNBDBDBBBBNBBBBBBBDBDBDBDBDDBBBDBBBBBBBBDBBDBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBNNDBB

S+ (Addition mutation) S0 (Clone mutation) S↓ (Simplification mutation) Npw (NN weights inherited) Nrw (NN weights not inherited) Homogeneous rule added 

1

Figure 18. Experimental run showing the stuck state for part of the evolutionary process. 

17 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> Performance

Correct ( ̂ptest ) Correct ( ̂pval ) Abstain ( ̂ ptest ) Abstain ( ̂ pval ) Wrong ( ̂ptest ) Wrong ( ̂pval )

0.0 

5.0 

10.0 

> Semantic loss

Semantic loss 

0.0 

0.5 

1.0 

> Recon. loss

Reconstruction loss 

5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175 180 185 

Generations                                                                                                                                                                                             

> BBBNNNNNNNNNNNNNNNBDNNNNNNNNNNNNNBNBDNNDNBNBDBDBDBDBDNBDBNNNBBDDBDDNDBBBDDNDNBBDDBDDBDDDNBDBBBDDDBDBDBBDDBNBDDDBBDBDDBDNBBBDBNBBBNDNBDBBDDBBBBBBBDDBDBBBDDBBBBBBBBBBNNNNNNNDDNNDNNNDBBDNNBBDB

S+ (Addition mutation) S0 (Clone mutation) S↓ (Simplification mutation) Npw (NN weights inherited) Nrw (NN weights not inherited) Homogeneous rule added 

1Figure 19. Experimental run showing the stuck state for part of the evolutionary process. 0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> Performance

Correct ( ̂ ptest ) Correct ( ̂ pval ) Abstain ( ̂ ptest ) Abstain ( ̂ pval ) Wrong ( ̂ptest ) Wrong ( ̂ pval )

0.0 

2.5 

5.0 

7.5 

> Semantic loss

Semantic loss 

0.0 

0.5 

1.0 

> Recon. loss

Reconstruction loss 

5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175 180 185 190 195 200 205 210 215 220 225 230 235 240 245 250 255 260 265 270 275 280 285 290 295 300 305 310 315 320 325 330 335 

Generations 

> BNNBDDBDBDBBNDNBBDBDDBDBBDBDDBDDBBDDDBBBBBBBDDDBBDBBDBDBDDBBDBDBBDDDNBBDBBDDDNNBBBBDBBDBBBDDDBBBDBDBDBDDBDBDNBDBBDDDBDBDNDDNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNBBBBBBDBDDBDBDBNBBDBBDDBBDDBBBDBBBDDBBDBBDBBBBBBBBDDBBBDBDBBBBBBDNDDBBBBBBBBDDDDBBBBDNBDDDDDDBBBBBDDDBBDBBDDNDNDBBBBDBBBBBBBBBBBNNNNNDNBDDBNBNNNNNBNNNNNNDNNNNNNNNNNNNNNNBDBNNNDBDNB

S+ (Addition mutation) S0 (Clone mutation) S↓ (Simplification mutation) Npw (NN weights inherited) Nrw (NN weights not inherited) Homogeneous rule added 

1

Figure 20. Experimental run showing the stuck state for part of the evolutionary process. 

18